<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 59]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: 本文提出了一种利用图卷积和LSTMs进行上下文嵌入的方法，在有限资源下实现了有效的下一个词预测。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语言模型需要大量的参数、文本数据和计算资源，而我们旨在解决语言建模中的一个子任务——上下文嵌入。

Method: 我们提出了一种利用图卷积操作来编码上下文，并将其与LSTMs结合以预测下一个词的方法。

Result: 我们在自定义的Wikipedia文本语料库上测试了该方法，并展示了其在有限资源下的有效性。

Conclusion: 该方法在有限的资源下能够有效地预测下一个词。

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [2] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Xuanzhao Dong,Hao Wang,Haiyu Wu,Huayu Li,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法DRA，以解决现有GRPO方法在低资源设置下无法捕捉语义多样性的问题。DRA通过引入子模态互信息来增强多样性的奖励，从而提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法通常依赖于解决方案级别的和标量奖励信号，这些信号无法捕捉到采样完成之间的语义多样性，导致了多样性-质量不一致的问题。

Method: 本文提出了DRA方法，该方法利用子模态互信息（SMI）来减少冗余的完成并增强多样性的奖励。DRA可以与GRPO及其变体DR.GRPO无缝集成，形成DRA-GRPO和DGA-DR.GRPO。

Result: 本文的方法在五个数学推理基准测试中表现优于最近的强基线，使用仅7,000个微调样本和大约55美元的总训练成本，平均准确率达到58.2%。

Conclusion: 本文提出了一种新的方法DRA，它能够更好地捕捉语言模型在低资源设置下的语义多样性，并在数学推理基准测试中取得了最先进的性能。

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [3] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger,Francesco Salvi,Jiacheng Liu,Xiaoli Nan,Ramit Debnath,Barbara Fasolo,Evelina Leivada,Gabriel Recchia,Fritz Günther,Ali Zarifhonarvar,Joe Kwon,Zahoor Ul Islam,Marco Dehnert,Daryl Y. H. Lee,Madeline G. Reinecke,David G. Kamper,Mert Kobaş,Adam Sandford,Jonas Kgomo,Luke Hewitt,Shreya Kapoor,Kerem Oktar,Eyup Engin Kucuk,Bo Feng,Cameron R. Jones,Izzy Gainsburg,Sebastian Olschewski,Nora Heinzelmann,Francisco Cruz,Ben M. Tappin,Tao Ma,Peter S. Park,Rayan Onyonka,Arthur Hjorth,Peter Slattery,Qingcheng Zeng,Lennart Finke,Igor Grossmann,Alessandro Salatiello,Ezra Karger*

Main category: cs.CL

TL;DR: 本研究比较了大型语言模型和激励的人类说服者在说服能力上的表现，发现AI在说服方面已经超越了人类，这一发现强调了对齐和治理框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在比较大型语言模型和激励的人类说服者在说服能力上的表现，并探讨AI在说服方面的潜力及其对社会的影响。

Method: 我们直接比较了前沿大型语言模型（LLM；Claude Sonnet 3.5）与激励的人类说服者在交互式、实时对话测验环境中的说服能力。在这个预注册的大规模激励实验中，参与者（测验答题者）完成了一个在线测验，其中说服者（人类或LLM）试图说服测验答题者朝着正确或错误的答案方向前进。

Result: 我们发现，LLM说服者在方向性说服尝试中取得了比激励的人类说服者更高的遵从性，这表明在诚实（朝向正确答案）和欺骗（朝向错误答案）情境下，LLM具有更优越的说服能力。此外，当引导测验答题者朝向正确答案时，LLM说服者显著提高了答题者的准确性，从而获得了更高的收益；而当引导他们朝向错误答案时，LLM说服者则显著降低了他们的准确性，导致收益减少。

Conclusion: 我们的研究结果表明，AI的说服能力已经超过了那些有真实金钱奖励的人类说服者。这一发现凸显了新兴对齐和治理框架的紧迫性。

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [4] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 本文介绍了双层系统提示优化的问题，并提出了一种元学习框架，以优化系统提示并使其适用于不同的任务和领域。实验结果表明，该方法在多个数据集上表现出色，能够有效推广系统提示并实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 现有的工作主要集中在针对单个查询或任务的特定用户提示上，而忽略了一旦优化后可在不同任务和领域中使用的系统提示。因此，我们引入了双层系统提示优化的新问题，其目标是设计对各种用户提示具有鲁棒性并可转移到未见过的任务的系统提示。

Method: 我们提出了一个元学习框架，通过在多个数据集上的各种用户提示上优化系统提示，同时以迭代方式更新用户提示，以确保它们之间的协同作用。

Result: 我们的方法在14个跨5个不同领域的未见过的数据集上进行了实验，结果表明，我们的方法能够有效地将系统提示推广到各种用户提示，并且优化后的系统提示能够在测试时用户提示上实现快速适应，同时提高性能。

Conclusion: 我们的方法在14个跨5个不同领域的未见过的数据集上进行了实验，结果表明，我们的方法能够有效地将系统提示推广到各种用户提示，并且优化后的系统提示能够在测试时用户提示上实现快速适应，同时提高性能。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [5] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu,Lechen Zhang,Sheza Munir,Yiyang Gu,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了 VeriFact 和 FactRBench，分别用于提高事实评估的准确性和全面性，并展示了它们在评估大型语言模型方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于生成的事实中存在复杂的句子间依赖关系，评估大型语言模型（LLMs）的事实性仍然具有挑战性。以前的解决方案大多遵循分解-去上下文化-验证的流程，但往往无法捕捉到必要的上下文并遗漏关键的关系事实。

Method: 本文引入了 VeriFact，这是一个事实评估框架，旨在通过识别和解决不完整和缺失的事实来增强事实提取，以支持更准确的验证结果。此外，还引入了 FactRBench，这是一个基准测试，用于评估长格式模型响应中的精度和召回率，而之前的工作主要关注精度。

Result: 实证评估显示，VeriFact 显著提高了事实完整性并保留了包含关键关系信息的复杂事实，从而实现了更准确的事实评估。在 FactRBench 上对各种开源和闭源 LLM 的基准测试表明，同一模型家族中的大模型提高了精度和召回率，但高精度并不总是与高召回率相关，这突显了全面事实评估的重要性。

Conclusion: VeriFact 显著提高了事实完整性并保留了包含关键关系信息的复杂事实，从而实现了更准确的事实评估。在 FactRBench 上对各种开源和闭源 LLM 的基准测试表明，同一模型家族中的大模型提高了精度和召回率，但高精度并不总是与高召回率相关，这突显了全面事实评估的重要性。

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [6] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: 本文介绍了一种利用大型语言模型（LLMs）进行文本分析的方法，包括开发、测试和应用分类法的步骤，并讨论了其可能性和局限性。


<details>
  <summary>Details</summary>
Motivation: 分析文本如开放式回答、标题或社交媒体帖子是一个耗时且易受偏见影响的过程。LLMs是用于文本分析的有前途的工具，可以使用预定义（自上而下）或数据驱动（自下而上）的分类法，而不牺牲质量。

Method: 本文提供了一个分步教程，通过研究人员与LLMs之间的迭代和协作过程，高效地开发、测试和应用用于分析非结构化数据的分类法。

Result: 本文展示了如何编写提示来审查数据集并生成生活领域分类法，通过提示和直接修改评估和优化分类法，测试分类法并评估编码者间的一致性，并使用高编码者间可靠性对整个数据集进行分类。

Conclusion: 本文讨论了使用大型语言模型（LLMs）进行文本分析的可能性和局限性。

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [7] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: 本文提出了一种新的分词器移植方法TokenAdapt，能够有效解决预训练语言模型中的分词器锁定问题，并在多个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的分词器替换方法通常需要大量的计算资源和残差微调，无法充分保留语义细微差别或解决压缩效率问题。

Method: TokenAdapt是一种与模型无关的分词器移植方法，结合了局部估计和全局估计来初始化新的唯一标记嵌入。此外，还提出了多词超标记的预分词学习以提高压缩效果并减少碎片化。

Result: 实验结果验证了TokenAdapt的有效性，其在零样本困惑度方面显著优于ReTok和TransTokenizer基线方法，并且在多个基础模型和新训练的目标分词器上表现优异。

Conclusion: TokenAdapt框架通过引入两种创新方法，成功解决了预训练语言模型中分词器锁定的问题，并在多个基准测试中表现出色。

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [8] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: 本研究探讨了使用自然语言处理（NLP）技术，特别是命名实体识别（NER），来自动从电子健康记录中提取与肺癌和乳腺癌相关的关键临床信息。研究利用了uQuery工具和微调后的bsc-bio-ehr-en3模型，结果显示NLP技术在识别某些实体方面表现良好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 手动从临床报告中提取信息的过程既耗时又容易出错，限制了数据驱动方法在医疗保健中的效率。因此，需要一种自动化的方法来提高数据提取的准确性和效率。

Method: 研究使用了GMV的NLP工具uQuery来识别临床文本中的相关实体，并将其转换为标准化格式。此外，还对基于RoBERTa的生物医学语言模型bsc-bio-ehr-en3进行了微调，以执行命名实体识别（NER）。

Result: 研究结果显示，NLP技术在从电子健康记录中提取关键临床信息方面表现良好，尤其是在识别如MET和PAT等实体方面。然而，对于较少见的实体如EVOL，仍存在挑战。

Conclusion: 研究结果表明，NLP技术在从电子健康记录中提取关键临床信息方面表现良好，特别是在识别如MET和PAT等实体方面。然而，对于较少见的实体如EVOL，仍存在挑战。

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [9] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 本文研究了LLM中真理方向在不同对话格式中的泛化能力，并提出了一种通过添加固定关键词短语来提高泛化能力的方法。结果表明，这种方法在某些情况下有效，但也揭示了构建可靠LLM谎言检测器的挑战。


<details>
  <summary>Details</summary>
Motivation: 最近的一些工作认为LLM具有一个普遍的真理方向，其中真实和虚假的陈述在模型的激活空间中是线性可分的。然而，这种真理方向在不同对话格式中的泛化能力仍有待研究。

Method: 我们探索了这种真相方向在各种对话格式之间的泛化情况，并通过在每段对话的末尾添加一个固定的关键词短语来提出一种解决方案。

Result: 我们发现，在以谎言结束的简短对话之间有良好的泛化能力，但在较长的对话格式中，当谎言出现在输入提示的早期时，泛化能力较差。通过添加一个固定的关键词短语，我们的解决方案显著提高了这种泛化能力。

Conclusion: 我们的结果突出了在可靠地检测LLM谎言方面面临的挑战，这些检测方法需要推广到新的设置。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [10] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui,Juan Diego Rodriguez,Philippe Laban,Dean Murphy,Joseph P. Dexter,Richard Jean So,Samuel Baker,Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 本文介绍了KRISTEVA，这是第一个用于评估解释性推理的细读基准，包含1331个选择题。通过三个逐步增加难度的任务集，测试大语言模型对文学作品的理解和推理能力，结果显示它们的表现仍落后于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 细读作为批判性思维的基础，被广泛采用为大学课程的必修部分，但从未在大型语言模型上进行过评估，而多学科基准如MMLU并不包括文学作为主题。

Method: 我们提出了KRISTEVA，这是第一个用于评估解释性推理的细读基准，包含从课堂数据中改编的1331个选择题。我们提出了三个逐步增加难度的任务集，以近似细读过程的不同元素，测试大语言模型如何理解并推理文学作品。

Result: 我们的基线结果发现，尽管最先进的大语言模型具有一些大学水平的细读能力（准确率49.7% - 69.7%），但它们的表现仍落后于经验丰富的评估者。

Conclusion: 尽管最先进的大语言模型具有一些大学水平的细读能力，但它们在11项任务中的10项上仍落后于经验丰富的评估者。

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [11] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在冲突预测中的潜力，并发现通过结合外部知识可以提高其性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型是否能够利用其预训练权重中的参数化知识来预测冲突升级和死亡人数，这对于早期预警系统、人道主义规划和政策制定至关重要。

Method: 研究比较了大型语言模型在参数化和非参数化设置下的冲突预测能力，其中参数化设置仅依赖预训练知识，而非参数化设置则结合了结构化和非结构化上下文信息。

Result: 研究结果表明，大型语言模型在冲突预测中表现出一定的能力，但其性能可以通过引入外部信息得到提升。

Conclusion: 研究发现，大型语言模型在冲突预测中既有优势也有局限性，并且通过结构化外部知识的增强可以提高其性能。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [12] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila,Esteban Villa Turek,Ellen Karina Chumbe Fernandez,Luis Felipe Polo Galvez,Luis Cadavid,Andrea Marroquin,Rebeca Vargas Quesada,Johanna Crew,Nicole Vallejo Galarraga,Christopher Rodriguez,Diego Gutierrez,Radhi Datla*

Main category: cs.CL

TL;DR: 本文探讨了拉丁美洲和西班牙之间西班牙语书面形式的差异，强调了区域本地化模型的重要性，以解决社会语言不和谐问题，并促进用户对人工智能语言模型的信任。


<details>
  <summary>Details</summary>
Motivation: 强调区域本地化模型的必要性，因为这些差异在方言群体的日常使用中造成了显著的空白，从而产生社会语言不和谐。

Method: 本文通过深入的社会文化及语言背景分析，考察了拉丁美洲和西班牙之间书面西班牙语的主要差异。

Result: 这种做法有助于制定更好的本地化策略，更有效地满足包容性目标，并确保在主要低风险投资地理区域的可持续活跃用户增长。

Conclusion: 实施至少提出的五个西班牙语子变体可以实现两个行动方向：促进用户对人工智能语言模型的信任和依赖，同时展示一种反映国际战略积极形象的文化、历史和社会语言意识。

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [13] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang,Yubing Ren,Yanan Cao,Binxing Fang*

Main category: cs.CL

TL;DR: 本文提出了一种多功能的共生水印框架，结合了基于logits和基于采样的方案，以优化水印的可检测性、鲁棒性、文本质量和安全性。实验结果表明，该方法优于现有基线并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的兴起加剧了对AI生成文本滥用的担忧，使水印成为一种有前景的解决方案。主流的LLM水印方案分为两类：基于logits和基于采样的方案。然而，当前方案在鲁棒性、文本质量和安全性之间存在权衡。

Method: 我们提出了一个多功能的共生水印框架，包含三种策略：串行、并行和混合。混合框架利用标记熵和语义熵自适应地嵌入水印，优化了可检测性、鲁棒性、文本质量和安全性之间的平衡。

Result: 通过在各种数据集和模型上的综合实验验证了我们的方法。实验结果表明，我们的方法优于现有的基线，并实现了最先进的（SOTA）性能。

Conclusion: 我们相信这个框架为多种水印范式提供了新的见解。

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [14] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu,Hanzhang Zhou,Zijian Feng,Tianjiao Li,Chua Jia Jim Deryl,Mak Lee Onn,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: 本文提出了一种名为MePO的轻量级、本地可部署的提示优化器，它通过学习清晰、可解释的优点，有效地泛化到大规模和轻量级推理模型。实验表明，MePO在各种任务和模型类型中取得了更好的结果，为实际部署提供了一个可扩展且稳健的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于像GPT-4这样的先进、大规模的LLM来生成优化的提示。然而，由于向下兼容性有限，来自先进LLM的冗长、指令密集的提示可能会使轻量级推理模型不堪重负并降低响应质量。

Method: 我们首先确定了一组与模型无关的提示质量优点，并通过实验证明了它们在提高提示和响应质量方面的有效性。然后，我们引入了MePO，这是一种基于优点引导的轻量级、本地可部署的提示优化器，它是在我们从由轻量级LLM生成的对齐优点提示构建的偏好数据集上训练的。

Result: MePO在各种任务和模型类型中取得了更好的结果，提供了一个可扩展且稳健的解决方案，适用于实际部署。

Conclusion: 实验表明，MePO在各种任务和模型类型中取得了更好的结果，为实际部署提供了一个可扩展且稳健的解决方案。我们的模型和数据集可在https://github.com/MidiyaZhu/MePO获取。

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [15] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.CL

TL;DR: 本案例研究应用了分阶段的超参数优化过程，比较了使用多阶段学习率调度和优化器参数分组的多任务自然语言模型变体。我们采用短时的贝叶斯优化会话，利用多保真度、超参数空间剪枝、渐进式减半和一定程度的人类指导。我们使用了Optuna TPE采样器和Hyperband剪枝器，以及Scikit-Learn高斯过程最小化。我们通过高效低保真度的冲刺来剪枝超参数空间，随后的冲刺逐步增加模型保真度并使用Hyperband剪枝以提高效率。此外，我们还使用了元学习器来调整分类概率的阈值。


<details>
  <summary>Details</summary>
Motivation: 为了提高多任务自然语言模型的性能，我们需要有效地优化其超参数，并通过使用元学习器来调整分类概率的阈值。

Method: 我们应用了分阶段的超参数优化过程，比较了使用多阶段学习率调度和优化器参数分组的多任务自然语言模型变体。我们采用了短时的贝叶斯优化会话，利用多保真度、超参数空间剪枝、渐进式减半和一定程度的人类指导。我们使用了Optuna TPE采样器和Hyperband剪枝器，以及Scikit-Learn高斯过程最小化。

Result: 我们通过高效低保真度的冲刺来剪枝超参数空间，随后的冲刺逐步增加模型保真度并使用Hyperband剪枝以提高效率。此外，我们还使用了元学习器来调整分类概率的阈值。

Conclusion: 我们的方法在2021年Eberts和Ulges提出的联合实体和关系抽取模型的变体上得到了验证。

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [16] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 本文提出了一种使用知识图谱来增强大型语言模型生成个性化响应的方法，并展示了其在准确性和效率上的优势。


<details>
  <summary>Details</summary>
Motivation: LLMs经常由于过度拟合而产生额外和错误的数据，导致输出中的幻觉。问题的一个根本原因是缺乏及时、事实性和个性化信息输入到LLM中。

Method: 我们提出了一种方法，通过引入检索增强生成（RAG）使用知识图谱（KGs）来辅助LLM生成个性化的用户响应。

Result: 实验结果表明，我们的方法在理解个人信息和生成准确响应方面显著优于使用个人数据作为文本输入的基线LLMs，同时响应时间有适度减少。

Conclusion: 我们的方法在理解个人信息和生成准确响应方面显著优于使用个人数据作为文本输入的基线LLMs，同时响应时间有适度减少。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [17] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin,Fan Huang*

Main category: cs.CL

TL;DR: 本文提出了一个衡量LLM隐性偏见的方法DIF，并发现其与回答准确性呈反向关系。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准方法来衡量LLM中的隐性偏见，本文旨在填补这一空白。

Method: 本文通过评估现有的LLM逻辑和数学问题数据集，并使用社会人口学角色来计算DIF基准。

Result: 本文发现，DIF方法可以统计验证LLM行为中的隐性偏见，并且发现回答准确性和隐性偏见之间存在反向趋势。

Conclusion: 本文认为，LLM中的隐性偏见不仅是伦理问题，也是技术问题，并提出了一种衡量这种偏见的方法DIF。

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [18] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng,Jinhao Jiang,Zican Dong,Wayne Xin Zhao,Lei Fang*

Main category: cs.CL

TL;DR: 本文介绍了CAFE，一种两阶段的粗到细方法，用于增强多文档问答能力。通过逐步消除背景和干扰文档的负面影响，CAFE使响应更依赖于证据文档。实验表明，CAFE在多个基准测试中表现优异，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在平衡检索精度和召回率方面仍然面临挑战，影响了它们在回答问题方面的效果。

Method: CAFE是一种两阶段的粗到细方法，通过逐步消除背景和干扰文档的负面影响，使响应更依赖于证据文档。首先，使用检索头进行粗粒度过滤以识别和排序相关文档。然后，使用细粒度引导方法将注意力引导到最相关的内容上。

Result: CAFE在多个基准测试中表现出色，优于基线方法，在Mistral模型上分别比SFT和RAG方法提升了22.1%和13.7%的SubEM。

Conclusion: CAFE在多个基准测试中表现出色，优于基线方法，在Mistral模型上分别比SFT和RAG方法提升了22.1%和13.7%的SubEM。

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [19] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: This paper discusses the vulnerability of Large Language Models (LLMs) to jailbreak attacks and highlights the growing threat posed by dark LLMs models. The research found a universal jailbreak attack that can compromise multiple state-of-the-art models, leading to harmful outputs. The study emphasizes the need for improved AI safety practices and highlights the risks associated with the proliferation of open-source LLMs.


<details>
  <summary>Details</summary>
Motivation: To identify the growing threat posed by dark LLMs models and highlight the need for improved AI safety practices.

Method: Research on the vulnerability of LLMs to jailbreak attacks and the threat posed by dark LLMs models.

Result: A universal jailbreak attack was uncovered that effectively compromises multiple state-of-the-art models, enabling them to produce harmful outputs upon request.

Conclusion: LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated without decisive intervention.

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [20] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah,Francois Meyer*

Main category: cs.CL

TL;DR: 本文首次系统地调查了PLMs在非洲语言中的语言知识，发现针对非洲语言调整的PLMs比大规模多语言PLMs编码了更多的目标语言语言信息，并强调了主动学习和多语言适应策略成功背后的内部机制。


<details>
  <summary>Details</summary>
Motivation: 尽管针对非洲语言的预训练语言模型（PLMs）不断改进，但这些进步的原因仍然不清楚。本文首次系统地调查了PLMs在非洲语言中的语言知识。

Method: 我们为六种语言学上多样化的非洲语言训练了逐层探测器，以分析语言特征的分布情况。我们还设计了控制任务，以解释探测器性能。

Result: 我们发现，针对非洲语言调整的PLMs比大规模多语言PLMs编码了更多的目标语言语言信息。结果确认了之前的发现，即词法级别的句法信息集中在中间到最后一层，而句子级别的语义信息分布在所有层中。通过控制任务和探测基线，我们确认性能反映了PLMs的内部知识，而不是探测器的记忆。

Conclusion: 我们的研究应用了已建立的可解释性技术来分析非洲语言的PLMs，并强调了主动学习和多语言适应策略成功背后的内部机制。

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [21] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu,Sony Trenous,Leonardo F. R. Ribeiro,Bill Byrne,Felix Hieber*

Main category: cs.CL

TL;DR: The paper introduces XRAG, a new benchmark for evaluating LLMs in cross-lingual RAG settings. It highlights the challenges faced by LLMs in monolingual and multilingual retrieval scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results.

Method: XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers real-world scenarios of monolingual and multilingual retrieval and provides relevancy annotations for each retrieved document.

Result: Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.

Conclusion: XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity.

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [22] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan,Di Wu,Yibin Lei,Christof Monz,Iacer Calixto*

Main category: cs.CL

TL;DR: 本文介绍了S-MedQA数据集，用于评估大型语言模型在医学问答中的表现，并发现改进主要来自于领域转移而非知识注入。


<details>
  <summary>Details</summary>
Motivation: 为了检验知识注入在医学QA中的适用性，并探讨微调数据在医学领域的作用。

Method: 我们引入了S-MedQA，一个用于基准测试大型语言模型在细粒度临床专业中的英语医学问答（QA）数据集。我们使用S-MedQA来检验一个与知识注入相关的流行假设在医学QA的知识密集场景中的适用性。

Result: 1) 在特定专业数据上进行训练并不一定能在该专业上获得最佳性能；2) 无论在哪个专业上进行微调，所有专业的临床相关术语的token概率都会一致增加。

Conclusion: 我们相信改进主要来自于领域转移（例如，从一般到医学），而不是知识注入，并建议重新考虑微调数据在医学领域的角色。

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [23] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da,Parth Mitesh Shah,Kuan-Ru Liou,Jiaxing Zhang,Hua Wei*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的检索增强生成框架GE-Chat，以提高大型语言模型输出的可靠性，并通过多种技术手段实现准确的证据检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在人类决策过程中起着关键作用，但它们的输出并不总是可靠的，用户需要手动评估。幻觉响应常常带来信任问题，因此需要一种可靠的方法来验证大型语言模型的结论。

Method: 本文提出了一种基于知识图谱的检索增强生成框架GE-Chat，通过构建知识图谱来增强代理的响应，并结合链式思维逻辑生成、多跳子图搜索和基于蕴含的句子生成技术实现准确的证据检索。

Result: 本文的方法在自由形式上下文中识别精确证据方面优于现有模型，为检查大型语言模型的结论资源提供了可靠的方法，并有助于判断其可信度。

Conclusion: 本文提出了一种基于知识图谱的检索增强生成框架GE-Chat，以提供基于证据的响应生成。该方法通过构建知识图谱并结合链式思维逻辑生成、多跳子图搜索和基于蕴含的句子生成，提高了现有模型在自由形式上下文中识别精确证据的能力，为检查大型语言模型结论的资源提供了可靠的方法，并有助于判断其可信度。

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [24] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: This study evaluates Reasoning CPT, a method that uses synthetic data to improve reasoning capabilities in large language models. The results show that Reasoning CPT improves performance across multiple domains and allows models to adjust their reasoning depth based on problem difficulty.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate Reasoning CPT, a form of continual pretraining that uses synthetic data to reconstruct hidden thought processes underlying texts, as an alternative to task-specific signals.

Method: The study applies Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compares it to standard CPT on the MMLU benchmark.

Result: Reasoning CPT improves performance across all evaluated domains, with significant gains on challenging problems. Models trained with hidden thoughts adjust the depth of their reasoning according to problem difficulty.

Conclusion: Reasoning CPT consistently improves performance across all evaluated domains, and reasoning skills acquired in one domain transfer effectively to others.

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [25] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文介绍了CoT Encyclopedie，一个用于分析和引导模型推理的自下而上框架。该方法能够自动提取推理标准，进行聚类，并推导出对比性标准。实验结果表明，该框架比现有方法更可解释和全面，并能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法受限于人类直觉，无法捕捉模型行为的全部多样性。我们需要一种更全面和可解释的方法来分析模型推理。

Method: 我们引入了CoT Encyclopedie，这是一个自下而上的框架，用于分析和引导模型推理。我们的方法从模型生成的CoT中自动提取多样的推理标准，将它们嵌入到语义空间中，聚类成代表性的类别，并推导出对比性标准来解释推理行为。

Result: 人类评估显示，该框架比现有方法产生了更可解释和全面的分析。此外，我们证明了这种理解可以带来性能提升：我们可以预测模型可能使用的策略，并引导它走向更有效的替代方案。

Conclusion: 我们的研究表明，通过分析和引导模型推理，可以提高模型性能，并且训练数据格式对推理行为的影响比数据领域更大。

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [26] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao,Hongyi Huang,Jiayi Wu,YiMing Cheng,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为VQ-Logits的新方法，利用向量量化（VQ）大幅减少大型语言模型（LLM）输出层的参数数量和计算负担。通过将大的V * dmodel输出嵌入矩阵替换为一个小的、共享的K个嵌入向量的代码本（K << V），每个词汇表中的标记被映射到这些K个代码本向量之一。LLM在紧凑的代码本上预测logits，然后使用学习或预分配的映射高效地“散开”到完整的词汇空间。实验表明，VQ-Logits可以在输出层实现高达99%的参数减少，并在logit计算中实现6倍的速度提升，与完整softmax基线相比，困惑度仅增加4%。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have achieved remarkable success but face significant computational and memory challenges, particularly due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference.

Method: VQ-Logits replaces the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V ). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently 'scattered' to the full vocabulary space using the learned or preassigned mapping.

Result: We demonstrate through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines. We further provide detailed ablation studies on codebook size, initialization, and learning strategies, showcasing the robustness and effectiveness of our approach.

Conclusion: VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines.

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [27] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang,Kaili Sun,Bowen Wu,Qun Yu,Ying Li,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文提出了RAIDEN-R1，一种新的强化学习框架，用于改善角色扮演对话代理的角色一致性。通过引入可验证的角色意识奖励和构建高质量的数据集，实验结果表明该方法在多个指标上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 角色扮演对话代理（RPCAs）在保持角色一致性方面面临持续的挑战。现有的方法难以量化角色一致性，因此需要一种新的方法来解决这一问题。

Method: 提出了一种新的强化学习框架RAIDEN-R1，集成了可验证的角色意识奖励（VRAR）。该方法引入了单个和多项挖掘策略，通过评估特定角色的关键因素来生成可量化的奖励。此外，还通过多LLM协作构建了一个高质量的角色意识思维链数据集，并进行了实验以提高推理连贯性。

Result: 在RAIDEN基准测试中，RAIDEN-R1表现出色：我们的14B-GRPO模型在基于脚本的知识和对话记忆指标上分别达到了88.04%和88.65%的准确率，优于基线模型，同时保持了鲁棒性。案例分析进一步揭示了模型在解决冲突上下文线索和维持第一人称叙述一致性方面的能力增强。

Conclusion: 本研究填补了RPCA训练中非量化性差距，并提供了关于角色感知推理模式的见解，推动了RPCA的发展。

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [28] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 本研究评估了多种先进LLM在零样本和少量样本条件下对包含俄语和乌克兰语社交媒体帖子的复杂文本数据集进行二分类标注的能力，重点是识别其中的人权侵犯参考。研究结果揭示了LLM在处理主观和依赖上下文的判断时的优缺点，并探讨了它们在跨语言适应性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理（NLP）系统的日益复杂，大型语言模型（LLMs）在各种应用中展现了巨大潜力，包括需要细微文本理解和上下文推理的任务。然而，对于敏感且特定领域的任务，如识别人权侵犯参考，仍需进一步研究LLMs的可靠性与适用性。

Method: 本研究评估了多种最先进的LLM（GPT-3.5、GPT-4、LLAMA3、Mistral 7B和Claude-2）在零样本和少量样本条件下对包含俄语和乌克兰语社交媒体帖子的复杂文本数据集进行二分类标注的能力，重点是识别其中的人权侵犯参考。此外，还分析了不同提示条件下的标注性能，并探索了每种模型表现出的独特错误模式和分歧。

Result: 研究发现，不同LLM在零样本和少量样本条件下的标注性能存在差异，并揭示了它们在处理主观和依赖上下文的判断时的优缺点。同时，研究还展示了这些模型在跨语言适应性方面的表现。

Conclusion: 本研究通过将LLM输出与人工标注进行对比，有助于理解LLM在多语言背景下对敏感领域任务的可靠性和适用性，并揭示了语言模型如何处理本质上主观和依赖上下文的判断，这对于其在现实场景中的部署至关重要。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [29] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 本文分析了19,123项研究，发现生成式大语言模型在开放性任务中优于传统自然语言处理，但伦理使用这些技术对医疗应用至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索生成式大语言模型（LLMs）与传统自然语言处理（NLP）在不同医学任务中的差异，以更好地理解它们的优劣。

Method: 分析了19,123项研究，比较了生成式大语言模型（LLMs）和传统自然语言处理（NLP）在不同医学任务中的表现。

Result: 生成式LLMs在开放性任务中表现出优势，而传统NLP在信息提取和分析任务中占主导地位。

Conclusion: 随着这些技术的进步，确保它们在医疗应用中的潜力，伦理使用它们是至关重要的。

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [30] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: 本研究介绍了Quicker，一个基于大型语言模型的循证临床决策支持系统，旨在自动化证据综合并生成符合标准临床指南开发流程的临床建议。实验结果表明，Quicker表现出色，能够有效支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 将临床证据整合到实时实践中具有挑战性，因为工作量大、专业流程复杂且时间有限。因此，需要自动化证据综合工具来支持更高效和准确的临床决策。

Method: 本研究引入了Quicker，这是一个基于大型语言模型（LLMs）的循证临床决策支持系统，旨在自动化证据综合并生成符合标准临床指南开发流程的临床建议。

Result: 实验结果表明，Quicker表现出色，具有细粒度的问题分解，检索灵敏度与人类专家相当，文献筛选性能接近全面纳入相关研究。此外，Quicker辅助的证据评估有效支持了人类评审员，而Quicker的建议比临床医生的建议更全面且逻辑更连贯。

Conclusion: 我们的研究结果证实了Quicker在帮助医生更快、更可靠地做出基于证据的临床决策方面的潜力。

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [31] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: J1 is a reinforcement learning method for training LLM-as-a-Judge models that improves their judgment ability by using verifiable rewards. It outperforms other models and provides analysis of different training strategies.


<details>
  <summary>Details</summary>
Motivation: The quality of evaluation is a bottleneck for AI progress, and powerful LLM-as-a-Judge models are a core solution. Improving their judgment ability requires finding the best training recipes.

Method: J1 uses reinforcement learning to train LLM-as-a-Judge models. It converts both verifiable and non-verifiable prompts into judgment tasks with verifiable rewards, which incentivize thinking and reduce bias.

Result: J1 outperforms all other existing 8B or 70B models, including those distilled from DeepSeek-R1. It also outperforms o1-mini and even R1 on some benchmarks despite being a smaller model.

Conclusion: J1 is a reinforcement learning approach that improves the judgment ability of LLM-as-a-Judge models by converting prompts to judgment tasks with verifiable rewards. It outperforms other models, including larger ones, and provides insights into effective training strategies.

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [32] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang,Zhanyu Shen,Hui Huang*

Main category: cs.CL

TL;DR: This paper proposes LDIR, a low-dimensional, dense, and interpretable text embedding method that offers both semantic representation and traceability.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods have excellent performance but lack traceability and interpretability. Bag-of-words, while interpretable, has poor performance. Benara et al. (2024) proposed interpretable text embeddings using large language models, but they are high-dimensional. We aim to create a low-dimensional, dense, and interpretable text embedding method.

Method: We propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling.

Result: LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions.

Conclusion: LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [33] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye,Shaonan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于从多种输入模态中重建连贯的语言，该方法在性能上与最先进的系统相当，并且具有适应性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究通常局限于单模态输入，如图像或音频，而人类思维本质上是多模态的。为了弥合这一差距，我们提出了一个统一且灵活的框架。

Method: 我们提出了一种统一且灵活的框架，用于从由不同输入模态（视觉、听觉和文本）引发的脑记录中重建连贯的语言。我们的方法利用了视觉-语言模型（VLMs），使用特定于模态的专家来共同解释跨模态的信息。

Result: 实验表明，我们的方法在性能上与最先进的系统相当，同时保持了适应性和可扩展性。

Conclusion: 这项工作朝着更生态有效和可推广的思维解码迈进。

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [34] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型（LLMs）设计基于方面的情感分析系统，并展示了单一的多领域模型在处理多个领域特定分类法时的表现与专门的单领域模型相当，同时减少了操作复杂性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索使用大型语言模型（LLMs）设计基于方面的情感分析系统的实际应用，并解决跨领域和跨语言的四元组情感提取问题。

Method: 本文使用内部数据集，研究了单一微调模型是否可以同时有效处理多个领域特定的分类法。

Result: 本文展示了单一的多领域模型在处理多个领域特定分类法时的表现与专门的单领域模型相当，同时减少了操作复杂性。

Conclusion: 本文展示了单一的多领域模型在处理多个领域特定分类法时的表现与专门的单领域模型相当，同时减少了操作复杂性。

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [35] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 本文提出了RPG方法，用于减轻代码生成中的重复问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 代码生成中的重复问题仍然存在，尤其是结构重复问题，这需要一种更有效的解决方案。

Method: RPG（基于语法的重复惩罚）方法利用语法规则识别代码生成中的重复问题，并战略性地衰减导致重复的关键标记的可能性，从而减轻代码生成中的重复问题。

Result: RPG在CodeRepetEval数据集以及HumanEval和MBPP基准测试中表现出色，有效减少了重复并提高了生成代码的质量。

Conclusion: RPG显著优于最佳基线，在CodeRepetEval数据集以及HumanEval和MBPP基准测试中有效减少了重复并提高了生成代码的质量。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [36] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo,Jae Ho Sohn,Gondy Leroy,Trevor Cohen*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）生成的普通语言摘要（PLSs）的质量，并发现尽管LLMs生成的PLS在主观评估中表现良好，但人类撰写的PLS在理解方面更优。此外，自动化评估指标未能准确反映人类判断，这引发了对其适用性的质疑。研究强调了需要更全面的评估框架和优化普通读者理解的生成方法。


<details>
  <summary>Details</summary>
Motivation: 目前，自动化生成PLS的方法尚未得到充分验证，现有的评估方法主要依赖于无法直接衡量可理解性的自动评分或从便利样本中获得的主观Likert量表评分，这些评分的推广性有限。因此，需要一种更全面的评估方法来确保PLS的有效性。

Method: 本研究通过亚马逊Mechanical Turk进行了大规模众包评估，有150名参与者参与。我们通过主观Likert量表评分（关注简单性、信息量、连贯性和忠实性）和客观的多项选择理解和回忆测量来评估PLS质量。此外，我们还检查了10个自动化评估指标与人类判断的一致性。

Result: 研究结果表明，虽然LLMs生成的PLS在主观评估中与人类撰写的PLS难以区分，但人类撰写的PLS在理解方面表现更好。此外，自动化评估指标未能反映人类判断，这引发了对其用于评估PLS适用性的质疑。

Conclusion: 本研究表明，尽管大型语言模型（LLMs）可以生成在主观评估中与人类撰写的PLS难以区分的摘要，但人类撰写的PLS在理解方面表现更好。此外，自动化评估指标未能反映人类判断，这引发了对其用于评估PLS适用性的质疑。研究强调了需要超越表面质量的评估框架以及明确优化普通读者理解的生成方法。

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [37] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yongkang Wu,Zhonghua Li,Qi Ye,Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner is an efficient refiner that leverages the structural characteristics of long documents to reduce computational costs and improve performance in RAG applications.


<details>
  <summary>Details</summary>
Motivation: Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance.

Method: LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model.

Result: Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline.

Conclusion: LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications.

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [38] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang,Zhiyang Chen,Zijun Wang,Tiancheng Li,Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT is a new reasoning framework for diffusion language models that uses reinforcement learning to optimize the entire reasoning trajectory, leading to improved performance on math and code generation tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the reasoning capabilities of diffusion language models by optimizing the entire reasoning trajectory with RL, allowing more flexible and effective reasoning compared to traditional Chain-of-Thought (CoT) methods.

Method: DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). It allows bidirectional, non-linear reasoning and uses a probabilistic policy for continuous-time models and a ranking-based Unmasking Policy Module (UPM) for discrete-time models.

Result: DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. DCoLT-reinforced LLaDA shows significant improvements in reasoning accuracy on math and code generation tasks.

Conclusion: DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both, and DCoLT-reinforced LLaDA boosts its reasoning accuracy significantly on various tasks.

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [39] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang,Licheng Zhang,Zheren Fu,Zhendong Mao*

Main category: cs.CL

TL;DR: 本文提出了一种基于多阶段课程学习的RAG系统训练框架CL-RAG，通过构建不同难度的训练数据并分阶段训练模型，提升了RAG系统的整体性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法主要关注优化检索器或生成器，但文档的有效性在不同用户查询中差异很大，这阻碍了检索器和生成器在训练中的适应性。受人类认知学习的启发，课程学习通过从简单到困难的样本训练模型，从而提高其泛化能力，因此将其整合到RAG系统的训练中。

Method: 提出了一种基于多阶段课程学习的RAG系统训练框架，称为CL-RAG。通过样本演化分别构建具有多个难度级别的训练数据，然后基于课程学习方法分阶段训练模型。

Result: CL-RAG框架在四个开放域问答数据集上表现出一致的有效性，并且在多个先进方法上实现了2%到4%的性能提升。

Conclusion: CL-RAG框架在四个开放域问答数据集上表现出一致的有效性，并且在多个先进方法上实现了2%到4%的性能提升。

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [40] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou,Xiao Deng,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 本文提出了一个全面评估LLM代码安全的多任务基准CoV-Eval，并开发了一个改进的判断模型VC-Judge。实验结果表明，虽然大多数LLM能识别有漏洞的代码，但它们在生成安全代码和修复漏洞方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 当前的代码安全基准仅专注于单一的评估任务和范式，如代码补全和生成，缺乏在安全代码生成、漏洞修复和识别等维度上的全面评估。

Method: 本文提出了CoV-Eval，一个涵盖多种任务的多任务基准，用于全面评估LLM代码安全。此外，还开发了VC-Judge，一种改进的判断模型，可以更高效和可靠地审查LLM生成的程序中的漏洞。

Result: 尽管大多数LLM能够很好地识别有漏洞的代码，但它们仍然倾向于生成不安全的代码，并且在识别特定漏洞类型和执行修复方面存在困难。

Conclusion: 本文通过实验和定性分析揭示了LLM代码安全中的关键挑战和优化方向，为未来的研究提供了见解。

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [41] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文研究了基于词对齐（WAs）的标签投影在跨语言迁移学习（XLT）中的应用，发现其性能可以与基于标记的方法相媲美。我们还提出了一种新的投影策略，结合了translate-train和translate-test预测，显著优于基于标记的投影，并提高了XLT的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于词对齐（WAs）的标签投影方法在跨语言迁移学习（XLT）中被广泛使用，但其低级设计决策尚未得到系统研究。此外，最近的基于标记的方法声称在XLT的标签投影中优于WAs。因此，我们需要重新评估WAs在XLT中的表现，并探索改进方法。

Method: 我们重新审视了词对齐（WAs）在标签投影中的应用，系统地研究了低级设计决策对token级XLT的影响，包括：(i) 在（多）token跨度之间投影标签的算法，(ii) 减少噪声映射标签数量的过滤策略，以及(iii) 翻译句子的预分词。我们还引入了一种新的投影策略，该策略结合了translate-train和translate-test预测。

Result: 我们发现这些低级设计决策对基于翻译的XLT性能有显著影响。通过优化选择，基于WAs的XLT性能至少可以与基于标记的方法相媲美。我们引入的新投影策略结合了translate-train和translate-test预测，显著优于基于标记的投影。此外，我们的集成方法减少了对低级WA设计选择的敏感性，从而提高了XLT的鲁棒性。

Conclusion: 我们的研究发现，通过优化选择，基于词对齐的XLT可以达到与基于标记的方法相当的性能。此外，我们引入了一种新的投影策略，该策略结合了translate-train和translate-test预测，显著优于基于标记的投影。最后，我们证明了所提出的集成方法也减少了对低级WA设计选择的敏感性，从而在token分类任务中实现了更稳健的XLT。

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [42] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: MuToR is a novel approach to multi-token prediction that improves language model pretraining by introducing learnable register tokens, offering compatibility with existing models and demonstrating effectiveness across various tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-token prediction has shown promise for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. The paper aims to address this limitation by proposing an effective and compatible approach.

Method: MuToR interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. It introduces only a negligible number of additional parameters, requires no architectural changes, and remains aligned with the next-token pretraining objective.

Result: MuToR demonstrates effectiveness and versatility across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains.

Conclusion: MuToR is a simple and effective approach to multi-token prediction that offers several key advantages, including negligible additional parameters, compatibility with off-the-shelf pretrained language models, and alignment with the next-token pretraining objective. It demonstrates effectiveness and versatility across various use cases in both language and vision domains.

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [43] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: This paper proposes WorldPM, a preference modeling approach that emphasizes scaling potential. It shows that WorldPM improves the generalization performance across human preference datasets of varying sizes and achieves significant improvements in in-house and public evaluation sets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find similar scaling laws in preference modeling as those in language modeling, which demonstrate how test loss scales as a power law with model and dataset sizes.

Method: WorldPM is proposed to emphasize the scaling potential of preference modeling. Preference data is collected from public forums, and extensive training is conducted using 15M-scale data across models ranging from 1.5B to 72B parameters.

Result: WorldPM shows distinct patterns across different evaluation metrics: adversarial metrics scale up with increased training data and base model size, objective metrics show emergent behavior in larger language models, and subjective metrics do not demonstrate scaling trends. WorldPM improves the generalization performance across human preference datasets of varying sizes, with performance gains exceeding 5% on many key subtasks. It also shows significant improvements in in-house and public evaluation sets.

Conclusion: WorldPM is effective for preference fine-tuning and improves the generalization performance across human preference datasets of varying sizes. It also shows significant improvements in in-house and public evaluation sets.

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [44] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu,Yibo Wang,Hanze Dong,Yuhui Xu,Amrita Saha,Caiming Xiong,Bryan Hooi,Junnan Li*

Main category: cs.CL

TL;DR: 本文提出了一种显式对齐模型的元能力的方法，以提高大型推理模型的可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结果的强化学习方法虽然可以引发一些高级推理行为，但这些行为的时间和一致性不可预测，限制了大型推理模型的推理能力的可扩展性和可靠性。

Method: 本文通过三个阶段的管道：个体对齐、参数空间合并和领域特定强化学习，显式对齐模型的演绎、归纳和类比能力。

Result: 实验结果表明，该方法在数学、编程和科学基准测试中，相对于指令调优基线提高了超过10%的性能，并且在领域特定的强化学习中进一步提高了2%的性能上限。

Conclusion: 本文提出了一种显式对齐模型的元能力的方法，以提高大型推理模型的可扩展性和可靠性。

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [45] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在生成准确、安全和可访问的癌症相关信息方面的能力和局限性。结果表明，通用LLMs在语言质量和感染力方面表现更好，而医学LLMs在沟通可及性方面表现更佳。然而，医学LLMs倾向于表现出更高的潜在危害、毒性和偏见，从而降低了其在安全性和可信度方面的表现。研究结果强调了在健康通信中领域特定知识和安全性的二元性，并提出了需要有意设计模型并进行针对性改进的建议。


<details>
  <summary>Details</summary>
Motivation: 有效的乳腺癌和宫颈癌沟通仍然是一个持续的健康挑战，公众对癌症预防、筛查和治疗的理解存在显著差距，可能导致诊断延迟和治疗不足。本研究评估了大型语言模型（LLMs）在生成准确、安全和可访问的癌症相关信息以支持患者理解方面的能力和局限性。

Method: 本研究使用混合方法评估框架评估了五种通用和三种医学LLMs，包括语言质量、安全性和可信度、沟通可及性和感染力。我们的方法利用了定量指标、定性专家评分和统计分析（Welch's ANOVA、Games-Howell和Hedges' g）。

Result: 研究结果显示，通用LLMs在语言质量和感染力方面表现更好，而医学LLMs在沟通可及性方面表现更佳。然而，医学LLMs倾向于表现出更高的潜在危害、毒性和偏见，从而降低了其在安全性和可信度方面的表现。

Conclusion: 研究结果表明，领域特定知识和安全性在健康通信中存在二元性。需要有意地设计模型，并针对减少伤害和偏见以及提高安全性和感染力进行有针对性的改进。这项研究为癌症沟通中的LLMs提供了全面评估，为改进AI生成的健康内容提供了关键见解，并指导了未来准确、安全和可访问的数字健康工具的发展。

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales,Ayah Elsayed,Debbie Zhao,Filip Loncaric,Ainhoa Aguado,Mireia Masias,Gina Quill,Marc Ramos,Ada Doltra,Ana Garcia,Marta Sitges,David Marlevi,Alistair Young,Martyn Nash,Bart Bijnens,Oscar Camara*

Main category: cs.CV

TL;DR: 本文介绍了第一个专为左心房4D流动MRI分析而设计的开源计算框架，能够处理不同中心的数据并实现高精度的自动分割，同时首次全面评估了血流动力学参数作为预后生物标志物的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统超声分析在左心房血流动力学的理解上存在局限，而4D流动MRI由于低速和有限的空间分辨率使得分析该腔室变得困难。此外，缺乏专门的计算框架以及多样化的采集协议和供应商，使得收集大型队列来研究4D流动MRI提供的血流动力学参数的预后价值变得复杂。

Method: 我们引入了第一个专为左心房4D流动MRI分析而设计的开源计算框架，实现了先进血流动力学参数的全面定性和定量分析。

Result: 我们的框架能够处理来自不同中心的不同质量的数据，产生高精度的自动分割（Dice > 0.9和Hausdorff 95 < 3 mm），并且首次全面评估了能量、涡度和压力参数在各种疾病中的潜在预后生物标志物作用。

Conclusion: 我们的框架证明了对不同中心的数据具有鲁棒性，即使训练数据有限，也能产生高精度的自动分割，并首次全面评估了能量、涡度和压力参数在各种疾病中的潜在预后生物标志物作用。

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [47] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke,Takashi Shibuya,Kengo Uchida,Koichi Saito,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文介绍了Dyadic Mamba，一种基于状态空间模型（SSM）的新方法，用于生成任意长度的高质量双人人体运动。该方法通过连接在个体运动序列之间促进信息流，避免了复杂的交叉注意力机制。实验表明，Dyadic Mamba在短期基准上表现良好，并在更长的序列上优于基于变压器的方法。此外，作者还提出了一个新的基准来评估长期运动合成质量。


<details>
  <summary>Details</summary>
Motivation: 从文本描述生成现实的双人运动面临重大挑战，特别是对于超过典型训练序列长度的长期互动。虽然基于变压器的方法在短期双人运动合成方面表现出色，但由于位置编码方案的固有局限性，它们在更长的序列上表现不佳。

Method: 我们引入了Dyadic Mamba，这是一种新颖的方法，利用状态空间模型（SSM）生成任意长度的高质量双人人体运动。我们的方法采用了一种简单但有效的架构，通过连接在个体运动序列之间促进信息流，消除了对复杂交叉注意力机制的需求。

Result: 我们证明了Dyadic Mamba在标准短期基准上实现了具有竞争力的性能，同时在更长的序列上显著优于基于变压器的方法。此外，我们提出了一种新的基准来评估长期运动合成质量，为未来的研究提供了标准化框架。

Conclusion: 我们的结果表明，基于状态空间模型（SSM）的架构为解决从文本描述中合成长期双人人体运动这一具有挑战性的任务提供了有希望的方向。

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [48] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: 本文提出了一种名为BoundarySeg的多任务框架，通过引入器官边界预测作为辅助任务，利用两个任务预测之间的一致性来提供额外监督，从而在不依赖未标记数据或增加计算需求的情况下，实现了与最先进的半监督方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 由于严格的隐私法规和数据保护政策，获取大规模医疗数据具有挑战性。此外，标注医学图像需要领域专家手动分割解剖结构，这使得过程既耗时又昂贵。因此，半监督方法因其减少标注成本而受到欢迎。然而，半监督方法的性能严重依赖于未标记数据的可用性，当这些数据稀缺或不存在时，其效果会下降。

Method: 我们提出了一种名为BoundarySeg的多任务框架，将器官边界预测作为辅助任务，利用两个任务预测之间的一致性提供额外监督。

Result: 该策略提高了分割精度，特别是在数据量少的情况下，使我们的方法能够实现与最先进的半监督方法相当或更好的性能。

Conclusion: 我们的方法在不依赖未标记数据或增加计算需求的情况下，实现了与最先进的半监督方法相当或更好的性能。

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [49] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh,Isabel Funke,Micha Pfeiffer,Fiona Kolbinger,Hanna Maria Schmeiser,Juergen Weitz,Marius Distler,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本条件扩散的方法，用于生成高保真的手术视频，以解决数据不平衡问题，并在两个下游任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于手术视频数据集中常常存在严重的数据不平衡，这阻碍了高性能模型的发展，因此我们希望通过合成手术视频来克服这一问题。

Method: 我们提出了一种独特的两阶段、文本条件扩散方法，用于生成高保真的手术视频，以解决数据不平衡问题。该方法将生成过程条件化为文本提示，并通过使用2D潜在扩散模型来捕捉空间内容，然后集成时间注意力层以确保时间一致性。此外，我们引入了一种拒绝采样策略来选择最合适的合成样本，以有效增强现有数据集以解决类别不平衡问题。

Result: 我们在两个下游任务——手术动作识别和术中事件预测中评估了我们的方法，结果表明，结合我们方法生成的合成视频可以显著提高模型性能。

Conclusion: 我们的方法在两个下游任务中得到了验证，表明使用我们方法生成的合成视频可以显著提高模型性能。

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [50] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee,Taylor Webb,Trevor Bihl,Keith Holyoak,Hongjing Lu*

Main category: cs.CV

TL;DR: 本文介绍了概率模式归纳（PSI），这是一种利用深度学习在少量示例的结构化表示上执行类比映射的原型模型，从而形成组合概念。PSI表现出类似人类的学习表现，并优于其他两种模型。研究结果表明，结构化表示和类比映射对于建模快速的人类类似学习组合视觉概念至关重要。


<details>
  <summary>Details</summary>
Motivation: 学习从有限的例子中学习新的视觉概念是人类认知的特征。传统的类别学习模型将每个例子表示为无结构的特征向量，而组合概念学习被认为依赖于（1）例子的结构化表示（例如，由对象及其关系组成的有向图）和（2）通过类比映射识别跨例子的共享关系结构。

Method: 我们引入了概率模式归纳（PSI），这是一种原型模型，它使用深度学习在仅少数示例的结构化表示上执行类比映射，形成一个称为模式的组合概念。PSI依赖于一种新颖的相似性概念，该概念权衡对象级相似性和关系相似性，并且有一个机制可以放大对分类相关的关系，类似于传统模型中的选择性注意参数。

Result: PSI产生了类似人类的学习表现，并优于两个对照组：一个使用从深度学习模型中提取的无结构特征向量的原型模型，以及一个结构化表示较弱的PSI变体。值得注意的是，我们发现PSI的类似人类的表现是由一种适应性策略驱动的，该策略增加了关系相似性相对于对象级相似性的权重，并提高了区分类别的关系的贡献。

Conclusion: 这些发现表明，结构化表示和类比映射对于建模快速的人类类似学习组合视觉概念至关重要，并展示了如何利用深度学习来创建心理模型。

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [51] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin,Chenyang Wu,Penghui Huang,Yanyong Zhang,Yinian Mao,Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM is a large-scale 3DGS-based visual SLAM that uses stereo cameras and improves performance in large-scale outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored.

Method: LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. It introduces feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. Continuous Gaussian Splatting submaps are used to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition, and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. A structure refinement module enhances the reconstruction quality after global optimization of camera poses and Gaussian points.

Result: LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.

Conclusion: LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [52] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP is a simple yet effective method for universal visual anomaly detection that outperforms existing methods by learning adaptive visual and textual representations alternately and incorporating both contextual and aligned residual features in comparative learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios.

Method: AdaptCLIP is a simple yet effective method based on two key insights: adaptive visual and textual representations should be learned alternately rather than jointly, and comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters at its input or output ends.

Result: AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. It achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains.

Conclusion: AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods.

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [53] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin,Shaolei Liu,Manning Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无源域适应（SFDA）框架，通过预适应、数据依赖的频率提示和风格相关层微调策略，有效解决了域间差异问题，并在实验中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无源域适应（SFDA）方法在生成高质量伪标签和域间差异方面存在不足，同时在有限监督下训练整个模型效率较低。因此，需要一种新的方法来解决这些问题。

Method: 本文提出了一种新的无源域适应（SFDA）框架，包括预适应阶段生成预适应模型、数据依赖的频率提示以更有效地将目标域图像转换为源域风格，以及专门设计用于SFDA的风格相关层微调策略。

Result: 在跨模态腹部和心脏SFDA分割任务中，本文提出的的方法优于现有的最先进的方法。

Conclusion: 本文提出了一种新的无源域适应（SFDA）框架，以解决现有方法在生成高质量伪标签和域间差异方面的不足。实验结果表明，该方法在跨模态腹部和心脏SFDA分割任务中优于现有最先进的方法。

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [54] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: 本文提出了VRU-CIPI框架，用于预测VRU在交叉口的过街意图，结合GRU和多头Transformer自注意力机制，实现了高精度和实时推理，并通过I2V通信提高了交叉口的安全性。


<details>
  <summary>Details</summary>
Motivation: 理解并预测城市交叉口中人类行为对于提高道路使用者之间的交互安全性至关重要。VRU的过街意图的误判可能导致与车辆的危险冲突。

Method: VRU-CIPI框架采用基于序列注意力的模型，利用GRU捕捉VRU运动的时间动态，并结合多头Transformer自注意力机制来编码上下文和空间依赖关系，以预测VRU的过街意图。

Result: VRU-CIPI在UCF-VRU数据集上达到了96.45%的准确率，并实现了每秒33帧的实时推理速度。通过与I2V通信的集成，该方法能够通过及时激活过街信号和向联网车辆提供早期警告，主动增强交叉口的安全性。

Conclusion: VRU-CIPI框架通过结合GRU和多头Transformer自注意力机制，在UCF-VRU数据集上实现了最先进的性能，并通过与I2V通信的集成，提高了交叉口的安全性。

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [55] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan,Lei Zhou,Liu Mao,Shaofan Chen,Chuanqiu Ren,Xia Xie*

Main category: cs.CV

TL;DR: 本文提出了非注册变化检测任务，通过构建八个现实场景和相应的图像转换方案，揭示了该任务对现有方法的潜在危害。


<details>
  <summary>Details</summary>
Motivation: 为了解决日益增多的紧急情况（如自然灾害、人为事故和军事打击）带来的挑战，需要研究非注册变化检测。

Method: 提出八个可能在现实世界中出现并可能导致非注册问题的场景，并开发了针对各种场景的不同图像转换方案，以将现有的注册变化检测数据集转换为非注册版本。

Result: 展示了非注册变化检测对最先进的方法可能造成的灾难性损害。

Conclusion: 非注册变化检测可能导致最先进的方法造成灾难性损害。

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [56] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng,Kexuan Li,Xingye Cui,Jiaxuan Li,Chang Long,Tian Pu,Zhenming Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的网络结构CSPENet，用于红外小目标检测，通过改进轮廓信息的感知和目标定位，提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集杂波环境下的微弱目标定位和轮廓信息感知方面存在不足，严重限制了它们的检测性能。

Method: 我们提出了一个轮廓感知和显著性先验嵌入网络（CSPENet），包括一个环绕收敛先验提取模块（SCPEM）、一个双分支先验嵌入架构（DBPEA）和一个注意力引导的特征增强模块（AGFEM）。

Result: 在公共数据集NUDT-SIRST、IRSTD-1k和NUAA-SIRST上的实验结果表明，我们的CSPENet在检测性能上优于其他最先进的方法。

Conclusion: 实验结果表明，我们的CSPENet在检测性能上优于其他最先进的方法。

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [57] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang,Tao Tan,Shuai Tan,Weiqin Yang,Kunyan Cai,Calvin Chen,Yue Sun*

Main category: cs.CV

TL;DR: MambaControl是一种新颖的框架，通过结合Mamba-based长程建模与图引导的解剖控制，以及傅里叶增强的频谱图表示，实现了高保真度的医学图像轨迹预测，在阿尔茨海默病预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在纵向依赖性和结构一致性方面存在局限，难以捕捉进行性疾病的复杂时空动态和保持解剖完整性。

Method: MambaControl结合了基于Mamba的长程建模与图引导的解剖控制，以及傅里叶增强的频谱图表示，以实现高保真度的医学图像轨迹预测。

Result: 定量和区域评估显示，MambaControl在进展预测质量和解剖保真度方面有所提高。

Conclusion: MambaControl展示了在阿尔茨海默病预测中的优越性能，并具有个性化预后和临床决策支持的潜力。

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [58] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，专注于Texture Key Driver Factors (TKDF)，以提高野外面部表情识别的性能。


<details>
  <summary>Details</summary>
Motivation: 由于表达相关特征的细微和局部性质，以及面部外观的复杂变化，野外的面部表情识别（FER）仍然是一个具有挑战性的任务。我们观察到某些纹理线索，如眉毛、眼睛和嘴巴周围的皮肤微小变化，是情感动态的主要指标。

Method: 我们提出了一个包含Texture-Aware Feature Extractor (TAFE)和Dual Contextual Information Filtering (DCIF)的FER架构。TAFE采用带有多分支注意力的ResNet骨干网络来提取细粒度纹理表示，而DCIF通过自适应池化和注意力机制过滤上下文来优化这些特征。

Result: 在RAF-DB和KDEF数据集上的实验结果表明，我们的方法实现了最先进的性能。

Conclusion: 实验结果表明，我们的方法在RAF-DB和KDEF数据集上实现了最先进的性能，验证了将TKDF纳入FER流程的有效性和鲁棒性。

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [59] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao,Shaobo Xia,Sheng Nie,Cheng Wang,Xiaohuan Xi,Bisheng Yang*

Main category: cs.CV

TL;DR: 本文提出了APCoTTA，一种针对ALS点云语义分割的连续测试时间适应方法，通过动态可训练层选择、基于熵的一致性损失和随机参数插值机制来提高模型性能，并构建了两个新基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在ALS点云上的连续测试时间适应（CTTA）方法有限，面临缺乏标准化数据集、灾难性遗忘和错误累积等挑战。

Method: 提出了一种动态可训练层选择模块，利用梯度信息选择低置信度层进行训练，其余层保持冻结，以减轻灾难性遗忘。还提出了基于熵的一致性损失和随机参数插值机制，以减少错误累积并平衡目标适应和源知识保留。

Result: APCoTTA在两个基准测试ISPRSC和H3DC上取得了最佳性能，mIoU分别提高了约9%和14%。

Conclusion: APCoTTA在两个基准测试中表现最佳，mIoU分别提高了约9%和14%。新的基准测试和代码已发布。

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [60] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou,Yichong Xia,Sicheng Pan,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: HQUIC is a new underwater image compression method that leverages specific features of underwater images to achieve better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Contemporary underwater image compression algorithms fail to fully leverage the unique characteristics of underwater scenes, leading to suboptimal performance.

Method: HQUIC employs an ALTC module to adaptively predict attenuation coefficients and global light information, uses a codebook to extract common objects, and dynamically weights multi-scale frequency components.

Result: HQUIC demonstrates superior performance on diverse underwater datasets compared to existing methods.

Conclusion: HQUIC outperforms state-of-the-art compression methods in underwater image compression.

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [61] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng,Jiafei Duan,Yi Ru Wang,Haoquan Fang,Boyang Li,Yushan Huang,Elvis Wang,Ainaz Eftekhar,Jason Lee,Wentao Yuan,Rose Hendrix,Noah A. Smith,Fei Xia,Dieter Fox,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文介绍了PointArena，一个全面的平台，用于评估多模态指针在各种推理场景中的表现。该平台包含三个组件：Point-Bench数据集、Point-Battle互动竞技场和Point-Act现实世界机器人系统。评估结果显示，Molmo-72B表现最佳，而专有模型的表现也在提高。监督训练可以显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 指针作为一种基础且直观的机制，用于在视觉上下文中定位语言，其应用范围涵盖机器人技术、辅助技术和交互式AI系统。虽然最近的多模态模型开始支持指针功能，但现有的基准测试通常只关注参考对象定位任务。

Method: 我们引入了PointArena，一个全面的平台，用于评估跨多种推理场景的多模态指针。PointArena包括三个组件：(1) Point-Bench，一个精心策划的数据集，包含五个推理类别中的约1,000个指针任务；(2) Point-Battle，一个交互式的基于网络的竞技场，促进盲目的成对模型比较，已经收集了超过4,500个匿名投票；(3) Point-Act，一个现实世界的机器人操作系统，允许用户直接在实际环境中评估多模态模型的指针能力。

Result: 我们对最先进的开源和专有多模态模型进行了广泛的评估。结果表明，Molmo-72B始终优于其他模型，尽管专有模型日益表现出相当的性能。此外，我们发现专门针对指针任务的监督训练显著提高了模型性能。在我们的多阶段评估流程中，我们还观察到强烈的相关性，这突显了精确指针能力在使多模态模型有效连接抽象推理与具体现实行动中的关键作用。

Conclusion: 结果表明，Molmo-72B始终优于其他模型，尽管专有模型日益表现出相当的性能。此外，我们发现专门针对指针任务的监督训练显著提高了模型性能。在我们的多阶段评估流程中，我们还观察到强烈的相关性，这突显了精确指针能力在使多模态模型有效连接抽象推理与具体现实行动中的关键作用。

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [62] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang,Jiyeong Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: DITM improves image-text matching by considering descriptive flexibility and enhancing hierarchical reasoning, outperforming existing methods in representing complex relationships.


<details>
  <summary>Details</summary>
Motivation: Existing approaches use sparse binary supervision, which neglects many-to-many image-text relationships and implicit connections from general to specific descriptions. DITM aims to address these limitations by learning graded contextual similarity.

Method: DITM proposes descriptive image-text matching by exploring the descriptive flexibility of language, using cumulative term frequency-inverse document frequency (TF-IDF) to balance pairwise similarity according to sentence keywords. It refines false negative labeling and builds precise matching by aligning sentences in a generic-to-specific order.

Result: Experiments on MS-COCO, Flickr30K, and CxC datasets show DITM's effectiveness in representing complex image-text relationships. It also enhances the model's hierarchical reasoning ability, as shown by analysis on the HierarCaps benchmark.

Conclusion: DITM enhances the hierarchical reasoning ability of the model and demonstrates effectiveness in representing complex image-text relationships compared to state-of-the-art approaches.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [63] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang,Yuanqi Hu,Xinyu Chen,Yuxia Xu,Suhui Wang,Chunan Yu,Lanyun Zhu,Deyi Ji,Xin Xu,Tianrun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D草图驱动的3D服装生成框架，使普通用户能够通过简单的3D草图在AR/VR环境中创建高质量的数字服装。


<details>
  <summary>Details</summary>
Motivation: 在沉浸式消费电子产品时代，人们越来越希望通过虚拟时尚来表达自己的身份，但现有的3D服装设计工具对普通用户来说仍然难以使用。

Method: 我们引入了一个3D草图驱动的3D服装生成框架，结合了条件扩散模型、在共享潜在空间中训练的草图编码器以及自适应课程学习策略。

Result: 通过广泛的实验和用户研究，我们的方法在保真度和易用性方面显著优于现有基线。

Conclusion: 我们的方法在保真度和易用性方面显著优于现有基线，证明了其在下一代消费平台上的民主化时尚设计的潜力。

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [64] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过集成多种技术，提高了多尺度、小目标和远距离目标的检测精度，具有广泛的实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 当前的技术如雷达、摄像头和车辆传感器网络存在成本高、易受天气和光照条件影响以及分辨率有限等问题，因此需要一种更高效的检测方法。

Method: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过集成结构重新参数化技术、双向金字塔结构网络模型和新的检测流程，实现了多尺度、小目标和远距离目标的高效精确检测。

Result: 实验结果表明，增强后的模型能够有效检测大目标和小目标，检测准确率达到65%，相比传统方法有显著提升。

Conclusion: 该改进模型在实际应用中具有巨大潜力，特别适合自动驾驶比赛，如Formula Student Autonomous China (FSAC)，在单目标和小目标检测场景中表现出色。

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [65] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的潜在扩散模型微调方法，用于遥感图像超分辨率。实验结果表明，该方法在提升超分辨率质量和场景适应性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着遥感技术的快速发展，超分辨率图像重建具有重要的研究和实际意义。现有的深度学习方法虽然取得了一定进展，但在处理复杂场景和保持图像细节方面仍存在局限性。

Method: 本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率。该方法构建了一个具有状态、动作和奖励的强化学习环境，并通过近端策略优化（PPO）在LDM模型的反向去噪过程中优化决策目标。

Result: 在RESISC45数据集上的实验表明，与基线模型相比，PSNR提高了3-4dB，SSIM提高了0.08-0.11，LPIPS降低了0.06-0.10，特别是在结构化和复杂的自然场景中效果显著。

Conclusion: 实验结果表明，该方法在增强超分辨率质量和场景适应性方面是有效的。

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [66] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: 本研究提出了DeepSeqCoco，一种基于深度学习的模型，用于从椰子树图像中准确自动地识别疾病。实验结果表明，该模型在准确性和效率方面优于现有模型，并展示了其在精准农业中的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 椰子树疾病对农业产量构成严重威胁，特别是在发展中国家，传统的耕作实践限制了早期诊断和干预。当前的疾病识别方法是手动的、劳动密集型的且不可扩展的。

Method: DeepSeqCoco是一种基于深度学习的模型，用于从椰子树图像中准确自动地识别疾病。该模型在各种优化器设置下进行了测试，包括SGD、Adam和混合配置，以找到准确度、损失最小化和计算成本之间的最佳平衡。

Result: 实验结果表明，DeepSeqCoco可以达到高达99.5%的准确率（比现有模型高出最多5%），混合SGD-Adam显示出最低的验证损失2.81%。此外，训练时间最多减少了18%，预测时间最多减少了85%。

Conclusion: 研究结果表明，DeepSeqCoco模型在精度农业中具有巨大的潜力，可以提供基于人工智能的可扩展且高效的疾病监测系统。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [67] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为UOD的领域自适应单次地标检测框架，用于处理多领域医学图像，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单次学习方法高度专业化于单一领域，并且在多领域未标记数据的情况下受到领域偏好的严重影响。此外，单次学习在标注次优图像时表现不佳。

Method: UOD由两个阶段和两个相应的通用模型组成，这些模型设计为领域特定模块和领域共享模块的组合。第一阶段使用自监督学习生成伪地标标签，第二阶段使用领域自适应变压器消除领域偏好并构建全局上下文。

Result: 即使每个领域只提供一个注释样本进行训练，领域共享模块帮助UOD聚合所有单次样本以检测更鲁棒和准确的地标。

Conclusion: UOD在三个广泛使用的公共X射线数据集上实现了最先进的性能，证明了其在多域医学图像中的有效性。

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [68] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang,Boyang Zheng,Xichen Pan,Sayak Paul,Saining Xie*

Main category: cs.CV

TL;DR: 本文不描述一种新方法，而是对文本到图像合成中一个重要的但未被充分研究的设计空间进行了深入探讨，特别是大型语言模型（LLMs）和扩散变压器（DiTs）的深度融合。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注整体系统性能，而不是与替代方法的详细比较，关键设计细节和训练方案通常未被公开。这些差距导致了对该方法实际潜力的不确定性。

Method: 我们进行了关于文本到图像生成的实证研究，与现有基线进行受控比较，分析重要的设计选择，并提供可清晰复现的训练方案。

Result: 我们希望这项工作能为多模态生成的未来研究提供有意义的数据点和实用指南。

Conclusion: 本文通过实证研究填补了现有研究的空白，提供了可复现的训练方案，为多模态生成领域提供了有价值的参考。

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [69] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan,Xuepu Zeng,Jing Zhang,Mingming Gong,Yuxiang Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文综述了200多篇关于使用辐射场进行动态场景表示的论文，提出了一个统一的表示框架，并讨论了该领域中的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 动态场景表示和重建近年来取得了重大进展，但仍然存在许多挑战。本文旨在为研究人员提供一个全面的概述，帮助他们了解该领域的概念原理和实践前沿。

Method: 本文通过多个关键视角对这些工作进行了分类和评估，包括运动表示范式、适用于不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。

Result: 本文系统地分析了200多篇关于使用辐射场进行动态场景表示的论文，并提出了一个统一的表示框架。

Conclusion: 本文通过系统分析200多篇关于使用辐射场进行动态场景表示的论文，提出了一个统一的表示框架，并对持续存在的挑战和有前景的研究方向进行了批判性审视。

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [70] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 本文创建了一个用于Pashto OCR的合成数据集PsOCR，并评估了多种大型多模态模型在该任务上的性能，发现Gemini表现最佳，而Qwen-7B在开源模型中表现突出。


<details>
  <summary>Details</summary>
Motivation: 由于Pashto语言的脚本具有连笔字特性且结构化数据集稀缺，因此自然语言处理面临诸多挑战。为了解决这些问题，本文旨在创建一个高质量的Pashto OCR数据集，并评估当前大型多模态模型在该任务上的表现。

Method: 本文开发了一个包含一百万张图像的合成Pashto OCR数据集PsOCR，这些图像在单词、行和文档级别上带有边界框注释，适用于不同架构（包括卷积神经网络和Transformer）的模型训练和评估。此外，还选择了10,000张图像的基准子集来评估多个大型多模态模型的性能。

Result: 实验结果表明，Gemini在所有模型中表现最佳，而在开源模型中，Qwen-7B表现出色。

Conclusion: 本文对当前大型多模态模型在Pashto语言OCR任务中的能力和局限性进行了深入评估，并为Pashto OCR以及其他类似脚本（如阿拉伯语、波斯语和乌尔都语）的研究奠定了基础。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [71] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju,Sheng-Yen Huang,Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB is a two-stage framework that uses an improved StyleGAN to generate a stylized video, which is then used to learn a stylized neutral head model and expression blendshapes for rendering stylized avatars with arbitrary expressions.


<details>
  <summary>Details</summary>
Motivation: The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. However, it is limited in its ability to synthesize diverse stylized 3D head avatars using Gaussian blendshapes.

Method: ToonifyGB is an efficient two-stage framework that first generates a stylized video using an improved StyleGAN, and then learns a stylized neutral head model and a set of expression blendshapes from the generated video to render stylized avatars with arbitrary expressions.

Result: ToonifyGB successfully generates a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. It also learns a stylized neutral head model and a set of expression blendshapes from the generated video, allowing for the efficient rendering of stylized avatars with arbitrary expressions.

Conclusion: ToonifyGB can efficiently render stylized avatars with arbitrary expressions, and its effectiveness has been validated on benchmark datasets using two styles: Arcane and Pixar.

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [72] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo,Xiaodong Gu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态表示学习（MMRL）方法，用于解决大型预训练视觉-语言模型在少量样本数据下的过拟合问题。通过引入共享的、可学习的、模态无关的表示空间，MMRL实现了更有效的跨模态交互。同时，还提出了MMRL++，进一步优化了参数效率和模态内交互。实验结果表明，MMRL和MMRL++在多个数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练视觉-语言模型（VLMs）在跨不同任务的迁移学习方面取得了显著进展。然而，使用有限的少样本数据适应这些模型往往会过拟合，削弱了它们在新任务上的泛化能力。

Method: 提出了一种多模态表示学习（MMRL），引入了一个共享的、可学习的、模态无关的表示空间。MMRL将空间标记投影到文本和图像编码器中作为表示标记，以实现更有效的跨模态交互。此外，还提出了MMRL++，这是一种参数高效且交互感知的扩展，显著减少了可训练参数并增强了模态内交互。

Result: MMRL和MMRL++在15个数据集上的实验结果表明，它们能够有效解决过拟合问题，并在任务特定适应和泛化之间取得良好平衡。

Conclusion: MMRL和MMRL++在15个数据集上的广泛实验中表现出色，优于最先进的方法，并在任务特定适应和泛化之间取得了良好的平衡。

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [73] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: This paper proposes Multi-Objective Balanced Covering (MoB) for visual token pruning, which addresses the inconsistency in performance caused by static strategies. MoB derives a closed-form error bound based on the Hausdorff distance and reveals a trade-off between prompt alignment and visual preservation. It reformulates pruning as a bi-objective covering problem, enabling efficient budget allocation. Experiments show that MoB achieves high performance with minimal visual tokens, making it suitable for various vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance.

Method: We derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging ε-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading.

Result: MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.

Conclusion: MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [74] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil,Lucas Robinet,Laurent Risser,Soleakhena Ken*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像配准形式，用于估计与未知条件相关的图像，基于两个或更多已知图像及其相关条件。通过使用一种新的条件U-Net架构来实际建模这种形式，该架构充分利用了条件信息，并不需要任何固定图像。该方法应用于放射治疗中不同呼吸幅度的图像移动肿瘤，利用胸腹区域的4D-CT（3D+t）扫描进行处理。该应用特别复杂，因为它需要将一系列2D切片拼接成不同器官位置的多个3D体积。标准方法的运动插值在组装后的体积中会产生已知的重建伪影，这是由于不规则的患者呼吸、滞后效应和呼吸信号与内部运动的相关性较差所致。在4D-CT临床数据上获得的结果展示了通过实时延迟实现的无伪影体积。代码可在https://github.com/Kheil-Z/IMITATE 上公开获取。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在放射治疗中由于不规则呼吸、滞后效应和呼吸信号与内部运动相关性差而导致的图像重建伪影问题。

Method: 本文提出了一种新的图像配准形式，基于两个或更多已知图像及其相关条件，以估计未知条件相关的图像。通过使用一种新的条件U-Net架构来实际建模这种形式，该架构充分利用了条件信息，并不需要任何固定图像。

Result: 在4D-CT临床数据上获得的结果展示了通过实时延迟实现的无伪影体积。

Conclusion: 本文提出的新图像配准形式能够有效解决放射治疗中由于不规则呼吸、滞后效应和呼吸信号与内部运动相关性差导致的图像重建伪影问题。

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [75] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦领域泛化方法MCSAD，通过多源协作风格增强和领域不变学习，显著提升了模型在未见过目标领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的风格增强方法要么探索孤立源域内的数据风格，要么在数据去中心化场景下跨现有源域插值风格信息，这导致了有限的风格空间。

Method: 我们提出了一种多源协作风格增强和领域不变学习方法（MCSAD），通过多源协作风格增强模块生成更广泛风格空间的数据，并通过跨域特征对齐和类关系集成蒸馏进行领域不变学习。

Result: 通过交替进行协作风格增强和领域不变学习，模型可以在未见过的目标领域中很好地泛化。

Conclusion: 我们的方法在多个领域泛化数据集上进行了广泛的实验，结果表明它显著优于最先进的联邦领域泛化方法。

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [76] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 本文研究了图像显著性预测中的数据集偏差问题，并提出了一种新架构来解决跨数据集的泛化问题。该模型在多个数据集上取得了优异表现，并揭示了复杂的多尺度显著性效应。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的图像显著性预测方法在现有基准上接近黄金标准性能水平，但我们发现跨多个显著性数据集预测注视点仍然具有挑战性，因为数据集存在偏差。

Method: 我们提出了一种新架构，扩展了一个大部分数据集无关的编码器-解码器结构，仅包含少于20个数据集特定参数，这些参数控制可解释的机制，如多尺度结构、中心偏差和注视扩散。

Result: 当模型在一个数据集上训练后应用于另一个数据集时，性能显著下降（约40%）。增加数据集多样性并不能解决这种跨数据集差距，近60%的差距归因于数据集特定的偏差。适应这些参数到新数据可以解决超过75%的泛化差距，其中大部分改进可以通过仅50个样本实现。

Conclusion: 我们的模型在MIT/Tuebingen Saliency Benchmark的三个数据集上达到了新的最先进水平，即使纯粹从不相关的数据集进行泛化，但当适应到相应的训练数据集时，效果有显著提升。此外，该模型还提供了关于空间显著性属性的有价值见解，揭示了结合绝对和相对大小的复杂多尺度效应。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [77] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon,Ahmad AlMughrabi,Thanasis Zoumpekas,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: 本文提出了VolE框架，利用移动设备驱动的3D重建来估计食品体积，无需参考和深度信息，并在多个数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的食品体积估计对于医疗营养管理和健康监测应用至关重要，但现有的方法通常受到单核数据、专用硬件或参考物体校准的限制。

Method: VolE利用移动设备驱动的3D重建来估计食品体积，通过自由运动拍摄图像和相机位置生成精确的3D模型，并使用食品视频分割生成食品掩码。

Result: 实验表明，VolE在多个数据集上实现了2.22%的MAPE，优于现有的体积估计技术。

Conclusion: VolE在食品体积估计方面表现出色，优于现有技术。

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [78] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文系统评估了MixUp和辅助傅里叶增强等替代增强策略，这些方法在不明确针对特定分布偏移源的情况下，减轻了多种变化的影响，从而显著提高了医学图像分割模型在实际应用中的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型通常在精心挑选的数据集上进行训练，导致在实际临床环境中部署时性能下降，这是由于训练和测试分布之间的不匹配。传统视觉一致的增强策略缺乏应对多样化现实场景所需的鲁棒性。

Method: 系统评估了替代增强策略，重点是MixUp和辅助傅里叶增强。这些方法在不明确针对特定分布偏移源的情况下，减轻了多种变化的影响。

Result: 这些增强方法显著提高了心脏电影MRI和前列腺MRI分割中分布外泛化能力和对成像变化的鲁棒性。定量分析表明，这些方法通过促进特征表示的可分性和紧凑性来增强学习到的特征表示。

Conclusion: 这些增强方法通过促进特征表示的可分性和紧凑性，显著提高了分布外泛化能力和对成像变化的鲁棒性，并且在nnU-Net训练流程中的集成提供了一种易于实现的有效解决方案，以提高医学分割模型在实际应用中的可靠性。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [79] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: 本文探讨了医学影像中的人机对齐和公平性问题，发现将人类见解纳入可以减少公平差距并提高跨领域泛化能力，但过度对齐可能会引入性能权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在医学影像中表现出色，但仍然容易产生偏见，导致不同人口群体之间的公平性差距。

Method: 我们提供了对这一领域的第一个系统探索，即人机对齐和公平性。

Result: 我们的结果表明，将人类见解纳入可以持续减少公平差距并提高跨领域泛化能力，尽管过度对齐可能会引入性能权衡。

Conclusion: 我们的研究结果表明，将人类见解纳入可以持续减少公平差距并提高跨领域泛化能力，尽管过度对齐可能会引入性能权衡，强调了需要校准策略。这些发现突显了人机对齐作为开发公平、稳健和泛化医学AI系统的有希望的方法，平衡了专家指导和自动化效率。

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [80] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: 本文提出了MTVCrafter，这是第一个直接建模原始3D运动序列（4D运动）的人类图像动画框架，通过引入4DMoT和MV-DiT模型，实现了更强大的时空线索和更灵活的控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于2D渲染的姿态图像进行运动指导，这限制了泛化能力并丢失了开放世界动画中的重要3D信息。

Method: 提出MTVCrafter框架，引入4DMoT和MV-DiT模型，直接建模原始的3D运动序列（即4D运动）进行人类图像动画。

Result: 实验表明，MTVCrafter在FID-VID指标上达到了最先进的结果（6.98），比第二名高出65%。MTVCrafter还能很好地泛化到各种开放世界角色。

Conclusion: MTVCrafter标志着该领域的重要进展，并为基于姿态的人类视频生成开辟了新方向。

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [81] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen,Wanqi Yin,Xiaofeng Yang,Cheng Chen,Chaoyue Song,Zhongang Cai,Lei Yang,Hao Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: ADHMR is a framework that improves human mesh recovery by aligning a diffusion-based model through preference optimization, resulting in better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issues of misalignment with 2D image observations and weak robustness to in-the-wild images in probabilistic methods for human mesh recovery.

Method: ADHMR is a framework that aligns a diffusion-based HMR model using preference optimization. It involves training a human mesh prediction assessment model called HMR-Scorer, creating a preference dataset, and finetuning the base model with direct preference optimization.

Result: ADHMR outperforms current state-of-the-art methods in human mesh recovery from single images.

Conclusion: ADHMR outperforms current state-of-the-art methods in human mesh recovery from single images.

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [82] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu,Jiaqi Tang,Jiyao Wang,Yunfan LU,Xu Cao,Qingyong Hu,Yin Wang,Yuting Zhang,Tianxin Xie,Yunpeng Zhang,Yong Chen,Jiayu. Gao,Bin Huang,Dengbo He,Shuiguang Deng,Hao Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文介绍了SAGE DeeR，这是一种超级对齐和通用的驾驶代理，能够根据不同人的偏好和偏见做出不同反应，并通过多视角和多模式输入理解用户的行为和决策。


<details>
  <summary>Details</summary>
Motivation: 智能驾驶舱需要匹配不同用户的舒适性、交互性和安全性需求，因此需要一种能够根据不同人的偏好和偏见做出不同反应的代理。

Method: 本文提出了一种名为SAGE DeeR的超级对齐和通用驾驶代理，通过多视角和多模式输入理解用户生理指标、面部情绪、手部动作、身体动作、驾驶场景和行为决策，并通过语言空间中的隐含思维链来提高其通用性和超级对齐能力。

Result: 本文提出的SAGE DeeR实现了三个亮点：超级对齐、通用性和自我激发。此外，还收集了多个数据集并建立了大规模基准测试，用于衡量驾驶代理的感知决策能力和超级对齐的准确性。

Conclusion: 本文提出了一个名为SAGE DeeR的超级对齐和通用驾驶代理，以满足不同用户的舒适性、交互性和安全性需求。此外，还收集了多个数据集并建立了大规模基准测试。

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [83] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz,Pascal Colling,Qi Han,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种新的离线映射方法，通过整合非正式路线来提高自动驾驶系统的地图创建效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了避免手动标注的大量工作，自动化地图创建的方法已经出现。然而，在线映射仍然面临时间一致性、传感器遮挡、运行时间和泛化方面的挑战。

Method: 我们提出了一种新的离线映射方法，将驾驶者使用的非正式路线整合到地图创建过程中。我们的方法利用基于变压器的深度学习模型，从自车和其他交通参与者聚合轨迹数据，构建全面的全局地图。

Result: 我们的方法在与最先进的在线映射方法相比表现出优越的性能，实现了对以前未见过的环境和传感器配置的改进泛化能力。

Conclusion: 我们的方法在两个基准数据集上得到了验证，展示了其在自动驾驶系统中的鲁棒性和适用性。

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [84] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: 本文提出了HandReader，一种用于手指拼写识别的新型架构，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经关注于处理视频的时间维度，但仍有改进的空间。本文旨在提高手指拼写识别的准确性，并提出了一种新的方法来解决这一问题。

Method: HandReader包含三个架构：HandReader_RGB使用了新的时间移位自适应模块（TSAM）来处理不同长度视频的RGB特征；HandReader_KP基于提出的时空姿态编码器（TPE）在关键点上操作；HandReader_RGB+KP是一种结合编码器的架构，可以利用RGB和关键点模态。

Result: HandReader的每个模型都具有独特的优势，并在ChicagoFSWild和ChicagoFSWild+数据集上取得了最先进的结果。此外，这些模型在俄罗斯手指拼写的第一开放数据集Znaki上也表现出色。

Conclusion: 本文介绍了HandReader，这是一种用于手指拼写识别的三种架构的集合，并在多个数据集上取得了最先进的结果。此外，还提出了Znaki数据集，并且HandReader预训练模型是公开可用的。

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [85] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu,Kaixin Chen,Heng Guo,Yixiang Huang,Ming Wu,Zhenwei Shi,Chuang Zhang,Jun Guo*

Main category: cs.CV

TL;DR: 本文介绍了MFogHub，一个用于海洋雾检测和预测的多区域、多卫星数据集，以解决现有数据集的局限性，并促进全球范围内的海洋雾动态研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常专注于单一区域或卫星，限制了模型性能评估的多样性，并阻碍了对海洋雾内在特性的探索。

Method: 引入了MFogHub，这是一个多区域和多卫星的数据集，整合了来自15个沿海雾易发地区的标注海洋雾观测数据和六颗静止卫星的数据。

Result: MFogHub包含超过68,000个高分辨率样本，能够揭示由于地区和卫星差异导致的泛化波动，并作为开发有针对性和可扩展的雾预测技术的宝贵资源。

Conclusion: 通过MFogHub，我们旨在在全球范围内推进海洋雾动态的实际监测和科学理解。

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [86] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang,Shuai Xu,Xuelin Zhu,Yicong Li*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段跨模态交互（MSCI）模型，以解决现有方法在捕捉细粒度局部特征方面的不足。MSCI通过利用CLIP视觉编码器的中间层信息，结合自适应聚合器提取局部和全局信息，并通过分阶段交互机制将其融入文本表示中，从而显著提升了模型对细粒度局部视觉信息的感知能力。实验结果验证了MSCI模型的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要依赖于CLIP的跨模态对齐能力，但往往忽视了其在捕捉细粒度局部特征方面的局限性，这源于其架构和训练范式。

Method: 我们提出了一个多层次跨模态交互（MSCI）模型，该模型有效地探索和利用了CLIP视觉编码器的中间层信息。具体来说，我们设计了两个自适应聚合器，分别从低级视觉特征中提取局部信息，并从高级视觉特征中整合全局信息。这些关键信息通过分阶段的交互机制逐步融入文本表示中，显著增强了模型对细粒度局部视觉信息的感知能力。此外，MSCI根据不同的组合以及同一组合中的不同元素动态调整全局和局部视觉信息之间的注意力权重，使其能够灵活适应各种场景。

Result: MSCI模型在三个广泛使用的数据集上进行了实验，结果表明该模型具有有效性和优越性。

Conclusion: MSCI模型在三个广泛使用的数据集上进行了实验，充分验证了所提出模型的有效性和优越性。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [87] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: 本文提出了一个包含4,178个故事的StoryReasoning数据集，旨在解决视觉叙事系统中的指称幻觉问题，并通过微调Qwen2.5-VL 7B创建了Qwen Storyteller模型，显著减少了幻觉。


<details>
  <summary>Details</summary>
Motivation: 视觉叙事系统难以在帧之间保持角色身份一致并正确关联动作与适当主体，导致指称幻觉问题。通过在视觉元素上对角色、物体和其他实体进行定位可以解决这些问题。

Method: 提出了一种基于视觉相似性和人脸识别的跨帧物体再识别方法，以及用于显式叙事建模的链式思维推理方法，并设计了一种将文本元素与多帧视觉实体关联的接地方案。

Result: 建立了包含4,178个故事的StoryReasoning数据集，每个故事在帧之间保持角色和物体一致性，并通过结构化表格表示显式建模多帧关系。评估显示，与非微调模型相比，平均每个故事的幻觉减少了12.3%。

Conclusion: 通过微调Qwen2.5-VL 7B，创建了Qwen Storyteller，该模型在保持故事中物体引用一致性的前提下，执行端到端的物体检测、再识别和地标检测，并且平均每个故事的幻觉减少了12.3%。

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [88] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo,Roger Trullo,Albert Pla Planas,Etienne Decenciere,Thomas Walter*

Main category: cs.CV

TL;DR: MIPHEI is a model that predicts multiplex immunofluorescence signals from H&E images, achieving high accuracy in cell-type classification and offering potential for large-scale analysis of H&E datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between histopathological analysis and multiplex immunofluorescence (mIF) due to cost and logistical constraints.

Method: MIPHEI (Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&E images.

Result: MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers.

Conclusion: MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [89] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu,Jirong Zha,Ding Li,Leye Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的成员推理方法PartCrop，用于攻击自监督视觉模型，并评估了防御方法的效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决自监督学习在隐私方面的挑战，特别是在视觉领域。由于攻击者通常面对的是黑盒系统，因此需要一种更现实的成员推理方法。

Method: 本文提出了一种名为PartCrop的统一成员推理方法，该方法基于模型共享的部分感知能力和对训练数据更强的部分响应。此外，还评估了两种常见的防御方法，并提出了一个改进的PartCrop-v2方法。

Result: 实验结果验证了PartCrop的有效性和泛化能力。此外，所有防御方法都被证明是有效的。

Conclusion: 本文提出了一种统一的成员推理方法PartCrop，并验证了其在不同训练协议和结构的自监督模型上的有效性。此外，还评估了两种常见的防御方法，并提出了一个改进的PartCrop-v2方法。

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [90] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: 本文提出了一种高效的基于脉冲的视频Transformer（SpikeVideoFormer），具有线性时间复杂度。通过设计基于脉冲的汉明注意力（SDHA），并在多种视频任务中验证了其优越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SNN的Transformer主要关注单图像任务，而未能充分利用SNN在视频视觉任务中的效率优势。因此，本文旨在开发一种高效的基于脉冲的视频Transformer，以解决这一问题。

Method: 本文设计了一种基于脉冲的汉明注意力（SDHA），并在此基础上分析了各种基于脉冲的空间-时间注意力设计，以找到一种最优方案，同时保持线性时间复杂度。

Result: 本文的方法在多个下游视频任务中取得了最先进的性能，包括分类、人体姿态跟踪和语义分割。此外，它在效率上显著优于现有的SNN方法和最近的ANN方法。

Conclusion: 本文提出了SpikeVideoFormer，这是一种高效的基于脉冲的视频Transformer，具有线性时间复杂度。实验结果表明，该方法在多个下游视频任务中表现出色，并且在效率上优于现有的SNN方法和最近的ANN方法。

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [91] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种新的无配对训练方法，用于可学习的ISP，消除了对原始图像和地面真实数据之间直接对应关系的需求。该方法利用对抗训练和多术语损失函数，实现了高保真度的结果。


<details>
  <summary>Details</summary>
Motivation: 在开发可学习的ISP时，获取像素对齐的配对数据是一个困难且昂贵的步骤。

Method: 我们提出了一种新的训练方法，用于可学习的ISP，消除了原始图像和具有匹配内容的地面真实数据之间的直接对应关系。我们的无配对方法使用了由对抗训练引导的多术语损失函数，其中多个判别器处理来自预训练网络的特征图，以保持内容结构的同时从目标RGB数据集中学习颜色和纹理特性。

Result: 我们的无配对学习策略在多个评估指标上表现出高保真度，并显示出强大的潜力。

Conclusion: 我们的方法在多个评估指标上表现出高保真度，展示了无配对学习策略的强大力量。

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [92] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型对虚拟物体的理解能力，并发现其处理能力不足。


<details>
  <summary>Details</summary>
Motivation: 本文旨在测试视觉语言模型对场景中虚拟物体的理解能力，以了解它们对场景的掌握程度。

Method: 本文通过系统评估最先进的视觉语言模型，分析它们处理虚拟物体的能力。

Result: 本文发现现有的视觉语言模型在处理虚拟物体方面的能力不足。

Conclusion: 本文认为，虚拟物体的描述可以帮助测试AI系统对场景的理解能力，但现有的视觉语言模型在处理虚拟物体方面的能力不足。

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [93] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS是一种3DGS优化方法，能够在保持高质量渲染的同时减少高斯数量，并支持广泛的调整范围和无级控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏让用户直观调整数量-质量权衡的能力，以适应不同的硬件和通信约束。

Method: ControlGS是一种3DGS优化方法，它通过单次训练运行使用固定设置和用户指定的超参数来实现语义上有意义且跨场景一致的数量-质量控制。

Result: ControlGS在保持高质量渲染的同时减少了高斯数量，并且支持广泛的调整范围和无级控制。

Conclusion: ControlGS能够通过单次训练运行自动找到不同场景下的理想数量-质量权衡点，并在保持高质量渲染的同时减少高斯数量，同时支持广泛的调整范围和无级控制。

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [94] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov,Petr Surovtsev,Karina Kvanchiani,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CV

TL;DR: 本文提出了Logos，一个新型的俄语手语（RSL）数据集，是目前最大的ISLR数据集，也是最大的RSL数据集。我们探索了跨语言迁移学习方法，并发现使用多个分类头的联合训练最能提高目标低资源数据集的准确性。通过明确标记视觉相似的手势，我们提高了训练模型的质量作为下游任务的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 本文研究了孤立手语识别（ISLR）任务的两个方面。首先，尽管有多个数据集，但大多数个别手语的数据量有限，这给跨语言ISLR模型训练带来了挑战，包括迁移学习。其次，相似的手势可能有不同的语义含义，这导致数据集标注的模糊性，并引发关于如何标注这些手势的最佳策略的问题。

Method: 本文提出了Logos，一个新型的俄语手语（RSL）数据集，是目前最大的ISLR数据集，也是最大的RSL数据集。我们探索了跨语言迁移学习方法，并发现使用多个分类头的联合训练最能提高目标低资源数据集的准确性。Logos数据集的关键特点是明确标注的视觉相似的手势组。我们证明了明确标记视觉相似的手势可以提高训练模型的质量作为下游任务的视觉编码器。

Result: 结果显示，预训练在Logos数据集上的模型可以作为其他语言SLR任务的通用编码器，包括少样本学习。我们探索了跨语言迁移学习方法，并发现使用多个分类头的联合训练最能提高目标低资源数据集的准确性。通过明确标记视觉相似的手势，我们提高了训练模型的质量作为下游任务的视觉编码器。

Conclusion: 基于提出的贡献，我们在WLASL数据集上超越了当前最先进的结果，并在AUTSL数据集上获得了具有竞争力的结果，仅使用单流模型处理纯RGB视频。源代码、数据集和预训练模型是公开的。

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [95] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文介绍了UniEval，这是一个用于统一多模态模型的首个无需额外模型、图像或注释的评估框架。该框架包含一个全面的基准测试UniBench和相应的UniScore度量标准。实验结果表明，UniBench比现有基准更具挑战性，UniScore与人类评估高度一致，并超越了当前指标。此外，对最先进的统一和视觉生成模型的广泛评估揭示了通用模型的独特价值。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的评估框架来评估这些模型，导致评估过程复杂且不全面。现有的模型在多个任务特定基准上进行评估，但存在许多限制，如缺乏整体结果、额外评估模型的错误、依赖大量标记图像、基准多样性不足以及指令跟随评估的度量标准有限。

Method: 引入了UniEval，这是第一个无需额外模型、图像或注释的统一多模态模型评估框架。该框架包含一个全面的基准测试UniBench和相应的UniScore度量标准。

Result: 实验结果表明，UniBench比现有基准更具挑战性，UniScore与人类评估高度一致，并超越了当前指标。此外，对最先进的统一和视觉生成模型的广泛评估揭示了通用模型的独特价值。

Conclusion: UniEval框架的引入为统一多模态模型提供了一个简化且统一的评估过程，实验结果表明UniBench比现有基准更具挑战性，UniScore与人类评估高度一致，并超越了当前指标。此外，对最先进的统一和视觉生成模型的广泛评估揭示了通用模型的独特价值。

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [96] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt,Pedro Sanchez,Yongchen Yao,Steven McDonagh,Sotirios A. Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: 本文介绍了CheXGenBench，这是一个评估合成胸部X光片生成的框架，评估了保真度、隐私风险和临床效用。通过标准化的数据划分和统一的评估协议，发现现有评估协议存在低效率，并发布了SynthCheX-75K数据集以支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在真实世界图像方面取得了快速进展，但医学领域的评估受到了方法不一致、过时的架构比较和脱离评估标准的阻碍，这些标准很少涉及合成样本的实际临床价值。

Method: 我们引入了CheXGenBench，这是一个严格且多方面的评估框架，用于合成胸部X光片生成，同时评估保真度、隐私风险和临床效用。通过标准化的数据划分和统一的评估协议，包含超过20个定量指标，系统地分析生成质量、潜在的隐私漏洞以及在11种领先文本到图像架构中的下游临床适用性。

Result: 我们的结果揭示了现有评估协议中的关键低效率，特别是在评估生成保真度方面，导致不一致和无信息的比较。

Conclusion: 我们的框架为医学AI社区建立了标准化的基准，使客观和可重复的比较成为可能，并促进了现有和未来生成模型的无缝集成。此外，我们发布了高质量的合成数据集SynthCheX-75K，以支持该关键领域的进一步研究。

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [97] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev,Nuno Goncalves*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过引入双分支分类策略来提高人脸识别系统对抗人脸伪装攻击的鲁棒性。该方法已在公共基准上得到验证，并且可以集成到现有的人脸识别训练流程中。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术的进步，人脸识别已经取得了显著进展，但这也增加了其暴露于演示攻击（包括人脸伪装）的风险，这会带来严重的安全威胁。因此，现代人脸识别系统必须具备对抗这些攻击的鲁棒性。

Method: 我们提出了一种新的方法，通过引入双分支分类策略来修改分类任务，从而有效处理人脸伪装标签的模糊性。这种适应使模型能够将伪装图像纳入训练过程，提高其区分伪装样本和真实样本的能力。

Result: 我们的方法在公共基准上得到了验证，证明了其在增强对抗人脸伪装攻击的鲁棒性方面的有效性。

Conclusion: 我们的方法已被验证在公共基准上，证明了其在增强对抗人脸伪装攻击的鲁棒性方面的有效性。此外，我们的方法具有普遍适用性，可以集成到现有的人脸识别训练流程中，以提高基于分类的识别方法。

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [98] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的检索框架，通过子模函数选择相关图像，以提高在多个图像场景下的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在处理多个图像的视觉-语言任务时存在可扩展性和检索性能的问题。

Method: 我们使用了基于子模函数的子集选择技术，如GraphCut，并结合基于锚点的查询和数据增强来改进检索框架。

Result: 我们的方法在大规模数据集中提高了检索效果，特别是在处理大量图像时。

Conclusion: 我们的方法在大规模数据集中展示了更高的有效性，特别是在处理大量图像时。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [99] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang,Guohai Xu,Weinong Wang,Junjie Yang,Jie Lou,Yunhua Xue*

Main category: cs.CV

TL;DR: 本文提出了一个新的基准和注意力准确性指标，以评估MLLMs对视觉输入的理解能力，并验证了其在单模态场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估答案的正确性，而忽视了模型是否真正理解了视觉输入。因此，本文旨在定义隐式视觉误解（IVM），并提出一种新的方法来评估模型的视觉理解能力。

Method: 本文通过解耦因果注意力模块中的视觉和文本模态，揭示了随着网络层数加深，注意力分布逐渐集中在与正确答案相关的图像上。基于此，引入了注意力准确性指标和一个新的基准来量化IVM。

Result: 本文提出了注意力准确性指标和一个新的基准，能够直接通过内部机制评估模型的视觉理解能力，并在单模态场景中验证了其有效性。

Conclusion: 本文提出了一个用于量化MLLMs中隐式视觉误解（IVM）的新基准，并通过注意力准确性指标评估模型的视觉理解能力，展示了其在单模态场景中的有效性。

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [100] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: 本文研究了可行性在生成用于CLIP基础分类器的合成训练数据时是否必要，并发现其影响很小。


<details>
  <summary>Details</summary>
Motivation: 研究可行性是否在生成用于CLIP基础分类器的合成训练数据时是必要的，特别是针对背景、颜色和纹理这三个目标属性。

Method: 引入了VariReal管道，该管道最小地编辑给定的源图像以包含由大型语言模型生成的文本提示给出的可行或不可行属性。

Result: 实验结果显示，可行性对LoRA微调的CLIP性能影响很小，且在三个细粒度数据集上的top-1准确率差异小于0.3%。此外，属性对可行/不可行图像是否对抗性地影响分类性能有影响。

Conclusion: 实验结果表明，可行性对LoRA微调的CLIP性能影响很小，且在训练数据集中混合可行和不可行图像不会显著影响性能。

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [101] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出利用代码作为跨模态对齐的监督，以解决当前多模态模型在数学推理方面的不足。通过共同开发图像到代码模型和数据集，构建了最大的图像-代码数据集ImgCode-8.6M，并利用该模型合成新的数学图形，构建了高质量的多模态数学指令微调数据集MM-MathInstruct-3M。最终提出的MathCoder-VL模型在多个指标上取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: 自然语言图像描述数据集主要用于训练大型多模态模型，主要关注自然场景，忽视了数学图形中对问题解决至关重要的复杂细节，阻碍了当前LMMs在多模态数学推理方面的进展。

Method: 我们提出利用代码作为跨模态对齐的监督，因为代码本质上包含了生成相应图形所需的所有信息，建立了两种模态之间的精确联系。具体来说，我们采用模型在环的方法共同开发图像到代码模型和数据集，结果得到了一个图像到代码模型FigCodifier和ImgCode-8.6M数据集，这是目前最大的图像-代码数据集。此外，我们利用FigCodifier合成新的数学图形，然后构建MM-MathInstruct-3M高质量多模态数学指令微调数据集。最后，我们提出了MathCoder-VL，在ImgCode-8.6M上进行跨模态对齐训练，并在MM-MathInstruct-3M上进行多模态数学问题求解的微调。

Result: 我们的模型在所有六个指标上达到了新的开源SOTA，并在MathVista的几何问题解决子集上超越了GPT-4o和Claude 3.5 Sonnet，分别提高了8.9%和9.2%。

Conclusion: 我们的模型在所有六个指标上达到了新的开源SOTA，并在MathVista的几何问题解决子集上超越了GPT-4o和Claude 3.5 Sonnet，分别提高了8.9%和9.2%。数据集和模型将在https://github.com/mathllm/MathCoder上发布。

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [102] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang,Fan Zhang,Yufeng Cui,Haiwen Diao,Zhuoyan Luo,Huchuan Lu,Jing Liu,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的视觉标记优化方法（ETT），通过联合优化视觉标记化和目标自回归任务，显著提升了多模态理解和视觉生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉标记化方法将视觉标记器的优化与下游训练分离，隐含地假设视觉标记可以跨各种任务泛化，例如图像生成和视觉问答。然而，为低级重建优化的视觉标记器对于需要不同表示和语义的下游任务是无关的。这种解耦范式引入了一个关键的不匹配：视觉标记化的损失可能成为目标任务的表示瓶颈。

Method: ETT是一种端到端的视觉标记优化方法，它实现了视觉标记化和目标自回归任务之间的联合优化。与之前仅使用冻结视觉标记器的离散索引的自回归模型不同，ETT利用了视觉标记器代码本的视觉嵌入，并通过重建和标题目标对视觉标记器进行了端到端优化。

Result: 实验表明，所提出的端到端视觉标记优化方法在多模态理解和视觉生成任务中相比冻结标记器基线提高了2-6%的性能，同时保持了原始的重建能力。

Conclusion: ETT方法在多模态理解和视觉生成任务中显著提升了性能，同时保持了原始的重建能力。这种方法简单且强大，可以增强除了图像生成和理解之外的多模态基础模型。

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [103] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang,Siyu Chen,Lihe Yang,Jialei Wang,Ziang Zhang,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度测量和深度预测的框架，生成准确、密集且详细的度量深度图。该框架通过从粗到细的流程整合两种互补的深度源，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的深度测量方法通常存在不完整但精确的度量信息，而深度预测方法则提供相对但完整的几何结构。因此，需要一种框架来结合这两种互补的深度源，生成准确、密集且详细的度量深度图。

Method: 我们设计了一个从粗到细的流程，逐步整合两种互补的深度源。首先，我们引入了像素级度量对齐和距离感知加权，通过显式使用深度预测来预填充多样化的度量先验。其次，我们开发了一个条件单目深度估计（MDE）模型来细化深度先验的固有噪声。

Result: 我们的模型在7个真实世界数据集上展示了惊人的零样本泛化能力，在深度补全、超分辨率和修复任务中匹配甚至超越了之前的任务特定方法。

Conclusion: 该模型在深度补全、超分辨率和修复任务中表现出色，能够处理具有挑战性的未见过的混合先验，并通过切换预测模型实现测试时的改进，提供了一种灵活的精度-效率权衡。

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [104] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alex Schwing,Liangyan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为3D-Fixup的新框架，用于通过学习的3D先验知识指导2D图像编辑。该框架支持困难的编辑情况，如对象平移和3D旋转。通过整合3D先验知识，3D-Fixup实现了高质量的结果，并推动了扩散模型在现实图像操作中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管在通过扩散模型建模图像先验方面取得了显著进展，但3D感知图像编辑仍然具有挑战性，因为对象仅通过单个图像指定。为了解决这个挑战，我们提出了3D-Fixup，一个新的框架，用于通过学习的3D先验知识指导2D图像编辑。

Method: 我们提出了3D-Fixup，这是一个新的框架，用于通过学习的3D先验知识指导2D图像编辑。我们利用基于训练的方法，利用扩散模型的生成能力。我们转向视频数据以生成训练数据对，即源帧和目标帧。我们结合来自Image-to-3D模型的3D指导，通过将2D信息显式投影到3D空间来解决这一具有挑战性的任务。我们设计了一个数据生成管道，以确保在整个训练过程中获得高质量的3D指导。

Result: 结果表明，通过整合这些3D先验知识，3D-Fixup有效地支持了复杂的、身份一致的3D感知编辑，实现了高质量的结果，并推动了扩散模型在现实图像操作中的应用。

Conclusion: 通过整合这些3D先验知识，3D-Fixup有效地支持了复杂的、身份一致的3D感知编辑，实现了高质量的结果，并推动了扩散模型在现实图像操作中的应用。

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 本文提出了一种新的搜索算法，在多种游戏中表现出色，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 目前尚无研究评估这些算法的泛化性能，因此本文旨在填补这一空白。

Method: 本文针对两人零和博弈且信息完全的情况，提出了一种新的搜索算法，并通过大规模实验验证了其性能。

Result: 在短时间搜索中，新算法在所有测试游戏中都优于所有研究的算法；在中等时间搜索中，它在22个测试游戏中的17个中优于所有研究的算法。

Conclusion: 本文提出了一种新的搜索算法，并证明了在短时间搜索中，该算法在所有测试游戏中都优于所有研究的算法；在中等时间搜索中，它在22个测试游戏中的17个中优于所有研究的算法。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [106] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文改进了特征重要性分析方法，提出了新的'有用性'概念，并在复杂模型中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地识别输入中的关键方面，并在复杂模型中高效检测必要性，本文旨在改进现有的特征重要性分析方法。

Method: 通过提出新的全局概念'有用性'，并开发高效的算法来检测决策树和其他复杂模型中的有用性，同时研究了相关性和必要性的概念。

Result: 本文展示了在复杂模型（如神经网络）中可以高效检测必要性，并提出了新的'有用性'概念，实验表明其在实际应用中的有效性。

Conclusion: 本文改进了现有技术并提出了新的全局概念'有用性'，并证明了其与相关性和必要性的关系。

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [107] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: 本文提出了一个广义动态目标识别问题，并采用无模型的目标条件强化学习方法，以实现跨变化任务的快速目标识别。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，目标众多且不断变化，传统的目标识别方法难以适应这些场景。因此，需要一种更广泛的目标识别定义和方法。

Method: 本文采用了一种无模型的目标条件强化学习方法，以实现跨各种变化任务的目标识别的快速适应。

Result: 本文提出的方法能够快速适应不同的变化任务，从而实现有效的目标识别。

Conclusion: 本文引入了广义动态目标识别问题，旨在实现实时目标识别系统并促进该领域的进一步研究。

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [108] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: 本文提出了XpertXAI模型，这是一种可解释的肺部病理检测模型，能够更好地与专家推理对齐，并展示了其在更广泛诊断场景中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在胸部X光片的肺部病理检测中表现出潜力，但其在临床中的广泛应用受到模型决策不透明的限制。因此，需要一种可解释的模型来提高临床采纳率。

Method: 本文介绍了XpertXAI模型，这是一种基于InceptionV3的分类器，结合了专家指导的概念瓶颈模型（CBM），以保持人类可解释的临床概念并扩展到检测多种肺部病理。

Result: XpertXAI在预测准确性上优于现有技术，并提供了与专家推理更一致的概念级解释。此外，该方法展示了如何将以人为本的模型设计扩展到更广泛的诊断场景。

Conclusion: 本文展示了XpertXAI模型在肺部病理检测中的优越性，并强调了以人为本的模型设计在医疗诊断中的可扩展性。

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [109] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 本文提出了一种多模态多智能体框架，用于放射学报告生成，以解决事实不一致、幻觉和跨模态错位等问题，并在自动指标和基于LLM的评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的方法利用多模态大语言模型（MLLMs）和检索增强生成（RAG）取得了良好的结果，但它们仍然面临事实不一致、幻觉和跨模态错位等挑战。

Method: 本文提出了一种多模态多智能体框架，该框架与逐步的临床推理工作流程相一致，其中特定任务的智能体处理检索、草稿生成、视觉分析、精炼和综合。

Result: 实验结果表明，我们的方法在自动指标和基于LLM的评估中都优于强基线，产生了更准确、结构化和可解释的报告。

Conclusion: 本文展示了临床对齐的多智能体框架在支持可解释和值得信赖的临床AI应用方面的潜力。

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [110] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: 本文研究了使用不同的离线强化学习算法进行微电网电压调节，特别是在环境交互不可行的情况下，通过之前收集的数据集进行离线训练来获得适用的模型。实验结果表明，所提出的方法在不同离线数据集上都具有可行性和有效性，包括仅包含低质量经验的数据集。


<details>
  <summary>Details</summary>
Motivation: 当由于技术和安全原因无法进行环境交互时，需要一种方法来获得适用的模型，以降低缺乏在线环境交互的负面影响。

Method: 本文研究了使用不同的离线强化学习算法进行微电网电压调节，特别是在环境交互不可行的情况下，通过之前收集的数据集进行离线训练来获得适用的模型。

Result: 在IEEE 33-bus系统上的实验结果证明了所提出方法的可行性和有效性，即使在仅包含低质量经验的数据集上也是如此。

Conclusion: 实验结果表明，所提出的方法在不同离线数据集上都具有可行性和有效性，包括仅包含低质量经验的数据集。

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [111] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本文提出了一个评估问题质量的框架，包含适当性和有效性两个维度，并通过动态上下文变量实现了灵活性和结构化。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏全面评估问题质量的研究，因此本文旨在定义好问题并提出一个系统评估框架。

Method: 本文提出了两个关键的评估维度：适当性（情境中的社会语言能力）和有效性（目标实现的战略能力），并开发了一个基于评分量表的系统。通过引入动态上下文变量，评估框架通过半自适应标准实现了结构和灵活性。

Result: 该方法在CAUS和SQUARE数据集上进行了验证，证明了该框架能够评估良好形成和存在问题的问题，并能适应不同的上下文。

Conclusion: 本文建立了一个灵活且全面的问题评估框架，为将提问行为与结构化分析方法相结合迈出了重要一步。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [112] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: 本文回顾了人工智能的发展历程，分析了其关键技术进步，并探讨了当前人工智能代理的潜力与社会影响，同时呼吁在快速发展的技术环境中保持智慧和远见。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人工智能的发展现状及其对未来社会的深远影响，并强调了在快速发展的技术环境中保持谨慎和前瞻性的重要性。

Method: 本文通过回顾人工智能的发展历程，分析了关键的技术里程碑，如提示技术、训练方法、硬件能力和架构创新，并探讨了这些智能代理的能力和潜在影响。

Result: 本文指出，当前的人工智能代理（如ChatGPT和Grok）代表了人工智能发展的顶峰阶段，可能构成我们目前所理解的‘最终一代’智能。此外，文章还提到智能的进步速度非常快，大约每六个月翻一番。

Conclusion: 本文强调了在人工智能发展过程中需要智慧和远见来应对所带来的机遇和挑战。

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [113] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Pre-Act 的新方法，通过生成多步骤执行计划来提高代理性能，并在 Almita 数据集上验证了其有效性，尤其在较小的模型上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的 ReAct 方法在大型模型中表现良好，但小型模型在实际应用中由于延迟和成本限制，在复杂推理任务中存在困难。

Method: 提出了一种新的方法 Pre-Act，通过创建多步骤执行计划和详细推理来增强代理性能，并提出了一个两层评估框架（回合级和端到端）。

Result: Pre-Act 在 Almita 数据集上的 Action Recall 比 ReAct 高 70%，微调后的 70B 模型在动作准确性和目标完成率上优于 GPT-4。

Conclusion: Pre-Act 提高了代理的性能，特别是在较小的模型上，通过微调可以实现比 GPT-4 更好的表现。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [114] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: 该论文提出了一个多模态人格感知抑郁检测挑战（MPDD），旨在通过结合多模态数据和个体差异因素来改善抑郁检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和检测方法主要集中在年轻人身上，忽视了更广泛的年龄范围和影响抑郁表现的个体差异。

Method: 提供了一个基线模型，该模型融合了音频和视频模态与个体差异信息以检测不同人群中的抑郁表现。

Result: 该挑战通过结合多模态数据和个体差异因素来解决这一差距。

Conclusion: 该挑战旨在促进更个性化和准确的抑郁检测方法的发展，推动心理健康研究并促进包容性的检测系统。

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [115] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs to help learners understand new knowledge concepts in MOOCs.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction between learners and instructors, making it challenging for learners to understand new knowledge concepts. LLMs are prone to hallucinations which limits their reliability. Current RAG systems do not actively guide learners toward their learning needs.

Method: We propose a Graph RAG pipeline that leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide learners to understand knowledge concepts in the MOOC platform CourseMapper. Specifically, we implement (1) a PKG-based Question Generation method to recommend personalized questions for learners in context, and (2) an EduKG-based Question Answering method that leverages the relationships between knowledge concepts in the EduKG to answer learner selected questions.

Result: The results of the evaluation show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.

Conclusion: Graph RAG has the potential to empower learners to understand new knowledge concepts in a personalized learning experience.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [116] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: 本文提出了一种AI辅助的方法，将非结构化的学术文本转化为结构化、交互式的知识表示，通过生成式AI技术和大型语言模型提取和标准化实体关系三元组，并利用D3.js系统进行可视化，构建了领域特定的知识图谱和向量数据库，从而实现了从线性文本消费到基于网络的知识导航的转变。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统地重新审视和重新组织基于台湾的中国研究学术成果，以满足日益增长的需求。

Method: 本文提出了一个AI辅助的方法，将非结构化的学术文本转化为结构化、交互式的知识表示。我们应用了生成式AI（GAI）技术和大型语言模型（LLMs）来从1996年至2019年间发表的1,367篇同行评审的中国研究文章中提取和标准化实体关系三元组。这些三元组通过基于D3.js的轻量级系统进行可视化，构成了该领域的领域特定知识图谱和向量数据库的基础。

Result: 通过将文本内容分解为图结构的知识单元，我们的系统使从线性文本消费到基于网络的知识导航范式转变成为可能。它增强了对CS文献的学术访问，同时提供了传统本体论构建的数据驱动替代方案。

Conclusion: 本文展示了生成式人工智能如何增强区域研究和数字人文学科，并突出了其支持重新构想区域知识系统学术基础设施的潜力。

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [117] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: 本文探讨了在医疗领域中，不同类型的AI生成解释如何影响医生对机器学习系统的信任和使用，通过用户研究和访谈发现最有效的解释类型。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域越来越多地采用人工智能，了解哪些类型的解释可以增加透明度并增强用户对机器学习系统预测的信任变得越来越重要。在共享决策场景中，建立相互信任至关重要。

Method: 该研究通过与医生进行用户研究和访谈，评估了不同类型的AI生成解释，并探讨了它们在诊断决策支持中的作用。

Result: 研究结果表明，通过用户研究和访谈，可以识别出最有效的解释类型，从而提高诊断过程的效果。

Conclusion: 该研究的结论是，通过用户研究和访谈，可以更好地理解在诊断决策支持中哪些类型的解释最有效。

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [118] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文介绍了MASS，一种用于投资组合构建的多代理规模模拟方法，通过逐步增加代理数量和反向优化过程来提高市场理解和代理分布优化，实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的多代理工作受到纯模拟或预定义工作流程的限制，限制了其适用性和效果。

Method: MASS通过逐步增加代理数量进行大规模模拟，以获得对市场的更好理解，并通过反向优化过程端到端地优化代理分布。

Result: 通过性能实验、消融研究、回测实验、更新数据和股票池的实验、缩放实验、参数敏感性实验和可视化实验，MASS在三个具有挑战性的A股股票池中与六个最先进的基线进行了比较，展示了其优越性。

Conclusion: MASS的范式有望扩展到具有类似特征的其他任务。

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [119] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估人工智能常识的方法，该方法考虑了人类之间的异质性，并发现小型模型在某些方面比大型模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的静态基准测试假设人类常识是同质的，但最近的实证研究显示人类在认为什么是常识方面存在巨大差异。因此，我们需要一种能够考虑这种异质性的新评估方法。

Method: 我们提出了一种新的方法来评估人工智能（AI）中的常识，特别是大型语言模型（LLMs），该方法通过测量模型判断与人类群体的一致性来体现人类之间的异质性。

Result: 大多数LLMs在个体常识能力上低于人类中位数，而当用作模拟假设人口时，LLMs与真实人类在同意同一组陈述的程度上仅有适度的相关性。较小的、开放权重的模型比更大的、专有的前沿模型更具竞争力。

Conclusion: 我们的评估框架将常识智能与其文化基础联系起来，有助于日益增长的呼吁，即调整人工智能模型以适应拥有不同且常常不兼容的社会知识库存的人类群体。

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [120] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: 本文探讨了SMT方法在人员排班中的应用，比较了Z3和Gurobi等求解器的性能，并发现SMT方法在某些问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT).

Method: We propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems.

Result: Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance.

Conclusion: SMT-based methods present a promising avenue for future research within the domain of personnel scheduling.

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [121] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 本文引入了塑料性这一概念，将其与赋权性联系起来，并揭示了它们之间的关系。


<details>
  <summary>Details</summary>
Motivation: 探讨代理如何被其观察到的内容所影响，以及这种影响的程度。

Method: 通过引入一种新的信息论量——广义定向信息，定义了塑料性。

Result: 塑料性是赋权性的镜像，代理的塑料性等于环境的赋权性，反之亦然。同时，代理的塑料性和赋权性之间存在张力。

Conclusion: 塑料性和赋权性及其关系对于理解代理行为至关重要。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [122] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: 本文提出了一种新的解释评估框架AXE，该框架不依赖于理想“地面真实”解释，能够独立评估解释质量，并用于检测解释公平性洗白。


<details>
  <summary>Details</summary>
Motivation: 当前的解释评估框架通过与理想的“地面真实”解释进行比较或验证模型对重要输入的敏感性来衡量质量，但这些方法存在局限性。

Method: 本文提出了三个期望的原则来指导局部特征重要性解释的评估策略的发展，并提出了一个基于这些原则的框架AXE。

Result: 通过与基线比较，验证了AXE的有效性，并展示了其在检测解释公平性洗白方面的应用。

Conclusion: 本文提出了一个不需要理想“地面真实”解释的框架AXE，可以独立评估解释质量，并能够检测解释公平性洗白。

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [123] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: 本文区分了AI代理和Agentic AI，分析了它们的设计哲学、能力以及应用领域，并提出了针对各自挑战的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了澄清AI代理和Agentic AI在设计哲学和能力上的不同，本文对它们进行了深入研究。

Method: 本文通过结构化的概念分类法、应用映射和挑战分析，批判性地区分了AI代理和Agentic AI，探讨了它们的设计哲学和能力差异。

Result: 本文提出了一个比较分析，涵盖了两种范式的架构演变、操作机制、交互风格和自主水平，并讨论了各自的应用领域和独特挑战。

Conclusion: 本文旨在为开发强大、可扩展和可解释的AI代理和Agentic AI驱动的系统提供明确的路线图。

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [124] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 本研究评估了自我反思、启发式突变和规划作为提示技术的有效性，以测试代理的适应能力。结果表明，较大的模型通常优于较小的模型，但战略提示可以缩小这种性能差距。太长的提示会负面影响较小模型的基本反应任务，而较大的模型表现出更多的鲁棒性。先进的提示技术主要对复杂游戏中的较小模型有益，但对已经表现良好的大型语言模型改善较少。然而，先进的推理方法会产生高度可变的结果，可能引入不稳定性并导致性能大幅下降。当前的大规模语言模型在关键领域如规划、推理和空间协调方面仍然存在持续的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型在静态基准上表现出色，但它们作为自我学习和推理代理在动态环境中的真正潜力仍不清楚。

Method: 本研究系统评估了自我反思、启发式突变和规划作为提示技术的有效性，以测试代理的适应能力。我们进行了各种开源语言模型在动态环境中的实验。

Result: 较大的模型通常优于较小的模型，但战略提示可以缩小这种性能差距。太长的提示会负面影响较小模型的基本反应任务，而较大的模型表现出更多的鲁棒性。先进的提示技术主要对复杂游戏中的较小模型有益，但对已经表现良好的大型语言模型改善较少。然而，我们发现先进的推理方法会产生高度可变的结果：当推理和决策一致时，它们可以显著提高性能，但也可能引入不稳定性并导致性能大幅下降。

Conclusion: 当前的大规模语言模型在关键领域如规划、推理和空间协调方面仍然存在持续的局限性，这表明它们仍存在根本性的不足，可能无法仅通过自我反思提示来完全克服。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [125] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: 本文提出了一种无损失的ANN-SNN转换方法（LAS），用于全脉冲驱动的LLMs。LAS通过引入新型神经元和定制的脉冲等效Transformer组件，实现了无性能损失的转换，并在多个模型上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的转换方法在处理基于ANN的LLMs的极端激活异常值和不兼容的非线性操作方面存在困难。因此，需要一种有效的转换方法，以实现无损失的转换并保持性能。

Method: 本文提出了一个无损失的ANN-SNN转换方法，称为LAS。LAS引入了两种新型神经元来转换基于ANN的LLMs的激活异常值和非线性操作，并为基于脉冲的LLMs定制了等效于脉冲的Transformer组件，以确保完全脉冲转换而不会有任何性能损失。

Result: 实验结果表明，LAS在六个语言模型和两个视觉-语言模型上实现了无损失转换。特别地，在OPT-66B上，LAS在WSC任务上的准确率提高了2%。参数和消融研究进一步验证了LAS的有效性。

Conclusion: 实验结果表明，LAS实现了无损失转换，并在OPT-66B上提高了WSC任务的准确率2%。参数和消融研究进一步验证了LAS的有效性。

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [126] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: 本文提出了一种通用且可扩展的方法，用于在噪声和低精度模拟硬件上执行大语言模型。该方法使最先进的模型在存在模拟噪声和量化约束的情况下，保持与4位权重、8位激活基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算（AIMC）是一种有前景的计算范式，可以超越传统冯·诺依曼架构的限制，提高神经网络推理的速度和能效。然而，AIMC带来了诸如计算噪声和输入输出量化严格限制等基本挑战。由于这些限制和不精确性，现成的大语言模型在部署到基于AIMC的硬件时无法达到4位级别的性能。

Method: 我们引入了一种通用且可扩展的方法，以稳健地适应大语言模型在噪声和低精度模拟硬件上执行。

Result: 我们的方法使最先进的模型（包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct）在存在模拟噪声和量化约束的情况下，保持与4位权重、8位激活基线相当的性能。此外，我们展示了作为训练方法的副产品，模拟基础模型可以被量化以在低精度数字硬件上进行推理。最后，我们展示了我们的模型在测试时计算缩放方面也受益，表现出比使用4位权重和8位静态输入量化的模型更好的缩放行为。

Conclusion: 我们的工作弥合了高容量大语言模型和高效模拟硬件之间的差距，为节能的基础模型提供了一条路径。

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [127] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文研究了图删除过程中是否引入偏见的问题，并提出了一个公平的图删除方法FGU，该方法在保持隐私和准确性的同时实现了优越的公平性。


<details>
  <summary>Details</summary>
Motivation: 当用户信息从模型中删除时，不同敏感群体之间的预测分布往往会改变。此外，图模型容易放大偏见，使得研究图删除中的公平性尤为重要。这引发了这样一个问题：图删除是否会引入偏见？

Method: 为了保证隐私，FGU在分割的子图上训练碎片模型，从相应的子图中删除请求的数据，并在修改后的子图上重新训练碎片模型。为了确保公平性，FGU采用了一个双层去偏过程：首先通过在碎片模型重新训练中引入公平正则化来实现碎片级公平性，然后通过对齐所有碎片模型以最小化全局差异来实现全局级公平性。

Result: 我们的研究结果表明，删除后的模型的预测与敏感属性高度相关，证实了图删除过程中引入了偏见。为此，我们提出了一种公平的图删除方法FGU。

Conclusion: 我们的实验表明，FGU在保持隐私和准确性的同时实现了优越的公平性。此外，FGU对各种删除请求具有鲁棒性，确保了在不同数据分布下的公平性和实用性。

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [128] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在人工智能物联网场景中的能源消耗问题，并提出了两种基于聚类的方法，以提高收敛速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有文献中常常忽视了联邦学习（FL）在人工智能物联网（AIoT）场景中的能源影响，因此本文旨在研究FL过程中的能耗问题。

Method: 本文提出了两种基于聚类的方案，旨在通过将具有相似标签分布的AIoT设备分组，从而减轻实际分布式学习应用中的异质性问题。

Result: 通过广泛的数值实验，本文展示了所提出的聚类策略在与其他最新方法相比时，通常能够实现较高的收敛速度并保持较低的能耗。

Conclusion: 本文提出的方法在保持低能耗的同时，通常能实现较高的收敛速度，从而有效缓解了实际分布式学习应用中常见的异质性问题。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [129] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 本文研究了深度形态神经网络（DMNNs），提出了一些新的架构，并验证了其可训练性和可剪枝性。同时，还提出了一种结合线性和形态层的混合架构，实验表明该架构能加速大规模批量梯度下降的收敛。


<details>
  <summary>Details</summary>
Motivation: 我们研究了深度形态神经网络（DMNNs），并证明尽管它们具有内在的非线性，但层间激活对于DMNNs是必要的。

Method: 我们提出了几种新的DMNN架构，每种架构都有不同的参数约束。第一（或第二）架构下，我们假设大多数参数（或可学习参数）应属于形态操作。

Result: 我们提出的网络可以成功训练，并且比线性网络更容易剪枝。据我们所知，我们是第一个在这些约束下成功训练DMNNs的团队，尽管网络的泛化能力仍然有限。

Conclusion: 我们提出了一种混合网络架构，结合了线性和形态层，实验证明形态层的引入显著加速了大规模批量梯度下降的收敛。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [130] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 本文探讨了如何验证算法是否真正学习到组合结构，并提出了新的网络架构以提高OOD性能，但发现即使有良好的性能，算法仍可能无法正确学习组合特征。


<details>
  <summary>Details</summary>
Motivation: 为了确认算法是否真正从数据中学习到了组合结构，不仅需要在OOD设置上进行测试，还需要确认所识别的特征确实是组合性的。

Method: 通过探索两个具有明确OOD指标的任务，验证了三种常用的神经网络（MLP、CNN和Transformer）无法解决这些任务，并开发了两种新的网络架构以提高OOD性能。

Result: 展示了即使在正确的偏差和接近完美的OOD性能下，算法仍然可能无法学习到正确的组合特征。

Conclusion: 即使在正确的偏差和几乎完美的OOD性能下，算法仍可能无法学习到正确的特征以实现组合泛化。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [131] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: 本文提出了一种解决联邦学习中数据质量问题的新方法，通过自适应噪声清洗、合成数据生成和鲁棒训练，提升了模型性能，并在资源受限设备上保持了实用性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保持数据隐私的同时，面临数据质量问题（如噪声标签、缺失类别和不平衡分布）的挑战。因此，需要一种有效的方法来提高联邦学习的效果。

Method: 本文提出了一种系统性解决数据质量问题的联邦学习方法，包括自适应噪声清洗、基于条件GAN的协作合成数据生成和鲁棒的联邦模型训练。

Result: 在基准数据集（MNIST和Fashion-MNIST）上的实验评估表明，在不同噪声和类别不平衡条件下，联邦模型性能有显著提升，特别是在宏F1分数方面。此外，所提出的框架在计算可行性和显著性能提升之间取得了良好的平衡，确保了资源受限边缘设备的实用性。

Conclusion: 本研究提出的方法能够有效缓解常见的数据质量问题，提供了一种强大、可扩展且符合隐私要求的解决方案，适用于各种现实世界的联邦学习场景。

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [132] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: 本文提出了一种生成性的端到端求解器，用于黑盒组合优化，该方法借鉴了基于退火的算法，并训练神经网络来模拟相关的玻尔兹曼分布。通过条件温度，网络捕捉到了从高温下的近似均匀分布到低温下围绕全局最优的尖锐分布的连续分布，从而学习了能量景观的结构并促进了全局优化。当查询昂贵时，温度依赖的分布自然地实现了数据增强并提高了样本效率；当查询便宜但问题仍然困难时，模型学习隐式变量交互，有效地“打开”黑盒。我们的方法在具有挑战性的组合任务上验证，表现出与最先进的黑盒优化器竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 当查询昂贵时，温度依赖的分布自然地实现了数据增强并提高了样本效率；当查询便宜但问题仍然困难时，模型学习隐式变量交互，有效地“打开”黑盒。

Method: 我们提出了一种生成性的端到端求解器，用于黑盒组合优化，该方法借鉴了基于退火的算法，并训练神经网络来模拟相关的玻尔兹曼分布。通过条件温度，网络捕捉到了从高温下的近似均匀分布到低温下围绕全局最优的尖锐分布的连续分布，从而学习了能量景观的结构并促进了全局优化。

Result: 我们的方法在具有挑战性的组合任务上验证，表现出与最先进的黑盒优化器竞争的性能。

Conclusion: 我们的方法在有限和无限查询预算的具有挑战性的组合任务上验证，表现出与最先进的黑盒优化器竞争的性能。

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [133] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的多智能体强化学习框架，该框架结合了社区结构、可迁移性和主动学习，并具有可证明的保证。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在处理多智能体强化学习时存在局限性，无法灵活地捕捉协调模式。因此，本文旨在提出一种新的框架，以更好地处理动态网络中的协作问题。

Method: 本文提出了一种基于社区的框架，其中每个智能体可以属于多个重叠的社区，并且每个社区维护共享的策略和价值函数。此外，设计了利用这种结构的actor-critic算法，使智能体能够继承社区级别的估计进行策略更新和价值学习。

Result: 本文提出的框架支持通过成员资格估计适应新智能体或任务的迁移学习，以及通过优先考虑不确定的社区进行主动学习。此外，理论分析表明，在线性函数近似下，演员和评论家的更新都有收敛保证。

Conclusion: 本文提出了一个新颖的多智能体强化学习框架，该框架结合了社区结构、可迁移性和主动学习，并具有可证明的保证。

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [134] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [135] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: 本文提出了一个高效的检查点压缩解决方案，称为Language Model Compressor (LMC)，它基于字节分组和霍夫曼编码。LMC在压缩性能上优于现有方案，同时显著减少了压缩时间。


<details>
  <summary>Details</summary>
Motivation: 在训练大型语言模型时，需要将张量数据定期“检查点”到持久化存储，以便在发生故障时恢复工作。然而，每次检查点所需复制的数据量很大，甚至使用低精度表示（如bfloat16）时也可能达到数百GB。此外，数据必须在网络上传输并写入存储系统，才能在下一个epoch开始前完成。本文旨在构建一个优化的检查点解决方案。

Method: 本文通过实验分析了检查点数据，以制定最大化使用无损压缩的方案。我们评估了现有常见的通用压缩引擎以及已知的数据优化技术，如字节分组和增量差分压缩的效果。

Result: LMC在压缩性能上优于最佳替代方案（BZ2），但压缩所需时间减少了一个数量级。16核并行实现的LMC可以达到2.78 GiB/s的压缩吞吐量和3.76 GiB/s的解压缩吞吐量。

Conclusion: 本文提出了一个有效的压缩解决方案，称为Language Model Compressor (LMC)，它基于字节分组和霍夫曼编码。LMC在压缩性能上优于最佳替代方案（BZ2），但压缩所需时间减少了一个数量级。

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [136] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 本研究评估了多种机器学习算法在预测中风风险方面的效果，并发现了影响预测的关键特征，同时提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 中风仍然是全球最重要的健康挑战之一，因此需要更有效的预测方法。

Method: 本研究探讨了机器学习算法在使用人口统计、临床和生活方式数据预测中风风险方面的有效性，并评估了逻辑回归、随机森林和XGBoost等多种模型的性能。

Result: 虽然这些模型实现了高准确性，但灵敏度仍然是实际临床应用中的限制因素。

Conclusion: 这些发现有助于开发更可靠和可解释的模型，用于中风风险的早期评估。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [137] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数梯度下降和Bregman投影方法的越狱技术，该技术在多个LLM上表现出更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管使用了诸如从人类反馈中强化学习（RLHF）等技术对许多模型进行对齐，但它们仍然容易受到越狱攻击。现有的对抗攻击方法在离散空间或连续空间中寻找可能越狱的token，但这些方法存在效率低下或无效的问题。

Method: 开发了一种基于指数梯度下降和Bregman投影方法的内在优化技术，以确保优化的one-hot编码始终位于概率单纯形内。

Result: 该技术在四个公开数据集上的实验结果表明，其成功率较高且效率良好。

Conclusion: 该技术在五个开源LLM上进行了测试，结果表明其成功率高于其他三种最先进的越狱技术，并且效率很高。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [138] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 本文研究了从平滑信号中学习Kronecker结构产品图的问题，并提出了一个交替方案来优化每个因子图，同时提供了理论保证，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于多向数据的普及，对产品图的兴趣增加，但目前可以学习的产品图类型仍然有限，无法建模多样化的依赖结构。

Method: 本文提出了一种交替方案来优化每个因子图，并提供了其渐近收敛的理论保证。此外，还修改了算法以学习强积因子图。

Result: 在合成和真实世界图上进行了实验，并展示了所提出方法的有效性和优越性。

Conclusion: 本文提出了一种从平滑信号中学习Kronecker结构产品图的方法，并通过实验验证了其有效性和优越性。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [139] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文介绍了一种新的销售优化和商业AI方法，称为因果预测优化与生成（CPOG），并展示了其在LinkedIn中的成功应用。


<details>
  <summary>Details</summary>
Motivation: 销售过程的优化对于B2B企业的成功至关重要，因此需要一种系统化的方法来提高销售效率和效果。

Method: 本文提出了一种基于因果机器学习、约束优化和上下文带武装算法以及生成AI和反馈循环的三层架构。

Result: 在LinkedIn中的实施展示了CPOG方法相比传统系统的显著优势，并分享了可广泛应用于该领域的学习和见解。

Conclusion: 本文提出了一个系统化的销售优化和商业AI方法，即因果预测优化与生成（CPOG），并在LinkedIn中成功实施，展示了显著的成果。

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [140] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 本文提出了一种新的异构双部图表示学习方法，用于利用结构MRI图像和基因表达数据进行阿尔茨海默病检测，并取得了良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 整合成像和基因组数据可以揭示疾病复杂景观的新见解，但目前缺乏有效的分类方法。

Method: 提出了一种新的异构双部图表示学习方法，利用结构MRI图像和基因表达数据进行阿尔茨海默病检测。

Result: 该框架能够有效将阿尔茨海默病分为三个阶段，并识别出每个分类组中的重要基因。

Conclusion: 该方法在小数据集上表现出色，并且可以扩展到其他疾病的基于放射基因组的分类。

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [141] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: 本文介绍了基于zentropy理论的数据驱动机器学习新方法，提出了一个能够有效处理异构数据的ZENN模型，并展示了其在科学问题中的强大性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于熵的方法在量化数据中的不确定性和无序性以及开发人工智能算法方面一直是非常重要的工具。然而，各个领域数据的快速增长带来了新的挑战，特别是整合具有内在差异的异构数据集。

Method: 我们通过引入内在熵，将zentropy理论扩展到数据科学领域，并提出了一种zentropy增强型神经网络（ZENN），该网络同时学习能量和内在熵组件，以捕捉多源数据的底层结构。我们重新设计了神经网络架构，以更好地反映不同数据集中的固有属性和变异性。

Result: 我们在分类任务和能量景观重构中展示了ZENN的有效性，显示了其优越的泛化能力和鲁棒性，尤其是在预测高阶导数方面。作为实际应用，我们使用ZENN重建了Fe3Pt的Helmholtz能量景观，并捕获了关键材料行为，包括负热膨胀和温度-压力空间中的临界点。

Conclusion: 我们的研究引入了一种基于zentropy理论的数据驱动机器学习新方法，突出了ZENN作为处理复杂、异构数据集的科学问题的多功能且稳健的深度学习框架。

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [142] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: This paper introduces Chisme, a novel suite of protocols for robust intelligence at the network edge, addressing challenges of heterogeneous data distributions, episodic connectivity, and lack of infrastructure. It includes synchronous DFL and asynchronous GL variants, along with a data similarity heuristic for better personalized training while maintaining collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing paradigms like federated learning (FL) and decentralized FL (DFL) face challenges in connectivity and synchronization in resource-constrained and infrastructure-less environments. Gossip learning (GL) algorithms are robust but generally designed for homogeneous data distributions, which may not suit all contexts.

Method: Chisme introduces a novel suite of protocols, including synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL), which enable collaborative yet decentralized model training. It also introduces a data similarity heuristic that allows agents to infer affinity with each other using existing communication of model updates in decentralized FL and GL.

Result: Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

Conclusion: Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [143] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型中的两种学习模式：权重内学习（IWL）和上下文内学习（ICL）。通过借鉴进化生物学的适应策略，我们发现环境稳定性和线索可靠性影响了这两种模式的平衡。高环境稳定性倾向于IWL，而高线索可靠性促进ICL。研究结果揭示了学习动态的复杂性，并提出了一个相对成本假设来解释这些变化。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解这两种学习模式之间的相互作用，我们受到进化生物学中类似适应策略的启发。

Method: 我们从进化生物学中类似的适应策略中获得灵感，如遗传编码（类似于IWL）和表型可塑性（类似于ICL）。我们实验操作了这些可预测性的维度，并系统地研究了它们对Transformer中ICL/IWL平衡的影响。

Result: 高环境稳定性明显有利于IWL，而高线索可靠性增强了ICL的效果，特别是在稳定性较低时。此外，学习动态显示了任务相关的时空演变：在某些情况下（例如，具有许多类别的分类），会发生典型的ICL到IWL的转变，但在其他情况下（例如，较少类别或较慢的ICL获取）可能初始为IWL阶段，随后被ICL主导。

Conclusion: 这些发现支持了一个相对成本假设，用于解释这些学习模式的转变，确立了可预测性作为控制Transformer中适应策略的关键因素，并提供了新的见解来理解ICL并指导训练方法。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [144] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文介绍了一种基于Transformer的统一归因方法，能够处理不同级别的数据和外部宏观因素，并在LinkedIn上进行了大规模实施，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的归因是现代营销智能的基础，对于任何营销业务和广告平台都至关重要。然而，现有的方法在处理不同级别的数据和外部因素方面存在局限性。

Method: 本文提出了一种基于Transformer的统一归因方法，能够处理成员级数据、聚合级数据以及外部宏观因素的整合。

Result: 本文在LinkedIn大规模实施了该方法，并展示了其显著影响。同时，分享了对营销和广告技术领域具有广泛适用性的学习和见解。

Conclusion: 本文介绍了基于Transformer的统一归因方法，展示了其在LinkedIn的大规模实施和显著影响，并分享了对营销和广告技术领域的广泛适用的学习和见解。

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [145] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO is a new pruning technique that reduces the computational cost of training large machine learning models while preserving their accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and cost of large machine learning models have made them inaccessible to non-wealthy individuals and increased costs for consumers. Current pruning methods are computationally and environmentally taxing.

Method: BINGO studies specific subsets of a neural network one at a time during the training pass to gauge how significant each weight plays in contributing to a network's accuracy. It generates a significance score for each weight, allowing for insignificant weights to be pruned in one shot.

Result: BINGO offers a less computationally intensive pruning technique that preserves accuracy, making AI development more accessible and sustainable.

Conclusion: BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [146] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA 是一种非参数变化检测算法，结合了 KQT 直方图和 EWMA 统计量，用于在线监控多变量数据流。它能够控制 ARL_0，并且在检测延迟方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 大多数非参数变化检测测试很少能事先控制 ARL_0，而 KQT-EWMA 能够通过操作预定义的 ARL_0 来控制误报。

Method: KQT-EWMA 结合了 Kernel-QuantTree (KQT) 直方图和 EWMA 统计量，用于在线监控多变量数据流。

Result: 实验表明，KQT-EWMA 在控制 ARL_0 的同时，检测延迟与最先进的方法相当或更低。

Conclusion: KQT-EWMA 可以控制 ARL_0，同时在检测延迟方面与最先进的方法相当或更低。

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [147] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: 本研究比较了大型语言模型、人类和MAB算法的探索-利用策略，并发现推理使大型语言模型更接近人类的行为，但在复杂环境中仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 我们想了解大型语言模型是否表现出与人类相似的决策行为，并且能否实现相当（或更好）的性能。

Method: 我们使用可解释的选择模型来捕捉代理的探索-利用策略，并研究通过提示策略和增强推理的模型如何影响大型语言模型的决策。

Result: 我们发现，推理使大型语言模型更接近人类的行为，这种行为由随机和定向探索的混合组成。在简单的静态任务中，推理增强的大型语言模型表现出与人类相似水平的随机和定向探索。然而，在更复杂、非静态的环境中，大型语言模型难以匹配人类的适应能力，尤其是在有效的定向探索方面，尽管在某些情况下达到了相似的遗憾值。

Conclusion: 我们的研究结果突显了大型语言模型在模拟人类行为和自动化决策方面的潜力和局限性，并指出了可能的改进领域。

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [148] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 本文综述了针对文本、图像、视频和音频四种模态的对抗性攻击，填补了多模态世界中对抗性威胁景观的空白，并提供了对多模态对抗性威胁演化的全面概述。


<details>
  <summary>Details</summary>
Motivation: 由于多模态模型在现实世界应用中的普及，需要为实践者提供对抗性威胁的视角，以便采取必要的预防措施。然而，目前缺乏针对多模态世界的实践者导向的攻击类型概述。

Method: 本文对文本、图像、视频和音频四种模态的对抗性攻击进行了综述，以提供多模态对抗性威胁的全面概述。

Result: 本文首次全面总结了多模态世界中的威胁景观，并提供了对抗性威胁演化的视图。

Conclusion: 本文通过调查针对文本、图像、视频和音频四种模态的对抗性攻击，填补了多模态世界中对抗性威胁景观的空白，并提供了对多模态对抗性威胁演化的全面总结。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [149] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: 本研究提出了一种混合深度学习模型TCN-MLP-Attention，用于哈斯牛油果价格预测，取得了优异的预测性能，并为农业市场的时间序列预测提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 随着对健康食品需求的增长，农产品价格预测变得越来越重要。哈斯牛油果作为一种高价值作物，其价格波动受到季节性、地区和天气等因素的影响。传统预测模型在处理高度非线性和动态数据时常常遇到困难。

Method: 我们提出了一个混合深度学习模型，TCN-MLP-Attention架构，结合了时间卷积网络（TCN）用于序列特征提取，多层感知器（MLP）用于非线性交互，以及注意力机制用于动态特征加权。

Result: 实验结果表明，TCN-MLP-Attention模型具有出色的预测性能，RMSE为1.23，MSE为1.51，优于传统方法。

Conclusion: 本研究提供了一种可扩展且有效的时间序列预测方法，为农业市场的智能供应链管理和价格策略优化提供了有价值的见解。

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [150] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest 是一种专为流式数据设计的新型异常检测方法，在效率方面优于所有竞争对手，适用于需要快速检测异常的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的在线异常检测方法通常无法解决这些约束，依赖于定期重新训练以适应在线环境。

Method: Online-iForest 是一种专为流式条件设计的新方法，能够无缝跟踪随时间演变的数据生成过程。

Result: 实验验证表明，Online-iForest 与在线替代方案相当，并且与定期重新训练的最先进离线异常检测技术相媲美。

Conclusion: Online-iForest 是一种在实时数据流中检测异常的有前途的解决方案，尤其在需要快速识别异常的应用中表现优异。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [151] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 本文研究了直接采样欧几里得扩散模型以处理一般流形约束数据，并提出了两种新方法来减轻得分函数的奇异性和提高采样精度。


<details>
  <summary>Details</summary>
Motivation: 我们研究了直接采样欧几里得扩散模型以处理一般流形约束数据，因为现有工作未显式利用特殊流形的结构。

Method: 我们提出了两种新方法：Niso-DM，它沿法向引入非各向同性噪声以减少尺度差异；Tango-DM，它仅使用仅切向的损失函数训练得分函数的切向分量。

Result: 数值实验表明，我们的方法在各种具有复杂几何结构的流形上的分布中表现出优越性能。

Conclusion: 我们的方法在各种具有复杂几何结构的流形上的分布中表现出色。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [152] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文提出了一种新的交互式持续学习框架RiCL，它利用大型语言模型从实时人类反馈中动态学习新技能，同时保留先前知识。RiCL包含三个关键组件，能够有效处理噪声反馈，并在基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习存在两个主要限制：(1) 使用流式实时人工标注数据进行动态模型更新，而不是使用固定标签的静态数据集；(2) 假设标签是干净的，而没有明确处理现实世界交互中常见的噪声反馈。本文旨在解决这些问题。

Method: 本文提出了RiCL，一种利用大型语言模型（LLMs）从动态反馈中有效学习新技能的强化交互持续学习框架。RiCL包含三个关键组件：一个时间一致性感知净化器，用于自动区分数据流中的干净样本和噪声样本；一种交互感知的直接偏好优化策略，通过协调AI生成的反馈和人类提供的反馈来使模型行为与人类意图对齐；以及一种抗噪声对比学习模块，通过利用数据的内在关系来捕获稳健的表示，从而避免依赖可能不可靠的标签。

Result: RiCL方法在两个基准数据集（FewRel和TACRED）上进行了广泛的实验，这些数据集被现实噪声模式污染，结果显示RiCL方法显著优于现有最先进的在线持续学习和噪声标签学习方法的组合。

Conclusion: 本文提出的RiCL方法在两个包含现实噪声模式的基准数据集（FewRel和TACRED）上进行了广泛的实验，结果表明该方法显著优于现有的最先进的在线持续学习和噪声标签学习方法的组合。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [153] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: 本研究利用大型语言模型分析高速公路事故数据，识别事故原因并提供全面解释。结果显示LLM能有效识别主要事故原因，结合事件数据可提供更深入的见解，验证了模型在改善交通安全方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 理解导致交通事故的因素并制定策略以减轻其严重程度至关重要。传统统计方法和机器学习模型往往难以捕捉各种因素之间的复杂相互作用以及每次事故的独特特征。

Method: 本研究利用大型语言模型（LLM）分析高速公路事故数据，并通过微调Llama3 8B模型来增强其对高速公路事故及其相关因素的理解。该模型使用QLoRA进行微调，并通过零样本分类识别事故原因，无需预标记数据。

Result: 结果表明，LLM能够有效地识别主要事故原因，如酒后驾驶、超速、危险驾驶和驾驶员分心。结合事件数据（如道路维护）可以提供更深入的见解。研究人员在交通安全领域的高度一致性和问卷调查结果（88.89%）验证了模型的实际适用性和改善交通安全措施的潜力。

Conclusion: 本研究强调了交通事故的复杂性，并展示了大型语言模型在全面分析事故原因和其他相关因素方面的潜力。此外，它为规划者和政策制定者提供了有价值的见解和潜在的应对措施，以制定更有效和高效的交通安全措施。

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [154] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [155] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: TransPL 是一种新的无监督域适应方法，通过代码转换矩阵建模时间序列数据的联合分布，能够有效捕捉时间过渡和通道变化，并提供可解释的伪标签生成。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签策略无法捕捉时间模式和域之间的通道差异，导致次优伪标签。

Method: TransPL 通过代码转换矩阵建模源域的联合分布 P(X, y)，其中代码来自时间序列块的向量量化 (VQ)。该方法构建了类和通道相关的代码转换矩阵，并利用贝叶斯规则进行目标域适应，基于通道加权的类条件似然生成伪标签。

Result: TransPL 在四个时间序列 UDA 基准上表现出色，相比最先进的伪标签方法，在准确率和 F1 分数上分别提高了 6.1% 和 4.9%。

Conclusion: TransPL 的有效性通过在四个时间序列 UDA 基准上的广泛分析得到验证，并且它在伪标签生成方面提供了可解释的见解。

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [156] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedRAG的联邦强化学习框架，通过共享近似行为度量状态投影函数来提高FRL性能并保护敏感信息。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦强化学习方法通常共享加密的本地状态或策略信息，以在保护隐私的同时让每个客户端从他人学习。然而，这些方法可能无法有效保护敏感任务特定信息。

Method: 本文提出了FedRAG框架，该框架为每个客户端学习计算上可行的状态投影函数，并在中央服务器聚合投影函数的参数。

Result: 实验结果表明，FedRAG方法在DeepMind Control Suite上取得了有见解的结果，能够提供信息增益而不共享敏感任务特定信息。

Conclusion: 共享基于近似行为度量的状态投影函数是一种增强FRL性能并有效保护敏感信息的有前途的方法。

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [157] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: 本研究提出了一种基于机器学习的心脏病预测框架，使用三种分类器进行评估，其中随机森林表现最佳，准确率达到91%。研究强调了机器学习在医疗保健中的应用潜力，并指出需要更大、更多样化的数据集进行未来研究。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用机器学习方法对心脏病进行预测，以辅助临床决策。

Method: 该研究采用了机器学习框架，包括数据预处理、模型训练和评估，使用了三种分类器：逻辑回归、K-最近邻（KNN）和随机森林。通过GridSearchCV和RandomizedSearchCV进行了超参数调优。

Result: 随机森林分类器表现最佳，准确率为91%，F1得分为0.89。评估指标包括精确率、召回率和混淆矩阵，显示了各类别之间的平衡性能。

Conclusion: 该模型展示了在临床决策支持中的强大潜力，但数据集的大小和泛化能力存在局限性，需要未来使用更大、更多样化的数据集进行研究。

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [158] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为AI2MMUM的多模态通用模型，用于处理多模态数据和执行各种空中接口任务。该模型在多个任务中表现出色，具有良好的性能。


<details>
  <summary>Details</summary>
Motivation: 设计一个6G导向的通用模型，能够处理多模态数据并执行各种空中接口任务，已成为未来无线系统的一个共同目标。

Method: 提出了一种可扩展、任务感知的人工智能-空气接口多模态通用模型（AI2MMUM），该模型通过固定任务关键词和可学习的隐式前缀提示来增强任务适应性，并使用轻量级任务特定头直接输出任务目标。

Result: AI2MMUM在五个代表性物理环境/无线信道下游任务中实现了最先进的性能。

Conclusion: AI2MMUM在基于WAIR-D和DeepMIMO数据集的五个代表性物理环境/无线信道下游任务中实现了最先进的性能。

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [159] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [160] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 该研究介绍了ImagineBench，这是一个用于评估离线强化学习算法的基准，这些算法利用真实轨迹和大型语言模型生成的想象轨迹。研究发现，现有算法在利用想象轨迹方面表现不佳，强调了改进算法的必要性，并提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准基准，该领域的发展受到阻碍。因此，需要一个全面的基准来评估离线RL算法在利用LLM生成轨迹方面的性能。

Method: 引入了ImagineBench，这是第一个全面的基准，用于评估利用真实轨迹和LLM生成轨迹的离线RL算法。

Result: 通过系统评估最先进的离线RL算法，发现简单地应用现有算法在未见过的任务上表现不佳，硬任务的成功率仅为35.44%，而使用真实轨迹训练的方法则达到64.37%。

Conclusion: 该研究强调了需要改进算法以更好地利用LLM生成的想象轨迹，并指出了未来研究的关键机会，包括更好地利用想象轨迹、快速在线适应和持续学习，以及扩展到多模态任务。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [161] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: 该研究提出了一种新的归一化方法，以提高量子-经典混合机器学习模型在抗癌药物反应预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 量子-经典混合机器学习模型在小数据集上表现出强大的性能和高泛化能力，这使其在抗癌药物反应预测中具有独特优势。然而，这些模型对神经网络和量子电路接口处的数据编码非常敏感，导致稳定性问题。

Method: 提出了一种基于修正梯度版本的$	anh$的归一化函数的新策略，以解决量子-经典混合机器学习模型对数据编码敏感的问题。

Result: 实验结果表明，当数据经过最优归一化时，量子-经典混合机器学习模型的表现优于传统深度学习模型。

Conclusion: 该研究为使用量子计算机进行生物医学数据分析开辟了新的可能性。

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [162] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，用于全面识别电路中的逻辑门，并揭示了它们在语言模型功能中的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现方法无法保证完整性，导致电路在不同运行中不一致，并且可能遗漏关键机制。OR 门的存在是不完整性的根源，通常在标准电路发现方法中只能部分检测到。

Method: 本文系统地引入了三种逻辑门：AND、OR 和 ADDER 门，并将电路分解为这些逻辑门的组合。提出了一个结合噪声和去噪干预的框架，可以轻松集成到现有的电路发现方法中，而不会显著增加计算复杂度。

Result: 该框架能够完全识别逻辑门并在电路中区分它们。实验验证了框架恢复电路忠实性、完整性和稀疏性的能力，并揭示了三种逻辑门的基本属性，如它们的比例和对输出的贡献。

Conclusion: 本文提出了一种结合噪声和去噪干预的框架，能够全面识别逻辑门并在电路中区分它们。此外，还揭示了三种逻辑门的基本属性，并探索了它们在语言模型功能中的行为。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [163] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: 本文提出了一种新的非示例连续图学习范式IPAL，通过原型对比学习（PCL）减少特征漂移，并利用图结构信息制定Topology-Integrated Gaussian Prototypes (TIGP)来增强模型吸收新知识的能力。此外，还引入了Instance-Prototype Affinity Distillation (IPAD)和Decision Boundary Perception (DBP)机制，以提高类间可区分性。实验结果显示，该方法在四个节点分类基准数据集上优于现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNN) 面临灾难性遗忘的问题，这削弱了它们在吸收新信息时保留先前获得知识的能力。基于重放的技术通过回顾历史示例来缓解这一现象，但内存爆炸和隐私侵犯限制了其效用。非示例方法通过原型重放（PR）规避了这些问题，但特征漂移带来了新的挑战。

Method: 我们提出了Instance-Prototype Affinity Learning (IPAL)，这是一种非示例连续图学习的新范式。我们利用图结构信息，制定了Topology-Integrated Gaussian Prototypes (TIGP)，指导特征分布向高影响节点发展，以增强模型吸收新知识的能力。Instance-Prototype Affinity Distillation (IPAD)通过规范类关系中的不连续性来保护任务记忆。此外，我们在PCL中嵌入了一个Decision Boundary Perception (DBP)机制，促进类间可区分性。

Result: 我们的实证结果表明，原型对比学习（PCL）比传统的PR表现出更小的漂移。基于PCL，我们提出了IPAL，一种用于非示例连续图学习的新范式。

Conclusion: 我们的方法在四个节点分类基准数据集上的评估表明，它优于现有的最先进方法，实现了更好的可塑性和稳定性之间的权衡。

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [164] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: 本文提出了一种结合梯度提升模型和可解释人工智能技术的欺诈检测框架，以提高模型的预测准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型通常优先考虑预测准确性，而牺牲了模型的透明度和可解释性。缺乏透明度使得组织难以遵守监管要求并获得利益相关者的信任。

Method: 我们提出了一个结合了众所周知的梯度提升模型（XGBoost、LightGBM 和 CatBoost）的堆叠集成的欺诈检测框架。此外，使用了可解释的人工智能（XAI）技术来增强模型决策的透明度和可解释性。

Result: 该模型在 IEEE-CIS 欺诈检测数据集上表现出色，准确率达到 99%，AUC-ROC 得分达到 0.99，优于几种最近的相关方法。

Conclusion: 这些结果表明，将高预测准确性与透明的可解释性相结合是可能的，并且可能在金融欺诈检测中带来更道德和值得信赖的解决方案。

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [165] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: 本文提出了一种自适应蒸馏方法，用于改进深度估计和场景分割的统一建模，通过动态调整知识量和避免知识遗忘，实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度估计和场景分割是智能交通系统中的重要任务，联合建模可以减少存储和训练需求。现有的解决方案以静态方式传递多个教师的知识，但这种方法存在知识遗忘的问题。

Method: 我们提出了一种自适应蒸馏方法，可以根据学生当前的学习能力动态调整从每个教师获取的知识量，并设计了一种基于轨迹的蒸馏损失来避免知识遗忘。

Result: 我们在多个基准数据集（包括Cityscapes和NYU-v2）上评估了我们的方法，结果表明我们的方法优于最先进的解决方案。

Conclusion: 我们的方法在多个基准数据集上取得了明显改进，证明了其有效性。

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [166] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 本文提出了一种多模态时间序列预测框架ChronoSteer，通过将文本事件转换为修订指令来引导时间序列基础模型。该方法在仅使用合成数据训练的情况下，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法依赖于单模态时间序列数据，限制了其利用丰富文本信息的能力。最近，大型语言模型（LLMs）和时间序列基础模型（TSFMs）分别在文本推理和时间建模方面表现出强大的能力。整合两者的优势以构建一个同时利用时间和文本信息进行未来推理的多模态模型已成为关键研究挑战。

Method: 提出了一种解耦框架：使用LLM将文本事件转换为修订指令，然后用于引导TSFM的输出。引入了ChronoSteer，这是一种可以通过文本修订指令进行引导的多模态TSFM。还设计了一种基于合成数据的两阶段训练策略，并构建了一个高质量的多模态时间序列预测基准。

Result: ChronoSteer在仅使用合成数据训练的情况下，与单模态基线相比预测准确性提高了25.7%，比之前的最先进多模态方法提高了22.5%。

Conclusion: ChronoSteer在仅使用合成数据训练的情况下，与单模态基线相比预测准确性提高了25.7%，比之前的最先进多模态方法提高了22.5%。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [167] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: This paper introduces MiCo, a hierarchical language agent framework that uses LLMs to solve the ODMBP problem in cloud services. It achieves high performance in large-scale and complex environments.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods, heuristic approaches, and learning-based methods face limitations in adapting to real-time changes, rigid strategies, and lack of generalizability and interpretability in solving the Online Dynamic Multidimensional Bin Packing (ODMBP) problem in cloud services.

Method: The paper proposes a hierarchical language agent framework named MiCo, which formulates ODMBP as a Semi-Markov Decision Process with Options (SMDP-Option) and employs a two-stage architecture: Option Miner and Option Composer. Option Miner discovers non-context-aware strategies using LLMs, while Option Composer integrates these strategies with contextual ones.

Result: MiCo achieves a 96.9% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.

Conclusion: MiCo demonstrates its effectiveness in complex and large-scale cloud environments by achieving a high competitive ratio and maintaining performance under nonstationary request flows and diverse configurations.

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [168] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 本研究提出了一种新的策略，通过结合图变换器神经网络和XGBoost模型生成的合成任务，提高了多任务分子属性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 将基于规则的模型如随机森林注入可微分神经网络框架仍然是机器学习中的一个开放挑战。尽管最近的进展表明预训练模型可以生成高效的分子嵌入，但这些方法通常需要大量的预训练和额外的技术来提高性能。

Method: 我们提出了一种新的策略，将单个图变换器神经网络同时训练在稀疏多任务分子属性实验目标和从XGBoost模型生成的合成目标上，这些合成任务作为独立的辅助任务。

Result: 我们的结果表明，在所有19个分子属性预测任务中，性能都有持续且显著的提升。在19个目标中的16个中，多任务图变换器优于XGBoost单任务学习者。

Conclusion: 我们的研究展示了合成任务增强是一种有效的方法，可以在不进行特征注入或预训练的情况下提高多任务分子属性预测中神经模型的性能。

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [169] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [170] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习框架FedAPC，通过原型增强来提高在领域异质性下的全局模型泛化能力。实验结果表明，该框架在Office-10和Digits数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许在分布式边缘设备上进行协作训练，同时确保数据隐私，使其成为隐私敏感应用的流行解决方案。然而，由于统计异质性，特别是领域异质性，FL面临重大挑战，这阻碍了全局模型的收敛。

Method: 我们引入了FedAPC（联邦增强原型对比学习），这是一种基于原型的联邦学习框架，旨在提高特征多样性和模型鲁棒性。FedAPC利用增强数据的平均特征生成原型，以捕获更丰富的表示。通过将本地特征与全局原型对齐，使模型能够学习有意义的语义特征，同时减少对任何特定领域的过拟合。

Result: 在Office-10和Digits数据集上的实验结果表明，我们的框架优于最先进的基线，展示了优越的性能。

Conclusion: 实验结果表明，我们的框架在Office-10和Digits数据集上优于最先进的基线，展示了优越的性能。

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [171] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: 本文研究了多智能体多臂老虎机中的最佳臂识别问题，提出了一种新的算法，并在样本复杂性和通信开销方面进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多智能体多臂老虎机中最佳臂识别的问题，同时最小化样本复杂性和通信开销。

Method: 本文提出了两种新的算法：Clustering then Best Arm Identification (Cl-BAI) 和 Best Arm Identification then Clustering (BAI-Cl)。这两种算法都利用了连续消除框架以确保计算效率和高精度。

Result: 本文建立了两种方法的δ-PC保证，推导了它们的样本复杂性界限，并提供了该问题类的下界。当M是小的（常数）时，BAI-Cl的一个变种在顺序意义上是最优的。实验表明，所提出的算法在样本和通信效率方面表现优越。

Conclusion: 本文提出了两种新的算法(Cl-BAI和BAI-Cl)，用于多智能体多臂老虎机中的最佳臂识别问题，并在样本复杂性和通信开销方面进行了实验验证。

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [172] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer is a new attention mechanism that improves the integration of semantic and positional information by using complex numbers and allows each head to learn distinct strategies for processing data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of integrating positional information effectively while allowing flexibility in multi-head attention mechanisms. Prior methods often model semantic and positional differences separately or apply uniform adjustments across heads, which may limit representational capacity.

Method: ComplexFormer introduces Complex Multi-Head Attention (CMHA), which allows each head to independently model semantic and positional differences within the complex plane. It includes a per-head Euler transformation and a per-head adaptive differential rotation mechanism.

Result: Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show that ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.

Conclusion: ComplexFormer demonstrates strong parameter efficiency and offers a more expressive, adaptable attention mechanism compared to existing methods like RoPE-Transformers.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [173] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架QuXAI，基于Q-MEDLEY，以提高混合量子-经典机器学习模型的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前，XAI在量子系统中仍处于初期阶段，缺乏针对采用量化特征编码并随后进行经典学习的HQML架构的稳健全局和局部解释方法。本文旨在填补这一研究空白。

Method: 本文介绍了QuXAI框架，该框架基于Q-MEDLEY，用于解释这些混合系统中的特征重要性。它涉及构建包含量子特征映射的HQML模型，并利用Q-MEDLEY结合基于特征的推断，保留量子变换阶段并可视化结果属性。

Result: 实验结果表明，Q-MEDLEY能够明确HQML模型中有影响力的经典方面，并区分其噪声，在经典验证设置中与现有的XAI技术竞争良好。消融研究进一步揭示了Q-MEDLEY中复合结构的优势。

Conclusion: 本研究提出了QuXAI框架，基于Q-MEDLEY，以提高混合量子-经典机器学习（HQML）模型的可解释性和可靠性，从而促进更安全和负责任的量子增强人工智能技术的使用。

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [174] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: 本研究提出了一种超轻量级时间序列预测模型Alinear，该模型在使用极少参数的情况下表现出色，并挑战了大型模型更好的传统观念。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模的快速扩展，时间序列预测面临关键挑战。尽管性能提升通常伴随着参数数量的指数增长，但这种扩展是否真正必要仍值得质疑。

Method: 我们提出了Alinear，这是一种超轻量级预测模型，使用k级参数即可实现具有竞争力的性能。我们引入了一种与时间范围相关的自适应分解机制，以及一种渐进式频率衰减策略，以在不增加计算开销的情况下实现稳定预测。

Result: 在七个基准数据集上的广泛实验表明，Alinear在使用不到其1%参数的情况下始终优于大规模模型，并在短期和超长预测时间范围内保持强准确性。此外，我们提出了一种新的参数感知评估指标，以更公平地评估模型效率。

Conclusion: 本研究挑战了大型模型本质上更好的普遍信念，并建议向更高效的时间序列建模范式转变。

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [175] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: 本文探讨了使用合成数据训练深度学习模型以检测半导体制造中的微小缺陷的可能性。结果表明，YOLOv8在检测较小缺陷方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于缺陷注释的质量数据缺乏良好的代表性，特别是更小的缺陷，这阻碍了基于深度学习的缺陷检测模型在制造线中的部署。

Method: 我们人工生成具有已知缺陷分布的线图案扫描电子显微镜（SEM）图像，并自主标注它们。然后，我们采用最先进的目标检测模型来研究缺陷检测性能作为缺陷尺寸的函数，比节距宽度小得多。

Result: YOLOv8在检测较小缺陷方面表现出最佳的平均精度（96%），而EfficientNet为83%，SSD为77%。当在真实SEM数据上测试时，YOLOv8模型正确检测了84.6%的桥接缺陷和78.3%的断裂缺陷。

Conclusion: 这些有希望的结果表明，合成数据可以作为开发稳健机器学习模型的现实世界数据的替代品。

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [176] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）中表示叠加对神经缩放定律的影响。通过构建一个玩具模型，作者发现当叠加较弱时，损失与模型大小的缩放取决于基础特征频率；而在强叠加下，损失与模型维度成反比。他们还分析了四个开源LLM家族，并发现它们符合模型预测。结论是表示叠加是神经缩放定律的重要机制。


<details>
  <summary>Details</summary>
Motivation: 了解神经缩放定律的起源，即损失随着模型大小呈幂律下降的现象。

Method: 从两个经验原理出发——LLM表示比它们的模型维度（宽度）更多的东西（即表示是叠加的），以及语言中的单词或概念出现的频率不同——我们构建了一个玩具模型来研究损失与模型大小的缩放关系。

Result: 当叠加较弱时，损失与模型大小的缩放取决于基础特征频率；如果特征频率遵循幂律，损失也是如此。在强叠加下，损失与模型维度成反比。这种稳健的缩放行为被几何解释：当许多向量被压缩到较低维空间时，向量之间的干扰（平方重叠）与该维度成反比。我们分析了四个开源LLM家族，并发现它们表现出强叠加，并定量地符合我们的玩具模型预测。

Conclusion: 表示超叠加是观察到的神经缩放定律的重要机制。我们预计这些见解将激发新的训练策略和模型架构，以更少的计算和更少的参数实现更好的性能。

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [177] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: 本文提出了一种基于声学和惯性信号融合的深度神经网络，以提高对奶牛进食行为的监测准确性。


<details>
  <summary>Details</summary>
Motivation: 监测奶牛的进食行为对于高效的牧场管理和资源利用至关重要。自动识别动物的进食活动可以改善饮食配方，以及早期检测代谢问题和动物不适等症状。

Method: 本文提出了一种基于声学和惯性信号融合的深度神经网络，包含卷积、循环和密集层。

Result: 特征级融合优于数据级和决策级融合，F1分数至少提高了0.14。此外，与最先进的机器学习方法相比，该模型的F1分数为0.802，比之前的方法提高了14%。

Conclusion: 该模型通过融合声学和惯性信号，提高了对奶牛进食行为监测的准确性，并且在F1分数上比之前的方法提高了14%。

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [178] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: ParScale是一种新的模型扩展方法，通过增加并行计算来提高推理效率，并在低资源场景下具有优势。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在扩展语言模型时需要付出显著的空间或时间成本，而ParScale提供了一种更高效的扩展方式。

Method: ParScale通过在训练和推理过程中增加模型的并行计算来实现扩展，使用P种多样且可学习的变换对输入进行处理，并动态聚合P个输出。

Result: ParScale可以使用更少的内存和延迟来达到与参数扩展相同的效果，并且可以通过微调现有的预训练模型来进一步减少训练预算。

Conclusion: ParScale可以提高模型的推理效率，并且在低资源场景下部署更强大的模型提供了可能。

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [179] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的跨领域知识迁移框架，以提高大型语言模型（LLMs）在时间序列预测中的性能。该方法系统地将结构化的时间信息注入LLMs，以提高其预测准确性。实验结果表明，基于知识的预测显著优于无辅助信息的基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，需要建立最佳实践来超越传统的自然语言任务。本文旨在通过跨领域知识迁移框架，提高LLMs在时间序列预测中的性能，以满足能源系统、金融和医疗等领域的需求。

Method: 本文提出了一种跨领域知识迁移框架，通过系统地将结构化的时间信息注入LLMs，以提高其在时间序列预测中的性能。

Result: 实验结果表明，基于知识的预测显著优于无辅助信息的基线模型，在预测准确性和泛化能力方面都有所提升。

Conclusion: 本文的研究结果表明，知识迁移策略可以有效弥合大型语言模型与特定领域预测任务之间的差距。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [180] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于路由器的架构，利用领域资源和语言模型生成高质量的合成数据，以解决LLM在函数调用任务中因缺乏真实数据而表现不佳的问题。实验结果表明，该方法在函数分类和API参数选择上均有显著提升，并建立了新的基准。


<details>
  <summary>Details</summary>
Motivation: 在数字内容创作工具中，用户通过自然语言查询表达需求，这些查询必须映射到API调用。由于缺乏真实世界任务特定的数据以及隐私限制，需要生成合成数据。然而，现有的合成数据生成方法在多样性和复杂性方面存在不足，无法复制真实世界的分布，导致微调后的性能不佳。

Method: 我们提出了一种基于路由器的架构，利用领域资源（如内容元数据和结构化知识图谱）以及文本到文本和视觉到文本的语言模型生成高质量的合成训练数据。

Result: 在一组真实用户查询上的评估显示，函数分类准确率和API参数选择都有显著提升。使用我们的合成数据微调的模型始终优于传统方法。

Conclusion: 我们的方法在函数调用任务中建立了新的基准，表明使用合成数据进行微调可以显著提高性能。

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [181] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV is a method that accelerates vision-language models by transforming small language models into effective multimodal drafters through a two-phase approach, achieving significant speedups in inference.


<details>
  <summary>Details</summary>
Motivation: Applying speculative decoding to vision-language models presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context.

Method: MASSV transforms existing small language models into effective multimodal drafters through a two-phase approach. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.

Result: Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

Conclusion: MASSV provides a scalable, architecture-compatible method for accelerating both current and future vision-language models (VLMs).

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [182] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: 本文提出了 SpecOffload，一种高效的推理引擎，通过将推测解码嵌入到卸载过程中，显著提高了 GPU 利用率和推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统在 GPU 内存有限的情况下将模型权重卸载到 CPU 内存，导致 CPU 和 GPU 之间的 I/O 开销大，GPU 核心利用率低，GPU 内存对性能影响小。

Method: 提出了一种称为 SpecOffload 的高吞吐推理引擎，将推测解码嵌入到卸载过程中，通过利用 GPU 资源存储和执行草稿模型来加速推理。

Result: 与最佳基线相比，SpecOffload 提高了 GPU 核心利用率 4.49 倍，并提升了推理吞吐量 2.54 倍。

Conclusion: SpecOffload 提高了 GPU 核心利用率并提升了推理吞吐量，展示了其在资源受限设备上的高效性。

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [183] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的电动公交车充电调度方法，通过分层结构和新的算法优化充电策略，以降低充电成本。


<details>
  <summary>Details</summary>
Motivation: 解决长距离多阶段规划中稀疏奖励的挑战，同时最小化充电成本并满足充电目标。

Method: 提出了一种分层深度强化学习（HDRL）方法，将原始MDP分解为一个高层半马尔可夫决策过程（SMDP）和多个低层MDP，并采用分层双深度Q网络（HDDQN）-事后经验回放（HER）算法来解决不同时间分辨率下的决策问题。

Result: 数值实验表明，所提出的算法在真实世界数据上的性能表现良好。

Conclusion: 通过将高层策略和低层策略的最优策略叠加，构建的平坦策略在性能上与原始MDP的最优策略相当。

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [184] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据重建攻击方法，克服了现有方法的局限性，并在图像和表格数据集上进行了广泛的实验，证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据重建攻击方法存在重要限制，例如依赖于对客户端数据分布的假设，或者当批量大小超过几十个样本时效率显著下降。因此，需要一种更有效的方法来应对这些挑战。

Method: 本文引入了一种基于全连接层的新几何视角的新型数据重建攻击方法，通过构造恶意模型参数，实现了对任意大规模数据批次的完美恢复。

Result: 实验结果表明，本文提出的攻击方法在图像和表格数据集上均优于现有方法，并且能够实现比现有技术大两个数量级的数据批次的完美重建。

Conclusion: 本文提出了一种新的数据重建攻击方法，克服了现有方法的局限性，并在图像和表格数据集上进行了广泛的实验，证明了其优越性。

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [185] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: 本文介绍了一种新的深度学习模型，用于在欧洲进行高分辨率概率降水预测，该模型整合了多种数据源，并在准确性、可解释性和计算效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 克服仅使用雷达的深度学习模型在短预报提前时间上的局限性。

Method: 我们提出了一种深度学习模型，用于在欧洲8小时时间范围内进行高分辨率概率降水预测，该模型有效地整合了多种数据源（包括雷达、卫星和基于物理的数值天气预报），同时捕捉长距离相互作用，通过一致的概率图实现准确的预测和稳健的不确定性量化。

Result: 广泛的实验表明，我们的模型超越了当前的操作性NWP系统、基于外推的方法和深度学习现在预报模型。

Conclusion: 我们的模型在欧洲高分辨率降水预报中设定了新标准，确保了准确性、可解释性和计算效率之间的平衡。

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [186] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: 本文将Hebbian的尖峰时间依赖可塑性规则与噪声梯度下降联系起来，证明了该规则能够识别出活动最高的前突触神经元，并发现其与噪声镜面下降有内在联系。


<details>
  <summary>Details</summary>
Motivation: 了解基于精确尖峰时间的学习规则，而不仅仅是基于神经元放电率的Hebbian学习。

Method: 将Hebbian的尖峰时间依赖可塑性规则与概率单纯形上的自然损失函数的噪声梯度下降联系起来。

Result: 该学习规则最终能够识别出活动最高的前突触神经元，并且与噪声镜面下降有内在联系。

Conclusion: 该学习规则最终能够识别出活动最高的前突触神经元。

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [187] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文提出了一种新的分层深度强化学习方法（DAC-MAPPO-E），用于优化电动公交车队的充电调度，并在真实数据上验证了其优越性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于旅行时间、能耗和电价的不确定性，优化电动公交车充电调度仍然是一个关键挑战。此外，为了应对现实世界的复杂性，充电政策必须在多个时间尺度上高效决策，并且能够扩展到大规模的电动公交车队。

Method: 本文提出了一个分层深度强化学习（HDRL）方法，将原始的马尔可夫决策过程（MDP）重新构造成两个增强的MDP，并引入了双演员-评论家多智能体近端策略优化增强（DAC-MAPPO-E）算法。

Result: 通过使用真实世界的数据进行广泛实验，证明了DAC-MAPPO-E在优化电动公交车队充电调度方面的优越性能和可扩展性。

Conclusion: 本文提出了一种新的分层深度强化学习方法（DAC-MAPPO-E），在优化电动公交车队充电调度方面表现出优越的性能和可扩展性。

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [188] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: 本文提出了一种名为FeRA的新联邦学习防御机制，利用跨客户端注意力来区分良性与恶意客户端。FeRA基于表示重构误差计算异常分数，有效识别出内部激活与群体共识显著偏离的客户端。实验结果表明，FeRA在各种联邦学习场景中表现出稳健性，有效降低了后门攻击的成功率，同时保持了主要任务的高准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）通过在边缘进行分布式模型训练来增强隐私并减少通信成本，但此类设备的异质性产生了多样化的、非独立同分布（non-IID）数据，使得检测后门攻击更具挑战性。

Method: 提出了一种新颖的基于联邦代表注意力的防御机制FeRA，利用跨客户端注意力来区分良性客户端和恶意客户端。FeRA基于表示重构误差计算异常分数，有效地识别出内部激活与群体共识显著偏离的客户端。

Result: FeRA在各种联邦学习场景中表现出稳健性，包括典型的边缘设备非独立同分布数据分布。实验结果表明，它有效降低了后门攻击的成功率，同时保持了主要任务的高准确性。

Conclusion: FeRA在各种联邦学习场景中表现出稳健性，包括典型的边缘设备非独立同分布数据分布。实验结果表明，它有效降低了后门攻击的成功率，同时保持了主要任务的高准确性。该方法是模型无关、攻击无关的，并且不需要标记的参考数据，使其非常适合异构和资源受限的边缘部署。

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [189] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文提出了一种新的异常检测方法PIF，结合了自适应隔离方法和偏好嵌入的优势，通过将数据嵌入高维空间并使用PI-Forest计算异常得分，实验结果表明PIF优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决结构化模式下的异常检测问题。

Method: PIF结合了自适应隔离方法的优点与偏好嵌入的灵活性，通过将数据嵌入高维空间并使用基于树的方法PI-Forest计算异常得分。

Result: 实验表明PIF在合成和真实数据集上表现良好，PI-Forest在测量任意距离和隔离偏好空间中的点方面更有效。

Conclusion: PIF在检测异常方面优于最先进的方法，并且PI-Forest在测量任意距离和隔离偏好空间中的点方面表现更好。

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [190] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于负度量学习的图对比学习方法（NML-GCL），通过引入可学习的负度量网络和双层优化目标，有效解决了假负例问题，并在多个基准测试中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有解决假负例问题的方法通常依赖于人工先验知识，导致图对比学习效果不理想。因此，需要一种无需人工干预的改进方法。

Method: NML-GCL采用了一个可学习的负度量网络（NMN）来构建负度量空间，以更好地区分假负例和真负例。此外，还提出了一个联合训练方案，结合双层优化目标，隐式利用自监督信号迭代优化编码器和负度量网络。

Result: NML-GCL在多个广泛使用的基准测试中表现出优越性，理论分析和实验结果均验证了其有效性。

Conclusion: NML-GCL通过引入可学习的负度量网络和双层优化目标，显著提升了图对比学习的效果，并在广泛使用的基准测试中得到了验证。

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [191] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL是一种基于NAS的增量学习框架，通过动态调整模型结构和跨蒸馏训练来平衡可塑性和稳定性，从而在资源受限的环境中实现高效的学习。


<details>
  <summary>Details</summary>
Motivation: 现有的基于NAS的增量学习方法通常在每个任务中扩展模型，这在资源受限的环境中不切实际。因此，需要一种更高效的框架来平衡模型的可塑性和稳定性。

Method: SEAL是一种基于NAS的框架，针对数据增量学习进行优化。它通过动态调整模型结构，在必要时进行扩展，并通过跨蒸馏训练保持稳定性。NAS组件共同搜索架构和最佳扩展策略。

Result: 在多个基准测试中，SEAL有效减少了遗忘并提高了准确性，同时保持了比先前方法更低的模型大小。

Conclusion: 这些结果突显了将NAS和选择性扩展相结合在增量场景中实现高效、自适应学习的前景。

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [192] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: 本文提出了一种新的异步去中心化随机梯度下降算法（ADSGD），它在各种场景中表现出更好的收敛性能，并且对通信和计算延迟具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化对于在没有中央控制的情况下利用分布式数据至关重要，但实际部署面临计算速度异构性和不可预测的通信延迟等挑战。

Method: 本文引入了在实际假设下的异步去中心化随机梯度下降（ADSGD）模型，并通过分析异步随机块坐标下降（ASBCD）来理解其收敛性。

Result: ADSGD 在各种场景中比现有方法在墙钟收敛时间上表现更好，并且其收敛结果不依赖于数据异质性的有界假设。

Conclusion: ADSGD 是一种适用于现实世界去中心化学习任务的算法，因其简单性、内存和通信效率以及对通信和计算延迟的鲁棒性。

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [193] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: 本文提出了一种名为ALERT的方法，用于检测特征分布变化并触发模型重新训练，适用于无线网络的两个用例。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，特征分布的变化可能会降低AI模型的性能并导致不良行为。为了应对未被检测到的模型退化，我们提出了ALERT。

Method: ALERT包括三个组件：表示学习、统计测试和效用评估。表示学习使用MLP，统计测试使用Kolmogorov-Smirnov和Population Stability Index测试，效用评估使用新函数。

Result: ALERT在两个无线网络用例中表现出优于文献中十种标准漂移检测方法的优越性。

Conclusion: ALERT可以有效检测特征分布变化并触发模型重新训练，适用于无线网络的两个用例。

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [194] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: 本文探讨了深度强化学习在动态环境中的适应问题，并提出两种关键能力：优先探索和采样策略，以及通过结构化表示选择性地保留先前知识。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的自主决策系统必须在变化的环境中运行。虽然深度强化学习在静态环境中表现出色，但大多数方法数据密集且假设世界在训练和测试时间之间不会变化。因此，当条件变化时，传统RL方法难以适应。这带来了根本性的挑战：如何在部署过程中遇到新环境变化时，有效地调整行为而不灾难性地遗忘有用的知识？

Method: 本文讨论了深度强化学习（RL）在非静态环境中的适应问题，并提出了两种关键能力：优先探索和采样策略，以及通过结构化表示选择性地保留先前知识。

Result: 本文证明了高效的在线适应需要两个关键能力：(1) 优先探索和采样策略，有助于识别和从相关经验中学习，以及(2) 通过结构化表示选择性地保留先前知识，这些表示可以在不干扰可重用组件的情况下进行更新。

Conclusion: 本文展示了高效的在线适应需要两个关键能力：(1) 优先探索和采样策略，有助于识别和从相关经验中学习，以及(2) 通过结构化表示选择性地保留先前知识，这些表示可以在不干扰可重用组件的情况下进行更新。

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [195] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 本文提出了一种理论模型，研究随机分类器集合中集体行为的出现，并证明了在特定温度下分类是最优的，且该温度具有普遍性。


<details>
  <summary>Details</summary>
Motivation: 随机性在数据科学和机器学习的许多应用中普遍存在。系统由随机组件组成时，通常表现出看似确定的宏观行为，这促使我们研究这些系统的集体行为及其优化方法。

Method: 本文引入了一个理论模型，通过采用分类损失作为能量的Gibbs测度对集成进行加权，以研究随机分类器集合中集体行为的出现。我们通过分析和数值验证证明了最优温度的存在，并通过物理类比解释了自组织现象。

Result: 我们证明了在特定温度参数下，分类是相对于损失最优的，并且这种最优温度不依赖于教师分类器或随机分类器的数量。实验结果表明该现象在高质量、无噪声的数据集中的重要性。

Conclusion: 本文提出了一个理论模型，用于研究随机分类器集合中集体行为的出现。我们证明了在特定温度参数下，分类是相对于损失（或能量）最优的，并且这种最优温度不依赖于教师分类器或随机分类器的数量，表明了所观察到行为的普遍性。实验结果表明该现象在高质量、无噪声的数据集中的重要性。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [196] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: 本文介绍了离散变分自编码器，提供了一种实用的训练方法和示例实现。


<details>
  <summary>Details</summary>
Motivation: 近年来，离散潜在空间在许多数据模态（如文本）中变得流行，因此需要一种系统的方法来理解和应用离散变分自编码器。

Method: 本文从基本数学背景出发，详细推导了离散变分自编码器的每个步骤，并开发了一个具体的训练方案。

Result: 本文提供了离散变分自编码器的严谨而实用的介绍，并给出了一个示例实现。

Conclusion: 本文介绍了离散变分自编码器（DVAEs），并提供了一个实用的训练方法和示例实现。

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [197] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: 本文评估了SMTOs在多任务学习中的有效性，发现它们表现良好，固定权重可以实现与SMTOs相当的性能，并且在某些情况下，统一损失的表现与SMTOs相似。


<details>
  <summary>Details</summary>
Motivation: 为了评估最近的批评，即SMTOs的结果可能受到不良超参数优化和缺乏正则化的干扰，以及探索SMTOs在多任务学习中的有效性。

Method: 通过广泛的实证评估对SMTOs进行评估，包括一些最新的方法，在更复杂的多任务问题上澄清这种行为。

Result: SMTOs在多任务学习中表现良好，固定权重可以实现与SMTOs相当的性能，并且在某些情况下，统一损失的表现与SMTOs相似。

Conclusion: SMTOs在多任务学习中表现良好，固定权重可以与SMTOs竞争性地表现，并且在某些情况下，统一损失的表现与SMTOs相似。

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [198] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 本文提出了一种名为FactsR的方法，用于在医疗咨询过程中实时提取关键临床信息，并利用这些信息递归生成最终的病历记录。该方法提高了病历的准确性和简洁性，并将临床医生纳入病历生成过程中，同时为实时决策支持开辟了新的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前的AI病历记录解决方案在生成病历时仍然依赖于一次或少量提示，缺乏足够的推理能力，导致病历冗长且容易出现幻觉和错误，这可能对患者安全构成威胁。因此，需要一种更准确、简洁的病历生成方法。

Method: 本文提出了一种名为FactsR的方法，用于在医疗咨询过程中实时提取关键临床信息，并利用这些信息递归生成最终的病历记录。

Result: FactsR方法通过将临床医生纳入病历生成过程，提高了病历的准确性和简洁性，并为实时决策支持开辟了新的应用场景。

Conclusion: FactsR方法能够提高病历的准确性和简洁性，并将临床医生纳入病历生成过程，从而改善患者安全。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [199] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: 本文提出了SCGP，一种基于群论的增强方法，通过Schreier-coset嵌入丰富节点特征，而无需改变输入图拓扑。SCGP在保持计算效率的同时提高了长距离消息传递的性能，并在处理分层和模块化图结构方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案，如图重布线和抗瓶颈架构（如Cayley图和expander图），虽然避免了过压缩问题，但引入了可扩展性瓶颈。Cayley图在SL(2,Z_n)上构造时表现出强大的理论性质，但由于节点增长为O(n^3)，导致高内存使用。

Method: SCGP是一种基于群论的增强方法，通过Schreier-coset嵌入丰富节点特征，而无需改变输入图拓扑。它将无瓶颈的连接模式嵌入到紧凑的特征空间中，从而提高长距离消息传递的效率。

Result: SCGP在标准节点和图分类基准上的实证评估表明，其性能可以与或超过expander图和重布线GNN基线相媲美。此外，SCGP在处理分层和模块化图结构方面具有显著优势。

Conclusion: SCGP在处理分层和模块化图结构时表现出色，具有较低的推理延迟、良好的可扩展性和低内存占用，适合实时和资源受限的应用。

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [200] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG is a two-stage VAE-based IA mesh generator that can generate realistic IA shapes with specific morphological measurements, which is useful for studies on shape variations and flow simulations.


<details>
  <summary>Details</summary>
Motivation: The absence of a large IA image dataset necessitates the need for a generative model to train networks for predicting blood flow forces in real time, which is crucial for understanding disease progression.

Method: AneuG is a two-stage VAE-based IA mesh generator. In the first stage, it generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes. In the second stage, it generates parent vessels conditioned on GHD tokens by generating vascular centreline and propagating the cross-section.

Result: AneuG can generate realistic IA shapes with specific morphological measurements, which can be used for studies on shape variations and flow simulations.

Conclusion: AneuG can generate realistic IA shapes with specific morphological measurements, which is useful for studies on shape variations and flow simulations.

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [201] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: 本文通过分析在线辅导环境中的归纳人类学习模拟，发现将学习分解为多个不同机制可以提高数据效率，使其与人类学习一致，并指出整合多个专门的学习机制可能是弥合机器学习与人类学习差距的关键。


<details>
  <summary>Details</summary>
Motivation: 人类学习依赖于专业化，而现代神经网络主要依赖单一机制。这引发了问题：人类学习者能否通过使用多个专门的学习机制组合来实现快速学习？

Method: 我们通过在线辅导环境中的归纳人类学习模拟的消融分析来研究这个问题。比较了强化学习与更高效的数据3机制符号规则归纳方法。

Result: 将学习分解为多个不同的机制显著提高了数据效率，使其与人类学习一致。此外，这种分解对效率的影响比符号和非符号学习的区别更大。

Conclusion: 我们的研究结果表明，整合多个专门的学习机制可能是弥合数据驱动机器学习与人类学习之间差距的关键。

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [202] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: 本研究探讨了梯度下降在神经网络中的学习能力，并揭示了分布无关学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 我们希望理解梯度下降在神经网络中的学习能力，并探索分布无关学习的局限性。

Method: 我们通过分析基于梯度的优化与随机特征线性组合优化之间的关系，引入了一种新的理论框架，称为平均概率维度复杂度（adc）。

Result: 结果表明，如果可以使用小批量随机梯度下降（bSGD）学习参数模型而不做数据分布假设，则目标函数可以用多项式大小的随机特征组合近似。

Conclusion: 我们的研究揭示了在神经网络中使用梯度下降进行分布无关学习的根本限制，并强调了在实践中对数据分布做出假设的重要性。

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [203] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: L2T is a framework for large language models that optimizes reasoning by reducing computational complexity and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the trade-off between reasoning effectiveness and computational efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens.

Method: L2T is an information-theoretic reinforcement fine-tuning framework that treats each query-response interaction as a hierarchical session of multiple episodes. It proposes a universal dense process reward, which quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. A method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix is also proposed.

Result: Theoretical analyses show that L2T significantly reduces computational complexity with high estimation accuracy. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.

Conclusion: L2T demonstrates advantages across different tasks by boosting both reasoning effectiveness and efficiency.

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [204] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 本文研究了基于评分的扩散模型在现在预报云和降水中的应用。实验表明，扩散模型能够平流现有的云，还能生成和消散云，包括对流的开始。最佳的模型是CorrDiff方法，在均方根误差上优于其他模型和传统方法。此外，扩散模型还能直接生成集合，显示出良好的校准。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报在模拟云和降水时具有挑战性，因为需要亚网格参数化。机器学习已被用于预测云和降水，但早期的机器学习方法通常会产生模糊的预测。本文旨在探索一种较新的方法，即基于评分的扩散模型，用于现在预报云和降水。

Method: 本文探讨了一种较新的方法，即基于评分的扩散模型，用于现在预报（零到三小时的预报）云和降水。我们讨论了基于评分的扩散模型的背景和直觉，从而为社区提供了一个起点，同时探索了该方法在现在预报静止红外图像中的应用。我们测试了三种主要类型的扩散模型：标准的基于评分的扩散模型（Diff）；残差校正扩散模型（CorrDiff）；以及潜在扩散模型（LDM）。

Result: 扩散模型能够不仅平流现有的云，还能生成和消散云，包括对流的开始。这些结果令人惊讶，因为预测仅以过去20分钟的红外卫星图像为起点。最佳的扩散模型是CorrDiff方法，在均方根误差上比所有其他扩散模型、传统的U-Net和一个持续性预测高出1到2开尔文。扩散模型还能够直接进行集合生成，这显示了良好的校准，集合的范围与误差有很好的相关性。

Conclusion: 扩散模型能够不仅平流现有的云，还能生成和消散云，包括对流的开始。这些结果令人惊讶，因为预测仅以过去20分钟的红外卫星图像为起点。最佳的扩散模型是CorrDiff方法，在均方根误差上比所有其他扩散模型、传统的U-Net和一个持续性预测高出1到2开尔文。扩散模型还能够直接进行集合生成，这显示了良好的校准，集合的范围与误差有很好的相关性。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [205] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: 本文讨论并解决了传统实验方法在推导燃气轮机发动机组件级和局部线性参数变化模型时的局限性，提出了一种基于数据驱动的识别技术，并设计了一个基于Koopman的控制器，该控制器在性能上优于其他基准控制器。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在推导组件级和局部线性参数变化模型时存在局限性，因此需要一种更有效的方法来处理燃气轮机发动机的复杂非线性动力学系统。

Method: 使用稀疏非线性动力学识别技术估计转子动力学，并将动力学的自主部分映射到最优构造的Koopman特征函数空间中。随后进行了特征值优化，使用元启发式算法和时间投影，然后进行基于梯度的特征函数识别。

Result: 基于Koopman的控制器在参考跟踪和扰动抑制方面表现出色，特别是在海平面和不同飞行条件下。

Conclusion: 结果表明，基于Koopman的控制器在参考跟踪和扰动抑制方面优于其他基准控制器，这是由于其全局性质。

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [206] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: This paper introduces NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy, achieving sample efficiency comparable to MLP+PPO and outperforming existing methods in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models due to the computational intractability of action likelihood estimation during the denoising process.

Method: Introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy.

Result: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.

Conclusion: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [207] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: PnPXAI 是一种通用的 XAI 框架，旨在解决现有 XAI 框架的局限性，通过自动检测模型架构、推荐解释方法和优化超参数来提高解释效果。


<details>
  <summary>Details</summary>
Motivation: 现有的 XAI 框架存在一些局限性，包括对不同模型架构和数据模态的灵活性不足、支持的 XAI 方法数量有限以及缺乏评估和优化阶段，导致难以在实际应用中推广 XAI 技术。

Method: PnPXAI 采用了一种即插即用（PnP）的方法，能够自动检测模型架构，并推荐适用的解释方法，同时优化超参数以获得最佳解释效果。

Result: PnPXAI 通过用户调查验证了其有效性，并展示了其在医学和金融等多个领域的广泛适用性。

Conclusion: PnPXAI 是一个通用的 XAI 框架，可以支持多种数据模态和神经网络模型，并通过自动检测模型架构、推荐适用的解释方法和优化超参数来提供最佳解释。

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [208] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 本文提出了QFIX，一种新的价值函数分解模型，能够提升现有方法的性能，并在多个环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的价值函数分解方法（如VDN、QMIX）在表示能力上有限，无法表示所有IGM值，而唯一的例外QPLEX则过于复杂。因此，需要一种简单且有效的解决方案。

Method: 提出了一种完整的IGM值的简单公式，并推导出QFIX，这是一种新的价值函数分解模型家族，通过一个薄的“修复”层扩展了先前模型的表示能力。

Result: QFIX在多个SMACv2和Overcooked环境中进行了实验评估，结果表明它能够提升现有方法的性能，学习更稳定，并且表现优于QPLEX。

Conclusion: QFIX能够增强现有方法的性能，学习更稳定，并且在使用最简单和最小的混合模型的情况下表现优于其主要竞争对手QPLEX。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [209] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: 本文介绍了神经热力学定律（NTL），这是一个新框架，揭示了大语言模型训练动态中的热力学量和原理，并为学习率调度提供了指导。


<details>
  <summary>Details</summary>
Motivation: 了解大语言模型（LLM）的规律，特别是在神经缩放定律之外。

Method: 通过假设河流山谷损失景观，展示了关键热力学量和经典热力学原理的自然出现。

Result: NTL框架提供了对LLM训练动态的新见解，并产生了直观的学习率调度设计指南。

Conclusion: NTL框架为理解大语言模型的训练动态提供了新的视角，并为设计学习率调度提供了直观的指导。

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


### [210] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a pharmacophore-conditioned diffusion model that integrates an atom-based representation of the 3D pharmacophore into the generative process, enabling precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses. It outperforms ligand-based drug design methods and achieves higher docking scores without requiring target protein structures.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules remains a central, time- and cost-heavy challenge in drug discovery, particularly for novel targets lacking structural or functional data. Pharmacophore modeling presents an alternative for capturing the key features required for molecular bioactivity against a biological target.

Method: PharmaDiff is a pharmacophore-conditioned diffusion model that uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.

Result: PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods. Additionally, it achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures.

Conclusion: PharmaDiff offers a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques.

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [211] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 本文提出了一种新的工作流程，通过整合可穿戴设备生理数据和实时环境暴露数据，利用AI模型预测个人对污染的健康反应，并通过迁移学习提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 空气污染对公共健康构成重大威胁，而气候变化带来的极端天气事件进一步加剧了污染水平。近年来，个人传感技术的进步使得行为和生理数据的收集成为可能，为医疗保健带来了新的改进潜力。因此，本文旨在利用这些数据和AI技术来监测和预测个体健康结果。

Method: 本文采用了一种结合可穿戴设备生理数据和实时环境暴露数据的方法，利用AI模型（如对抗自编码器神经网络）进行时间序列预测，以监测和预测个体健康结果。同时应用了迁移学习技术，以提高模型的泛化能力。

Result: AI模型（如对抗自编码器神经网络）能够准确重建时间依赖的健康信号，并捕捉污染的非线性反应。通过使用个人智能手表数据进行迁移学习，提高了AI模型的泛化能力，并展示了该方法在真实世界用户生成数据中的适应性。

Conclusion: 本文提出了一种新的工作流程，通过整合可穿戴健身设备的生理数据和实时环境暴露数据，预测个人对污染的健康反应。AI模型在云基础模块化框架中训练，能够准确重建时间依赖的健康信号，并捕捉污染的非线性反应。转移学习的应用提高了AI模型的泛化能力，展示了该方法在真实世界用户生成数据中的适应性。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [212] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.CR

TL;DR: 本文提出了一种基于sybil的虚拟数据中毒攻击方法，通过生成sybil节点来放大中毒模型的影响，并设计了三种目标模型获取方案，适用于不同场景。在模拟中，该方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到恶意对手的中毒攻击。现有方法通常需要高昂的成本才能实现有效的攻击。

Method: 我们提出了一种基于sybil的虚拟数据中毒攻击，其中恶意客户端生成sybil节点以放大中毒模型的影响。为了降低神经网络计算复杂度，我们开发了一种基于梯度匹配的虚拟数据生成方法。我们还设计了三种针对目标模型获取的方案，适用于在线本地、在线全局和离线场景。

Result: 我们的方法在模拟中优于其他攻击算法，因为我们的方法可以在非独立均匀分布的数据下获得全局目标模型。

Conclusion: 我们的方法在模拟中优于其他攻击算法，因为我们的方法可以在非独立均匀分布的数据下获得全局目标模型。

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>
