<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 122]
- [cs.CV](#cs.CV) [Total: 251]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.LG](#cs.LG) [Total: 283]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [stat.OT](#stat.OT) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.AS](#eess.AS) [Total: 4]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.SD](#cs.SD) [Total: 16]
- [cs.NE](#cs.NE) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [eess.IV](#eess.IV) [Total: 30]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 2]
- [cs.SI](#cs.SI) [Total: 6]
- [quant-ph](#quant-ph) [Total: 12]
- [cs.MA](#cs.MA) [Total: 4]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.CR](#cs.CR) [Total: 42]
- [physics.space-ph](#physics.space-ph) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.NI](#cs.NI) [Total: 2]
- [stat.ME](#stat.ME) [Total: 8]
- [math.CT](#math.CT) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 4]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.FL](#cs.FL) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 2]
- [math.ST](#math.ST) [Total: 4]
- [cs.RO](#cs.RO) [Total: 48]
- [cs.MM](#cs.MM) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cs.SE](#cs.SE) [Total: 14]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [cs.CC](#cs.CC) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 20]
- [math.NA](#math.NA) [Total: 2]
- [econ.GN](#econ.GN) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LO](#cs.LO) [Total: 4]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 12]
- [cs.HC](#cs.HC) [Total: 20]
- [stat.ML](#stat.ML) [Total: 34]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: 研究使用作者验证技术量化AI在学术写作中的协助，通过三个阶段的调查，开发并评估了一种改进的特征向量差异AV方法，以支持学术诚信调查。


<details>
  <summary>Details</summary>
Motivation: 随着人-AI合作在教育环境中日益普遍，理解和衡量这种互动的程度和性质面临重大挑战。研究旨在通过作者验证技术量化AI在学术写作中的协助，以促进透明度、可解释性和学生发展。

Method: 研究分为三个阶段：数据集选择和扩展、AV方法开发和系统评估。使用三个数据集，包括公共数据集（PAN-14）和墨尔本大学学生的两个数据集，扩展数据以包括LLM生成的文本，总计1,889篇文档和540个作者问题。开发了一种改进的特征向量差异AV方法，构建学生学术写作的稳健档案，捕捉其写作的有意义的个体特征。

Result: 结果表明，改进的AV分类器能够识别文体差异，并在单词和句子级别衡量人-AI合作，同时为教育工作者提供透明的工具，以支持学术诚信调查。

Conclusion: 这项工作推进了AV技术，为AI驱动时代学术写作的动态提供了可操作的见解。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [2] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: 研究AI驱动的动态叙事在教育游戏中的潜力，发现动态叙事对学习者动机有积极影响，但需平衡教学目标与叙事动态性。


<details>
  <summary>Details</summary>
Motivation: 动机是成功学习的重要因素，以往研究证明静态互动叙事游戏对动机有积极影响，AI技术的发展使动态和自适应互动叙事越来越可行，但动态叙事对学习者动机的影响研究有限。

Method: 比较两种版本的教育互动叙事游戏《Academical》，一种是传统的手工编写分支剧情（静态叙事），另一种在游戏过程中动态排序剧情（动态叙事）。

Result: 结果强调了响应性内容和多种选择对玩家参与度的重要性，同时也展示了在平衡教学目标与叙事动态性方面的挑战。

Conclusion: 这项工作为AI驱动的动态叙事在教育游戏中的潜力提供了初步的启示。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [3] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: 重新评估了大型语言模型在理解复杂英语语句上的表现，发现人类表现被高估，模型能力被低估。在限制重读的条件下，人类准确率低于某些大型语言模型。研究结果表明，人类和模型在涉及潜在互惠行为的查询上都面临挑战，这表明存在共享的语用敏感性，而非模型特定的缺陷。此外，通过额外分析发现模型性能被系统性低估。这些发现强调了在评估大型语言模型时需要更谨慎的实验设计和编码实践，并挑战了当前模型在语言理解上天生弱于人类的假设。


<details>
  <summary>Details</summary>
Motivation: 重新评估大型语言模型在理解复杂英语语句上的表现，纠正之前研究中可能存在的对人类表现的高估和对模型能力的低估。

Method: 使用相同的刺激材料，进行预注册研究，比较人类在允许重读和限制重读两种条件下的反应，同时对比不同大型语言模型的表现。通过额外分析模型的对数概率、开放性回答的重新编码以及对其他句子的语法性评分，进一步评估模型性能。

Result: 人类在限制重读条件下的准确率（73%）低于Falcon-180B-Chat（76%）和GPT-4（81%），而新的GPT-o1模型达到完美准确率。人类和模型在涉及潜在互惠行为的查询上都面临挑战，表明存在共享的语用敏感性。通过额外分析发现模型性能被系统性低估，GPT-4o可以根据提示框架与新手或专家的语法性判断一致。

Conclusion: 需要更谨慎的实验设计和编码实践来评估大型语言模型，当前模型在语言理解上并不天生弱于人类。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [4] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: 研究探讨了GPT-4对英语信息结构和长距离依赖结构的判断能力，发现其与人类判断相似，表明自然语言和GPT-4生成英语之间存在紧密联系。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型（LM）对自然语言的理解和生成能力，特别是对英语信息结构和长距离依赖（LDD）结构的判断能力。

Method: 通过实验，让GPT-4完成与人类相同的任务，包括信息结构和接受度任务，并通过操纵信息结构来验证因果关系。

Result: GPT-4在信息结构和接受度任务上表现出可靠的元语言技能，复制了人类的交互效应。操纵信息结构可以增加后续LDD结构的接受度。

Conclusion: GPT-4生成的英语与自然英语之间存在紧密联系，信息结构和句法之间也存在紧密关系，需要进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [5] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: This research investigates the use of authorship verification (AV) techniques to quantify AI assistance in academic writing, promoting transparency and student development. The study is structured into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the challenges posed by human-AI collaboration in educational contexts, particularly the need to understand and measure such interactions effectively. The authors aim to promote transparency, interpretability, and student development through the application of AV techniques.

Method: The method involves structuring the investigation into three stages: 1) Dataset selection and expansion - using three datasets including LLM-generated texts; 2) AV method development - adapting a Feature Vector Difference methodology to capture individual writing characteristics; 3) Systematic evaluation - evaluating effectiveness across multiple scenarios including distinguishing between student-authored and LLM-generated texts.

Result: The results show that the enhanced AV classifier can successfully identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels. It also demonstrates resilience against LLMs' attempts to mimic student writing styles.

Conclusion: This work advances AV technology, providing actionable insights into the dynamics of academic writing in an AI-driven era. It offers educators a transparent tool to support academic integrity investigations.

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [6] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: The paper compares two versions of an educational game, one with a static narrative and the other with a dynamic narrative, to explore their impact on learner motivation.


<details>
  <summary>Details</summary>
Motivation: Previous research has shown that static interactive narrative games can positively affect motivation, and advances in AI have made dynamic narratives more accessible. However, there is limited exploration into how dynamic narratives impact learner motivation.

Method: The study compares two versions of Academical, an educational game about research ethics: one version uses a traditional hand-authored branching plot (static narrative), while the other dynamically sequences plots during play (dynamic narrative).

Result: Results show that responsive content and a variety of choices are important for player engagement. It also highlights the challenge of balancing pedagogical goals with the dynamic aspects of the narrative.

Conclusion: This work provides initial insights into the potential of AI-driven dynamic narratives in educational games, discussing design implications from the findings.

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [7] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: Reanalysis of language model comprehension shows that previous studies overestimated human performance and underestimated LLMs, especially in naturalistic conditions without rereading. Models like Falcon-180B-Chat and GPT-4 outperform humans in certain tasks, and newer models achieve perfect accuracy.


<details>
  <summary>Details</summary>
Motivation: To reassess the claim that large language models underperform humans in comprehending minimally complex English statements by revisiting the experimental setup and considering different conditions.

Method: Conducted a preregistered study comparing human responses in two conditions: one allowing rereading (replicating an original study) and another restricting rereading (a more naturalistic test). Evaluated LLMs' performance using the same stimuli and additional analyses such as log probabilities, recoding of open-ended responses, and grammaticality ratings.

Result: Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieved perfect accuracy. Both humans and models struggled with queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities.

Conclusion: The findings challenge the assumption that current models are inherently weaker than humans at language comprehension and highlight the need for more careful experimental design and coding practices in evaluating LLMs.

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [8] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: GPT-4展示了与人类相似的元语言技能，能够理解信息结构和可接受性任务之间的关系，并且研究发现信息结构对长距离依赖构造的可接受性有因果影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LM）是否能够捕捉到英语使用者在信息结构和长距离依赖（LDD）结构之间的关系，这种关系已经在人类中被验证。

Method: 通过让GPT-4完成与人类相同的任务，研究其在信息结构和可接受性判断上的表现；并通过操纵基础句子的信息结构来确认因果关系。

Result: GPT-4在零样本条件下表现出可靠的元语言技能，复制了信息结构和可接受性任务之间的显著交互作用；信息结构的突出程度影响后续LDD构造的可接受性评分。

Conclusion: GPT-4生成的英语与自然英语之间存在紧密联系，信息结构与句法之间也有密切关系，需要进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [9] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）常产生事实性幻觉，为解决此问题，本文提出了一种名为ACPO的自监督偏好调整方法，该方法无需外部监督即可提高事实准确性。实验结果表明，ACPO在LongFact和BioGen数据集上比FactAlign高出1.95分。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解LLMs事实性幻觉的方法通常依赖于更强的模型或外部知识库来评估事实正确性，这在实际应用中可能并不总是可行的。因此，需要一种不依赖外部监督的方法来提高LLMs的事实准确性。

Method: 提出了Atomic Consistency Preference Optimization (ACPO) 方法，这是一种自监督的偏好调整方法。ACPO利用原子一致性信号，即多个随机响应中单个事实的一致性，来识别高质量和低质量的数据对，以进行模型对齐。通过消除对外部模型或知识库的需求，ACPO提供了一种可扩展且高效的改进事实性问答的方法。

Result: 尽管是自监督方法，实证结果表明，ACPO在LongFact和BioGen数据集上比强监督对齐基线FactAlign高出1.95分。

Conclusion: ACPO是一种有效的增强事实可靠性的方法，无需依赖外部模型或知识库。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [10] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在技术交互方面有显著进步，但其输出相似性、多样性和伦理影响仍存疑问。研究通过5000个提示词和12个LLMs生成约3百万文本发现：同模型输出更相似；WizardLM-2-8x22b输出相似度高，GPT-4则更多样；不同模型写作风格差异大；LLM内容有独特语言特征；部分模型性别平衡更好、偏见减少。这些结果为LLM未来开发和伦理评估提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现出色，但对其输出相似性、多样性和伦理影响尚存疑问，因此需要深入研究以了解不同模型的表现及特性。

Method: 使用5000个涵盖多种任务的提示词，从包括OpenAI、Google、Microsoft、Meta和Mistral在内的12个LLMs（包含专有和开源系统）生成约3百万文本进行分析。

Result: 1. 同一LLM的输出比与人类文本更相似；2. WizardLM-2-8x22b输出高度相似，GPT-4输出更多样；3. 不同LLM写作风格差异大，Llama 3和Mistral相似度高，GPT-4独具特色；4. LLM词汇和语调差异体现其独特性；5. 部分LLM性别平衡更好且偏见减少。

Conclusion: 研究结果揭示了LLM输出的行为和多样性，有助于指导未来LLM的开发和伦理评估。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [11] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: This paper presents S-DAT, a scalable multilingual framework for assessing divergent thinking using language models and embeddings, showing robust scoring across 11 languages and providing a fairer evaluation tool.


<details>
  <summary>Details</summary>
Motivation: Traditional creativity assessments are labor-intensive, language-specific, and subjective, limiting scalability and cross-cultural applicability.

Method: S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance as a proxy for divergent thinking, evaluated across 11 diverse languages.

Result: S-DAT demonstrates robust and consistent scoring across linguistic contexts, shows convergent validity with other DT measures, and correct discriminant validity with convergent thinking.

Conclusion: S-DAT addresses limitations of earlier approaches by offering cross-linguistic flexibility for inclusive, global-scale creativity research.

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [12] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

TL;DR: Recent advancements in large language models (LLMs) have shown excellent Chinese text processing capabilities, especially in Chinese Spelling Correction (CSC). However, there are still challenges in reliability and generalization. This paper proposes CEC-Zero, a new reinforcement learning (RL) framework that allows LLMs to self-correct without external supervision. Experiments show that RL-enhanced LLMs achieve high accuracy and good cross-domain generalization, providing a scalable solution for optimizing reliability in Chinese NLP applications.


<details>
  <summary>Details</summary>
Motivation: Although LLMs outperform traditional models in accuracy and robustness in CSC tasks, issues with reliability and generalization remain unresolved.

Method: Propose CEC-Zero, a reinforcement learning framework that enables LLMs to self-correct by learning error strategies autonomously without external supervision.

Result: Experiments demonstrate that RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, reducing reliance on annotated data or auxiliary models.

Conclusion: CEC-Zero offers a scalable solution to optimize reliability in Chinese NLP applications and establishes a new paradigm for self-improving language models.

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [13] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

TL;DR: This paper introduces a new UML modeling tool which integrates class diagrams and object diagrams, executes objects, and provides a stimulating learning experience for students. It stems from an international research project focused on multi-level architecture.


<details>
  <summary>Details</summary>
Motivation: To create an advanced UML modeling tool that goes beyond conventional tools by integrating class and object diagrams, executing objects, and providing a valuable teaching aid.

Method: Design, implement, and utilize a new UML modeling tool emerging from an international research project aimed at a comprehensive multi-level architecture.

Result: The new tool allows integration of class and object diagrams as well as execution of objects, leading to new software architectures and an engaging educational tool.

Conclusion: This project exemplifies how research can yield valuable results as side effects of other work.

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [14] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

TL;DR: Effectively analyzing online review data is essential across industries. This study proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection, demonstrating its effectiveness through various experiments.


<details>
  <summary>Details</summary>
Motivation: Existing studies on analyzing online review data are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets.

Method: The method involves applying automatic labeling to Korean and English review datasets spanning various domains. Aspect category candidates are extracted through clustering, and each review is represented as an aspect-aware embedding vector using negative sampling.

Result: The models achieve high performance in multi-aspect labeling, showing that the automatically generated labels are suitable for training. The framework exhibits superior consistency and scalability compared to publicly available large language models. Human evaluation confirms the quality of the automatic labels.

Conclusion: This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments.

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [15] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: The paper introduces InForage, a reinforcement learning framework that improves LLMs' information retrieval through dynamic and adaptive search behaviors.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static retrieval strategies in complex tasks involving ambiguous, multi-step, or evolving information needs.

Method: Proposes InForage, inspired by Information Foraging Theory (IFT), which formalizes retrieval-augmented reasoning as a dynamic information-seeking process. It rewards intermediate retrieval quality to encourage iterative gathering and integration of information.

Result: InForage outperforms baseline methods in general question answering, multi-hop reasoning tasks, and a new real-time web QA dataset.

Conclusion: InForage is effective in building robust, adaptive, and efficient reasoning agents for LLMs.

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [16] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

TL;DR: This paper observes a novel phenomenon called contextual entrainment in language models, where LMs are distracted by irrelevant contextual information. The authors hypothesize the existence of 'entrainment heads' within the model's attention mechanism and use differentiable masking to identify these heads. Turning off these heads reduces the effect of contextual entrainment.


<details>
  <summary>Details</summary>
Motivation: To understand why and how language models become distracted by irrelevant contextual information, introducing the concept of contextual entrainment.

Method: Observing the logits assigned by LMs to tokens that previously appeared in context prompts, using counterfactual vs factual prompts, and employing differentiable masking to discover 'entrainment heads'.

Result: Statistically significant evidence shows that contextual entrainment is influenced by semantic factors, and turning off identified 'entrainment heads' significantly reduces the effect of contextual entrainment.

Conclusion: The discovery of contextual entrainment and the investigation into LM distraction via entrainment heads marks a key step towards mechanistic analysis and mitigation of the distraction problem.

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [17] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

TL;DR: Qwen3 is the latest version in the Qwen model family, featuring advanced performance, efficiency, and multilingual capabilities. It integrates thinking and non-thinking modes for dynamic responses, introduces a thinking budget mechanism, and expands multilingual support from 29 to 119 languages. Qwen3 achieves state-of-the-art results across various benchmarks and is publicly accessible under Apache 2.0.


<details>
  <summary>Details</summary>
Motivation: To advance the capabilities of large language models in terms of performance, efficiency, and multilingual support while reducing the need for switching between different models for various tasks.

Method: Development of Qwen3 series with dense and MoE architectures, integration of thinking and non-thinking modes, introduction of a thinking budget mechanism, and expansion of multilingual support from 29 to 119 languages.

Result: Qwen3 achieves state-of-the-art results in diverse benchmarks including code generation, mathematical reasoning, and agent tasks, competitive against larger MoE models and proprietary models.

Conclusion: Qwen3 successfully advances the state of the art in large language models, enhancing global accessibility through improved cross-lingual understanding and generation capabilities, and is publicly accessible under Apache 2.0.

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [18] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: QEDACVC is a quantum computing solution for multilingual machine translation, achieving 82% accuracy on the OPUS dataset.


<details>
  <summary>Details</summary>
Motivation: Current cloud-based multilingual translation services use large multilingual language models with classical computing as the backend. There is potential to explore quantum computing for this task.

Method: QEDACVC introduces a quantum encoder-decoder architecture that uses quantum convolution, quantum pooling, quantum variational circuit, and quantum attention mechanisms. It is designed to run on quantum computing hardware.

Result: QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.

Conclusion: QEDACVC demonstrates the potential of quantum computing in multilingual machine translation.

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [19] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: Parameter-efficient fine-tuning (PEFT) methods for adapting large language models have shown promise, but existing approaches exhibit counter-intuitive phenomena. This paper proposes PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. It achieves state-of-the-art performance in both question answering and mathematical problem solving tasks while using 25% fewer parameters than LoRA.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods show counter-intuitive phenomena: integrating router into prompt tuning increases training efficiency but does not universally improve performance; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of prompt tuning, the authors propose PT-MoE.

Method: PT-MoE integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. Matrix decomposition enables efficient parameter sharing across experts, while MoE provides dynamic adaptation.

Result: PT-MoE achieves state-of-the-art performance in both question answering and mathematical problem solving tasks. It improves F1 score by 1.49 points over prompt tuning and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over prompt tuning and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA.

Conclusion: The integration of matrix decomposition and MoE in PT-MoE yields complementary benefits, enabling cross-task consistency and generalization abilities. These findings provide insights for future PEFT methods.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [20] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

TL;DR: WorldView-Bench is a new benchmark designed to evaluate Global Cultural Inclusivity in LLMs. It uses free-form generative evaluation and two intervention strategies to significantly increase cultural inclusivity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarking frameworks fail to adequately capture the bias in LLMs towards Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality.

Method: The method involves introducing WorldView-Bench, a benchmark for evaluating Global Cultural Inclusivity in LLMs, using free-form generative evaluation instead of rigid, closed-form assessments. Two intervention strategies are implemented: Contextually-Implemented Multiplex LLMs and Multi-Agent System (MAS)-Implemented Multiplex LLMs.

Result: Results show a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance.

Conclusion: These findings indicate the potential of multiplex-aware AI evaluation in reducing cultural bias in LLMs, promoting more inclusive and ethically aligned AI systems.

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [21] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: The paper explores an alternative method for language modeling by focusing on context embedding, using Graph Convolution in GNNs with LSTMs to predict the next word with limited resources.


<details>
  <summary>Details</summary>
Motivation: To address the high resource demands of current state-of-the-art language models, the study focuses on a sub-task of language modeling - context embedding.

Method: The approach uses Graph Convolution operation in GNNs to encode context and combines it with LSTMs to predict the next word based on preceding words' local context.

Result: The method was tested on a custom Wikipedia text corpus with limited resources and showed promising results in predicting the next word.

Conclusion: This novel approach provides a feasible alternative for language modeling tasks with significantly fewer resources.

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [22] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Xuanzhao Dong,Hao Wang,Haiyu Wu,Huayu Li,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi*

Main category: cs.CL

TL;DR: An abstract about a new method called Diversity-aware Reward Adjustment (DRA) which incorporates semantic diversity into reward computation for reinforcement learning in language model post-training. It outperforms recent baselines achieving state-of-the-art performance with an average accuracy of 58.2%.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of Group Relative Policy Optimization (GRPO) that fails to capture semantic diversity among sampled completions leading to a diversity-quality inconsistency.

Method: The proposed method, Diversity-aware Reward Adjustment (DRA), uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning while maintaining stable exploitation of high-quality samples.

Result: Evaluated on five mathematical reasoning benchmarks, DRA outperforms recent strong baselines achieving state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55.

Conclusion: DRA integrates seamlessly with GRPO and its variant DR.~GRPO, resulting in DRA-GRPO and DGA-DR.~GRPO, improving performance in low-resource settings.

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [23] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger,Francesco Salvi,Jiacheng Liu,Xiaoli Nan,Ramit Debnath,Barbara Fasolo,Evelina Leivada,Gabriel Recchia,Fritz Günther,Ali Zarifhonarvar,Joe Kwon,Zahoor Ul Islam,Marco Dehnert,Daryl Y. H. Lee,Madeline G. Reinecke,David G. Kamper,Mert Kobaş,Adam Sandford,Jonas Kgomo,Luke Hewitt,Shreya Kapoor,Kerem Oktar,Eyup Engin Kucuk,Bo Feng,Cameron R. Jones,Izzy Gainsburg,Sebastian Olschewski,Nora Heinzelmann,Francisco Cruz,Ben M. Tappin,Tao Ma,Peter S. Park,Rayan Onyonka,Arthur Hjorth,Peter Slattery,Qingcheng Zeng,Lennart Finke,Igor Grossmann,Alessandro Salatiello,Ezra Karger*

Main category: cs.CL

TL;DR: In a large-scale experiment, an advanced LLM (Claude Sonnet 3.5) outperformed incentivized human persuaders in persuading participants towards correct or incorrect answers in a real-time conversational quiz setting.


<details>
  <summary>Details</summary>
Motivation: To directly compare the persuasion capabilities of a frontier large language model against incentivized human persuaders and evaluate its impact on participants' accuracy and earnings in a quiz setting.

Method: A preregistered, large-scale incentivized experiment where participants completed an online quiz while being persuaded by either humans or the LLM towards correct or incorrect answers.

Result: The LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than humans, improving participants' accuracy and earnings when guiding towards correct answers and decreasing them when steering towards incorrect answers.

Conclusion: AI's persuasion capabilities already surpass those of humans with real-money bonuses tied to performance, highlighting the need for emerging alignment and governance frameworks.

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [24] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance.


<details>
  <summary>Details</summary>
Motivation: Existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains.

Method: To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them.

Result: We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts.

Conclusion: Our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [25] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu,Lechen Zhang,Sheza Munir,Yiyang Gu,Lu Wang*

Main category: cs.CL

TL;DR: VeriFact is a new framework for factuality evaluation that enhances fact extraction and verification by identifying incomplete or missing facts, while FactRBench is a benchmark to evaluate both precision and recall in long-form model responses.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating the factuality of LLMs often fail to capture essential context and miss key relational facts due to their decompose-decontextualize-verify pipeline.

Method: Introduced VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts. Also introduced FactRBench, a benchmark that evaluates both precision and recall in long-form model responses.

Result: Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information. Benchmarking various LLMs on FactRBench indicate larger models within same model family improve precision and recall, but high precision does not always correlate with high recall.

Conclusion: VeriFact improves factuality evaluation by enhancing fact completeness and preserving complex facts. FactRBench provides a comprehensive way to assess both precision and recall in long-form model responses.

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [26] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: This paper presents a tutorial for developing, testing and applying taxonomies to analyze unstructured data using LLMs, demonstrating the process with personal goals as an example.


<details>
  <summary>Details</summary>
Motivation: To provide a methodological guide for efficiently analyzing unstructured text data while reducing bias and maintaining quality.

Method: An iterative and collaborative process involving researchers and LLMs to develop, test, and apply taxonomies for text analysis, including writing prompts, evaluating and refining taxonomies, assessing intercoder agreements, and categorizing datasets.

Result: Demonstrated a successful application of LLMs in generating a taxonomy of life domains from personal goals, achieving high intercoder reliability.

Conclusion: LLMs offer promising possibilities for text analysis with predefined or data-driven taxonomies, though there are limitations that need to be considered.

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [27] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: Pretrained language models (LLMs) suffer from fixed tokenization schemes which limit their efficiency and performance, especially in multilingual or specialized scenarios. To address this issue, the authors propose TokenAdapt, a model-agnostic tokenizer transplantation method, and novel pre-tokenization learning for multi-word Supertokens. TokenAdapt uses a hybrid heuristic to initialize new unique token embeddings by combining subword decomposition with semantic similarity estimates. This approach preserves semantics while reducing retraining needs. Empirical results show that TokenAdapt outperforms existing methods like Retok and Transtokenizer, achieving significant improvements in zero-shot perplexity scores.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the limitations imposed by fixed tokenization schemes in pretrained language models. Current methods to adapt tokenizers are either computationally expensive or fail to fully preserve semantic nuances and address compression inefficiencies.

Method: The paper introduces two innovations: 1) TokenAdapt - a model-agnostic tokenizer transplantation method that initializes new token embeddings using a hybrid heuristic combining subword decomposition and semantic similarity; 2) Pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation.

Result: Empirical investigations demonstrate that TokenAdapt successfully initializes unique tokens, outperforming conventional baselines and sophisticated methods such as Transtokenizer and ReTok. The Supertokens achieve notable compression gains. Additionally, TokenAdapt shows consistent lower perplexity ratios compared to ReTok and Transtokenizer across different base models and newly trained target tokenizers, with at least a 2-fold improvement in aggregate scores.

Conclusion: TokenAdapt offers an effective solution to the tokenizer lock-in problem in pretrained language models by providing a model-agnostic tokenizer transplantation method that preserves semantics and reduces retraining requirements. It outperforms existing methods in terms of zero-shot perplexity and provides significant improvements in compression and performance.

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [28] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: The paper explores the application of NLP techniques for automating data extraction from EHRs related to lung and breast cancer, using GMV's uQuery tool and fine-tuning a RoBERTa-based model. It reports strong performance in identifying certain clinical entities.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of information from clinical reports is time-consuming and error-prone, limiting the efficiency of data-driven approaches in healthcare.

Method: Utilized GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model (RoBERTa-based) for Named Entity Recognition tasks on a dataset of annotated breast and lung cancer reports.

Result: Demonstrated strong overall performance in entity recognition, especially for common entities like MET and PAT, with challenges remaining for less frequent entities like EVOL.

Conclusion: NLP techniques can significantly enhance the accuracy and efficiency of data extraction from EHRs in oncology, contributing to better patient outcomes.

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [29] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 尽管线性探针在检测短对话中的谎言方面表现出良好的泛化能力，但在处理更长格式的对话时效果较差。本文提出了一种通过在每段对话末尾添加固定关键短语来显著改善这种泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）中普遍存在的真理方向，探讨线性探针是否能够在线性激活空间内区分真实和虚假陈述，并进一步探索其在不同对话形式中的泛化能力。

Method: 分析线性探针在不同对话格式（尤其是包含谎言的对话）中的表现；提出在每段对话的末尾添加固定关键短语以改善泛化性能的方法。

Result: 发现线性探针在短对话中具有良好泛化能力，但在较长对话中表现不佳；提出的方法显著提高了对长对话的泛化能力。

Conclusion: 虽然在短对话中可以实现较好的谎言检测，但要构建可靠的、能适应新环境的LLM谎言检测器仍面临挑战。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [30] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui,Juan Diego Rodriguez,Philippe Laban,Dean Murphy,Joseph P. Dexter,Richard Jean So,Samuel Baker,Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 尽管密切阅读是大学课程的重要组成部分，但尚未在大规模语言模型（LLM）上进行评估。为填补这一空白，本文提出了KRISTEVA，这是第一个包含1331个多项选择题的密切阅读基准测试。通过三个逐步复杂的任务集来评估LLM对文学作品的理解和推理能力，结果显示最先进的LLM虽然具备一定程度的大学水平密切阅读能力（准确率49.7%-69.7%），但在11项任务中的10项上仍不及有经验的人类评估者。


<details>
  <summary>Details</summary>
Motivation: 密切阅读作为培养批判性思维的基础，在大学课程中被广泛采用，然而尚未针对大型语言模型（LLMs）进行评估，且多学科基准如MMLU未涵盖文学领域。因此需要一个专门的基准来评估LLMs在文学文本分析方面的能力。

Method: 提出KRISTEVA——一个由1331道课堂数据改编的多项选择题组成的基准测试，并设计了三个逐步复杂的任务集：1) 提取风格特征；2) 从参数化知识中检索相关上下文信息；3) 在风格与外部上下文之间进行多跳推理。以此来测试LLMs在密切阅读过程不同元素上的表现。

Result: 实验结果表明，最先进的LLMs在密切阅读任务上表现出一定的大学水平能力，准确率范围为49.7%-69.7%，但在11项任务中的10项上，其表现仍逊色于有经验的人类评估者。

Conclusion: 当前最先进的LLMs在文学作品的理解和推理方面已展现出一定能力，但仍显著落后于人类专家的表现，特别是在涉及复杂推理的任务上。这表明在提升LLMs的密切阅读能力方面还有很大改进空间。

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [31] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）在仅使用预训练权重的情况下预测暴力冲突升级和伤亡的能力，并评估了结合外部数据（如冲突数据集和新闻报告）对模型性能的提升。研究通过参数化和非参数化两种设置，在非洲之角和中东地区2020-2024年的冲突预测中进行实验，发现外部知识能有效增强LLMs的冲突预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言任务上表现出色，但其预测暴力冲突的能力尚未得到充分探索。这对于早期预警系统、人道主义规划和政策制定至关重要。

Method: 研究比较了LLMs在参数化（仅依靠预训练权重）和非参数化（结合冲突数据集和新闻报告等外部信息）两种设置下的冲突预测能力。采用两部分评估框架，覆盖2020-2024年非洲之角和中东地区的冲突多发区域。

Result: 在参数化设置下，LLMs能够仅依靠预训练知识预测冲突趋势和伤亡；在非参数化设置下，结合外部信息显著提升了模型性能。

Conclusion: LLMs具备一定的冲突预测能力，但结合结构化的外部知识可以进一步提高其预测效果。这为冲突预测领域提供了新的思路和方法。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [32] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila,Esteban Villa Turek,Ellen Karina Chumbe Fernandez,Luis Felipe Polo Galvez,Luis Cadavid,Andrea Marroquin,Rebeca Vargas Quesada,Johanna Crew,Nicole Vallejo Galarraga,Christopher Rodriguez,Diego Gutierrez,Radhi Datla*

Main category: cs.CL

TL;DR: This paper explores the differences in written Spanish across Latin America and Spain, emphasizing the need for locale-sensitive AI models to bridge sociolinguistic gaps and improve inclusivity.


<details>
  <summary>Details</summary>
Motivation: To highlight the critical need for regional localized models by examining the primary differences between variants of written Spanish across Latin America and Spain.

Method: An in-depth sociocultural and linguistic contextualization of the differences in written Spanish among dialectal groups.

Result: Locale-sensitive AI models can significantly bridge sociolinguistic divides and inform better localization strategies that meet inclusivity goals and secure sustainable user growth.

Conclusion: Implementing at least five sub variants of Spanish addresses key actions such as fostering user trust and reliance on AI language models while demonstrating cultural, historical, and sociolinguistic awareness.

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [33] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang,Yubing Ren,Yanan Cao,Binxing Fang*

Main category: cs.CL

TL;DR: The paper introduces a symbiotic watermarking framework with serial, parallel, and hybrid strategies for Large Language Models (LLMs), which optimizes the balance between detectability, robustness, text quality, and security.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs among robustness, text quality, and security in current watermarking schemes for LLMs.

Method: Proposes a versatile symbiotic watermarking framework with three strategies - serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy.

Result: Experimental results indicate that the method outperforms existing baselines and achieves state-of-the-art (SOTA) performance.

Conclusion: This framework provides novel insights into diverse watermarking paradigms for LLMs.

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [34] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu,Hanzhang Zhou,Zijian Feng,Tianjiao Li,Chua Jia Jim Deryl,Mak Lee Onn,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: The paper introduces MePO, a lightweight prompt optimizer that enhances the quality of prompts for both large-scale and lightweight language models without relying on online optimization, reducing costs and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To address the issue where optimized prompts from advanced large language models (LLMs) can overwhelm lightweight inference models and degrade response quality.

Method: MePO is trained on a preference dataset built from merit-aligned prompts generated by a lightweight LLM. It focuses on model-agnostic prompt quality merits to improve prompt and response quality.

Result: Experiments show that MePO achieves better results across various tasks and model types, providing a scalable and robust solution for real-world deployment.

Conclusion: MePO offers an effective, interpretable, and locally deployable solution for prompt optimization, enhancing performance across diverse tasks and model sizes.

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [35] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 通过使用知识图谱增强检索生成（RAG）的方法，解决大语言模型（LLM）在生成个性化回应时出现的幻觉问题，相较于仅用文本输入个人数据的基础LLM，该方法能更好地理解个人信息并生成精确回应，但响应时间略有减少。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已经被应用于多种场景，但由于训练数据庞大，常常发生过度拟合，导致生成额外和不正确的数据，从而在输出中产生幻觉。其中一个根本原因是缺乏及时、准确和个性化的信息输入到LLM中。

Method: 提出了一种利用知识图谱（KGs）进行检索增强生成（RAG）的方法，以帮助LLM生成个性化的回应。此方法中的知识图谱可以储存持续更新的事实信息，并且作者将重点放在日历数据上。

Result: 实验结果表明，与使用个人数据作为文本输入的基础LLMs相比，该方法能够显著更好地理解和生成准确的回应，不过响应时间有适度的减少。

Conclusion: 引入了知识图谱辅助的检索增强生成方法，有助于改善LLM在个性化回应生成中的表现，同时提高了对个人信息的理解能力。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [36] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin,Fan Huang*

Main category: cs.CL

TL;DR: 研究人员开发了一种名为DIF（人口统计隐式公平）的方法，用于衡量大型语言模型（LLMs）中的隐式偏差。通过使用带有社会人口特征的角色评估现有的逻辑和数学问题数据集，他们发现回答准确性与隐式偏差之间存在反比趋势。这表明LLMs的技术局限性，即无法很好地处理额外信息。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在近年来变得越来越重要，但人们对其从训练数据中继承的潜在偏见表示担忧。先前的研究已经探讨了LLMs如何表现出隐式偏见，例如当引入不同的社会背景时，响应生成会发生变化。本文作者认为这种隐式偏见不仅是伦理问题，也是技术问题，因为这揭示了LLMs无法适应额外信息的能力。然而，与其他LLM智能度量标准不同，目前尚无标准方法来衡量这一特定子集的LLM偏见。

Method: 为了填补这一空白，研究人员开发了一种可以计算出易于解释的基准的方法——DIF（Demographic Implicit Fairness），通过使用社会人口特征角色来评估现有的LLM逻辑和数学问题数据集。

Result: 研究结果表明，该方法可以在统计上验证LLM行为中隐式偏见的存在，并且发现了问题回答准确性和隐式偏见之间的反向趋势。

Conclusion: 这一发现支持了作者关于隐式偏见是技术问题的观点，并揭示了LLMs在处理额外信息方面的不足。

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [37] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng,Jinhao Jiang,Zican Dong,Wayne Xin Zhao,Lei Fang*

Main category: cs.CL

TL;DR: The paper introduces CAFE, a two-stage coarse-to-fine method for enhancing multi-document question-answering by filtering and steering relevant documents, leading to significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing retrieval precision and recall in long-context inputs for LLMs, affecting their ability to answer questions effectively.

Method: CAFE uses a coarse-grained filtering method to identify and rank relevant documents followed by a fine-grained steering method to guide attention to the most relevant content.

Result: Experiments demonstrate that CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model respectively.

Conclusion: CAFE successfully mitigates issues related to background and distracting documents, improving reliance on evidence documents and enhancing multi-document question-answering capabilities.

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [38] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: The paper explores the vulnerability of Large Language Models (LLMs) to jailbreak attacks, revealing a universal attack that compromises multiple models and enables harmful outputs. Despite disclosure, industry responses have been inadequate, raising concerns about AI safety practices as LLMs proliferate.


<details>
  <summary>Details</summary>
Motivation: To highlight the growing threat posed by dark LLMs—models either deliberately designed without ethical guardrails or modified through jailbreak techniques—and to address the inadequacies in current industry practices regarding AI safety.

Method: Identification of a universal jailbreak attack that effectively compromises state-of-the-art LLMs, enabling them to produce harmful outputs upon request. Responsible disclosure of this attack to major LLM providers followed by analysis of their responses.

Result: Discovery of a universal jailbreak attack that many tested LLMs were still vulnerable to, indicating significant gaps in current AI safety measures. Inadequate responses from major LLM providers further emphasize these gaps.

Conclusion: As model training becomes more accessible and cheaper, the risk of misuse escalates. Decisive intervention is needed to prevent LLMs from democratizing access to dangerous knowledge.

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [39] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah,Francois Meyer*

Main category: cs.CL

TL;DR: Pretrained language models (PLMs) for African languages encode more linguistic information when adapted specifically for African languages, compared to massively multilingual PLMs. Token-level syntactic info is in middle-to-last layers and sentence-level semantic info across all layers.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate how pretrained language models capture linguistic knowledge specific to African languages.

Method: Trained layer-wise probes on six typologically diverse African languages using the MasakhaPOS dataset with control tasks designed to interpret probe performance.

Result: Adapted PLMs for African languages encode more linguistic information about target languages than massively multilingual PLMs. Token-level syntactic information is concentrated in middle-to-last layers while sentence-level semantic information is distributed across all layers.

Conclusion: The study confirms that probe performance reflects internal knowledge of PLMs rather than memorization, applying established interpretability techniques to African-language PLMs and highlighting mechanisms behind successful strategies like active learning and multilingual adaptation.

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [40] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu,Sony Trenous,Leonardo F. R. Ribeiro,Bill Byrne,Felix Hieber*

Main category: cs.CL

TL;DR: A new benchmark named XRAG is proposed to evaluate LLMs' generation abilities in cross-lingual RAG settings. It highlights two challenges: response language correctness in monolingual retrieval and reasoning over retrieved information across languages in multilingual retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the evaluation of LLMs' generation abilities in scenarios where user language doesn't match the retrieval results, especially focusing on cross-lingual complexity.

Method: XRAG is constructed from recent news articles ensuring questions require external knowledge. It covers both monolingual and multilingual retrieval scenarios with relevancy annotations.

Result: Experimental results on five LLMs reveal two main challenges: 1) response language correctness in monolingual retrieval; 2) reasoning over retrieved information across languages in multilingual retrieval.

Conclusion: XRAG serves as a valuable benchmark for studying LLM reasoning abilities, highlighting significant gaps between human and LLM performance in cross-lingual RAG settings.

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [41] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan,Di Wu,Yibin Lei,Christof Monz,Iacer Calixto*

Main category: cs.CL

TL;DR: The paper introduces S-MedQA, an English medical QA dataset for evaluating large language models in clinical specialties. It challenges the hypothesis that training on specialty data leads to best performance and suggests domain shifting is more impactful than knowledge injection.


<details>
  <summary>Details</summary>
Motivation: To benchmark large language models' performance in fine-grained clinical specialties and evaluate the hypothesis related to knowledge injection in medical QA.

Method: Introduced S-MedQA dataset and used it to test model performance across different clinical specialties after fine-tuning.

Result: 1) Training on specialty data does not necessarily lead to the best performance on that specialty; 2) Token probabilities of clinically relevant terms increase consistently regardless of the specialty fine-tuned on.

Conclusion: Improvement gains come mostly from domain shifting rather than knowledge injection, suggesting a rethinking of the role of fine-tuning data in the medical domain.

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [42] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da,Parth Mitesh Shah,Kuan-Ru Liou,Jiaxing Zhang,Hua Wei*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are essential in human decision-making, but their outputs can be unreliable. This paper introduces GE-Chat, a framework that uses knowledge graphs and retrieval-augmented generation to provide evidence-based responses, improving the reliability of LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of untrustworthy responses from LLMs that may contain mistakes or plausible yet incorrect information, leading to complications and trust issues among users.

Method: GE-Chat utilizes a knowledge graph enhanced retrieval-augmented generation framework. When a user uploads a document, a knowledge graph is created to support a retrieval-augmented agent. The method employs Chain-of-Thought logic generation, n-hop sub-graph searching, and entailment-based sentence generation for accurate evidence retrieval.

Result: The proposed method enhances the ability to identify exact evidence in a free-form context, providing a reliable way to examine the resources behind LLM conclusions and aiding in assessing their trustworthiness.

Conclusion: GE-Chat offers an effective solution to improve the reliability of LLMs by generating evidence-based responses, thereby helping users make more informed decisions.

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [43] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: Reasoning CPT uses synthetic data to reconstruct hidden thought processes, improving model performance across domains and problem difficulties.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of synthesizing training data for reasoning and its impact on a wide range of domains, addressing the limitations of task-specific signals in supervised fine-tuning and reinforcement learning.

Method: Apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark.

Result: Reasoning CPT consistently improves performance across all evaluated domains, with notable gains in challenging problems and effective transfer of reasoning skills between domains.

Conclusion: Reasoning CPT is an effective method for enhancing reasoning capabilities in language models, allowing them to adjust reasoning depth according to problem difficulty.

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [44] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: The paper introduces CoT Encyclopedia, a framework that automatically extracts and clusters reasoning criteria from model-generated CoTs to produce more interpretable analyses. It also shows performance gains by guiding models toward better strategies.


<details>
  <summary>Details</summary>
Motivation: Current understanding of reasoning strategies in large language models is limited, and predefined categorization methods are constrained by human intuition, failing to capture the full diversity of model behaviors.

Method: The method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior.

Result: Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods, can predict which strategy a model is likely to use, and guide it toward more effective alternatives.

Conclusion: The CoT Encyclopedia provides practical insights, such as the significant impact of training data format on reasoning behavior compared to data domain.

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [45] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao,Hongyi Huang,Jiayi Wu,YiMing Cheng,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: VQ-Logits is a new method using Vector Quantization to reduce parameters and computation in LLM output layers, achieving significant speedup with minor performance loss.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have large output vocabularies leading to high computational and memory costs, especially in the final linear projection layer. Current solutions like adaptive softmax add structural complexities.

Method: The VQ-Logits approach replaces the large output embedding matrix with a small codebook of K vectors. Tokens are mapped to these vectors, and logits over this compact space are 'scattered' back to full vocabulary space.

Result: Experiments on benchmarks like WikiText-103 and C4 show up to 99% parameter reduction and 6x speedup in logit computation, with only a 4% increase in perplexity compared to full softmax baselines.

Conclusion: VQ-Logits effectively reduces parameters and computation in LLM output layers while maintaining strong performance, demonstrated through extensive experiments and ablation studies.

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [46] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang,Kaili Sun,Bowen Wu,Qun Yu,Ying Li,Baoxun Wang*

Main category: cs.CL

TL;DR: The paper proposes RAIDEN-R1, a reinforcement learning framework with Verifiable Role-Awareness Reward (VRAR) to improve role consistency in role-playing conversational agents. It uses singular and multi-term mining strategies for rewards and develops a Chain-of-Thought dataset via multi-LLM collaboration. The 14B-GRPO model shows superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Role-playing conversational agents often struggle with maintaining role consistency, which this research aims to address.

Method: RAIDEN-R1 framework integrates VRAR using singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. A high-quality Chain-of-Thought dataset is constructed through multi-LLM collaboration.

Result: The 14B-GRPO model achieved 88.04% accuracy on Script-Based Knowledge and 88.65% on Conversation Memory metrics, outperforming baseline models while maintaining robustness.

Conclusion: RAIDEN-R1 bridges the gap of non-quantifiability in RPCA training and offers insights into role-aware reasoning patterns, thus advancing the development of RPCAs.

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [47] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 在多语言背景下，评估多个最先进大语言模型在零样本和少样本情况下对复杂文本数据集（包括俄语和乌克兰语社交媒体帖子）进行标注的能力，特别是识别涉及人权侵犯的二分类任务。通过与人工双重标注的黄金标准比较，研究探讨了不同提示条件下的模型表现、错误模式及跨语言适应性，为LLM在敏感领域应用提供了可靠性见解。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理系统的日益复杂，大型语言模型在多种任务中展现出巨大潜力，本研究旨在探索这些模型在多语言环境下处理敏感、特定领域任务的能力，特别是在零样本和少样本学习场景下标注涉及人权侵犯的社交媒体帖子。

Method: 研究选取了GPT-3.5、GPT-4、LLAMA3、Mistral 7B和Claude-2等最先进的大语言模型，通过提供英语和俄语两种语言的提示，对包含俄语和乌克兰语的社交媒体帖子数据集进行二分类任务（识别涉及人权侵犯的参考）。模型的表现通过与1000个样本的人工双重标注黄金标准进行比较来评估，并分析了不同提示条件下的性能和错误模式。

Result: 各模型在不同提示条件下的表现各异，展现了各自的优势、局限性和跨语言适应能力。通过对模型输出与人工标注的对比，揭示了模型在处理主观性强且依赖上下文判断的任务中的独特错误和分歧模式。

Conclusion: 大型语言模型在多语言背景下的敏感领域任务中表现出一定的可靠性和适用性，但其在处理主观性和上下文依赖性判断时仍存在挑战。这为未来在实际场景中部署语言模型提供了重要考虑因素。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [48] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: This paper analyzed 19,123 studies and found that generative LLMs have advantages in open-ended tasks, while traditional NLP is better for information extraction and analysis tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the differences between traditional NLP and generative LLMs across different medical tasks.

Method: Analyzed 19,123 studies related to the application of NLP and LLMs in medicine.

Result: Generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks.

Conclusion: As these technologies advance, ethical use of them is essential to ensure their potential in medical applications.

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [49] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: Quicker is an evidence-based clinical decision support system powered by LLMs designed to automate evidence synthesis and generate clinical recommendations. It has strong performance in question decomposition, retrieval sensitivities, literature screening, and evidence assessment. Collaboration with Quicker reduces the time required for recommendation development.


<details>
  <summary>Details</summary>
Motivation: Integrating clinical evidence into real-time practice is challenging due to workload, complex processes, and time constraints. There is a need for tools that can automate evidence synthesis for more efficient decision making.

Method: Quicker implements a fully automated chain covering all phases from questions to clinical recommendations. It uses large language models and includes integrated tools and interactive user interfaces. A benchmark dataset Q2CRBench-3 was developed based on clinical guideline development records for evaluation.

Result: Experimental results showed Quicker's strong performance in fine-grained question decomposition, retrieval sensitivities comparable to human experts, literature screening approaching comprehensive inclusion of relevant studies, effective evidence assessment support, and more comprehensive and coherent recommendations than clinicians.

Conclusion: The findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [50] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: A new reinforcement learning method named J1 is introduced to train LLM-as-a-Judge models, which converts various prompts to judgment tasks with verifiable rewards. This approach outperforms other existing models in certain sizes and provides detailed analysis on different training aspects.


<details>
  <summary>Details</summary>
Motivation: The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models are crucial solutions.

Method: J1 is a reinforcement learning approach that converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards, incentivizing thinking and mitigating judgment bias.

Result: J1 outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. It also surpasses o1-mini and even R1 on some benchmarks despite training a smaller model.

Conclusion: The study provides analysis and ablations comparing different training recipes, reward strategies, and variations in thought length and content, concluding that the models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [51] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang,Zhanyu Shen,Hui Huang*

Main category: cs.CL

TL;DR: An abstract about a new method called LDIR for creating low-dimensional dense and interpretable text embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods either lack interpretability or perform poorly. Recent work on interpretable embeddings using large language models results in high-dimensional embeddings.

Method: LDIR creates low-dimensional (under 500) dense and interpretable text embeddings. It uses numerical values indicating semantic relatedness to anchor texts via farthest point sampling.

Result: LDIR performs similarly to black-box baseline models and surpasses interpretable embedding baselines, despite having significantly fewer dimensions.

Conclusion: LDIR provides a novel approach for generating interpretable text embeddings with strong performance and reduced dimensionality.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [52] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye,Shaonan Wang*

Main category: cs.CL

TL;DR: 提出了一种统一且灵活的框架，利用视觉-语言模型（VLMs）和特定模态专家，从由视觉、听觉和文本输入引发的大脑记录中重建连贯的语言。该方法性能与现有最佳系统相当，同时更具适应性和可扩展性，推动了更生态有效和可泛化的思维解码研究。


<details>
  <summary>Details</summary>
Motivation: 以往的研究在从fMRI数据重建语言方面取得了一定进展，但通常局限于单一模态输入（如图像或音频），而人类思维本质上是多模态的，因此需要一种能够处理多模态输入的方法来更真实地反映人类认知过程。

Method: 通过构建一个统一且灵活的框架，结合视觉-语言模型（VLMs）和特定模态专家，跨多种模态（视觉、听觉和文本）解释信息，从而实现从大脑活动记录中重建连贯语言的目标。

Result: 实验表明，该方法在性能上可与现有最佳系统媲美，并且具有更高的适应性和可扩展性。

Conclusion: 这项工作为更加生态有效和可泛化的思维解码技术奠定了基础，进一步推动了人类认知研究和脑机交互应用的发展。

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [53] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: This paper explores the design of an aspect-based sentiment analysis system using LLMs for real-world use, focusing on quadruple opinion extraction and demonstrating that a combined multi-domain model achieves performance comparable to specialized single-domain models.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of a single fine-tuned model in handling multiple domain-specific taxonomies simultaneously for aspect-based sentiment analysis.

Method: Designing an aspect-based sentiment analysis system using large language models with focus on quadruple opinion extraction including aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages.

Result: A combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity.

Conclusion: Lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks are shared.

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [54] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 随着神经语言模型的发展，代码生成性能得到了显著提升，但重复问题依然存在。本文正式定义了结构重复问题，并提出了基于语法的重复惩罚解码方法RPG，通过识别和减少重复模式来改善代码生成质量。实验表明，RPG在多个数据集上有效减少了重复并提升了生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 尽管神经语言模型在代码生成方面取得了显著进展，但在生成过程中仍存在重复问题。以往研究主要关注内容重复，而更普遍且具有挑战性的是结构重复，即以固定结构出现在不同模式中的重复代码。

Method: 提出了一种名为RPG（Repetition Penalization based on Grammar）的解码方法，首先利用语法规则识别代码生成中的重复问题，然后通过降低导致重复的关键标记的概率来缓解重复现象。同时构建了一个新数据集CodeRepetEval以全面评估解决重复问题的方法。

Result: 大量实验结果表明，RPG在CodeRepetEval数据集以及HumanEval和MBPP基准测试中显著优于最佳基线方法，有效减少了重复现象并提高了生成代码的质量。

Conclusion: RPG方法能够有效缓解代码生成中的重复问题，提高生成代码的质量，为未来的研究提供了新的方向和评估工具。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [55] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo,Jae Ho Sohn,Gondy Leroy,Trevor Cohen*

Main category: cs.CL

TL;DR: This paper evaluates large language models (LLMs) in generating plain language summaries (PLSs) of health information using subjective and objective measures. Results show LLM-generated PLSs match human-written ones in subjective evaluations but lag behind in improving reader comprehension. Automated metrics poorly align with human judgment.


<details>
  <summary>Details</summary>
Motivation: To address the unclear effectiveness of LLMs in supporting health information comprehension and the limitations of prior evaluations that either rely on automated scores or subjective Likert-scale ratings with limited generalizability.

Method: Conduct a large-scale crowdsourced evaluation using Amazon Mechanical Turk with 150 participants. Assess PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, faithfulness, and objective multiple-choice comprehension and recall measures. Examine the alignment between 10 automated evaluation metrics and human judgments.

Result: LLM-generated PLSs appear indistinguishable from human-written ones in subjective evaluations but lead to significantly worse comprehension outcomes. Automated evaluation metrics fail to reflect human judgment accurately.

Conclusion: There is a need for evaluation frameworks that go beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension.

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [56] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yongkang Wu,Zhonghua Li,Qi Ye,Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner is an efficient plug-and-play refiner designed for long-context input scenarios in RAG applications, which reduces inference costs and enhances performance.


<details>
  <summary>Details</summary>
Motivation: Real-world RAG applications face challenges with long-context inputs due to redundant information and noise, leading to higher inference costs and reduced performance.

Method: LongRefiner uses dual-level query analysis, hierarchical document structuring, and adaptive refinement via multi-task learning on a single foundation model.

Result: Experiments on seven QA datasets show that LongRefiner performs competitively while using 10x fewer computational resources and latency compared to the best baseline.

Conclusion: LongRefiner is scalable, efficient, and effective, offering valuable insights for practical long-text RAG applications.

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [57] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang,Zhiyang Chen,Zijun Wang,Tiancheng Li,Guo-Jun Qi*

Main category: cs.CL

TL;DR: The paper introduces DCoLT, a reasoning framework for diffusion language models which optimizes the entire reasoning trajectory using Reinforcement Learning. It differs from traditional methods by allowing bidirectional, non-linear reasoning. The framework is implemented on two models - SEDD and LLaDA. Experiments show that DCoLT-reinforced models outperform others.


<details>
  <summary>Details</summary>
Motivation: To create a more effective reasoning framework for diffusion language models that can optimize the entire reasoning process and not just individual steps.

Method: DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and uses outcome-based Reinforcement Learning to optimize the entire reasoning trajectory. It is applied to two diffusion language models: SEDD and LLaDA.

Result: Experiments on math and code generation tasks show that DCoLT-reinforced Diffusion Language Models outperform other models trained by SFT or RL. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy significantly on various benchmarks.

Conclusion: DCoLT provides a novel approach to reasoning in diffusion language models, demonstrating superior performance compared to existing methods.

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [58] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang,Licheng Zhang,Zheren Fu,Zhendong Mao*

Main category: cs.CL

TL;DR: A new framework called CL-RAG is proposed, which uses multi-stage curriculum learning to train RAG systems more effectively. It constructs training data with multiple difficulty levels and optimizes the overall performance and generalization of the RAG system.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing the retriever or generator in the RAG system do not consider the varying effectiveness of documents across user queries, hindering adaptation during training.

Method: Propose a multi-stage Curriculum Learning based RAG system training framework named CL-RAG. Construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution and train the model in stages based on the curriculum learning approach.

Result: CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.

Conclusion: The CL-RAG framework can optimize the overall performance and generalization of the RAG system more effectively.

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [59] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou,Xiao Deng,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: Code security and usability are crucial for coding assistant applications driven by LLMs. Current benchmarks lack comprehensive assessment across dimensions. This paper proposes CoV-Eval, a multi-task benchmark for evaluating LLM code security, and VC-Judge, an improved judgment model. Experiments reveal challenges and offer insights for future research.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current code security benchmarks that focus solely on single evaluation tasks and lack comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination.

Method: Propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, detection and classification. Develop VC-Judge, an improved judgment model aligning closely with human experts to review LLM-generated programs for vulnerabilities.

Result: Most LLMs identify vulnerable codes well but tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs.

Conclusion: Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security.

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [60] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing,Goran Glavaš*

Main category: cs.CL

TL;DR: 通过优化词对齐工具(WA)的设计选择，跨语言迁移(XLT)在标记投影中的表现可与基于标记的方法相媲美，并且提出了一种新的集成策略，该策略结合了translate-train和translate-test预测，显著优于基于标记的投影，同时降低了对WA低级设计选择的敏感性，从而提高了XLT在分词任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管词对齐工具(WA)常用于标记投影，但将其应用于基于翻译的XLT的低级设计决策尚未得到系统研究；此外，最近基于标记的方法声称在XLT的标记投影中优于WA，因此有必要重新审视WA并探索其潜力。

Method: 系统研究影响基于翻译的XLT性能的低级设计决策，包括：标签在(多)标记跨度之间的投影算法、减少噪声映射标签数量的过滤策略以及翻译句子的预分词处理；引入一种新的投影策略，集成translate-train和translate-test预测。

Result: 优化WA设计选择后，基于WA的XLT性能至少可与基于标记的方法相媲美；新提出的集成策略显著优于基于标记的投影，并降低了对WA低级设计选择的敏感性，提高了XLT在分词任务中的鲁棒性。

Conclusion: 经过优化的WA可以提供与基于标记方法相当甚至更优的XLT性能；集成translate-train和translate-test预测的新策略进一步提升了性能并增强了鲁棒性，为XLT在分词任务中的应用提供了改进方向。

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [61] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: The paper introduces MuToR, a new method for multi-token prediction that integrates learnable register tokens into the input sequence to predict future targets. It has minimal additional parameters, requires no architectural changes, and works well with pre-trained language models. The method is effective for supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining across language and vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-token prediction methods have not consistently shown benefits when applied to fine-tuning scenarios. There is a need for an approach that can effectively leverage multi-token prediction while remaining compatible with pretrained models and suitable for various fine-tuning tasks.

Method: MuToR interleaves learnable register tokens into the input sequence. Each register token predicts future targets in the sequence. This approach introduces only a small number of additional parameters, does not require any changes to the model architecture, and remains aligned with the next-token pretraining objective. It also supports scalable prediction horizons.

Result: MuToR demonstrates effectiveness and versatility across multiple use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining. It performs well on challenging generative tasks in both language and vision domains.

Conclusion: MuToR is a simple yet powerful approach for multi-token prediction that enhances performance in various fine-tuning and pretraining scenarios without requiring significant modifications to existing models.

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [62] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: The paper explores scaling laws in preference modeling akin to those in language modeling, proposing World Preference Modeling (WorldPM) which shows scalability potential and effectiveness in improving generalization performance across human preference datasets.


<details>
  <summary>Details</summary>
Motivation: Scaling laws in language modeling inspired the exploration of similar patterns in preference modeling.

Method: Preference data was collected from public forums, with extensive training on 15M-scale data using models ranging from 1.5B to 72B parameters. Three types of evaluation metrics were analyzed: adversarial, objective, and subjective.

Result: Adversarial metrics improve with more training data and larger models; objective metrics show emergent behavior in larger models; subjective metrics do not scale. WorldPM improves generalization performance across datasets of varying sizes with gains exceeding 5% on key subtasks and 4%-8% in in-house evaluations.

Conclusion: WorldPM is effective as a foundation for preference fine-tuning and significantly enhances performance when integrated into the RLHF pipeline.

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [63] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu,Yibo Wang,Hanze Dong,Yuhui Xu,Amrita Saha,Caiming Xiong,Bryan Hooi,Junnan Li*

Main category: cs.CL

TL;DR: Large reasoning models (LRMs) have a hidden ability for long-term reasoning. Although reinforcement learning (RL) can sometimes bring out advanced reasoning behaviors, these are unpredictable and unreliable. This paper proposes aligning models with three meta-abilities (deduction, induction, and abduction) through automatically generated tasks to improve performance by over 10% compared to instruction-tuned baselines.


<details>
  <summary>Details</summary>
Motivation: LRMs already have the potential for long-term reasoning, but the advanced reasoning behaviors elicited by RL are not consistent or controllable, limiting scalability and reliability.

Method: The method involves a three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning, using automatically generated self-verifiable tasks aligned with deduction, induction, and abduction.

Result: This approach boosts performance by more than 10% relative to instruction-tuned baselines. Domain-specific RL from the aligned checkpoint yields an additional average gain of 2% in performance across math, coding, and science benchmarks.

Conclusion: Explicit alignment with meta-abilities provides a scalable and reliable foundation for reasoning in LRMs.

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [64] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: 本研究探讨了作者身份验证技术在学术写作中测量人机协作的潜力，通过构建学生写作档案并评估其有效性，为AI驱动时代的学术诚信提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在教育环境中的日益普及，理解并衡量这种互动的程度和性质提出了重大挑战。本研究探讨了作者身份验证（AV）技术的使用，不仅作为惩罚措施，而且作为量化AI辅助学术写作的手段，重点关注促进透明度、可解释性和学生发展。

Method: 我们开发了一种适应性的特征向量差异AV方法，以构建学生稳健的学术写作档案，旨在捕捉其写作中的有意义的、个人的特征。

Result: 结果表明，增强的AV分类器能够识别风格学差异，并在单词和句子层面测量人机协作，同时为教育工作者提供了一个透明的工具来支持学术诚信调查。

Conclusion: 本研究推进了AV技术，为AI驱动时代的学术写作动态提供了可操作的见解。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [65] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: 本文比较了静态和动态叙事对学习者动机的影响，并讨论了设计上的启示。


<details>
  <summary>Details</summary>
Motivation: 动机是成功学习的重要因素。之前的研究已经证明了静态互动叙事游戏对动机的积极影响。同时，AI的进步使得动态和自适应的互动叙事方法变得更加可行。然而，有限的工作探讨了动态叙事对学习者动机的影响。

Method: 本文比较了两种版本的Academical，一种是传统的手工编写的分支剧情（静态叙事），另一种是在游戏中动态序列剧情（动态叙事）。

Result: 结果强调了响应式内容和多样选择对于玩家参与的重要性，同时也说明了平衡教学目标与叙事动态方面的挑战。

Conclusion: 本文为AI驱动的动态叙事在教育游戏中的潜在应用提供了初步步骤。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [66] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在语言理解方面可能并不逊色于人类，且现有模型的能力可能被低估。研究强调了改进实验设计和评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在重新审视最近关于大型语言模型（LMs）在理解简单英语陈述方面表现不佳的主张，并探讨人类表现是否被高估，而语言模型的能力是否被低估。

Method: 研究使用了相同的刺激材料，进行了一项预注册研究，比较了人类在两种条件下的反应：一种允许重读（复制原始研究），另一种限制重读（更自然的阅读测试）。此外，还使用Llama-2-70B的日志概率、对开放式模型响应的重新编码以及对其他句子的语法性评分进行了额外分析。

Result: 当允许重读时，人类的准确率显著下降（73%），低于Falcon-180B-Chat（76%）和GPT-4（81%）。最新的GPT-o1模型实现了完美的准确率。此外，人类和模型在涉及可能互惠行为的查询（如接吻）上都面临更大的挑战，这表明共享的语用敏感性而非模型特定缺陷。

Conclusion: 研究结果表明，当前的语言模型在语言理解方面并不一定比人类弱，这挑战了现有的假设。同时，研究强调了在评估语言模型时需要更谨慎的实验设计和编码实践。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [67] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: 研究发现GPT-4能够捕捉自然语言中的信息结构和可接受性关系，并表现出与人类相似的元语言技能。


<details>
  <summary>Details</summary>
Motivation: 探讨LM如何理解自然语言或生成可靠的元语言判断，以及LM是否能表示和尊重语言学家提出的形式与功能之间的微妙关系。

Method: 通过探测GPT-4在与人类相同的任务上的表现，并进行新的扩展来确定任何LM是否能捕捉这种关系。

Result: GPT-4在信息结构和可接受性任务上表现出可靠的元语言技能，复制了这两个任务之间的显著相互作用。此外，研究2确认了信息结构对LDD构造的可接受性评分有因果影响。

Conclusion: 研究结果表明自然语言和GPT-4生成的英语之间存在紧密关系，以及信息结构和句法之间的关系，这需要进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [68] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: ACPO是一种无需外部监督的自我监督偏好调整方法，能够有效提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的模型对齐方法通常依赖于更强的模型（如GPT-4）或外部知识库来评估事实正确性，这可能并不总是可用。因此，需要一种不需要外部监督的方法来提高事实准确性。

Method: ACPO利用原子一致性信号，即多个随机响应中单个事实的一致性，来识别高质量和低质量的数据对以进行模型对齐。

Result: 实证结果表明，ACPO在LongFact和BioGen数据集上比FactAlign（一个强大的监督对齐基线）高出1.95分，证明了其在提高事实可靠性方面的有效性。

Conclusion: ACPO是一种有效的自我监督偏好调整方法，能够在不依赖外部模型或知识库的情况下提高事实可靠性。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [69] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

TL;DR: 该研究分析了12个大型语言模型（LLM）的输出，发现同一模型生成的文本比人类撰写的文本更相似，而GPT-4的输出更具多样性。此外，不同模型的写作风格存在显著差异，某些模型在性别平衡和减少偏见方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成文本的相似性、变异性以及伦理影响，例如同一模型生成的文本有多相似，不同模型之间如何比较，以及哪些模型最符合伦理标准。

Method: 我们使用了5000个跨不同任务的提示，包括生成、解释和重写，产生了约300万条来自12个LLM的文本，包括来自OpenAI、Google、Microsoft、Meta和Mistral的专有和开源系统。

Result: （1）同一LLM的输出比人类撰写的文本更相似；（2）像WizardLM-2-8x22b这样的模型生成高度相似的输出，而GPT-4产生更多样化的响应；（3）LLM的写作风格差异显著，Llama 3和Mistral显示出更高的相似性，而GPT-4则因其独特性脱颖而出；（4）词汇和语气的差异突显了LLM生成内容的语言独特性；（5）一些LLM表现出更大的性别平衡和减少的偏见。

Conclusion: 这些结果为LLM输出的行为和多样性提供了新的见解，有助于指导未来的发展和伦理评估。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [70] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: This paper introduces S-DAT, a scalable, multilingual framework for automated assessment of divergent thinking, which uses large language models and advanced multilingual embeddings to compute semantic distance as a language-agnostic proxy for DT.


<details>
  <summary>Details</summary>
Motivation: Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability.

Method: S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance, which serves as a language-agnostic proxy for divergent thinking (DT).

Result: S-DAT was evaluated across eleven diverse languages, demonstrating robust and consistent scoring across linguistic contexts. It shows convergent validity with other DT measures and correct discriminant validity with convergent thinking.

Conclusion: S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online.

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [71] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习框架CEC-Zero，使大型语言模型能够通过自主错误策略学习进行自我纠正，而无需外部监督。实验结果表明，该方法在中文文本校正中具有较高的准确性和跨领域泛化能力，为中文自然语言处理应用提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在准确性和鲁棒性方面优于传统的基于BERT的模型，但在可靠性和泛化能力方面仍存在挑战。因此，需要一种新的方法来提高LLMs在中文文本校正中的可靠性。

Method: 本文提出了一种新的强化学习（RL）框架，称为CEC-Zero，它结合了RL和LLMs的生成能力，使LLMs能够通过自主错误策略学习进行自我纠正，而无需依赖注释数据或辅助模型。

Result: 实验表明，增强RL的LLMs实现了行业可行的准确性，并在跨领域泛化方面表现出色，为中文自然语言处理应用的可靠性优化提供了一种可扩展的解决方案。

Conclusion: 本文提出了CEC-Zero，这是一种新的强化学习（RL）框架，使大型语言模型（LLMs）能够通过自主错误策略学习进行自我纠正，而无需外部监督。实验表明，增强RL的LLMs实现了行业可行的准确性，并在跨领域泛化方面表现出色，为中文自然语言处理应用的可靠性优化提供了一种可扩展的解决方案。这一突破促进了LLMs在实际中文文本校正场景中的部署，并建立了一个新的自我改进语言模型范式。

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [72] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

TL;DR: 本文介绍了一种新的UML建模工具，它在传统工具的基础上进行了重大改进，可以集成类图和对象图并执行对象，适用于新的软件架构和教学。


<details>
  <summary>Details</summary>
Motivation: 开发新的UML建模工具是为了改进传统的工具，提供更强大的功能，如集成类图和对象图以及执行对象，从而支持新的软件架构和教学应用。

Method: 该论文介绍了新的UML建模工具的设计、实现和使用，通过集成类图和对象图以及执行对象来实现其功能。

Result: 该工具允许集成类图和对象图以及执行对象，为软件架构和教学提供了新的可能性。

Conclusion: 该论文描述了一个新的UML建模工具的设计、实现和使用，该工具在传统工具上有了显著的改进。它允许集成类图和对象图以及执行对象，这不仅能够实现新的软件架构，而且在教学中也非常有用。该项目源于一个长期的国际研究项目，展示了研究如何产生有价值的副产品。

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [73] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

TL;DR: 本文提出了一种多语言、可扩展且无监督的跨领域方面检测框架，用于对多语言和多领域评论数据进行多方面标记。通过聚类提取方面类别候选，并使用负采样生成方面感知嵌入向量。实验结果显示，自动生成的标签质量高，能够有效用于训练，并且在处理大规模数据时具有更好的一致性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 有效分析在线评论数据对于各个行业至关重要。然而，许多现有研究仅限于特定领域和语言，或者依赖需要大规模标记数据集的监督学习方法。为了解决这些限制，我们提出了一个适用于多语言、多领域评论数据的多方面标记框架。

Method: 本文提出了一种多语言、可扩展且无监督的跨领域方面检测框架。该框架旨在对多语言和多领域评论数据进行多方面标记。首先通过聚类提取方面类别候选，然后使用负采样将每条评论表示为方面感知嵌入向量。为了评估框架，我们进行了多方面标记并微调了几个预训练语言模型，以衡量自动生成标签的有效性。

Result: 结果表明，这些模型表现出色，证明生成的标签适合用于训练。此外，与公开可用的大语言模型进行比较，突显了该框架在处理大规模数据时的一致性和可扩展性。人工评估也证实了自动标签的质量与手动创建的标签相当。

Conclusion: 本研究展示了多方面标记方法的潜力，克服了监督方法的限制，并适应多语言、多领域环境。未来的研究将探索自动评论摘要和人工智能代理的整合，以进一步提高评论分析的效率和深度。

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [74] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: InForage is a reinforcement learning framework that enhances large language models by enabling dynamic, adaptive retrieval during inference, leading to improved performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval.

Method: InForage is a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. It explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors.

Result: Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods.

Conclusion: InForage's results highlight its effectiveness in building robust, adaptive, and efficient reasoning agents.

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [75] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

TL;DR: 本文发现了一种新的现象，即上下文同步，并提出了一种新方法来识别与该现象相关的注意力头。通过关闭这些头，可以减轻语言模型受到的干扰。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究语言模型如何受到输入提示中的无关信息的影响，并探索如何减轻这种干扰问题。

Method: 本文通过实验和分析，发现了上下文同步现象，并提出了一种基于可微分掩码的新方法来识别与该现象相关的注意力头。

Result: 本文发现，语言模型会为之前出现在上下文中的任何标记分配更高的logits（或概率），即使这些标记是随机的。此外，作者发现通过关闭与上下文同步相关的注意力头可以显著减少这种干扰。

Conclusion: 本文发现了一种新的现象，即上下文同步，并通过实验证明了这种现象在语言模型中普遍存在。此外，作者提出了一种新的方法来识别与上下文同步相关的注意力头，并通过关闭这些头来减轻模型的干扰问题。

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [76] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

TL;DR: Qwen3 is the latest version of the Qwen model family, featuring advanced performance, efficiency, and multilingual capabilities. It integrates thinking and non-thinking modes into a unified framework, introduces a thinking budget mechanism, and offers competitive results across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To advance performance, efficiency, and multilingual capabilities of large language models (LLMs) while eliminating the need to switch between different models for complex reasoning and rapid responses.

Method: Qwen3 integrates thinking mode and non-thinking mode into a unified framework, introduces a thinking budget mechanism, and leverages knowledge from flagship models to reduce computational resources for smaller-scale models.

Result: Qwen3 demonstrates competitive performance against larger MoE models and proprietary models in tasks such as code generation, mathematical reasoning, and agent tasks, with expanded multilingual support.

Conclusion: Qwen3 achieves state-of-the-art results across diverse benchmarks and expands multilingual support, enhancing global accessibility.

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [77] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: QEDACVC 是一种基于量子计算的多语言机器翻译方法，在 OPUS 数据集上实现了 82% 的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的云服务如 Google Translate 和 Microsoft Translator 使用大型多语言语言模型，但它们依赖于经典计算。QEDACVC 探索量子计算领域，以研究和展示多语言机器翻译。

Method: QEDACVC 引入了量子编码器-解码器架构，通过量子卷积、量子池化、量子变分电路和量子注意力作为软件修改，在量子计算硬件上进行模拟和运行。

Result: QEDACVC 在 OPUS 数据集上实现了 82% 的准确率，展示了其在多语言机器翻译中的潜力。

Conclusion: QEDACVC 是一种基于量子计算的多语言机器翻译解决方案，它在 OPUS 数据集上实现了 82% 的准确率。

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [78] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: PT-MoE is a novel framework that integrates matrix decomposition with MoE routing for efficient PT, achieving state-of-the-art performance in QA and math tasks with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods exhibit counter-intuitive phenomena, such as increased training efficiency without universal performance improvement and parameter reduction through matrix decomposition that improves performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE.

Method: PT-MoE integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning.

Result: PT-MoE achieves state-of-the-art performance in both QA and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA.

Conclusion: PT-MoE achieves state-of-the-art performance in both question answering and mathematical problem solving tasks, while using fewer parameters than LoRA. The integration of matrix decomposition and MoE yields complementary benefits, enabling cross-task consistency and generalization abilities.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [79] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

TL;DR: 本文提出了WorldView-Bench，一个用于评估大型语言模型中全球文化包容性的基准，通过分析其容纳多样世界观的能力。结果表明，应用多重视角可以显著提高文化包容性，并减少文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试框架未能充分捕捉这种偏见，因为它们依赖于刚性、封闭形式的评估，忽视了文化包容性的复杂性。

Method: 我们引入了WorldView-Bench，这是一个用于评估大型语言模型（LLMs）中的全球文化包容性（GCI）的基准，通过分析它们容纳多样世界观的能力来衡量文化极化，即排除其他观点的程度。我们通过两种干预策略实现应用的多重视角：(1) 嵌入多重视角原则的上下文实施多重视角LLMs，以及(2) 多智能体系统（MAS）实施的多重视角LLMs，其中代表不同文化视角的多个LLM代理协作生成响应。

Result: 我们的结果表明，Perspectives Distribution Score（PDS）熵从基线的13%增加到MAS-Implementation多重视角LLMs的94%，同时转向积极情绪（67.7%）并增强了文化平衡。

Conclusion: 这些发现表明，具有多重视角意识的AI评估在减轻大型语言模型中的文化偏见方面具有潜力，为更包容和符合伦理的AI系统铺平了道路。

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [80] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: 本文提出了一种利用图卷积和LSTMs进行上下文嵌入的方法，在有限资源下实现了有效的下一个词预测。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型需要大量的参数、文本数据和计算资源，而本研究旨在解决语言建模中的一个子任务——上下文嵌入。

Method: 提出了一种利用图卷积操作的GNN方法来编码上下文，并将其与LSTMs结合用于预测下一个词。

Result: 在自定义的Wikipedia文本语料库上进行了测试，结果表明该方法在有限的资源下表现良好。

Conclusion: 该方法在有限的资源下能够有效地预测下一个词，展示了其潜力。

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [81] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Xuanzhao Dong,Hao Wang,Haiyu Wu,Huayu Li,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi*

Main category: cs.CL

TL;DR: 本文提出了一种名为DRA的方法，该方法通过引入语义多样性来改进强化学习中的奖励计算，从而解决了多样性-质量不一致的问题。DRA在数学推理任务中表现出色，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO通常依赖于解决方案级别和标量奖励信号，这些信号无法捕捉到采样完成之间的语义多样性。这导致了我们称之为多样性-质量不一致的问题，其中不同的推理路径可能获得无法区分的奖励。

Method: 我们提出了DRA，一种将语义多样性显式纳入奖励计算的方法。DRA使用子模态互信息（SMI）来降低冗余完成的权重并放大多样完成的奖励。

Result: 我们的方法与GRPO及其变体DR.GRPO无缝集成，产生了DRA-GRPO和DGA-DR.GRPO。我们在五个数学推理基准上评估了我们的方法，并发现它优于最近的强基线。

Conclusion: 我们的方法在五个数学推理基准上进行了评估，并优于最近的强基线。它在仅使用7,000个微调样本和大约55美元的总训练成本的情况下，达到了58.2%的平均准确率。

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [82] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger,Francesco Salvi,Jiacheng Liu,Xiaoli Nan,Ramit Debnath,Barbara Fasolo,Evelina Leivada,Gabriel Recchia,Fritz Günther,Ali Zarifhonarvar,Joe Kwon,Zahoor Ul Islam,Marco Dehnert,Daryl Y. H. Lee,Madeline G. Reinecke,David G. Kamper,Mert Kobaş,Adam Sandford,Jonas Kgomo,Luke Hewitt,Shreya Kapoor,Kerem Oktar,Eyup Engin Kucuk,Bo Feng,Cameron R. Jones,Izzy Gainsburg,Sebastian Olschewski,Nora Heinzelmann,Francisco Cruz,Ben M. Tappin,Tao Ma,Peter S. Park,Rayan Onyonka,Arthur Hjorth,Peter Slattery,Qingcheng Zeng,Lennart Finke,Igor Grossmann,Alessandro Salatiello,Ezra Karger*

Main category: cs.CL

TL;DR: 本研究比较了大型语言模型和激励的人类说服者在说服能力方面的表现，发现LLM在说服能力上优于人类，并且在引导答题者走向正确或错误答案时产生了显著影响。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估AI在说服能力方面是否超过人类，并探讨其对实际应用的影响。

Method: 我们直接比较了前沿大型语言模型（LLM；Claude Sonnet 3.5）与激励的人类说服者在互动、实时对话测验设置中的说服能力。在一项预先注册的大规模激励实验中，参与者（测验答题者）完成了一个在线测验，其中说服者（人类或LLM）试图说服测验答题者走向正确或错误的答案。

Result: 我们发现LLM说服者在方向性说服尝试中获得了比激励的人类说服者更高的合规性，这表明在诚实（向正确答案）和欺骗（向错误答案）情境下，LLM具有优越的说服能力。此外，当引导测验答题者走向正确答案时，LLM说服者显著提高了答题者的准确性，从而获得更高的收益；而当引导他们走向错误答案时，LLM说服者则显著降低了他们的准确性，导致收益减少。

Conclusion: 我们的研究结果表明，AI的说服能力已经超过了那些有真实金钱奖励的真人说服者。这些发现强调了发展新兴对齐和治理框架的紧迫性。

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [83] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 本文提出了一个元学习框架，用于优化系统提示，使其在不同任务和领域中具有良好的泛化能力，并能快速适应未见过的任务。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化工作主要集中在特定于单个查询或任务的用户提示上，而忽略了可以跨不同任务和领域应用的系统提示。

Method: 我们提出了一种元学习框架，通过在多个数据集上的各种用户提示优化系统提示，同时迭代更新用户提示以确保它们之间的协同作用。

Result: 我们在14个跨5个不同领域的未见过的数据集上进行了实验，结果表明我们的方法能够生成有效泛化的系统提示，并且优化后的系统提示能够在未见过的任务上实现快速适应。

Conclusion: 我们的方法能够生成在不同用户提示上表现良好的系统提示，并且在未见过的任务上也能快速适应，同时减少了测试时用户提示的优化步骤。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [84] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu,Lechen Zhang,Sheza Munir,Yiyang Gu,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了VeriFact事实评估框架和FactRBench基准，以提高大型语言模型生成内容的事实准确性，特别是在事实完整性和关系信息方面。


<details>
  <summary>Details</summary>
Motivation: 由于生成的事实中存在复杂的句子间依赖关系，评估大型语言模型（LLMs）的事实性仍然具有挑战性。以前的解决方案主要遵循分解-去上下文-验证的流程，但往往无法捕捉到关键的上下文和遗漏了重要的关系事实。

Method: 本文介绍了VeriFact，这是一个事实评估框架，旨在通过识别和解决不完整和缺失的事实来增强事实提取，以支持更准确的验证结果。此外，还引入了FactRBench，这是一个评估长文本模型响应中精确度和召回率的基准，而之前的工作主要关注精确度。

Result: 实证评估显示，VeriFact显著提高了事实完整性并保留了包含关键关系信息的复杂事实，从而实现了更准确的事实评估。在FactRBench上对各种开源和闭源大语言模型进行基准测试表明，同一模型家族中的大型模型在精度和召回率上都有所提高，但高精度并不总是与高召回率相关，这突显了全面事实评估的重要性。

Conclusion: VeriFact显著提高了事实完整性并保留了包含关键关系信息的复杂事实，从而实现了更准确的事实评估。在FactRBench上对各种开源和闭源大语言模型进行基准测试表明，同一模型家族中的大型模型在精度和召回率上都有所提高，但高精度并不总是与高召回率相关，这突显了全面事实评估的重要性。

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [85] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: 本文介绍了一种使用LLMs进行文本分析的方法，通过迭代和协作过程开发、测试和应用分类法。


<details>
  <summary>Details</summary>
Motivation: 分析文本如开放式回答、标题或社交媒体帖子是一个耗时且劳动密集型的过程，容易受到偏见的影响。LLMs是用于文本分析的有前途的工具。

Method: 本文提供了一个逐步教程，通过研究人员和LLMs之间的迭代和协作过程，高效地开发、测试和应用分类法。

Result: 本文展示了如何编写提示来审查数据集并生成生活领域分类法，通过提示和直接修改评估和优化分类法，测试分类法并评估编码者间的一致性，并应用分类法对整个数据集进行分类，具有高编码者间可靠性。

Conclusion: 本文讨论了使用LLMs进行文本分析的可能性和局限性。

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [86] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: 本文提出了一种名为TokenAdapt的框架，通过引入模型无关的分词器移植方法和预分词学习，显著提高了预训练语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分词器替换方法通常需要大量的计算资源，并且可能无法完全保留语义细微差别或解决压缩效率问题。

Method: TokenAdapt框架包括两个创新：一是模型无关的分词器移植方法，二是用于多词超令牌的预分词学习以提高压缩和减少碎片化。

Result: 实验结果验证了这两种贡献：移植启发式方法成功初始化了唯一标记，并明显优于传统基线和复杂方法，而我们的超令牌实现了显著的压缩增益。

Conclusion: TokenAdapt框架通过引入两种创新方法，显著提高了预训练语言模型的效率和性能，特别是在多语言或专业应用中。

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [87] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: 本研究探讨了使用NLP技术，特别是命名实体识别（NER），来自动提取肺癌和乳腺癌的临床信息，结果显示该方法在识别某些实体方面表现良好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 手动从临床报告中提取信息的过程既耗时又容易出错，限制了数据驱动方法在医疗保健中的效率。因此，需要一种自动化的方法来提高数据提取的准确性和效率。

Method: 研究使用了GMV的NLP工具uQuery，以及基于RoBERTa的生物医学语言模型bsc-bio-ehr-en3，并通过Transformers架构进行了微调，以执行命名实体识别（NER）任务。

Result: 研究结果表明，NLP技术在自动提取肺癌和乳腺癌的临床信息方面表现出色，特别是在识别像MET和PAT这样的实体方面。然而，对于较少见的实体如EVOL，仍存在挑战。

Conclusion: 研究结果表明，NLP技术在自动提取肺癌和乳腺癌的临床信息方面表现出色，特别是在识别像MET和PAT这样的实体方面。然而，对于较少见的实体如EVOL，仍存在挑战。

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [88] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 本研究探讨了LLM中的真实方向在不同对话格式之间的泛化能力，并提出了一种通过在对话末尾添加固定关键词短语来提高泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，LLM在激活空间中具有普遍的真实方向，真实和虚假陈述是线性可分的。然而，这种真实方向在不同对话格式之间的泛化能力仍有待探索。

Method: 我们通过在每段对话的末尾添加一个固定的关键词短语来提出一种解决方案，以显著改善这种泛化。

Result: 我们发现，对于以谎言结束的简短对话，这种真实方向有良好的泛化能力，但对较长的对话格式（其中谎言出现在输入提示的早期）则泛化能力较差。

Conclusion: 我们的结果突显了开发能够推广到新设置的可靠LLM谎言检测器所面临的挑战。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [89] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui,Juan Diego Rodriguez,Philippe Laban,Dean Murphy,Joseph P. Dexter,Richard Jean So,Samuel Baker,Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 本文介绍了KRISTEVA，这是第一个用于评估解释性推理的细读基准，包含1331个选择题。我们提出了三个逐步增加难度的任务集，以测试大型语言模型对文学作品的理解和推理能力。结果显示，尽管这些模型具有一些大学水平的细读能力，但它们的表现仍落后于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 由于细读从未在大型语言模型上进行过评估，且多学科基准如MMLU不包括文学作为主题，因此需要填补这一空白。

Method: 我们提出了KRISTEVA，这是一个用于评估解释性推理的首次细读基准，包含从课堂数据中改编的1331个选择题。我们提出了三个逐步增加难度的任务集，以近似细读过程的不同元素，并测试大型语言模型如何理解文学作品。

Result: 我们的基线结果发现，尽管最先进的大型语言模型具有一些大学水平的细读能力（准确率49.7% - 69.7%），但它们的表现仍落后于经验丰富的评估者。

Conclusion: 虽然最先进的大型语言模型具有一些大学水平的细读能力，但它们在11项任务中的10项上仍然落后于经验丰富的评估者。

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [90] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在预测冲突中的应用，发现其在没有外部数据的情况下也能提供有价值的预测，但结合外部信息可以显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs在预测暴力冲突方面的潜力，以支持早期预警系统、人道主义规划和政策制定。

Method: 本研究比较了LLMs的参数化知识（仅依赖预训练权重）与非参数化能力（通过检索增强生成RAG访问结构化和非结构化上下文数据）。

Result: 研究发现，LLMs在冲突趋势和死亡人数预测方面表现良好，但通过结合外部信息可以进一步提升性能。

Conclusion: 研究结果突显了大型语言模型（LLMs）在冲突预测中的优势和局限性，并表明通过结构化外部知识增强它们可以提高性能。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [91] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila,Esteban Villa Turek,Ellen Karina Chumbe Fernandez,Luis Felipe Polo Galvez,Luis Cadavid,Andrea Marroquin,Rebeca Vargas Quesada,Johanna Crew,Nicole Vallejo Galarraga,Christopher Rodriguez,Diego Gutierrez,Radhi Datla*

Main category: cs.CL

TL;DR: 本文探讨了西班牙语不同变体之间的差异，并提出实施五个西班牙语子变体以促进用户信任和国际战略的积极影响。


<details>
  <summary>Details</summary>
Motivation: 强调区域本地化模型的必要性，因为这些差异在西班牙语方言群体的日常使用中造成了显著的空白，从而产生了社会语言不和谐。

Method: 本文研究了拉丁美洲和西班牙的书面西班牙语变体之间的主要差异，并进行了深入的社会文化及语言背景分析。

Result: 这种做法有助于制定更好的本地化策略，以更有效地满足包容性目标，并确保在低风险投资地理区域内的可持续活跃用户增长。

Conclusion: 实施至少提出的五个西班牙语子变体可以实现两个行动方向：促进用户对人工智能语言模型的信任和依赖，同时展示出反映国际战略积极影响的文化、历史和社会语言意识。

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [92] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang,Yubing Ren,Yanan Cao,Binxing Fang*

Main category: cs.CL

TL;DR: 本文提出了一种多功能的共生水印框架，通过结合logits-based和sampling-based方案的优势，实现了更好的水印性能。


<details>
  <summary>Details</summary>
Motivation: 当前的水印方案在鲁棒性、文本质量和安全性之间存在权衡，因此需要一种能够结合logits-based和sampling-based方案优势的方法。

Method: 本文提出了一种多功能的共生水印框架，包含三种策略：串行、并行和混合。混合框架利用标记熵和语义熵自适应地嵌入水印，以优化可检测性、鲁棒性、文本质量和安全性。

Result: 实验结果表明，本文的方法优于现有的基线，并实现了最先进的（SOTA）性能。

Conclusion: 本文提出的框架为多样化的水印范式提供了新的见解，并在各种数据集和模型上验证了其优越性。

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [93] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu,Hanzhang Zhou,Zijian Feng,Tianjiao Li,Chua Jia Jim Deryl,Mak Lee Onn,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: 本文提出了一种新的提示优化方法MePO，它通过可解释的设计提高了提示和响应质量，并在各种任务和模型类型中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于先进的大型LLM（如GPT-4）来生成优化的提示。然而，由于向下兼容性有限，来自先进LLM的冗长、指令密集型提示可能会使轻量级推理模型过载并降低响应质量。

Method: 我们首先确定了一组与模型无关的提示质量优点，并通过实验证明了它们在提高提示和响应质量方面的有效性。然后，我们引入了MePO，这是一种基于优点引导的轻量级、本地可部署的提示优化器，它在我们从由轻量级LLM生成的对齐优点提示构建的偏好数据集上进行训练。

Result: 实验表明，MePO在各种任务和模型类型中取得了更好的结果，为实际部署提供了一个可扩展且稳健的解决方案。

Conclusion: 实验表明，MePO在各种任务和模型类型中取得了更好的结果，为实际部署提供了一个可扩展且稳健的解决方案。我们的模型和数据集可在https://github.com/MidiyaZhu/MePO获取。

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [94] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 本文提出了一种使用知识图谱来增强大型语言模型生成个性化响应的方法，以解决过度拟合和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLMs经常因过度拟合而产生额外和错误的数据，导致输出中的幻觉。缺乏及时、事实性和个性化信息是这些问题的根本原因。

Method: 我们提出了一种方法，通过引入检索增强生成（RAG）使用知识图谱（KGs）来辅助LLM生成个性化的响应。

Result: 实验结果表明，我们的方法在理解个人信息和生成准确响应方面显著优于使用个人数据作为文本输入的基线LLMs，同时响应时间有适度的减少。

Conclusion: 我们的方法在理解个人信息和生成准确响应方面显著优于使用个人数据作为文本输入的基线LLMs，同时响应时间有适度的减少。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [95] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin,Fan Huang*

Main category: cs.CL

TL;DR: 本文提出了一种衡量LLM隐性偏见的新方法DIF，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准方法来衡量LLM中的隐性偏见，本文旨在填补这一空白。

Method: 本文通过评估现有的LLM逻辑和数学问题数据集，并结合社会人口学人物角色来计算一个易于解释的基准DIF。

Result: 本文发现，DIF方法可以统计验证LLM行为中隐性偏见的存在，并且发现问答准确性和隐性偏见之间存在反向趋势。

Conclusion: 本文认为，LLM中的隐性偏见不仅是一个伦理问题，也是一个技术问题，并提出了一个衡量LLM偏见的基准方法DIF。

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [96] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng,Jinhao Jiang,Zican Dong,Wayne Xin Zhao,Lei Fang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CAFE的两阶段方法，用于增强多文档问答能力，通过逐步消除背景和干扰文档的影响，使响应更依赖于证据文档。实验表明，CAFE在多个基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在平衡检索精度和召回率方面面临挑战，影响了它们回答问题的效果。因此，需要一种更有效的方法来增强多文档问答能力。

Method: CAFE是一种两阶段的粗到细方法，通过逐步消除背景和干扰文档的负面影响，使响应更依赖于证据文档。首先，使用检索头进行粗粒度过滤，以识别和排序相关文档。然后，使用细粒度引导方法引导注意力到最相关的内容。

Result: CAFE在多个基准测试中表现出色，超过了基线方法，在Mistral模型上分别比SFT和RAG方法提高了22.1%和13.7%的SubEM。

Conclusion: CAFE在多个基准测试中表现出色，超过了基线方法，在Mistral模型上分别比SFT和RAG方法提高了22.1%和13.7%的SubEM。

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [97] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: This paper highlights the growing threat of dark LLMs and the vulnerability of state-of-the-art models to jailbreak attacks. The research reveals a universal jailbreak method that can compromise multiple models, leading to harmful outputs. Despite responsible disclosure, industry responses have been inadequate, raising concerns about AI safety practices.


<details>
  <summary>Details</summary>
Motivation: The fundamental vulnerability of LLMs to jailbreak attacks stems from the data they learn from. Training data that includes unfiltered, problematic, or 'dark' content can lead to undesirable patterns or weaknesses that allow users to circumvent safety controls.

Method: Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. We uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models.

Result: Our universal jailbreak attack effectively compromised multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. Many tested LLMs were still vulnerable to this attack despite our responsible disclosure efforts.

Conclusion: Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [98] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah,Francois Meyer*

Main category: cs.CL

TL;DR: 本文首次系统地研究了预训练语言模型在非洲语言中的语言知识。通过训练逐层探测器和设计控制任务，我们发现针对非洲语言调整的PLMs比大规模多语言PLMs编码了更多的目标语言语言信息。我们的研究应用了已建立的可解释性技术来分析非洲语言的PLMs，并突显了主动学习和多语言适应策略成功背后的内部机制。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLMs）在非洲语言上的表现持续改善，但这些进步的原因仍然不清楚。本文首次系统地研究了PLMs在非洲语言中的语言知识。

Method: 我们为六种语言学上多样的非洲语言训练了逐层探测器，以分析语言特征的分布情况。我们还设计了控制任务，以解释探测器性能，并在MasakhaPOS数据集上进行了实验。

Result: 我们发现，针对非洲语言调整的PLMs比大规模多语言PLMs编码了更多的目标语言语言信息。我们的结果证实了之前的发现，即词法级句法信息集中在中层到最后一层，而句子级语义信息分布在所有层中。通过控制任务和探测基线，我们确认性能反映了PLMs的内部知识，而不是探测器的记忆。

Conclusion: 我们的研究应用了已建立的可解释性技术来分析非洲语言的PLMs。通过这样做，我们突显了主动学习和多语言适应策略成功背后的内部机制。

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [99] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu,Sony Trenous,Leonardo F. R. Ribeiro,Bill Byrne,Felix Hieber*

Main category: cs.CL

TL;DR: XRAG is a novel benchmark designed to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings. It is constructed from recent news articles and covers both monolingual and multilingual retrieval scenarios. The experimental results reveal two previously unreported challenges in cross-lingual RAG.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results.

Method: XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers monolingual and multilingual retrieval scenarios and provides relevancy annotations for each retrieved document.

Result: Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.

Conclusion: XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity.

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [100] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan,Di Wu,Yibin Lei,Christof Monz,Iacer Calixto*

Main category: cs.CL

TL;DR: 本文介绍了S-MedQA数据集，用于评估大型语言模型在医学问答任务中的表现，并发现改进主要来自于领域转换而非知识注入。


<details>
  <summary>Details</summary>
Motivation: 为了评估在医学QA这种知识密集型场景中，知识注入的假设是否适用，并探索微调数据在医学领域的作用。

Method: 我们引入了S-MedQA，一个用于基准测试大型语言模型在细粒度临床专科中的英语医学问答（QA）数据集。我们使用S-MedQA来检查一个与知识注入相关的流行假设在医学QA的知识密集场景中的适用性。

Result: 1) 在特定专科的数据上进行训练并不一定能在该专科上取得最佳性能；2) 无论在哪个专科上进行微调，所有专科的临床相关术语的token概率都会一致增加。

Conclusion: 我们相信改进主要来自于领域转换（例如，从一般到医学），而不是知识注入，并建议重新考虑微调数据在医学领域的角色。

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [101] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da,Parth Mitesh Shah,Kuan-Ru Liou,Jiaxing Zhang,Hua Wei*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的检索增强生成框架GE-Chat，以提高LLM输出的可靠性，通过创建知识图谱并结合链式思维逻辑生成、多跳子图搜索和基于蕴含的句子生成，实现了准确的证据检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）现在是人类决策过程中的关键助手，但它们的输出并不总是可靠的，用户必须手动评估。幻觉响应常常带有看似合理的解释，给用户带来复杂性和信任问题。因此，需要一种可靠的方法来检查LLM的结论并帮助判断其可信度。

Method: 本文提出了GE-Chat，这是一个基于知识图谱的检索增强生成框架。当用户上传材料文档时，会创建一个知识图谱，这有助于构建一个检索增强代理，以额外的知识增强代理的响应。然后利用链式思维（CoT）逻辑生成、多跳子图搜索和基于蕴含的句子生成来实现准确的证据检索。

Result: 本文的方法在自由形式上下文中识别精确证据方面提高了现有模型的性能，提供了一种可靠的方式来检查LLM结论的资源，并帮助判断其可信度。

Conclusion: 本文提出了一种基于知识图谱的检索增强生成框架GE-Chat，以提供基于证据的响应生成。该方法通过创建知识图谱并结合链式思维逻辑生成、多跳子图搜索和基于蕴含的句子生成，提高了现有模型在自由形式上下文中识别精确证据的能力，从而提供了一种可靠的方式来检查LLM结论的资源并帮助判断其可信度。

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [102] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 本研究探索了基于合成数据的持续预训练（Reasoning CPT）方法，发现其在多个领域中能显著提升模型性能，并且在处理复杂问题时效果更好。


<details>
  <summary>Details</summary>
Motivation: 当前，监督微调和强化学习主要用于特定领域（如数学和编程）的推理模型训练，这限制了训练数据的广度和可扩展性。而持续预训练（CPT）不需要任务特定信号，但如何有效合成推理数据以及这些数据对广泛领域的影响仍不明确。

Method: 本研究采用Reasoning CPT方法，利用来自STEM和法律语料库的隐藏思维生成合成数据，并将其应用于Gemma2-9B模型上，与标准CPT进行比较。

Result: 实验结果表明，Reasoning CPT在所有评估领域都显著提升了性能，特别是在更复杂的问题上，与传统方法的差距增大，最高可达8分。此外，经过隐藏思维训练的模型能够根据问题难度调整推理深度。

Conclusion: 研究发现，通过使用包含隐藏思维的合成数据进行持续预训练（Reasoning CPT），可以显著提高模型在多个领域的表现，并且在面对更复杂的问题时，这种优势更加明显。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [103] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文介绍了CoT Encyclopedia，一个自下而上的框架，用于分析和引导模型推理。该框架能够自动提取多样化的推理标准，并将其聚类为代表性类别，从而提供更可解释和全面的分析。研究还表明，训练数据格式对推理行为的影响大于数据领域，强调了格式感知模型设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究尝试使用预定义的策略类型对CoT进行分类，但这些方法受到人类直觉的限制，无法捕捉模型行为的全部多样性。因此，我们需要一种更全面和自动化的框架来分析和引导模型推理。

Method: 我们引入了CoT Encyclopedia，这是一个自下而上的框架，用于分析和引导模型推理。我们的方法从模型生成的CoTs中自动提取多样的推理标准，将它们嵌入到语义空间中，聚类成代表性的类别，并推导出对比性标准来解释推理行为。

Result: 人类评估显示，该框架比现有方法产生了更可解释和全面的分析。此外，我们证明了这种理解可以带来性能提升：我们可以预测模型可能使用的策略，并引导它走向更有效的替代方案。

Conclusion: 我们的研究表明，通过CoT Encyclopedia框架可以更全面和可解释地分析模型的推理行为，并且这种理解能够提升模型性能。此外，我们发现训练数据格式对推理行为的影响大于数据领域，这强调了格式感知模型设计的重要性。

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [104] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao,Hongyi Huang,Jiayi Wu,YiMing Cheng,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: VQ-Logits is a novel approach that uses Vector Quantization to reduce the parameter count and computational load of the LLM output layer, achieving significant improvements in efficiency with minimal impact on performance.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face significant computational and memory challenges due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference.

Method: VQ-Logits leverages Vector Quantization (VQ) to replace the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently 'scattered' to the full vocabulary space using the learned or preassigned mapping.

Result: Through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4), VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines.

Conclusion: VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines.

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [105] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang,Kaili Sun,Bowen Wu,Qun Yu,Ying Li,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习框架RAIDEN-R1，用于角色扮演对话代理，通过引入可验证的角色意识奖励和构建高质量的数据集，显著提高了角色一致性和推理连贯性。


<details>
  <summary>Details</summary>
Motivation: 角色扮演对话代理（RPCAs）面临持续保持角色一致性的挑战。为了应对这一问题，我们提出了RAIDEN-R1，这是一种新颖的强化学习框架，集成了可验证的角色意识奖励（VRAR）。

Method: 我们提出了RAIDEN-R1，这是一种新颖的强化学习框架，集成了可验证的角色意识奖励（VRAR）。该方法引入了单个和多术语挖掘策略，通过评估角色特定的关键点来生成可量化的奖励。此外，我们通过多LLM协作构建了一个高质量的角色意识思维链数据集，并进行了实验以增强推理连贯性。

Result: 在RAIDEN基准测试中，RAIDEN-R1表现出色：我们的14B-GRPO模型在基于脚本的知识和对话记忆指标上分别达到了88.04%和88.65%的准确率，优于基线模型，同时保持了稳健性。案例分析进一步揭示了模型在解决冲突上下文线索和维持第一人称叙述一致性方面的能力增强。

Conclusion: 本研究填补了RPCA训练中非量化性的空白，并为角色意识推理模式提供了见解，推动了RPCAs的发展。

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [106] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 本研究评估了多个大型语言模型在零样本和少量样本条件下对俄语和乌克兰语社交媒体帖子进行二分类标注的能力。结果表明，这些模型在处理敏感领域任务时存在不同的性能表现，并揭示了它们在跨语言任务中的优缺点。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理系统日益复杂，大型语言模型在多种应用中表现出显著潜力。然而，对于涉及敏感领域任务的多语言环境，需要更深入地了解这些模型的可靠性与适应性。因此，本研究旨在评估大型语言模型在识别人权侵犯引用方面的表现，并探讨其在跨语言任务中的优势和局限性。

Method: 本研究评估了多个最先进的大型语言模型（如GPT-3.5、GPT-4、LLAMA3、Mistral 7B和Claude-2）在零样本和少量样本条件下对包含俄语和乌克兰语社交媒体帖子的复杂文本数据集进行二分类标注的能力。通过将模型的标注与人类双重标注的标准进行比较，分析了不同提示条件下的标注性能，并探索了每个模型的错误和不一致模式。

Result: 研究发现，不同大型语言模型在零样本和少量样本条件下的标注性能存在差异。同时，模型在不同提示条件（英语和俄语）下的表现也有所不同。此外，研究还揭示了各模型在错误和不一致模式上的独特特征，提供了关于它们在处理主观和上下文依赖性判断时的表现见解。

Conclusion: 本研究通过将大型语言模型的输出与人工标注进行对比，有助于理解这些模型在多语言环境中执行敏感领域任务的可靠性和适用性。此外，它还揭示了语言模型如何处理本质上主观和依赖上下文的判断，这对它们在现实场景中的部署至关重要。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [107] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 本文分析了19,123项研究，发现生成式大语言模型在开放式任务中优于传统自然语言处理，但伦理使用这些技术对医疗应用至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索生成式大语言模型（LLMs）和传统自然语言处理（NLP）在不同医学任务中的差异，以更好地理解它们的应用潜力。

Method: 分析了19,123项研究，比较了生成式大语言模型（LLMs）和传统自然语言处理（NLP）在不同医学任务中的表现。

Result: 生成式大语言模型在开放式任务中表现出优势，而传统自然语言处理在信息提取和分析任务中占主导地位。

Conclusion: 随着这些技术的发展，确保它们在医疗应用中的潜力，伦理使用它们是至关重要的。

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [108] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: 本研究介绍了一种名为Quicker的基于大型语言模型的临床决策支持系统，旨在自动化证据综合并生成符合标准临床指南开发流程的临床建议。实验结果表明，Quicker在问题分解、检索灵敏度和文献筛选方面表现出色，并能有效支持人类评审员，同时提供更全面和逻辑连贯的建议。


<details>
  <summary>Details</summary>
Motivation: 将临床证据整合到实时实践中具有挑战性，因为工作量大、专业流程复杂且时间有限。这凸显了需要自动化证据综合工具以支持更高效和准确的临床决策。

Method: 本研究引入了Quicker，这是一个基于大型语言模型（LLMs）的循证临床决策支持系统，旨在自动化证据综合并生成符合标准临床指南开发流程的临床建议。

Result: 实验结果表明，Quicker表现出色，具有细粒度的问题分解，检索灵敏度与人类专家相当，文献筛选性能接近全面纳入相关研究。此外，Quicker辅助的证据评估有效支持了人类评审员，而Quicker的建议比临床医生的建议更全面、逻辑更连贯。在系统级测试中，单个评审员与Quicker合作将推荐开发时间减少到20-40分钟。

Conclusion: 我们的研究结果证实了Quicker在帮助医生做出更快、更可靠的循证临床决策方面的潜力。

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [109] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: This paper introduces J1, a reinforcement learning approach for training LLM-as-a-Judge models, which improves their judgment ability by converting prompts to judgment tasks with verifiable rewards. J1 outperforms other models and provides insights into better training strategies.


<details>
  <summary>Details</summary>
Motivation: The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think.

Method: J1 is a reinforcement learning approach that converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards, incentivizing thinking and mitigating judgment bias.

Result: J1 outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model.

Conclusion: J1 is a reinforcement learning approach that improves the judgment ability of LLM-as-a-Judge models by converting prompts to judgment tasks with verifiable rewards, outperforming other models and providing insights into better training strategies.

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [110] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang,Zhanyu Shen,Hui Huang*

Main category: cs.CL

TL;DR: LDIR is a low-dimensional, dense, and interpretable text embedding method that shows good performance on various NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing text embeddings are either difficult to trace and interpret or suffer from poor performance. The goal is to create a low-dimensional, dense, and interpretable text embedding method.

Method: LDIR uses farthest point sampling to determine the numerical values of its dimensions, which indicate semantic relatedness to different anchor texts.

Result: LDIR was validated on multiple semantic textual similarity, retrieval, and clustering tasks, showing competitive performance with fewer dimensions compared to existing methods.

Conclusion: LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [111] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye,Shaonan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于从多种输入模态中重建连贯的语言，以更准确地解码人类思维。


<details>
  <summary>Details</summary>
Motivation: 以往的研究通常局限于单模态输入，如图像或音频，而人类思维本质上是多模态的。为了弥补这一差距，我们提出了一个统一且灵活的框架。

Method: 我们提出了一种统一且灵活的框架，用于从由不同输入模态（视觉、听觉和文本）引发的脑记录中重建连贯的语言。我们的方法利用了视觉-语言模型（VLMs），使用模态特定的专家来联合解释跨模态的信息。

Result: 实验表明，我们的方法在性能上与最先进的系统相当，同时保持了适应性和可扩展性。

Conclusion: 本研究推动了更生态有效和通用的思维解码的发展。

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [112] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: 本文研究了基于大语言模型的方面情感分析系统，重点在于四元组观点提取。结果表明，多领域模型在性能上可以与单领域模型相媲美，同时降低了操作复杂性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索基于大语言模型（LLMs）的方面情感分析系统的实际应用设计，并解决跨领域和跨语言的四元组观点提取问题。

Method: 本文使用内部数据集，研究了是否可以通过微调一个模型来同时有效处理多个领域特定的分类法。

Result: 本文证明了一个结合的多领域模型在性能上可以与专门的单领域模型相媲美，同时减少了操作复杂性。

Conclusion: 本文展示了单一的多领域模型在处理多个领域特定分类法时可以达到与专用单领域模型相当的性能，同时降低了操作复杂性。

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [113] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为RPG的高效解码方法，以缓解代码生成中的重复问题，并通过构建新的数据集CodeRepetEval进行评估，结果表明RPG在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管神经语言模型显著提升了代码生成的性能，但生成过程中的重复问题仍然存在。以往的工作主要集中在内容重复上，而结构重复是更普遍且更具挑战性的问题。

Method: RPG（基于语法的重复惩罚）方法首先利用语法规则识别代码生成中的重复问题，然后战略性地降低导致重复的关键标记的可能性，从而减轻代码生成中的重复问题。

Result: RPG在CodeRepetEval数据集以及HumanEval和MBPP基准测试中表现出色，有效减少了重复并提高了生成代码的质量。

Conclusion: RPG在CodeRepetEval数据集以及HumanEval和MBPP基准测试中显著优于最佳基线，有效减少了重复并提高了生成代码的质量。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [114] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo,Jae Ho Sohn,Gondy Leroy,Trevor Cohen*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型生成的普通语言摘要（PLS）的效果，发现尽管LLM生成的PLS在主观评价中表现良好，但人类撰写的PLS在理解方面更优。同时，自动化评估指标未能准确反映人类判断，这表明需要改进评估框架和生成方法。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型生成的PLS的有效性评估主要依赖于自动化分数或便利样本的主观Likert量表评分，这些方法缺乏直接衡量可理解性的能力，且结果的普适性有限。因此，需要一种更系统的方法来评估LLM生成的PLS。

Method: 本研究通过亚马逊Mechanical Turk进行了大规模的众包评估，有150名参与者参与。我们通过主观Likert量表评分（关注简单性、信息量、连贯性和忠实性）以及客观的多项选择理解和回忆测量来评估PLS的质量。此外，我们还检查了10个自动化评估指标与人类判断的一致性。

Result: 研究发现，虽然LLM生成的PLS在主观评估中看起来与人类撰写的PLS无法区分，但人类撰写的PLS在理解方面表现更好。此外，自动化评估指标未能反映人类判断，这引发了它们在评估PLS中的适用性的疑问。

Conclusion: 本研究表明，尽管大型语言模型可以生成看起来与人类撰写的PLS无法区分的摘要，但人类撰写的PLS在理解方面表现更好。此外，自动化评估指标未能反映人类判断，这引发了它们在评估PLS中的适用性的疑问。需要开发超越表面质量的评估框架和优化为非专业读者理解的方法。

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [115] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yongkang Wu,Zhonghua Li,Qi Ye,Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner 是一种高效的长文本 RAG 解决方案，通过结构特征和多任务学习实现自适应精炼，显著降低了计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的 RAG 应用常常遇到长上下文输入场景，其中冗余信息和噪声导致更高的推理成本和性能下降。

Method: LongRefiner 通过利用长文档的固有结构特征，采用双级查询分析、分层文档结构和基于单个基础模型的多任务学习进行自适应精炼。

Result: 在七个 QA 数据集上的实验表明，LongRefiner 在各种场景中实现了具有竞争力的性能，同时计算成本和延迟比最佳基线低 10 倍。

Conclusion: LongRefiner 是一种高效、可扩展且有效的长文本 RAG 应用解决方案，为实际应用提供了实用见解。

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [116] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang,Zhiyang Chen,Zijun Wang,Tiancheng Li,Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT is a reasoning framework for diffusion language models that uses reinforcement learning to optimize the entire reasoning trajectory, leading to improved performance in math and code generation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to introduce a reasoning framework for diffusion language models that allows bidirectional, non-linear reasoning without strict grammatical rules in intermediate steps.

Method: DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL).

Result: Experiments on math and code generation tasks show that DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by significant percentages on various benchmarks.

Conclusion: DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both, demonstrating the effectiveness of the proposed framework.

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [117] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang,Licheng Zhang,Zheren Fu,Zhendong Mao*

Main category: cs.CL

TL;DR: 本文提出了一种基于多阶段课程学习的RAG系统训练框架CL-RAG，通过构建多难度训练数据并分阶段训练模型，提升了RAG系统的整体性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法主要关注优化检索器或生成器，但不同文档对用户查询的有效性差异较大，这阻碍了检索器和生成器在训练中的适应性。受人类认知学习的启发，课程学习通过从简单到困难的样本训练模型，从而提高其泛化能力，因此将其整合到RAG系统的训练中。

Method: 本文提出了一种基于多阶段课程学习的RAG系统训练框架，称为CL-RAG。首先通过样本进化构建具有多个难度级别的训练数据，然后基于课程学习方法分阶段训练模型。

Result: CL-RAG框架在四个开放领域问答数据集上表现出一致的有效性，并且在多个先进方法上实现了2%到4%的性能提升。

Conclusion: CL-RAG框架在四个开放领域问答数据集上表现出一致的有效性，并且在多个先进方法上实现了2%到4%的性能提升。

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [118] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou,Xiao Deng,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 本文提出了CoV-Eval多任务基准和VC-Judge判断模型，以全面评估LLM代码安全，并发现LLM在生成安全代码和修复漏洞方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的代码安全基准仅关注单一的评估任务和范式，如代码补全和生成，缺乏在安全代码生成、漏洞修复和识别等维度上的综合评估。

Method: 本文提出了CoV-Eval，一个涵盖多种任务的多任务基准，用于全面评估LLM代码安全。此外，还开发了VC-Judge，一种改进的判断模型，能够更高效和可靠地审查LLM生成的程序中的漏洞。

Result: 尽管大多数LLM能够很好地识别有漏洞的代码，但它们仍然倾向于生成不安全的代码，并且在识别特定类型的漏洞和执行修复方面存在困难。

Conclusion: 本文通过实验和定性分析揭示了LLM代码安全中的关键挑战和优化方向，为未来的研究提供了见解。

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [119] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文研究了基于词对齐的标签投影方法在跨语言迁移学习（XLT）中的应用，发现其性能可以与基于标记的方法相媲美。同时，我们提出了一种新的投影策略，显著提升了XLT的性能，并增强了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于词对齐的方法在标签投影中被广泛使用，但其低级设计决策在翻译-based XLT中的影响尚未得到系统研究。此外，最近的一些基于标记的方法声称在XLT的标签投影中优于词对齐方法。因此，我们需要重新评估词对齐方法并探索其优化可能性。

Method: 我们重新审视了词对齐（WAs）在标签投影中的应用，系统地研究了低级设计决策对基于翻译的XLT的影响，包括：(i) 在多词跨度之间投影标签的算法，(ii) 减少噪声映射标签数量的过滤策略，以及(iii) 翻译句子的预分词。我们还引入了一种新的投影策略，该策略融合了translate-train和translate-test的预测。

Result: 我们发现这些设计决策对基于翻译的XLT性能有显著影响，并且通过优化选择，基于词对齐的XLT性能至少可以与基于标记的方法相媲美。我们引入的新投影策略显著优于基于标记的投影，并且减少了对低级词对齐设计选择的依赖。

Conclusion: 我们的实验表明，通过优化选择，基于词对齐的XLT可以达到与基于标记的方法相当的性能。我们还引入了一种新的投影策略，该策略融合了translate-train和translate-test的预测，并且显著优于基于标记的投影。此外，我们的集成方法减少了对低级词对齐设计选择的敏感性，从而提高了XLT的鲁棒性。

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [120] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: MuToR is a new approach to multi-token prediction that is simple, effective, and compatible with existing language models. It has shown effectiveness across various tasks in both language and vision domains.


<details>
  <summary>Details</summary>
Motivation: Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning.

Method: MuToR is a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets.

Result: MuToR introduces only a negligible number of additional parameters, requires no architectural changes, and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. It also naturally supports scalable prediction horizons.

Conclusion: MuToR is effective and versatile for various use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains.

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [121] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: This paper explores scaling laws in preference modeling and introduces WorldPM, which improves generalization performance across human preference datasets and shows significant gains when integrated into RLHF pipelines.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes. It aims to explore similar scaling laws in preference modeling.

Method: The paper proposes World Preference Modeling (WorldPM) to emphasize the scaling potential of preference modeling. It collects preference data from public forums and conducts extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters.

Result: The paper observes distinct patterns across different evaluation metrics: adversarial metrics scale up with increased training data and base model size, objective metrics show emergent behavior in larger models, and subjective metrics do not demonstrate scaling trends. WorldPM improves generalization performance across human preference datasets and shows significant improvements when integrated into RLHF pipelines.

Conclusion: WorldPM is effective for preference fine-tuning and can significantly improve the generalization performance across human preference datasets of varying sizes. Integrating WorldPM into RLHF pipelines leads to notable improvements in evaluations.

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [122] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu,Yibo Wang,Hanze Dong,Yuhui Xu,Amrita Saha,Caiming Xiong,Bryan Hooi,Junnan Li*

Main category: cs.CL

TL;DR: 本文提出了一种显式对齐模型元能力的方法，以提高大型推理模型的可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于提示和偶然的'顿悟时刻'，但这些行为的时间和一致性不可预测，限制了大型推理模型的推理能力的可扩展性和可靠性。

Method: 本文提出了一个三阶段的管道：个体对齐、参数空间合并和领域特定的强化学习，以显式对齐模型的演绎、归纳和类比能力。

Result: 该方法在数学、编程和科学基准测试中，相对于指令调优基线提高了超过10%的性能，并且在领域特定的强化学习中进一步提高了2%的性能上限。

Conclusion: 本文提出了一种显式对齐模型的元能力的方法，以提高大型推理模型的可扩展性和可靠性。

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: 本研究提出了一种基于行为的在线监测系统，使用定制的有向图神经网络（DGNN）对列车司机的状态进行分类，结合面部和骨骼特征的模型在三分类模型中准确率最高（80.88%），在二元警觉性分类中准确率超过99%，并引入了包含模拟病理条件的新数据集，以扩大对疲劳和健康相关风险的评估范围。


<details>
  <summary>Details</summary>
Motivation: 传统的列车司机疲劳监测系统（如死人开关）功能有限，无法有效监测司机的警觉性状态，因此需要一种更先进的在线监测系统来提高铁路安全。

Method: 本研究开发了一种基于行为的在线监测系统，使用定制的有向图神经网络（DGNN）对列车司机的状态进行分类。通过消融研究比较了三种特征配置：仅骨骼、仅面部和两者的组合，以优化模型的输入表示。

Result: 实验结果表明，结合面部和骨骼特征的模型在三分类模型中准确率最高（80.88%），在二元警觉性分类中准确率超过99%。此外，本研究还引入了一个新的数据集，首次将模拟病理条件纳入列车司机监测中，扩大了对疲劳和健康相关风险的评估范围。

Conclusion: 本研究通过使用先进的视觉技术进行在线监测，为提高铁路安全迈出了重要一步。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [124] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: This paper presents an online monitoring system for train drivers using a customized DGNN, achieving high accuracy in classifying driver states and introducing a novel dataset with simulated pathological conditions.


<details>
  <summary>Details</summary>
Motivation: Driver fatigue is a significant challenge to railway safety, requiring more advanced systems than traditional ones like the dead-man switch.

Method: The study utilized a Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories. An ablation study compared three feature configurations: skeletal-only, facial-only, and a combination of both.

Result: Combining facial and skeletal features yielded the highest accuracy (80.88%) in the three-class model and over 99% accuracy in binary alertness classification. A novel dataset incorporating simulated pathological conditions was also introduced.

Conclusion: This work enhances railway safety through advanced online monitoring using vision-based technologies.

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [125] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

TL;DR: The paper proposes an OptiGait-LGBM model for person re-identification using skeletal data, introduces the RUET-GAIT dataset, and demonstrates superior performance in accuracy, memory usage, and training time compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in real-world gait recognition such as uncontrolled environments, varying illumination, non-overlapping camera views, and computational efficiency, which current datasets and models fail to handle simultaneously.

Method: The method involves constructing a dataset from landmark positions using a skeletal model approach, introducing the RUET-GAIT benchmark dataset, extracting skeletal joint landmarks, generating numerical datasets, and developing the OptiGait-LGBM gait classification model.

Result: The proposed OptiGait-LGBM model outperforms ensemble techniques like Random Forest and CatBoost in terms of accuracy, memory usage, and training time.

Conclusion: The paper presents a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [126] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的稀疏表示方法SparseMeXt，在高精地图构建任务中超越了密集表示方法，显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏表示在高精地图构建中具有更高的计算效率潜力，但现有方法由于缺乏定制设计而表现不佳，难以与密集表示竞争。因此需要重新审视并改进稀疏表示技术以缩小与密集方法的差距。

Method: 作者提出了专用的网络架构、稀疏-密集分割辅助任务以及基于物理先验的去噪模块，这些技术共同优化了稀疏地图特征提取，并充分利用了几何和语义信息。

Result: SparseMeXt-Tiny、SparseMeXt-Base和SparseMeXt-Large分别达到了55.5%、65.2%和68.9%的mAP，同时保持较高的帧率（32 fps、20 fps以上）。这在nuScenes数据集上展示了其优越性能。

Conclusion: 该研究表明稀疏方法具有未开发的潜力，挑战了对密集表示的传统依赖，重新定义了高精地图构建领域中的效率-性能权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [127] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/abs/2505.08811)
*Shijie Lian,Ziyi Zhang,Laurence Tianruo Yang and,Mengyu Ren,Debin Liu,Hua Li*

Main category: cs.CV

TL;DR: An underwater 3D scene reconstruction method named Tensorized Underwater Gaussian Splatting (TUGS) is proposed, which can accurately simulate light attenuation and backscatter effects in underwater environments with less memory usage and faster rendering speeds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for underwater 3D scene reconstruction are unable to model the interactions between light propagation, water medium, and object surfaces accurately. Moreover, these methods have expensive training and rendering costs that limit their practical application in underwater robotic systems.

Method: The proposed method, TUGS, employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module. This enables accurate simulation of both light attenuation and backscatter effects in underwater environments.

Result: Compared to other NeRF-based and GS-based methods designed for underwater, TUGS can render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated its superior reconstruction quality using a limited number of parameters.

Conclusion: TUGS can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction, making it particularly suitable for memory-constrained underwater UAV applications.

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [128] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

TL;DR: 这篇论文通过一系列实证实验，研究了四种神经网络覆盖度量（主要功能、边界、层次和结构覆盖）之间的关系和模式，使用LeNet、VGG和ResNet作为不同的深度神经网络架构，并分析了模型深度、配置信息和覆盖度量之间的关系。此外，还探讨了修改后的决策/条件覆盖率与数据集大小之间的关系，并提出了三个潜在的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNNs）的广泛应用和进步，不同类型的神经行为引起了关注，出现了各种神经网络覆盖度量。然而，目前缺乏关于这些覆盖度量的经验研究，特别是在分析模型深度、配置信息和神经网络覆盖之间的关系和模式方面。

Method: 选择LeNet、VGG和ResNet作为不同的DNN架构，选取10个深度从5到54层不等的模型，比较和研究不同深度、配置信息和各种神经网络覆盖度量之间的关系。同时，对修改后的决策/条件覆盖率与数据集大小之间的关系进行调查。

Result: 研究揭示了四种覆盖度量之间的关系和模式，以及模型深度、配置信息和覆盖度量之间的联系。此外，还发现了修改后的决策/条件覆盖率与数据集大小之间的关系。

Conclusion: 本研究为理解神经网络覆盖度量之间的关系提供了重要见解，并为未来在DNN模型安全测试方面的研究指明了方向。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [129] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

TL;DR: Score-based generative models (SBM) are leading in image synthesis but can generate NSFW content. Current prevention methods use model knowledge and fine-tuning. This paper explores using external sources for safe outputs in SBMs, presenting an SFW sampler with Conditional Trajectory Correction that uses multimodal models and CLIP for user-defined NSFW classes. Experiments on Stable Diffusion show the SFW sampler reduces explicit content generation while maintaining competitive performance with minimal impact on image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of SBMs generating NSFW content and explore methods to ensure safe outputs without relying solely on the model's own knowledge or requiring fine-tuning.

Method: The method involves implementing a Conditional Trajectory Correction step in the SFW sampler, which uses multimodal models to guide samples away from undesired regions. CLIP is used to allow user-defined NSFW classes, providing flexibility in different settings.

Result: The experiments demonstrate that the SFW sampler effectively reduces the generation of explicit content while performing competitively with fine-tuning-based approaches. The correction scheme has a minor cost with negligible effect on image quality for samples not needing correction.

Conclusion: The study concludes that the SFW sampler is suitable for aligned SBM models and highlights the potential of model-agnostic conditioning for preventing unwanted images.

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [130] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

TL;DR: The paper adapts a Stable Diffusion model with ControlNet to generate high-fidelity satellite imagery for urban planning, conditioned on land use descriptions, infrastructure, and natural environments. It achieves realistic urban landscapes, high FID/KID scores, and positive feedback from planners.


<details>
  <summary>Details</summary>
Motivation: Existing generative AI approaches struggle to produce realistic and practical urban designs at scale.

Method: Adapted a state-of-the-art Stable Diffusion model extended with ControlNet, spatially linking satellite imagery with structured land use and constraint information from OpenStreetMap using data from three major U.S. cities.

Result: Generated realistic and diverse urban landscapes, achieved high FID and KID scores, showed robustness across urban contexts, and received positive qualitative assessments from urban planners and the public.

Conclusion: Established a benchmark for controlled urban imagery generation and demonstrated the potential of generative AI in enhancing planning workflows and public engagement.

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [131] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: This paper proposes a combination of self-supervised training and Multi-Column CNN for crowd counting, and a spatiotemporal model based on VGG19 for crowd anomaly detection, both showing superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in crowd scene analysis including the need for large annotated datasets and the difficulties posed by occluded scenes, nonuniform density, complex backgrounds, and scale invariance in crowd counting; and lighting, environmental conditions, unexpected objects, and scalability in crowd anomaly detection.

Method: For crowd counting, the method combines self-supervised training with Multi-Column CNN. For crowd anomaly detection, a spatiotemporal model based on VGG19 is used where spatial features are learned using CNN and temporal features using LSTM blocks, with dense residual blocks replacing fully connected layers.

Result: The proposed crowd counting model achieves better results on ShanghaiTech and UCF-QNRF datasets in terms of MAE and MSE. The crowd anomaly detection model outperforms other state-of-the-art approaches on the Hockey Fight dataset and SCVD dataset.

Conclusion: The proposed models effectively tackle the challenges in crowd counting and crowd anomaly detection, achieving superior performance over existing state-of-the-art methods.

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [132] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Generative Artificial Intelligence (GenAI) is a transformative force that can potentially solve the challenge of achieving fully autonomous driving, especially Level 5 autonomy. This survey explores the role of GenAI across the autonomous driving stack, including its principles, applications, obstacles and possibilities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive synthesis of the emerging role of Generative Artificial Intelligence (GenAI) in achieving reliable, fully autonomous driving, particularly focusing on Level 5 autonomy.

Method: The paper reviews modern generative modeling techniques such as VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). It maps their applications in areas like image, LiDAR, trajectory, occupancy, video generation, LLM-guided reasoning and decision making. Practical applications include synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI.

Result: Identifies key obstacles such as generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects. Proposes research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence.

Conclusion: This survey serves as a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility.

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [133] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/abs/2505.08882)
*Ali Almakhluk,Uthman Baroudi,Yasser El-Alfy*

Main category: cs.CV

TL;DR: This study develops a system using Raspberry Pi, a camera module, deep learning model, laptop, and cloud service to detect road damages like potholes and cracks, classify their sizes, broadcast warning signals to nearby vehicles, and transmit data to the cloud for improving transportation safety.


<details>
  <summary>Details</summary>
Motivation: Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. The study aims to improve transportation safety by detecting these anomalies and notifying authorities and drivers.

Method: The system uses Raspberry Pi, a camera module, a deep learning model, a laptop, and cloud services. It detects potholes and cracks, classifies their sizes, broadcasts warning signals to nearby vehicles, counts anomalies in real-time, and transmits data to the cloud.

Result: The developed system can successfully detect road anomalies, classify their severity, warn nearby vehicles about severe anomalies, and send data to the cloud for further action by authorities.

Conclusion: By deploying this innovative solution, the study aims to proactively enhance road safety and mitigate potential accidents arising from road hazards, leading to safer road conditions for the whole community.

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [134] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

TL;DR: This study combines image processing and machine learning (neuro-fuzzy and colonial competition algorithms) to improve skin cancer diagnostics using dermoscopic images, achieving 94% accuracy on a dataset of 560 images from the ISIC database.


<details>
  <summary>Details</summary>
Motivation: The increasing incidence of skin cancer, lack of public awareness, and shortage of clinical expertise necessitate advanced diagnostic aids. AI shows promise in distinguishing between malignant and benign skin lesions.

Method: The study utilized a fusion of image processing techniques with machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches, applied to dermoscopic images from the ISIC database.

Result: The method achieved an accuracy of 94% when tested on a dataset of 560 images.

Conclusion: This approach holds significant potential in assisting clinicians with the early detection of melanoma, contributing to advancements in skin cancer diagnostics.

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [135] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

TL;DR: The paper proposes a cocoercive conservative (CoCo) denoiser for Poisson inverse problems in imaging, which can be expansive and improves denoising performance. It introduces a new training strategy using Hamiltonian and spectral regularization, proves the CoCo denoiser's properties, establishes global convergence of PnP methods with this denoiser, and shows superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing Plug-and-play (PnP) methods with deep denoisers require strong convexity or smoothness of the fidelity term and a non-expansive denoiser for convergence, but these assumptions are violated in Poisson inverse problems and can hinder denoising performance.

Method: The authors propose a cocoercive conservative (CoCo) denoiser that may be expansive, leading to improved denoising performance. They leverage the generalized Helmholtz decomposition to introduce a novel training strategy combining Hamiltonian regularization for conservativeness and spectral regularization for cocoerciveness. The CoCo denoiser is proven to be a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior.

Result: The proposed approach outperforms closely related methods in both visual quality and quantitative metrics through extensive experimental results.

Conclusion: The introduction of the cocoercive conservative (CoCo) denoiser addresses challenges in Poisson inverse problems by allowing expansiveness for better denoising, providing theoretical guarantees, and demonstrating superior performance in imaging tasks.

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [136] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

TL;DR: The paper presents Maya, an open-source Multilingual Vision-Language Model (VLM) to improve performance in low-resource languages and varied cultural contexts. It includes a pretraining dataset in eight languages based on LLaVA and a model enhancing cultural and linguistic comprehension.


<details>
  <summary>Details</summary>
Motivation: There is a lack of performance in existing large Vision-Language Models (VLMs) for low-resource languages and varied cultural contexts despite their impressive results on academic benchmarks mainly in widely spoken languages.

Method: The authors introduce Maya, which consists of a multilingual image-text pretraining dataset in eight languages derived from the LLaVA pretraining dataset and a multilingual image-text model supporting these languages to enhance cultural and linguistic understanding.

Result: Maya addresses the limitations of current VLMs by providing support for low-resource languages and improving cultural and linguistic comprehension in vision-language tasks.

Conclusion: The development of Maya contributes towards more inclusive Vision-Language Models capable of handling multiple languages and cultural contexts, with the code being available for further research and development.

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [137] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

TL;DR: The paper introduces DCS-Attention, a novel attention module that selects informative channels for feature extraction in DNNs, improving performance on person Re-ID tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the feature extraction capabilities of deep neural networks by developing an attention mechanism that focuses on selecting the most informative channels. This is motivated by the principle of Information Bottleneck (IB) and aims to improve the accuracy of person re-identification (Re-ID) tasks.

Method: The proposed method involves creating the Differentiable Channel Selection Attention (DCS-Attention) module which performs channel selection in a differentiable manner. It can be integrated with either fixed neural network backbones (DCS-FB) or learnable backbones using Differentiable Neural Architecture Search (DCS-DNAS). The authors derive a novel variational upper bound for the IB loss that can be optimized by SGD and incorporated into the training loss.

Result: Experiments on multiple person Re-ID benchmarks show that DCS-Attention significantly improves the prediction accuracy of DNNs for person Re-ID, demonstrating its effectiveness in learning discriminative features critical to identifying person identities.

Conclusion: The DCS-Attention module successfully enhances the performance of DNNs in person Re-ID tasks by selecting the most informative channels for feature extraction. This approach represents a state-of-the-art advancement in the field.

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [138] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

TL;DR: The paper introduces PRIOR, a vision-language pre-training method that prioritizes image-related tokens through differential weighting in the NTP loss using a reference text-only LLM. This approach improves performance and scaling properties compared to standard NTP.


<details>
  <summary>Details</summary>
Motivation: Standard large vision-language models (LVLMs) pre-training uses next-token prediction (NTP), but since only a small subset of caption tokens directly relate to the visual content, this can unintentionally fit the model to noise and increase hallucination risk.

Method: PRIOR introduces a reference model - a text-only large language model (LLM) trained on captions without image inputs - to weight each token based on its probability for LVLMs training. Tokens directly related to visual inputs receive lower probabilities from the text-only reference LLM and thus are re-weighted during training.

Result: PRIOR shows 19% and 8% average relative improvement in two distinct settings of LVLMs compared to NTP. It also exhibits superior scaling properties with significantly higher scaling coefficients.

Conclusion: PRIOR is a simple yet effective approach for vision-language pre-training that addresses the issue of noise fitting in standard NTP by prioritizing image-related tokens.

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [139] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/abs/2505.08999)
*Wei-Long Tian,Peng Gao,Xiao Liu,Long Xu,Hamido Fujita,Hanan Aljuai,Mao-Li Wang*

Main category: cs.CV

TL;DR: The paper proposes an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking that enhances the transferability and attack effectiveness of adversarial examples.


<details>
  <summary>Details</summary>
Motivation: To reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks.

Method: AMGA integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing. It randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model.

Result: Extensive experimental results on large-scale datasets demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples.

Conclusion: AMGA is an effective method to enhance the transferability and attack effectiveness of adversarial examples in visual tracking.

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [140] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: A multimodal deep learning framework that integrates CGM time-series data, Demographic/Microbiome info, and pre-meal food images to improve caloric intake estimation for Type 2 diabetes management is presented. It reduces RMSRE to 0.2544, surpassing baseline models by over 50%.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of caloric intake is essential for managing Type 2 diabetes but remains challenging. Current methods using Continuous Glucose Monitors (CGMs) lack comprehensive nutritional profiling due to variability among individuals and meals.

Method: The introduced framework uses a combination of attention-based encoding for meal imagery, convolutional feature extraction, multi-layer perceptrons for CGM and Microbiome data, followed by a late fusion strategy for joint reasoning. This allows integration of diverse data types such as CGM time-series, demographic/microbiome information, and pre-meal food images.

Result: Evaluated on a dataset with over 40 participants, the model achieved an RMSRE of 0.2544, showing more than 50% improvement compared to baseline models.

Conclusion: The findings highlight the potential of multimodal sensing in enhancing automated dietary assessment tools, which could be beneficial for chronic disease management like Type 2 diabetes.

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [141] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/abs/2505.09073)
*J. Brennan Peace,Shuowen Hu,Benjamin S. Riggan*

Main category: cs.CV

TL;DR: The paper proposes a novel domain adaptive framework for improving facial recognition performance across large pose differences.


<details>
  <summary>Details</summary>
Motivation: To address the degradation in performance of facial recognition due to substantial perspective (pose) differences between enrollment and query imagery.

Method: The framework uses a shared (joint) attention mapping to emphasize common patterns correlated between 2D facial images and 3D facial data, along with a joint entropy regularizing loss to promote consistency by enhancing correlations among the intersecting 2D and 3D representations.

Result: Evaluated on FaceScape and ARL-VTF datasets, the framework outperforms competitive methods with improvements of at least 7.1% and 1.57% in profile (90°+) TAR @ 1% FAR respectively.

Conclusion: The proposed domain adaptive framework facilitates improved performances across large discrepancies in pose for facial recognition.

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [142] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/abs/2505.09092)
*Yuhang Wang,Abdulaziz Alhuraish,Shengming Yuan,Hao Zhou*

Main category: cs.CV

TL;DR: The paper introduces OpenLKA, the first open large-scale dataset for LKA evaluation and improvement, containing 400 hours of driving data from over 50 vehicle models. It includes CAN bus streams, dash-cam video, Openpilot outputs, and scene annotations.


<details>
  <summary>Details</summary>
Motivation: Lane Keeping Assist (LKA) systems' real-world performance remains underexplored due to proprietary systems and limited data access.

Method: OpenLKA is a dataset that integrates vehicle-internal signals with high-fidelity perception and rich semantic context, collected through extensive road testing and global contributions.

Result: OpenLKA provides a comprehensive platform for benchmarking production LKA systems, identifying safety-critical operational scenarios, and assessing road infrastructure readiness for autonomous driving.

Conclusion: OpenLKA is publicly available and aims to improve the understanding and performance of LKA systems.

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [143] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/abs/2505.09118)
*Dayong Liang,Changmeng Zheng,Zhiyuan Wen,Yi Cai,Xiao-Yong Wei,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces Interaction-augmented Scene Graph Reasoning (ISGR), a framework enhancing vision-language models' ability to reason about complex interactions in visual scenes through three components: dual-stream graph constructor, targeted interaction queries, and long-term memory reinforcement learning. It outperforms baselines on interaction-heavy benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional scene graphs focus only on spatial relationships, limiting vision-language models' ability to reason about complex interactions in visual scenes. Conventional methods produce unfocused relationship sets and fail to form persistent memories for generalizing interaction reasoning.

Method: The ISGR framework includes: 1) A dual-stream graph constructor combining spatial relation extraction with interaction-aware captioning; 2) Targeted interaction queries activating VLMs' latent knowledge of object functionalities; 3) A long-term memory reinforcement learning strategy with an interaction-focused reward function.

Result: Extensive experiments show that ISGR significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, especially improving complex scene understanding tasks.

Conclusion: ISGR enhances VLMs' interactional reasoning capabilities by addressing limitations in conventional detection-to-construction methods and persistent memory formation.

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [144] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/abs/2505.09123)
*Guoying Liang,Su Yang*

Main category: cs.CV

TL;DR: This study successfully applies the Segment Anything Model (SAM) to Camouflaged Object Detection (COD), showing that with appropriate promotion, SAM can work effectively. It introduces a new framework involving Promotion Point Targeting Network (PPT-net) and key point selection (KPS) algorithm to guide segmentation.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of using big models like SAM for COD, overcoming previous claims that SAM is not workable for this task.

Method: Devised a new framework including PPT-net for predicting camouflaged object presences and KPS algorithm for deploying point promotions contrastively to SAM.

Result: Achieves plausible results experimentally over existing methods on 3 data sets under 6 metrics.

Conclusion: Demonstrates an effective off-the-shelf methodology for COD by leveraging SAM, which performs better than designing professional models from scratch.

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [145] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

TL;DR: This paper proposes a lightweight anomaly detection framework based on color features for surveillance video clips in high-risk security tasks, fusing unsupervised KMeans clustering with RGB channel histogram modeling. It successfully identifies anomalous frames in an African country's operation surveillance video without access to original data, showing strong deployability and tactical interpretation value.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models face significant challenges when deployed in high-risk security tasks within unlabeled and data-non-exploitable video intelligence environments.

Method: The method uses a lightweight anomaly detection framework that combines unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutations in key frames of surveillance video clips.

Result: The experiment successfully identified multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference in an African country's operation surveillance video, without needing the original data.

Conclusion: This method is effective for tactical assassination warning, suspicious object screening, and environmental change monitoring. The study highlights the importance of color features as low semantic battlefield signal carriers and suggests future work involving graph neural networks and temporal modeling.

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [146] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/abs/2505.09139)
*Lucas Choi,Ross Greer*

Main category: cs.CV

TL;DR: This paper presents a method for automated prompt refinement in vision-language models (VLMs) using the Contrastive Class Alignment Score (CCAS). It improves object detection accuracy by selecting high-precision prompts without extra model training or labeled data.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) provide flexible object detection through natural language prompts but have performance variability depending on prompt phrasing.

Method: The method generates diverse prompt candidates via a large language model and filters them through CCAS, which is computed using prompt embeddings from a sentence transformer. CCAS ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes.

Result: The approach improves object detection accuracy when evaluated on challenging object categories.

Conclusion: The scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [147] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/abs/2505.09140)
*Zechao Guan,Feng Yan,Shuai Du,Lin Ma,Qingshan Liu*

Main category: cs.CV

TL;DR: A new model TopoDiT-3D is proposed, which integrates topological information for better 3D point cloud generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on local feature extraction but ignore global topological information, such as voids, which are essential for shape consistency and complex geometries.

Method: The method involves designing a bottleneck structure using Perceiver Resampler that incorporates topological information from persistent homology into feature learning and filters out redundant local features.

Result: TopoDiT-3D surpasses state-of-the-art models in visual quality, diversity, and training efficiency.

Conclusion: TopoDiT-3D highlights the significance of rich topological information in 3D point cloud generation and its cooperation with traditional local feature learning.

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [148] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/abs/2505.09155)
*Yichen Shi,Zhuofu Tao,Yuhao Gao,Li Huang,Hongyang Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.CV

TL;DR: Current MLLMs have difficulty understanding circuit schematics due to limited recognition capabilities. This is partly because of the lack of high-quality schematic-netlist training data. The proposed method introduces a novel net detection mechanism based on segmentation with high robustness, which recovers positional information and allows digital reconstruction of schematics. Additionally, AMSnet dataset is expanded to create AMSnet 2.0.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current MLLMs in understanding circuit schematics and overcome the challenges posed by insufficient high-quality schematic-netlist training data.

Method: The proposed method employs a novel net detection mechanism based on segmentation with high robustness. It also recovers positional information for circuit components and nets, enabling digital reconstruction of schematics.

Result: The expansion of AMSnet leads to AMSnet 2.0, which contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information. In contrast, AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.

Conclusion: The novel net detection mechanism and the creation of AMSnet 2.0 aim to improve the ability of MLLMs to understand circuit schematics.

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [149] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为DRRNet的四阶段架构，通过全局上下文特征提取模块、局部细节提取模块以及反向精炼模块等设计，在伪装目标检测任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的伪装目标检测方法在处理与背景颜色、纹理和形状相似的目标时存在困难，容易丢失边缘细节或受类似背景干扰。

Method: 设计了DRRNet，包含四个阶段：全场景特征提取模块获取全局伪装模式；局部细节提取模块补充微观结构信息；双表示生成模块融合全景和局部特征；解码器中的反向精炼模块进行两阶段逆向精炼以增强目标边界连续性和抑制背景干扰。

Result: 实验结果表明，DRRNet在基准数据集上显著超越了当前最先进的方法。

Conclusion: DRRNet有效地解决了伪装目标检测中的挑战，显著提升了检测性能，并且代码已开源。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [150] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/abs/2505.09178)
*Yitao Zhu,Yuan Yin,Zhenrong Shen,Zihao Zhao,Haiyu Song,Sheng Wang,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: The paper introduces UniCAD, a unified architecture that uses pre-trained vision models to efficiently handle 2D and 3D medical images with minimal parameters. It includes low-rank adaptation for efficiency and a plug-and-play modular design for task expansion. Experiments show superior performance and efficiency across 12 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of developing multi-task CAD systems due to growing complexity and lack of an open-source CAD platform in the medical imaging community.

Method: UniCAD leverages pre-trained vision models with two key innovations: (1) Efficiency - Low-rank adaptation strategy introducing only 0.17% trainable parameters; (2) Plug-and-Play - Modular architecture combining frozen foundation model with multiple experts for diverse tasks.

Result: Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency.

Conclusion: UniCAD establishes an open-source platform for sharing lightweight CAD experts, promoting equitable and efficient research ecosystem.

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [151] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/abs/2505.09188)
*Minjun Kim,Jaehyeon Choi,Jongkeun Lee,Wonjin Cho,U Kang*

Main category: cs.CV

TL;DR: 本文提供了一个关于零样本量化（ZSQ）方法的全面概述，定义了ZSQ问题，分类了现有方法，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法需要训练数据，但在许多实际场景中由于隐私、安全或法规限制而不可行，因此需要一种无需真实数据的量化方法。

Method: 作者首先正式定义了ZSQ问题并强调了关键挑战，然后根据数据生成策略将现有的ZSQ方法分为几类，并分析了它们的动机、核心思想和主要收获。

Result: 提供了对ZSQ方法及其最新进展的全面了解，指出了当前方法的局限性，并为未来研究提供了方向。

Conclusion: 本文是迄今为止首个深入调查ZSQ的论文，旨在推动该领域的发展。

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [152] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/abs/2505.09196)
*Tong Li,Lizhi Wang,Hansen Feng,Lin Zhu,Hua Huang*

Main category: cs.CV

TL;DR: In this paper, the authors explore a phenomenon in low-light image enhancement (LLIE) called the gene effect, where resetting certain parameters to random values can improve enhancement performance. They propose a solution named parameter dynamic evolution (PDE), inspired by biological evolution, which employs techniques analogous to gene mutation and recombination.


<details>
  <summary>Details</summary>
Motivation: The motivation is the observation of a peculiar phenomenon in LLIE models, termed the gene effect, where random parameter resettings can outperform learned parameters for some images, limiting model performance.

Method: The proposed method, parameter dynamic evolution (PDE), addresses the gene effect by simulating biological evolution processes. It uses a parameter orthogonal generation technique to implement concepts similar to gene recombination and mutation, allowing adaptation to different images.

Result: Experiments validate the effectiveness of PDE in mitigating the gene effect and improving enhancement performance.

Conclusion: The authors conclude that PDE successfully adapts to varying images and alleviates the gene effect, with plans to release the code publicly.

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [153] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/abs/2505.09251)
*Vineetha Joy,Aditya Anand,Nidhi,Anshuman Kumar,Amit Sethi,Hema Singh*

Main category: cs.CV

TL;DR: A surrogate model using CNN with Huber loss function is proposed for predicting EM responses of metasurface-based RAS, achieving high accuracy and significant reduction in computational time.


<details>
  <summary>Details</summary>
Motivation: Metasurface-based radar absorbing structures (RAS) are crucial for applications like stealth technology and EM shielding. However, conventional design and optimization methods using full wave simulation tools are computationally intensive, time consuming, and require exploration of large design spaces.

Method: The authors propose a surrogate model based on convolutional neural network (CNN) with Huber loss function to predict the reflection characteristics of multi-layered metasurface-based RAS. The model is trained within 1000 epochs.

Result: The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001. It significantly reduced computational time while maintaining high predictive accuracy, as demonstrated by full wave simulations and experiments.

Conclusion: The surrogate model using CNN with Huber loss function successfully accelerates the prediction of EM responses of metasurface-based RAS, offering an efficient alternative to conventional methods.

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [154] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/abs/2505.09252)
*Yinuo Wang,Yue Zeng,Kai Chen,Cai Meng,Chao Pan,Zhouping Tang*

Main category: cs.CV

TL;DR: This study compares zero-shot multi-modal large language models (MLLMs) with traditional deep learning methods in identifying intracranial hemorrhage (ICH) subtypes from non-contrast computed tomography scans. Although MLLMs are inferior in accuracy, they offer enhanced interpretability through interactive capabilities.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely identification of ICH subtypes on non-contrast computed tomography is crucial for prognosis prediction and therapeutic decision-making, but it remains challenging due to low contrast and blurring boundaries.

Method: The study utilized a dataset provided by RSNA, comprising 192 NCCT volumes. It compared various MLLMs (GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2) with conventional deep learning models (ResNet50 and Vision Transformer). Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation.

Result: Traditional deep learning models outperformed MLLMs comprehensively in the ICH binary classification task. For subtype classification, MLLMs also exhibited inferior performance compared to traditional deep learning models.

Conclusion: MLLMs have lower overall accuracy in ICH subtyping compared to deep networks, but they enhance interpretability through language interactions, showing potential in medical imaging analysis.

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [155] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/abs/2505.09256)
*Jaemin Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

TL;DR: This paper introduces Pose-TTA, a novel approach that improves face recognition by aligning faces during the inference phase without additional training. It uses a portrait animator to transfer identities and proposes a weighted feature aggregation strategy to reduce distortions. Experiments show consistent performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing face recognition either rely on training with frontalised images or learning pose-invariant representations, both of which require re-training and testing for each dataset and involve significant effort.

Method: Pose-TTA employs a portrait animator to transfer the identity from a source image to the pose of a driving image at inference time. It also proposes a weighted feature aggregation strategy to handle distortions from synthetic data.

Result: Extensive experiments across diverse datasets and pre-trained models demonstrate that Pose-TTA consistently enhances inference performance in face recognition tasks.

Conclusion: Pose-TTA is an effective method to improve face recognition performance without the need for retraining or fine-tuning underlying models, making it easy to integrate into existing pipelines.

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [156] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

TL;DR: Anomaly detection faces challenges due to scarce anomaly samples. Existing methods using synthetic anomalies have a semantic gap with real-world ones. This paper proposes AnoGen, a few-shot Anomaly-driven Generation method that uses a diffusion model and only a few real anomalies to generate realistic anomalies. It involves three stages: learning anomaly distribution, guiding the diffusion model for generation, and training an anomaly detection model. Experiments on MVTec dataset show improved performance in both classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is crucial but difficult because of the lack of anomaly samples. Current approaches to synthesize anomalies do not bridge the semantic gap with real-world anomalies effectively.

Method: The AnoGen method consists of three stages: (1) Learning the anomaly distribution from a few real anomalies and embedding the knowledge; (2) Guiding the diffusion model using the embedding and bounding boxes to generate realistic anomalies on specific objects/textures; (3) Training a weakly-supervised anomaly detection model using the generated anomalies.

Result: Experiments on the MVTec dataset demonstrate simultaneous improvements in both anomaly classification and segmentation tasks. Notably, DRAEM and DesTSeg achieved 5.8% and 1.5% improvements in AU-PR metric on the segmentation task respectively.

Conclusion: The proposed AnoGen method successfully generates realistic and diverse anomalies using a few real samples, improving the performance of anomaly detection models in both classification and segmentation tasks.

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [157] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: The paper presents OneNIP, a method for anomaly detection that reconstructs normal features and restores anomaly features using one normal image prompt. It also introduces a supervised refiner to improve pixel-level anomaly segmentation.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised reconstruction models may perfectly reconstruct both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Also, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space.

Method: OneNIP reconstructs normal features and restores anomaly features with just one normal image prompt. Additionally, a supervised refiner is proposed that regresses reconstruction errors by using both real normal and synthesized anomalous images.

Result: OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA.

Conclusion: OneNIP effectively boosts unified anomaly detection performance and significantly improves pixel-level anomaly segmentation.

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [158] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: This paper proposes MetaUAS, a one-prompt Meta-learning framework for universal anomaly segmentation using a pure vision model without pre-trained visual-language models. It significantly outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: Current zero- and few-shot visual anomaly segmentation methods rely on powerful vision-language models that use manually designed textual prompts, but visual representations are inherently independent of language.

Method: The authors present a novel paradigm unifying anomaly segmentation into change segmentation, propose the MetaUAS framework trained on large-scale synthetic image pairs, and introduce a soft feature alignment module to handle geometrical variations.

Result: MetaUAS effectively and efficiently segments any anomalies with only one normal image prompt and outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods.

Conclusion: This work demonstrates the potential of a pure visual foundation model for universal visual anomaly segmentation and provides an alternative to widely used vision-language models.

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [159] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/abs/2505.09274)
*Fares Bougourzi,Abdenour Hadid*

Main category: cs.CV

TL;DR: Medical imaging segmentation faces challenges like data accessibility, annotation complexity, and domain adaptation. Recent advancements in Generative AI, Few-Shot Learning, Foundation Models, and Universal Models provide promising solutions. This survey reviews these methodologies, their applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in medical image segmentation such as data accessibility, annotation complexity, structural variability, and privacy constraints.

Method: Exploration of cutting-edge advancements including Generative AI, Few-Shot Learning, Foundation Models, and Universal Models for medical image segmentation.

Result: Provided a comprehensive overview of theoretical foundations, state-of-the-art techniques, and recent applications of these methods.

Conclusion: Discussed inherent limitations, unresolved issues, and outlined future research directions to enhance practicality and accessibility of segmentation models.

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [160] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

TL;DR: This paper presents a new dataset for predicting butterfly species presence from satellite data in the UK and develops a soft, supervised contrastive regularisation loss to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing availability of large-scale citizen-science wildlife observation data has led to interest in predicting multi-species presence directly from satellite images for scalable biodiversity monitoring.

Method: The authors experimentally optimize a Resnet-based model to predict multi-species presence from 4-band satellite images and develop a soft, supervised contrastive regularisation loss tailored to probabilistic labels like species-presence data.

Result: The Resnet-based model outperforms the mean rate baseline particularly for locations with high species biodiversity and the contrastive regularisation method improves prediction accuracy.

Conclusion: The new dataset and contrastive regularisation method contribute towards accurately predicting species biodiversity from remote sensing data which is crucial for efficient biodiversity monitoring.

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [161] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

TL;DR: An ROI-based neural video compression model using 2D Gaussian Splatting is proposed, which speeds up encoding time by 88% through a content-aware initialization strategy and a novel Gaussian inter-frame redundancy-reduction mechanism.


<details>
  <summary>Details</summary>
Motivation: Traditional video codecs have been used for decades, but recent advancements in deep learning-based techniques offer better adaptability and higher compression efficiency. However, the computational demands of neural video codecs limit their use in real-time applications like video conferencing.

Method: The researchers developed a region-of-interest (ROI) based neural video compression model leveraging 2D Gaussian Splatting. They introduced a content-aware initialization strategy and a new Gaussian inter-frame redundancy-reduction mechanism to speed up the encoding process.

Result: This approach reduces encoding time by 88% compared to previous Gaussian splatting-based image codecs, making it feasible for video codec solutions.

Conclusion: This work presents an innovative solution in the neural video codec field, offering significant improvements in encoding speed while maintaining quality, thus expanding potential applications in video streaming platforms.

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [162] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

TL;DR: 通过自监督学习开发可扩展的医学视觉基础模型，研究了模型大小、训练算法、数据大小和成像方式的扩展行为，并引入了包含2100万张生物医学图像的大规模数据集BioVFM-21M。提出的大规模医学视觉基础模型BioVFM在12个医学基准上超越了先前的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管在通用任务的扩展行为上有广泛的研究，但医学图像与自然数据有显著差异，缺乏对医学领域扩展行为的广泛理解，因此需要探索开发可扩展医学视觉基础模型的关键因素。

Method: 通过自监督学习探索跨模型大小、训练算法、数据大小和成像模态的扩展行为，介绍了一个大规模生物医学图像数据集BioVFM-21M，涵盖了广泛的生物医学图像模态和解剖结构。

Result: 观察到扩展确实提供了好处，但因任务而异；提出了一个预训练的大规模医学视觉基础模型BioVFM，它在12个医学基准上超越了以前最先进的基础模型。

Conclusion: 扩展对于追求更好的性能是有益的，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医学基础模型的关键考虑因素。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [163] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: The paper introduces MultiviewVLM, a vision-language model for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. It outperforms existing methods and can be adapted to various real-world applications.


<details>
  <summary>Details</summary>
Motivation: To create a model that can effectively learn representations of facial emotions from 3D/4D data in an unsupervised manner, capturing shared information across multi-views without explicit supervision.

Method: The method integrates pseudo-labels from generated textual prompts, proposes a joint embedding space for multiview alignment, employs a novel multiview contrastive learning strategy with stable positive-negative pair sampling, and uses a gradient-friendly loss function for optimization.

Result: Extensive experiments show that MultiviewVLM outperforms existing state-of-the-art methods in unsupervised contrastive multiview representation learning of facial emotions.

Conclusion: MultiviewVLM is a successful vision-language model for unsupervised learning of facial emotion representations from 3D/4D data, and it can be easily adapted to various real-world applications.

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [164] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

TL;DR: Marigold，一种从预训练的潜在扩散模型中提取知识并将其适应于密集图像分析任务的方法，具有最少的架构修改和强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在计算机视觉方面取得了成功，但在数据稀缺的情况下，高质量的预训练模型对于有效的迁移学习至关重要。当前主要依赖图像分类和自监督学习进行预训练，但文本到图像生成模型展现出了对视觉世界的深刻理解，这为新的预训练方法提供了可能性。

Method: 提出了一种名为Marigold的条件生成模型家族及其微调协议，该方法从预训练的潜在扩散模型（如Stable Diffusion）中提取知识，并将其应用于密集图像分析任务（如单目深度估计、表面法线预测和内在分解）。此方法仅需对预训练模型的架构进行少量修改，并使用小型合成数据集在单个GPU上进行训练。

Result: Marigold展示了最先进的零样本泛化能力，在密集图像分析任务中表现出色。

Conclusion: Marigold提供了一种有效利用预训练文本到图像生成模型知识的方法，适用于多种密集图像分析任务，且具有高效的训练需求和出色的零样本泛化性能。

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [165] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

TL;DR: The paper introduces RobustSpring, a new dataset and benchmark for evaluating the robustness of optical flow, scene flow, and stereo models to image corruptions. It applies 20 different corruptions to the Spring dataset, creating 20,000 challenging images. The benchmark includes a new metric for measuring corruption robustness and allows public evaluation of both accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for optical flow, scene flow, and stereo vision algorithms focus on model accuracy rather than robustness to real-world image corruptions like noise or rain.

Method: RobustSpring applies 20 different image corruptions in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, generating a suite of 20,000 corrupted images. A new corruption robustness metric is introduced for comparing model robustness.

Result: RobustSpring enables public two-axis evaluations of both accuracy and robustness via integration with the Spring benchmark. Initial model benchmarking shows that accurate models are not necessarily robust and robustness varies widely by corruption type.

Conclusion: RobustSpring is a new computer vision benchmark treating robustness as a first-class citizen, fostering the development of models that combine accuracy with resilience.

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [166] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/abs/2505.09372)
*Siyuan Yan,Xieji Li,Ming Hu,Yiwen Jiang,Zhen Yu,Zongyuan Ge*

Main category: cs.CV

TL;DR: Dermatological diagnosis is complex and requires combining visual features with clinical knowledge. Despite the advancements of vision-language pretraining (VLP) in medical AI, its application in dermatology is restricted by text length limitations and lack of structured texts. This paper presents MAKE, a Multi-Aspect Knowledge-Enhanced VLP framework for zero-shot dermatological tasks. It includes a multi-aspect contrastive learning strategy, a fine-grained alignment mechanism, and a diagnosis-guided weighting scheme. Pretrained on 403,563 image-text pairs, MAKE surpasses existing VLP models on various tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to overcome the limitations of current VLP models in dermatology, particularly addressing the issues of text length constraints and unstructured texts, which hinder effective dermatological diagnosis.

Method: The method involves introducing MAKE, a framework that incorporates three key components: a multi-aspect contrastive learning strategy that breaks down clinical narratives into sub-texts using large language models, a fine-grained alignment mechanism that links subcaptions with diagnostic image features, and a diagnosis-guided weighting scheme that prioritizes sub-captions based on their clinical significance.

Result: MAKE significantly outperforms state-of-the-art VLP models across eight datasets on tasks such as zero-shot skin disease classification, concept annotation, and cross-modal retrieval.

Conclusion: The authors conclude that MAKE effectively addresses the challenges posed by dermatological diagnosis through its innovative approach, setting a new benchmark for zero-shot dermatological tasks.

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [167] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/abs/2505.09379)
*Ali Rida Sahili,Najett Neji,Hedi Tabia*

Main category: cs.CV

TL;DR: 文本驱动的运动生成提供了一种强大且直观的方法，可以直接从自然语言创建人类动作。本文回顾了传统的运动合成视角，并对现代文本到运动生成方法进行了全面和结构化的调查。此外，还探讨了最常用的数据库、评估方法和最近的基准测试。


<details>
  <summary>Details</summary>
Motivation: 通过文本驱动的运动生成，可以提供一种强大且直观的方法来直接从自然语言创建人类动作，这种方法不需要预定义的运动输入，提供了灵活且易访问的动画角色控制方式。

Method: 从架构和运动表示两个互补的角度对现代文本到运动生成方法进行分类和调查，包括基于VAE、扩散和混合模型的方法，以及离散和连续运动生成策略的区别。

Result: 本文对文本到运动生成领域进行了全面的回顾，明确了当前的研究状态，指出了关键挑战和局限性，并强调了未来探索的有希望的方向。

Conclusion: 这项工作为研究人员和从业者提供了一个有价值的起点，以推动语言驱动的人类运动合成的边界。

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [168] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

TL;DR: NeoMedSys, a radiology software platform, was evaluated for three months in real-world clinical settings. It enabled iterative improvements in an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage detection, significantly enhancing its diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the feasibility and effectiveness of running NeoMedSys in real-world clinical settings and to improve the performance of VIOLA-AI, an in-house developed AI model for intracranial hemorrhage detection.

Method: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. The study used clinical cases from two sites in Norway to assess ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining.

Result: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. The classification sensitivity rose to 90.3% (from 79.2%), and specificity reached 89.3% (from 80.7%). The AUC for bleed detection improved to 0.949 (from 0.873).

Conclusion: The study concludes that NeoMedSys can enable efficient deployment and refinements of AI models in radiology, highlighting the value of real-time radiologist feedback.

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [169] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

TL;DR: Federated semantic segmentation is improved by a new framework FedSaaS which addresses class-consistency representation problem, enhancing average segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing federated semantic segmentation research often neglects fine-grained class relationships within the semantic space, leading to ambiguities in class representation when dealing with heterogeneous problems like domain shift.

Method: The paper proposes FedSaaS, a novel federated segmentation framework that uses class exemplars as a criterion for both local- and global-level class representations. On the server side, class prototypes are modeled from uploaded exemplars to supervise the global branch of clients. On the client side, an adversarial mechanism harmonizes contributions of global and local branches. Multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space.

Result: Extensive experiments on driving scene segmentation datasets show that the proposed framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy.

Conclusion: FedSaaS effectively addresses the class-consistency representation problem in federated semantic segmentation, offering a significant improvement in performance.

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [170] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/abs/2505.09406)
*Yue Wen,Liang Song,Yijia Liu,Siting Zhu,Yanzi Miao,Lijun Han,Hesheng Wang*

Main category: cs.CV

TL;DR: This paper proposes FreeDriveRF, a method for reconstructing dynamic driving scenes using only sequential RGB images without requiring poses inputs. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss to better constrain the dynamic modeling process.


<details>
  <summary>Details</summary>
Motivation: Dynamic scene reconstruction for autonomous driving is crucial for vehicles to perceive and interpret complex scene changes more precisely. However, existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity.

Method: The proposed method, FreeDriveRF, reconstructs dynamic driving scenes using only sequential RGB images. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss utilizing optical flow. Additionally, it incorporates estimated dynamic flow to constrain the pose optimization process.

Result: Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of FreeDriveRF in dynamic scene modeling for autonomous driving.

Conclusion: FreeDriveRF successfully reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs, showing superior performance in dynamic scene modeling for autonomous driving.

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [171] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/abs/2505.09413)
*Ma Changfeng,Bi Ran,Guo Jie,Wang Chongjun,Guo Yanwen*

Main category: cs.CV

TL;DR: This paper introduces a new point cloud rendering method that predicts 2D Gaussians from point clouds, which can generalize to multiple datasets and categories without needing categorical priors, dense point clouds, or additional refinements.


<details>
  <summary>Details</summary>
Motivation: Current learning-based methods for rendering rely on categorical priors, dense point clouds, or extra refinements, which limits their flexibility and generalization ability.

Method: The method uses two identical modules with an entire-patch architecture. These modules normalize and initialize 2D Gaussians based on point cloud information (normals, colors, distances). Splitting decoders then refine these initial Gaussians by duplicating them for more accurate predictions. This approach accommodates sparse point clouds effectively.

Result: Extensive experiments on various datasets show that this method outperforms existing techniques in terms of superiority and generalization, achieving state-of-the-art (SOTA) performance.

Conclusion: The proposed method provides a novel way to render point clouds using 2D Gaussian prediction, offering direct generalization across different categories without requiring dense point clouds or additional image refinements.

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [172] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/abs/2505.09415)
*Hongyang Wang,Yichen Shi,Zhuofu Tao,Yuhao Gao,Liepiao Zhang,Xun Lin,Jun Feng,Xiaochen Yuan,Zitong Yu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出FaceShield，一种用于面部防欺骗的多模态大语言模型，包括预训练和监督微调数据集。该模型在四个面部防欺骗任务中显著优于以前的深度学习模型和通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有的面部防欺骗方法将任务视为分类问题，缺乏可解释性和预测结果背后的推理。虽然多模态大语言模型（MLLMs）在视觉任务中表现出强大的感知、推理和决策能力，但目前还没有专门为面部防欺骗任务设计的通用和全面的MLLM和数据集。

Method: 提出了FaceShield，一个专门用于面部防欺骗的MLLM，以及相应的预训练和监督微调数据集FaceShield-pre10K和FaceShield-sft45K。使用spoof-aware vision perception (SAVP)，结合原始图像和基于先验知识的辅助信息，并采用prompt-guided vision token masking (PVTM)策略来随机遮蔽视觉token，从而提高模型的泛化能力。

Result: 在三个基准数据集上进行了广泛的实验，结果表明FaceShield在四个面部防欺骗任务（即粗粒度分类、细粒度分类、推理和攻击定位）中显著优于以前的深度学习模型和通用MLLMs。

Conclusion: FaceShield为面部防欺骗任务提供了一个新的解决方案，具有判断面部真实性、识别欺骗攻击类型、提供推理依据和检测攻击区域的能力。

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [173] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2505.09422)
*Xiangyuan Peng,Yu Wang,Miao Tang,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 提出了一种名为MoRAL的运动感知多帧4D雷达和LiDAR融合框架，通过Motion-aware Radar Encoder（MRE）补偿移动物体的帧间雷达错位，并利用Motion Attention Gated Fusion（MAGF）模块将雷达运动特征与LiDAR特征结合以聚焦动态前景物体。实验结果表明，MoRAL在View-of-Delft数据集上优于现有方法，在整体区域和驾驶走廊中的最高mAP分别为73.30%和88.68%，并对行人和骑自行车者有最佳检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多帧雷达点云的4D雷达和LiDAR融合方法虽然有效缩小了点密度差距，但常常忽略由于物体运动导致的帧间雷达点云错位问题，且未充分利用4D雷达提供的物体动态信息。

Method: 设计了一个运动感知雷达编码器（MRE），用于补偿来自移动物体的帧间雷达错位；然后采用运动注意门控融合（MAGF）模块，将雷达运动特征整合进来，引导LiDAR特征关注动态前景物体。

Result: 在View-of-Delft数据集上的广泛评估表明，MoRAL的表现优于现有方法，整体区域的最高mAP为73.30%，驾驶走廊为88.68%。此外，对于行人的AP为69.67%，驾驶走廊中骑自行车者的AP为96.25%。

Conclusion: 所提出的MoRAL框架显著提高了对动态前景物体的检测性能，特别是在自动驾驶系统所需的交通参与者准确检测方面表现出色。

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [174] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/abs/2505.09433)
*Jiahao Zhu,Kang You,Dandan Ding,Zhan Ma*

Main category: cs.CV

TL;DR: SerLiC is a serialization-based neural compression framework that transforms 3D LiDAR point clouds into 1D sequences for effective reflectance analysis, achieving significant volume reduction and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods.

Method: SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization. Each point is tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance. Mamba is incorporated with a dual parallelization scheme for efficient sequential modeling.

Result: Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. A lightweight version of SerLiC achieves > 10 fps with just 111K parameters.

Conclusion: SerLiC provides an effective approach to compress LiDAR reflectance data with significant volume reduction and high efficiency, making it attractive for real-world applications.

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [175] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

TL;DR: Endo-CLIP是一种新颖的自监督框架，通过三阶段方法（清理、调谐和统一）改进了对比语言-图像预训练（CLIP），以应对内窥镜图像分析中的挑战。该模型在零样本和少样本息肉检测和分类任务中显著优于现有最先进的预训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前的图像-文本结肠镜记录预训练方法存在非信息背景图像、复杂的医学术语和多病变描述模糊等问题，这些问题限制了内窥镜图像分析的性能提升。

Method: Endo-CLIP采用三阶段框架：1) 清理阶段 - 移除背景帧；2) 调谐阶段 - 利用大型语言模型提取临床属性用于细粒度对比学习；3) 统一阶段 - 使用患者级别的交叉注意力解决多息肉模糊问题。

Result: 实验表明，Endo-CLIP在零样本和少样本息肉检测与分类任务中显著超越了现有的最先进的预训练方法。

Conclusion: Endo-CLIP为更准确和临床上更相关的内窥镜分析铺平了道路。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [176] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/abs/2505.09450)
*Yuelin Zhang,Qingpeng Ding,Long Lei,Yongxuan Feng,Raymond Shing-Yan Tang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: MrTrack, an aspiration needle tracker with a mamba-based register mechanism for ultrasound-guided FNA biopsy, outperforms state-of-the-art trackers in accuracy, robustness, and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Ultrasound-guided fine needle aspiration (FNA) biopsy lacks an effective tracker for needles with rapid reciprocating motion. To address this gap, MrTrack is proposed.

Method: MrTrack uses a Mamba-based register extractor to distill global context from historical search maps and stores them in a register bank. A Mamba-based register retriever retrieves these temporal cues when current vision features are unusable due to rapid motion or imaging degradation. Additionally, a self-supervised register diversify loss encourages feature diversity and dimension independence within the learned register.

Result: Comprehensive experiments on motorized and manual aspiration datasets show that MrTrack surpasses state-of-the-art trackers in terms of accuracy, robustness, and inference efficiency.

Conclusion: MrTrack successfully addresses the challenge of tracking aspiration needles with rapid reciprocating motion in ultrasound-guided FNA biopsies, demonstrating superior performance.

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [177] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/abs/2505.09455)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.CV

TL;DR: This paper enhances spatio-temporal action detection (STAD) in soccer analytics by adding a denoising sequence transduction task, using game-level reasoning and Transformer-based models to improve precision and recall.


<details>
  <summary>Details</summary>
Motivation: Current STAD methods lack contextual understanding when operated in high-recall, low-precision regimes for soccer event extraction from broadcast videos, leading to many false positives that could be resolved by considering broader sequences of actions and game-state information.

Method: The method involves processing sequences of noisy, context-free player-centric predictions alongside clean game state information using a Transformer-based encoder-decoder model, modeling extended temporal context and reasoning jointly over team-level dynamics.

Result: This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video.

Conclusion: The enhancement through the addition of a denoising sequence transduction task complements existing pixel-based methods and generates 'denoised' sequences of actions.

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [178] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

TL;DR: Vision transformers use self-attention to capture long-range dependencies but existing position encoding techniques fail to capture semantic-aware positional relationships. This paper proposes 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) which dynamically adapts position representations leveraging local content, enhancing model's generalization and translation equivariance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing position encoding techniques that primarily focus on 1D linear position relationship and neglect semantic similarity between distant patches in images.

Method: Propose a novel position encoding method called 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates.

Result: Enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches.

Conclusion: Integrating SaPE^2 into vision transformers bridges the gap between position encoding and perceptual similarity, improving performance on computer vision tasks.

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [179] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2505.09484)
*Yingjie Ma,Xun Lin,Zitong Yu,Xin Liu,Xiaochen Yuan,Weicheng Xie,Linlin Shen*

Main category: cs.CV

TL;DR: This paper presents MMDA framework for Face Anti-Spoofing (FAS) which enhances cross-modal alignment generalization and multimodal detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current multimodal FAS methods face challenges in effective generalization due to modality-specific biases and domain shifts.

Method: The MMDA framework uses CLIP's zero-shot generalization capability, MD2A module for mitigating domain and modality noise, RS2 strategy for aligning multi-domain multimodal data into a generalized representation space, and U-DSA module for enhancing adaptability of representations while maintaining generalization performance.

Result: Experimental results on four benchmark datasets under different evaluation protocols show that the MMDA framework surpasses existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy.

Conclusion: The MMDA framework significantly improves generalization capabilities and complex representation abilities for FAS.

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [180] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Flash-VL 2B的新方法，用于优化视觉-语言模型(VLMs)，使其在不影响准确性的情况下实现超低延迟和高吞吐量。通过多项技术改进，Flash-VL 2B在多个基准测试中表现出色，是实时应用的理想选择。


<details>
  <summary>Details</summary>
Motivation: 随着对实时视觉-语言处理需求的增加，传统的VLMs无法满足超低延迟和高吞吐量的要求，因此需要一种新的优化方法来解决这一问题。

Method: Flash-VL 2B采用了一系列优化策略，包括定制架构设计、令牌压缩机制、数据整理、训练方案以及一种称为隐式语义拼接的新型图像处理技术。这些技术共同作用以减少处理时间并保持模型性能。

Result: 通过对11个标准VLM基准的广泛评估，Flash-VL 2B在速度和准确性上均达到了最先进的水平。

Conclusion: Flash-VL 2B为资源受限环境和大规模实时应用提供了一个有前景的解决方案，能够同时实现超低延迟和高吞吐量。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [181] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/abs/2505.09528)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: The paper integrates conformal prediction with approximate posterior sampling to establish bounds on FRIQ metrics for imaging inverse problems, ensuring safety in applications like medical imaging.


<details>
  <summary>Details</summary>
Motivation: In imaging inverse problems, it's crucial to assess the closeness of the recovered image to the true image using FRIQ metrics. This is particularly vital in safety-critical domains such as medical imaging where misdiagnosis must be avoided.

Method: Combine conformal prediction with approximate posterior sampling to create bounds on FRIQ that hold up to a user-specified error probability.

Result: Successfully demonstrated the approach on image denoising and accelerated MRI problems.

Conclusion: This method provides reliable FRIQ bounds, contributing to safer imaging applications.

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [182] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: This paper explores the use of time event cameras for contact-free heart rate monitoring by extracting cardiac pulse signals from facial recordings using a CNN model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the advantages of event cameras, such as low latency and high temporal resolution, for remote physiological signal extraction, specifically heart rate monitoring.

Method: A supervised CNN model is trained end-to-end to extract cardiac signals from a 2D representation of the event stream generated by an event camera recording faces. Model performance is assessed based on heart rate accuracy.

Result: The model achieved an RMSE of 3.32 bpm when compared to a baseline model with an RMSE of 2.92 bpm using standard camera frames. Event frames at higher FPS (60 and 120) outperformed standard 30 FPS camera frames with RMSEs of 2.54 and 2.13 bpm respectively.

Conclusion: Event cameras effectively preserve physiological cardiac information in the facial region, showing potential for remote heart rate monitoring with improved accuracy at higher frame rates.

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [183] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/abs/2505.09562)
*Nicola Marinello,Simen Cassiman,Jonas Heylen,Marc Proesmans,Luc Van Gool*

Main category: cs.CV

TL;DR: This paper presents a new framework for 3D panoptic scene completion which includes Object Module and Panoptic Module to improve existing models.


<details>
  <summary>Details</summary>
Motivation: 3D panoptic scene completion is underexplored despite its importance in autonomous vehicles for path planning and decision-making.

Method: The authors propose a novel framework that integrates Object Module and Panoptic Module with existing 3D occupancy and scene completion methods. This approach leverages annotations in occupancy benchmarks to learn individual object shapes as a differentiable problem.

Result: The proposed method aims to advance the state of the art in 3D panoptic scene completion, although specific experimental results are not provided in the abstract.

Conclusion: This work introduces a new framework for 3D panoptic scene completion that extends current models using Object and Panoptic Modules.

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [184] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/abs/2505.09564)
*Anne-Marie Rickmann,Stephanie L. Thorn,Shawn S. Ahn,Supum Lee,Selen Uman,Taras Lysyy,Rachel Burns,Nicole Guerrera,Francis G. Spinale,Jason A. Burdick,Albert J. Sinusas,James S. Duncan*

Main category: cs.CV

TL;DR: This paper explores the use of foundation models to create accurate pseudo-labels for porcine cardiac CT segmentation without manual annotations, using a self-training approach that improves segmentation accuracy and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Cardiac image segmentation is crucial in many cardiac analysis tasks but deep learning advancements have been limited in pre-clinical imaging, particularly with porcine models. The anatomical differences between species lead to domain shifts complicating model transfer from human to pig data.

Method: The researchers investigate whether foundation models can generate accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels without manually annotated pig data.

Result: The self-training process enhances segmentation accuracy and smooths out temporal inconsistencies across consecutive frames.

Conclusion: While the results are encouraging, there is still room for improvement by incorporating more sophisticated self-training strategies and exploring additional foundation models and other cardiac imaging technologies.

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [185] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: 研究统一图像理解与生成的框架，探索自回归和扩散模型在多模态设置中的应用，提出使用扩散Transformer生成CLIP图像特征的新方法，以及分阶段预训练策略，并开发了高质量数据集BLIP3o-60k和模型BLIP3-o，开源所有资源。


<details>
  <summary>Details</summary>
Motivation: 尽管图像理解的设计选择已被广泛研究，但结合图像生成的统一框架的最佳模型架构和训练方法仍需深入探索。自回归和扩散模型因其高质量生成能力和可扩展性成为研究重点。

Method: 1. 使用扩散Transformer生成语义丰富的CLIP图像特征，而非传统的VAE-based表示。
2. 采用顺序预训练策略：先进行图像理解训练，再进行图像生成训练。
3. 创建高质量指令调优数据集BLIP3o-60k，涵盖多种场景、对象和人类姿态等。
4. 基于上述方法开发BLIP3-o系列多模态模型。

Result: BLIP3-o在多个流行基准测试中表现出色，涵盖图像理解和生成任务。证明了新方法在提高训练效率和生成质量方面的有效性。

Conclusion: 本研究通过创新的模型设计、训练方法和数据集，推动了统一多模态模型的发展，并全面开放源代码和资源以促进未来研究。

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [186] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/abs/2505.09571)
*Guillermo Gomez-Trenado,Pablo Mesejo,Oscar Cordón,Stéphane Lathuilière*

Main category: cs.CV

TL;DR: The paper presents SAGE(Self-Attention Guidance for image Editing), a new technique using pre-trained diffusion models to edit images more efficiently and accurately. It solves the problem of poor reconstructions or high computation in current methods by incorporating a guidance mechanism based on self-attention layers. Through evaluations and user studies, SAGE proves superior in image editing.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods either require heavy computation or lead to poor reconstructions, making it difficult to achieve efficient and high-quality image editing.

Method: SAGE is built upon DDIM algorithm and introduces a novel guidance mechanism that uses the self-attention layers of the diffusion U-Net. This mechanism computes reconstruction objectives from attention maps during the inverse DDIM process, allowing efficient reconstruction of unedited regions without needing to reconstruct the entire input image.

Result: SAGE outperforms other methods in both quantitative and qualitative evaluations. In a user study with 47 participants, all preferred SAGE over competing methods. Additionally, SAGE ranks first in seven, second in two, and third in one out of ten quantitative analyses.

Conclusion: SAGE successfully addresses the challenges in image editing with its innovative use of self-attention guidance, demonstrating superior performance compared to existing methods.

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [187] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

TL;DR: The paper proposes a Variational VQA approach to improve calibration and reliability of multimodal models in Visual Question Answering without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Multimodal models for Visual Question Answering (VQA) have major reliability concerns as they can be overconfident and miscalibrated, particularly in out-of-distribution settings. Existing solutions mainly focus on unimodal models.

Method: The authors employ a variational algorithm called IVON instead of AdamW for fine-tuning vision-language models, yielding a posterior distribution over model parameters.

Result: The Variational VQA approach improves calibration and abstentions without losing the accuracy of AdamW. It reduces Expected Calibration Error by more than 50% compared to AdamW baseline and raises Coverage by 4% vs. SOTA (for a fixed risk of 1%). In OOD settings with 50% test cases being OOD, it achieves 8% Coverage improvement vs. SOTA (@ 1% risk).

Conclusion: Variational learning is presented as a feasible method to enhance the reliability of multimodal models.

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [188] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/abs/2505.09608)
*Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: This paper introduces a diffusion-based method for precise control over light sources in an image, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing relighting methods either rely on multiple input views or fail to provide explicit control over light changes.

Method: The method fine-tunes a diffusion model on real raw photograph pairs and synthetically rendered images, leveraging the linearity of light to synthesize image pairs depicting controlled light changes.

Result: The model can achieve precise illumination changes with explicit control over light intensity and color, and achieves compelling light editing results that outperform existing methods based on user preference.

Conclusion: The presented diffusion-based method provides fine-grained, parametric control over light sources in an image, showing superior performance compared to existing methods.

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [189] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/abs/2505.09615)
*Yung-Hsuan Lai,Janek Ebbers,Yu-Chiang Frank Wang,François Germain,Michael Jeffrey Jones,Moitreya Chatterjee*

Main category: cs.CV

TL;DR: This paper addresses the challenge of Audio-Visual Video Parsing (AVVP) in a weakly-supervised setting, where training data lacks detailed annotations. It proposes a novel method called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV), which improves upon existing methods by considering inter-segment dependencies and reducing bias towards absent labels. The approach incorporates uncertainty estimation for pseudo-labels and uses feature mixup based training regularization. Empirical results demonstrate that UWAV outperforms state-of-the-art methods on two datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current weakly-supervised AVVP techniques, particularly their inability to effectively capture inter-segment dependencies and their bias towards predicting absent labels. This limits their performance in localizing both uni-modal and multi-modal events in videos.

Method: The proposed method, UWAV, introduces an innovative approach to generate segment-level pseudo-labels while factoring in the uncertainty associated with these labels. It also incorporates a feature mixup based training regularization technique to improve the training process.

Result: Empirical evaluations show that UWAV surpasses state-of-the-art methods in the AVVP task across multiple metrics on two different datasets, demonstrating its effectiveness and generalizability.

Conclusion: The paper concludes that the proposed UWAV method successfully overcomes the limitations of previous approaches by considering inter-segment dependencies and reducing bias. The incorporation of uncertainty estimation and feature mixup based training regularization leads to improved performance in weakly-supervised AVVP.

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [190] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales,Ayah Elsayed,Debbie Zhao,Filip Loncaric,Ainhoa Aguado,Mireia Masias,Gina Quill,Marc Ramos,Ada Doltra,Ana Garcia,Marta Sitges,David Marlevi,Alistair Young,Martyn Nash,Bart Bijnens,Oscar Camara*

Main category: cs.CV

TL;DR: The study introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium (LA), providing robust automated segmentations and conducting a comprehensive assessment of hemodynamic parameters.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional ultrasound analysis and improve understanding of left atrium hemodynamics using 4D Flow MRI, despite challenges such as low velocities, limited spatial resolution, and lack of dedicated computational frameworks.

Method: Development of an open-source computational framework tailored for 4D Flow MRI analysis in the LA, enabling qualitative and quantitative analysis of advanced hemodynamic parameters. The framework is tested on data from different centers and used to assess energy, vorticity, and pressure parameters across various disorders.

Result: The framework produces high-accuracy automated segmentations (Dice > 0.9 and Hausdorff 95 < 3 mm) even with limited training data. A comprehensive assessment of hemodynamic parameters reveals their potential as prognostic biomarkers.

Conclusion: The introduced computational framework offers a robust solution for analyzing 4D Flow MRI in the LA, advancing the study of hemodynamic parameters as potential prognostic biomarkers.

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [191] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke,Takashi Shibuya,Kengo Uchida,Koichi Saito,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 生成逼真的双人人体运动是具有挑战性的，尤其是对于超过典型训练序列长度的长时间交互。本文提出了Dyadic Mamba方法，利用状态空间模型（SSM）生成高质量的任意长度的双人人体运动，解决了长序列生成问题，并提出新的长期运动合成质量评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer的方法在短期双人运动合成方面表现良好，但在处理更长的序列时受到限制，因为位置编码方案存在固有的局限性。

Method: 引入了Dyadic Mamba方法，利用状态空间模型（SSM），通过简单有效的架构促进个体运动序列之间的信息流动，无需复杂的交叉注意力机制。

Result: Dyadic Mamba在标准短期基准上表现出色，在较长序列上显著优于基于transformer的方法，并且提出了一个新的长期运动合成质量评估基准。

Conclusion: 基于SSM的架构为从文本描述中进行长期双人人体运动合成提供了有希望的方向。

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [192] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: An effective and efficient medical image segmentation approach, BoundarySeg, is proposed to improve segmentation accuracy without unannotated data or extra computation.


<details>
  <summary>Details</summary>
Motivation: Obtaining large-scale medical data is challenging due to privacy regulations and data protection policies. Moreover, annotating medical images is time-consuming and costly as it requires domain experts to manually delineate anatomical structures. Semi-supervised methods have gained popularity for reducing annotation costs, but they heavily depend on the availability of unannotated data.

Method: BoundarySeg, a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision.

Result: This strategy improves segmentation accuracy, especially in low data regimes, achieving performance comparable to or exceeding state-of-the-art semi supervised approaches.

Conclusion: BoundarySeg can achieve better performance without relying on unannotated data or increasing computational demands.

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [193] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh,Isabel Funke,Micha Pfeiffer,Fiona Kolbinger,Hanna Maria Schmeiser,Juergen Weitz,Marius Distler,Stefanie Speidel*

Main category: cs.CV

TL;DR: The paper proposes a two-stage, text-conditioned diffusion-based method to synthesize surgical videos for under-represented classes in surgical video datasets, aiming to overcome data imbalance. The approach uses a 2D latent diffusion model and temporal attention layers to ensure spatial and temporal consistency respectively. A rejection sampling strategy is introduced to select the most suitable synthetic samples. The method enhances model performance on surgical action recognition and intra-operative event prediction.


<details>
  <summary>Details</summary>
Motivation: To address the issue of severe data imbalance in surgical video datasets that hinders the development of high-performing models.

Method: A unique two-stage, text-conditioned diffusion-based method is proposed. This method generates high-fidelity surgical videos by conditioning on text prompts and decoupling spatial and temporal modeling using a 2D latent diffusion model and temporal attention layers. Additionally, a rejection sampling strategy is used to select the most suitable synthetic samples.

Result: Incorporating synthetic videos from the proposed approach significantly improves model performance on downstream tasks such as surgical action recognition and intra-operative event prediction.

Conclusion: The authors successfully demonstrate that their method effectively addresses class imbalance in surgical video datasets and improves model performance on relevant tasks.

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [194] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee,Taylor Webb,Trevor Bihl,Keith Holyoak,Hongjing Lu*

Main category: cs.CV

TL;DR: This paper presents Probabilistic Schema Induction (PSI), a model that uses deep learning to perform analogical mapping over structured representations of visual concepts from limited examples, forming schemas. PSI outperforms controls and mimics human-like learning.


<details>
  <summary>Details</summary>
Motivation: To understand and replicate the human ability to learn new visual concepts from limited examples using structured representations and analogical mapping.

Method: Introduced PSI, a prototype model employing deep learning for analogical mapping over structured representations to form compositional concepts called schemas, with mechanisms weighing similarity and amplifying relevant relations.

Result: PSI produces human-like learning performance, outperforming models using unstructured feature vectors or weaker structured representations. Its success is attributed to increasing relational similarity and emphasizing distinguishing relations.

Conclusion: Structured representations and analogical mapping are crucial for modeling rapid human-like learning of compositional visual concepts, demonstrating how deep learning can be used in psychological models.

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [195] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin,Chenyang Wu,Penghui Huang,Yanyong Zhang,Yinian Mao,Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM is a large-scale 3DGS-based visual SLAM method with stereo cameras that introduces multi-modality pose estimation, feature-alignment warping constraints, continuous Gaussian Splatting submaps, and a structure refinement module for superior performance in large-scale outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Current methods using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) for visual SLAM require RGBD sensors and are mostly effective in indoor environments. The robustness of these methods in large-scale outdoor scenarios has not been adequately explored.

Method: The proposed LSG-SLAM uses a multi-modality strategy to estimate prior poses under large view changes, feature-alignment warping constraints to mitigate appearance similarity issues in rendering losses, continuous Gaussian Splatting submaps for scalability in large-scale scenarios, place recognition for loop detection between GS submaps, and a structure refinement module to enhance reconstruction quality after global optimization of camera poses and Gaussian points.

Result: LSG-SLAM demonstrates superior performance over existing Neural, 3DGS-based, and traditional approaches through extensive evaluations on the EuRoc and KITTI datasets.

Conclusion: LSG-SLAM effectively addresses the challenges of large-scale outdoor visual SLAM by integrating advanced techniques such as multi-modality pose estimation, feature alignment, scalable submaps, and structure refinement.

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [196] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP is a method for universal visual anomaly detection that builds on CLIP models with added adapters for visual, textual, and prompt-query processing, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for universal visual anomaly detection struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning which limits their flexibility.

Method: AdaptCLIP learns adaptive visual and textual representations alternately rather than jointly, incorporates comparative learning between query and normal image prompts using both contextual and aligned residual features, and adds three simple adapters to CLIP models: visual adapter, textual adapter, and prompt-query adapter.

Result: AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods.

Conclusion: AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset.

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [197] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin,Shaolei Liu,Manning Wang*

Main category: cs.CV

TL;DR: The paper proposes a new source-free domain adaptation (SFDA) framework that includes preadaptation, frequency prompt, and style-related layer fine-tuning to improve pseudo-label quality and model performance.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods have limitations in producing high-quality pseudo-labels and translating image styles effectively, leading to inefficiencies in model training under limited supervision.

Method: The proposed method introduces three key components: preadaptation for generating a preadapted model to enhance pseudo-labels, data-dependent frequency prompt for effective image style translation, and style-related layer fine-tuning for efficient target model training.

Result: Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks show that the proposed method outperforms existing state-of-the-art techniques.

Conclusion: The novel SFDA framework with preadaptation, frequency prompt, and style-related layer fine-tuning successfully addresses current challenges in SFDA, offering superior performance in medical imaging tasks.

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [198] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: This paper proposes VRU-CIPI, a framework using GRU and Transformer for predicting crossing intentions of Vulnerable Road Users (VRUs) at intersections. It achieves 96.45% accuracy and can be integrated with Infrastructure-to-Vehicles (I2V) communication to enhance intersection safety.


<details>
  <summary>Details</summary>
Motivation: To improve the interaction safety between road users, especially focusing on understanding and predicting the crossing intentions of Vulnerable Road Users (VRUs) at urban intersections.

Method: The VRU-CIPI framework uses Gated Recurrent Unit (GRU) for capturing temporal dynamics in VRU movements and a multi-head Transformer self-attention mechanism for encoding contextual and spatial dependencies.

Result: Evaluated on the UCF-VRU dataset, the proposed framework achieves state-of-the-art performance with an accuracy of 96.45% and real-time inference speed of 33 frames per second.

Conclusion: By integrating with Infrastructure-to-Vehicles (I2V) communication, the approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles.

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [199] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan,Lei Zhou,Liu Mao,Shaofan Chen,Chuanqiu Ren,Xia Xie*

Main category: cs.CV

TL;DR: The paper introduces a new remote sensing change detection task called non-registration change detection, outlines eight real-world scenarios leading to non-registration problems, devises image transformation schemes for these scenarios, and shows the negative impact on current methods.


<details>
  <summary>Details</summary>
Motivation: To tackle emergencies like natural disasters, anthropogenic accidents, and military strikes by proposing a novel remote sensing change detection task.

Method: Propose eight real-world scenarios for non-registration problems, develop tailored image transformation schemes, and demonstrate the catastrophic damage to state-of-the-art methods.

Result: Non-registration change detection causes significant issues to existing advanced methods.

Conclusion: This study highlights the challenges of non-registration change detection and provides resources (code and dataset) for further research.

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [200] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng,Kexuan Li,Xingye Cui,Jiaxuan Li,Chang Long,Tian Pu,Zhenming Peng*

Main category: cs.CV

TL;DR: 提出了一种新的网络CSPENet用于红外小目标检测，该网络通过提取和嵌入轮廓感知和显著性先验来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的红外小目标检测方法在定位微弱目标和在密集杂波环境中感知轮廓信息方面存在不足，限制了检测性能。

Method: 设计了一个包围收敛先验提取模块（SCPEM），提取增强的显著性先验和多尺度结构先验；提出了双分支先验嵌入架构（DBPEA），以最优网络位置嵌入这两种先验；开发了注意力引导特征增强模块（AGFEM）来细化特征表示并提高显著性估计的准确性。

Result: 在NUDT-SIRST、IRSTD-1k和NUAA-SIRST公共数据集上的实验结果表明，CSPENet的检测性能优于其他最先进的方法。

Conclusion: CSPENet通过结合轮廓感知和显著性先验，在红外小目标检测任务中取得了优异的表现，并且代码已公开。

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [201] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang,Tao Tan,Shuai Tan,Weiqin Yang,Kunyan Cai,Calvin Chen,Yue Sun*

Main category: cs.CV

TL;DR: The paper presents MambaControl, a new framework that integrates selective state-space modelling with diffusion processes for predicting medical image trajectories in disease progression. It combines Mamba-based long-range modelling with graph-guided anatomical control and introduces Fourier-enhanced spectral graph representations to achieve high accuracy in Alzheimer's disease prediction.


<details>
  <summary>Details</summary>
Motivation: Current methods for modelling disease progression have difficulty handling longitudinal dependencies and maintaining structural consistency in progressive disorders. This necessitates a more effective approach to capture complex spatio-temporal dynamics while preserving anatomical integrity.

Method: MambaControl integrates selective state-space modelling with diffusion processes. It uses Mamba-based long-range modelling combined with graph-guided anatomical control to represent anatomical correlations. Additionally, it incorporates Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail.

Result: Quantitative and regional evaluations show improved progression prediction quality and anatomical fidelity in the context of Alzheimer's disease prediction, demonstrating state-of-the-art performance.

Conclusion: MambaControl offers significant improvements in predicting disease progression with high anatomical fidelity, showcasing potential for personalized prognosis and clinical decision support.

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [202] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: A novel framework focusing on Texture Key Driver Factors (TKDF) for Facial Expression Recognition (FER) is introduced, which includes a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF). This method achieves state-of-the-art performance in FER tasks.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition in the wild remains challenging due to subtle expression-related features and complex variations in facial appearance.

Method: The proposed framework identifies Texture Key Driver Factors (TKDF), localized texture regions with strong discriminative power across emotional categories. It uses a Texture-Aware Feature Extractor (TAFE) with ResNet-based backbone enhanced with multi-branch attention to extract fine-grained texture representations, and Dual Contextual Information Filtering (DCIF) to refine these features through adaptive pooling and attention mechanisms.

Result: Experimental results on RAF-DB and KDEF datasets show that the method achieves state-of-the-art performance.

Conclusion: The incorporation of TKDFs into FER pipelines proves effective and robust, significantly improving FER performance.

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [203] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao,Shaobo Xia,Sheng Nie,Cheng Wang,Xiaohuan Xi,Bisheng Yang*

Main category: cs.CV

TL;DR: APCoTTA是一种针对ALS点云语义分割的连续测试时适应(CTTA)方法，通过动态可训练层选择、熵基一致性损失和随机参数插值机制等创新手段，缓解了灾难性遗忘和误差累积问题。在新构建的ISPRSC和H3DC基准数据集上，APCoTTA相较于直接推理分别提升了约9%和14%的mIoU性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA研究在ALS点云领域有限，面临无标准化数据集、灾难性遗忘和误差累积等问题。为解决这些问题并提升模型在不同域上的适应能力，需要一种专门针对ALS点云语义分割的CTTA方法。

Method: 提出了一种名为APCoTTA的方法，包括：1）动态可训练层选择模块，利用梯度信息选择低置信度层进行训练以缓解灾难性遗忘；2）熵基一致性损失，仅对可靠样本施加一致性损失以减少误差累积；3）随机参数插值机制，将目标域适应与源知识保留平衡；4）构建了两个新基准数据集ISPRSC和H3DC。

Result: 实验结果表明，APCoTTA在ISPRSC和H3DC两个基准数据集上表现最佳，相较于直接推理分别实现了约9%和14%的mIoU提升。

Conclusion: APCoTTA是首个专为ALS点云语义分割设计的CTTA方法，有效解决了灾难性遗忘和误差累积问题，并在新建的ISPRSC和H3DC基准数据集上展现出优越性能。

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [204] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou,Yichong Xia,Sicheng Pan,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: An underwater image compression algorithm HQUIC is developed, which leverages underwater-image-specific features for better performance.


<details>
  <summary>Details</summary>
Motivation: Current underwater image compression algorithms do not fully utilize the unique characteristics of underwater images, leading to suboptimal performance.

Method: HQUIC uses an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamically weights multi-scale frequency components.

Result: HQUIC shows superior performance compared to state-of-the-art compression methods on various underwater datasets.

Conclusion: HQUIC is an effective solution for underwater image compression that takes advantage of underwater-image-specific features.

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [205] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng,Jiafei Duan,Yi Ru Wang,Haoquan Fang,Boyang Li,Yushan Huang,Elvis Wang,Ainaz Eftekhar,Jason Lee,Wentao Yuan,Rose Hendrix,Noah A. Smith,Fei Xia,Dieter Fox,Ranjay Krishna*

Main category: cs.CV

TL;DR: Pointing is crucial for grounding language in visual contexts. PointArena evaluates multimodal pointing through Point-Bench, Point-Battle, and Point-Act. Molmo-72B leads but proprietary models are catching up. Supervised training boosts performance.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive platform for evaluating multimodal pointing capabilities across diverse reasoning scenarios, addressing the limitation of existing benchmarks focusing only on referential object localization tasks.

Method: Introduced PointArena comprising Point-Bench (1,000 pointing tasks), Point-Battle (interactive pairwise model comparisons with 4,500 votes), and Point-Act (real-world robotic manipulation system). Evaluated state-of-the-art open-source and proprietary multimodal models.

Result: Molmo-72B outperforms other models, proprietary models show comparable performance. Supervised training improves pointing task performance. Strong correlations observed across evaluation stages.

Conclusion: Precise pointing capabilities are essential for enabling multimodal models to bridge abstract reasoning with real-world actions.

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [206] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang,Jiyeong Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: The paper introduces DITM, a method for descriptive image-text matching that explores language flexibility to improve learning of graded contextual similarity.


<details>
  <summary>Details</summary>
Motivation: Most existing image-text matching approaches use sparse binary supervision which limits their ability to cover the many-to-many relationships between images and texts. They also neglect implicit connections from general to specific descriptions.

Method: DITM formulates descriptiveness score with cumulative term frequency-inverse document frequency (TF-IDF) and leverages sentence descriptiveness in two ways: refining false negative labeling and building more precise matching by aligning sentences in a generic-to-specific order.

Result: Experiments on MS-COCO, Flickr30K, and CxC datasets show DITM's effectiveness in representing complex image-text relationships compared to state-of-the-art methods. It also enhances hierarchical reasoning as shown by analysis on HierarCaps benchmark.

Conclusion: DITM successfully moves beyond rigid binary supervision to enhance discovery of optimal matches and potential positive pairs, improving both image-text matching and hierarchical reasoning.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [207] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang,Yuanqi Hu,Xinyu Chen,Yuxia Xu,Suhui Wang,Chunan Yu,Lanyun Zhu,Deyi Ji,Xin Xu,Tianrun Chen*

Main category: cs.CV

TL;DR: In the age of immersive electronics, there's a growing interest in virtual fashion for identity expression. However, current 3D garment design tools are difficult for everyday users due to technical challenges and limited data. This paper presents a 3D sketch-driven framework that allows even inexperienced users to generate high-quality digital clothing via simple 3D sketches in AR/VR environments. The system uses a conditional diffusion model, a sketch encoder, and adaptive curriculum learning to interpret rough inputs and produce realistic garments. Additionally, they introduce KO3DClothes, a new dataset to tackle the lack of training data. Experiments show that this method surpasses existing ones in fidelity and usability, promoting accessible fashion design on future platforms.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to make 3D garment design more accessible to everyday users who want to express their identity through virtual fashion, overcoming the steep technical barriers and limited data that hinder current tools.

Method: The method involves creating a 3D sketch-driven 3D garment generation framework. It combines a conditional diffusion model with a sketch encoder trained in a shared latent space and an adaptive curriculum learning strategy to translate imprecise, free-hand input into realistic, personalized garments. A new dataset, KO3DClothes, consisting of paired 3D garments and user-created sketches, was also introduced to address the scarcity of training data.

Result: The results from extensive experiments and user studies indicate that the proposed method significantly outperforms existing baselines in terms of both fidelity and usability. This highlights the potential of the method to democratize fashion design on next-generation consumer platforms.

Conclusion: This work concludes by demonstrating the effectiveness of the 3D sketch-driven 3D garment generation framework in empowering ordinary users to create high-quality digital clothing. By introducing KO3DClothes and showing superior performance compared to existing methods, the study paves the way for more inclusive and accessible fashion design experiences.

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [208] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: An improved autonomous target detection network based on YOLOv8 is presented to overcome challenges in object detection for autonomous driving, achieving 65% detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current technologies for environmental perception in autonomous driving face challenges such as high costs, vulnerability to weather and lighting conditions, and limited resolution.

Method: The paper integrates structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework.

Result: The enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showing significant improvements over traditional methods.

Conclusion: This improved model has substantial potential for real-world applications, particularly excelling in scenarios involving single-target and small-object detection.

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [209] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution, demonstrating significant improvements in PSNR, SSIM, and LPIPS metrics.


<details>
  <summary>Details</summary>
Motivation: Super-resolution image reconstruction is crucial with the development of remote sensing technology. Current deep learning methods struggle with complex scenes and detail preservation.

Method: A reinforcement learning-based latent diffusion model (LDM) fine-tuning method is proposed. It constructs a reinforcement learning environment optimized by proximal policy optimization (PPO) during the reverse denoising process of the LDM model.

Result: Experiments on the RESISC45 dataset showed improvements over the baseline model: PSNR increased by 3-4dB, SSIM improved by 0.08-0.11, and LPIPS reduced by 0.06-0.10, especially in structured and complex natural scenes.

Conclusion: The proposed method effectively enhances super-resolution quality and demonstrates adaptability across various scenes.

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [210] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: This paper proposes a domain-adaptive one-shot landmark detection framework named Universal One-shot Detection (UOD) to handle multi-domain medical images. It consists of two stages and corresponding universal models, achieving state-of-the-art performances on three public X-ray datasets.


<details>
  <summary>Details</summary>
Motivation: One-shot medical landmark detection has label-efficient training process but existing methods suffer from domain preference when dealing with multi-domain unlabeled data and are not robust for sub-optimal image annotation.

Method: The UOD framework includes two stages: 1) A domain-adaptive convolution model is self-supervised learned to generate pseudo landmark labels; 2) A domain-adaptive transformer is designed to eliminate domain preference and build the global context for multi-domain data. The framework uses domain-specific and domain-shared modules.

Result: UOD was investigated both qualitatively and quantitatively on three widely-used public X-ray datasets in different anatomical domains (head, hand, chest) and achieved state-of-the-art performances in each domain.

Conclusion: The proposed UOD framework can effectively handle multi-domain medical images with only one annotated sample from each domain, providing more robust and accurate landmark detection.

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [211] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: 研究人员开发了DeepSeqCoco，一种基于深度学习的模型，用于从椰子树图像中准确且自动地识别疾病。该模型在不同的优化器设置下进行测试，结果表明其准确性高达99.5%，比现有模型高出5%。混合SGD-Adam优化器显示出最低的验证损失2.81%，同时训练和预测时间显著减少。这证明了通过AI为基础的可扩展且高效的疾病监测系统改进精确农业的潜力。


<details>
  <summary>Details</summary>
Motivation: 椰子树疾病对农业产量构成严重威胁，特别是在传统耕作方式限制早期诊断和干预的发展中国家。目前的疾病识别方法是手动的、劳动密集型的且不可扩展。

Method: 提出了DeepSeqCoco，这是一种基于深度学习的模型，可以从椰子树图像中准确且自动地识别疾病。该模型在各种优化器设置（如SGD、Adam和混合配置）下进行了测试，以找到准确性和计算成本之间的最佳平衡。

Result: 实验结果表明，DeepSeqCoco可以达到高达99.5%的准确率（比现有模型高出多达5%），其中混合SGD-Adam优化器显示出最低的验证损失2.81%。此外，与现有模型相比，训练时间减少了多达18%，预测时间减少了多达85%。

Conclusion: DeepSeqCoco模型展示了通过AI为基础的、可扩展且高效的疾病监测系统改进精确农业的潜力。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [212] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang,Boyang Zheng,Xichen Pan,Sayak Paul,Saining Xie*

Main category: cs.CV

TL;DR: This paper explores the design space of fusing LLMs and DiTs for text-to-image generation, offering comparisons, design insights, and a reproducible training recipe.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding the potential of deep fusion between LLMs and DiTs for multi-modal generation due to lack of detailed comparisons and undisclosed design details in previous studies.

Method: Conduct empirical study on text-to-image generation with controlled comparisons to established baselines, analysis of design choices, and provision of a reproducible large-scale training recipe.

Result: Provides meaningful data points and practical guidelines for future research in multi-modal generation.

Conclusion: This work aims to offer valuable insights and reproducible methods for advancing multi-modal generation research.

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [213] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan,Xuepu Zeng,Jing Zhang,Mingming Gong,Yuxiang Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: Dynamic scene representation and reconstruction have advanced significantly due to developments in neural radiance fields and 3D Gaussian splatting techniques, transitioning from static to dynamic environments. This survey reviews over 200 papers on this topic, categorizing approaches and identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is the transformative advances in dynamic scene representation and reconstruction, driven by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques, which have evolved from addressing static to dynamic environments.

Method: The method involves a systematic analysis of over 200 papers focused on dynamic scene representation using radiance fields, categorized through multiple critical lenses such as motion representation paradigms, reconstruction techniques, auxiliary information integration strategies, and regularization approaches.

Result: The result is a comprehensive overview of the field, organizing diverse methodological approaches under a unified representational framework, and providing insights into persistent challenges and promising research directions.

Conclusion: This survey concludes by establishing a definitive reference for researchers entering the field of dynamic scene reconstruction, offering both experienced practitioners and newcomers a systematic understanding of conceptual principles and practical frontiers.

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [214] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 本文评估了大型多模态模型在低资源Pashto语言上的OCR性能，开发了一个名为PsOCR的合成数据集，并对多个开源和闭源模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: Pashto语言的自然语言处理面临许多挑战，包括其连笔字体和缺乏结构化数据集。为了解决这些问题并评估现有模型的能力和局限性，作者创建了一个新的合成Pashto OCR数据集。

Method: 创建了一个包含一百万张图像的合成Pashto OCR数据集PsOCR，这些图像标注有单词、行和文档级别的边界框。该数据集涵盖了1000种独特的字体家族、颜色、图像大小和布局的变化。使用一个包含10K张图像的基准子集来评估几个LMMs的性能，包括七个开源模型和四个闭源模型。

Result: 实验结果显示Gemini在所有模型中表现最佳，而在开源模型中，Qwen-7B表现出色。

Conclusion: 这项工作提供了关于当前LMMs在Pashto OCR任务中的能力和局限性的深入评估，并为未来在Pashto OCR及其他类似脚本（如阿拉伯语、波斯语和乌尔都语）的研究奠定了基础。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [215] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju,Sheng-Yen Huang,Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB is an efficient two-stage framework that extends Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes. It generates stylized video in Stage 1 and learns stylized neutral head model and expression blendshapes in Stage 2.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to extend the widely used Toonify framework for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, overcoming the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN.

Method: The method involves a two-stage framework called ToonifyGB. In Stage 1, an improved StyleGAN is employed to generate stylized video from input video frames. In Stage 2, a stylized neutral head model and a set of expression blendshapes are learned from the generated video.

Result: ToonifyGB efficiently renders stylized avatars with arbitrary expressions and its effectiveness is validated on the benchmark dataset using two styles: Arcane and Pixar.

Conclusion: ToonifyGB provides an efficient way to synthesize diverse stylized 3D head avatars using Gaussian blendshapes.

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [216] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo,Xiaodong Gu*

Main category: cs.CV

TL;DR: The paper introduces MMRL and MMRL++, methods that enhance cross-modal interactions in Vision-Language Models using a shared representation space, reducing overfitting and improving generalization. Extensive experiments show they outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale pre-trained Vision-Language Models often suffer from overfitting when adapted with limited few-shot data, which undermines their ability to generalize to new tasks.

Method: MMRL proposes a shared, learnable, modality-agnostic representation space by inserting representation tokens into higher encoder layers. A regularization term aligns class and text features with the frozen VLM's zero-shot features. MMRL++ enhances this by significantly reducing trainable parameters and strengthening intra-modal interactions.

Result: Extensive experiments on 15 datasets demonstrate that both MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.

Conclusion: MMRL and MMRL++ are effective methods for multi-modal representation learning, promoting better generalization and reducing overfitting in Vision-Language Models.

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [217] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: MoB is a new method for visual token pruning which provides a closed-form error bound based on Hausdorff distance and uses ϵ-covering theory to balance two objectives, achieving high performance preservation with minimal tokens.


<details>
  <summary>Details</summary>
Motivation: Current visual token pruning methods use static strategies that do not account for the varying importance of objectives across different tasks, resulting in inconsistent performance.

Method: The method derives a closed-form error bound using Hausdorff distance to characterize both objectives of prompt alignment and visual preservation. It leverages ϵ-covering theory to reveal a trade-off between these objectives and proposes MoB (Multi-Objective Balanced Covering) to reformulate visual token pruning as a bi-objective covering problem.

Result: Experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B with only 11.1% of original visual tokens, and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. It also works well with Qwen2-VL and Video-LLaVA.

Conclusion: MoB offers a provable performance bound and linear scalability, making it adaptable to challenging pruning scenarios and integrating well into advanced MLLMs and diverse vision-language tasks.

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [218] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil,Lucas Robinet,Laurent Risser,Soleakhena Ken*

Main category: cs.CV

TL;DR: The paper proposes a novel image registration method using a conditional U-Net architecture to estimate unknown condition-related images in real-time without reconstruction artefacts for radiotherapy treatment.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of estimating unknown condition-related images based on known images and their conditions, particularly in the context of moving tumors for radiotherapy treatment with 4D-CT scans.

Method: A new conditional U-Net architecture is used to model the image registration formalism, fully incorporating conditional information without needing any fixed image. This approach is applied to stitch sequential 2D slices into several 3D volumes at different organ positions.

Result: The method successfully generates artefact-free volumes through real-time latencies when applied to 4D-CT clinical data in thoracoabdominal regions.

Conclusion: The proposed method offers a solution for complex image registration tasks, such as those encountered in radiotherapy treatment planning, providing accurate and real-time results.

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [219] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: 提出了一种多源协同风格增强与领域不变学习方法（MCSAD），通过交替进行协同风格增强和领域不变学习，使模型在未见目标领域上具有良好的泛化能力，并且在多个领域泛化数据集上的实验表明该方法显著优于现有的联邦领域泛化方法。


<details>
  <summary>Details</summary>
Motivation: 联合领域泛化的目标是从多个去中心化的源领域中学习一个可泛化的模型，用于部署到未见的目标领域。尽管风格增强方法在领域泛化方面取得了很大进展，但现有的风格增强方法要么探索孤立源领域的数据风格，要么在数据去中心化场景下跨现有源领域插值风格信息，这导致了风格空间有限。

Method: 为了解决上述问题，提出了多源协同风格增强与领域不变学习方法（MCSAD）。具体来说，提出了一种多源协同风格增强模块，以生成更广泛风格空间的数据。此外，通过跨域特征对齐和类别关系集成蒸馏，在原始数据和增强数据之间进行领域不变学习，从而学习到一个领域不变的模型。

Result: 通过交替进行协同风格增强和领域不变学习，该模型能够在未见目标领域上表现良好。广泛的实验结果表明，该方法在多个领域泛化数据集上显著优于现有的联合领域泛化方法。

Conclusion: 提出的MCSAD方法通过扩展风格空间和领域不变学习，有效提高了模型在未见目标领域的泛化性能，是一种先进的联合领域泛化方法。

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [220] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 尽管在图像显著性预测方面取得成功，但由于数据集偏差，跨多个显著性数据集预测注视点仍然具有挑战性。本文提出了一种新的架构，通过扩展几乎与数据集无关的编码器-解码器结构，并使用少于20个数据集特定参数来控制可解释机制，如多尺度结构、中心偏差和注视点分布，从而解决了这一问题。该模型在MIT/Tuebingen显著性基准的所有三个数据集上达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管当前图像显著性预测方法在现有基准上接近黄金标准性能水平，但在不同数据集之间应用模型时存在显著性能下降的问题（约40%），并且增加数据集多样性并不能解决这一跨数据集差距，约60%的差距归因于数据集特定偏差。因此，需要一种能够解决剩余泛化差距的方法。

Method: 提出了一种新型架构，该架构扩展了一个几乎与数据集无关的编码器-解码器结构，并引入了少于20个数据集特定参数，这些参数控制诸如多尺度结构、中心偏差和注视点分布等可解释机制。仅通过适应这些参数到新数据，可以弥补超过75%的泛化差距，且即使只有50个样本也能实现大部分改进。

Result: 该模型在MIT/Tuebingen显著性基准的所有三个数据集（MIT300、CAT2000和COCO-Freeview）上达到了新的最先进水平，即使在从不相关数据集纯泛化的情况下也是如此，但在适应各自训练数据集时性能有显著提升。此外，该模型还提供了有关空间显著性特性的宝贵见解，揭示了结合绝对和相对大小的复杂多尺度效应。

Conclusion: 所提出的模型通过引入少量数据集特定参数解决了显著性预测中的跨数据集泛化问题，并在多个基准数据集上取得了最佳性能，同时提供了对空间显著性特性的深入理解。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [221] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon,Ahmad AlMughrabi,Thanasis Zoumpekas,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: VolE is a novel framework leveraging mobile device-driven 3D reconstruction to accurately estimate food volume without reference or depth information, achieving 2.22% MAPE and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Accurate food volume estimation is essential for medical nutrition management and health monitoring but current methods are limited by mononuclear data, single-purpose hardware, sensor-oriented information, or reliance on camera calibration using a reference object.

Method: VolE captures images and camera locations in free motion using AR-capable mobile devices to generate precise 3D models. It uses food video segmentation for food mask generation and is a reference- and depth-free framework.

Result: VolE outperforms existing volume estimation techniques across multiple datasets with a 2.22% MAPE.

Conclusion: VolE presents a new approach to food volume estimation that is more accurate and less reliant on specific hardware or conditions.

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [222] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 通过MixUp和辅助傅里叶增强等方法，可以提高医学图像分割模型在真实临床环境中的泛化能力和鲁棒性。这些方法能够显著改善心脏电影MRI和前列腺MRI分割中对各种变换的鲁棒性，并通过提升特征表示的可分性和紧凑性来增强nnU-Net训练管道的可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型通常在精心策划的数据集上进行训练，在实际临床环境中部署时，由于训练和测试分布之间的差异，性能会下降。传统的数据增强技术缺乏应对多样化现实场景所需的鲁棒性。

Method: 系统评估了MixUp和辅助傅里叶增强等替代增强策略，这些方法不明确针对特定的分布变化来源，而是缓解多种变化的影响。将这些方法集成到nnU-Net训练管道中。

Result: 定量研究发现，这些增强方法通过促进可分性和紧凑性提高了学习到的特征表示的质量，从而显著改善了模型在各种变换下的泛化能力和鲁棒性。

Conclusion: MixUp和辅助傅里叶增强等方法为提高医学分割模型在实际应用中的可靠性提供了一个易于实现且有效的解决方案。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [223] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: Deep neural networks in medical imaging can have biases causing fairness gaps. This study explores Human-AI alignment and fairness, finding that human insights reduce these gaps and improve generalization, but excessive alignment may compromise performance. A balanced strategy is needed for fair and robust medical AI systems.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the impact of Human-AI alignment on fairness and out-of-domain generalization in medical imaging, where deep neural networks are effective but prone to biases leading to fairness gaps across demographic groups.

Method: Incorporating human insights into AI models in the medical imaging domain to examine how it affects fairness gaps and out-of-domain generalization. The study also investigates potential trade-offs from excessive alignment.

Result: Human insights consistently reduce fairness gaps and enhance out-of-domain generalization in medical AI systems. However, excessive alignment can introduce performance trade-offs, indicating the importance of calibrated strategies.

Conclusion: Human-AI alignment offers a promising approach for developing fair, robust, and generalizable medical AI systems, requiring a balance between expert guidance and automated efficiency.

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [224] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: The paper introduces MTVCrafter, a framework using 4D motion tokens for human image animation that surpasses existing methods relying on 2D-rendered pose images.


<details>
  <summary>Details</summary>
Motivation: Existing human image animation methods depend heavily on 2D-rendered pose images, limiting generalization and losing essential 3D information.

Method: MTVCrafter proposes the use of 4DMoT to quantize 3D motion sequences into 4D motion tokens and MV-DiT with unique motion attention for effective leverage of these tokens as context for animation.

Result: MTVCrafter achieves state-of-the-art results with an FID-VID score of 6.98, outperforming the second-best method by 65%. It also shows strong generalization across diverse characters and scenarios.

Conclusion: MTVCrafter marks a significant advancement in human image animation by directly modeling raw 3D motion sequences and opens new possibilities for pose-guided human video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [225] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen,Wanqi Yin,Xiaofeng Yang,Cheng Chen,Chaoyue Song,Zhongang Cai,Lei Yang,Hao Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper presents ADHMR, a framework that improves human mesh recovery by aligning a diffusion-based HMR model through preference optimization. It uses an assessment model (HMR-Scorer) to evaluate predictions and create a preference dataset for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Human mesh recovery from a single image is challenging due to depth ambiguity and occlusions. Current probabilistic methods produce numerous plausible 3D human mesh predictions but often misalign with 2D image observations and lack robustness on in-the-wild images.

Method: The authors propose ADHMR, which includes training a human mesh prediction assessment model called HMR-Scorer. This model can evaluate predictions for in-the-wild images without 3D annotations. They use HMR-Scorer to create a preference dataset consisting of winner and loser mesh predictions for each input image. This dataset is used to fine-tune the base model via direct preference optimization. Additionally, HMR-Scorer helps improve existing HMR models through data cleaning.

Result: Extensive experiments demonstrate that ADHMR outperforms current state-of-the-art methods in human mesh recovery.

Conclusion: ADHMR addresses the limitations of existing probabilistic methods by using preference optimization to better align 3D human mesh predictions with 2D image observations and enhance robustness for in-the-wild images.

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [226] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu,Jiaqi Tang,Jiyao Wang,Yunfan LU,Xu Cao,Qingyong Hu,Yin Wang,Yuting Zhang,Tianxin Xie,Yunpeng Zhang,Yong Chen,Jiayu. Gao,Bin Huang,Dengbo He,Shuiguang Deng,Hao Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: The paper introduces SAGE DeeR, a driving agent with super alignment, generalist capabilities and self-eliciting features.


<details>
  <summary>Details</summary>
Motivation: To meet different users' comfort, interaction, and safety needs in intelligent driving cockpits.

Method: SAGE DeeR achieves super alignment by reacting to individual preferences and biases, generalist by understanding multi-view and multi-mode inputs, and self-eliciting by revealing implicit thought chains in the language space.

Result: Collected multiple datasets and built a large-scale benchmark for measuring perceptual decision-making ability and super alignment accuracy.

Conclusion: SAGE DeeR aims to fulfill diverse user requirements in the intelligent driving cockpit through its unique abilities.

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [227] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz,Pascal Colling,Qi Han,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper presents a new offline mapping approach for autonomous driving that integrates driver trails into HD map creation using transformer-based deep learning models, showing better generalization and performance than online methods.


<details>
  <summary>Details</summary>
Motivation: Current online mapping approaches for autonomous driving lack in temporal consistency, sensor occlusion handling, runtime efficiency, and generalization. There is a need for an improved method that can create accurate and updatable maps with less manual effort.

Method: The proposed method aggregates trail data from the ego vehicle and other traffic participants to construct a comprehensive global map. It employs transformer-based deep learning models, enabling continuous updates and being sensor-agnostic.

Result: The method outperforms state-of-the-art online mapping approaches in terms of generalization to unseen environments and sensor configurations. It has been validated on two benchmark datasets, proving its robustness and applicability.

Conclusion: This novel offline mapping approach offers a promising solution for creating high-definition maps in autonomous driving systems, providing superior performance and adaptability compared to existing online mapping techniques.

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [228] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: The paper presents HandReader, a set of three architectures for fingerspelling recognition in Sign Language, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Fingerspelling is an important part of Sign Language and previous works have room for improvement in accuracy.

Method: HandReader_RGB uses Temporal Shift-Adaptive Module (TSAM) to process RGB features. HandReader_KP uses Temporal Pose Encoder (TPE) on keypoints as tensors. HandReader_RGB+KP combines both modalities with a joint encoder.

Result: All HandReader models achieve state-of-the-art results on ChicagoFSWild and ChicagoFSWild+ datasets, and high performance on the new Znaki dataset for Russian fingerspelling.

Conclusion: The HandReader models are effective for fingerspelling recognition and the Znaki dataset and pre-trained models are publicly available.

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [229] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu,Kaixin Chen,Heng Guo,Yixiang Huang,Ming Wu,Zhenwei Shi,Chuang Zhang,Jun Guo*

Main category: cs.CV

TL;DR: The paper introduces MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting that includes over 68,000 high-resolution samples. It addresses limitations of existing datasets by enabling evaluation across diverse conditions and supporting development of scalable fog prediction techniques.


<details>
  <summary>Details</summary>
Motivation: Deep learning approaches have shown promise in marine fog detection and forecasting, but the lack of open-source datasets with diverse regional and satellite data limits model evaluation and exploration of marine fog characteristics.

Method: MFogHub integrates annotated marine fog observations from 15 coastal fog-prone regions and six geostationary satellites, providing over 68,000 high-resolution samples to facilitate rigorous evaluation of detection and forecasting methods under varying conditions.

Result: Experiments with 16 baseline models show that MFogHub reveals generalization fluctuations due to regional and satellite discrepancies and serves as a valuable resource for developing targeted and scalable fog prediction techniques.

Conclusion: MFogHub aims to advance practical monitoring and scientific understanding of marine fog dynamics globally.

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [230] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang,Shuai Xu,Xuelin Zhu,Yicong Li*

Main category: cs.CV

TL;DR: The paper proposes a Multi-Stage Cross-modal Interaction (MSCI) model to enhance Compositional Zero-Shot Learning (CZSL) by effectively utilizing intermediate-layer information from CLIP's visual encoder, designing self-adaptive aggregators for local and global information extraction, and dynamically adjusting attention weights. Experiments on three datasets validate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing CZSL studies rely on CLIP but overlook its limitations in capturing fine-grained local features due to architectural and training paradigm constraints.

Method: Propose MSCI model with two self-adaptive aggregators to extract and integrate local and global visual features, progressively incorporating them into textual representations through stage-by-stage interaction mechanism, and dynamically adjusting attention weights based on different combinations.

Result: Experiments on three widely used datasets fully validate the effectiveness and superiority of the proposed model.

Conclusion: MSCI model significantly enhances perception capability for fine-grained local visual information and flexibly adapts to diverse scenarios.

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [231] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: The paper introduces StoryReasoning, a dataset with grounded stories and structured scene analyses to maintain character and object consistency across frames in visual storytelling. By fine-tuning Qwen2.5-VL 7B into Qwen Storyteller, they achieve a 12.3% reduction in hallucinations per story.


<details>
  <summary>Details</summary>
Motivation: Visual storytelling systems often fail to maintain character identity and link actions appropriately due to referential hallucinations. This motivates the need for grounding characters, objects, and entities on visual elements.

Method: Proposed StoryReasoning dataset includes 4,178 stories from 52,016 movie images with structured scene analyses and grounded stories. The approach uses cross-frame object re-identification, chain-of-thought reasoning, and a grounding scheme linking textual elements to visual entities.

Result: Fine-tuned Qwen2.5-VL 7B (Qwen Storyteller) shows a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story compared to non-fine-tuned model.

Conclusion: StoryReasoning dataset and Qwen Storyteller model provide effective solutions for reducing referential hallucinations in visual storytelling.

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [232] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo,Roger Trullo,Albert Pla Planas,Etienne Decenciere,Thomas Walter*

Main category: cs.CV

TL;DR: This paper introduces MIPHEI, a model that predicts multiplex immunofluorescence signals from H&E images using U-Net and ViT architecture. It is trained on the ORION dataset and validated on two independent datasets, achieving high F1 scores for cell-type classification.


<details>
  <summary>Details</summary>
Motivation: Histopathological analysis with H&E staining is standard in cancer diagnosis, but multiplex immunofluorescence (mIF) provides more precise cell type identification which is not widely adopted due to cost and constraints. The motivation is to bridge this gap by predicting mIF signals from H&E images.

Method: The method involves developing MIPHEI, a U-Net-inspired architecture that uses ViT foundation models as encoders to predict mIF signals from H&E images. It targets various markers and is trained on the ORION dataset, then validated on two other datasets.

Result: MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming state-of-the-art baselines and random classifiers for most markers.

Conclusion: MIPHEI effectively captures relationships between nuclear morphologies and molecular markers, offering a promising approach for cell-type-aware analysis of large-scale H&E datasets to uncover relationships between spatial cellular organization and patient outcomes.

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [233] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu,Jirong Zha,Ding Li,Leye Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PartCrop的统一成员推断方法，用于在更真实的环境下对视觉自监督模型进行成员推断攻击，并研究了其有效性和泛化性。此外，还评估了两种常见的防御方法并提出了改进的PartCrop-v2。


<details>
  <summary>Details</summary>
Motivation: 自我监督学习在利用大量未标记数据方面显示出潜力，但在视觉领域也面临着显著的隐私问题。因此，在对手通常面对黑盒系统的实际情况下，研究如何在未知自我监督训练方法和细节的情况下进行成员推断。

Method: 提出了一种名为PartCrop的方法，该方法通过裁剪图像中的对象部分来查询表示空间内的响应。此外，还评估了两种常见的防御方法：提前停止和差分隐私，并提出了改进方法：缩小裁剪比例范围。最后，提出了可扩展的PartCrop-v2，引入了两个结构上的改进。

Result: 广泛的攻击实验验证了PartCrop的有效性和泛化性。防御实验表明所有方法都是有效的。定量研究表明，从数据和模型方面进行扩展的影响，并提出了可扩展的PartCrop-v2。

Conclusion: PartCrop是一种有效的成员推断方法，具有良好的泛化性能。防御方法如提前停止、差分隐私和缩小裁剪比例范围均能有效抵御攻击。此外，提出的PartCrop-v2在实际场景中表现出良好的可扩展性。

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [234] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: The paper introduces SpikeVideoFormer, an efficient spike-driven video Transformer with linear temporal complexity. It uses spike-driven Hamming attention and performs well on video tasks like classification, human pose tracking, and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing SNN-based Transformers mainly focus on single-image tasks and do not effectively utilize SNNs' efficiency in video-based vision tasks.

Method: The authors design a spike-driven Hamming attention (SDHA) for adapting traditional real-valued attention to spike-driven attention. They also analyze various spike-driven space-time attention designs and identify an optimal scheme with linear temporal complexity.

Result: SpikeVideoFormer achieves state-of-the-art performance compared to existing SNN approaches, with over 15% improvement on human pose tracking and semantic segmentation tasks. It matches the performance of recent ANN-based methods while providing significant efficiency gains (x16, x10, and x5 improvements on three tasks).

Conclusion: SpikeVideoFormer is an efficient spike-driven video Transformer that demonstrates strong generalization ability and efficiency across diverse downstream video tasks.

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [235] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: This paper presents a novel unpaired training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data. It employs a multi-term loss function guided by adversarial training with multiple discriminators, using lightweight neural network architectures suitable for mobile devices as backbones.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the challenge of acquiring pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images when developing a learned ISP.

Method: The method proposed in this paper is an unpaired approach which employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks. This approach maintains content structure while learning color and texture characteristics from the target RGB dataset.

Result: The method was evaluated on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, the unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics.

Conclusion: The authors conclude that their unpaired training method for a learnable ISP has strong potential and can achieve high fidelity without needing direct correspondences between raw images and ground-truth data.

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [236] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: Vision language models (VLMs) are AI systems that can process multimodal input, but their comprehension of visuospatial properties needs improvement.


<details>
  <summary>Details</summary>
Motivation: To test the scene comprehension ability of VLMs by using descriptions of virtual objects in images.

Method: Systematically evaluate state-of-the-art VLMs with prompts involving virtual objects and analyze their responses.

Result: The results indicate that VLMs have inadequate ability to process virtual objects and reason about spatial relations.

Conclusion: VLMs need further development to better comprehend visuospatial properties of scenes depicted in images.

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [237] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS是一种优化的3D高斯点绘方法，通过单次训练和用户指定的超参数，实现语义上有意义且跨场景一致的数量-质量控制，同时保持强大的数量-质量性能。相比基线方法，它能以更少的高斯点达到更高的渲染质量，并支持广泛的调整范围和无级调控。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点绘方法虽然追求更好的数量-质量性能，但缺乏让用户直观调整这种权衡以适应实际需求（如在不同硬件和通信限制下的模型部署）的能力。

Method: ControlGS通过一次固定设置的训练运行和一个反映数量-质量偏好的用户指定超参数，能够自动在各种场景中找到理想的数量-质量权衡点，包括紧凑物体到大型室外场景。该方法实现了语义上有意义且跨场景一致的数量-质量控制。

Result: ControlGS不仅能在减少高斯点数量的同时提高渲染质量，还支持广泛调整范围内的无级控制，优于基线方法。

Conclusion: ControlGS提供了一种有效的解决方案，使得用户可以在不同场景下根据实际需求灵活地调整3D高斯点绘的数量-质量权衡，同时保持高性能。

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [238] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov,Petr Surovtsev,Karina Kvanchiani,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CV

TL;DR: This paper addresses two key aspects of isolated sign language recognition (ISLR): limited data for individual sign languages and ambiguity in labeling similar signs. It introduces Logos, a large Russian Sign Language dataset, demonstrating its utility for cross-language ISLR model training, including transfer learning. By explicitly annotating visually similar sign groups, the study improves model quality and outperforms state-of-the-art results on the WLASL dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in tackling two main challenges in ISLR: the scarcity of data for most individual sign languages and the ambiguity caused by similar signs having different semantic meanings.

Method: The method involves creating and utilizing the Logos dataset, a comprehensive Russian Sign Language dataset. The approach includes pre-training models on Logos for universal encoding in other SLR tasks, exploring cross-language transfer learning, and using explicit annotations for visually similar signs to enhance model performance.

Result: Results show that pre-trained models on the Logos dataset can effectively serve as universal encoders, improving accuracy especially in low-resource datasets through joint training with multiple classification heads. Explicit labeling of visually similar signs also enhances model quality.

Conclusion: The study concludes by surpassing current state-of-the-art results on the WLASL dataset and achieving competitive results on the AUTSL dataset, all while using a single stream RGB video model. The source code, dataset, and pre-trained models have been made publicly available.

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [239] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper presents UniEval, a new framework for evaluating unified multimodal models without extra models, images, or annotations. It includes UniBench and UniScore which provide more challenging benchmarks and align closely with human evaluations respectively.


<details>
  <summary>Details</summary>
Motivation: There is currently no unified evaluation framework for multimodal models that can provide an overall evaluation without redundancy.

Method: Introduced UniEval, which consists of UniBench (a holistic benchmark supporting unified and visual generation models) and UniScore (a corresponding metric). UniBench includes 81 fine-grained tags for high diversity.

Result: Experimental results show that UniBench is more challenging than existing benchmarks and UniScore closely aligns with human evaluations, surpassing current metrics.

Conclusion: UniEval provides a simplified and unified way to evaluate multimodal models, offering new insights into their unique values.

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [240] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt,Pedro Sanchez,Yongchen Yao,Steven McDonagh,Sotirios A. Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: The paper introduces CheXGenBench, an evaluation framework for synthetic chest radiograph generation that assesses fidelity, privacy risks, and clinical utility across 11 text-to-image models. It highlights inefficiencies in current protocols, establishes a new benchmark, and releases SynthCheX-75K dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in medical domain evaluations of generative AI, such as methodological inconsistencies, outdated comparisons, and lack of practical clinical value assessment.

Method: Developed CheXGenBench with standardized data partitioning and a unified evaluation protocol including over 20 quantitative metrics to evaluate generation quality, privacy vulnerabilities, and clinical applicability.

Result: Revealed critical inefficiencies in existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent comparisons. Established a new benchmark for medical AI community.

Conclusion: Introduced CheXGenBench as a rigorous evaluation framework, released SynthCheX-75K dataset, and provided resources at https://raman1121.github.io/CheXGenBench/.

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [241] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev,Nuno Goncalves*

Main category: cs.CV

TL;DR: A novel dual-branch classification strategy is proposed to enhance the robustness of face recognition systems against face morphing attacks.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to presentation attacks, especially face morphing, which allows one identity to impersonate another. Thus, there is a need for more robust face recognition systems.

Method: The method modifies the classification task by introducing a dual-branch classification strategy that effectively handles the ambiguity in the labeling of face morphs, incorporating morph images into the training process.

Result: The strategy has been validated on public benchmarks and proved effective in enhancing robustness against face morphing attacks.

Conclusion: This approach can be universally applied and integrated into existing face recognition training pipelines to improve classification-based recognition methods.

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [242] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 提出了一种基于子模子集选择技术的检索框架增强方法，用于处理多图像问答任务，该方法在大规模数据中表现出更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在处理单张图像相关的视觉-语言任务时表现良好，但在面对多张图像集合时（如多图像问答场景），其可扩展性和检索性能面临挑战。

Method: 通过引入子模子集选择技术，利用GraphCut等查询感知子模函数，在主要检索组件之前预选语义相关的图像子集，同时采用基于锚点的查询和数据增强策略来提升检索管道的效果。

Result: 实验表明，该方法在大规模数据集中显著提高了子模检索管道的有效性。

Conclusion: 所提出的增强方法可以有效应对多图像任务中的可扩展性和检索性能问题，特别是在大规模数据环境中。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [243] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang,Guohai Xu,Weinong Wang,Junjie Yang,Jie Lou,Yunhua Xue*

Main category: cs.CV

TL;DR: Recent advancements in MLLMs have improved multi-image comprehension, but existing benchmarks overlook whether models truly understand visual input. This paper defines IVM, where MLLMs give correct answers without full visual comprehension. By analyzing causal attention modules, the authors introduce 'attention accuracy' as a new metric and benchmark for quantifying IVMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of current benchmarks that focus on answer correctness rather than genuine visual comprehension by MLLMs.

Method: The method involves decoupling visual and textual modalities within the causal attention module to analyze how attention distribution converges on images associated with correct answers as network layers deepen. A scale-agnostic metric called 'attention accuracy' is introduced to evaluate visual understanding directly via internal mechanisms.

Result: The result is the development of a novel metric and benchmark for quantifying implicit visual misunderstandings (IVMs) in MLLMs, demonstrating effectiveness in both multimodal and unimodal scenarios.

Conclusion: The conclusion is that 'attention accuracy' provides a reliable way to assess MLLMs' visual understanding, highlighting its versatility and generalizability.

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [244] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: This paper explores the concept of feasibility in synthetic images and its impact on CLIP-based classifier performance. The authors introduce VariReal, a pipeline that edits source images with feasible or infeasible attributes. Experiments reveal minimal effect of feasibility on LoRA-fine-tuned CLIP performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture.

Method: Introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model.

Result: Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.

Conclusion: The study concludes that feasibility has minimal impact on CLIP-based classifier performance, suggesting that it may not be crucial to enforce feasibility in synthetic data generation for such models.

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [245] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses the limitation of current multimodal models in mathematical reasoning by introducing a new approach using code as supervision for cross-modal alignment, resulting in improved performance.


<details>
  <summary>Details</summary>
Motivation: Natural language image-caption datasets used for training Large Multimodal Models focus on natural scenarios and neglect important details of mathematical figures, which are crucial for problem-solving. This limits the progress of LMMs in multimodal mathematical reasoning.

Method: The authors propose leveraging code as supervision for cross-modal alignment because code contains all information needed to generate corresponding figures. They co-develop an image-to-code model (FigCodifier) and a large dataset (ImgCode-8.6M). Then, they use FigCodifier to synthesize new mathematical figures and create MM-MathInstruct-3M, a high-quality fine-tuning dataset. Finally, they introduce MathCoder-VL, trained with ImgCode-8.6M and fine-tuned on MM-MathInstruct-3M for solving multimodal math problems.

Result: MathCoder-VL achieves a new open-source state-of-the-art across six metrics. It outperforms GPT-4o and Claude 3.5 Sonnet in geometry problem-solving, improving scores by 8.9% and 9.2%, respectively.

Conclusion: The proposed method significantly enhances the ability of multimodal models in mathematical reasoning, particularly in geometry problem-solving. The authors will release their datasets and models at https://github.com/mathllm/MathCoder.

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [246] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang,Fan Zhang,Yufeng Cui,Haiwen Diao,Zhuoyan Luo,Huchuan Lu,Jing Liu,Xinlong Wang*

Main category: cs.CV

TL;DR: 现有的视觉标记化方法在下游训练中独立优化视觉标记器，假设视觉标记在不同任务中具有良好的通用性。然而，这种分离范式导致了表示瓶颈。为了解决这个问题，本文提出了ETT，一种端到端的视觉标记器调整方法，通过联合优化视觉标记化和目标自回归任务来解决这一问题。实验表明，与冻结的标记器基线相比，ETT在多模态理解和视觉生成任务上带来了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉标记化方法在下游训练中独立优化视觉标记器，假设视觉标记在不同任务中具有良好的通用性。然而，这种分离范式导致了表示瓶颈，即视觉标记化的损失可能成为目标任务的表示瓶颈。

Method: 提出了一种名为ETT的端到端视觉标记器调整方法，该方法通过联合优化视觉标记化和目标自回归任务来解决这一问题。ETT利用标记器词典的视觉嵌入，并通过重建和标题目标对视觉标记器进行端到端优化。

Result: 广泛的实验证明，与冻结的标记器基线相比，所提出的端到端视觉标记器调整方法在多模态理解和视觉生成任务上带来了2-6%的显著性能提升，同时保留了原有的重建能力。

Conclusion: 本文提出了一种简单的端到端视觉标记器调整方法ETT，该方法可以显著提高多模态理解和视觉生成任务的性能，同时保留原有的重建能力。希望这种方法能够为多模态基础模型赋能。

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [247] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang,Siyu Chen,Lihe Yang,Jialei Wang,Ziang Zhang,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene.


<details>
  <summary>Details</summary>
Motivation: To generate accurate, dense, and detailed metric depth maps by combining incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction.

Method: Design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. Second, develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors.

Result: The model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods.

Conclusion: This framework performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [248] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alex Schwing,Liangyan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为3D-Fixup的新框架，通过学习到的3D先验知识来引导2D图像编辑。该框架支持诸如物体平移和3D旋转等复杂的编辑情况。利用扩散模型的生成能力，并从视频数据中生成训练数据对（源帧和目标帧），同时结合Image-to-3D模型提供的3D指导，以确保高质量的3D引导。实验结果表明，3D-Fixup能够有效地支持复杂的、身份连贯的3D感知编辑，推动了扩散模型在真实图像操作中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管通过扩散模型对图像先验进行建模取得了显著进展，但由于对象仅通过单个图像指定，因此具有3D意识的图像编辑仍然具有挑战性。

Method: 提出了3D-Fixup框架，该框架使用基于训练的方法，利用扩散模型的生成能力，并从视频数据中生成源帧和目标帧的数据对。此外，还结合了一个Image-to-3D模型，以提供3D指导，将2D信息显式投影到3D空间。设计了一个数据生成管道，以确保在整个训练过程中高质量的3D引导。

Result: 通过整合这些3D先验，3D-Fixup有效地支持复杂的、身份连贯的3D感知编辑，实现了高质量的结果，推动了扩散模型在真实图像操作中的应用。

Conclusion: 3D-Fixup是一种有效的框架，可以支持复杂的3D感知图像编辑，其成功在于结合了高质量的3D先验和扩散模型的强大生成能力。

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


### [249] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: 本研究提出了一种基于行为的在线监测系统，利用定制的有向图神经网络（DGNN）对列车司机状态进行分类，并通过结合面部和骨骼特征提高了准确率。同时引入了一个新的数据集，以模拟病理状况，增强铁路安全。


<details>
  <summary>Details</summary>
Motivation: 传统系统如死人开关提供的警觉性检查有限且基础，因此需要一种更先进的在线行为监测系统来提高铁路安全。

Method: 本研究采用了一种定制的有向图神经网络（DGNN）来分类列车司机的状态，并进行了消融研究，比较了三种特征配置：仅骨骼、仅面部以及两者的组合。

Result: 实验结果表明，结合面部和骨骼特征在三类模型中实现了最高的准确率（80.88%），并且在二元警觉性分类中达到了超过99%的准确率。此外，还引入了一个新的数据集，首次将模拟的病理状况纳入列车司机监控中。

Conclusion: 本研究通过使用定制的有向图神经网络（DGNN）提出了一个在线行为监测系统，以提高铁路安全。实验结果表明，结合面部和骨骼特征可以实现最高的准确率，同时引入了一个新的数据集来模拟病理状况，为评估疲劳和健康相关的风险提供了更广泛的范围。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [250] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨骼模型的OptiGait-LGBM步态识别方法，在复杂户外环境中表现出色，具有低计算成本和高效内存使用的特点。


<details>
  <summary>Details</summary>
Motivation: 目前没有数据集能同时解决步态识别中的核心挑战，如不受控的户外环境、非重叠摄像头视角、光照变化和计算效率问题。因此，本文旨在提出一种能够应对这些挑战的解决方案。

Method: 本文采用骨骼模型方法，通过从骨骼关节关键点构建数据集，以减少内存使用。同时引入了一个基准数据集RUET-GAIT，用于表示复杂户外环境中的非受控步态序列。

Result: 与集成技术如随机森林和CatBoost相比，所提出的方法在准确率、内存使用和训练时间方面表现更优。

Conclusion: 本文提出了一种OptiGait-LGBM模型，该模型能够在各种约束条件下进行人员再识别，并且相比现有方法在准确率、内存使用和训练时间方面表现更优。该方法为现实场景提供了一种新颖、低成本且内存高效的视频步态识别解决方案。

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [251] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

TL;DR: 本文系统地回顾并改进了稀疏表示技术，提出了一种专门的网络架构、一个稀疏-密集的分割辅助任务以及一个由物理先验引导的去噪模块，从而在HD地图构建和中心线检测方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于缺乏定制设计而往往落后于密集表示，这限制了稀疏表示在在线HD地图构建中的竞争力。

Method: 我们引入了一个专门的网络架构，用于稀疏地图特征提取，一个稀疏-密集的分割辅助任务，以更好地利用几何和语义线索，并且有一个由物理先验引导的去噪模块来优化预测。

Result: 我们的方法在nuScenes数据集上实现了最先进的性能，显著推进了HD地图构建和中心线检测。SparseMeXt-Tiny在32帧/秒时达到55.5%的mAP，SparseMeXt-Base达到65.2%的mAP。扩大主干和解码器后，SparseMeXt-Large在超过20帧/秒时达到68.9%的mAP，为稀疏表示在HD地图构建中设立了新基准。

Conclusion: 这些结果突显了稀疏方法的未开发潜力，挑战了传统依赖密集表示的做法，并重新定义了该领域中的效率-性能权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [252] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/abs/2505.08811)
*Shijie Lian,Ziyi Zhang,Laurence Tianruo Yang and,Mengyu Ren,Debin Liu,Hua Li*

Main category: cs.CV

TL;DR: TUGS is a method for underwater 3D scene reconstruction that uses lightweight tensorized higher-order Gaussians and a physics-based AME module to simulate light attenuation and backscatter effects. It achieves high-quality images with faster rendering speeds and less memory usage compared to other NeRF-based and GS-based methods.


<details>
  <summary>Details</summary>
Motivation: Underwater 3D scene reconstruction is crucial for underwater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems.

Method: TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments.

Result: Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications.

Conclusion: TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications.

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [253] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

TL;DR: 本文研究了不同深度、配置信息和神经网络覆盖度量之间的关系，并提出了三种未来的研究方向，以进一步促进DNN模型的安全测试。


<details>
  <summary>Details</summary>
Motivation: 随着DNN的广泛应用和进步，不同类型的神经行为引起了关注，导致了各种神经网络覆盖度量的出现。然而，目前缺乏对这些覆盖度量的实证研究，特别是在分析模型深度、配置信息和神经网络覆盖之间的关系方面。

Method: 本文通过一系列实证实验，选择了LeNet、VGG和ResNet作为不同的DNN架构，并选择了10个不同深度的模型（从5层到54层）来比较和研究不同深度、配置信息和各种神经网络覆盖度量之间的关系。此外，还研究了修改后的决策/条件覆盖与数据集大小之间的关系。

Result: 本文研究了四种覆盖度量：主要功能、边界、层次和结构覆盖度量之间的关系和模式。实证实验表明，模型深度、配置信息和覆盖度量之间存在一定的关系。此外，修改后的决策/条件覆盖与数据集大小之间也存在一定的关系。

Conclusion: 本文提出了三种未来的研究方向，以进一步促进DNN模型的安全测试。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [254] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

TL;DR: 本文提出了一种基于外部条件的SFW采样器，用于防止扩散模型生成不安全内容，实验表明其效果良好且对图像质量影响小。


<details>
  <summary>Details</summary>
Motivation: 当前的方法基于模型自身的知识，大多数需要微调。本文探索了使用外部源来确保SBM的安全输出。

Method: 本文提出了一个安全的SFW采样器，该采样器实现了条件轨迹校正步骤，利用多模态模型作为条件来源，引导样本远离不需要的区域。此外，使用CLIP，我们的方法允许用户定义NSFW类别。

Result: 实验表明，所提出的SFW采样器有效减少了显式内容的生成，同时与其他基于微调的方法具有竞争力。此外，评估了SFW采样器对图像质量的影响，结果显示提出的校正方案成本较低，对不需要校正的样本影响可以忽略不计。

Conclusion: 本研究确认了SFW采样器适用于对齐的SBM模型，并展示了使用模型无关的条件来防止不想要的图像的潜力。

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [255] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

TL;DR: 本研究介绍了使用生成式AI来自动化城市规划的方法，通过适应Stable Diffusion模型并结合ControlNet，生成高保真卫星图像。研究展示了该模型在生成逼真和多样化城市景观方面的有效性，并强调了其在增强规划工作流程和公众参与方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法常常难以在大规模上生成现实且实用的设计。因此，我们希望通过生成式AI来自动化城市规划，创造特定于场地的城市布局并实现灵活的设计探索。

Method: 我们适应了最先进的Stable Diffusion模型，并通过ControlNet进行扩展，以生成基于土地利用描述、基础设施和自然环境的高保真卫星图像。我们还将卫星图像与来自OpenStreetMap的结构化土地利用和约束信息进行空间关联，以克服数据可用性的限制。

Result: 我们的模型在三个主要美国城市的数据上进行了测试，结果显示它能够通过改变土地利用配置、道路网络和水体生成逼真且多样的城市景观，促进跨城市学习和设计多样性。我们还系统地评估了不同语言提示和控制图像对卫星图像生成质量的影响。模型实现了高FID和KID分数，并在多样化的城市环境中表现出稳健性。城市规划者和普通公众的定性评估表明，生成的图像与设计描述和约束非常接近，并且通常优于真实图像。

Conclusion: 本研究建立了一个受控城市图像生成的基准，并突显了生成式AI作为增强规划工作流程和公众参与工具的潜力。

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [256] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: 本文提出了两种用于人群场景分析的方法：一种是基于自监督训练和多列CNN的人群计数模型，另一种是基于VGG19的时空异常检测模型。这两种方法在多个公开数据集上均表现出色，优于其他最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型需要大量标注数据，而标注过程耗时且成本高，因此本文旨在通过自监督训练和多列CNN来减少对标注数据的依赖，并提高模型在复杂场景下的性能。此外，本文还希望解决人群异常检测中的光照、环境条件和可扩展性等挑战。

Method: 本文采用了自监督训练和多列CNN来解决人群计数中的数据需求问题，并使用基于VGG19的时空模型进行异常检测，结合了卷积神经网络和长短期记忆块来提取空间和时间特征。

Result: 提出的模型在ShanghaiTech和UCFQNRF数据集上通过MAE和MSE进行了评估，结果显示其在人群计数任务中具有较高的准确性。同时，基于VGG19的时空模型在Hockey Fight和SCVD数据集上表现出色，优于其他最先进的方法。

Conclusion: 本文提出了两种用于人群场景分析的方法，即基于自监督训练和多列CNN的计数模型以及基于VGG19的时空异常检测模型。实验结果表明，这些模型在公开数据集上表现优于其他最先进的方法。

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [257] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文综述了生成式人工智能在自动驾驶系统中的作用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能是一种变革性的技术浪潮，通过其无与伦比的内容创作、推理、规划和多模态理解能力重新配置了行业。本文旨在全面而批判性地综述生成式人工智能在自动驾驶系统中的新兴作用。

Method: 本文首先总结了现代生成建模的原理和权衡，包括VAEs、GANs、扩散模型和大型语言模型（LLMs），然后映射了它们在图像、LiDAR、轨迹、占用、视频生成以及LLM引导的推理和决策中的前沿应用。

Result: 本文分类了实际应用，如合成数据工作流、端到端驾驶策略、高保真数字孪生系统、智能交通网络和跨领域迁移至具身人工智能，并识别了关键障碍和可能性，如在罕见情况下的全面泛化、评估和安全检查、预算限制的实施、监管合规、伦理问题和环境影响。

Conclusion: 本文综述了生成式人工智能在自动驾驶系统中的新兴作用，并提出了未来研究方向，为研究人员、工程师和政策制定者提供了前瞻性参考。

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [258] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/abs/2505.08882)
*Ali Almakhluk,Uthman Baroudi,Yasser El-Alfy*

Main category: cs.CV

TL;DR: 该研究开发了一种综合系统，用于检测和分类道路损坏异常（如坑洼和裂缝），并将数据传输到云端以采取适当行动。该系统还能实时统计道路异常情况，并向附近车辆广播警告信号，以提高道路安全性。


<details>
  <summary>Details</summary>
Motivation: 道路损坏异常（如坑洼和裂缝）已成为事故的重要且反复出现的原因。为了解决这个问题并提高道路安全性，开发了一个综合系统来检测坑洼和裂缝，并向附近车辆广播警告信号。

Method: 该系统利用树莓派、摄像头模块、深度学习模型、笔记本电脑和云服务进行模拟，以检测坑洼和裂缝，分类其大小，并将数据传输到云端以采取适当行动。此外，该系统还能实时统计道路异常情况。

Result: 该系统能够检测坑洼和裂缝，分类其大小，并将数据传输到云端以供相关部门采取行动。此外，该系统还能实时统计道路异常情况，并向附近车辆广播警告信号。

Conclusion: 该研究旨在通过通知相关部门和驾驶员有关坑洼和裂缝的存在，从而减轻由此产生的潜在事故，提高道路安全性。

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [259] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

TL;DR: 本研究通过结合图像处理技术和机器学习算法，开发了一种高效的皮肤癌诊断方法，实现了94%的准确率，为临床医生提供了有力的早期检测工具。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率上升，公众意识有限，临床专业知识不足，迫切需要先进的诊断辅助工具。人工智能在区分恶性与良性皮肤病变方面显示出前景。

Method: 采用图像处理技术与机器学习算法（特别是神经模糊和殖民竞争方法）的融合。

Result: 在ISIC数据库的560张图像数据集上，我们的方法达到了94%的显著准确率。

Conclusion: 我们的方法在皮肤癌诊断中展现出显著潜力，有助于临床医生早期检测黑色素瘤。

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [260] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种新的CoCo去噪器，用于解决Poisson逆问题中的挑战，并展示了其在视觉质量和定量指标上的优越性能。


<details>
  <summary>Details</summary>
Motivation: Poisson逆问题中的传统假设被违反，非扩张性会阻碍去噪性能。

Method: 我们提出了一个cocoercive conservative (CoCo)去噪器，结合了Hamiltonian正则化和频谱正则化来促进保守性和cocoerciveness。

Result: CoCo去噪器是弱凸函数的近似算子，可以建立具有隐式弱凸先验的恢复模型，并且PnP方法可以全局收敛到该模型的驻点。

Conclusion: 我们的方法在视觉质量和定量指标上都优于相关方法。

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [261] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

TL;DR: 本文介绍了Maya，一个开源的多语言视觉-语言模型（VLM），旨在解决现有大型VLM在低资源语言和多样化文化背景下的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型在主流语言上表现优异，但在低资源语言和多样化的文化背景下表现不佳。

Method: 本文提出了Maya，一个开源的多语言VLM，包含八个语言的图像-文本预训练数据集，并支持这些语言的图像-文本模型，以增强视觉-语言任务中的文化和语言理解能力。

Result: Maya提供了多语言图像-文本预训练数据集，并通过支持多种语言的模型增强了视觉-语言任务中的文化和语言理解能力。

Conclusion: Maya作为一个开源的多语言VLM，能够有效提升视觉-语言任务中对低资源语言和多样化文化背景的理解能力。

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [262] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DCS-Attention的新注意力模块，它通过选择信息丰富的通道来改进传统的自注意力机制，并在行人重识别任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络在行人重识别任务中的性能，需要一种有效的注意力机制来选择最相关信息的通道。传统自注意力机制无法有效地选择信息丰富的通道，因此需要一种新的方法来改进这一问题。

Method: 提出了一种新型的注意力模块，称为可微通道选择注意力模块（DCS-Attention模块）。该模块通过在计算注意力权重时选择信息丰富的通道来改进传统的自注意力机制，并以可微的方式进行特征通道的选择，从而可以无缝集成到深度神经网络的训练中。DCS-Attention兼容固定神经网络主干或可学习的主干与可微神经架构搜索（DNAS），分别称为DCS-FB和DCS-DNAS。此外，DCS-Attention受到信息瓶颈（IB）原理的启发，并推导出一种新的变分上界用于IB损失，该损失可以通过SGD优化，并被整合到带有DCS-Attention模块的网络的训练损失中。

Result: 在多个行人重识别基准测试中，使用DCS-FB和DCS-DNAS的实验表明，DCS-Attention显著提高了深度神经网络的预测准确性，证明了其在学习判别特征方面的有效性。

Conclusion: DCS-Attention模块在行人重识别任务中表现出色，能够显著提高深度神经网络的预测准确性，证明了其在学习判别特征方面的有效性。

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [263] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

TL;DR: PRIOR is a vision-language pre-training approach that prioritizes image-related tokens through differential weighting in the NTP loss, leading to improved performance and scalability.


<details>
  <summary>Details</summary>
Motivation: The naive NTP in standard large vision-language models (LVLMs) pre-training unintentionally fits the model to noise and increases the risk of hallucination because only a small subset of caption tokens directly relates to the visual content.

Method: PRIOR introduces a reference model, a text-only LLM, to weight each token based on its probability for LVLMs training. It uses a token-specific re-weighting term based on importance scores to adjust each token's loss.

Result: PRIOR achieves 19% and 8% average relative improvement on several vision-language benchmarks compared to NTP. It also shows higher scaling coefficients, indicating greater potential for performance gains with increasing compute and data.

Conclusion: PRIOR exhibits superior scaling properties and shows significant improvements in vision-language benchmarks compared to NTP.

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [264] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/abs/2505.08999)
*Wei-Long Tian,Peng Gao,Xiao Liu,Long Xu,Hamido Fujita,Hanan Aljuai,Mao-Li Wang*

Main category: cs.CV

TL;DR: 本文提出了一种自适应元梯度对抗攻击（AMGA）方法，该方法结合了多模型集成和元学习策略，能够显著提高对抗样本的攻击性能和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型暴露的众多安全问题逐渐影响了视觉跟踪方法在现实场景中的可靠应用，因此需要通过有效的对抗攻击来揭示现有视觉跟踪器的安全漏洞。

Method: AMGA方法结合了多模型集成和元学习策略，结合动量机制和高斯平滑，可以显著增强对抗样本的可迁移性和攻击效果。

Result: AMGA在OTB2015、LaSOT和GOT-10k等大规模数据集上的实验结果表明，它显著提高了对抗样本的攻击性能、可迁移性和欺骗性。

Conclusion: AMGA在大规模数据集上的实验结果表明，它显著提高了对抗样本的攻击性能、可迁移性和欺骗性。

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [265] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种多模态深度学习框架，结合CGM数据、人口统计学/微生物组数据和餐前食物图像来提高卡路里估计的准确性。在包含40多名参与者的实验中，该模型表现出色，优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 有效的饮食监测对于管理2型糖尿病至关重要，但准确估算卡路里摄入量仍然是一个重大挑战。虽然连续血糖监测仪（CGMs）提供了有价值的生理数据，但由于个体差异和餐食特异性变化，它们往往无法捕捉到餐食的完整营养概况。

Method: 我们引入了一个多模态深度学习框架，该框架联合利用CGM时间序列数据、人口统计学/微生物组数据和餐前食物图像来增强卡路里估计。我们的模型使用基于注意力的编码和卷积特征提取用于餐食图像，多层感知器用于CGM和微生物组数据，然后采用晚期融合策略进行联合推理。

Result: 我们在一个包含40多名参与者的精心整理的数据集上评估了我们的方法，包括同步的CGM、人口统计学和微生物组数据以及带有标准化卡路里标签的餐食照片。我们的模型实现了0.2544的均方根相对误差（RMSRE），比基线模型高出50%以上。

Conclusion: 这些发现表明，多模态传感在改善慢性病管理的自动化饮食评估工具方面具有潜力。

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [266] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/abs/2505.09073)
*J. Brennan Peace,Shuowen Hu,Benjamin S. Riggan*

Main category: cs.CV

TL;DR: 本文提出了一种新的领域自适应框架，以提高在大姿态差异下的面部识别性能。


<details>
  <summary>Details</summary>
Motivation: 由于在注册和查询图像之间存在显著的姿态差异，导致面部识别性能下降，因此需要一种新的方法来解决这个问题。

Method: 提出了一种新的领域自适应框架，通过使用共享的（联合）注意力映射和联合熵正则化损失来提高姿态不变性。

Result: 该框架在Profile（90°±）TAR @ 1% FAR方面分别提高了至少7.1%和1.57%。

Conclusion: 该框架在FaceScape和ARL-VTF数据集上表现出色，优于现有方法。

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [267] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/abs/2505.09092)
*Yuhang Wang,Abdulaziz Alhuraish,Shengming Yuan,Hao Zhou*

Main category: cs.CV

TL;DR: This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement, which includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing and global contributions. The dataset is multimodal, comprising full CAN bus streams, synchronized high-resolution dash-cam video, real-time outputs from Openpilot, and enhanced scene annotations generated by Vision Language Models.


<details>
  <summary>Details</summary>
Motivation: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access.

Method: The paper introduces OpenLKA, an open, large-scale dataset for LKA evaluation and improvement, which includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing and global contributions. It also describes the multimodal nature of the dataset, comprising full CAN bus streams, synchronized high-resolution dash-cam video, real-time outputs from Openpilot, and enhanced scene annotations generated by Vision Language Models.

Result: OpenLKA is a comprehensive dataset that spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions, and surrounding traffic. It integrates vehicle-internal signals with high-fidelity perception and rich semantic context, providing a platform for benchmarking the real-world performance of production LKA systems.

Conclusion: OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving.

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [268] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/abs/2505.09118)
*Dayong Liang,Changmeng Zheng,Zhiyuan Wen,Yi Cai,Xiao-Yong Wei,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架ISGR，用于增强视觉-语言模型对视觉场景中复杂交互的推理能力。该框架通过三个互补组件实现这一点：双流图构造器、针对性的交互查询以及长期记忆强化学习策略。实验结果表明，该方法在交互密集的推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统场景图主要关注空间关系，限制了视觉-语言模型（VLMs）对视觉场景中复杂交互的推理能力。本文解决了两个关键挑战：(1) 传统的检测到构造方法产生不集中、上下文无关的关系集；(2) 现有方法未能形成持久的记忆，以将交互推理推广到新场景。

Method: 我们提出了Interaction-augmented Scene Graph Reasoning (ISGR)，通过三个互补组件增强VLMs的交互推理能力。首先，我们的双流图构造器结合了SAM驱动的空间关系提取和交互感知的描述生成，以生成功能显著的场景图。其次，我们使用针对性的交互查询来激活VLMs中物体功能的潜在知识，将被动识别转化为对物体如何协同工作的主动推理。最后，我们引入了一种长期记忆强化学习策略，结合专门的交互聚焦奖励函数，将短暂模式转化为长期推理启发式方法。

Result: 广泛的实验表明，我们的方法在交互密集的推理基准上显著优于基线方法，特别是在复杂场景理解任务上有显著提升。

Conclusion: 我们的方法在交互密集的推理基准上显著优于基线方法，特别是在复杂场景理解任务上有显著提升。

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [269] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/abs/2505.09123)
*Guoying Liang,Su Yang*

Main category: cs.CV

TL;DR: 该研究利用Segment Anything Model (SAM)进行伪装物体检测（COD），提出了一个新的框架，通过点提示来促进SAM，实现了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 以往的研究认为SAM无法用于COD，但该研究揭示了如果适当促进，SAM是可行的。因此，该研究旨在探索如何利用大模型进行COD，并实现令人满意的实验结果。

Method: 该研究提出了一种新的框架，通过点提示来促进SAM，包括开发了Promotion Point Targeting Network (PPT-net)来预测给定候选点在图像中存在伪装物体的概率，以及开发了一个关键点选择(KPS)算法，以对比方式部署正负点提示来引导分割。

Result: 该研究在三个数据集上的六个指标下，实验结果优于现有方法，这是首次利用大模型进行COD的工作。

Conclusion: 该研究展示了通过利用SAM进行COD的现成方法，不仅在性能上优于从头设计专业模型，而且将问题转化为一个不太具有挑战性的任务，即寻找有信息量但不精确的提示。

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [270] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

TL;DR: 本文提出了一种基于颜色特征的轻量级异常检测框架，用于在资源受限和数据敏感条件下快速识别潜在威胁事件。该方法融合了无监督KMeans聚类和RGB通道直方图建模，实验结果表明其在战术暗杀预警、可疑物体筛查和环境剧烈变化监测中具有良好的效果和应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在无标签、数据不可获取的视频智能环境中进行高风险安全任务部署面临重大挑战。本文旨在提出一种基于颜色特征的轻量级异常检测框架，以快速识别和解释资源受限和数据敏感条件下的潜在威胁事件。

Method: 该方法融合了无监督KMeans聚类和RGB通道直方图建模，以实现关键帧中结构异常和颜色突变信号的复合检测。

Result: 实验以一个非洲国家发生的操作监控视频为研究样本，在无法访问原始数据的情况下成功识别出与高能光源、目标存在和反射干扰相关的多个高度异常帧。

Conclusion: 该方法在战术暗杀预警、可疑物体筛查和环境剧烈变化监测中具有很强的可部署性和战术解释价值。研究强调了颜色特征作为低语义战场信号载体的重要性，并将在未来通过结合图神经网络和时序建模进一步扩展战场智能感知能力。

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [271] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/abs/2505.09139)
*Lucas Choi,Ross Greer*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比类对齐分数（CCAS）的自动提示优化方法，以提高视觉-语言模型在目标检测任务中的性能，无需额外的模型训练或标记数据。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）虽然可以通过自然语言提示实现灵活的目标检测，但其性能会因提示的措辞而有所不同。因此，需要一种自动化的方法来优化提示，以提高检测准确性。

Method: 本文提出了一种基于对比类对齐分数（CCAS）的自动提示优化方法。该方法通过大型语言模型生成多样化的提示候选，并利用CCAS进行过滤，其中CCAS通过句子转换器的提示嵌入计算得出。

Result: 本文的方法在具有挑战性的目标类别上进行了评估，结果表明，自动选择高精度提示可以提高目标检测的准确性，而无需额外的模型训练或标记数据。

Conclusion: 本文提出了一种自动提示优化方法，通过引入对比类对齐分数（CCAS）来提高视觉-语言模型在目标检测任务中的性能，无需额外的模型训练或标记数据。该方法为基于VLM的检测系统提供了一种可扩展且与模型无关的替代方案。

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [272] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/abs/2505.09140)
*Zechao Guan,Feng Yan,Shuai Du,Lin Ma,Qingshan Liu*

Main category: cs.CV

TL;DR: TopoDiT-3D is a new method for 3D point cloud generation that incorporates topological information to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries.

Method: TopoDiT-3D is a Topology-Aware Diffusion Transformer with a bottleneck structure utilizing Perceiver Resampler to integrate topological information extracted through persistent homology into feature learning.

Result: Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency.

Conclusion: TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning.

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [273] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/abs/2505.09155)
*Yichen Shi,Zhuofu Tao,Yuhao Gao,Li Huang,Hongyang Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种基于分割的新型网络检测机制，解决了多模态大语言模型在理解电路图方面的不足，并创建了一个更大的数据集AMSnet 2.0。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在理解电路图方面存在局限，这主要是由于缺乏高质量的电路图-网表训练数据。现有的方法依赖于硬编码的启发式规则，难以应用于复杂或噪声较大的电路图。

Method: 本文提出了一种基于分割的新型网络检测机制，以解决当前多模态大语言模型在理解电路图方面的不足。同时，本文扩展了AMSnet数据集，创建了AMSnet 2.0，包含更多电路和更详细的信息。

Result: 本文提出的基于分割的网络检测机制具有高鲁棒性，并能恢复位置信息，从而实现电路图的数字重建。此外，本文创建的AMSnet 2.0数据集包含更多的电路和更详细的信息。

Conclusion: 本文提出了一个基于分割的新型网络检测机制，该机制具有高鲁棒性，并恢复了位置信息，允许电路图的数字重建。此外，本文扩展了AMSnet数据集，创建了AMSnet 2.0，包含更多电路和更详细的信息。

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [274] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为 DRRNet 的四阶段架构，通过全局和局部特征的结合，有效解决了伪装目标检测中的问题，并在基准数据集上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么过度依赖全局语义信息而丢失边缘细节（如发丝状细结构），要么仅依赖局部特征时受到相似背景的干扰（如植被图案）。

Method: 我们提出了 DRRNet，这是一种四阶段架构，具有“上下文-细节-融合-精炼”流程，以解决这些问题。具体来说，我们引入了一个 Omni-Context 特征提取模块来捕捉全局伪装模式，并设计了一个模块来形成场景理解和结构感知的双重表示，以及一个反向精炼模块来执行两阶段的逆精炼。

Result: 实验结果表明，DRRNet 在基准数据集上显著优于最先进的方法。

Conclusion: DRRNet 显著优于最先进的方法，并在基准数据集上展示了其有效性。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [275] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/abs/2505.09178)
*Yitao Zhu,Yuan Yin,Zhenrong Shen,Zihao Zhao,Haiyu Song,Sheng Wang,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: UniCAD 是一种统一的架构，利用预训练视觉基础模型的能力来处理 2D 和 3D 医疗图像，同时仅需要少量任务特定参数。它引入了效率和即插即用两个关键创新，并在多个医学数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发和部署多任务计算机辅助诊断 (CAD) 系统变得越来越具有挑战性和资源密集型，同时医疗影像社区缺乏一个开源 CAD 平台来促进高效和可扩展诊断模型的快速创建。

Method: UniCAD 引入了两种关键创新：(1) 效率：使用低秩适应策略将预训练视觉模型适应到医学图像领域；(2) 即插即用：一个模块化架构，结合冻结的基础模型和多个即插即用专家，以实现多样化的任务和无缝功能扩展。

Result: 在 12 个不同的医学数据集上进行的全面实验表明，UniCAD 在准确性和部署效率方面都优于现有方法。

Conclusion: UniCAD 是一种统一的架构，能够处理 2D 和 3D 医疗图像，并且在准确性和部署效率方面优于现有方法。

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [276] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/abs/2505.09188)
*Minjun Kim,Jaehyeon Choi,Jongkeun Lee,Wonjin Cho,U Kang*

Main category: cs.CV

TL;DR: 本文是对零样本量化（ZSQ）方法及其最新进展的全面概述，包括其定义、挑战、分类及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法通常依赖于访问训练数据，这在许多实际场景中由于隐私、安全或监管限制而不切实际。因此，零样本量化（ZSQ）作为一种无需任何真实数据即可实现量化的有前途的解决方案被提出。

Method: 本文首先对ZSQ问题进行了正式定义，然后根据数据生成策略对现有的ZSQ方法进行分类，并分析了它们的动机、核心思想和关键见解。

Result: 本文提供了对ZSQ方法的全面概述，并提出了未来的研究方向，以解决剩余的局限性并推动ZSQ领域的发展。

Conclusion: 本文提供了对零样本量化（ZSQ）方法及其最新进展的全面概述，并提出了未来的研究方向以解决剩余的局限性，推动ZSQ领域的发展。

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [277] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/abs/2505.09196)
*Tong Li,Lizhi Wang,Hansen Feng,Lin Zhu,Hua Huang*

Main category: cs.CV

TL;DR: 本文研究了低光图像增强中的基因效应，并提出参数动态进化（PDE）方法来解决这一问题。PDE通过模拟基因重组和突变，提高了图像增强的效果。


<details>
  <summary>Details</summary>
Motivation: 低光图像增强（LLIE）是计算摄影中的基础任务，旨在改善照明、减少噪声和提高图像质量。然而，现有的方法在设计复杂的神经网络模型时，发现某些参数重置为随机值可以意外地提高增强性能，这种现象被称为基因效应。

Method: 本文提出了一种基于参数正交生成技术的参数动态进化（PDE）方法，模拟基因重组和突变，以适应不同图像并减轻基因效应。

Result: 实验验证了所提方法的有效性，表明PDE能够有效缓解基因效应，提高低光图像增强的性能。

Conclusion: 本文通过引入参数动态进化（PDE）方法，解决了低光图像增强中的基因效应问题，并验证了该方法的有效性。

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [278] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/abs/2505.09251)
*Vineetha Joy,Aditya Anand,Nidhi,Anshuman Kumar,Amit Sethi,Hema Singh*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络的代理模型，以加速多层超表面雷达吸波结构的电磁响应预测，取得了高精度和高效的结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在电磁设计和优化中计算量大、耗时长且需要探索大量设计空间，因此需要一种更高效的替代方案。

Method: 采用基于卷积神经网络（CNN）的架构和Huber损失函数来估计RAS模型的反射特性。

Result: 该模型在1000个训练周期内实现了99.9%的余弦相似度和0.001的均方误差，并在计算时间和预测准确性方面表现出色。

Conclusion: 该模型通过全波仿真和实验验证，证明了其在计算时间和预测准确性方面的显著优势。

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [279] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/abs/2505.09252)
*Yinuo Wang,Yue Zeng,Kai Chen,Cai Meng,Chao Pan,Zhouping Tang*

Main category: cs.CV

TL;DR: 本研究评估了零样本多模态大语言模型（MLLMs）在颅内出血（ICH）二分类和亚型分类中的表现，并将其与传统深度学习方法进行了比较。结果显示，传统深度学习模型在这些任务中表现更好，而MLLMs在可解释性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 及时识别非对比CT上的颅内出血（ICH）亚型对于预后预测和治疗决策至关重要，但由于对比度低和边界模糊，这仍然具有挑战性。本研究评估了零样本多模态大语言模型（MLLMs）在ICH二分类和亚型分类中的表现，与传统深度学习方法进行比较。

Method: 本研究使用RSNA提供的数据集，包含192个非对比CT体积，比较了各种MLLM（包括GPT-4o、Gemini 2.0 Flash和Claude 3.5 Sonnet V2）与传统深度学习模型（包括ResNet50和Vision Transformer）的性能。精心设计的提示用于指导MLLM完成ICH存在性、亚型分类、定位和体积估计等任务。

Result: 结果表明，在ICH二分类任务中，传统深度学习模型全面优于MLLMs。在亚型分类中，MLLMs的表现也逊于传统深度学习模型，Gemini 2.0 Flash在宏平均精度上达到0.41，在宏平均F1分数上达到0.31。

Conclusion: 虽然MLLM在交互能力方面表现出色，但其在ICH亚型分类的整体准确性仍低于深度网络。然而，MLLM通过语言交互增强了可解释性，表明在医学影像分析中的潜力。未来的工作将集中在模型优化和开发更精确的MLLM以提高三维医学图像处理的性能。

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [280] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/abs/2505.09256)
*Jaemin Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

TL;DR: This paper introduces Pose-TTA, an approach that enhances face recognition performance by aligning faces at inference time without additional training, using a portrait animator and a weighted feature aggregation strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing face recognition performance often require re-training and testing for each dataset, which involves substantial effort. The study aims to propose a novel approach that aligns faces at inference time without additional training.

Method: Pose-TTA aligns faces at inference time without additional training by employing a portrait animator that transfers the source image identity into the pose of a driving image, along with a weighted feature aggregation strategy to address distortions or biases from synthetic data.

Result: Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance.

Conclusion: Pose-TTA consistently improves inference performance and is straightforward to integrate into existing face recognition pipelines.

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [281] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

TL;DR: 本文提出了一种少样本异常驱动生成方法（AnoGen），通过引导扩散模型生成现实且多样的异常，从而提高异常检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 由于工业检测中异常样本稀缺，现有的异常检测方法通过噪声或外部数据合成异常来解决这个问题。然而，合成异常与真实世界异常之间存在较大的语义差距，导致异常检测性能较弱。因此，需要一种能够生成现实且多样异常的方法。

Method: 我们提出了一个少样本异常驱动生成（AnoGen）方法，该方法通过仅使用少量真实异常来引导扩散模型生成现实且多样的异常，从而有助于训练异常检测模型。具体来说，我们的工作分为三个阶段：第一阶段，我们基于少量给定的真实异常学习异常分布，并将所学知识注入嵌入中；第二阶段，我们使用嵌入和给定的边界框来指导扩散模型在特定对象（或纹理）上生成现实且多样的异常；第三阶段，我们提出了一种弱监督的异常检测方法，以生成的异常训练更强大的模型。

Result: 实验结果表明，我们的生成异常有效提高了DRAEM和DesTSeg在分割任务中的AU-PR指标，分别提高了5.8%和1.5%。

Conclusion: 我们的方法在工业异常检测数据集MVTec上进行了实验，结果表明生成的异常有效提高了异常分类和分割任务的模型性能。

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [282] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种简单而有效的方法，即使用一个正常图像提示（OneNIP）来重建正常特征并恢复异常特征。此外，我们还提出了一种监督精炼器，通过使用真实的正常图像和合成的异常图像来回归重建误差，从而显著提高了像素级异常分割。OneNIP在三个工业异常检测基准测试中表现优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space.

Method: We propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). Additionally, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images.

Result: OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. The supervised refiner significantly improves pixel-level anomaly segmentation.

Conclusion: OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA.

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [283] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯视觉基础模型的通用异常分割方法，能够在没有语言引导的情况下有效地分割任何异常。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本和少样本视觉异常分割方法依赖于强大的视觉-语言模型，这些模型使用手动设计的文本提示检测未见过的异常。然而，视觉表示本质上与语言无关。因此，本文探索了纯视觉基础模型作为替代方案的可能性。

Method: 本文提出了一种名为MetaUAS的一次提示元学习框架，该框架在合成数据集上进行训练，并能推广到真实世界中分割任何新颖或未见过的视觉异常。此外，还提出了一种软特征对齐模块来处理提示和查询图像之间的几何变化。

Result: 本文的方法在零样本、少样本甚至全样本异常分割方法中显著优于以往的方法。

Conclusion: 本文提出了一种基于纯视觉基础模型的通用异常分割方法，能够有效地和高效地分割任何异常，仅需一个正常图像提示，并且无需语言引导。

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [284] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/abs/2505.09274)
*Fares Bougourzi,Abdenour Hadid*

Main category: cs.CV

TL;DR: 本文综述了医学影像分割的最新进展，重点介绍了生成式AI、少样本学习、基础模型和通用模型等方法，并讨论了它们的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割面临数据可访问性、注释复杂性、结构变异、医学影像模态变化和隐私限制等挑战。尽管取得了进展，但实现稳健的泛化和领域适应仍然是一个重大障碍，特别是由于一些提出的模型资源密集且依赖领域专业知识。

Method: 本文通过综述医学影像分割的最新进展，分析了生成式AI、少样本学习、基础模型和通用模型等方法的理论基础、最先进的技术和近期应用。

Result: 本文提供了这些方法的全面概述，并讨论了其内在限制、未解决的问题和未来的研究方向，以增强医学影像中分割模型的实用性和可及性。

Conclusion: 本文综述了医学影像分割的最新进展，探讨了生成式AI、少样本学习、基础模型和通用模型等方法，并讨论了这些方法的局限性和未来研究方向，以提高分割模型在医学影像中的实用性和可及性。

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [285] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

TL;DR: 本文提出了一种新的数据集，用于从卫星数据预测英国蝴蝶物种的存在，并开发了一种新的对比正则化损失方法，以提高多物种存在的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 由于遥感数据的广泛可用性和广泛覆盖，对可扩展的生物多样性监测方法的需求不断增加。传统上，遥感在生物多样性研究中的应用集中在绘制和监测生境上，但随着大规模公民科学野生动物观测数据的增加，最近的方法开始探索直接从卫星图像预测多物种存在。

Method: 我们实验优化了一个基于Resnet的模型，以从4波段卫星图像中预测多物种存在，并开发了一种软监督对比正则化损失，专门针对概率标签（如物种存在数据）。

Result: 我们发现该模型在高物种生物多样性地点特别优于平均率基线，并且通过对比正则化损失提高了预测准确性。

Conclusion: 我们的新数据集和对比正则化方法有助于解决从遥感数据准确预测物种生物多样性的开放性挑战，这对于高效的生物多样性监测至关重要。

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [286] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯点图的神经视频压缩模型，显著提高了编码速度，并为神经视频编解码器领域提供了一种全新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器在实时应用中存在计算需求高的问题，而神经视频编解码器（NVC）虽然具有更好的适应性和更高的压缩效率，但其高计算需求限制了其在实时应用中的使用。因此，本文旨在解决这一问题。

Method: 本文设计了一种视频处理流程，通过内容感知初始化策略和新颖的高斯帧间冗余减少机制，将基于高斯点图的图像编解码器的编码时间加快了88%。

Result: 本文提出的视频处理流程成功地将基于高斯点图的图像编解码器的编码时间加快了88%，并首次在神经视频编解码器领域实现了高斯点图的应用。

Conclusion: 本文提出了一种基于区域感兴趣（ROI）的神经视频压缩模型，利用2D高斯点图技术，显著提高了编码速度，并为神经视频编解码器领域提供了一种全新的解决方案。

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [287] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

TL;DR: 本文研究了医学视觉基础模型的缩放行为，并提出了一个大规模的医学视觉基础模型BioVFM，该模型在多个医学基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医学图像与自然数据有显著差异，因此在医学领域缺乏对缩放行为的深入理解，这使得开发医学视觉基础模型的关键因素尚不清楚。

Method: 我们通过自监督学习探索了在开发可扩展的医学视觉基础模型中模型大小、训练算法、数据大小和成像模态的缩放行为。为了支持可扩展的预训练，我们引入了BioVFM-21M，这是一个包含广泛生物医学图像模态和解剖结构的大规模生物医学图像数据集。

Result: 我们观察到扩大规模确实提供了好处，但因任务而异。额外的分析揭示了与缩放收益相关的几个因素。最后，我们提出了BioVFM，一个在2100万张生物医学图像上预训练的大规模医学视觉基础模型，在12个医学基准测试中表现优于之前最先进的基础模型。

Conclusion: 我们的结果表明，尽管扩大规模对于追求更好的性能是有益的，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展的医学基础模型的关键考虑因素。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [288] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: This paper introduces MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. The model uses pseudo-labels and a joint embedding space to align emotional semantics across multiple views, enhancing discriminability through a novel contrastive learning strategy and a gradient-friendly loss function.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce a vision-language model for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data.

Method: MultiviewVLM integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. It proposes a joint embedding space that aligns multiview representations without requiring explicit supervision. A novel multiview contrastive learning strategy is used to enhance the discriminability of the model, along with a gradient-friendly loss function and distributed training optimization.

Result: Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.

Conclusion: MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [289] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

TL;DR: Marigold is a method that adapts pretrained latent diffusion models for dense image analysis tasks, achieving state-of-the-art results with minimal modifications.


<details>
  <summary>Details</summary>
Motivation: In data-scarce settings, the quality of pretrained models becomes crucial for effective transfer learning. The work aims to adapt pretrained latent diffusion models for dense image analysis tasks.

Method: Marigold is a family of conditional generative models and a fine-tuning protocol that extracts knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks.

Result: Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization.

Conclusion: Marigold demonstrates state-of-the-art zero-shot generalization by extracting knowledge from pretrained latent diffusion models and adapting them for dense image analysis tasks.

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [290] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

TL;DR: 本文介绍了RobustSpring，这是一个新的计算机视觉基准，专注于模型对图像退化的鲁棒性，旨在促进同时具备准确性和韧性的模型。


<details>
  <summary>Details</summary>
Motivation: 标准的光学流、场景流和立体视觉算法基准通常关注模型准确性，而不是对图像退化（如噪声或雨）的鲁棒性。因此，模型对这些现实世界扰动的弹性在很大程度上未被量化。

Method: 提出RobustSpring数据集和基准，用于评估光学流、场景流和立体模型对图像退化的鲁棒性。应用20种不同的图像退化，包括噪声、模糊、颜色变化、质量退化和天气失真，并创建了20,000张退化的图像。

Result: 通过新的图像退化鲁棒性度量，RobustSpring使模型鲁棒性的比较成为可能。与Spring基准的集成使得准确性和鲁棒性的公共双轴评估成为可能。基准测试了一组精选的初始模型，观察到准确的模型不一定具有鲁棒性，且鲁棒性因退化类型而异。

Conclusion: RobustSpring是一个新的计算机视觉基准，将鲁棒性作为首要考虑因素，以促进结合准确性和韧性的模型。

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [291] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/abs/2505.09372)
*Siyuan Yan,Xieji Li,Ming Hu,Yiwen Jiang,Zhen Yu,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文介绍了MAKE，一种用于零样本皮肤病任务的多方面知识增强的视觉-语言预训练框架。该框架通过多方面对比学习、细粒度对齐和诊断引导加权方案，提高了皮肤病学中的视觉-语言预训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言预训练方法在皮肤病学中的有效性受到文本长度限制和缺乏结构化文本的限制。全面的皮肤病描述需要多个知识方面，这些方面超出了标准文本约束。

Method: MAKE框架引入了多方面对比学习策略、细粒度对齐机制和诊断引导加权方案，通过大型语言模型将临床叙述分解为增强知识的子文本，并将子标题与诊断相关的图像特征对齐，同时根据临床重要性优先级自适应地优先考虑不同的子标题。

Result: MAKE在零样本皮肤疾病分类、概念注释和跨模态检索任务的八个数据集上显著优于最先进的视觉-语言预训练模型。

Conclusion: MAKE显著优于最先进的视觉-语言预训练模型，在零样本皮肤疾病分类、概念注释和跨模态检索任务的八个数据集上表现出色。

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [292] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/abs/2505.09379)
*Ali Rida Sahili,Najett Neji,Hedi Tabia*

Main category: cs.CV

TL;DR: 本文综述了文本驱动运动生成的研究现状，涵盖了方法分类、数据集、评估方法以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 文本驱动运动生成提供了一种强大且直观的方式来直接从自然语言创建人类运动，这使得它在虚拟现实、游戏、人机交互和机器人等领域特别有用。

Method: 本文首先回顾了传统的运动合成观点，然后对现代文本到运动生成方法进行了全面且结构化的调查，从两个互补的角度进行分类：(i) 架构，将方法分为基于VAE、基于扩散和混合模型；(ii) 运动表示，区分离散和连续运动生成策略。

Result: 本文提供了对文本到运动生成方法的全面调查，包括常用的数据库、评估方法和最近的基准测试，以推动该领域的发展。

Conclusion: 本文旨在总结当前文本驱动运动生成的研究现状，指出其关键挑战和局限性，并提出未来研究的潜在方向。

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [293] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

TL;DR: 研究描述了一个名为NeoMedSys的放射学软件平台，该平台可以实现AI模型的高效部署和优化。通过在真实临床环境中运行NeoMedSys三个月，评估了其可行性与效果，并专注于改进一个内部开发的用于颅内出血检测的AI模型（VIOLA-AI）。结果表明，NeoMedSys促进了AI模型的迭代改进，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models.

Method: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway with suspected traumatic brain injury or patients with suspected stroke. ICH classification performance was assessed as VIOLA-AI encountered new data and underwent pre-planned model retraining.

Result: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).

Conclusion: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. The iterative refinement process yielded a marked improvement in classification sensitivity and specificity, demonstrating the value of real-time radiologist feedback.

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [294] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦分割框架FedSaaS，通过引入类样本作为标准，并在服务器和客户端采用对抗机制和多级对比损失，以解决类别一致性表示问题，实验结果表明该框架优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的研究通常忽视了语义空间内的细粒度类别关系，特别是在领域转移的情况下，这种忽视导致了类别表示之间的模糊性。为了解决这个挑战，我们提出了FedSaaS框架，以解决类别一致性表示问题。

Method: 我们提出了一种新颖的联邦分割框架，称为FedSaaS，通过引入类样本作为本地和全局级别类表示的标准，以及在服务器端利用上传的类样本建模类原型，监督客户端的全局分支，确保与全局级别表示对齐。在客户端，我们引入了对抗机制来协调全局和局部分支的贡献，从而实现一致的输出。此外，在两侧都采用了多级对比损失，以在相同语义空间中强制两个级别表示之间的一致性。

Result: 在多个驾驶场景分割数据集上的广泛实验表明，我们的框架优于最先进的方法，显著提高了平均分割准确率，并有效解决了类别一致性表示问题。

Conclusion: 我们的框架在多个驾驶场景分割数据集上进行了广泛的实验，结果表明它优于最先进的方法，显著提高了平均分割准确率，并有效解决了类一致性表示问题。

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [295] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/abs/2505.09406)
*Yue Wen,Liang Song,Yijia Liu,Siting Zhu,Yanzi Miao,Lijun Han,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FreeDriveRF的新方法，用于仅使用顺序RGB图像重建动态驾驶场景，无需姿态输入。通过语义监督解耦动态和静态部分，并利用光流和动态流来改善动态建模和姿态优化过程。实验结果表明，该方法在动态场景建模方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的许多方法严重依赖准确的姿态输入和多传感器数据，导致系统复杂性增加。因此，我们需要一种更简单的方法来重建动态驾驶场景。

Method: 我们提出了FreeDriveRF，它仅使用顺序RGB图像来重建动态驾驶场景，而不需要姿态输入。我们创新地在早期采样级别使用语义监督解耦动态和静态部分，减轻了图像模糊和伪影。为了克服单目相机中的物体运动和遮挡问题，我们引入了扭曲射线引导的动态物体渲染一致性损失，利用光流更好地约束动态建模过程。此外，我们结合估计的动态流来约束姿态优化过程，提高了无界场景重建的稳定性和准确性。

Result: 在KITTI和Waymo数据集上进行的大量实验表明，我们的方法在动态场景建模方面表现优越。

Conclusion: 我们的方法在动态场景建模方面表现出色，特别是在自动驾驶中。

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [296] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/abs/2505.09413)
*Ma Changfeng,Bi Ran,Guo Jie,Wang Chongjun,Guo Yanwen*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云渲染方法，通过从点云预测2D高斯，无需额外细化即可直接用于渲染，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的方法从点云预测NeRF或3D高斯以实现照片级真实感渲染，但仍依赖于类别先验、密集点云或额外的细化。因此，我们引入了一种新颖的点云渲染方法，通过从点云预测2D高斯。

Method: 我们引入了一种新颖的点云渲染方法，通过从点云中预测2D高斯。该方法包含两个具有完整补丁架构的相同模块，使网络能够推广到多个数据集。模块利用点云信息（包括法线、颜色和距离）对高斯进行归一化和初始化。然后使用分割解码器通过复制高斯并预测更准确的结果来细化初始高斯，使我们的方法有效地适应稀疏点云。

Result: 我们的方法在各种数据集上进行了广泛的实验，结果证明了其优越性和泛化能力，并实现了SOTA性能。

Conclusion: 我们的方法在各种数据集上进行了广泛的实验，结果证明了其优越性和泛化能力，并实现了SOTA性能。

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [297] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/abs/2505.09415)
*Hongyang Wang,Yichen Shi,Zhuofu Tao,Yuhao Gao,Liepiao Zhang,Xun Lin,Jun Feng,Xiaochen Yuan,Zitong Yu,Xiaochun Cao*

Main category: cs.CV

TL;DR: This paper proposes FaceShield, a MLLM for face anti-spoofing (FAS), along with pre-training and SFT datasets. FaceShield can determine face authenticity, identify spoofing attack types, provide reasoning, and detect attack areas. It outperforms previous models on FAS tasks.


<details>
  <summary>Details</summary>
Motivation: Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. There is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task.

Method: We propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. We employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use a prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model's generalization ability.

Result: FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks.

Conclusion: FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization.

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [298] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2505.09422)
*Xiangyuan Peng,Yu Wang,Miao Tang,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态融合框架MoRAL，用于提高自动驾驶系统中3D物体检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法往往忽略了雷达点云由于物体移动导致的帧间错位，并未充分利用4D雷达中的物体动态信息。

Method: 提出了一种名为MoRAL的运动感知多帧4D雷达和LiDAR融合框架，包括运动感知雷达编码器（MRE）和运动注意力门控融合（MAGF）模块。

Result: MoRAL在View-of-Delft (VoD)数据集上取得了最好的性能，特别是在整个区域和驾驶走廊中的mAP以及对行人和骑自行车者的检测精度。

Conclusion: MoRAL在View-of-Delft (VoD)数据集上的广泛评估表明，它优于现有方法，在整个区域和驾驶走廊中分别达到了最高的mAP 73.30%和88.68%。此外，该方法在行人和骑自行车者检测中也取得了最佳成绩。

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [299] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/abs/2505.09433)
*Jiahao Zhu,Kang You,Dandan Ding,Zhan Ma*

Main category: cs.CV

TL;DR: SerLiC 是一种基于序列化的神经压缩框架，用于 LiDAR 反射率数据的高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的神经压缩方法在 LiDAR 反射率属性方面研究不足，需要一种更有效的压缩方法。

Method: SerLiC 通过扫描顺序序列化将 3D LiDAR 点云转换为 1D 序列，并使用 Mamba 进行高效序列建模。

Result: SerLiC 在压缩体积上实现了超过 2 倍的减少，并且在参数使用上比现有方法少 98%。

Conclusion: SerLiC 是一种有效的 LiDAR 反射率压缩框架，能够显著减少数据体积并提高压缩效率。

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [300] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

TL;DR: 本文介绍了Endo-CLIP，这是一种用于结肠镜图像分析的新自监督框架，通过三个阶段解决了非信息性背景图像、复杂医学术语和模糊的多病变描述等挑战，并在零样本和少量样本的息肉检测和分类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在图像-文本结肠镜记录上进行预训练对于改善内窥镜图像分析具有巨大的潜力，但面临包括非信息性背景图像、复杂的医学术语和模糊的多病变描述等挑战。

Method: Endo-CLIP是一种新颖的自监督框架，通过三个阶段（清理、调谐和统一）来解决这些挑战，包括移除背景帧、利用大型语言模型提取临床属性以进行细粒度对比学习，以及使用患者级交叉注意力来解决多息肉模糊问题。

Result: 广泛的实验表明，Endo-CLIP在零样本和少量样本的息肉检测和分类中显著优于最先进的预训练方法。

Conclusion: Endo-CLIP显著优于最先进的预训练方法，在零样本和少量样本的息肉检测和分类中，为更准确和临床相关的内窥镜分析铺平了道路。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [301] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/abs/2505.09450)
*Yuelin Zhang,Qingpeng Ding,Long Lei,Yongxuan Feng,Raymond Shing-Yan Tang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出了MrTrack，一种基于Mamba的注册机制的穿刺针跟踪器，能够在快速往复运动和成像退化的情况下提供更准确、鲁棒和高效的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一种能够应对快速往复运动的超声引导细针穿刺活检（FNA）针跟踪器。

Method: MrTrack使用基于Mamba的注册机制，通过序列蒸馏从每个历史搜索图中提取全局上下文，并将这些时间线索存储在注册银行中。然后，基于Mamba的注册检索器从注册银行中检索时间提示，以在当前视觉特征由于快速往复运动和成像退化而暂时不可用时提供外部提示。此外，提出了一种自监督的注册多样化损失，以鼓励学习到的注册中的特征多样性及维度独立性，从而缓解特征崩溃问题。

Result: 在电动和手动穿刺数据集上进行的全面实验表明，MrTrack不仅在准确性和鲁棒性方面优于最先进的跟踪器，而且在推理效率方面也表现出色。

Conclusion: MrTrack在准确性和鲁棒性方面优于最先进的跟踪器，并且在推理效率方面表现出色。

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [302] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/abs/2505.09455)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的编码器-解码器模型，通过引入去噪序列转换任务来改进时空动作检测，从而提高低置信度区域的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的时空动作检测方法在需要高召回率、低精确度的场景下缺乏上下文理解，导致许多误报。本文旨在通过游戏级别的推理和添加去噪序列转换任务来解决这一限制。

Method: 本文提出了一种基于Transformer的编码器-解码器模型，用于处理嘈杂的、与上下文无关的以球员为中心的预测，并结合干净的游戏状态信息，通过建模扩展的时间上下文和联合推理团队级动态来生成“去噪”的动作序列。

Result: 本文的方法在低置信度区域提高了精度和召回率，实现了更可靠的事件提取，并补充了现有的基于像素的方法。

Conclusion: 本文提出了一种通过引入去噪序列转换任务来改进时空动作检测的方法，该方法通过建模扩展的时间上下文和联合推理团队级动态，提高了低置信度区域的精度和召回率，从而实现了更可靠的事件提取。

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [303] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

TL;DR: 本文提出了$	ext{SaPE}^2$，一种新的位置编码方法，能够动态适应位置表示，提高视觉变压器在计算机视觉任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码技术无法有效捕捉图像块之间的语义感知位置关系，这限制了模型的泛化能力、平移等变性和处理重复或结构化模式的能力。

Method: 提出了一种具有语义意识的二维语义感知位置编码（$	ext{SaPE}^2$），该方法通过利用局部内容而不是固定的线性位置关系或空间坐标来动态适应位置表示。

Result: 我们的方法增强了模型在不同图像分辨率和尺度上的泛化能力，提高了平移等变性，并更好地聚合了视觉相似但空间上距离较远的块的特征。

Conclusion: 通过将$	ext{SaPE}^2$集成到视觉变压器中，我们弥合了位置编码与感知相似性之间的差距，从而提高了计算机视觉任务的性能。

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [304] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2505.09484)
*Yingjie Ma,Xun Lin,Zitong Yu,Xin Liu,Xiaochen Yuan,Weicheng Xie,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMDA的多模态去噪和对齐框架，以解决多模态面部反欺骗（FAS）方法在泛化能力上的不足。通过利用CLIP的零样本泛化能力，MMDA框架有效地抑制了多模态数据中的噪声，并通过改进的注意力机制减轻了领域和模态噪声的影响。实验结果表明，MMDA框架在跨领域泛化和多模态检测准确性方面优于现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态FAS方法在有效泛化方面存在困难，主要是由于模态特定偏差和领域转移问题。为了应对这些挑战，我们提出了MMDA框架。

Method: 我们引入了MMDA框架，该框架利用CLIP的零样本泛化能力，通过去噪和对齐机制有效抑制多模态数据中的噪声，从而显著提高跨模态对齐的泛化性能。此外，我们设计了MD2A模块、RS2对齐策略和U-DSA模块来增强模型的适应性和表示能力。

Result: MMDA框架在四个基准数据集的不同评估协议下的实验结果表明，它在跨领域泛化和多模态检测准确性方面优于现有最先进的方法。

Conclusion: 我们的实验结果表明，MMDA框架在跨领域泛化和多模态检测准确性方面优于现有的最先进方法。

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [305] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

TL;DR: 本文介绍了Flash-VL 2B，这是一种优化视觉-语言模型以实现超低延迟和高吞吐量的新方法，同时保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 为了优化视觉-语言模型（VLMs）以用于实时应用，目标是实现超低延迟和高吞吐量，同时不牺牲准确性。

Method: 通过先进的架构增强和高效的计算策略，Flash-VL 2B 旨在通过减少处理时间来最大化吞吐量，同时保持在多个视觉-语言基准上的竞争力。方法包括定制的架构选择、令牌压缩机制、数据整理、训练方案以及一种称为隐式语义拼接的新图像处理技术。

Result: 通过在11个标准VLM基准上的广泛评估，我们展示了Flash-VL 2B 在速度和准确性方面都达到了最先进水平。

Conclusion: Flash-VL 2B 是一种有前途的解决方案，适用于资源受限环境和大规模实时应用。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [306] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/abs/2505.09528)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 本文提出了一种结合符合预测和近似后验采样的方法，用于构建FRIQ的边界，这些边界在用户指定的误差概率下是保证成立的。


<details>
  <summary>Details</summary>
Motivation: 在成像逆问题中，了解恢复的图像与真实图像之间的距离非常重要，尤其是在医疗成像等安全关键应用中。然而，由于不知道真实图像，计算FRIQ是非显而易见的。

Method: 本文结合了符合预测和近似后验采样，以构建FRIQ的边界。

Result: 本文在图像去噪和加速磁共振成像（MRI）问题上展示了所提出的方法。代码可在https://github.com/jwen307/quality_uq上获得。

Conclusion: 本文提出了一种结合了符合预测和近似后验采样的方法，用于构建FRIQ的边界，这些边界在用户指定的误差概率下是保证成立的。

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [307] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: 本研究利用监督卷积神经网络（CNN）模型，从时间事件记录中无接触重建个体的心脏信号，并验证了该技术在远程心率监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 时间事件摄像机是一种新技术，可以以极低延迟和低功耗记录场景信息。事件摄像机输出包含像素级光强度变化的事件流，其动态范围和时间分辨率高于传统摄像机。因此，研究如何从时间事件记录中无接触重建心脏信号具有重要意义。

Method: 本研究利用监督卷积神经网络（CNN）模型，从个体面部的时间事件记录中无接触重建其心脏脉冲信号。训练了一个端到端的模型，以从事件流的二维表示中提取心脏信号，并根据计算的心率准确性评估模型性能。

Result: 实验结果确认了面部区域的生理心脏信息在事件流中得到有效保留。与基于标准摄像机帧的基线模型相比，基于事件帧的模型实现了更低的均方根误差（RMSE）。此外，以60和120 FPS生成的事件帧训练的模型表现优于30 FPS标准摄像机的结果。

Conclusion: 实验结果表明，生理心脏信息在面部区域有效地保留在事件流中，展示了这种新型传感器在远程心率监测中的潜力。

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [308] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/abs/2505.09562)
*Nicola Marinello,Simen Cassiman,Jonas Heylen,Marc Proesmans,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，用于3D panoptic scene completion，并展示了其在现有3D语义场景完成模型上的扩展能力。


<details>
  <summary>Details</summary>
Motivation: 3D panoptic scene completion目前研究较少，但它是路径规划和决策制定的关键任务。因此，需要一种新的方法来解决这一问题。

Method: 本文提出了一个Object Module和Panoptic Module，可以轻松集成到文献中的3D占用和场景完成方法中。此外，该方法利用了占用基准中的可用注释，使个体对象形状能够作为可微问题进行学习。

Result: 本文提出的框架可以有效地扩展现有的3D语义场景完成模型，并且能够学习个体对象形状作为可微问题。

Conclusion: 本文提出了一种新的框架，用于3D panoptic scene completion，并展示了其在现有3D语义场景完成模型上的扩展能力。

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [309] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/abs/2505.09564)
*Anne-Marie Rickmann,Stephanie L. Thorn,Shawn S. Ahn,Supum Lee,Selen Uman,Taras Lysyy,Rachel Burns,Nicole Guerrera,Francis G. Spinale,Jason A. Burdick,Albert J. Sinusas,James S. Duncan*

Main category: cs.CV

TL;DR: 本文研究了基础模型是否可以为猪心脏CT生成足够准确的伪标签，并提出了一种无需手动标注数据的自我训练方法，以提高分割质量。


<details>
  <summary>Details</summary>
Motivation: 由于物种之间的差异导致从人类数据到猪数据的领域转移问题，因此需要研究基础模型是否可以为猪心脏CT生成足够准确的伪标签。

Method: 我们提出了一种简单的自我训练方法，通过迭代更新来提高分割质量，而无需手动标注的猪数据。

Result: 我们的实验表明，这种自我训练过程不仅提高了分割准确性，还平滑了连续帧之间的时序不一致性。

Conclusion: 虽然我们的结果令人鼓舞，但仍存在改进的空间，例如通过引入更复杂的自我训练策略，以及探索其他基础模型和其他心脏成像技术。

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [310] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，利用扩散变压器生成语义丰富的CLIP图像特征，与传统的VAE表示相比。这种方法在训练效率和生成质量上都有所提高。此外，我们展示了统一模型的顺序预训练策略（先进行图像理解，然后进行图像生成）具有实际优势，因为它在保持图像理解能力的同时发展了强大的图像生成能力。最后，我们精心策划了一个高质量的指令调优数据集BLIP3o-60k，用于图像生成，通过提示GPT-4o使用涵盖各种场景、物体、人类手势等的多样化标题。基于我们的创新模型设计、训练方案和数据集，我们开发了BLIP3-o，一套最先进的统一多模态模型。BLIP3-o在大多数流行的基准测试中表现出色，涵盖了图像理解和生成任务。为了促进未来的研究，我们完全开源了我们的模型，包括代码、模型权重、训练脚本和预训练及指令调优数据集。


<details>
  <summary>Details</summary>
Motivation: Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies.

Method: We introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more.

Result: This design yields both higher training efficiency and improved generative quality. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.

Conclusion: BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [311] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/abs/2505.09571)
*Guillermo Gomez-Trenado,Pablo Mesejo,Oscar Cordón,Stéphane Lathuilière*

Main category: cs.CV

TL;DR: SAGE is a new technique that improves image editing by using self-attention mechanisms in diffusion models, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of editing real images remains difficult due to computational intensity and poor reconstructions, prompting the need for a more efficient and effective method.

Method: SAGE (Self-Attention Guidance for Image Editing) leverages pre-trained diffusion models and incorporates a novel guidance mechanism using self-attention layers of the diffusion U-Net.

Result: SAGE outperforms other methods in seven out of 10 quantitative analyses and ranks second or third in the remaining three. It also receives high user preference in a comprehensive study.

Conclusion: SAGE demonstrates superior performance in image editing compared to other methods, as shown through quantitative and qualitative evaluations and a user study.

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [312] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

TL;DR: 本文提出了一种变分VQA方法，通过使用IVON算法来提高多模态模型的可靠性，实验结果表明该方法在不牺牲准确性的情况下显著提升了校准和拒绝接受能力。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在分布外（OOD）设置中存在可靠性问题，如过度自信和校准不良。

Method: 我们采用了一种称为IVON的变分算法，而不是使用AdamW微调视觉-语言模型。

Result: 我们的方法在不牺牲AdamW准确性的情况下提高了校准和拒绝接受能力。例如，与AdamW微调相比，预期校准误差减少了50%以上，并且在固定风险为1%的情况下，覆盖率提高了4%。在分布偏移存在的情况下，性能提升更高，当50%的测试案例是OOD时，覆盖率提高了8%。

Conclusion: 我们提出了变分VQA方法，以提高多模态模型的可靠性。

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [313] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/abs/2505.09608)
*Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的方法，用于对图像中的光源进行细粒度、参数化的控制。


<details>
  <summary>Details</summary>
Motivation: 现有的重新照明方法要么依赖于多个输入视图来进行逆向渲染，要么无法提供对光照变化的显式控制。

Method: 我们对一个小规模的真实原始照片对集进行微调，并结合大规模合成图像，以激发其逼真的光照先验。我们利用光的线性特性来合成显示受控光照变化的图像对。

Result: 我们训练了一个模型，可以精确地改变光照，并且可以显式控制光照强度和颜色。

Conclusion: 我们的方法在光照编辑方面表现出色，并且优于现有方法。

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [314] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/abs/2505.09615)
*Yung-Hsuan Lai,Janek Ebbers,Yu-Chiang Frank Wang,François Germain,Michael Jeffrey Jones,Moitreya Chatterjee*

Main category: cs.CV

TL;DR: 本文提出了一种新的弱监督音频-视觉视频解析方法UWAV，通过考虑伪标签的不确定性并引入基于特征混合的训练正则化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据的成本高昂，AVVP技术需要在弱监督设置下进行训练，但现有的方法在生成伪标签时缺乏段间依赖性，并且存在对段中不存在标签的偏差。

Method: UWAV通过考虑伪标签的不确定性并引入基于特征混合的训练正则化来改进训练。

Result: UWAV在多个指标和两个不同数据集上表现出色，证明了其有效性。

Conclusion: UWAV在多个指标和两个不同数据集上优于最先进的方法，证明了其有效性和泛化能力。

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [315] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales,Ayah Elsayed,Debbie Zhao,Filip Loncaric,Ainhoa Aguado,Mireia Masias,Gina Quill,Marc Ramos,Ada Doltra,Ana Garcia,Marta Sitges,David Marlevi,Alistair Young,Martyn Nash,Bart Bijnens,Oscar Camara*

Main category: cs.CV

TL;DR: 本文介绍了第一个针对左心房4D流动磁共振成像分析的开源计算框架，能够对高级血流动力学参数进行全面的定性和定量分析，并展示了其在不同中心数据中的鲁棒性。此外，还首次全面评估了能量、涡度和压力参数在各种疾病中的潜在预后生物标志物价值。


<details>
  <summary>Details</summary>
Motivation: 由于传统超声分析的限制，左心房的血流动力学理解受到显著限制。4D Flow MRI虽然有潜力提高对心房血流动力学的理解，但由于低速和有限的空间分辨率，分析左心房仍然具有挑战性。此外，缺乏专门的计算框架以及不同的采集协议和供应商使得大规模研究4D Flow MRI提供的血流动力学参数的预后价值变得复杂。

Method: 本文提出了一种新的开源计算框架，用于分析4D Flow MRI在左心房中的应用。该框架能够实现高精度的自动分割，并对能量、涡度和压力等参数进行分析。

Result: 本文提出的框架在不同质量的数据中表现出鲁棒性，即使在训练数据有限的情况下也能产生高精度的自动分割（Dice > 0.9 和 Hausdorff 95 < 3 mm）。此外，首次全面评估了能量、涡度和压力参数在各种疾病中的潜在预后生物标志物价值。

Conclusion: 本文介绍了第一个针对左心房（LA）4D流动磁共振成像（4D Flow MRI）分析的开源计算框架，该框架能够对高级血流动力学参数进行全面的定性和定量分析，并展示了其在不同中心数据中的鲁棒性。此外，还首次全面评估了能量、涡度和压力参数在各种疾病中的潜在预后生物标志物价值。

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [316] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke,Takashi Shibuya,Kengo Uchida,Koichi Saito,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文介绍了Dyadic Mamba，一种基于状态空间模型的新方法，用于生成任意长度的高质量双人人类运动。该方法通过连接在个体运动序列之间促进信息流，避免了复杂的交叉注意机制。实验结果表明，Dyadic Mamba在短期基准上表现良好，并在更长的序列上优于基于变压器的方法。同时，本文还提出了一个新的基准来评估长期运动合成质量。


<details>
  <summary>Details</summary>
Motivation: 从文本描述生成现实的双人运动存在重大挑战，特别是对于超过典型训练序列长度的长时间交互。虽然最近的基于变压器的方法在短期双人运动合成方面表现出色，但由于位置编码方案的固有局限性，它们在更长的序列上表现不佳。

Method: 我们引入了Dyadic Mamba，这是一种新颖的方法，利用状态空间模型（SSMs）生成任意长度的高质量双人人类运动。我们的方法采用了一种简单但有效的架构，通过连接在个体运动序列之间促进信息流，消除了对复杂交叉注意机制的需求。

Result: 我们证明，Dyadic Mamba在标准短期基准上实现了具有竞争力的性能，而在更长的序列上显著优于基于变压器的方法。此外，我们提出了一个新的基准来评估长期运动合成质量，为未来的研究提供了标准化框架。

Conclusion: 我们的结果表明，基于状态空间模型的架构为解决从文本描述中合成长期双人人类运动这一具有挑战性的任务提供了有希望的方向。

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [317] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: 本文提出了一种仅利用现有标注的简单但有效的医学图像分割方法，通过引入器官边界预测作为辅助任务来提高分割精度。


<details>
  <summary>Details</summary>
Motivation: 由于严格的隐私法规和数据保护政策，获取大规模医疗数据具有挑战性。此外，医学图像的标注需要领域专家手动描绘解剖结构，这使得过程既耗时又昂贵。因此，半监督方法因其减少标注成本而受到欢迎。然而，半监督方法的性能严重依赖于未标注数据的可用性，当这些数据稀缺或不存在时，其效果会下降。

Method: 我们提出了BoundarySeg，一个将器官边界预测作为辅助任务的多任务框架，利用两个任务预测之间的一致性提供额外监督。

Result: 该策略提高了分割准确性，特别是在数据量少的情况下，使我们的方法能够实现与最先进的半监督方法相当或更好的性能。

Conclusion: 我们的方法在不依赖未标注数据或增加计算需求的情况下，实现了与最先进的半监督方法相当或更好的性能。

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [318] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh,Isabel Funke,Micha Pfeiffer,Fiona Kolbinger,Hanna Maria Schmeiser,Juergen Weitz,Marius Distler,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本条件扩散的方法，用于生成高保真的手术视频，以解决手术视频数据集中的数据不平衡问题，并在两个下游任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 手术视频数据集中的严重数据不平衡阻碍了高性能模型的发展。因此，我们旨在通过合成手术视频来克服数据不平衡问题。

Method: 我们提出了一种独特的两阶段、文本条件扩散方法，用于生成高保真手术视频。该方法利用2D潜在扩散模型来捕捉空间内容，并通过集成时间注意力层来确保时间一致性。此外，我们引入了一种拒绝采样策略，以选择最合适的合成样本。

Result: 我们在两个下游任务——手术动作识别和术中事件预测中评估了我们的方法，证明了从我们的方法中获得的合成视频可以显著提高模型性能。

Conclusion: 我们的方法在两个下游任务中得到了验证，证明了合成视频可以显著提高模型性能。

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [319] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee,Taylor Webb,Trevor Bihl,Keith Holyoak,Hongjing Lu*

Main category: cs.CV

TL;DR: 本文介绍了Probabilistic Schema Induction (PSI)模型，该模型使用深度学习在结构化表示上进行类比映射，形成组合概念。PSI表现出类似人类的学习性能，并优于其他两种对照模型。研究结果表明，结构化表示和类比映射对于模拟快速的人类类似学习组合视觉概念至关重要。


<details>
  <summary>Details</summary>
Motivation: 人类认知的一个特征是能够从有限的例子中学习新的视觉概念。传统类别学习模型将每个例子表示为无结构的特征向量，而组合概念学习被认为依赖于(1)例子的结构化表示和(2)通过类比映射识别跨例子的共享关系结构。

Method: PSI模型使用深度学习在结构化表示上进行类比映射，形成称为模式的组合概念。它依赖于一种新的相似性概念，权衡对象级相似性和关系相似性，并具有增强与分类相关的机制，类似于传统模型中的选择性注意参数。

Result: PSI产生了类似人类的学习表现，并优于两个对照组：一个使用从深度学习模型中提取的无结构特征向量的原型模型，以及一个结构化表示较弱的PSI变体。值得注意的是，PSI的类似人类的表现是由一种适应性策略驱动的，该策略增加了关系相似性相对于对象级相似性的权重，并提高了区分类别的关系的贡献。

Conclusion: 这些发现表明，结构化表示和类比映射对于建模快速的人类类似学习组合视觉概念至关重要，并展示了如何利用深度学习创建心理模型。

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [320] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin,Chenyang Wu,Penghui Huang,Yanyong Zhang,Yinian Mao,Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM is a large-scale 3DGS-based visual SLAM with stereo cameras that improves robustness in large-scale outdoor scenarios through multi-modality strategies, feature-alignment warping constraints, continuous Gaussian Splatting submaps, and structure refinement.


<details>
  <summary>Details</summary>
Motivation: Most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored.

Method: LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. It introduces feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. Continuous Gaussian Splatting submaps are used to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition, and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. A structure refinement module enhances the reconstruction quality after global optimization of camera poses and Gaussian points.

Result: LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.

Conclusion: LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [321] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP is a simple yet effective method for universal visual anomaly detection that outperforms existing methods by leveraging adaptive visual and textual representations and comparative learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility.

Method: AdaptCLIP is a simple yet effective method based on two key insights: adaptive visual and textual representations should be learned alternately rather than jointly, and comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters at its input or output ends.

Result: AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. It achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains.

Conclusion: AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods.

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [322] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin,Shaolei Liu,Manning Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无源域适应框架，通过预适应、数据依赖的频率提示和风格相关的层微调策略，提高了目标模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无源域适应方法在生成高质量伪标签和域间差异方面存在不足，同时在有限监督下训练整个模型效率较低。因此，需要一种新的方法来解决这些问题。

Method: 本文引入了预适应阶段，生成一个预适应模型，作为目标模型的初始化，并允许生成高质量的增强伪标签而不引入额外参数。此外，还提出了一种数据依赖的频率提示，以更有效地将目标域图像转换为源域风格，并采用一种与风格相关的层微调策略来训练目标模型。

Result: 在跨模态腹部和心脏无源域适应分割任务中，所提出的方法表现出色，优于现有的最先进方法。

Conclusion: 本文提出了一种新的无源域适应（SFDA）框架，以解决现有方法在生成高质量伪标签和域间差异方面的不足。实验结果表明，所提出的方法在跨模态腹部和心脏SFDA分割任务中优于现有的最先进方法。

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [323] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: 本文提出了VRU-CIPI框架，利用基于序列注意力的模型预测VRU在交叉口的过街意图。通过使用GRU和多头Transformer自注意力机制，该方法在UCF-VRU数据集上取得了96.45%的准确率，并实现了实时推理速度。此外，通过集成I2V通信，该方法可以主动增强交叉口的安全性。


<details>
  <summary>Details</summary>
Motivation: 理解并预测真实世界中的人类行为，特别是在城市交叉口，对于提高道路使用者之间的互动安全性仍然至关重要。其中最关键的的行为是脆弱道路用户（VRUs）的过街意图，误解可能导致与来车发生危险冲突。

Method: 我们提出了VRU-CIPI框架，该框架采用基于序列注意力的模型来预测交叉口的VRU过街意图。VRU-CIPI使用门控循环单元（GRU）捕捉VRU运动的时间动态，并结合多头Transformer自注意力机制来编码对预测过街方向至关重要的上下文和空间依赖性。

Result: 在UCF-VRU数据集上评估，我们的方法达到了最先进的性能，准确率为96.45%，并且实现了达到每秒33帧的实时推理速度。

Conclusion: 通过集成基础设施到车辆（I2V）通信，我们的方法可以通过及时激活过街信号和向联网车辆提供早期警告来主动增强交叉口的安全性，确保所有道路使用者的顺畅和安全交互。

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [324] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan,Lei Zhou,Liu Mao,Shaofan Chen,Chuanqiu Ren,Xia Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新的遥感变化检测任务——非注册变化检测，并通过实验验证了其对现有方法的潜在危害。


<details>
  <summary>Details</summary>
Motivation: 为了解决日益增加的紧急情况（如自然灾害、人为事故和军事打击）带来的挑战，我们需要一种新的遥感变化检测任务——非注册变化检测。

Method: 我们开发了针对各种场景的不同图像变换方案，将现有的注册变化检测数据集转换为非注册版本。

Result: 我们系统地提出了八个可能在现实世界中出现并可能导致非注册问题的情景，并展示了非注册变化检测对最先进方法的破坏性影响。

Conclusion: 非注册变化检测可能导致最先进的方法造成灾难性损害。

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [325] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng,Kexuan Li,Xingye Cui,Jiaxuan Li,Chang Long,Tian Pu,Zhenming Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的红外小目标检测方法CSPENet，通过引入轮廓感知和显著性先验嵌入网络，提高了在密集杂波环境下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集杂波环境下对微弱目标的定位和轮廓信息感知存在不足，严重影响了检测性能。

Method: 提出了一种轮廓感知和显著性先验嵌入网络（CSPENet），包括一个环绕收敛先验提取模块（SCPEM）、一个双分支先验嵌入架构（DBPEA）和一个注意力引导的特征增强模块（AGFEM）。

Result: CSPENet在检测性能上优于其他最先进的方法。

Conclusion: CSPENet在公开数据集NUDT-SIRST、IRSTD-1k和NUAA-SIRST上的实验结果表明，它在检测性能上优于其他最先进的方法。

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [326] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang,Tao Tan,Shuai Tan,Weiqin Yang,Kunyan Cai,Calvin Chen,Yue Sun*

Main category: cs.CV

TL;DR: MambaControl is a novel framework that combines Mamba-based long-range modelling with graph-guided anatomical control and Fourier-enhanced spectral graph representations to improve the prediction of medical image trajectories, achieving state-of-the-art results in Alzheimer's disease prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with longitudinal dependencies and structural consistency in progressive disorders, requiring better models for capturing complex spatio-temporal dynamics while preserving anatomical integrity.

Method: MambaControl integrates selective state-space modelling with diffusion processes, combining Mamba-based long-range modelling with graph-guided anatomical control, and introduces Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail.

Result: MambaControl achieves state-of-the-art performance in Alzheimer's disease prediction, with quantitative and regional evaluations showing improved progression prediction quality and anatomical fidelity.

Conclusion: MambaControl demonstrates improved progression prediction quality and anatomical fidelity, highlighting its potential for personalized prognosis and clinical decision support.

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [327] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: 本文介绍了一种新的框架，该框架专注于Texture Key Driver Factors (TKDF)，并通过一种包含Texture-Aware Feature Extractor (TAFE)和Dual Contextual Information Filtering (DCIF)的FER架构来有效捕捉和利用这些线索。实验结果表明，该方法在RAF-DB和KDEF数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于表情相关特征的细微和局部性质以及面部外观的复杂变化，使得在野外进行面部表情识别（FER）仍然是一个具有挑战性的任务。我们观察到某些纹理线索，如眉毛、眼睛和嘴巴周围的皮肤微小变化，是情感动态的主要指标。

Method: 我们提出了一种包含Texture-Aware Feature Extractor (TAFE)和Dual Contextual Information Filtering (DCIF)的FER架构。TAFE使用带有多分支注意力机制的ResNet骨干网络来提取细粒度纹理表示，而DCIF通过自适应池化和注意力机制过滤上下文来优化这些特征。

Result: 在RAF-DB和KDEF数据集上的实验结果表明，我们的方法实现了最先进的性能。

Conclusion: 实验结果表明，我们的方法在RAF-DB和KDEF数据集上实现了最先进的性能，验证了将TKDF纳入FER流程的有效性和鲁棒性。

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [328] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao,Shaobo Xia,Sheng Nie,Cheng Wang,Xiaohuan Xi,Bisheng Yang*

Main category: cs.CV

TL;DR: 本文提出了APCoTTA，一种针对ALS点云语义分割的连续测试时适应方法，通过动态可训练层选择、基于熵的一致性损失和随机参数插值机制来解决灾难性遗忘和错误累积问题，并构建了两个新基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在ALS点云上的连续测试时适应（CTTA）方法有限，面临缺乏标准化数据集、灾难性遗忘和错误累积等挑战。

Method: 提出了一种动态可训练层选择模块，利用梯度信息选择低置信度层进行训练，并保持其余层冻结以减轻灾难性遗忘。此外，还提出了基于熵的一致性损失和随机参数插值机制，以减少错误累积并平衡目标适应和源知识保留。

Result: APCoTTA在两个基准测试ISPRSC和H3DC上取得了最佳性能，mIoU分别提高了约9%和14%。

Conclusion: APCoTTA在两个基准测试中表现出最佳性能，mIoU分别提高了约9%和14%。新的基准测试和代码已发布。

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [329] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou,Yichong Xia,Sicheng Pan,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: HQUIC is a new underwater image compression algorithm that leverages specific features of underwater images to improve compression efficiency.


<details>
  <summary>Details</summary>
Motivation: Contemporary underwater image compression algorithms fail to fully leverage the unique characteristics of underwater scenes, leading to suboptimal performance.

Method: HQUIC employs an ALTC module to adaptively predict attenuation coefficients and global light information, uses a codebook to extract common objects, and dynamically weights multi-scale frequency components.

Result: HQUIC demonstrates superior performance on diverse underwater datasets compared to existing methods.

Conclusion: HQUIC outperforms state-of-the-art compression methods in underwater image compression.

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [330] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng,Jiafei Duan,Yi Ru Wang,Haoquan Fang,Boyang Li,Yushan Huang,Elvis Wang,Ainaz Eftekhar,Jason Lee,Wentao Yuan,Rose Hendrix,Noah A. Smith,Fei Xia,Dieter Fox,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文介绍了PointArena，一个全面的平台，用于评估多模态指针在各种推理场景中的表现。该平台包括Point-Bench数据集、Point-Battle竞技场和Point-Act机器人系统。评估结果显示，Molmo-72B在性能上优于其他模型，而监督训练可以显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 指针作为在视觉上下文中将语言定位的基本且直观的机制，具有广泛的应用，包括机器人技术、辅助技术和交互式AI系统。虽然最近的多模态模型开始支持指针功能，但现有的基准测试通常只关注参考对象定位任务。

Method: 我们引入了PointArena，一个全面的平台，用于评估跨多样推理场景的多模态指针。PointArena包含三个组件：(1) Point-Bench，一个精心策划的数据集，包含五个推理类别中的约1,000个指针任务；(2) Point-Battle，一个交互式、基于网络的竞技场，促进盲目的成对模型比较，已经收集了超过4,500个匿名投票；和(3) Point-Act，一个现实世界的机器人操作系统，允许用户直接在实际环境中评估多模态模型的指针能力。

Result: 我们对最先进的开源和专有多模态模型进行了广泛的评估。结果表明，Molmo-72B始终优于其他模型，尽管专有模型日益展现出相当的性能。此外，我们发现专门针对指针任务的监督训练显著提高了模型性能。在我们的多阶段评估流程中，我们还观察到强烈的相关性，这突显了精确指针能力在使多模态模型有效连接抽象推理与具体现实行动中的关键作用。

Conclusion: 结果表明，Molmo-72B始终优于其他模型，尽管专有模型日益展现出相当的性能。此外，我们发现专门针对指针任务的监督训练显著提高了模型性能。在我们的多阶段评估流程中，我们还观察到强烈的相关性，这突显了精确指针能力在使多模态模型有效连接抽象推理与具体现实行动中的关键作用。

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [331] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang,Jiyeong Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: DITM improves image-text matching by considering descriptive flexibility and enhancing hierarchical reasoning, outperforming existing methods in representing complex relationships.


<details>
  <summary>Details</summary>
Motivation: Existing approaches use sparse binary supervision, which covers limited subsets of image-text relationships and neglects many-to-many correspondences and implicit connections from general to specific descriptions.

Method: DITM proposes descriptive image-text matching by exploring the descriptive flexibility of language, using cumulative term frequency-inverse document frequency (TF-IDF) to balance pairwise similarity according to keywords in the sentence. It refines false negative labeling and builds more precise matching by aligning relevant sentences in a generic-to-specific order.

Result: Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of DITM in representing complex image-text relationships. Additionally, DITM enhances the hierarchical reasoning ability of the model, supported by analysis on the HierarCaps benchmark.

Conclusion: DITM enhances the hierarchical reasoning ability of the model and demonstrates effectiveness in representing complex image-text relationships compared to state-of-the-art approaches.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [332] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang,Yuanqi Hu,Xinyu Chen,Yuxia Xu,Suhui Wang,Chunan Yu,Lanyun Zhu,Deyi Ji,Xin Xu,Tianrun Chen*

Main category: cs.CV

TL;DR: 本文介绍了一种新的3D服装生成框架，使普通用户能够在AR/VR环境中通过简单的3D草图创建高质量的数字服装。


<details>
  <summary>Details</summary>
Motivation: 在沉浸式消费电子产品时代，人们越来越希望通过虚拟时尚来表达自己的身份，但现有的3D服装设计工具对普通用户来说仍然难以使用。

Method: 我们引入了一个3D草图驱动的3D服装生成框架，结合了条件扩散模型、在共享潜在空间中训练的草图编码器以及自适应课程学习策略。

Result: 我们的系统能够解释不精确的自由手输入并生成逼真、个性化的服装。此外，我们还介绍了KO3DClothes数据集，解决了训练数据稀缺的问题。

Conclusion: 我们的方法在保真度和易用性方面显著优于现有基线，展示了其在下一代消费平台上的民主化时尚设计的前景。

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [333] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过整合结构重新参数化技术、双向金字塔结构网络模型和新的检测流程，实现了多尺度、小目标和远距离目标的高效精确检测。实验结果表明，增强型模型能够有效检测大目标和小目标，检测准确率达到65%，相较于传统方法有显著提升。该改进模型在实际应用中具有巨大潜力，特别适用于自动驾驶比赛，如Formula Student Autonomous China (FSAC)，在单目标和小目标检测场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的环境感知雷达、道路感知相机和车辆传感器网络面临成本高、易受天气和光照条件影响以及分辨率有限等挑战。为解决这些限制，本文提出了改进的自主目标检测网络。

Method: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过整合结构重新参数化技术、双向金字塔结构网络模型和新的检测流程，实现了多尺度、小目标和远距离目标的高效精确检测。

Result: 实验结果表明，增强型模型能够有效检测大目标和小目标，检测准确率达到65%，相较于传统方法有显著提升。

Conclusion: 该改进模型在实际应用中具有巨大潜力，特别适用于自动驾驶比赛，如Formula Student Autonomous China (FSAC)，在单目标和小目标检测场景中表现出色。

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [334] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的潜在扩散模型微调方法，用于提高遥感图像超分辨率的质量和适应性。实验结果表明，该方法在多个指标上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着遥感技术的快速发展，超分辨率图像重建具有重要的研究和实际意义。现有的深度学习方法虽然取得了一定进展，但在处理复杂场景和保持图像细节方面仍存在局限性。

Method: 本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率。该方法构建了一个具有状态、动作和奖励的强化学习环境，并通过近端策略优化（PPO）在LDM模型的反向去噪过程中优化决策目标。

Result: 在RESISC45数据集上的实验表明，与基线模型相比，该方法在PSNR、SSIM和LPIPS指标上都有显著提升，PSNR提高了3-4dB，SSIM提高了0.08-0.11，LPIPS降低了0.06-0.10，特别是在结构化和复杂的自然场景中表现尤为突出。

Conclusion: 该方法在增强超分辨率质量和场景适应性方面表现出有效性。

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [335] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为UOD的领域自适应单次检测框架，用于处理多领域医学图像，通过两个阶段的模型设计，实现了在多个解剖领域中的先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单次学习方法在单一领域中高度专业化，并且在多领域未标记数据的情况下遭受领域偏好。此外，单次学习在标注次优图像时表现不佳。

Method: UOD由两个阶段和两个相应的通用模型组成，这些模型设计为领域特定模块和领域共享模块的组合。第一阶段使用自监督学习生成伪地标标签，第二阶段设计了一个领域适应的Transformer来消除领域偏好并构建全局上下文。

Result: 即使每个领域只提供一个标注样本进行训练，领域共享模块帮助UOD聚合所有单次样本以检测更稳健和准确的地标。

Conclusion: UOD在三个广泛使用的公共X射线数据集上获得了最先进的性能，表明其在多领域医学图像中的有效性。

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [336] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: 本文介绍了DeepSeqCoco，一种基于深度学习的模型，用于从椰子树图像中准确自动地识别疾病。实验结果表明，该模型在准确率、训练时间和预测时间方面都优于现有模型，展示了其在精准农业中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统疾病识别方法是手动的、劳动密集型的且不可扩展的，这限制了早期诊断和干预。

Method: 提出了DeepSeqCoco，一种基于深度学习的模型，用于从椰子树图像中准确自动地识别疾病。

Result: 实验结果表明，DeepSeqCoco可以达到高达99.5%的准确率（比现有模型高出最多5%），混合SGD-Adam显示出最低的验证损失2.81%。训练时间和预测时间分别减少了最多18%和85%。

Conclusion: 该模型展示了通过基于人工智能的可扩展且高效的疾病监测系统改善精准农业的前景。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [337] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang,Boyang Zheng,Xichen Pan,Sayak Paul,Saining Xie*

Main category: cs.CV

TL;DR: 本文对文本到图像生成中的深度融合大型语言模型和扩散变压器的方法进行了实证研究，分析了设计选择并提供了可复现的训练方案。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注整体系统性能，而不是与替代方法的详细比较，关键设计细节和训练方案通常未被披露。这些差距导致对该方法实际潜力的不确定性。

Method: 本文进行了实证研究，进行了与现有基线的受控比较，分析了重要的设计选择，并提供了可清晰复现的训练方案。

Result: 本文进行了文本到图像生成的实证研究，分析了重要的设计选择，并提供了可清晰复现的训练方案。

Conclusion: 本文希望为多模态生成领域的未来研究提供有意义的数据点和实用指南。

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [338] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan,Xuepu Zeng,Jing Zhang,Mingming Gong,Yuxiang Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文综述了200多篇关于动态场景表示的论文，涵盖了从隐式神经表示到显式高斯基元的各种方法，并提出了一个统一的表示框架。


<details>
  <summary>Details</summary>
Motivation: 动态场景表示和重建近年来取得了重大进展，但仍然存在许多挑战，需要系统性的分析和总结。

Method: 本文对动态场景表示的方法进行了分类和评估，包括运动表示范式、用于不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。

Result: 本文提供了关于动态场景表示的全面概述，为研究人员提供了一个权威的参考，同时为经验丰富的从业者提供了概念原理和实际前沿的系统理解。

Conclusion: 本文通过系统分析200多篇关于使用辐射场进行动态场景表示的论文，提出了一个统一的表示框架，并对持续存在的挑战和有前景的研究方向进行了批判性审视。

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [339] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 本文开发了Pashto OCR数据集PsOCR，用于评估大型多模态模型在Pashto语言OCR任务中的性能，并发现Gemini和Qwen-7B表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于Pashto语言的脚本具有连笔特性且结构化数据集稀缺，自然语言处理面临诸多挑战。为此，本文旨在通过创建一个大规模的合成OCR数据集来解决这一问题。

Method: 本文开发了一个包含一百万张图像的合成Pashto OCR数据集PsOCR，这些图像在单词、行和文档级别都有边界框标注，适用于不同架构（包括卷积神经网络和Transformer）的模型训练和评估。此外，还选择了10,000张图像的基准子集来评估多个大型多模态模型的性能。

Result: 实验结果表明，Gemini在所有模型中表现最佳，而在开源模型中，Qwen-7B表现突出。

Conclusion: 本文对当前大型多模态模型在Pashto语言OCR任务中的能力和局限性进行了深入评估，并为Pashto OCR及其他类似脚本（如阿拉伯语、波斯语和乌尔都语）的进一步研究奠定了基础。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [340] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju,Sheng-Yen Huang,Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB is a two-stage framework that uses an improved StyleGAN to generate stylized videos and then learns a stylized neutral head model and expression blendshapes to render stylized avatars with arbitrary expressions.


<details>
  <summary>Details</summary>
Motivation: To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes.

Method: ToonifyGB is an efficient two-stage framework that first generates a stylized video using an improved StyleGAN and then learns a stylized neutral head model and a set of expression blendshapes from the generated video.

Result: ToonifyGB was validated on the benchmark dataset using two styles: Arcane and Pixar, showing its effectiveness in generating high-quality animation.

Conclusion: ToonifyGB can efficiently render stylized avatars with arbitrary expressions by combining a stylized neutral head model with expression blendshapes.

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [341] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo,Xiaodong Gu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMRL的多模态表示学习方法，以及其改进版本MMRL++，旨在解决大型预训练视觉-语言模型在少量数据下的过拟合问题，提高模型的泛化能力。通过实验验证，MMRL和MMRL++在多个数据集上表现优异，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练视觉-语言模型（VLMs）在各种任务上显著提升了迁移学习，但使用有限的少量数据适应这些模型常常导致过拟合，削弱了它们在新任务上的泛化能力。

Method: 提出了一种多模态表示学习（MMRL），引入了一个共享的、可学习的、模态无关的表示空间。MMRL将空间标记投影到文本和图像编码器中作为表示标记，以实现更有效的跨模态交互。此外，还提出了MMRL++，这是一种参数高效且交互感知的扩展，显著减少了可训练参数并增强了模态内交互。

Result: MMRL和MMRL++在15个数据集上的实验表明，它们在任务特定适应和泛化之间取得了良好的平衡，并优于最先进的方法。

Conclusion: MMRL和MMRL++在15个数据集上的广泛实验中表现出色，优于最先进的方法，并在任务特定适应和泛化之间取得了良好的平衡。

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [342] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: This paper introduces MoB, a method for visual token pruning that addresses the trade-off between prompt alignment and visual preservation by deriving a closed-form error bound and using ε-covering theory. MoB reformulates the problem as a bi-objective covering problem, achieving high performance with minimal visual tokens.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance.

Method: We derive the first closed-form error bound for visual token pruning based on the Hausdorff distance and leverage ε-covering theory to reveal an intrinsic trade-off between objectives. We propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem, reducing the attainment trade-off to budget allocation via greedy radius trading.

Result: MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. Evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.

Conclusion: MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. It preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. Additionally, MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [343] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil,Lucas Robinet,Laurent Risser,Soleakhena Ken*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像配准形式，用于估计与未知条件相关的图像，基于两个或更多已知图像及其相关条件。通过使用一种新的条件U-Net架构，该方法充分利用了条件信息，并且不需要任何固定图像。该形式被应用于放射治疗中不同呼吸幅度的图像移动肿瘤，使用胸腹部区域的4D-CT（3D+t）扫描。该应用特别复杂，因为它需要将一系列2D切片拼接成不同器官位置的多个3D体积。标准方法在组装体积中会产生已知的重建伪影，由于不规则的患者呼吸、滞回效应和呼吸信号与内部运动的相关性差。在4D-CT临床数据上获得的结果展示了通过实时延迟实现的无伪影体积。代码可在https://github.com/Kheil-Z/IMITATE 上公开获取。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在放射治疗中由于不规则呼吸、滞回效应和呼吸信号与内部运动相关性差而导致的图像重建伪影问题。

Method: 本文提出了一种新的图像配准形式，基于两个或更多已知图像及其相关条件，使用新的条件U-Net架构来建模。

Result: 在4D-CT临床数据上获得的结果展示了通过实时延迟实现的无伪影体积。

Conclusion: 本文提出的图像配准形式能够有效解决放射治疗中由于不规则呼吸等问题导致的图像重建伪影问题，并且代码已公开可用。

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [344] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦领域泛化方法（MCSAD），通过跨源协作风格增强和领域不变学习来提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的风格增强方法要么探索孤立源域内的数据风格，要么在数据去中心化场景下跨现有源域插值风格信息，这导致了有限的风格空间。

Method: 我们提出了一个跨源协作风格增强和领域不变学习方法（MCSAD），通过跨域特征对齐和类间关系集成蒸馏进行领域不变学习，并通过交替进行协作风格增强和领域不变学习来提升模型的泛化能力。

Result: 我们的方法在多个领域泛化数据集上进行了广泛的实验，结果表明它显著优于最先进的联邦领域泛化方法。

Conclusion: 我们的方法在多个领域泛化数据集上进行了广泛的实验，结果表明它显著优于最先进的联邦领域泛化方法。

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [345] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 本文研究了跨数据集显著性预测的挑战，并提出了一种新的架构，通过少量数据集特定参数来提高模型的泛化能力。该模型在多个数据集上取得了最先进的性能，并揭示了复杂的多尺度显著性效应。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的图像显著性预测方法在现有基准上接近黄金标准性能，但我们发现跨多个显著性数据集预测注视仍然具有挑战性，因为数据集存在偏差。当在不同数据集上应用模型时，性能会显著下降（约40%）。增加数据集多样性并不能解决这一跨数据集差距，接近60%的差距归因于数据集特定的偏差。

Method: 我们提出了一种新颖的架构，扩展了一个大部分数据集无关的编码器-解码器结构，仅包含少于20个数据集特定的参数，这些参数控制可解释的机制，如多尺度结构、中心偏差和注视扩散。

Result: 我们的模型在MIT/Tuebingen Saliency Benchmark的三个数据集（MIT300、CAT2000和COCO-Freeview）上达到了新的最先进水平，即使纯粹从不相关的数据集进行泛化，但当适应到相应的训练数据集时，效果有显著提升。

Conclusion: 我们的模型在MIT/Tuebingen Saliency Benchmark的三个数据集上达到了新的最先进水平，即使纯粹从不相关的数据集进行泛化，但当适应到相应的训练数据集时，效果有显著提升。此外，该模型还提供了关于空间显著性属性的有价值见解，揭示了结合绝对和相对大小的复杂多尺度效应。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [346] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon,Ahmad AlMughrabi,Thanasis Zoumpekas,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: 本文提出了一种名为VolE的新框架，利用移动设备驱动的3D重建来估计食品体积，无需参考物或深度信息，实验结果表明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的食品体积估计方法受到单核数据、专用硬件或依赖参考物体相机校准的限制，因此需要一种更准确和实用的方法。

Method: VolE利用移动设备驱动的3D重建来估计食品体积，通过自由运动捕捉图像和相机位置生成精确的3D模型，并使用食品视频分割生成食品掩码。

Result: VolE在多个数据集上实现了2.22%的MAPE，证明了其在食品体积估计方面的优越性能。

Conclusion: VolE在食品体积估计方面表现出色，优于现有的体积估计技术。

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [347] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文评估了MixUp和辅助傅里叶增强等替代增强策略，以提高医学图像分割模型在真实临床环境中的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在真实临床环境中由于训练和测试分布之间的不匹配而性能下降。传统视觉一致的增强策略缺乏应对多样化现实场景所需的鲁棒性。

Method: 系统评估了替代增强策略，重点是MixUp和辅助傅里叶增强。

Result: 这些技术显著提高了分布外泛化能力和对成像变化的鲁棒性，并且在心脏电影MRI和前列腺MRI分割中表现出广泛变换下的效果。

Conclusion: 这些增强方法通过促进可分离性和紧凑性来提高学习到的特征表示，并且它们的集成为提高医学分割模型在现实应用中的可靠性提供了一种易于实现的有效解决方案。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [348] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: 本文探讨了医学影像中人机对齐和公平性的问题，发现整合人类见解可以减少公平性差距并提高泛化能力，但过度对齐可能带来性能权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在医学影像中表现出色，但仍然容易产生偏差，导致不同人口群体之间的公平性差距。

Method: 我们提供了对这个领域中人机对齐和公平性的首次系统性探索。

Result: 我们的结果表明，整合人类见解可以持续减少公平性差距并提高域外泛化能力，尽管过度对齐可能会引入性能权衡，强调需要校准策略。

Conclusion: 这些发现表明，人机对齐是开发公平、稳健和可泛化的医疗AI系统的一种有前景的方法，平衡了专家指导和自动化效率。

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [349] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: 本文提出了MTVCrafter框架，直接建模原始的4D运动序列以实现人类图像动画，通过4DMoT和MV-DiT技术显著提升了性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于2D渲染的姿态图像进行运动指导，这限制了泛化能力并丢弃了开放世界动画中必要的3D信息。

Method: 提出MTVCrafter框架，通过4DMoT将3D运动序列量化为4D运动标记，并引入MV-DiT模型，利用4D位置编码设计独特的运动注意力机制。

Result: MTVCrafter在FID-VID上取得了最先进的结果，优于第二名65%。它还能够很好地泛化到各种开放世界角色（单人/多人，全身/半身）以及不同风格和场景。

Conclusion: MTVCrafter标志着该领域的重要进展，并为基于姿态的人类视频生成开辟了新方向。实验表明，MTVCrafter在FID-VID上达到了最先进的结果，优于第二名65%。

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [350] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen,Wanqi Yin,Xiaofeng Yang,Cheng Chen,Chaoyue Song,Zhongang Cai,Lei Yang,Hao Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: 本文提出了ADHMR，一种基于扩散的HMR框架，通过偏好优化方式对齐模型。通过训练HMR-Scorer评估模型并创建偏好数据集，提高了模型性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决单图像中人体网格恢复（HMR）由于深度模糊性和遮挡而固有的不适定问题。现有的概率方法生成了许多可能的3D人体网格预测，但它们常常与2D图像观测不一致，并且对野外图像的鲁棒性较弱。

Method: 提出了一种基于扩散的HMR模型，在偏好优化方式下进行对齐。首先，训练了一个评估模型HMR-Scorer，能够评估预测结果，即使对于没有3D注释的野外图像也是如此。然后使用HMR-Scorer创建一个偏好数据集，其中每个输入图像都有一个获胜和失败的网格预测对。该数据集用于通过直接偏好优化微调基础模型。此外，HMR-Scorer还可以通过数据清理来提高现有的HMR模型，即使训练样本较少。

Result: ADHMR在广泛的实验中表现出优于当前最先进的方法。代码可在https://github.com/shenwenhao01/ADHMR获得。

Conclusion: ADHMR在现有最先进方法上取得了更好的性能。

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [351] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu,Jiaqi Tang,Jiyao Wang,Yunfan LU,Xu Cao,Qingyong Hu,Yin Wang,Yuting Zhang,Tianxin Xie,Yunpeng Zhang,Yong Chen,Jiayu. Gao,Bin Huang,Dengbo He,Shuiguang Deng,Hao Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了SAGE DeeR，一个超级对齐和通用的驾驶代理，能够根据不同的用户偏好和偏差做出不同的反应，并能理解多视角和多模式输入以推理用户的生理指标、面部情绪、手部动作、身体动作、驾驶场景和行为决策。此外，还收集了多个数据集并构建了一个大规模基准测试，用于衡量驾驶代理的感知决策能力和超级对齐的准确性。


<details>
  <summary>Details</summary>
Motivation: 智能驾驶舱需要匹配不同用户的舒适度、交互和安全需求，因此需要一个能够根据用户偏好和偏差做出不同反应的驾驶代理。同时，现有的方法在处理多视角和多模式输入以及隐式思维链方面存在不足，因此需要一种新的方法。

Method: 本文提出了SAGE DeeR，一个超级对齐和通用的驾驶代理，通过多视角和多模式输入来推理用户的生理指标、面部情绪、手部动作、身体动作、驾驶场景和行为决策，并通过语言空间中的隐式思维链来提高其通用性和超级对齐能力。此外，还收集了多个数据集并构建了一个大规模基准测试。

Result: 本文提出了SAGE DeeR，一个超级对齐和通用的驾驶代理，能够根据不同的用户偏好和偏差做出不同的反应，并能理解多视角和多模式输入以推理用户的生理指标、面部情绪、手部动作、身体动作、驾驶场景和行为决策。此外，还收集了多个数据集并构建了一个大规模基准测试，用于衡量驾驶代理的感知决策能力和超级对齐的准确性。

Conclusion: 本文提出了SAGE DeeR，一个超级对齐和通用的驾驶代理，能够根据不同的用户偏好和偏差做出不同的反应，并能理解多视角和多模式输入以推理用户的生理指标、面部情绪、手部动作、身体动作、驾驶场景和行为决策。此外，还收集了多个数据集并构建了一个大规模基准测试，用于衡量驾驶代理的感知决策能力和超级对齐的准确性。

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [352] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz,Pascal Colling,Qi Han,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种新的离线映射方法，通过整合驾驶员的非正式路线，使用基于变压器的深度学习模型构建全面的全局地图。该方法能够持续更新并保持传感器无关性，从而实现高效的数据传输。实验结果表明，该方法在两个基准数据集上表现优异，具有良好的泛化能力和适用性。


<details>
  <summary>Details</summary>
Motivation: 为了避免手动标记的大量工作，出现了自动化地图创建的方法。最近的趋势已从离线映射转向在线映射，以确保所用地图的可用性和时效性。然而，尽管近年来性能有所提高，在线映射仍然面临时间一致性、传感器遮挡、运行时间和泛化方面的挑战。

Method: 我们提出了一种新的离线映射方法，将驾驶员使用的非正式路线（trail）整合到地图创建过程中。我们的方法从自车和其他交通参与者聚合轨迹数据，使用基于变压器的深度学习模型构建全面的全局地图。

Result: 我们的方法在与最先进的在线映射方法相比时表现出优越的性能，实现了对以前未见过的环境和传感器配置的改进泛化。

Conclusion: 我们的方法在两个基准数据集上进行了验证，展示了其在自动驾驶系统中的鲁棒性和适用性。

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [353] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: 本文提出了HandReader，一种用于手指拼写识别的新方法，它在多个数据集上取得了最先进的结果，并且其数据集和模型是公开可用的。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经关注于处理视频的时间维度，但手指拼写识别的准确性仍有提升空间。因此，本文旨在提出一种新的方法来提高手指拼写识别的准确性。

Method: 本文提出了HandReader，包括HandReader_{RGB}、HandReader_{KP}和HandReader_RGB+KP三种架构。HandReader_{RGB}使用了新的时间位移自适应模块（TSAM）来处理不同长度视频的RGB特征。HandReader_{KP}基于提出的时空姿态编码器（TPE），在关键点上进行操作。HandReader_RGB+KP是一种联合编码器架构，结合了RGB和关键点模态。

Result: HandReader模型在芝加哥FSWild和ChicagoFSWild+数据集上取得了最先进的结果，并且在俄罗斯手指拼写的第一开放数据集Znaki上表现出色。Znaki数据集和预训练的HandReader模型是公开可用的。

Conclusion: 本文介绍了HandReader，它是一组三个架构，旨在解决手指拼写识别任务，并在芝加哥FSWild和ChicagoFSWild+数据集上取得了最先进的结果。此外，这些模型在俄罗斯手指拼写的第一开放数据集Znaki上表现出色。Znaki数据集和预训练的HandReader模型是公开可用的。

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [354] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu,Kaixin Chen,Heng Guo,Yixiang Huang,Ming Wu,Zhenwei Shi,Chuang Zhang,Jun Guo*

Main category: cs.CV

TL;DR: 本文介绍了MFogHub，这是一个多区域和多卫星的海洋雾数据集，包含超过68,000个高分辨率样本，用于改进海洋雾检测和预测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常专注于单一区域或卫星，限制了在不同条件下评估模型性能的能力，并阻碍了对海洋雾内在特性的探索。

Method: 引入了MFogHub，这是第一个多区域和多卫星数据集，整合了来自15个沿海雾易发区域和六颗静止卫星的注释海洋雾观测数据。

Result: 广泛实验表明，MFogHub可以揭示由于区域和卫星差异导致的泛化波动，同时作为开发针对性和可扩展的雾预测技术的宝贵资源。

Conclusion: 通过MFogHub，我们旨在在全球范围内推进海洋雾动态的实际监测和科学理解。

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [355] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang,Shuai Xu,Xuelin Zhu,Yicong Li*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段跨模态交互（MSCI）模型，以解决现有方法在捕捉细粒度局部特征方面的不足。通过利用CLIP视觉编码器的中间层信息，并设计自适应聚合器来提取局部和全局信息，MSCI显著提升了模型对细粒度视觉信息的感知能力。实验结果表明，该模型在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要依赖于CLIP的跨模态对齐能力，但往往忽视了其在捕捉细粒度局部特征方面的局限性，这些局限性源于其架构和训练范式。

Method: 我们提出了一个多层次跨模态交互（MSCI）模型，该模型有效地探索并利用了CLIP视觉编码器的中间层信息。具体来说，我们设计了两个自适应聚合器，分别从低层次视觉特征中提取局部信息，并从高层次视觉特征中整合全局信息。这些关键信息通过逐阶段的交互机制逐步融入文本表示中，显著增强了模型对细粒度局部视觉信息的感知能力。此外，MSCI根据不同的组合以及同一组合中的不同元素动态调整全局和局部视觉信息之间的注意力权重，使其能够灵活适应各种场景。

Result: 在三个广泛使用的数据集上的实验充分验证了所提出模型的有效性和优越性。

Conclusion: 实验结果充分验证了所提出模型的有效性和优越性。数据和代码可在https://github.com/ltpwy/MSCI获取。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [356] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: 本文提出了StoryReasoning数据集，包含4,178个故事，这些故事来源于52,016张电影图像，并且具有结构化的场景分析和定位的故事。通过跨帧对象重新识别、链式思维推理和地基方案，改进了视觉叙事系统的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉叙事系统难以在帧之间保持角色身份并将其动作链接到适当的主题，这通常会导致参考性幻觉。这些问题可以通过对角色、物体和其他实体进行可视化元素的定位来解决。

Method: 提出了一种跨帧对象重新识别方法，使用视觉相似性和人脸识别；链式思维推理用于显式叙事建模；以及一种将文本元素与多帧中的视觉实体关联的地基方案。

Result: 建立了基准性能，通过微调Qwen2.5-VL 7B，创建了Qwen Storyteller，该模型在保持对象引用一致性的同时执行端到端的对象检测、重新识别和地标检测，并且在平均每个故事的幻觉数量上减少了12.3%。

Conclusion: 通过微调Qwen2.5-VL 7B，创建了Qwen Storyteller，该模型在保持对象引用一致性的同时执行端到端的对象检测、重新识别和地标检测，并且在平均每个故事的幻觉数量上减少了12.3%。

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [357] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo,Roger Trullo,Albert Pla Planas,Etienne Decenciere,Thomas Walter*

Main category: cs.CV

TL;DR: MIPHEI is a model that predicts multiplex immunofluorescence signals from H&E images, achieving high accuracy in cell-type classification and offering a promising approach for analyzing large-scale H&E datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between histopathological analysis using H&E staining and the more precise but less accessible multiplex immunofluorescence (mIF) technique.

Method: MIPHEI (Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&E images.

Result: MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers.

Conclusion: MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [358] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu,Jirong Zha,Ding Li,Leye Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的成员推理方法PartCrop，用于攻击自监督视觉模型，并验证了其有效性。同时，还提出了防御方法并进行了实验。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在利用大量未标记数据方面具有潜力，但面临严重的隐私问题，特别是在视觉领域。在实际中，攻击者通常面对的是黑盒系统，因此需要一种更现实的成员推理方法。

Method: 提出了一种统一的成员推理方法PartCrop，通过裁剪图像中的对象部分来查询表示空间中的响应，并引入了两种结构改进以实现可扩展的PartCrop-v2。

Result: 在三个广泛使用的图像数据集上进行了广泛的攻击实验，结果验证了PartCrop的有效性和泛化能力。此外，评估了两种常见的防御方法，并提出了一个定制的方法。

Conclusion: PartCrop和其改进版本PartCrop-v2在不同训练协议和结构的自监督模型上表现出有效的成员推理能力，同时提出的防御方法也有效。

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [359] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: 本文介绍了SpikeVideoFormer，一种高效的基于脉冲的视频Transformer，具有线性时间复杂度。通过设计基于脉冲的Hamming注意力，该方法在多个视频任务中取得了最先进的性能，并且在效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SNN的Transformer主要关注单图像任务，而没有充分利用SNN在视频视觉任务中的效率。因此，本文旨在开发一种高效的基于脉冲的视频Transformer，以更好地利用SNN的优势。

Method: 本文提出了一种基于脉冲的Hamming注意力（SDHA），并在此基础上分析了各种基于脉冲的空间-时间注意力设计，以找到一种在视频任务中表现优异且保持线性时间复杂度的最优方案。

Result: 实验结果表明，本文的方法在视频任务中取得了最先进的性能，同时在效率上显著优于现有的SNN方法和最近的ANN方法。

Conclusion: 本文提出了SpikeVideoFormer，这是一种高效的基于脉冲的视频Transformer，具有线性时间复杂度。实验结果表明，该方法在多个下游视频任务中取得了最先进的性能，并且在效率上显著优于现有的SNN方法和最近的ANN方法。

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [360] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种无需直接对应关系的新型学习ISP训练方法，通过对抗训练和多术语损失函数来保持内容结构并学习颜色和纹理特性。在多个数据集上的实验结果表明，该方法在多个评估指标上实现了高保真度，展示了无配对学习策略的潜力。


<details>
  <summary>Details</summary>
Motivation: 开发学习型ISP的一个困难且昂贵的步骤是获取像素对齐的成对数据，这些数据将智能手机相机传感器捕获的原始图像映射到高质量参考图像。

Method: 我们提出了一种新颖的可学习ISP训练方法，该方法消除了原始图像和具有匹配内容的地面真实数据之间的直接对应关系。我们的无配对方法使用了由对抗训练引导的多术语损失函数，其中多个判别器处理来自预训练网络的特征图以保持内容结构，同时从目标RGB数据集中学习颜色和纹理特性。

Result: 我们在Zurich RAW to RGB和Fujifilm UltraISP数据集上评估了我们的方法，结果表明与配对训练方法相比，我们的无配对学习策略表现出色。

Conclusion: 我们的方法在多个评估指标上实现了高保真度，表明无配对学习策略具有很大的潜力。

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [361] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: 本文研究了VLMs对虚拟物体的理解能力，并发现其处理能力不足。


<details>
  <summary>Details</summary>
Motivation: 本文旨在测试VLMs是否能够理解图像中未视觉呈现的虚拟物体，并评估其对场景的理解能力。

Method: 本文通过系统评估最先进的VLMs，分析它们处理虚拟物体的能力。

Result: 本文发现，最先进的VLMs在处理虚拟物体方面的能力不足。

Conclusion: 本文认为，虚拟物体的描述可以帮助测试AI系统对场景的理解能力，但目前最先进的VLMs在处理虚拟物体方面的能力不足。

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [362] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS是一种3DGS优化方法，能够在不同场景中自动找到理想的数量-质量权衡点，并且在使用更少高斯分布的情况下实现更高的渲染质量，同时支持广泛的无级控制范围。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏让用户直观调整数量-质量权衡的能力，以适应不同的硬件和通信约束条件。

Method: ControlGS是一种3DGS优化方法，通过单次训练运行和用户指定的超参数来实现语义上有意义且跨场景一致的数量-质量控制。

Result: ControlGS在不同场景中表现出色，能够实现更高的渲染质量并减少高斯分布的数量，同时支持广泛的无级控制范围。

Conclusion: ControlGS能够在不同场景中自动找到理想的数量-质量权衡点，并且在使用更少高斯分布的情况下实现更高的渲染质量，同时支持广泛的无级控制范围。

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [363] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov,Petr Surovtsev,Karina Kvanchiani,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CV

TL;DR: 本文提出了Logos，一个新颖的俄语手语（RSL）数据集，是目前最大的ISLR数据集，拥有最多的签名者和最大的RSL数据集规模和词汇量。我们探索了跨语言迁移学习方法，并发现使用多个分类头的联合训练最有助于目标低资源数据集的准确性。Logos数据集的关键特点是明确标注的视觉相似签名组。我们证明了明确标记视觉相似的签名可以提高训练模型的质量作为下游任务的视觉编码器。基于提出的贡献，我们在WLASL数据集上超越了当前最先进的结果，并在AUTSL数据集上获得了具有竞争力的结果，仅使用单流模型处理纯RGB视频。源代码、数据集和预训练模型均可公开获取。


<details>
  <summary>Details</summary>
Motivation: 本文研究了孤立手语识别（ISLR）任务的两个方面。首先，尽管有多个数据集，但大多数个别手语的数据量有限。这带来了跨语言ISLR模型训练的挑战，包括迁移学习。其次，相似的手势可能有不同的语义含义。这会导致数据集标注的歧义，并引发关于如何标注这些手势的最佳策略的问题。

Method: 本文提出了Logos，一个新颖的俄语手语（RSL）数据集，是目前最大的ISLR数据集，拥有最多的签名者和最大的RSL数据集规模和词汇量。我们探索了跨语言迁移学习方法，并发现使用多个分类头的联合训练最有助于目标低资源数据集的准确性。Logos数据集的关键特点是明确标注的视觉相似签名组。我们证明了明确标记视觉相似的签名可以提高训练模型的质量作为下游任务的视觉编码器。

Result: 结果显示，预训练在Logos数据集上的模型可以作为其他语言SLR任务的通用编码器，包括少样本学习。通过提出的贡献，我们在WLASL数据集上超越了当前最先进的结果，并在AUTSL数据集上获得了具有竞争力的结果，仅使用单流模型处理纯RGB视频。

Conclusion: 基于提出的贡献，我们在WLASL数据集上超越了当前最先进的结果，并在AUTSL数据集上获得了具有竞争力的结果，仅使用单流模型处理纯RGB视频。源代码、数据集和预训练模型均可公开获取。

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [364] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了UniEval，这是一个用于统一多模态模型的首个无需额外模型、图像或注释的评估框架，并构建了包含81个细粒度标签的UniBench基准测试，以及对应的UniScore指标。实验结果表明，该框架比现有基准更具挑战性，且评估结果与人类评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的评估框架来评估统一多模态模型，现有的评估方法存在诸多限制，如缺乏整体结果、依赖大量标注图像、基准测试多样性不足以及评估指标能力有限。

Method: 引入了UniEval，这是第一个无需额外模型、图像或注释的统一多模态模型评估框架，并构建了包含81个细粒度标签的UniBench基准测试，以及对应的UniScore指标。

Result: 实验结果表明，UniBench比现有基准更具挑战性，UniScore与人类评估高度一致，并超越了现有指标。此外，对最先进的统一和视觉生成模型进行了广泛评估，揭示了统一模型的独特价值。

Conclusion: UniEval框架和UniBench基准测试为统一多模态模型提供了更全面、简化和统一的评估方法，实验结果表明其比现有基准更具挑战性，且UniScore与人类评估高度一致。

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [365] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt,Pedro Sanchez,Yongchen Yao,Steven McDonagh,Sotirios A. Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: 我们引入了CheXGenBench，这是一个评估合成胸部X光片生成的框架，同时评估保真度、隐私风险和临床效用。通过标准化的数据划分和统一的评估协议，我们揭示了现有评估协议中的关键低效问题，并发布了SynthCheX-75K数据集以支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 尽管生成AI在现实世界图像方面取得了快速进展，但医学领域的评估受到方法不一致、过时的架构比较和脱离评估标准的阻碍，这些标准很少涉及合成样本的实际临床价值。

Method: 我们引入了CheXGenBench，这是一个严格且多方面的评估框架，用于合成胸部X光片生成，同时评估保真度、隐私风险和临床效用。通过标准化的数据划分和统一的评估协议，包括超过20个定量指标，系统地分析生成质量、潜在隐私漏洞以及下游临床适用性。

Result: 我们的结果揭示了现有评估协议中的关键低效问题，特别是在评估生成保真度方面，导致不一致和无信息的比较。我们的框架为医学AI社区建立了一个标准化的基准。

Conclusion: 我们的框架为医学AI社区建立了标准化的基准，使客观和可重复的比较成为可能，并促进了现有和未来生成模型的无缝集成。此外，我们发布了高质量的合成数据集SynthCheX-75K，以支持该关键领域的进一步研究。

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [366] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev,Nuno Goncalves*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过引入双分支分类策略来修改分类任务，从而有效处理人脸变形标签的模糊性。这种适应使模型能够将变形图像纳入训练过程，提高其区分它们与真实样本的能力。我们的策略已在公开基准上得到验证，证明了其在增强对抗人脸变形攻击的鲁棒性方面的有效性。此外，我们的方法具有普遍适用性，可以集成到现有的面部识别训练流程中，以改进基于分类的识别方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术的进步，人脸识别已经取得了显著进展，但这也增加了其暴露于演示攻击（包括人脸变形）的风险，这会带来严重的安全威胁。因此，现代人脸识别系统必须具备对这些攻击的鲁棒性。

Method: 我们提出了一种新的方法，通过引入双分支分类策略来修改分类任务，从而有效处理人脸变形标签的模糊性。这种适应使模型能够将变形图像纳入训练过程，提高其区分它们与真实样本的能力。

Result: 我们的策略已在公开基准上得到验证，证明了其在增强对抗人脸变形攻击的鲁棒性方面的有效性。

Conclusion: 我们的策略已在公开基准上得到验证，证明了其在增强对抗人脸变形攻击的鲁棒性方面的有效性。此外，我们的方法具有普遍适用性，可以集成到现有的面部识别训练流程中，以改进基于分类的识别方法。

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [367] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的检索器框架，使用子模态子集选择技术来提高多图像问答任务中的检索性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在涉及单个图像的视觉-语言任务中表现出色，但在面对多个图像的集合时表现不佳（Multiple Image Question Answering场景）。这些任务涉及对大量图像进行推理，存在可扩展性问题（随着图像数量的增加）和检索性能问题。

Method: 我们提出了对MIRAGE模型中引入的检索器框架的改进，使用了子模态子集选择技术。我们的方法利用了查询感知的子模态函数，如GraphCut，以在主要检索组件之前预选语义相关的图像子集。

Result: 我们展示了使用基于锚点的查询和数据增强可以提高子模态检索器管道的有效性，特别是在大型haystack大小的情况下。

Conclusion: 我们的方法通过使用基于锚点的查询和数据增强，提高了子模态检索器管道的有效性，特别是在大型haystack大小的情况下。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [368] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang,Guohai Xu,Weinong Wang,Junjie Yang,Jie Lou,Yunhua Xue*

Main category: cs.CV

TL;DR: 本文提出了一种新的指标和基准来评估多模态大语言模型的视觉理解能力，发现模型可能在不真正理解视觉输入的情况下给出正确答案。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要评估答案的正确性，而忽略了模型是否真正理解了视觉输入。为此，本文定义了隐式视觉误解（IVM），即MLLMs提供正确答案但并未完全理解视觉输入的情况。

Method: 本文通过分析因果注意力模块中的视觉和文本模态，揭示了随着网络层数加深，注意力分布逐渐集中在与正确答案相关的图像上。基于这一见解，提出了注意力准确性指标和一个新的基准来量化IVM。

Result: 本文提出的注意力准确性指标能够直接评估模型的视觉理解能力，并且对位置偏差具有鲁棒性。此外，该方法在更细粒度和单模态场景中也表现出有效性，证明了其多功能性和泛化能力。

Conclusion: 本文提出了一个尺度无关的指标，即注意力准确性，并引入了一个新的基准来量化隐式视觉误解（IVM）。注意力准确性通过内部机制直接评估模型的视觉理解能力，对位置偏差具有鲁棒性，从而提供了更可靠的评估。此外，我们还扩展了方法到更细粒度，并在单模态场景中展示了其有效性，突显了其多功能性和泛化能力。

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [369] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: 本研究探讨了在生成用于CLIP分类器的合成训练数据时，可行性是否重要，并发现其影响较小。


<details>
  <summary>Details</summary>
Motivation: 研究可行性是否在生成用于CLIP基于分类器的合成训练数据时是必要的，特别是针对背景、颜色和纹理这三个目标属性。

Method: 引入了VariReal管道，该管道最小地编辑给定的源图像，以包含由大型语言模型生成的文本提示给出的可行或不可行属性。

Result: 实验结果显示，可行性对LoRA微调的CLIP性能影响很小，且在三个细粒度数据集上的top-1准确率差异小于0.3%。此外，属性对可行/不可行图像是否对抗性地影响分类性能有影响。

Conclusion: 实验结果表明，可行性对LoRA微调的CLIP性能影响很小，且在训练数据集中混合可行和不可行图像不会显著影响性能。

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [370] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出利用代码作为跨模态对齐的监督，以解决当前多模态模型在数学推理方面的不足。通过开发图像到代码的模型和数据集，以及构建高质量的多模态数学指令微调数据集，最终提出了MathCoder-VL模型，在多个指标上取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: 自然语言图像描述数据集主要用于训练大型多模态模型，主要关注自然场景，忽略了数学图表中对问题解决至关重要的复杂细节，阻碍了当前LMMs在多模态数学推理方面的进展。

Method: 我们提出利用代码作为跨模态对齐的监督，因为代码本质上包含了生成相应图形所需的所有信息，建立了两种模态之间的精确联系。具体来说，我们通过模型在循环的方法共同开发了图像到代码的模型和数据集，结果得到了一个图像到代码的模型FigCodifier和ImgCode-8.6M数据集，这是目前最大的图像-代码数据集。此外，我们使用FigCodifier合成新颖的数学图形，然后构建MM-MathInstruct-3M，这是一个高质量的多模态数学指令微调数据集。最后，我们提出了MathCoder-VL，在ImgCode-8.6M上训练以进行跨模态对齐，并随后在MM-MathInstruct-3M上进行微调以进行多模态数学问题解决。

Result: 我们的模型在所有六个指标上达到了新的开源SOTA。值得注意的是，它在MathVista的几何问题解决子集上超过了GPT-4o和Claude 3.5 Sonnet，分别提高了8.9%和9.2%。

Conclusion: 我们的模型在所有六个指标上达到了新的开源SOTA。值得注意的是，它在MathVista的几何问题解决子集上超过了GPT-4o和Claude 3.5 Sonnet，分别提高了8.9%和9.2%。

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [371] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang,Fan Zhang,Yufeng Cui,Haiwen Diao,Zhuoyan Luo,Huchuan Lu,Jing Liu,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的视觉标记优化方法ETT，该方法能够实现视觉标记化和目标自回归任务之间的联合优化，从而显著提高多模态理解和视觉生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉标记化方法将视觉标记器的优化与下游训练分离，隐含地假设视觉标记可以在各种任务中泛化，例如图像生成和视觉问答。然而，针对低级重建优化的视觉标记器对于需要不同表示和语义的下游任务是无关的。这种解耦范式引入了一个关键的不匹配：视觉标记化的损失可能成为目标任务的表示瓶颈。

Method: ETT是一种端到端的视觉标记优化方法，它实现了视觉标记化和目标自回归任务之间的联合优化。ETT利用了视觉标记器代码本的视觉嵌入，并通过重建和标题目标对视觉标记器进行端到端优化。

Result: 实验结果表明，我们的端到端视觉标记优化方法在多模态理解和视觉生成任务上相比冻结标记器基线有2-6%的显著性能提升，同时保持了原始的重建能力。

Conclusion: ETT是一种简单且强大的方法，可以增强多模态基础模型，而不仅仅是图像生成和理解。

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [372] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang,Siyu Chen,Lihe Yang,Jialei Wang,Ziang Zhang,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合不完整但精确的度量信息和相对完整的几何结构的框架，能够生成准确、密集且详细的度量深度图，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整但精确的度量信息和相对完整的几何结构时存在局限，因此需要一种能够结合两者优势的框架。

Method: 本文设计了一个从粗到细的流程，逐步整合两种互补的深度源。首先，引入像素级度量对齐和距离感知加权来预填充多样化的度量先验。其次，开发了一个条件单目深度估计（MDE）模型来细化深度先验的固有噪声。

Result: 本文的方法在7个真实世界数据集上表现出色，匹配甚至超越了之前的任务特定方法，并且在具有挑战性的未见过的混合先验情况下表现良好。

Conclusion: 本文提出的Prior Depth Anything框架能够生成准确、密集且详细的度量深度图，展示了在深度补全、超分辨率和修复任务中的出色零样本泛化能力，并提供了灵活的精度-效率权衡。

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [373] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alex Schwing,Liangyan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 3D-Fixup是一种新的框架，通过学习的3D先验知识引导2D图像编辑。它利用基于训练的方法，借助扩散模型的生成能力。我们转向视频数据以生成训练数据对，即源帧和目标帧。我们结合来自Image-to-3D模型的3D指导，通过将2D信息显式投影到3D空间来解决这一具有挑战性的任务。结果表明，通过整合这些3D先验知识，3D-Fixup能够有效支持复杂的、身份一致的3D感知编辑，实现高质量的结果，并推动扩散模型在现实图像处理中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管在通过扩散模型建模图像先验方面取得了显著进展，但3D感知图像编辑仍然具有挑战性，因为对象仅通过单张图像指定。为了应对这一挑战，我们提出了3D-Fixup，这是一种新的框架，用于通过学习的3D先验知识引导2D图像编辑。

Method: 我们提出了一种新的框架3D-Fixup，该框架通过学习的3D先验知识引导2D图像编辑。我们利用基于训练的方法，借助扩散模型的生成能力。我们转向视频数据以生成训练数据对，即源帧和目标帧。我们结合来自Image-to-3D模型的3D指导，通过将2D信息显式投影到3D空间来解决这一具有挑战性的任务。我们设计了一个数据生成管道以确保训练过程中高质量的3D指导。

Result: 结果表明，通过整合这些3D先验知识，3D-Fixup能够有效支持复杂的、身份一致的3D感知编辑，实现高质量的结果，并推动扩散模型在现实图像处理中的应用。

Conclusion: 通过整合这些3D先验知识，3D-Fixup能够有效支持复杂的、身份一致的3D感知编辑，实现了高质量的结果，并推动了扩散模型在现实图像处理中的应用。

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [374] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: 研究提出了一种基于深度强化学习的信号灯交叉口纵向车辆控制策略，通过综合奖励函数和DRL算法（DDPG和SAC）训练模型，结果表明该策略能提高交通安全性、效率和舒适性。


<details>
  <summary>Details</summary>
Motivation: 开发信号灯交叉口的自动驾驶车辆控制策略是一项挑战，因为其决策过程复杂。该研究旨在提出一种基于深度强化学习的纵向车辆控制策略，以应对这一挑战。

Method: 研究提出了一个综合奖励函数，重点关注距离间隔效率奖励、黄灯决策标准和非对称加减速响应，同时考虑传统安全和舒适性标准。将该奖励函数与两种流行的DRL算法（DDPG和SAC）结合，处理连续的加减速动作空间。使用真实世界前车轨迹和基于Ornstein-Uhlenbeck过程生成的模拟轨迹训练模型。

Result: 通过累积分布函数（CDF）图与真实世界轨迹数据比较，结果表明RL模型成功保持了较低的距离间隔（即更高效率）和抖动，同时不降低安全性。在多种安全关键场景（包括跟车和交通信号遵守）中评估模型的鲁棒性，DDPG和SAC模型均成功处理了这些场景，DDPG模型的动作曲线比SAC模型更平滑。

Conclusion: 总体而言，结果证实基于DRL的信号灯交叉口纵向车辆控制策略有助于提高交通安全、效率和舒适性。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [375] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: This paper explores a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy for autonomous vehicles at signalised intersections, incorporating a comprehensive reward function. It evaluates two DRL algorithms, showing improved traffic safety, efficiency, and comfort.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling autonomous vehicles at signalised intersections may not fully address the complex decision-making processes involved, prompting the need for more efficient and safer strategies.

Method: The method involves developing a comprehensive reward function focused on distance headway-based efficiency, decision-making during amber light, and asymmetric acceleration/deceleration response. This is combined with two DRL algorithms: DDPG and SAC, which handle continuous action spaces of acceleration and deceleration. The models are trained using real-world and simulated trajectories.

Result: The DRL models successfully maintained lower distance headway and jerk compared to human-driven vehicles without compromising safety. Both DDPG and SAC models handled safety-critical scenarios well, with DDPG showing smoother action profiles.

Conclusion: A DRL-based longitudinal vehicle control strategy at signalised intersections can enhance traffic safety, efficiency, and comfort.

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [376] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

TL;DR: Language Models are advancing rapidly, but human-made benchmarks can't keep up. This paper proposes automating fact-based synthetic data model evaluations using the LMs themselves, with strong correlations to human-curated questions.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of humans creating evaluation benchmarks for every domain due to the rapid advancement and scale of Language Models.

Method: Proposing a methodology that leverages Language Models to automatically evaluate domain-specific knowledge using grounding documents as input, generating both multiple choice and open-ended questions.

Result: The synthetic data benchmarking approach shows strong correlation with human-curated questions (Spearman ranking correlation of 0.96 and Pearson accuracy correlation of 0.79). When applied to an arXiv preprint, it revealed strong performance from Gemma3 models.

Conclusion: This novel tool supports gaining diagnostic insight into Language Model capabilities across various domains using automated, fact-based synthetic data evaluations.

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [377] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

TL;DR: 在强化学习中，奖励通常总是可观测的。然而，在许多现实场景中，奖励并不总是可观测的，这可以通过监控马尔可夫决策过程（Mon-MDP）来建模。本文探讨了使用函数逼近（FA）解决Mon-MDP的方法，并研究了其中的挑战。我们展示了结合函数逼近与学习到的奖励模型可以使智能体从具有可观测奖励的监控状态泛化到具有不可观测奖励的非监控环境状态。这样的泛化可以实现接近最优的策略，即使在形式上定义为不可解的环境中也是如此。然而，我们也发现了一个关键限制：由于过拟合，智能体会错误地推断奖励，从而导致不期望的行为。为了解决过拟合问题，我们提出了一种基于奖励不确定性的谨慎策略优化方法。这项工作是弥合Mon-MDP理论与实际应用之间差距的一步。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，奖励并不总是可观测的，而传统的强化学习假设奖励总是可观测的。因此，需要一种新的方法来处理这种不可观测奖励的情况，以使强化学习能够更好地应用于实际问题。

Method: 本文使用函数逼近（FA）和学习到的奖励模型来解决监控马尔可夫决策过程（Mon-MDP）。通过这种方式，智能体可以从具有可观测奖励的监控状态泛化到具有不可观测奖励的非监控环境状态。此外，为了防止过拟合，本文还提出了一种基于奖励不确定性的谨慎策略优化方法。

Result: 实验结果表明，结合函数逼近与学习到的奖励模型可以使智能体在具有不可观测奖励的状态下实现接近最优的策略。然而，也发现了由于过拟合导致的奖励错误推断的问题，但提出的谨慎策略优化方法可以在一定程度上缓解这一问题。

Conclusion: 本文展示了如何使用函数逼近和学习到的奖励模型来解决监控马尔可夫决策过程（Mon-MDP），并揭示了其中的关键挑战，即过拟合问题。提出的谨慎策略优化方法为缓解过拟合提供了一种可能的解决方案，这是弥合Mon-MDP理论与实际应用之间差距的重要一步。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [378] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

TL;DR: This paper proposes a Hierarchical Multi-Agent Reinforcement Learning framework for simulated air combat scenarios, which divides decision-making into low-level and high-level policies to address challenges such as complex flight dynamics and large state/action spaces.


<details>
  <summary>Details</summary>
Motivation: To identify effective Courses of Action in air combat simulations leading to mission success, allowing real-world defense exploration at low cost and in a safe environment.

Method: A two-level abstraction is used where low-level policies control individual units and high-level commander policy issues macro commands aligned with mission targets. Low-level policies are trained through increasing complexity curriculums while high-level commander is trained on mission targets given pre-trained control policies.

Result: Empirical validation confirms the advantages of the proposed framework.

Conclusion: The Hierarchical Multi-Agent Reinforcement Learning framework successfully addresses the challenges of applying deep reinforcement learning in air combat simulations by exploiting policy symmetries and separating control/command tasks.

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [379] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

TL;DR: The paper addresses multi-stage cascading failures in power grids using reinforcement learning, validating the method on IEEE bus systems.


<details>
  <summary>Details</summary>
Motivation: Cascading failures in power grids can lead to severe disruptions, and current mitigation strategies often overlook the complexity of multi-stage scenarios.

Method: The multi-stage cascading failure problem is treated as a reinforcement learning task, with a simulation environment developed and an agent trained via deterministic policy gradient algorithm for continuous actions.

Result: The effectiveness of the approach is demonstrated through validation on the IEEE 14-bus and IEEE 118-bus systems.

Conclusion: Reinforcement learning offers a promising approach to mitigating multi-stage cascading failures in power grids.

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [380] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: This paper presents a meta-prompting method using agentic reinforcement learning where an LLM as a Judge (LLMaaJ) teaches another LLM to produce content aligning with human mental expectations. Experiments at the US Open 2024 showed 53.8% alignment with human reviewers after an average of 4.38 iterations, enhancing content quality and coverage.


<details>
  <summary>Details</summary>
Motivation: To optimize the similarity between human mental expectations and the neural processing states of Large Language Models when producing complex texts.

Method: Meta-prompting method combined with agentic reinforcement learning where an LLM (LLMaaJ) instructs another LLM through in-context learning to generate text considering both intended and unintended traits.

Result: Human content reviewer expectations aligned with AI 53.8% of the time, requiring an average of 4.38 iterations. Content quality improved with extended coverage of tennis action.

Conclusion: The deployment at the US Open 2024 demonstrated the effectiveness of the method in aligning AI-generated content with human expectations, which has been applied successfully in other sports and entertainment events.

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [381] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

TL;DR: MCBS是一种结合了TD3、束搜索和蒙特卡洛rollouts的新方法，用于提升策略学习中的探索和动作选择。在多个连续控制基准测试中，相较于TD3、SAC、PPO和A2C等方法，MCBS展示了更高的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于噪声的探索方法（如TD3）可能导致次优的策略收敛。为了改进探索和动作选择，需要一种新的方法来提高强化学习算法的性能和效率。

Method: 引入了一种名为Monte Carlo Beam Search (MCBS)的混合方法，该方法将束搜索和蒙特卡洛rollouts与TD3结合。MCBS通过生成候选动作并进行短视域rollouts评估，使智能体能够做出更明智的选择。此外，还对关键超参数（如束宽和rollout深度）进行了详细分析，并探讨了适应性策略以优化复杂控制任务中的MCBS表现。

Result: 实验结果表明，MCBS在包括HalfCheetah-v4、Walker2d-v5和Swimmer-v5在内的多个连续控制基准上表现出比TD3、SAC、PPO和A2C更好的样本效率和性能。例如，在约20万步内达到了最大可实现奖励的90%，而第二好的方法则需要约40万步。

Conclusion: MCBS通过结构化的前瞻搜索增强了策略学习，同时保持了计算效率。它在不同环境中展示了更高的收敛速度，为复杂控制任务提供了一种有效的解决方案。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [382] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

TL;DR: 结合链式思维(CoT)与检索增强生成(RAG)，以及自我一致性与自我验证策略，可以减少大型语言模型的幻觉问题并提高事实准确性。本文对比了基础LLM与CoT、CoT+RAG、自我一致性及自我验证技术的效果，找出了在保持流畅性和推理深度的同时，最小化幻觉的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维(CoT)能改善多步骤推理，但它单独无法完全解决大型语言模型在复杂开放任务中的幻觉问题。因此，需要探索其他方法来进一步减少幻觉并提高回答的事实准确性。

Method: 将链式思维(CoT)与检索增强生成(RAG)相结合，并应用自我一致性和自我验证策略。通过引入外部知识源和让模型验证或修正自身输出，生成更准确连贯的响应。

Result: 每种方法都有其效果，研究确定了在保持语言流畅性和推理深度的同时，能够最大程度减少幻觉的最稳健方法。

Conclusion: 结合CoT与RAG，同时使用自我一致性和自我验证策略，是减少大型语言模型幻觉的有效途径。这为未来设计更精确和可靠的模型提供了方向。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [383] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

TL;DR: CRDT is a new framework that improves Decision Transformers by using counterfactual reasoning, leading to better decision-making in unseen scenarios and outperforming conventional DT approaches.


<details>
  <summary>Details</summary>
Motivation: Decision Transformers need high-quality data for optimal performance but face challenges due to lack of training data and scarcity of optimal behaviours. The authors aim to enhance DT's ability to reason beyond known data.

Method: Proposed the Counterfactual Reasoning Decision Transformer (CRDT), which generates and utilizes counterfactual experiences to improve decision-making in unseen scenarios.

Result: Experiments on Atari and D4RL benchmarks show CRDT outperforms conventional DT approaches, especially in scenarios with limited data and altered dynamics. Also, it enables the agent to combine suboptimal trajectories without architectural modifications.

Conclusion: Counterfactual reasoning can significantly enhance reinforcement learning agents' performance and generalization capabilities.

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [384] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

TL;DR: This study replicates and extends Piatti et al.'s GovSim framework, confirming large language models' cooperative decision-making capabilities generalize across models, scenarios, and languages. Notably, high-performing models can influence lower-performing ones in heterogeneous multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To validate the findings of Piatti et al. on the cooperative decision-making capabilities of large language models (LLMs) using the GovSim framework, and to explore its applicability to new settings, languages, and model architectures.

Method: Replicate key experiments from Piatti et al., evaluate additional LLMs (e.g., DeepSeek-V3, GPT-4o-mini), introduce new settings (heterogeneous multi-agent environment, Japanese instructions, inverse environment), and assess the impact of the universalization principle.

Result: Large models like GPT-4-turbo can achieve sustainable cooperation with or without the universalization principle, while smaller models fail without it. Cooperative behavior generalizes across different architectures and model sizes. High-performing models can influence lower-performing ones in heterogeneous environments.

Conclusion: The GovSim benchmark is applicable to various models, scenarios, and languages, providing insights into LLM adaptability in complex cooperative tasks. Findings suggest potential improvements in computational resource efficiency and cooperative AI system development.

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [385] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

TL;DR: The paper proposes a conceptual access control framework to solve the dual-use dilemma in AI safety systems by using verified user credentials and risk category classifiers.


<details>
  <summary>Details</summary>
Motivation: AI safety systems encounter the dual-use dilemma where the same request can be harmless or harmful based on context, leading to potential refusal of legitimate queries and approval of harmful ones.

Method: A conceptual access control framework is proposed. It uses verified user credentials and model output classifiers assigning outputs to risk categories. Responses are permitted only when user credentials match category requirements. For implementing classifiers, a theoretical approach with small, gated expert modules integrated into the generator model via gradient routing is introduced.

Result: This framework enables granular governance of AI capabilities, allowing verified users access to specialized knowledge without arbitrary restrictions while blocking adversaries.

Conclusion: The proposed contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma in AI safety systems.

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [386] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 研究了基于大型语言模型（LLM）的代理在博弈论场景中的战略性推理能力，通过三种不同的代理设计评估了代理复杂度对性能的影响，并发现人类启发的认知结构能提升LLM代理与人类战略行为的一致性，但这种关系是非线性的，受限于底层LLM能力和简单架构增强的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的迅速发展，AI研究转向更具代理特性的系统，因此需要探讨这些基于LLM的代理在多大程度上能够复制人类的战略性推理，特别是在博弈论背景下。

Method: 研究采用了三种代理设计：简单的博弈论模型、无结构的LLM作为代理模型以及整合到传统代理框架中的LLM。通过猜数字游戏作为测试平台，对这些代理进行了基准测试，涵盖了总体推理模式和基于个体角色的目标。此外，还引入了混淆的游戏场景以评估代理在训练分布之外的泛化能力。

Result: 通过对超过2000个推理样本和25种代理配置的分析，发现人类启发的认知结构可以增强LLM代理与人类战略行为的一致性。然而，代理设计复杂度与人类相似性之间的关系是非线性的，这取决于基础LLM的能力，并表明简单架构增强存在局限性。

Conclusion: 尽管人类启发的认知结构可以改善LLM代理的表现，但其效果受到基础LLM能力和架构增强方式的限制，因此提高LLM代理的人类相似性需要更深入的研究和更复杂的架构设计。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [387] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

TL;DR: This paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to address sequential decision-making tasks, encoding them as solutions to non-linear optimization problems and demonstrating practical viability.


<details>
  <summary>Details</summary>
Motivation: Counterfactuals are useful in AI for explaining changes leading to different outputs, but established methods focus on one-step decision-making and aren't directly applicable to sequential decision-making tasks.

Method: The paper encodes counterfactual strategies as solutions to non-linear optimization problems for MDPs, identifying minimal changes to an initial strategy that leads to an undesired outcome. It further extends the encoding to synthesize diverse counterfactual strategies.

Result: The approach is evaluated on four real-world datasets, showing its practical viability in sophisticated sequential decision-making tasks.

Conclusion: The introduction of counterfactual strategies for MDPs fills a gap in addressing sequential decision-making tasks.

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [388] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

TL;DR: The paper introduces a method to compute robust policies for HM-POMDPs, combining formal verification and subgradient ascent, showing better generalization and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in optimal policies for POMDPs against environmental perturbations, the paper explores HM-POMDPs which encompass multiple potential environment models.

Method: The approach involves two techniques: (1) deductive formal verification to evaluate robust policy by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for the worst-case POMDP.

Result: Empirical results demonstrate that the computed policies are more robust and generalize better to unseen POMDPs compared to various baselines. Additionally, the method scales well to HM-POMDPs with over a hundred thousand environments.

Conclusion: The proposed method effectively computes robust policies for HM-POMDPs, providing enhanced generalization capabilities and scalability.

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [389] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

TL;DR: Language model agents' ability to explore and infer causal relationships was examined using the Blicket Test paradigm. They reliably infer disjunctive causal relationships but struggle with conjunctive ones, a bias also seen in human adults. A test-time sampling method was proposed to reduce this bias.


<details>
  <summary>Details</summary>
Motivation: To determine whether language model agents possess the capability to efficiently explore and understand the causal structure of the world, which is crucial for robust reasoning. Also, to investigate if these models exhibit systematic biases leading to erroneous conclusions.

Method: Using the Blicket Test paradigm from developmental psychology to examine the LMs' ability to infer causal relationships. Quantifying similarities between LMs and humans in terms of inference profiles. Proposing a test-time sampling method to eliminate hypotheses about causal relationships.

Result: LMs reliably infer disjunctive causal relationships but struggle with conjunctive ones. This bias persists across different models and strategies, and performance decreases with task complexity. LMs exhibit adult-like inference profiles, not children-like. The proposed sampling method significantly reduces the disjunctive bias.

Conclusion: Language models show a disjunctive bias in causal reasoning similar to human adults, likely inherited from training data. The proposed test-time sampling method can help move LMs towards more causally rigorous reasoning.

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [390] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 在数学意义上，游戏无处不在。游戏中的搜索算法是人工智能方法。但目前尚无对这些算法性能普遍性的评估研究。本文以双人零和完美信息博弈为例填补这一空白，并提出一种新的搜索算法，在短时间搜索时全面优于现有算法，在中等搜索时间内于22个研究游戏中有17个表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对游戏搜索算法性能普遍性的评估研究，特别是针对双人零和完美信息博弈的场景。

Method: 提出了一种新的搜索算法，并在大规模实验中将其与现有算法进行比较。

Result: 新算法在短时间内搜索时全面优于所有已研究算法；在中等搜索时间内，在22个研究游戏中有17个表现最佳。

Conclusion: 提出的搜索算法在双人零和完美信息博弈中表现出色，为评估搜索算法的普遍性提供了一个新视角。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [391] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: This paper enhances techniques for identifying relevant and necessary features in classification models, introduces the concept of usefulness, and provides efficient algorithms for its detection.


<details>
  <summary>Details</summary>
Motivation: To improve existing strategies for ranking features according to their importance in predictions from classification models, especially focusing on the concepts of relevancy and necessity.

Method: Generalize the notion of relevancy, introduce a global concept of usefulness, develop efficient algorithms for detecting necessity and usefulness in complex models like neural networks and decision trees.

Result: Showed that necessity can be efficiently detected in complex models, established relationships between usefulness, relevancy and necessity, and demonstrated practical utility through experiments on three datasets.

Conclusion: The introduced concept of usefulness is related to relevancy and necessity, and efficient algorithms were developed for its detection, contributing to understanding feature importance in model behavior.

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [392] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper introduces the General Dynamic GR problem and uses a model-free goal-conditioned RL approach for real-time Goal Recognition in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Traditional Goal Recognition methods are not suitable for dynamic environments with numerous and constantly evolving goals.

Method: Employing a model-free goal-conditioned Reinforcement Learning approach to facilitate rapid adaptation for Goal Recognition in diverse, changing tasks.

Result: Enables real-time Goal Recognition systems suitable for dynamic scenarios.

Conclusion: Introduces a broader definition of Goal Recognition and promotes further research in this area.

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [393] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI is an expert-driven model that extends the ClinicXAI approach, focusing on interpretable multi-pathology detection from chest X-rays. It outperforms other methods in predictive accuracy and provides concept-level explanations aligning with expert reasoning in lung cancer detection.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of deep learning models for detecting lung pathologies, their lack of transparency limits clinical adoption. The authors aim to address this by creating a generalizable, human-interpretable model that can scale to multiple lung pathologies while maintaining interpretability.

Method: The authors developed XpertXAI, which builds on their previous work with ClinicXAI. It uses a high-performing InceptionV3-based classifier and compares against post-hoc explainability methods and unsupervised CBMs using a public dataset of chest X-rays with radiology reports.

Result: XpertXAI surpasses existing techniques in both predictive accuracy and providing clinically meaningful explanations that align closely with expert radiologist annotations and judgments.

Conclusion: XpertXAI demonstrates the effectiveness of human-centric model design in extending interpretability to broader diagnostic contexts, offering a scalable solution for clinically meaningful explainable AI in medical diagnostics.

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [394] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: The paper introduces a multimodal multi-agent framework for radiology report generation (RRG) that surpasses current methods by producing more accurate, structured, and interpretable reports through task-specific agents.


<details>
  <summary>Details</summary>
Motivation: Radiology report generation has challenges like factual inconsistency, hallucination, and cross-modal misalignment which need to be addressed to improve clinical workflows and reduce workload.

Method: A multimodal multi-agent framework is proposed where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis aligning with the stepwise clinical reasoning workflow.

Result: Experimental results show that this approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations.

Conclusion: This work emphasizes the potential of clinically aligned multi-agent frameworks in supporting explainable and trustworthy clinical AI applications.

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [395] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: This paper explores the use of various offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration, demonstrating its feasibility and effectiveness on different offline datasets including low-quality experience.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of microgrid voltage regulation in environments where online interaction is not feasible due to technical or safety reasons.

Method: The study employs different offline reinforcement learning algorithms trained on previously collected datasets to create a model for microgrid voltage regulation with solar power penetration.

Result: Experiment results on the IEEE 33-bus system show that the proposed approach is both feasible and effective across various offline datasets, even those with low-quality experience.

Conclusion: Offline reinforcement learning algorithms can successfully be used for microgrid voltage regulation when online environment interactions are not possible.

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [396] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本研究定义了好问题，并提出了一个系统性的评估框架，通过适配性与有效性两个维度进行评价，开发了基于评分表的系统，并在数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前对于问题质量的全面评估研究有限，而提问对于人类和人工智能都变得越来越重要。

Method: 提出适配性和有效性作为两个关键评估维度，构建基于评分表的打分系统，结合动态上下文变量，形成半自适应标准的评估框架，并使用CAUS和SQUARE数据集进行验证。

Result: 该框架能够评估结构良好和存在问题的问题，同时适应不同的上下文情境。

Conclusion: 本研究为问题评估提供了一个灵活且全面的框架，向将提问行为与基于问题本质的结构化分析方法整合迈出了重要的一步。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [397] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: The paper discusses the evolution of AI from basic rule-based systems to advanced autonomous agents, like ChatGPT and Grok, potentially representing the 'final generation' of AI as we know it. It highlights key milestones in AI development, societal implications, and the rapid pace of progress, while emphasizing the need for wisdom in navigating this new era.


<details>
  <summary>Details</summary>
Motivation: To chronicle the remarkable journey of AI evolution and understand the capabilities and underlying technologies of current AI agents, exemplified by systems like ChatGPT and Grok.

Method: Exploring practical examples and examining advancements in prompting, training methodologies, hardware capabilities, and architectural innovations that have led to today's AI agents.

Result: AI has reached a culminating phase with sophisticated agents capable of complex reasoning and interaction, with intelligence doubling approximately every six months.

Conclusion: There is a critical need for wisdom and foresight in addressing the opportunities and challenges brought about by this powerful new era of intelligence.

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [398] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: Pre-Act 是一种新方法，通过创建多步骤执行计划和详细推理来增强代理性能。在 Almita 数据集上，Pre-Act 在动作召回率上比 ReAct 高 70%，并且经过微调的小型模型（如 Llama 3.1）在动作准确性和目标完成率上也有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型 (LLMs) 强调通过生成大量中间标记进行推理，但小型模型在实际应用中因延迟和成本限制而难以处理复杂的推理任务。为了改进这一点，提出了一种新的方法 Pre-Act。

Method: Pre-Act 方法通过为给定的用户输入创建一个多步骤执行计划和详细的推理过程来增强代理性能。该计划逐步结合之前的步骤和工具输出，并在每次步骤执行后进行自我完善，直到获得最终响应。此外，还提出了一个两层评估框架：回合级别和端到端。

Result: 实验结果表明，在 Almita 数据集上，Pre-Act 的动作召回率比 ReAct 高 70%。经过微调的 70B 模型在动作准确性（回合级别）上比 GPT-4 提高了 69.5%，在目标完成率（端到端）上提高了 28%。

Conclusion: Pre-Act 方法在提高代理性能方面表现良好，特别是在动作召回率、动作准确性和目标完成率方面。此方法适用于各种规模的模型，包括对延迟和成本敏感的小型模型。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [399] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: An analysis of the abstract of a paper on depression detection, focusing on the MPDD Challenge.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on diverse age groups and individual differences in existing depression datasets and detection methods.

Method: The MPDD Challenge consists of two tracks using specific datasets (MPDD-Elderly and MPDD-Young) for detecting depression in older adults and younger participants. It incorporates multimodal data and individual difference factors.

Result: Provides a baseline model that fuses audio and video modalities with individual difference information to detect depression manifestations in diverse populations.

Conclusion: Aims to promote the development of more personalized and accurate depression detection methods, advancing mental health research.

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [400] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: In this paper, the authors tackle the challenge of learners understanding new knowledge concepts in MOOCs by proposing a Graph RAG pipeline that uses EduKGs and PKGs. This pipeline includes personalized question generation and concept-relation based question answering. The evaluation with expert instructors on different MOOCs shows its potential to enhance personalized learning.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction between learners and instructors, making it hard for learners to understand new knowledge concepts. While LLMs can support knowledge acquisition, they suffer from hallucinations. RAG addresses hallucinations but is limited by unstructured MOOC materials and lacks active guidance for learners.

Method: The authors propose a Graph RAG pipeline which incorporates (1) a PKG-based Question Generation method for personalized questions and (2) an EduKG-based Question Answering method utilizing relationships between knowledge concepts. These methods aim to guide learners effectively within the CourseMapper platform.

Result: A study involving 3 expert instructors across 3 different MOOCs demonstrated the potential of the Graph RAG approach to empower learners with a personalized learning experience for understanding new knowledge concepts.

Conclusion: The Graph RAG pipeline using EduKGs and PKGs has shown promise in enhancing learner understanding of knowledge concepts through personalized questions and answers in MOOC platforms.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [401] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: This study proposes an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations for Taiwanese China Studies (CS) scholarship. By applying generative AI techniques and large language models, the system extracts and standardizes entity relation triples from peer reviewed CS articles, visualizes them through a D3.js based system, and forms a domain specific knowledge graph and vector database. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.


<details>
  <summary>Details</summary>
Motivation: Taiwanese China Studies has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. There is a growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship.

Method: The study applies generative AI techniques and large language models to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field.

Result: The infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. It enables a paradigm shift from linear text consumption to network based knowledge navigation.

Conclusion: This work demonstrates how generative AI can augment area studies and digital humanities, offering a scalable, data driven alternative to traditional ontology construction and supporting a reimagined scholarly infrastructure for regional knowledge systems.

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [402] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: In this paper, researchers aim to identify the most effective AI-generated explanations for diagnostic decision support in healthcare by conducting a user study with physicians.


<details>
  <summary>Details</summary>
Motivation: With the increasing adoption of artificial intelligence in healthcare, there is a need to understand which types of explanations can increase transparency and trust in ML systems' predictions. Establishing mutual trust between doctors and ML systems is crucial in shared decision-making scenarios.

Method: The researchers explored different approaches to generating explanations in XAI and conducted a user study with physicians. The physicians filled out a survey to assess various types of AI-generated explanations and participated in interviews post-survey for qualitative insights.

Result: The study provided insights into the types of explanations that are most effective for enhancing the diagnostic process. These findings contribute to understanding how to empower users with confidence and trust in ML predictions.

Conclusion: Effective AI-generated explanations play an important role in empowering medical professionals to develop trust in ML systems, ultimately improving the diagnostic decision-making process.

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [403] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: MASS is a multi-agent system for portfolio construction that achieves stable excess returns through large-scale simulations and end-to-end optimization, outperforming 6 state-of-the-art baselines in various experiments.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing multi-agent systems which are either pure simulations or constrained by predefined workflows, thus restricting their applicability and effectiveness.

Method: Introduce MASS, which progressively increases the number of agents for large-scale simulations to enhance market understanding and optimizes agent distribution via a reverse optimization process instead of a fixed workflow.

Result: Demonstrates superiority through multiple types of experiments compared with 6 state-of-the-art baselines on 3 challenging A-share stock pools.

Conclusion: The paradigm established by MASS has the potential to be applied to other tasks with similar characteristics and has been open-sourced.

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [404] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: The paper proposes a new method to evaluate common sense in AI, specifically LLMs, by incorporating human heterogeneity. It finds that most LLMs are below human median in commonsense competence and smaller models are more competitive than larger ones.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for assessing machine commonsense intelligence assume homogeneous human common sense, which is not accurate as humans vary greatly in what they consider commonsensical.

Method: The method evaluates correspondence between model's judgment and that of a human population, treating LLMs both as independent survey respondents and as simulators of a hypothetical population.

Result: Most LLMs remain below the human median in individual commonsense competence. LLMs correlate modestly with real humans when agreeing on statements. Smaller, open-weight models are more competitive than larger, proprietary models.

Conclusion: The evaluation framework ties commonsense intelligence to its cultural basis and supports adapting AI models to human collectivities with different social knowledge.

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [405] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: The paper explores the application of Satisfiability Modulo Theories (SMT) in healthcare personnel scheduling, comparing it with Mixed Integer Linear Programming (MILP). It highlights the strengths and weaknesses of both methods based on problem constraints and concludes that SMT is a promising approach for future research.


<details>
  <summary>Details</summary>
Motivation: Despite thorough documentation on how personnel scheduling affects care quality and working conditions, healthcare scheduling remains challenging due to high demand and constraint variability. While studied for decades, limited research has focused on applying SMT techniques.

Method: Generic constraint formulations are created to model real-world scheduling constraints. These are then formulated as SMT and MILP problems to compare state-of-the-art solvers Z3 (SMT) and Gurobi (MILP) on academic and real-world inspired rostering problems.

Result: Experimental results indicate that MILP performs better for highly constrained or infeasible problems, while SMT excels otherwise. On real-world problems with varied shifts and personnel, SMT outperforms MILP. However, SMT's performance is sensitive to how generic constraints are formulated.

Conclusion: SMT-based methods offer a promising direction for future research in personnel scheduling.

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [406] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 在本文中，作者提出了一种新的信息论度量——广义定向信息，定义了智能体的可塑性，并揭示了其与授权之间的基本联系。研究发现可塑性是授权的镜像：智能体的可塑性等同于环境的授权，反之亦然。同时，还存在智能体可塑性和授权之间的张力，这意味着在设计智能体时需要兼顾这两个特性。


<details>
  <summary>Details</summary>
Motivation: 目前对于智能体的研究大多关注其影响未来观察的能力（即授权），但对智能体受过去观察影响的能力（即可塑性）尚未有系统性的研究。为了填补这一空白，作者试图定义和量化智能体的可塑性，并探索其与授权的关系。

Method: 作者基于一系列期望的性质定义了可塑性，使用一种新的信息论量——广义定向信息来衡量。该量严格推广了Massey（1990）引入的定向信息，同时保留了其所有良好性质。通过数学推导，作者揭示了可塑性和授权之间的关系。

Result: 研究发现了两个重要结果：一是可塑性是授权的镜像，即智能体的可塑性等于环境的授权，反之亦然；二是智能体的可塑性和授权之间存在张力，这表明在设计智能体时需要权衡这两者。

Conclusion: 可塑性和授权及其关系对于理解智能体的本质至关重要。在未来的智能体设计中，应充分考虑这两种特性以实现更优的性能。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [407] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: The paper identifies limitations in current model explanation evaluation methods and proposes AXE, a ground-truth Agnostic eXplanation Evaluation framework that independently measures explanation quality without needing ideal ground-truth explanations or model sensitivity.


<details>
  <summary>Details</summary>
Motivation: There are many competing and contradictory explanations for a single model prediction, making it hard to select the appropriate one. Current frameworks evaluate explanation quality by comparing against ideal 'ground-truth' explanations or verifying model sensitivity, but these approaches have limitations.

Method: The authors propose three principles for future explanation evaluation strategies and introduce AXE (ground-truth Agnostic eXplanation Evaluation), which evaluates and compares model explanations while not requiring access to ideal ground-truth explanations or relying on model sensitivity.

Result: AXE is verified by comparison with baselines and demonstrates its ability to detect explanation fairwashing.

Conclusion: AXE provides an independent measure of explanation quality and does not need ideal ground-truth explanations or model sensitivity.

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [408] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: This study distinguishes AI Agents from Agentic AI, offering a taxonomy and analysis of their design philosophies, capabilities, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To clarify the differences between AI Agents and Agentic AI in terms of design philosophies and capabilities by providing a structured conceptual taxonomy, application mapping, and challenge analysis.

Method: Outlining search strategy and foundational definitions; characterizing AI Agents as modular systems driven by LLMs and LIMs for task-specific automation; positioning generative AI as precursor to AI Agents advancing through tool integration, prompt engineering, reasoning enhancements; presenting comparative analysis across both paradigms including architectural evolution, operational mechanisms, interaction styles, autonomy levels.

Result: A clear distinction between AI Agents and Agentic AI systems was established with Agentic AI representing a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Challenges such as hallucination, brittleness, emergent behavior, and coordination failure were examined and targeted solutions proposed.

Conclusion: The work provides a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems.

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [409] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 尽管大语言模型在静态基准测试中表现出色，但作为动态环境中自学习和推理代理的真正潜力尚不清楚。本研究评估了自我反思、启发式变异和规划等提示技术的效果。实验表明，较大的模型通常优于较小的模型，但策略性提示可以缩小这一差距。过长的提示对小型模型的基本反应任务有负面影响，而大型模型表现更稳健。高级提示技术主要使小型模型在复杂游戏中受益，但对已经高性能的大模型改进较少。先进的推理方法结果高度可变，虽然能够显著提高性能，但也引入不稳定性。与人类表现相比，当前大语言模型在规划、推理和空间协调等关键领域存在持续的局限性，表明仅通过自我反思提示可能无法完全克服这些根本缺陷。推理是一个多方面的任务，需要超越静态基准来捕捉推理的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为动态环境中自学习和推理代理的潜力，以及不同提示技术对其适应能力的影响。

Method: 系统评估自我反思、启发式变异和规划等提示技术对不同规模语言模型在动态环境中的适应能力的影响，并比较其与人类的表现。

Result: 较大的模型通常优于较小的模型，但策略性提示可以缩小差距；过长的提示对小型模型不利；高级提示技术主要使小型模型在复杂游戏中受益；先进的推理方法结果可变，可能带来性能提升或下降。

Conclusion: 当前一代大语言模型在规划、推理和空间协调等关键领域存在根本性缺陷，仅通过自我反思提示可能无法完全克服这些限制。需要超越静态基准以更好地理解和评估模型的推理能力。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


### [410] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习的信号交叉口纵向车辆控制策略，通过综合奖励函数和两种DRL算法（DDPG和SAC），提高了交通的安全性、效率和舒适性。


<details>
  <summary>Details</summary>
Motivation: 开发一种自主车辆控制策略以应对信号交叉口的复杂决策过程是一个具有挑战性的任务。

Method: 本文提出了一种基于深度强化学习（DRL）的信号交叉口纵向车辆控制策略，并结合了两种流行的DRL算法（DDPG和SAC）。

Result: 实验结果表明，RL模型能够保持较低的车距（即更高的效率）和较小的急加速/急减速，同时不牺牲安全性。此外，DDPG模型在关键场景中表现出更平滑的动作特性。

Conclusion: 结果表明，基于深度强化学习（DRL）的纵向车辆控制策略在信号交叉口可以提高交通安全性、效率和舒适性。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [411] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

TL;DR: 本文提出了一种自动化构建基于事实的合成数据模型评估的方法，利用语言模型自身来评估特定领域的知识，仅需基础文档作为输入。该方法与人工构建的问题有很高的相关性，并且能够生成多种类型的合成问题以诊断模型能力。在评估Gemma3模型时，发现其表现非常出色。


<details>
  <summary>Details</summary>
Motivation: 当前构建评估基准需要大量的人力，而模型的规模和范围迅速扩大，使得手动构建每个领域基准变得不切实际。因此，需要一种自动化的方法来构建评估基准。

Method: 本文提出了一种基于文档群体的自动化合成数据基准构建方法，利用语言模型自身来评估特定领域的知识，仅需基础文档作为输入。

Result: 该方法与人工构建的问题具有很高的相关性（Spearman排名相关系数为0.96，Pearson准确率相关系数为0.79），并且能够生成多种类型的合成问题以诊断模型能力。在评估Gemma3模型时，发现其表现非常出色。

Conclusion: 本文提出了一种自动化构建基于事实的合成数据模型评估的方法，该方法利用语言模型自身来自动评估特定领域的知识，仅需提供基础文档（如教科书）作为输入。这种方法与人工构建的问题有很高的相关性，并且能够生成多种类型的合成问题以诊断模型能力。

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [412] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

TL;DR: 本研究探讨了使用函数逼近的监控马尔可夫决策过程（Mon-MDP），并提出了一个谨慎的策略优化方法，以减轻因过度泛化而导致的不良行为。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，奖励并不总是可观察的，这可以建模为一个监控马尔可夫决策过程（Mon-MDP）。然而，之前关于Mon-MDP的研究仅限于简单的表格案例，限制了其在现实问题中的适用性。

Method: 本研究探索了使用函数逼近的Mon-MDP，并调查了其中的挑战。我们提出了一种谨慎的策略优化方法，利用奖励不确定性来减轻过度泛化的问题。

Result: 我们展示了将函数逼近与学习到的奖励模型相结合，使智能体能够从具有可观察奖励的监控状态推广到具有不可观察奖励的非监控环境状态。因此，我们证明了这种带有奖励模型的泛化在形式上定义为无法解决的环境中实现了接近最优的策略。然而，我们发现这种函数逼近存在一个关键限制，即智能体由于过度泛化而错误地外推奖励，导致不良行为。

Conclusion: 本研究为弥合Mon-MDP理论与实际应用之间的差距迈出了重要一步。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [413] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

TL;DR: 本文提出了一种分层多智能体强化学习框架，用于分析模拟空战场景，以识别导致任务成功的有效行动方案。


<details>
  <summary>Details</summary>
Motivation: 在模拟空战场景中应用深度强化学习面临复杂飞行动力学、状态和动作空间的指数级增长以及实时控制与前瞻规划的整合等挑战。需要一种有效的框架来解决这些问题。

Method: 提出了一种分层多智能体强化学习框架，将决策过程分为两个抽象层次：低层次策略控制单个单位，高层次指挥策略发布与整体任务目标对齐的宏观命令。

Result: 通过实证验证，确认了所提出的框架的优势。

Conclusion: 该框架在模拟空战场景中表现出优势，能够有效识别导致任务成功的行动方案。

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [414] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

TL;DR: 本文将多阶段级联故障问题视为强化学习任务，并开发了一个仿真环境。通过确定性策略梯度算法训练强化学习代理，最终在IEEE 14-bus和IEEE 118-bus系统上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的级联故障缓解策略通常是单阶段的，忽略了多阶段场景的复杂性。因此，需要一种能够处理多阶段级联故障的新方法。

Method: 本文将多阶段级联故障问题视为强化学习任务，并开发了一个仿真环境。然后通过确定性策略梯度算法训练强化学习代理以实现连续动作。

Result: 本文提出的基于强化学习的方法在IEEE 14-bus和IEEE 118-bus系统上验证了其有效性。

Conclusion: 本文提出的基于强化学习的方法在IEEE 14-bus和IEEE 118-bus系统上验证了其有效性，为多阶段级联故障问题提供了一种新的解决方案。

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [415] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 本文介绍了一种元提示方法，通过优化人类心理预期与大型语言模型的神经处理之间的相似性，提高复杂任务的文本生成质量。应用代理强化学习技术，使LLMaaJ能够预测并包含人类编辑，从而解决心智理论对齐问题。实验结果表明，人类内容评审的期望与AI对齐率达到53.8%。


<details>
  <summary>Details</summary>
Motivation: 为了衡量人类在内容生产中的心理信念，用户在2024年美国公开赛前修改长篇AI生成的文章。LLMaaJ可以通过预测并包含人类编辑来解决心智理论（ToM）对齐问题，在创建文本时考虑这些编辑。

Method: 我们引入了一种元提示方法，该方法联合生成流畅文本以完成复杂任务，同时优化人类心理预期与大型语言模型（LLM）神经处理之间的相似性。应用了代理强化学习技术，其中LLMaaJ作为裁判通过上下文学习教另一个LLM如何通过解释生成文本的意图和非意图特征来生成内容。

Result: 通过实验和对实时生产系统的解读，人类内容评审的期望有100%与AI对齐，其中53.8%的时间平均迭代次数为4.38。内容特征如事实性、新颖性、重复性和相关性的几何解释在希尔伯特向量空间中结合了空间体积（所有特征重要性）和顶点对齐（单个特征相关性），使LLMaaJ能够优化人类心智理论。这导致了内容质量的提升，扩展了网球动作的覆盖范围。

Conclusion: 我们的工作在2024年美国公开赛上部署，并已用于其他体育和娱乐领域的实时事件。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [416] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

TL;DR: 本文提出了一种新的混合方法MCBS，结合了束搜索和蒙特卡洛模拟与TD3，以提高探索和动作选择的效果。实验结果表明，MCBS在多个连续控制基准测试中表现优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于噪声的探索方法可能导致次优策略收敛，因此需要一种更有效的探索和动作选择方法。

Method: MCBS结合了束搜索和蒙特卡洛模拟与TD3，以改进探索和动作选择。MCBS生成政策输出周围的几个候选动作，并通过短时程模拟评估它们，使代理能够做出更好的决策。

Result: MCBS在多个连续控制基准测试中表现出色，包括HalfCheetah-v4、Walker2d-v5和Swimmer-v5，显示了比标准TD3和其他基线方法（如SAC、PPO和A2C）更高的样本效率和性能。此外，MCBS在不同环境中显示出更高的收敛速度。

Conclusion: MCBS的实验结果表明，它在不同环境中比TD3、SAC、PPO和A2C等方法具有更高的收敛速度。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [417] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

TL;DR: 本文探讨了如何通过结合思维链与检索增强生成，以及应用自一致性和自验证策略来减少大型语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 幻觉是大型语言模型（LLMs）在应用于复杂、开放性任务时的一个关键限制。思维链提示作为一种有前景的方法，可以改善多步骤推理，但单独的CoT并不能完全解决幻觉问题。

Method: 我们研究了将思维链（CoT）与检索增强生成（RAG）相结合，以及应用自一致性和自验证策略如何减少幻觉并提高事实准确性。

Result: 我们对基线LLM与CoT、CoT+RAG、自一致性以及自验证技术进行了比较评估。结果表明了每种方法的有效性。

Conclusion: 我们的结果突出了每种方法的有效性，并确定了在保持流畅性和推理深度的同时最小化幻觉的最稳健方法。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [418] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

TL;DR: 本文提出了一种基于反事实推理的新型决策变压器（CRDT），以提高强化学习代理在有限数据和动态变化环境中的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: DT需要高质量、全面的数据才能发挥最佳性能，但在实际应用中，缺乏训练数据和最优行为使得在离线数据集上进行训练具有挑战性。

Method: CRDT是一种基于反事实推理的新框架，通过生成和利用反事实经验来增强决策变压器（DT）的能力。

Result: CRDT在Atari和D4RL基准测试中表现出色，特别是在数据有限和动态变化的场景中。此外，反事实推理使DT代理能够获得拼接能力，无需架构修改即可组合次优轨迹。

Conclusion: CRDT展示了反事实推理在增强强化学习代理性能和泛化能力方面的潜力。

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [419] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

TL;DR: 本研究验证并扩展了GovSim框架，证明大型语言模型在资源共享场景中具有良好的合作能力，并展示了其在多种设置中的适应性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估和扩展Piatti等人提出的GovSim框架，以测试大型语言模型在资源共享场景中的合作决策能力，并探索其在新设置中的适用性。

Method: 通过复制Piatti等人的关键实验，验证了大型模型（如GPT-4-turbo）与小型模型的性能比较，并探讨了普遍化原则的影响。此外，研究还扩展了框架以适应新的设置，包括评估其他模型、创建异构多智能体环境、研究使用日语指令的场景以及探索“逆向环境”。

Result: 研究结果表明，大型模型即使在没有普遍化原则的情况下也能实现可持续合作，而小型模型则无法做到这一点。此外，研究还发现高性能模型可以影响低性能模型的行为，同时验证了该基准测试在新模型、场景和语言中的适用性。

Conclusion: 该研究验证了大型语言模型（LLMs）在资源共享场景中的合作决策能力，并展示了其在不同模型架构、规模和语言环境下的适应性。此外，研究还表明高性能模型可以影响低性能模型，从而促进更有效的合作AI系统的发展。

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [420] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

TL;DR: 本文提出了一种基于用户凭证和风险分类的AI安全框架，旨在解决双重用途困境，使合法用户能够访问专业知识，同时阻止恶意行为。


<details>
  <summary>Details</summary>
Motivation: AI安全系统面临双重用途困境。由于同一请求可能对不同的人或原因而言是无害或有害的，如果系统仅根据请求内容做出决策，它将拒绝一些合法查询并让有害查询通过。因此，需要一种能够根据用户身份和意图进行区分的解决方案。

Method: 本文提出了一种基于验证用户凭证（如机构隶属关系）和分类器的访问控制框架，这些分类器将模型输出分配到风险类别（如高级病毒学）。系统仅在用户的验证凭证与类别要求匹配时才允许响应。为了实现模型输出分类器，我们引入了一种理论方法，利用集成到生成器模型中的小型门控专家模块，并通过梯度路由进行训练，以高效检测风险，而不会出现外部监控器的能力差距问题。

Result: 本文提出的框架实现了基于用户凭证和风险分类的访问控制，使得经过验证的用户可以访问特定知识，而对手则被阻止访问。这种方法在不牺牲模型效用的前提下提高了安全性。

Conclusion: 本文提出的框架为AI能力的细粒度治理提供了第一步，使经过验证的用户能够在没有任意限制的情况下获得专业知识，同时阻止对手获得这些知识。这种上下文方法调和了模型效用与安全性的矛盾，解决了双重用途困境。

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [421] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文探讨了代理复杂性与人类相似性之间的非线性关系，并强调了底层LLM能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的迅速崛起已经将人工智能（AI）研究转向了代理系统，促使使用更弱和更灵活的代理概念。然而，这种转变引发了关于LLM代理在博弈论环境中复制人类战略推理程度的关键问题。

Method: 我们评估了三种代理设计：一个简单的博弈论模型、一个无结构的LLM作为代理模型，以及一个集成到传统代理框架中的LLM。使用猜测游戏作为测试平台，我们对这些代理进行了基准测试，并与人类参与者在一般推理模式和基于个体角色的目标上进行了比较。此外，我们引入了模糊的博弈场景来评估代理在训练分布之外的泛化能力。

Result: 我们的分析涵盖了25种代理配置下的2000多个推理样本，结果显示，人类启发的认知结构可以增强LLM代理与人类战略行为的一致性。

Conclusion: 人类启发的认知结构可以增强LLM代理与人类战略行为的一致性，但代理设计的复杂性与人类相似性之间的关系是非线性的，这表明对底层LLM能力的依赖，并暗示了简单架构增强的局限性。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [422] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

TL;DR: 本文提出了一种用于马尔可夫决策过程（MDP）的反事实策略，以解决传统方法在顺序决策任务中的不足。通过将反事实策略编码为非线性优化问题的解决方案，并扩展以合成多样化的反事实策略，验证了其在现实世界数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有计算反事实的方法主要针对单步决策，无法直接应用于顺序决策任务，因此需要一种适用于MDP的反事实策略。

Method: 将反事实策略编码为非线性优化问题的解决方案，并扩展以合成多样化的反事实策略。

Result: 在四个真实世界数据集上验证了该方法的有效性，并展示了其在复杂顺序决策任务中的实用性。

Conclusion: 本文提出的反事实策略为顺序决策任务提供了一种有效的方法，并展示了其实际应用潜力。

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [423] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法，用于在隐藏模型POMDP（HM-POMDP）中计算鲁棒策略，该方法结合了形式验证和梯度上升技术，能够在大量环境中保持良好的性能，并且具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的最优策略可能无法抵御环境中的扰动，因此需要一种能够在不同环境模型中保持性能的鲁棒策略。

Method: 该方法结合了两种正交技术：(1) 一种演绎形式验证技术，通过计算HM-POMDP中的最坏情况POMDP来支持可扩展的鲁棒策略评估；(2) 梯度上升优化候选策略以适应最坏情况POMDP。

Result: 实验结果表明，与各种基线相比，该方法产生的策略更加鲁棒，并且能够更好地泛化到未见过的POMDP，同时可以扩展到包含超过十万个环境的HM-POMDP。

Conclusion: 该研究提出了一种计算鲁棒策略的方法，该方法在隐藏模型POMDP（HM-POMDP）中表现出色，能够处理大量环境，并且在未见过的POMDP上具有更好的泛化能力。

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [424] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

TL;DR: 本文研究了语言模型在推断因果关系方面的能力，发现它们存在析取偏差，并提出了一种测试时间采样方法来减少这种偏差。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理作为自主决策者，需要主动收集信息来指导他们的决策。然而，尚不清楚语言模型是否具备这种能力或表现出系统性偏差导致错误结论。

Method: 本文使用了发展心理学中已建立的“Blicket Test”范式来研究语言模型探索和推断因果关系的能力。此外，还提出了一种测试时间采样方法，以显式地从语言模型中采样并消除关于因果关系的假设。

Result: 语言模型可靠地推断出常见的、直观的析取因果关系，但系统性地难以处理不寻常的、但同样（有时甚至更多）有证据支持的合取因果关系。这种“析取偏差”在不同模型家族、大小和提示策略中都存在，并且随着任务复杂性的增加而进一步下降。

Conclusion: 本文提出了一种测试时间采样方法，可以显著减少语言模型的析取偏差，并使其更接近科学、因果严谨的推理目标。

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [425] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 该研究提出了一种新的搜索算法，并在大量实验中证明了其在短时间和中等时间内的优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前尚无对这些算法的通用性能进行评估的研究，因此需要填补这一空白。

Method: 该研究针对具有完美信息的两人零和博弈，提出了一个新的搜索算法，并通过大规模实验评估了其性能。

Result: 在短时间内，新算法在所有游戏中都优于所有研究过的算法；在中等时间内，它在22个游戏中的17个中表现更优。

Conclusion: 该研究提出了一种新的搜索算法，并证明了在短时间搜索中，该算法在所有游戏中都优于所有研究过的算法，在中等时间搜索中，它在22个研究游戏中的17个中优于所有研究过的算法。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [426] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文改进了现有技术，以确定特征的相关性和必要性，并提出了一个新的全局概念“有用性”。我们开发了高效的算法，并在三个数据集上进行了实验，验证了其实际效用。


<details>
  <summary>Details</summary>
Motivation: 现有技术在确定特征的重要性方面存在局限，尤其是在复杂的模型中。因此，本文旨在改进这些技术，并提出新的概念来更好地理解特征的重要性。

Method: 本文基于命题逻辑和“充分理由”的概念，改进了现有的技术与算法，以确定哪些特征是相关的和/或必要的。我们还提出了一个新的全局概念“有用性”，并开发了高效的算法来检测决策树和其他更复杂模型中的有用性。

Result: 本文展示了必要性可以在复杂的模型中高效检测，并提出了一个新的全局概念“有用性”。此外，我们在三个数据集上进行了实验，分析了其实际效用。

Conclusion: 本文改进了现有的技术与算法，以确定哪些特征是相关的和/或必要的，并展示了必要性可以在复杂的模型（如神经网络）中高效检测。此外，我们提出了一个新的全局概念“有用性”，并证明了它与相关性和必要性有关。

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [427] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: 本文介绍了通用动态目标识别（GR）问题，并采用无模型的目标条件强化学习方法，以实现快速适应不同变化任务的GR。


<details>
  <summary>Details</summary>
Motivation: 传统GR方法在动态环境中难以适应，因为目标众多且不断变化。因此需要一种更广泛的GR定义和方法。

Method: 本文提出了一种无模型的目标条件强化学习方法，以实现快速适应不同变化任务的GR。

Result: 本文提出了通用动态GR问题，并通过目标条件强化学习方法实现了快速适应不同变化任务的GR。

Conclusion: 本文的工作为实时GR系统和进一步研究提供了基础。

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [428] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI是一种可解释的深度学习模型，能够检测多种肺部病理，并提供与专家判断一致的解释，为可解释的医疗AI提供了可扩展的路径。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型的决策过程不透明，其在临床中的广泛应用受到限制。因此，需要一种可解释的模型来提高临床信任度。

Method: XpertXAI是一种基于InceptionV3的分类器，结合了专家指导的概念瓶颈模型，以保持可解释的临床概念并检测多种肺部病理。

Result: XpertXAI在预测准确性上优于现有技术，并且提供了与专家推理更一致的概念级解释。

Conclusion: 本文展示了XpertXAI模型在肺部病理检测中的有效性，并强调了人类中心模型设计在更广泛诊断场景中的可扩展性。

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [429] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 本文提出了一种多模态多智能体框架用于放射学报告生成，该框架与逐步的临床推理工作流程相一致，能够生成更准确、结构化和可解释的报告，并展示了其在支持可解释和可信的临床AI应用方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 放射学报告生成旨在从医学图像中自动生成诊断报告，具有增强临床流程和减轻放射科医生工作量的潜力。然而，现有的方法仍然面临事实不一致、幻觉和跨模态错位等挑战。

Method: 本文提出了一种多模态多智能体框架用于放射学报告生成，该框架与逐步的临床推理工作流程相一致，其中特定任务的智能体处理检索、草稿生成、视觉分析、精炼和综合。

Result: 实验结果表明，我们的方法在自动指标和基于LLM的评估中都优于强大的基线，生成的报告更加准确、结构化和可解释。

Conclusion: 本文展示了临床对齐的多智能体框架在支持可解释和可信的临床AI应用方面的潜力。

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [430] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: 本文研究了使用不同的离线强化学习算法进行微电网电压调节，并在IEEE 33-bus系统上验证了其在不同离线数据集上的可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 当由于技术或安全原因无法进行环境交互时，需要一种方法来获得适用的模型，以降低缺乏在线环境交互的负面影响。

Method: 该论文研究了使用不同的离线强化学习算法进行微电网电压调节，特别是在环境交互不可行的情况下，通过在之前收集的数据集上进行离线训练来获得适用的模型。

Result: 在IEEE 33-bus系统上的实验结果表明，所提出的方法在不同离线数据集上都具有可行性，并且在仅包含低质量经验的数据集上也有效。

Conclusion: 实验结果表明，所提出的方法在不同的离线数据集上都具有可行性和有效性，包括仅包含低质量经验的数据集。

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [431] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本文提出了一个灵活且全面的问题评估框架，通过两个关键维度——适当性和有效性，结合动态情境变量，实现了结构和灵活性。该框架在多个数据集上得到验证，能够评估各种问题并适应不同情境。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对问题质量的全面研究，因此本文旨在定义好问题并提出一个系统评估框架。

Method: 本文提出了两个关键的评估维度：适当性（情境中的社会语言能力）和有效性（目标实现的战略能力），并开发了一个基于评分量表的系统。通过引入动态情境变量，评估框架实现了结构和灵活性。

Result: 本文的方法在CAUS和SQUARE数据集上得到了验证，证明了该框架能够评估良好形成和有问题的问题，并适应不同的情境。

Conclusion: 本文建立了一个灵活且全面的问题评估框架，为将提问行为与结构化分析方法相结合奠定了基础。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [432] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: 本文回顾了人工智能的发展历程，分析了其关键技术进步，并探讨了其对社会的深远影响，同时呼吁在面对人工智能的快速发展时保持智慧和远见。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人工智能的最新进展及其对社会的影响，同时指出当前人工智能可能代表了我们目前所能设想的‘最终一代’智能。

Method: 本文通过回顾人工智能的发展历程，分析了关键技术里程碑，并探讨了这些智能代理的能力和潜在影响。

Result: 本文指出，人工智能的发展速度非常快，智能水平大约每六个月翻一番，并强调了在这一新时代中需要谨慎对待的必要性。

Conclusion: 本文强调了在应对人工智能带来的机遇和挑战时，需要智慧和远见。

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [433] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Pre-Act 的新方法，通过创建多步骤执行计划和详细推理来增强代理性能。实验表明，Pre-Act 在 Almita 数据集上优于 ReAct，并且在微调的小型模型上也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代代理系统依赖于大型语言模型（LLMs）的 ReAct（推理+行动）能力。然而，较小的模型在处理复杂推理任务时存在困难，这限制了它们在实际应用中的使用。

Method: Pre-Act 通过创建多步骤执行计划并结合详细推理来增强代理性能。该计划在每一步执行后逐步改进，直到获得最终响应。此外，还提出了一种两层评估框架：（1）回合级和（2）端到端。

Result: Pre-Act 在 Almita 数据集上的 Action Recall 指标上比 ReAct 提高了 70%。经过微调的 70B 模型在 Action Accuracy（回合级）和目标完成率（端到端）上分别优于 GPT-4 69.5% 和 28%。

Conclusion: Pre-Act 是一种有效的增强代理性能的方法，即使在较小的模型上也能显著提升性能。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [434] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: 该挑战通过结合多模态数据和个体差异因素，旨在解决现有抑郁症检测方法在年龄和个体差异方面的不足，并促进更个性化和准确的检测方法的发展。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和检测方法主要关注年轻人，忽略了影响抑郁症表现的更广泛年龄范围和个体差异。当前方法往往建立多模态数据与抑郁指标之间的直接映射，未能捕捉到个体之间抑郁症的复杂性和多样性。

Method: 提供了一个基线模型，该模型融合了音频和视频模态与个体差异信息，以检测不同人群中的抑郁症表现。

Result: 该挑战包括两个基于年龄特定子集的轨道：轨道1使用MPDD-Elderly数据集检测老年人的抑郁症，轨道2使用MPDD-Young数据集检测年轻参与者的抑郁症。

Conclusion: 该挑战旨在促进更个性化和准确的抑郁症检测方法的发展，推动心理健康研究并促进包容性的检测系统。

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [435] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to help learners understand new knowledge concepts in MOOCs. The approach includes generating personalized questions based on PKGs and answering questions using EduKGs. Evaluation with expert instructors shows the potential of this method.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction between learners and instructors, making it challenging for learners to understand new knowledge concepts. LLMs are prone to hallucinations which limits their reliability. Current RAG systems do not actively guide learners toward their learning needs.

Method: We propose a Graph RAG pipeline that leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide learners to understand knowledge concepts in the MOOC platform CourseMapper. Specifically, we implement (1) a PKG-based Question Generation method to recommend personalized questions for learners in context, and (2) an EduKG-based Question Answering method that leverages the relationships between knowledge concepts in the EduKG to answer learner selected questions.

Result: The results of the evaluation show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.

Conclusion: Graph RAG has the potential to empower learners to understand new knowledge concepts in a personalized learning experience.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [436] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: 本文提出了一种AI辅助的方法，将非结构化的学术文本转化为结构化、交互式的知识表示，并通过知识图谱和向量数据库实现对中国研究文献的重新组织和探索。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统地回顾和重新组织基于台湾的中国研究学术成果，以满足日益增长的需求。

Method: 本文提出了一种AI辅助的方法，将非结构化的学术文本转化为结构化、交互式的知识表示。我们应用生成式AI（GAI）技术和大型语言模型（LLMs）从1996年至2019年间发表的1,367篇同行评审的中国研究文章中提取和标准化实体关系三元组，并通过基于D3.js的轻量级系统进行可视化，构建领域特定的知识图谱和向量数据库。

Result: 通过将文本内容分解为图结构的知识单元，我们的系统实现了从线性文本消费到基于网络的知识导航的范式转变。它增强了学者对中国研究文献的访问，同时提供了一种可扩展的数据驱动方法，作为传统本体构建的替代方案。

Conclusion: 本文展示了生成式人工智能如何增强区域研究和数字人文，同时也突显了其支持重新构想区域知识系统学术基础设施的潜力。

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [437] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: 本文研究了医生对AI生成解释的看法，发现不同类型的解释在增强诊断过程方面有不同的效果，这为未来开发更有效的可解释AI系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域越来越多地采用人工智能，理解哪些类型的解释能提高透明度并增强用户对机器学习系统预测的信任变得尤为重要。在共享决策场景中，建立相互信任至关重要。

Method: 本文通过一项针对医生的用户研究，调查了他们对各种AI生成解释的看法，并进行了问卷调查和后续访谈以获得定性见解。

Result: 研究发现，不同类型的解释在增强诊断过程方面有不同的效果，这为未来开发更有效的可解释AI系统提供了见解。

Conclusion: 研究结果表明，不同类型的解释在增强诊断过程方面有不同的效果，这为未来开发更有效的可解释AI系统提供了见解。

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [438] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文介绍了MASS，一种用于投资组合构建的多智能体规模模拟方法，通过逐步增加智能体数量和反向优化过程来提高市场理解和代理分布优化，实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅限于纯模拟或受预定义工作流程的限制，限制了其适用性和有效性。

Method: MASS通过逐步增加代理数量进行大规模模拟，以获得对市场的更深入了解，并通过反向优化过程端到端地优化代理分布。

Result: 通过性能实验、消融研究、回测实验、更新数据和股票池的实验、扩展实验、参数敏感性实验和可视化实验，MASS在三个具有挑战性的A股股票池中与六个最先进的基线进行了比较，证明了其优越性。

Conclusion: MASS的范式有望扩展到具有类似特征的其他任务。

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [439] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法来评估人工智能中的常识，考虑了人类之间的异质性。结果表明，大多数大型语言模型的常识能力低于人类中位数，并且与真实人类的相关性较低。


<details>
  <summary>Details</summary>
Motivation: 现有的静态基准测试假设人类常识是同质的，但最近的实证工作表明，人类在他们认为常识性的事情上存在巨大差异。因此，我们认为需要一种新的评估方法来考虑这种异质性。

Method: 我们提出了一种新的方法来评估人工智能（AI）中的常识，特别是大型语言模型（LLMs），通过测量模型的判断与人类群体的对应关系来纳入人类之间的异质性。

Result: 我们发现，当被视为独立的调查受访者时，大多数LLMs的常识能力低于人类中位数。此外，当用作假设人口的模拟器时，LLMs与真实人类在同意同一组陈述的程度上的相关性仅适中。较小的、开放权重的模型比更大的、专有的前沿模型表现得更具竞争力。

Conclusion: 我们的评估框架将常识智能与其文化基础联系起来，有助于日益增长的呼吁，即调整人工智能模型以适应拥有不同且常常不兼容的社会知识库的人类群体。

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [440] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: This paper explores the application of Satisfiability Modulo Theories (SMT) in healthcare scheduling, comparing SMT solvers like Z3 with traditional mathematical programming techniques like Gurobi. It finds that SMT solvers perform better in certain scenarios, especially with varied shift and personnel configurations, but require careful formulation of constraints for optimal performance.


<details>
  <summary>Details</summary>
Motivation: The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT).

Method: We propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems.

Result: Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance.

Conclusion: SMT-based methods present a promising avenue for future research within the domain of personnel scheduling.

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [441] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 本文提出了一个称为塑料性的新概念，并将其与自主性联系起来，揭示了两者之间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究代理如何被其观察到的内容所影响，以及这种影响的程度。

Method: 通过引入一种新的信息论量——广义定向信息，定义了塑料性。

Result: 塑料性是自主性的镜像，代理的塑料性等于环境的自主性，反之亦然。同时，代理的塑料性和自主性之间存在张力。

Conclusion: 塑料性和自主性及其关系对于理解代理行为是至关重要的。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [442] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [443] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: 本文区分了AI代理和Agentic AI，分析了它们的设计哲学、能力、应用领域和挑战，并提出了相应的解决方案。


<details>
  <summary>Details</summary>
Motivation: 本文旨在澄清AI代理和Agentic AI的不同设计哲学和能力，以促进它们的发展和应用。

Method: 本文通过构建概念分类法、应用映射和挑战分析，区分了AI代理和Agentic AI，并通过顺序评估架构演变、操作机制、交互风格和自主级别进行了比较分析。

Result: 本文提出了针对每个范式的独特挑战的解决方案，如ReAct循环、RAG、编排层和因果建模，并展示了在不同应用领域中的对比。

Conclusion: 本文旨在为开发强大、可扩展和可解释的AI代理和Agentic AI驱动的系统提供明确的路线图。

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [444] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 本研究评估了自我反思、启发式变异和计划作为提示技术的有效性，发现较大模型通常表现优于较小模型，但战略提示可以缩小性能差距。太长的提示会负面影响较小模型的基本反应任务，而较大的模型表现出更稳健的行为。先进的提示技术主要对较小模型在复杂游戏中有益，但对已经表现良好的大型语言模型改善较少。然而，高级推理方法的结果高度可变，可能引入不稳定性并导致性能大幅下降。研究发现，大型语言模型在关键领域如规划、推理和空间协调上仍然存在根本性不足，无法通过自我反思提示完全克服。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在动态环境中的自学习和推理能力的真正潜力。

Method: 系统评估了自我反思、启发式变异和计划作为提示技术的有效性，以测试代理的适应能力。

Result: 较大的模型通常表现优于较小的模型，但战略提示可以缩小这种性能差距。太长的提示会负面影响较小模型的基本反应任务，而较大的模型表现出更稳健的行为。先进的提示技术主要对较小模型在复杂游戏中有益，但对已经表现良好的大型语言模型改善较少。然而，高级推理方法的结果高度可变：当推理和决策一致时，它们可以显著提高性能，但也可能引入不稳定性并导致性能大幅下降。

Conclusion: 当前大型语言模型在关键领域如规划、推理和空间协调上仍然存在根本性不足，无法通过自我反思提示完全克服。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [445] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: Intersectionality framework can support the development of technologies that acknowledge and support all members of society, but it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact.


<details>
  <summary>Details</summary>
Motivation: Machine learning technologies may not provide adequate support for societal identities and experiences.

Method: Amplify the foundational intersectionality scholarship to create a socially relevant preliminary framework in developing machine-learning solutions.

Result: Evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.

Conclusion: Intersectionality framework can support the development of technologies that acknowledge and support all members of society.

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [446] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: This paper discusses the importance of intersectionality in machine learning to support societal identities and experiences, proposing a framework based on Crenshaw, Combahee, and Collins' scholarship to evaluate its application in ML literature.


<details>
  <summary>Details</summary>
Motivation: Studies have shown that machine learning technologies may not adequately support societal identities and experiences. Intersectionality is a sociological framework that can support the development of inclusive technologies but has been adopted in ways that do not always align with its original foundations.

Method: The authors amplify the foundational intersectionality scholarship of Crenshaw, Combahee, and Collins to create a socially relevant preliminary framework for developing machine-learning solutions. This framework is then used to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.

Result: The proposed framework provides a mechanism for explicitly considering complex social identities in machine learning, focusing on social justice and power. It highlights misalignments in how intersectionality has been applied in machine learning literature.

Conclusion: Adopting intersectionality through the lens of Crenshaw, Combahee, and Collins can help create more equitable technological outcomes. The framework offers a path towards ensuring that intersectionality is appropriately adopted in machine learning.

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [447] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: This paper is a survey that explores methodologies to overcome challenges in onboard learning for edge AI, such as limited resources and security issues, by focusing on model optimization, inference acceleration, and collaborative learning.


<details>
  <summary>Details</summary>
Motivation: Onboard learning in edge AI holds significant promise for real-time data processing, decision-making, and adaptive model training on resource-constrained devices without centralized servers. However, it faces challenges like limited computational resources, high inference costs, and security vulnerabilities.

Method: The survey examines techniques that optimize model efficiency, accelerate inference, support collaborative learning, reduce model complexity, improve inference speed, ensure privacy-preserving computation, and enhance scalability and adaptability through advancements in hardware-software co-design, model compression, and decentralized learning.

Result: The analysis provides insights into the current state of onboard learning, highlighting methods that enable robust, efficient, and secure AI deployment at the edge.

Conclusion: By addressing key challenges through various optimization strategies, onboard learning can achieve more effective and secure AI deployment on edge devices.

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [448] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

TL;DR: An fast algorithm for embedding hierarchical structures in 3D Minkowski spacetime is presented, which perfectly encodes data correlations in causal structures and applies to datasets like WordNet, suggesting that all discrete data may have a perfect 3D geometric representation.


<details>
  <summary>Details</summary>
Motivation: To find a method that can efficiently embed hierarchical structures into 3D Minkowski spacetime and encode the correlation of data purely within the causal structure without needing global symbolic structure.

Method: Using oriented token pairs as local hierarchical signals, the model embeds hierarchical structures such as the mammal sub-tree of WordNet with ambiguities. It also extends to a maximal unambiguous subset of WordNet nouns.

Result: The model successfully provides a perfect embedding of the mammal sub-tree of WordNet including ambiguities and also achieves a perfect embedding of the maximal unambiguous subset of WordNet nouns. A novel retrieval mechanism based on causality rather than distance is introduced.

Conclusion: All discrete data might have a perfect three-dimensional geometrical representation. The embeddings are nearly conformally invariant, linking to general relativity and field theory. Hierarchical meaning itself appears to be geometric.

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [449] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

TL;DR: This paper investigates model collapse in multi-modal vision-language generative systems and recursive generate-train loops, providing insights and guidelines for mitigating collapse.


<details>
  <summary>Details</summary>
Motivation: To understand model collapse in more realistic scenarios such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving.

Method: Expand the study of synthetic data training and model collapse to multi-modal vision-language generative systems including VLMs and text-to-image diffusion models, and explore recursive generate-train loops with multiple models.

Result: Model collapse in multi-modal context shows distinct characteristics like improved vision-language alignment and increased variance in VLM image-captioning task. General approaches can effectively mitigate model collapse.

Conclusion: The findings provide initial insights and practical guidelines for reducing model collapse risk in self-improving multi-agent AI systems and creating robust multi-modal synthetic datasets.

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [450] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

TL;DR: Large language models can be effectively quantized to 2-bit precision using RMS normalization and a layer-wise quantization schedule, maintaining accuracy without added model complexity.


<details>
  <summary>Details</summary>
Motivation: To reduce the cost of deploying large language models by achieving stable ternary (2-bit) quantization without significant accuracy degradation or added model complexity.

Method: Insert RMS normalization before every linear projection and apply a gradual, layer-wise quantization schedule to fine-tune full-precision checkpoints into ternary LLMs.

Result: This method matches or surpasses more complex knowledge-distillation pipelines on language-modeling benchmarks.

Conclusion: Careful normalization can significantly close the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [451] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 通过自我评判，大语言模型能够在没有参考答案的情况下有效自我提升。实验表明，在 Countdown 拼图和 MIT 积分蜂问题上，模型可以提供可靠的奖励信号，从而在以前不可能的领域实现强化学习。结合合成问题生成，建立了一个完整的自我改进循环，使Qwen 2.5 7B比基线提高了8%，并在积分任务上超过了GPT-4o的表现。这表明LLM评判者能够为训练模型提供有效的奖励信号，可能带来向自我导向学习的范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在许多领域受到缺乏编程奖励的限制，难以进行强化学习。因此，探索一种无需参考答案的自我改进方法成为重要的研究方向。

Method: 利用生成和验证解决方案之间的固有不对称性，通过自我评判实现模型的自我改进。具体地，在 Countdown 拼图和 MIT 积分蜂问题上测试模型，并结合合成问题生成技术，形成一个完整的自我改进循环：生成练习问题、解决问题并评估自身性能。

Result: Qwen 2.5 7B实现了8%的性能提升，超过基线并在积分任务上超越了GPT-4o的表现。证明了LLM评判者能够提供有效的奖励信号以训练模型。

Conclusion: 这项研究表明，通过自我评判和自我导向学习，AI系统能够持续改进，可能加速那些训练数据稀缺或评估要求复杂的领域的进步。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [452] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: This paper argues for using a linear combination of accuracy and fairness metrics to measure the overall value of predictive algorithms, based on Harsanyi's result in preference aggregation. It also analyzes accuracy-fairness trade-offs using the COMPAS dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the conceptual challenges of balancing accuracy and fairness in predictive algorithms, as well as determining how to aggregate preferences for different measures of these qualities.

Method: The method involves making a formal argument based on Harsanyi's classic result in preference aggregation literature to justify the use of a linear combination of accuracy and fairness metrics. This approach is then applied to analyze accuracy-fairness trade-offs using the COMPAS dataset.

Result: The result is a justification for using a linear combination of accuracy and fairness metrics to evaluate the overall value of predictive algorithms, with an application to the COMPAS dataset illustrating the analysis of such trade-offs.

Conclusion: The conclusion emphasizes that there are good reasons to use a linear combination of accuracy and fairness metrics when evaluating predictive algorithms, providing a normative guideline for managing trade-offs between accuracy and fairness.

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [453] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

TL;DR: This paper introduces metrics to evaluate simplified time series for interpretability of Time Series Classifier (TSC), showing that simplifications are better than original time series in certain conditions.


<details>
  <summary>Details</summary>
Motivation: Time series data is not intuitively understandable to humans, unlike text and image data. Simplified time series could improve interpretability of TSC.

Method: Introduced metrics related to complexity (number of segments) and loyalty (likelihood to maintain classification). Evaluated four distinct simplification algorithms across several TSC algorithms and datasets with varying characteristics.

Result: Simplifications for interpretability of TSC are much better than using the original time series, especially when the time series are seasonal, non-stationary and/or with low entropy.

Conclusion: Simplified time series can significantly enhance the interpretability of TSCs under specific conditions.

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [454] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

TL;DR: Recent experiments indicate that training trajectories of various deep neural networks evolve on a low-dimensional 'hyper-ribbon-like' manifold. Inspired by this, the paper analytically characterizes this phenomenon for linear networks using dynamical systems theory.


<details>
  <summary>Details</summary>
Motivation: To understand why training trajectories of different neural networks evolve on a low-dimensional manifold.

Method: Using tools in dynamical systems theory, analyze how factors such as eigenvalue decay rate, scale of ground-truth output to initial weights, and number of gradient descent steps control the geometry of this manifold. Also extend analysis to kernel machines and linear models trained with stochastic gradient descent.

Result: The geometry of the low-dimensional manifold is controlled by three key factors and phase boundaries of the region where hyper-ribbons are expected can be characterized by analytically computing and bounding these factors.

Conclusion: This study provides analytical insights into why training trajectories evolve on low-dimensional manifolds for linear networks, kernel machines, and linear models trained with stochastic gradient descent.

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [455] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

TL;DR: The paper explores machine learning methods for analyzing exoplanet atmospheres from spectral data, emphasizing uncertainty estimation's role in performance and highlighting the limitations of a business-driven approach.


<details>
  <summary>Details</summary>
Motivation: To address the complex challenge of characterizing exoplanetary atmospheres through spectral analysis by exploring machine learning techniques provided by the NeurIPS 2024 Ariel Data Challenge.

Method: Focused on a data-centric business approach with prioritization of generalization. Explored multiple experimental axes such as feature extraction, signal transformation, and heteroskedastic uncertainty modeling.

Result: Uncertainty estimation significantly impacts the Gaussian Log-Likelihood (GLL) score, improving it by 11%. However, limitations were found in tabular modeling and feature engineering for this task.

Conclusion: The study reveals trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis under a competition framework.

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [456] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

TL;DR: The paper introduces ForeCite, a framework using pre-trained causal language models with a linear head to predict average monthly citation rates of biomedical papers, achieving high test correlation and establishing a new state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To automate research evaluation and accelerate scientific progress by predicting future citation rates of academic papers.

Method: ForeCite adapts transformers for regression tasks by appending pre-trained causal language models with a linear head. It uses gradient-based saliency heatmaps.

Result: Achieves a test correlation of ρ = 0.826 on a dataset of 900K+ biomedical papers, a 27-point improvement over previous methods.

Conclusion: ForeCite establishes a new state-of-the-art in forecasting long-term influence of academic research, providing groundwork for automated evaluation of scientific contributions.

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [457] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

TL;DR: GPML库通过将原始网络流量跟踪转换为图形表示，支持社区和光谱指标提取，实现实时检测和历史取证分析，应对动态网络中的复杂攻击。


<details>
  <summary>Details</summary>
Motivation: 在网络中复杂、多步骤和快速演变的攻击增加的情况下，需要先进的网络威胁检测器来应对这些挑战。

Method: GPML库采用基于图的方法，将原始网络流量跟踪转换为图表示，提供工具以检测动态网络中的交互异常和社区变化，并支持社区和光谱度量的提取。

Result: 该方法可以增强实时检测和历史取证分析，支持现代网络安全挑战。

Conclusion: GPML库通过其强大的、基于图的方法，为现代网络安全问题提供了有效的解决方案。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [458] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: State-Space Models (SSMs) have been limited to a few polynomial bases, but this paper introduces SaFARi, a generalized method allowing any frame or basis in SSMs, providing infinite diversity in SSM architecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of using only a few polynomial bases in State-Space Models for online function approximation and machine learning models dealing with long-range dependent data.

Method: The method involves creating a generalized framework called SaFARi which allows building an SSM with any frame or basis, not restricted to polynomials. This includes the HiPPO approach as a subset and opens up many new possibilities within the SSM architecture.

Result: This approach potentially leads to more versatile and effective SSMs that can be applied to a wider range of problems.

Conclusion: SaFARi provides a flexible and powerful tool for enhancing the capabilities of State-Space Models by enabling the use of diverse frames and bases.

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [459] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: The paper proposes an online prediction method for unknown, non-explosive linear stochastic systems by incorporating exponential forgetting to balance the regression model and reduce accumulation error. A sharper logarithmic regret bound of O(log^3 N) is provided.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for predicting unknown linear stochastic systems may suffer from degraded performance due to imbalanced regression models leading to overfitting and reduced prediction accuracy.

Method: Inject an inductive bias into the regression model via exponential forgetting to better balance between regression and regularization errors, reducing accumulation error as well.

Result: Achieves a better trade-off between regression and regularization errors and provides a sharper logarithmic regret bound of O(log^3 N).

Conclusion: The proposed method effectively tackles the problem of imbalanced regression models in online prediction for unknown linear stochastic systems.

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [460] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

TL;DR: 研究提出了一种结合策略优化与熟悉度自编码器的端到端持续学习系统，该系统无需外部信号即可成功实现持续学习，能够识别新任务、匹配已知环境，并在重新遇到已知环境时选择性检索相关知识。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理中的持续学习面临重大挑战，尤其是在没有外部信号指示任务或环境变化的情况下，如何保存和利用现有信息成为关键问题。

Method: 通过将策略优化与熟悉度自编码器集成到一个端到端持续学习系统中，该方法能够在不依赖外部信号的情况下，识别和学习新任务或环境，同时保留先前的经验知识，并在重新遇到已知环境时选择性检索相关知识。

Result: 初步结果显示，该系统能够在没有外部信号指示任务变化或重新遇到任务的情况下成功实现持续学习，证明了该方法的有效性。

Conclusion: 所提出的结合策略优化与熟悉度自编码器的持续学习系统为解决强化学习中的持续学习挑战提供了有希望的方法。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [461] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

TL;DR: The paper presents an AI-driven software solution for quantifying metastatic bone disease from WB-DWI scans, demonstrating high accuracy, sensitivity, and specificity compared to a reference standard.


<details>
  <summary>Details</summary>
Motivation: To develop a reproducible and automated method for quantifying metastatic bone disease from WB-DWI scans, providing useful measurements for clinical decision-making in APC patients.

Method: The method includes a weakly-supervised Residual U-Net model for generating a skeleton probability map, a statistical framework for WB-DWI intensity normalisation, and a shallow convolutional neural network for processing outputs to generate a mask of suspected bone lesions. This mask is applied to the gADC map to extract TDV and gADC statistics.

Result: The tool achieved a Dice score of 0.6 for lesions within pelvis and spine, relative differences for log-TDV and median gADC below 9% and 5%, respectively, coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, and intraclass correlation coefficients above 0.9. It also achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing treatment response.

Conclusion: The developed software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, offering potentially valuable measurements for clinical decision-making.

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [462] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

TL;DR: The paper proposes DyGSSM, a novel method for dynamic graph representation learning that integrates local and global features using GCN, random walk with GRU, and cross-attention mechanism. It also incorporates SSM based on HiPPO algorithm to manage long-term dependencies in parameter updates. Experiments show its superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic graph representation learning methods fail to simultaneously extract global and local information within each snapshot and lack effective temporal dependency management during model parameter updates.

Method: DyGSSM combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. A cross-attention mechanism integrates these features, while an SSM based on the HiPPO algorithm manages long-term dependencies in parameter updates.

Result: Experiments on five public datasets demonstrate that DyGSSM outperforms existing baseline and state-of-the-art methods in 17 out of 20 cases.

Conclusion: DyGSSM addresses the limitations of existing methods by effectively capturing both local and global structures in dynamic graphs and managing temporal dependencies in parameter updates.

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [463] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: Mamba model has weakness in long-range sequential tasks despite being designed for long-range dependencies. This paper analyzes Mamba's limitations and proposes an extension called B2S6 which improves its performance.


<details>
  <summary>Details</summary>
Motivation: To understand and address the gap in Mamba's performance on long-range sequential tasks, improving its universality and versatility.

Method: Analyze Mamba's limitations through three perspectives - expressiveness, inductive bias, and training stability; Propose B2S6, an extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias.

Result: Theoretically proves that B2S6 equips the model with a better-suited inductive bias and improves its expressiveness and stability. Empirically, B2S6 outperforms S4 and S4D on Long-Range Arena tasks while maintaining Mamba's performance on language modeling benchmarks.

Conclusion: B2S6 is a successful extension to Mamba that addresses its limitations in long-range sequential tasks.

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [464] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

TL;DR: 提出了一种名为Flexi-VAE的数据驱动框架，用于高效单次预测非线性参数偏微分方程（PDEs），在保持高精度和稳定性的同时，消除了迭代时间步长的需要。该模型在1D粘性Burgers方程和2D对流扩散方程的经典PDE基准测试中表现良好，比自编码器-LSTM基线模型快50倍（CPU）和90倍（GPU）。


<details>
  <summary>Details</summary>
Motivation: 当前非线性参数偏微分方程的预测方法通常依赖于迭代时间步长，这可能影响效率和准确性。因此，研究者希望开发一种能够消除迭代时间步长需求、同时保持高准确性和稳定性的新方法。

Method: Flexi-VAE框架包含一个神经传播器，该传播器将潜在表示向前推进时间，结合变分自动编码器设置中的潜在演化与物理状态重建。文中评估了两种传播策略：直接连接传播器（DCP）和位置编码传播器（PEP）。通过表示理论分析表明，DCP通过促进解耦和物理意义明确的潜在空间，提供更好的长期泛化能力。几何诊断（包括雅可比谱分析）显示，传播的潜在状态位于解码器敏感性较低和局部几何更稳定的区域，增强了长时间预测的鲁棒性。

Result: 在1D粘性Burgers方程和2D对流扩散方程的经典PDE基准测试中，Flexi-VAE实现了广泛的参数范围内的精确预测，并且相比自编码器-LSTM基线模型，在大时间步长移动时提供了超过50倍（CPU）和90倍（GPU）的速度提升。

Conclusion: Flexi-VAE作为一种可扩展且可解释的代理建模工具，适用于加速计算流体动力学（CFD）和其他由参数PDE驱动的应用中的高保真模拟，并且具有扩展到更高维度和更复杂系统的能力。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [465] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: The paper presents AdaFortiTran, a new model improving channel estimation in OFDM systems under fast-fading channels and low-SNR scenarios by combining convolutional layers and transformer encoder, achieving up to 6 dB MSE reduction compared to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for channel estimation in OFDM systems have limitations in performance under fast-fading channels and low-SNR scenarios.

Method: The method uses convolutional layers to capture correlations between neighboring channel elements and a transformer encoder with global Attention mechanism to model long-range dependencies and spectro-temporal interactions. Nonlinear representations of channel statistics are integrated as priors, and residual connections merge global and local features.

Result: AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB), and delay spreads (50-300 ns).

Conclusion: AdaFortiTran demonstrates superior robustness in high-mobility environments, making it a significant advancement in channel estimation for OFDM systems.

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [466] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

TL;DR: 通过脑信号进行监督学习，可以显著提高深度神经网络对抽象概念的理解能力，并提升其在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模参数和训练数据提升了DNN的能力，但实现复杂的认知能力（如理解抽象概念、推理和适应新场景）仍然是一个重大挑战。

Method: 使用脑-环路监督学习方法，利用少量脑信号将人类的概念结构转移到DNN中，从而增强其对抽象概念的理解能力。

Result: 实验结果表明，该方法不仅提高了DNN在小样本/零样本学习和分布外识别等任务中的性能，还生成了高度可解释的概念表示。

Conclusion: 人类-环路监督可以有效增强大型模型的复杂认知能力，为开发更像人类认知能力的人工系统提供了有希望的方向。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [467] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

TL;DR: This paper proposes a time-consistency discriminator that allows pretrained image diffusion models to generate realistic spatiotemporal dynamics, which performs equally well as Video Diffusion Models (VDMs) in temporal consistency while showing advantages in uncertainty calibration and bias reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of training Video Diffusion Models (VDMs) from scratch, which demands large computational resources and limits their application. The authors aim to leverage pretrained image diffusion models to generate realistic spatiotemporal dynamics with less computational burden.

Method: The method involves introducing a time-consistency discriminator that guides the sampling inference process of pretrained image diffusion models without requiring extensions or fine-tuning of these models. This approach is compared against VDMs trained from scratch on turbulence simulation and precipitation datasets.

Result: The results indicate that the proposed approach matches VDMs in terms of temporal consistency, provides better uncertainty calibration, reduces biases, and successfully achieves stable long-term climate simulations at daily time steps.

Conclusion: The conclusion is that the time-consistency discriminator offers an effective alternative to VDMs for generating realistic spatiotemporal dynamics, with advantages in computational efficiency, uncertainty handling, and reduced bias.

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [468] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

TL;DR: In this paper, a new asynchronous algorithm named Argus is developed for non-convex and non-smooth decentralized federated bilevel learning in SAGIN. Argus allows networked agents to tackle bilevel learning problems asynchronously and avoids the problem of stragglers. Theoretical analysis and numerical experiments demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized and synchronous optimization algorithms are not suitable for space-air-ground integrated networks (SAGIN) due to infrastructureless and time-varying environments.

Method: The paper proposes an asynchronous algorithm called Argus to solve non-convex and non-smooth decentralized federated bilevel learning over SAGIN. This algorithm enables networked agents to handle bilevel learning problems asynchronously in time-varying networks.

Result: The theoretical analysis of iteration complexity, communication complexity, and computational complexity of Argus is provided. Numerical experiments further confirm the effectiveness of the proposed algorithm.

Conclusion: Argus is an effective asynchronous algorithm for addressing non-convex and non-smooth decentralized federated bilevel learning in SAGIN, allowing networked agents to deal with bilevel learning problems without being hindered by stragglers.

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [469] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

TL;DR: This paper proposes DSIV-CFR framework to address the challenge of unmeasured confounders in estimating cumulative causal effects of sequential treatments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the problem of latent confounding bias in sequential treatment effect estimation, which cannot be addressed by existing advanced causal methods even though they use transformers to model time sequences.

Method: The method proposed is Decomposing Sequential Instrumental Variable framework for CounterFactual Regression (DSIV-CFR), which relies on a common negative control assumption. It utilizes an instrumental variable as a special negative control exposure and previous outcome as a negative control outcome to recover latent IVs in observation variables.

Result: Experiments on 4 datasets showed significant performance improvement in one- and multi-step prediction.

Conclusion: The DSIV-CFR framework can effectively adjust latent confounding bias and identify optimal treatments for dynamic systems.

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [470] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

TL;DR: A new fair clustering algorithm, Fair Clustering via Alignment (FCA), is proposed to balance proportions of instances assigned to each cluster with respect to a given sensitive attribute. It guarantees approximately optimal clustering utility for any given fairness level without complex constraints.


<details>
  <summary>Details</summary>
Motivation: Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice.

Method: The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.

Result: Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.

Conclusion: FCA theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice.

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [471] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) have great potential in Customer Experience Management (CXM), but their evaluation is restricted by data scarcity and benchmark limitations. This paper introduces CXMArena, a new large-scale synthetic benchmark dataset for evaluating AI in operational CXM contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of evaluating LLMs' practical utility in complex operational environments due to data scarcity and inadequate benchmarks.

Method: Developed a scalable LLM-powered pipeline that simulates brand's CXM entities forming the foundation of datasets including knowledge articles, product specifications, issue taxonomies, and contact center conversations with controlled noise injection and automated validation. Introduced CXMArena which provides benchmarks targeting five important operational tasks.

Result: Baseline experiments showed the difficulty of the benchmark: state-of-the-art models achieved only 68% accuracy on article search and a low F1 score of 0.3 for knowledge base refinement.

Conclusion: Current models face significant challenges necessitating complex pipelines and solutions over conventional techniques.

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [472] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

TL;DR: The paper introduces DSoftKI, a scalable GP method for fitting and predicting full derivative observations. It enhances SoftKI's interpolation scheme to incorporate directional orientation, enabling accurate high-dimensional predictions with larger datasets.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable Gaussian Process method capable of handling full derivative observations in high-dimensional settings.

Method: DSoftKI extends SoftKI by enhancing its interpolation scheme to include the directional orientation of interpolation points relative to the data, allowing construction of a scalable approximate kernel with first and second-order derivatives through interpolation.

Result: DSoftKI shows accuracy in synthetic function benchmarks and high-dimensional molecular force field prediction (100-1000 dimensions), scaling to larger datasets with full derivative observations than previously possible.

Conclusion: DSoftKI is an effective and scalable method for making accurate predictions with full derivative observations in high-dimensional spaces.

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [473] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 当前自监督学习在无线信道表示中的应用通常借用为文本和图像处理开发的范式，而没有充分解决无线通信的独特特性和约束。为了填补这一空白，本文提出了WiMAE（无线掩码自动编码器），这是一种基于变压器的编码器-解码器基础模型，预训练在一个现实的开源多天线无线信道数据集上。在此基础上，我们开发了ContraWiMAE，通过在统一的多任务框架中结合对比学习目标和重建任务来增强WiMAE。通过对未见过的场景进行广泛的评估，我们展示了这两种方法在多个下游任务中的有效性，ContraWiMAE在无线环境中的线性可分性和适应性方面表现出进一步的改进。与最先进的无线信道基础模型的比较评估证实了我们模型的优越性能和数据效率，突显了它们作为未来自监督无线信道表示学习研究的强大基线的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习在无线信道表示中的应用，通常借用文本和图像处理的范式，但未充分考虑无线通信的独特特性和约束。因此需要一种新的方法来更好地满足无线通信的需求。

Method: 提出了一种名为WiMAE的基于变压器的编码器-解码器基础模型，并在此基础上开发了ContraWiMAE，通过结合对比学习目标和重建任务来增强表示质量。

Result: 通过广泛的评估，证明了WiMAE和ContraWiMAE在多个下游任务中的有效性，尤其是在线性可分性和适应性方面有显著提升。

Conclusion: WiMAE和ContraWiMAE在无线信道表示学习中表现出了优越的性能和数据效率，可以作为未来研究的强大基线。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [474] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

TL;DR: The paper proposes Quotient Complex Transformer (QCformer) based on quotient complexes for predicting properties of hybrid organic-inorganic perovskites (HOIPs). It outperforms state-of-the-art models in HOIP property prediction.


<details>
  <summary>Details</summary>
Motivation: Novel functional materials are crucial for sustainable energy and climate change challenges. Hybrid organic-inorganic perovskites (HOIPs) have exceptional optoelectronic properties but traditional graph neural networks (GNNs) fail to capture their periodic structures and higher-order interactions effectively.

Method: A material structure is modeled as a quotient complex that encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. The model uses a simplex-based Transformer module to leverage higher-order features defined on simplices. QCformer is pretrained on benchmark datasets like Materials Project and JARVIS, then fine-tuned on HOIP datasets.

Result: QCformer outperforms state-of-the-art models in predicting properties of HOIPs.

Conclusion: Quotient complex representation and QCformer provide a powerful new tool for predictive modeling of perovskite materials.

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [475] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

TL;DR: The paper proposes a framework for prioritizing urban green space development in Tehran, using socio-economic, environmental, and sensitivity indices. Machine learning models were used for vegetation cover classification, with Random Forest showing the best performance. The framework was validated through microclimate simulation, demonstrating a reduction in air temperature after implementing green roof technology.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient meteorological stations and provide a valuable tool for urban planners to develop green spaces in urban areas.

Method: Using diverse socio-economic, environmental, and sensitivity indices derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the WRF model. Several machine learning models (XGBoost, LightGBM, Random Forest, Extra Trees) were employed for binary vegetation cover classification.

Result: Random Forest achieved the highest performance exceeding 94% in Overall Accuracy, Recall, and F1-score. The framework's performance was validated through microclimate simulation which demonstrated reducing air temperature by up to 0.67°C after utilizing the green roof technology.

Conclusion: This framework provides a valuable tool for urban planners to prioritize and develop green spaces effectively.

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [476] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

TL;DR: 本研究提出了一种剪枝感知的大规模人工智能模型（LAIM）协同推理方案，通过优化剪枝比例、传输功率和计算频率，在系统延迟、能量和资源约束下，最小化推理失真。实验表明该设计在推理性能、系统延迟和能耗之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着对大规模人工智能模型服务需求的增长，为了实现低延迟和保护隐私的应用，从传统的基于云的推理转向基于边缘的推理成为趋势。特别地，边缘设备协同推理作为一种资源高效的LAIM执行策略在无线网络中崭露头角。

Method: 研究首先证明了LAIM输出失真与其参数失真之间的关系，并通过率失真理论推导出参数失真的下界，从而捕捉剪枝比例与协同推理性能之间的关系。接着，研究将LAIM协同推理失真最小化问题公式化，联合优化剪枝比例、传输功率和计算频率，同时考虑系统延迟、能量和可用资源的约束。最后，提出了一种高效算法来解决这一高度非凸问题。

Result: 广泛的模拟验证了所提设计的有效性。结果表明模型参数失真能够可靠地限定输出失真。此外，所提出的联合剪枝比例和资源管理设计相比基准方案（如完全基于设备或服务器的推理），在推理性能、系统延迟和能耗之间的权衡上表现出优越性能。此外，分割点在异构和资源受限的边缘环境中对系统性能优化起着关键作用。

Conclusion: 剪枝感知的LAIM协同推理方案可以有效降低推理失真，并在推理性能、系统延迟和能耗之间取得良好的平衡。分割点的选择对于优化系统性能至关重要，特别是在异构和资源受限的边缘环境中。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [477] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

TL;DR: The paper proposes Birch SGD, a unifying framework for analyzing and designing distributed SGD methods using computation trees. It introduces eight new methods with optimal computational time complexity and reveals common iteration rates and trade-offs among methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for a unified approach to analyze and design efficient asynchronous and parallel optimization methods, specifically distributed SGD techniques.

Method: Birch SGD represents each method as a weighted directed tree (computation tree) and uses this representation to reduce convergence analysis to studying the geometry of these trees. This graph-based interpretation allows for the development of new methods and analysis of existing ones.

Result: Eight new methods were designed with at least six having optimal computational time complexity. All methods share the same iteration rate, but exhibit different trade-offs in terms of update frequency, communication efficiency, etc.

Conclusion: Birch SGD provides a unifying framework for understanding, analyzing, and designing efficient distributed SGD methods by navigating trade-offs among different approaches.

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [478] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

TL;DR: The paper presents a novel method for stable and convex Information Bottleneck optimization using symbolic continuation and entropy-regularized trajectories, proving solution path uniqueness and providing sensitivity analyses.


<details>
  <summary>Details</summary>
Motivation: The Information Bottleneck method often experiences unstable optimization with sudden representation shifts near critical points of the trade-off parameter beta.

Method: A new approach achieving stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories is introduced. Analytical proof of convexity and uniqueness of the IB solution path with an entropy regularization term is provided.

Result: This method stabilizes representation learning across a wide range of beta values and provides extensive sensitivity analyses around critical points with uncertainty quantification.

Conclusion: The open-source implementation, experimental results, and reproducibility framework offer a practical path for deploying and extending the proposed method.

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [479] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: Analyze the abstract of a paper introducing SDIFT, Sequential DIffusion in Functional Tucker space.


<details>
  <summary>Details</summary>
Motivation: Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations is crucial in scientific research but challenging. Current diffusion-based generative modeling approaches work well with on-grid data but struggle with sparsely observed real-world physical dynamics.

Method: The authors introduce SDIFT (Sequential DIffusion in Functional Tucker space), which uses functional Tucker model as latent space representer for universal approximation property. It represents observations as latent functions and Tucker core sequences. A sequential diffusion model with temporally augmented UNet in functional Tucker space denoises noise from Gaussian process to generate core tensor sequence. Additionally, a Message-Passing Posterior Sampling mechanism allows conditional generation guided by limited time step observations.

Result: SDIFT was validated on three physical systems across astronomical, environmental, and molecular domains. It showed significant improvements in reconstruction accuracy and computational efficiency compared to state-of-the-art methods.

Conclusion: SDIFT provides a novel framework for generating full-field evolution of physical dynamics from irregular sparse observations, outperforming existing methods in both accuracy and efficiency.

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [480] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

TL;DR: Digital textbooks produce learning log data used in EDM studies, but privacy concerns limit data integration across schools. This study proposes a method combining federated learning and differential features to overcome these challenges, demonstrating comparable performance to centralized learning while addressing privacy issues.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating confidential educational data across schools due to privacy concerns, which limits the development of high-performing and generalizable models.

Method: The study proposes a method that combines federated learning, which allows model training without centralizing data thus preserving student privacy, and differential features, which use relative values instead of absolute values to enhance model performance and generalizability.

Result: Experimental results showed that the proposed method resolves privacy concerns while achieving performance on par with centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Using differential features improved prediction performance across all datasets compared to non-differential approaches. The models were also effective for early prediction of at-risk students.

Conclusion: The proposed method successfully integrates data across schools while maintaining student privacy, leading to models with performance comparable to those trained via centralized learning, and offering improved prediction capabilities with differential features.

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [481] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

TL;DR: This paper introduces augmented Gini impurity and LACForest approach to handle the situation where an augmented class appears in testing data but not in training data. It also develops deep neural forests with a new optimization objective.


<details>
  <summary>Details</summary>
Motivation: Decision trees and forests have been successful in various applications, but most work with all testing classes known in training data. This research aims to address the challenge when an augmented class may appear in testing data yet not in training data.

Method: A new splitting criterion called augmented Gini impurity is introduced to incorporate information of the augmented class into trees' splitting. The LACForest approach constructs shallow forests based on this impurity and splits forests with pseudo-labeled augmented instances. Additionally, deep neural forests with a novel optimization objective are developed.

Result: Theoretical convergence analysis for augmented Gini impurity is presented. Experiments verify the effectiveness of the approaches proposed in the paper.

Conclusion: The paper successfully addresses learning with augmented class via forests by introducing augmented Gini impurity and developing the LACForest approach as well as deep neural forests.

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [482] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

TL;DR: The paper uses the Unconstrained Feature Model (UFM) to explore two questions in neural multivariate regression: the comparison between multi-task and single-task models, and the effect of whitening and normalizing regression targets. The UFM predicts, and empirical results confirm, that multi-task models have smaller training MSE with same or stronger regularization on single-task models. Whitening and normalizing reduce training MSE when the average variance across target dimensions is less than one.


<details>
  <summary>Details</summary>
Motivation: To provide qualitative insights into neural multivariate regression using the UFM framework, addressing two key questions about model performance and data preprocessing.

Method: Leveraging the UFM to theoretically predict and empirically validate the impact of multi-task versus single-task models and the effects of whitening and normalizing regression targets on training MSE.

Result: Empirical results confirm UFM predictions: multi-task models achieve smaller training MSE under certain regularization conditions, and whitening/normalizing reduces MSE when the average variance across target dimensions is less than one.

Conclusion: The UFM is a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [483] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

TL;DR: The paper proposes a multi-scale structural-temporal link prediction model (MUST) for unmanned aerial vehicle (UAV) ad hoc networks (UANETs). It uses graph attention networks (GATs) and long short-term memory (LSTM) networks to capture structural and temporal patterns, while addressing sparsity via a custom loss function. Experiments show MUST outperforms existing methods in dynamic and sparse UANETs.


<details>
  <summary>Details</summary>
Motivation: Existing link prediction methods focus on temporal dynamics at a single structural scale and neglect the effects of sparsity in UANETs, leading to insufficient information capture and limited applicability.

Method: The proposed model, MUST, first employs GATs to capture structural features at multiple levels (individual UAV, UAV community, and overall network). Then it uses LSTM networks to learn the temporal dynamics of these multi-scale structural features. A sophisticated loss function is introduced to address the impact of sparsity during model optimization.

Result: Extensive experimental results using simulated UANET datasets demonstrate that MUST achieves state-of-the-art performance in link prediction for highly dynamic and sparse UANETs.

Conclusion: MUST effectively captures meaningful structural and temporal patterns in UANETs, overcoming the challenges posed by their highly dynamic and sparse nature. It outperforms existing methods in link prediction tasks.

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [484] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

TL;DR: GreenFactory is an ensemble of zero-cost proxies that uses a random forest regressor to predict model test accuracy without training. Evaluated on NATS-Bench, it shows high Kendall correlations across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing zero-cost proxies for estimating the performance of deep neural networks during neural architecture search (NAS). Current proxies lack generalization and provide only relative rankings rather than predicted accuracies.

Method: The proposed method, GreenFactory, is an ensemble of zero-cost proxies that leverages a random forest regressor. It combines the strengths of multiple predictors to directly predict model test accuracy without the need for training.

Result: GreenFactory achieves robust results on NATS-Bench across multiple datasets. Specifically, it achieves high Kendall correlations: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120 on NATS-Bench-SSS; and 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120 on NATS-Bench-TSS.

Conclusion: GreenFactory demonstrates substantial agreement between its predicted scores and actual performance, showcasing its reliability in both search spaces.

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [485] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

TL;DR: This paper proposes a new calibration strategy called CleanSE to diminish the impact of false-positive labels in partial label learning by utilizing clean samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve disambiguation in partial label learning by leveraging the strong supervision information of clean samples within datasets, which existing strategies neglect.

Method: The method involves collecting clean samples to guide and enhance confidence in most possible candidates. It uses a differentiable count loss strategy and K-Nearest-Neighbor algorithm to attribute higher significance to reliable candidates. Clean samples also help characterize sample distributions by restricting label counts to a specific interval.

Result: Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL datasets show that this calibration strategy can be applied to most state-of-the-art PLL methods and enhances their performance.

Conclusion: CleanSE is an effective calibration strategy for partial label learning that leverages clean samples to improve disambiguation and overall performance.

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [486] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) are crucial for large-scale graph applications, but their computational demands require efficient methods. Mixed precision quantization offers a solution by enhancing GNN efficiency without sacrificing prediction performance. This paper presents a theorem for quantized message passing and introduces the MixQ-GNN framework, which selects optimal integer bit-widths for GNN components. MixQ-GNN integrates with existing methods and achieves significant reductions in bit operations.


<details>
  <summary>Details</summary>
Motivation: To address the high computational demands of GNNs and improve inference efficiency without affecting prediction performance.

Method: Developed a theorem for efficient quantized message passing that ensures numerical equality with full precision. Introduced MixQ-GNN, a framework that flexibly selects integer bit-widths for all components within GNN layers, optimizing efficiency while maintaining performance.

Result: MixQ-GNN achieved 5.5x reduction in bit operations for node classification and 5.1x reduction for graph classification compared to FP32 precision architectures.

Conclusion: Mixed precision quantization via MixQ-GNN effectively accelerates GNN inference while preserving prediction performance.

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [487] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

TL;DR: 研究了KANs中可学习激活函数在下肢假肢个性化控制中的潜力，并评估了用户特定数据与汇总数据对ML和DL模型性能的影响。结果表明，可学习激活函数在KAN和FKAN中没有显著优势，而用户特定数据在ML模型中表现更好，但在DL模型中无显著差异。这表明可学习激活函数可能在更复杂任务和更大数据集中有优势，同时DL模型可以利用汇总数据进行训练。


<details>
  <summary>Details</summary>
Motivation: 探索可学习激活函数在KANs中的应用潜力，以及用户特定数据与汇总数据对ML和DL模型在预测转向意图时的性能影响。目的是提高下肢假肢个性化控制的效果。

Method: 从五名下肢截肢者在实验室环境中执行转向任务时收集胫骨的IMU数据。使用MLP、KAN、CNN和FKAN四种模型进行即将发生转向的分类能力评估。通过比较MLP和KAN（针对ML模型）以及FKAN和CNN（针对DL模型），评估可学习激活函数的有效性。模型分别在用户特定数据和汇总数据上进行训练，以评估训练数据对性能的影响。

Result: 可学习激活函数在KAN和FKAN中并未显著优于MLP和CNN。对于ML模型，用户特定数据训练的结果显著优于汇总数据（p<0.05）。而对于DL模型，未观察到用户特定数据与汇总数据之间的显著差异。

Conclusion: 可学习激活函数可能在更复杂任务和更大数据集中具有明显优势。此外，在DL模型中，汇总训练表现出与用户特定训练相当的性能，表明假肢控制的模型训练可以利用来自多个参与者的数据。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [488] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath is a framework that enhances LLM-based path planning with safety guarantees, reducing uncertainty and collisions in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To address the safety concerns of overconfidence and hallucinations in Large Language Models (LLMs) when applied to autonomous driving path planning.

Method: SafePath operates in three stages: generating diverse candidate paths using an LLM, filtering out high-risk trajectories with conformal prediction while ensuring at least one safe option, and selecting the safest path or delegating to a human if uncertainty is too high.

Result: Theoretically proven to guarantee a safe trajectory with a user-defined probability. Experiments on nuScenes and Highway-env show a 77% reduction in planning uncertainty and up to a 70% reduction in collision rates.

Conclusion: SafePath effectively makes LLM-driven path planning safer by integrating formal safety guarantees, balancing autonomy and safety.

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [489] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: The paper constructs a convex smooth surrogate loss with linear surrogate regret bound for arbitrary discrete target losses.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between smoothness and linear regret bound for convex smooth surrogate losses.

Method: Constructing a convex smooth surrogate loss based on Fenchel-Young losses generated by the convolutional negentropy, equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk.

Result: Achieves a smooth loss while maintaining the surrogate regret bound linear, and provides a consistent estimator of the underlying class probability.

Conclusion: Demonstrates how convex analysis contributes to optimization and statistical efficiency in risk minimization.

Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap
between the convergence rates of surrogate and target losses, with linear
bounds favorable for their lossless regret transfer. While convex smooth
surrogate losses are appealing in particular due to the efficient estimation
and optimization, the existence of a trade-off between the smoothness and
linear regret bound has been believed in the community. That being said, the
better optimization and estimation properties of convex smooth surrogate losses
may inevitably deteriorate after undergoing the regret transfer onto a target
loss. We overcome this dilemma for arbitrary discrete target losses by
constructing a convex smooth surrogate loss, which entails a linear surrogate
regret bound composed with a tailored prediction link. The construction is
based on Fenchel-Young losses generated by the convolutional negentropy, which
are equivalent to the infimal convolution of a generalized negentropy and the
target Bayes risk. Consequently, the infimal convolution enables us to derive a
smooth loss while maintaining the surrogate regret bound linear. We
additionally benefit from the infimal convolution to have a consistent
estimator of the underlying class probability. Our results are overall a novel
demonstration of how convex analysis penetrates into optimization and
statistical efficiency in risk minimization.

</details>


### [490] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: The paper introduces Variational Rank Reduction Autoencoders (VRRAEs), which combines the strengths of Deterministic Rank Reduction Autoencoders (RRAEs) and Variational Autoencoders (VAEs). By applying careful sampling and KL divergence regularization to RRAEs' latent space, VRRAEs outperform both RRAEs and VAEs in random generation and interpolation tasks on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Autoencoders can be regularized by using a truncated Singular Value Decomposition (SVD) to limit the rank of their latent space. This method is powerful but deterministic, making it counter-intuitive for generative purposes. VAEs, however, are known for their generative capabilities due to their probabilistic latent space. The motivation is to create a model that leverages the advantages of both RRAEs and VAEs.

Method: The authors propose VRRAEs, which combine the latent space regularization of RRAEs with the probabilistic approach of VAEs. They apply a truncated SVD for rank reduction and use KL divergence to further regularize the latent space, allowing for better generative performance.

Result: VRRAEs outperform both RRAEs and VAEs in various tasks including random generation and interpolation, as measured by the FID score. The model shows robustness against posterior collapse, even on small synthetic datasets, and performs well on real-world datasets such as MNIST, CelebA, and CIFAR-10.

Conclusion: By integrating the strengths of RRAEs and VAEs, VRRAEs provide superior generative performance and reduce the risk of posterior collapse. The findings indicate that VRRAEs are a promising advancement in the field of autoencoders.

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [491] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

TL;DR: 提出了一种名为AdaLin的新方法，通过动态调整每个神经元的激活函数来缓解深度神经网络中的塑性损失问题。该方法在多种任务和场景中表现出色，并且不需要额外的超参数或明确的任务边界。


<details>
  <summary>Details</summary>
Motivation: 研究动机来源于近期发现深层线性网络对塑性损失具有较强的抵抗力。为了在非平稳问题设定下改善模型的增量学习能力，需要一种新方法来减轻神经网络中的塑性损失。

Method: 提出了Adaptive Linearization（AdaLin）方法，为每个神经元配备一个可学习参数和门控机制，基于梯度流动将线性注入到激活函数中，从而实现自适应调节，确保足够的梯度信号并持续进行连续学习。

Result: AdaLin与传统激活函数（如ReLU、Tanh和GeLU）结合使用时，在标准基准测试（如Random Label和Permuted MNIST等）上显著提高了性能。此外，在更复杂的场景（如CIFAR-100上的类增量学习和离策略强化学习代理中）也验证了其有效性。

Conclusion: AdaLin是一种通用的方法，能够在不增加额外超参数或无需明确任务边界的情况下有效缓解塑性损失，提升模型在多种任务和场景中的表现。系统性的消融实验表明，神经元级别的适应对于良好性能至关重要。

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [492] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

TL;DR: The paper explores how post-training methods modify language models, focusing on their brittleness. It proposes Layered Unlearning (LU), an unlearning algorithm that enhances robustness to adversarial relearning in language models.


<details>
  <summary>Details</summary>
Motivation: To understand the modifications and brittleness caused by post-training methods such as fine-tuning, alignment, and unlearning in language models.

Method: Design an unlearning algorithm called Layered Unlearning (LU) which creates distinct inhibitory mechanisms for subsets of data during different stages, limiting relearning abilities on parts of the dataset.

Result: LU improves robustness to adversarial relearning for several different unlearning methods when evaluated through synthetic and large language model experiments.

Conclusion: The study contributes to the state-of-the-art in machine unlearning and offers insights into the effects of post-training updates.

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [493] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kahou,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: This paper explores the fairness issues in tabular in-context learning (ICL) and proposes three preprocessing strategies to mitigate bias, finding that uncertainty-based demonstration selection improves group fairness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to understand and address potential biases in tabular in-context learning (ICL), which has emerged as a powerful method for making predictions on structured data without updating model parameters.

Method: The authors investigate fairness implications of tabular ICL and explore three preprocessing strategies: correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection.

Result: Experiments show that uncertainty-based demonstration selection consistently enhances the group fairness of in-context predictions.

Conclusion: The study concludes that uncertainty-based demonstration selection is an effective strategy to improve fairness in tabular ICL.

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [494] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

TL;DR: The paper investigates gradient flows in fully connected feedforward neural networks with smooth activation functions, showing that the gradient flow either converges to a critical point or diverges to infinity while the loss approaches an asymptotic critical value. For well-initialized models, the loss can converge to zero for polynomial target functions given sufficient architecture and data. However, in practical settings, gradient flows often diverge to infinity.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of gradient flows in the context of loss landscapes for neural networks with smooth activation functions.

Method: Analysis of gradient flows in fully connected feedforward neural networks using continuously differentiable activation functions. Theoretical proofs regarding convergence or divergence of gradient flows, and numerical experiments confirming these findings in real-world scenarios.

Result: Gradient flows either converge to a critical point or diverge to infinity, with the loss approaching an asymptotic critical value. For polynomial target functions, optimal loss is zero but only achievable asymptotically. In practice, gradient flows with good initialization tend to diverge to infinity.

Conclusion: Gradient flows in neural networks exhibit specific convergence or divergence behaviors, depending on initialization and target function complexity. The theoretical results are supported by numerical experiments.

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [495] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

TL;DR: The paper introduces Rhomboid Tiling (RT) clustering and RTPool, a hierarchical graph clustering pooling model, which leverages complex geometric information to extract higher-order geometric structures. It outperforms 21 state-of-the-art competitors on 7 benchmark datasets for graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current message passing frameworks in Graph Neural Networks rely heavily on the connectivity structure of graphs, limiting their ability to capture rich geometric features inherent in geometric graphs.

Method: The authors propose Rhomboid Tiling (RT) clustering, a novel method based on the rhomboid tiling structure that clusters data leveraging its complex geometric information. They also design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks.

Result: RTPool demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

Conclusion: Rhomboid Tiling (RT) clustering and RTPool effectively extract higher-order geometric structures and perform exceptionally well in graph classification tasks.

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [496] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper presents Online-iForest, a new method for anomaly detection in streaming data that performs well compared to other online methods and offline techniques with periodic retraining, while being more efficient.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods, both offline and online, are not fully suitable for streaming contexts due to impractical assumptions or the need for periodic retraining.

Method: Online-iForest is a novel anomaly detection method designed specifically for streaming data conditions. It tracks the evolving data generating process over time without requiring periodic retraining.

Result: Experimental results on real-world datasets show that Online-iForest performs comparably to other online methods and offline techniques with periodic retraining, but it outperforms them in terms of efficiency.

Conclusion: Online-iForest is a promising solution for applications requiring fast anomaly detection, such as cybersecurity, fraud, and fault detection.

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [497] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

TL;DR: The paper presents Adversarial Suffix Filtering (ASF), a lightweight and model-agnostic defensive pipeline to protect LLMs against adversarial suffix attacks, reducing attack efficacy below 4% without significantly affecting non-adversarial performance.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are increasingly deployed in various environments but remain vulnerable to jailbreak attacks. Current defense mechanisms either require access to internal model architecture, increase computational costs, or can be bypassed through simple methods.

Method: ASF is an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes from prompts. It is designed as a lightweight, model-agnostic pipeline that can operate effectively in both black-box and white-box settings.

Result: ASF reduces the success rate of state-of-the-art adversarial suffix attacks to below 4%, while having minimal impact on the performance of LLMs in non-adversarial scenarios.

Conclusion: ASF offers a comprehensive defense solution for protecting LLMs against adversarial suffix attacks, enhancing their security and trustworthiness.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [498] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: The paper proposes LAS, a loss-less ANN-SNN conversion method for fully spike-driven LLMs that introduces novel neurons and tailored spike-equivalent Transformer components. Experiments show its effectiveness with no performance loss and even accuracy improvement on certain tasks.


<details>
  <summary>Details</summary>
Motivation: To effectively obtain spiking LLMs with energy efficiency while overcoming the challenges of activation outliers and incompatible nonlinear operations in existing ANN-to-SNN conversion methods.

Method: LAS proposes two novel neurons to handle activation outlier and nonlinear operation issues in ANN-based LLMs. It also tailors spike-equivalent Transformer components ensuring full spiking conversion without any performance loss.

Result: Experimental results on six language models and two vision-language models demonstrate loss-less conversion. Notably, on OPT-66B, LAS improves the accuracy by 2% on the WSC task.

Conclusion: LAS achieves loss-less ANN-SNN conversion for fully spike-driven LLMs. The parameter and ablation studies verify its effectiveness.

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [499] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: This paper presents a method to adapt large language models (LLMs) for execution on analog in-memory computing (AIMC) hardware, overcoming challenges like noise and quantization constraints. The approach allows state-of-the-art LLMs to retain performance comparable to digital 4-bit weight, 8-bit activation baselines while being executed on AIMC hardware.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing (AIMC) offers potential improvements in speed and power efficiency for neural network inference compared to traditional von Neumann-based architectures. However, it introduces challenges such as noisy computations and strict input/output quantization constraints that hinder the deployment of LLMs on AIMC-based hardware.

Method: The researchers developed a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. This involves training methodologies that enable LLMs to overcome the accuracy gap when deployed on AIMC hardware.

Result: The approach successfully enables state-of-the-art LLMs, such as Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct, to retain performance comparable to 4-bit weight, 8-bit activation baselines despite analog noise and quantization constraints. Additionally, the models can be quantized for inference on low-precision digital hardware and benefit from test-time compute scaling.

Conclusion: This work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models.

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [500] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: Graph unlearning is essential for user privacy protection. However, it may introduce bias when deleting user information. This paper proposes FGU, a fair graph unlearning method that ensures fairness and maintains privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of bias introduction in the process of graph unlearning while protecting user privacy.

Method: FGU trains shard models on partitioned subgraphs, unlearns the requested data from the corresponding subgraphs, and retrains the shard models on the modified subgraphs. It employs a bi-level debiasing process to ensure fairness at both shard-level and global-level.

Result: Experiments show that FGU achieves superior fairness while maintaining privacy and accuracy. It is also robust to diverse unlearning requests.

Conclusion: FGU is an effective method for fair graph unlearning that can protect user privacy, maintain model performance, and reduce bias.

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [501] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 研究了联邦学习在AIoT场景中的能耗问题，提出两种基于聚类的设备选择方法以优化模型训练的收敛速度和能源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习研究多关注模型性能、收敛速度和通信效率，而忽视了其在AIoT场景中的能耗影响。

Method: 通过观察发现设备/客户端选择对分布式AIoT环境下的模型训练收敛速度至关重要，进而提出两种基于聚类的方法，将具有相似标签分布的AIoT设备分组，形成几乎异构的设备集群，从而缓解实际分布式学习应用中的异构性问题。

Result: 通过广泛的数值实验表明，所提出的聚类策略通常能够在保持低能耗的同时实现高收敛速度，优于其他近期文献中的方法。

Conclusion: 该研究表明，在AIoT场景中，通过合理的设备选择可以有效降低联邦学习的能耗并提高训练效率。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [502] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 研究了深度形态神经网络（DMNNs），提出了几种新架构并在参数上施加不同约束，证明所提网络可成功训练且比线性网络更易剪枝，还提出一种结合线性和形态层的混合架构以加速大批次梯度下降收敛。


<details>
  <summary>Details</summary>
Motivation: 探索深度形态神经网络（DMNNs）的工作机制及其优化潜力，尤其是激活函数在线性与非线性层间的作用。

Method: 提出多种DMNNs架构并分别在参数和可学习参数上设置不同的约束条件，其中要求大部分参数参与形态学操作；同时设计一种混合网络架构将线性和形态层相结合。

Result: 实验表明所提出的网络能够成功训练，比线性网络更容易剪枝，且形态层的加入可以显著加速大批次梯度下降的收敛速度。但网络的泛化能力仍然有限。

Conclusion: 首次成功训练了受特定约束的DMNNs，并验证了其可训练性和易于剪枝的特点，同时也展示了形态层对加速收敛的效果，但仍需改进网络的泛化性能。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [503] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 为了实现OOD，智能系统需要识别适当的任务不变和可组合的输入特征及组合方法。我们提出仅仅测试OOD设置不足以确认算法确实从数据中学习了组合结构，还需要确认识别的特征确实是组合性的。通过两个任务展示了三个常用神经网络无法解决OOD问题，并开发了两种新型网络架构以在OOD场景中取得成功。即使有正确的偏差和接近完美的OOD性能，算法仍可能无法学习到正确的组合泛化特征。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络在处理OOD问题时存在局限性，需要探索新的方法来实现真正的组合泛化。

Method: 设计两个具有明确OOD度量的任务，测试三种常见神经网络（MLP、CNN、Transformer）在OOD场景中的表现；开发两种新型网络架构，引入偏差使其在OOD场景中表现良好；分析算法是否能够学习到正确的组合特征。

Result: 三种常见神经网络无法解决特定OOD任务；两种新型网络架构在OOD场景中表现出色，但仍可能存在未能学习到正确组合特征的情况。

Conclusion: 仅仅测试OOD性能不足以验证算法是否真正学习了组合结构，还需确认其识别的特征是否具有组合性。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [504] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: This paper presents a federated learning methodology that addresses data quality issues such as noisy labels, missing classes, and imbalanced distributions. It enhances data integrity through adaptive noise cleaning, synthetic data generation, and robust model training. Experiments on MNIST and Fashion-MNIST show significant improvements in macro-F1 Score under varying conditions. The framework is practical for edge devices while maintaining data privacy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges due to data quality issues like noisy labels, missing classes, and imbalanced distributions which affect its effectiveness.

Method: The proposed method includes adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation for class imbalance and missing labels, and robust federated model training.

Result: Experimental evaluations on benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant improvements in the macro-F1 Score under different noise and class imbalance conditions.

Conclusion: The proposed methodology effectively mitigates common data quality challenges providing a robust, scalable, and privacy-compliant solution suitable for real-world federated learning scenarios.

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [505] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: We propose a generative end-to-end solver for black-box combinatorial optimization which learns the structure of energy landscape and facilitates global optimization.


<details>
  <summary>Details</summary>
Motivation: Current methods for black-box combinatorial optimization may not perform well in terms of sample efficiency and solution quality simultaneously, especially on NP problems.

Method: The method treats the black-box objective as an energy function and trains a neural network to model the associated Boltzmann distribution. By conditioning on temperature, the network captures different distributions that help in learning the structure of the energy landscape.

Result: Validated on challenging combinatorial tasks with both limited and unlimited query budgets, showing competitive performance against state-of-the-art black-box optimizers.

Conclusion: This approach can improve sample efficiency when queries are expensive and learn implicit variable interactions when queries are cheap but the problem is hard.

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [506] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: A new MARL framework based on community structures is proposed, enabling flexible coordination patterns and structured information sharing.


<details>
  <summary>Details</summary>
Motivation: Existing MARL frameworks often rely on neighbor-based or fixed interaction graphs, which may not capture complex coordination patterns effectively.

Method: The method involves creating a community-based framework where agents can belong to multiple overlapping communities. Each community maintains shared policy and value functions that agents aggregate according to membership weights. Actor-critic algorithms are designed to exploit this structure for policy updates and value learning.

Result: The approach supports transfer learning and active learning, with theoretical convergence guarantees under linear function approximation for both actor and critic updates.

Conclusion: This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees.

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [507] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: 本研究探讨了在存在噪声和对抗性数据管理的情况下，生成模型在自我消耗再训练循环中的演变，并设计了针对竞争对抗场景的攻击算法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的进步，区分真实数据和合成数据变得越来越困难。使用合成数据进行后续训练可能导致模型崩溃或训练不稳定。此外，合成数据通常受到人为反馈的影响并根据用户偏好进行管理，这可能使得模型收敛到优化这些偏好的分布。然而，在实践中，数据管理往往是嘈杂的或被对抗性地操控。

Method: 作者对生成模型进行了理论分析，以评估这种嘈杂的数据管理对其影响，并确定再训练过程稳健性的条件。基于这一分析，设计了用于竞争对抗场景的攻击算法，其中平台通过恶意用户干扰对手模型与实际用户偏好的一致性。

Result: 实验结果表明，所提出的算法在合成和真实世界数据集上都具有有效性。

Conclusion: 本研究揭示了嘈杂和对抗性数据管理对生成模型的影响，并提供了确保再训练过程稳健性的条件。同时，所设计的攻击算法在竞争对抗场景中展现了其有效性。

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [508] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.LG

TL;DR: This case study applies a phased hyperparameter optimization process to compare multitask natural language model variants.


<details>
  <summary>Details</summary>
Motivation: To optimize hyperparameters in multitask natural language models utilizing multiphase learning rate scheduling and optimizer parameter grouping.

Method: Employ short, Bayesian optimization sessions that leverage multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance using the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn Gaussian process minimization. Initially use low-fidelity sprints to prune the hyperparameter space and progressively increase fidelity. Use a meta-learner to tune threshold values for classification probabilities during inference.

Result: Demonstrates the method on variants of the 2021 Joint Entity and Relation Extraction model proposed by Eberts and Ulges.

Conclusion: The phased hyperparameter optimization process is successfully applied to compare multitask natural language model variants.

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [509] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: This paper analyzes checkpoint data in Large Language Models (LLMs) training and proposes a optimized compression solution, Language Model Compressor (LMC).


<details>
  <summary>Details</summary>
Motivation: The large volume of checkpoint data during LLMs training leads to significant resource consumption. Reducing the data volume can improve efficiency.

Method: Analyze the tensor data evolution during model training, evaluate existing compression engines and optimization techniques, then build a new compression solution named LMC based on byte-grouping and Huffman encoding.

Result: LMC outperforms BZ2 with significantly less compression time. A 16-core parallel implementation of LMC achieves high compression and decompression throughput, reducing CPU resources needed and allowing higher-frequency checkpoints.

Conclusion: The proposed LMC is an effective compression solution for checkpoint data in LLMs training, improving efficiency and enabling more frequent checkpoints.

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [510] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 中风是全球第二大死亡原因和第三大致残原因。本研究探讨了使用人口统计学、临床和生活方式数据的机器学习算法在预测中风风险方面的有效性。通过解决类别不平衡和缺失数据等方法论挑战，评估了包括逻辑回归、随机森林和XGBoost在内的多种模型的性能。结果表明，尽管这些模型具有高准确性，但敏感性仍然是实际临床应用中的限制因素。此外，确定了最有影响力的预测特征，并提出了改进基于机器学习的中风预测的策略。这些发现有助于开发更可靠和可解释的早期中风风险评估模型。


<details>
  <summary>Details</summary>
Motivation: 中风是一个关键的全球健康挑战，迫切需要有效的预测工具来降低其影响。现有的预测方法可能无法充分处理数据中的复杂性和挑战，例如类别不平衡和缺失数据。因此，有必要探索和优化机器学习算法在中风风险预测中的应用。

Method: 使用来自中风预测数据集的人口统计学、临床和生活方式数据，评估了逻辑回归、随机森林和XGBoost等机器学习模型的性能。解决了类别不平衡和缺失数据等方法论问题。

Result: 尽管所评估的模型具有高准确性，但敏感性仍然是一个限制因素，可能影响其在临床环境中的实际应用。同时识别出了最有影响力的预测特征。

Conclusion: 为了提高中风风险预测的可靠性和可解释性，需要进一步改进机器学习模型。确定的关键特征和提出的策略为未来的研究提供了方向。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [511] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: This paper presents an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method for jailbreaking Large Language Models (LLMs). It ensures one-hot encoding stays within the probability simplex, proves its convergence, and demonstrates higher success rates and efficiency compared to other techniques.


<details>
  <summary>Details</summary>
Motivation: To systematically understand LLMs and enhance their safety by developing a more effective method to conduct adversarial attacks or 'jailbreak' these models.

Method: The method involves using exponentiated gradient descent combined with the Bregman projection method to optimize within the probability simplex. This approach aims to overcome limitations of discrete token search and continuous token embedding optimizations.

Result: The proposed technique successfully jailbreaks several widely used LLMs with higher success rates and great efficiency on five open-source LLMs across four datasets, outperforming three state-of-the-art methods.

Conclusion: The developed intrinsic optimization technique is proven to be effective in adversarial attacks on LLMs, demonstrating superior performance and efficiency.

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [512] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 本研究提出了一种用于从平滑信号中学习Kronecker结构化乘积图的交替优化方案，并提供了理论收敛保证，实验表明该方法在合成和真实世界图上的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近对能够自然分解不同方式间依赖关系的乘积图的兴趣日益浓厚，但可用于建模多样化依赖结构的图乘积类型仍然有限。因此，研究如何学习Kronecker结构化的乘积图变得尤为重要，因为Kronecker乘积以更复杂、不可分的方式建模依赖关系。

Method: 为了解决这个非凸问题，作者提出了一种交替方案来优化每个因子图，并为该方案的渐近收敛提供了理论保证。此外，还修改了该算法以学习强乘积的因子图。

Result: 通过在合成和真实世界的图上进行实验，证明了所提出方法的有效性和优于现有方法的性能。

Conclusion: 该研究表明，所提出的交替优化方案可以成功地学习Kronecker结构化乘积图，并且在处理复杂依赖关系方面表现出色，具有理论支持和实证验证。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [513] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: This paper presents Causal Predictive Optimization and Generation, a three-layered approach to optimize the sales process in B2B businesses. It includes prediction with causal ML, optimization with constraint optimization and contextual bandit, and serving with Generative AI. The system was deployed in LinkedIn, showing significant improvements over legacy systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to introduce a principled approach to optimize the sales process, which is crucial for the success of any B2B business.

Method: The method involves a three-layered system: 1) Prediction layer using causal ML, 2) Optimization layer using constraint optimization and contextual bandit, 3) Serving layer using Generative AI and a feedback-loop for system enhancement.

Result: The deployment of this system in LinkedIn showed significant wins over legacy systems.

Conclusion: This work details the implementation and deployment of the Causal Predictive Optimization and Generation system in LinkedIn, demonstrating its effectiveness and sharing insights broadly applicable to the field of sales optimization and business AI.

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [514] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: This paper presents a novel approach for Alzheimer's disease detection using radiogenomic data, including structural MRI images and gene expression data, with a heterogeneous bipartite graph representation learning framework that can classify AD into three stages and identify significant genes.


<details>
  <summary>Details</summary>
Motivation: To integrate imaging and genomic data to gain new insights into diseases and demonstrate the potential of radiogenomic-based classification for other diseases.

Method: A heterogeneous bipartite graph representation learning framework featuring two distinct node types (genes and images) is used to classify Alzheimer's disease into three stages (AD, MCI, CN) with a small dataset.

Result: The network effectively classifies Alzheimer's disease into three distinct stages and identifies significant genes in each classification group, evaluated by metrics such as accuracy, recall, precision, and F1 score.

Conclusion: The proposed technique shows potential for extending radiogenomic-based classification to other diseases.

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [515] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: The paper introduces intrinsic entropy via zentropy theory and proposes a ZENN model for handling heterogeneous data, demonstrating its effectiveness in classification tasks, energy landscape reconstructions, and material behavior predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating heterogeneous datasets with intrinsic disparities in various domains.

Method: Extending zentropy theory by introducing intrinsic entropy and proposing a zentropy-enhanced neural network (ZENN) that learns both energy and intrinsic entropy components. The neural network architecture is redesigned to reflect the intrinsic properties and variability in diverse datasets.

Result: ZENN shows superior generalization capabilities and robustness in classification tasks and energy landscape reconstructions, especially in predicting high-order derivatives. It successfully reconstructs the Helmholtz energy landscape of Fe3Pt and captures key material behaviors.

Conclusion: The study presents a novel data-driven machine learning approach based on zentropy theory, showcasing ZENN as a versatile deep learning framework for scientific problems involving complex, heterogeneous datasets.

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [516] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Chisme is a new suite of protocols for robust intelligence at the network edge, including Chisme-DFL and Chisme-GL, which outperform standard methods in distributed and heterogeneous data model training.


<details>
  <summary>Details</summary>
Motivation: To address challenges in implementing robust intelligence at the network edge, such as heterogeneous data distributions, episodic connectivity, and lack of infrastructure.

Method: Introduced Chisme-DFL (synchronous decentralized FL) and Chisme-GL (asynchronous gossip learning), along with a data similarity heuristic that allows agents to infer affinity with each other using model updates. The heuristic extends DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration.

Result: Chisme methods outperform standard counterparts in model training over distributed and heterogeneous data across various network scenarios.

Conclusion: Chisme provides effective solutions for privacy-preserving distributed learning at the network edge under challenging conditions.

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [517] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: Transformer模型通过权重内学习(IWL)和上下文中学习(ICL)两种模式获取知识。受进化生物学启发，研究探讨了环境可预测性对这两种学习模式平衡的影响，揭示了任务相关的时间演化规律，并提出了相对成本假说来解释这些转变。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解Transformer模型中IWL与ICL之间的相互作用，研究从进化生物学中获得灵感，将IWL类比为遗传编码，ICL类比为表型可塑性，并探索环境可预测性如何影响这两种学习模式的平衡。

Method: 通过回归和分类任务实验操作环境可预测性的维度，系统地研究其对IWL/ICL平衡的影响。分析高环境稳定性对IWL的偏好以及高线索可靠性对ICL效能的提升，并观察不同任务条件下的时间演化规律。

Result: 高环境稳定性显著促进IWL，而低稳定性下高线索可靠性增强ICL效能；任务难易度和学习动态决定了IWL与ICL的转换过程，支持了相对成本假说。

Conclusion: 环境可预测性是决定Transformer模型适应策略的关键因素，研究结果为理解ICL提供了新视角，并指导了训练方法的发展。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [518] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: The paper presents a unified transformer-based approach for data driven attribution that can process different types of data and external factors, with significant impact shown through its implementation at LinkedIn.


<details>
  <summary>Details</summary>
Motivation: Data Driven Attribution is crucial for modern marketing intelligence, requiring advanced methods to assign conversion credits based on causal patterns learned from data.

Method: A unified transformer-based attribution approach is introduced, capable of handling member-level data, aggregate-level data, and integrating external macro factors.

Result: The approach was successfully implemented at large scale at LinkedIn, demonstrating significant impact, with learnings applicable broadly to marketing and ad tech fields.

Conclusion: The transformer-based attribution method offers an effective solution for data driven attribution, showcasing potential for broad application in marketing and advertising platforms.

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [519] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper introduces KQT-EWMA, a non-parametric change-detection algorithm that merges Kernel-QuantTree histogram and EWMA statistic for online multivariate data stream monitoring. It allows controlling false alarms using a pre-determined ARL_0 and demonstrates performance comparable to state-of-the-art methods in experiments.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective change-detection algorithms that can monitor multivariate data streams online with controlled false alarm rates, particularly those that can operate at a pre-determined Average Run Length (ARL_0). Most existing non-parametric methods lack the ability to control ARL_0 a priori.

Method: The method involves combining the Kernel-QuantTree (KQT) histogram with the Exponentially Weighted Moving Average (EWMA) statistic to create the KQT-EWMA algorithm. This algorithm is designed to monitor multivariate data streams in a flexible and practical manner, allowing for non-parametric monitoring without dependence on the data distribution in stationary conditions.

Result: Experiments conducted on both synthetic and real-world datasets show that KQT-EWMA is capable of controlling ARL_0 while achieving detection delays that are either comparable to or lower than other advanced methods designed for similar conditions.

Conclusion: KQT-EWMA represents an advancement in non-parametric change-detection by providing a way to control false alarms through the use of a pre-determined ARL_0. It offers competitive performance in terms of detection delay compared to state-of-the-art techniques.

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [520] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO is a new method for pruning neural network models that reduces computational costs while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the high computational and financial costs associated with training and operating large machine learning models, as well as the limitations of current pruning methods that are computationally and environmentally taxing.

Method: BINGO studies specific subsets of a neural network during the training pass to determine the significance of each weight in contributing to the network's accuracy. It then generates a significance score for each weight, enabling the pruning of insignificant weights in one step.

Result: BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than existing methods.

Conclusion: BINGO offers a solution where AI advancement does not necessarily lead to increased model sizes and computational demands.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [521] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: The paper surveys adversarial attacks in multimodal models encompassing text, image, video, and audio, highlighting the evolution of threats and providing a comprehensive view of the threat landscape.


<details>
  <summary>Details</summary>
Motivation: Multimodal models are vulnerable to adversarial attacks across different modalities. With practitioners increasingly adopting these models for real-world applications, there is a need for a focused view on attack types and preventive actions.

Method: The paper conducts a survey of adversarial attacks targeting all four modalities: text, image, video, and audio, to provide an understanding of the threat landscape and its evolution.

Result: A comprehensive summarization of the threat landscape in the multimodal world is presented, which includes adversarial attacks on text, image, video, and audio modalities.

Conclusion: This survey is the first of its kind to outline the adversarial attack landscape in multimodal models, helping practitioners understand potential threats and take necessary actions.

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [522] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: Large language models (LLMs) show human-like exploration-exploitation behavior in simple tasks but struggle to adapt in complex environments.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs can simulate human decision-making behavior and achieve comparable or superior performance, particularly focusing on the exploration-exploitation tradeoff.

Method: Using multi-armed bandit tasks from cognitive science and psychiatry, the study compares the E&E strategies of LLMs, humans, and MAB algorithms through interpretable choice models. It also examines how reasoning, prompted by strategies or enhanced models, influences LLM decision-making.

Result: Reasoning makes LLMs more human-like in their decision-making, with similar levels of random and directed exploration in simple tasks. However, in complex environments, LLMs lack adaptability compared to humans, especially in directed exploration.

Conclusion: LLMs have potential as tools for simulating human behavior and automated decision-making, but they face limitations, especially in adapting to non-stationary environments.

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [523] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: The paper proposes a hybrid deep learning model TCN-MLP-Attention for Hass avocado price forecasting, achieving RMSE of 1.23 and MSE of 1.51.


<details>
  <summary>Details</summary>
Motivation: Agricultural product price forecasting has become increasingly important with the growing demand for healthy foods. Traditional prediction models struggle with highly nonlinear and dynamic data like Hass avocado prices which are influenced by seasonality, region, and weather.

Method: The proposed method is a hybrid deep learning model named TCN-MLP-Attention Architecture that combines Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting.

Result: The model was trained on a dataset with over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018. The experimental results show excellent predictive performance with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.

Conclusion: This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [524] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 本研究探讨了在流形约束数据上直接采样欧几里得扩散模型的方法，揭示了分数函数的多尺度奇异性，并提出了两种改进采样精度的新方法：Niso-DM和Tango-DM。


<details>
  <summary>Details</summary>
Motivation: 尽管欧几里得扩散模型在生成建模领域取得了显著成功，并已被扩展到流形情况，但现有方法通常显式利用特定流形的结构。本文旨在研究如何对一般流形约束数据直接进行采样。

Method: 1. 分析分数函数在流形嵌入空间中的多尺度奇异性；2. 提出Niso-DM方法，通过在法线方向引入非各向同性噪声来减少尺度差异；3. 提出Tango-DM方法，仅训练分数函数的切线分量并使用切线损失函数。

Result: 数值实验表明，所提出的方法在具有复杂几何形状的各种流形分布上表现出优越的性能。

Conclusion: 本文揭示了分数函数的奇异性问题，并通过提出的两种方法有效提高了流形约束数据上的采样精度。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [525] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: This paper introduces RiCL, a framework for interactive continual learning that addresses limitations of traditional methods by handling dynamic real-time feedback and noisy labels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon traditional continual learning methods which struggle with static datasets and the assumption of clean labels, by enabling AI models to dynamically learn new skills from real-time human feedback while retaining prior knowledge.

Method: RiCL incorporates three key components: a temporal consistency-aware purifier to discern clean from noisy samples; an interaction-aware direct preference optimization strategy to align model behavior with human intent; and a noise-resistant contrastive learning module that captures robust representations.

Result: Extensive experiments on two benchmark datasets (FewRel and TACRED) demonstrate that RiCL substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods.

Conclusion: RiCL effectively addresses the limitations of traditional continual learning paradigms by incorporating mechanisms to handle dynamic feedback and noisy labels.

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [526] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: This paper explores the use of large language models (LLMs), specifically a fine-tuned Llama3 8B model, for analyzing freeway crash data and identifying causation factors without pre-labeled data through zero-shot classification. The research compiles 226 traffic safety studies to create a training dataset covering various factors related to crashes. Results show that LLMs effectively identify primary causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. The model's practical applicability is validated by high agreement among researchers.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors contributing to traffic crashes and their unique characteristics. Thus, there is a need for more advanced tools that can provide comprehensive analysis.

Method: The research leverages a large language model (LLM) to analyze freeway crash data. A training dataset was created from 226 traffic safety studies encompassing environmental, driver, traffic, and geometric design factors. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors. Zero-shot classification was used to identify crash causation without pre-labeled data.

Result: Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%.

Conclusion: This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. It provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices.

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [527] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: This paper focuses on long-term continual learning (CL), proposing a novel framework called Long-CL inspired by human memory mechanisms to mitigate catastrophic forgetting and presenting two benchmarks for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods struggle with the challenges of long-term CL, particularly with handling a significantly larger number of tasks and avoiding catastrophic forgetting.

Method: The authors introduce a task-core memory management strategy and a long-term memory consolidation mechanism. The former indexes crucial memories and adaptively updates them, while the latter selectively retains hard and discriminative samples. Two benchmarks, MMLongCL-Bench and TextLongCL-Bench, are also introduced.

Result: Experimental results indicate that Long-CL surpasses the previous state-of-the-art methods by 7.4% and 6.5% AP on the two benchmarks respectively.

Conclusion: The proposed Long-CL framework effectively addresses the issues in long-term CL, providing a significant improvement over existing methods.

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [528] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: The paper introduces TransPL, a novel approach for unsupervised domain adaptation (UDA) in time series data. It uses code transition matrices derived from vector quantization to model temporal transitions and channel-wise shifts between domains, generating explainable pseudo-labels using Bayes' rule.


<details>
  <summary>Details</summary>
Motivation: Traditional pseudo-labeling strategies fail to capture temporal patterns and channel-wise shifts in UDA for time series data, resulting in sub-optimal pseudo-labels.

Method: TransPL models the joint distribution of the source domain through code transition matrices derived from vector quantization of time series patches. It constructs class- and channel-wise code transition matrices and applies Bayes' rule for target domain adaptation, producing pseudo-labels based on channel-wise weighted class-conditional likelihoods.

Result: TransPL outperforms state-of-the-art pseudo-labeling methods by a significant margin (6.1% accuracy improvement, 4.9% F1 improvement) on four time series UDA benchmarks.

Conclusion: TransPL provides a robust solution for UDA in time series data, offering explicit modeling of temporal transitions and channel-wise shifts, versatility towards different UDA scenarios, and explainable pseudo-label generation.

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [529] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: In this paper, the authors propose FedRAG, a federated reinforcement learning (FRL) framework that enhances performance and protects privacy by sharing an approximated behavior metric-based state projection function rather than sensitive information.


<details>
  <summary>Details</summary>
Motivation: Current FRL methods share encrypted local state or policy information to help clients learn from each other while preserving privacy. The authors aim to explore a new way to enhance FRL performance and protect sensitive information more effectively.

Method: The authors introduce FedRAG, which learns a computationally practical projection function of states for each client and aggregates the parameters of these functions at a central server. This approach shares no task-specific sensitive information but still provides information gain for each client.

Result: Extensive experiments on the DeepMind Control Suite demonstrate insightful results, indicating that the proposed method is effective in enhancing FRL performance while protecting privacy.

Conclusion: FedRAG is a promising FRL framework that enhances performance and protects privacy by sharing approximated behavior metric-based state projection functions.

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [530] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: A machine learning framework for heart disease prediction using 303 samples and 14 features was developed. Random Forest classifier achieved the best performance with 91% accuracy, showing potential in clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: To develop an effective predictive model for heart disease using machine learning techniques to aid clinical decision-making.

Method: The study utilized a heart-disease dataset with 303 samples and 14 features. Data preprocessing was performed followed by model training and evaluation using Logistic Regression, KNN, and Random Forest classifiers. Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV.

Result: Random Forest classifier outperformed other models with an accuracy of 91% and F1-score of 0.89. Evaluation metrics indicated balanced performance across classes.

Conclusion: The proposed machine learning-based framework shows strong potential for aiding clinical decision-making in predicting heart disease. However, limitations such as dataset size and generalizability highlight the need for future studies.

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [531] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.LG

TL;DR: In order to reduce the cost of poisoning attacks in federated learning, this paper proposes a sybil-based virtual data poisoning attack method, which can generate sybil nodes to amplify the impact of poisoning model and uses gradient matching to reduce the computational complexity. In addition, three schemes for target model acquisition are designed for different scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to poisoning attacks by malicious adversaries, but existing methods often involve high costs to achieve effective attacks.

Method: The authors propose a sybil-based virtual data poisoning attack, where a malicious client generates sybil nodes to amplify the poisoning model's impact. They also develop a virtual data generation method based on gradient matching to reduce neural network computational complexity and design three schemes for target model acquisition applicable to online local, online global, and offline scenarios.

Result: In simulation, the proposed method outperforms other attack algorithms because it can obtain a global target model under non-independent uniformly distributed data.

Conclusion: The sybil-based virtual data poisoning attack with gradient matching can effectively reduce the cost of poisoning attacks in federated learning and perform well in different scenarios.

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


### [532] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: The paper introduces AI2MMUM, a scalable model for executing diverse 6G air interface tasks by processing multi-modal data, showing SOTA performance in physical layer tasks.


<details>
  <summary>Details</summary>
Motivation: To design a universal model for future wireless systems that can handle multi-modal data and various air interface tasks.

Method: Proposes AI2MMUM with LLM backbone for contextual comprehension, fine-tuning for domain-specific knowledge, task instructions with keywords and prompts, frozen radio modality encoders, adapter layers, and lightweight task-specific heads.

Result: Achieves state-of-the-art performance in five representative physical environment/wireless channel-based downstream tasks using WAIR-D and DeepMIMO datasets.

Conclusion: AI2MMUM is a promising approach for flexible and effective execution of various physical layer tasks in 6G-oriented wireless systems.

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [533] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [534] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: A new benchmark, ImagineBench, is introduced to evaluate offline RL algorithms using real and LLM-imaginary rollouts. Current algorithms perform suboptimally on unseen tasks, with a 35.44% success rate on hard tasks compared to 64.37% when trained only on real data.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation in reinforcement learning (RL) which heavily relies on extensive real-world interaction data. The authors aim to create a standard benchmark for evaluating offline RL algorithms that use both real and synthetic (LLM-imaginary) rollouts to overcome this issue.

Method: The method involves introducing ImagineBench, a comprehensive benchmark that includes datasets with environment-collected and LLM-imaginary rollouts, diverse domains covering locomotion, robotic manipulation, and navigation tasks, and natural language task instructions with varying complexity levels. State-of-the-art offline RL algorithms are systematically evaluated using this benchmark.

Result: The result shows that applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving only a 35.44% success rate on hard tasks compared to 64.37% when methods are trained on real rollouts for hard tasks.

Conclusion: The conclusion highlights the need for advancements in algorithms to better utilize LLM-imaginary rollouts. Future research opportunities include better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks.

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [535] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: Quantum-classical Hybrid Machine Learning (QHML) models show better performance in anti-cancer drug response prediction when data is optimally normalized.


<details>
  <summary>Details</summary>
Motivation: Anti-cancer drug response prediction needs robust models that work well with small datasets. Quantum-classical Hybrid Machine Learning (QHML) models have this potential but are sensitive to data encoding.

Method: Propose a normalization function based on a moderated gradient version of the $\tanh$ to transform neural network outputs without concentrating them at extreme value ranges.

Result: Evaluated on a dataset of gene expression and drug response measurements, QHML models performed better than classical deep learning models when data was optimally normalized.

Conclusion: This study shows new possibilities for biomedical data analysis using quantum computers.

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [536] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 通过引入AND、OR和ADDER逻辑门的概念，提出了一种结合噪声和去噪干预的框架，以实现电路发现方法的完整性与保真性，并揭示了这些逻辑门在语言模型功能中的基本特性。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现方法无法保证完整性和保真性，容易遗漏关键机制，特别是对OR门的检测不完全。

Method: 系统地引入了三种逻辑门（AND、OR、ADDER），将电路分解为这些逻辑门的组合；提出了一个结合噪声和去噪干预的框架，可以集成到现有电路发现方法中，用于全面识别和区分电路中的逻辑门。

Result: 实验验证了该框架能够恢复电路的保真性、完整性和稀疏性，并揭示了三种逻辑门的比例和对输出的贡献等基本属性。

Conclusion: 提出的框架有助于提高电路发现方法的可靠性和解释性，并为进一步研究语言模型的功能提供了新的视角。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [537] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNN) suffer from catastrophic forgetting. This paper proposes Instance-Prototype Affinity Learning (IPAL), a new method for Non-Exemplar Continual Graph Learning, which integrates graph structural information and decision boundary perception to improve knowledge retention and acquisition.


<details>
  <summary>Details</summary>
Motivation: To address the issue of catastrophic forgetting in GNNs while overcoming limitations such as memory explosion and privacy concerns that arise with rehearsal-based techniques.

Method: The proposed method, Instance-Prototype Affinity Learning (IPAL), builds on Prototype Contrastive Learning (PCL). It incorporates Topology-Integrated Gaussian Prototypes (TIGP) to leverage graph structure and enhance knowledge assimilation, Instance-Prototype Affinity Distillation (IPAD) to preserve task memory, and Decision Boundary Perception (DBP) to improve class discriminability.

Result: Evaluations on four node classification benchmark datasets show that IPAL outperforms existing state-of-the-art methods, achieving a better balance between plasticity (learning new information) and stability (retaining old knowledge).

Conclusion: IPAL is an effective approach for Non-Exemplar Continual Graph Learning that mitigates catastrophic forgetting in GNNs while addressing challenges like feature drift and maintaining high performance.

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [538] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: The paper proposes a fraud detection framework using a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and XAI techniques (SHAP, LIME, PDP, PFI) to improve both accuracy and interpretability. Evaluated on the IEEE-CIS dataset, it achieved 99% accuracy and 0.99 AUC-ROC score.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning models focus on predictive accuracy but lack transparency and interpretability, which is problematic for regulatory compliance and trust in sectors like financial fraud detection.

Method: A stacking ensemble of XGBoost, LightGBM, and CatBoost is used for fraud detection. SHAP is employed for feature selection, while LIME, PDP, and PFI are used to explain model predictions. The IEEE-CIS Fraud Detection dataset with over 590,000 transaction records is utilized for evaluation.

Result: The model achieves 99% accuracy and an AUC-ROC score of 0.99, surpassing several recent related approaches.

Conclusion: It's possible to combine high prediction accuracy with transparent interpretability, leading to more ethical and trustworthy solutions in financial fraud detection.

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [539] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: This paper proposes a self-adaptive distillation method and a trajectory-based distillation loss for multi-task learning in depth estimation and scene segmentation, achieving better performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for joint modeling of depth estimation and scene segmentation transfer knowledge from multiple teachers in a static way, which may not be optimal for the student's current learning ability and could lead to knowledge forgetting.

Method: The proposed method includes a self-adaptive distillation approach that dynamically adjusts knowledge transfer based on the student's learning ability and a knowledge trajectory with a trajectory-based distillation loss to prevent erroneous gradient updates and knowledge forgetting.

Result: The method shows clear improvements over state-of-the-art solutions when evaluated on benchmark datasets like Cityscapes and NYU-v2.

Conclusion: The self-adaptive distillation method and trajectory-based distillation loss effectively improve multi-task learning in depth estimation and scene segmentation.

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [540] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 通过结合大型语言模型（LLM）和时间序列基础模型（TSFM），提出了一种名为ChronoSteer的多模态TSFM框架，该框架能够通过文本修订指令进行引导，有效连接LLM和TSFM。此外，还设计了基于合成数据的两阶段训练策略，并构建了一个高质量的多模态时间序列预测基准。实验结果表明，仅在合成数据上训练的ChronoSteer与单模态主干相比，预测精度提高了25.7%，比之前的多模态最佳方法高出22.5%。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法依赖于单模态的时间序列数据，限制了对丰富文本信息的利用。而大型语言模型（LLM）和时间序列基础模型（TSFM）分别在文本推理和时间建模方面表现出强大的能力，因此将两者的优势结合起来，构建一个多模态模型以同时利用时间和文本信息进行未来推断成为一个重要的研究挑战。

Method: 为了解决事件-序列配对数据的稀缺性，提出了一个解耦框架：使用LLM将文本事件转换为修订指令，这些指令再用于引导TSFM的输出。具体来说，引入了ChronoSteer，一种可以通过文本修订指令进行引导的多模态TSFM，从而有效地连接了LLM和TSFM。为了缓解跨模态指令-序列配对数据的短缺问题，设计了基于合成数据的两阶段训练策略。此外，还构建了一个高质量的多模态时间序列预测基准，以解决评估过程中的信息泄露问题。

Result: 经过整合LLM后，仅在合成数据上训练的ChronoSteer相较于单模态主干提升了25.7%的预测准确性，并且比之前最先进的多模态方法高出22.5%。

Conclusion: ChronoSteer作为一种多模态TSFM，通过结合LLM和TSFM的优点，在预测准确性上取得了显著提升，尤其是在合成数据上的表现优异，为多模态时间序列预测提供了新的方向和解决方案。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [541] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: MiCo is a hierarchical language agent framework that solves ODMBP by formulating it as SMDP-Option, achieving 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for VM scheduling in cloud services have limitations such as inability to adapt to real-time changes, rigid strategies, and lack of generalizability and interpretability.

Method: MiCo uses LLMs in a two-stage architecture - Option Miner discovers non-context-aware strategies, and Option Composer integrates these with contextual ones.

Result: MiCo achieves 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.

Conclusion: MiCo provides an effective solution for complex and large-scale cloud environments.

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [542] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Researchers have found a way to improve the performance of neural networks in predicting molecular properties without needing extensive pretraining or extra techniques. They did this by creating a new method that trains a Graph Transformer on both real and synthetic tasks, leading to better results in most cases.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge of integrating rule-based models into differentiable neural network frameworks effectively. Current methods often rely heavily on pretraining and supplementary techniques to achieve good performance.

Method: The researchers proposed a strategy that jointly trains a single Graph Transformer neural network. This network is trained on both sparse multitask molecular property experimental targets and synthetic targets derived from XGBoost models trained on Osmordred molecular descriptors. These synthetic tasks act as auxiliary tasks.

Result: This approach led to consistent and significant improvements in performance across all 19 molecular property prediction tasks. Specifically, for 16 out of the 19 targets, the multitask Graph Transformer performed better than the XGBoost single-task learner.

Conclusion: The study concludes that synthetic task augmentation is an effective method to enhance the performance of neural models in multitask molecular property prediction without requiring feature injection or pretraining.

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [543] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: 在联邦学习中，由于客户端数据分布的异构性和数据隐私问题，训练出高性能的全局模型具有挑战性。本文通过提高本地模型的适应性来增强全局模型的性能，并提出了一种可行的解决方案以优化本地模型的训练目标。实验表明，该方法显著提高了本地模型的适应性，并实现了优于基线方法的全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的客户端数据分布异构性和数据隐私问题导致难以训练出高性能的全局模型。为了解决这一问题，需要提升本地模型的适应性。

Method: 1. 提出适当本地模型的性质，这些模型在客户端的数据分布上具有良好的适应性。
2. 将该性质形式化为带有约束的本地训练目标。
3. 提出一种可行的解决方案以训练本地模型。

Result: 广泛的实验表明，该方法显著提高了本地模型的适应性，并实现了一个性能优越的全局模型，其表现始终优于基线方法。

Conclusion: 通过改进本地模型的适应性，可以有效提升联邦学习中全局模型的性能。

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [544] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: Federated Learning (FL) is a popular solution for privacy-sensitive applications but faces challenges due to statistical heterogeneity. This study introduces FedAPC, a prototype-based FL framework that enhances feature diversity and model robustness, outperforming SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of domain heterogeneity in Federated Learning which impedes the global model's convergence.

Method: Introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a prototype-based FL framework. It leverages prototypes derived from the mean features of augmented data to capture richer representations and aligns local features with global prototypes.

Result: Experimental results on the Office-10 and Digits datasets show that the framework outperforms SOTA baselines.

Conclusion: FedAPC enhances feature diversity and model robustness in FL under domain heterogeneity.

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [545] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: This paper investigates the best arm identification problem for multi-agent multi-armed bandits, proposing two algorithms Cl-BAI and BAI-Cl that achieve high accuracy and efficiency in terms of sample complexity and communication overhead.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying the best arm for each agent in a multi-agent multi-armed bandit setting where agents are grouped into clusters and the mapping between agents and bandits is unknown.

Method: Two algorithms, Cl-BAI and BAI-Cl, are proposed. Cl-BAI clusters agents first then identifies the best arm, while BAI-Cl identifies the best arms first then clusters agents. Both use the successive elimination framework.

Result: Both algorithms provide δ-PC guarantees with bounds on sample complexity. BAI-Cl's variant is minimax optimal when the number of clusters M is small. Experiments show superior performance in sample and communication efficiency especially when M≪N.

Conclusion: The proposed algorithms effectively solve the best arm identification problem in multi-agent multi-armed bandits with unknown agent-bandit mappings, achieving high accuracy and efficiency.

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [546] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: The paper introduces QuXAI, a framework featuring Q-MEDLEY to explain feature importance in hybrid quantum-classical machine learning (HQML) models. Q-MEDLEY highlights influential classical aspects, separates noise, and performs well against established XAI techniques. Ablation studies validate its composite structure, improving HQML interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust global and local explainability approaches for HQML architectures that use quantized feature encoding followed by classical learning.

Method: Creation of HQML models with quantum feature maps and utilization of Q-MEDLEY explainer which combines feature-based inferences, preserves the quantum transformation stage, and visualizes resulting attributions.

Result: Q-MEDLEY delineates influential classical aspects in HQML models, separates their noise, and performs well against established XAI techniques in classical validation settings. Ablation studies further expose the virtues of the composite structure used in Q-MEDLEY.

Conclusion: This work provides a way to improve the interpretability and reliability of HQML models, promoting greater confidence and safer use of quantum-enhanced AI technology.

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [547] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: Alinear is an ultra-lightweight time series forecasting model that achieves competitive performance with significantly fewer parameters, challenging the necessity of scaling laws in this domain.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of model size in time series forecasting has led to exponentially increasing parameter counts, raising questions about the necessity of such scaling for performance gains.

Method: A horizon-aware adaptive decomposition mechanism dynamically rebalances component emphasis across different forecast lengths, and a progressive frequency attenuation strategy ensures stable prediction without attention mechanisms.

Result: Alinear consistently outperforms large-scale models while using less than 1% of their parameters, across seven benchmark datasets. A new parameter-aware evaluation metric highlights its efficiency under constrained model budgets.

Conclusion: This work challenges the belief that larger models are inherently better, suggesting a paradigm shift towards more efficient time series modeling.

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [548] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: In semiconductor manufacturing, detecting small defects during EUV patterning is challenging due to lack of quality data. Researchers generate synthetic SEM images with known defect distributions and annotations, then compare object detection models (YOLOv8, EfficientNet, SSD). YOLOv8 shows best performance in detecting smaller defects, suggesting synthetic data can replace real-world data for robust machine-learning models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting extremely small defects in semiconductor manufacturing which cause false or missed detections due to lack of defect-annotated quality data.

Method: Artificially generate scanning electron microscopy (SEM) images of line patterns with known defect distributions and autonomously annotate them. Then employ state-of-the-art object detection models like YOLOv8, EfficientNet, and SSD to investigate defect detection performance as a function of defect size.

Result: YOLOv8 has the best mean average precision of 96% compared to EfficientNet (83%) and SSD (77%). It can reliably detect the smallest defect sizes. Tested on real SEM data, YOLOv8 correctly detected 84.6% of Bridge defects and 78.3% of Break defects.

Conclusion: Synthetic data can be used as an alternative to real-world data to develop robust machine-learning models for defect detection in semiconductor manufacturing.

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [549] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer with Complex Multi-Head Attention (CMHA) improves modeling of semantic and positional information, leading to superior performance in various tasks compared to strong baselines.


<details>
  <summary>Details</summary>
Motivation: Transformer models face challenges in effectively integrating positional information while allowing multi-head attention flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity.

Method: Introduced ComplexFormer featuring Complex Multi-Head Attention (CMHA). CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. Two key improvements are incorporated: per-head Euler transformation and per-head adaptive differential rotation mechanism.

Result: Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.

Conclusion: ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [550] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: The paper presents a deep neural network model that fuses acoustic and inertial signals for monitoring cattle feeding behavior using multiple sensors. This model, based on convolutional, recurrent, and dense layers, improves precision through automatic feature extraction. Feature-level fusion outperforms other methods by 0.14 in F1-score and shows a 14% increase compared to state-of-the-art methods. An ablation study and post-training quantization evaluation are also provided.


<details>
  <summary>Details</summary>
Motivation: Efficient herd management requires effective monitoring of feeding behaviors in cattle, which can lead to better diet formulation and early detection of health issues. Existing sensor-based approaches have limitations, prompting the exploration of simultaneous multi-sensor use to enhance estimation precision.

Method: A deep neural network combining convolutional, recurrent, and dense layers is introduced to fuse acoustic and inertial signals. The model explores different levels of information fusion (feature, data, and decision) and selects feature-level fusion as the optimal approach.

Result: The proposed model achieved an F1-score of 0.802, surpassing existing methods by 14%. Feature-level fusion outperformed data and decision-level fusion by at least 0.14 in F1-score. Additional evaluations include an ablation study and post-training quantization.

Conclusion: The deep neural network model successfully enhances the precision of cattle feeding behavior monitoring through the fusion of acoustic and inertial signals. Feature-level fusion was identified as the most effective strategy, and the model outperforms current state-of-the-art methods.

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [551] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的跨域知识迁移框架，以提高大型语言模型在时间序列预测中的性能。通过将结构化的时序信息注入LLMs，可以显著提高预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，除了传统的自然语言任务外，还需要利用其能力来解决其他领域的问题，例如时间序列预测。这在能源系统、金融和医疗等领域变得越来越重要。

Method: 提出了一种新的跨域知识迁移框架，该框架系统地将结构化的时序信息注入到LLMs中，以提高其在时间序列预测任务中的准确性。

Result: 实验结果表明，在真实世界的时间序列数据集上，与未提供辅助信息的基线方法相比，知识引导的预测方法在预测准确性和泛化能力方面显著优于基线。

Conclusion: 研究结果表明，知识迁移策略有潜力弥合大型语言模型与特定领域预测任务之间的差距，从而提高预测性能。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [552] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: The paper explores why larger language models perform better, focusing on the role of representation superposition in the neural scaling law.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of the neural scaling law where loss decreases with model size.

Method: Constructed a toy model based on two empirical principles: LLMs represent more things than their dimensions and language features occur with varying frequencies. Analyzed loss scaling under weak and strong superposition scenarios.

Result: Under weak superposition, loss scales with feature frequency; under strong superposition, loss is inversely proportional to model dimension. Open-sourced LLMs exhibit strong superposition and match the toy model predictions.

Conclusion: Representation superposition is crucial for the observed neural scaling laws and can inspire new training strategies and model architectures.

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [553] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload is a new inference engine that combines speculative decoding with offloading to improve GPU core utilization and inference throughput for LLMs on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Efficient LLM inference on devices with limited resources is challenging due to compute and memory constraints. Current systems offload model weights to CPU memory, leading to inefficiencies in GPU usage and performance.

Method: Propose SpecOffload, which embeds speculative decoding into offloading. It uses latent GPU resources to store and execute a draft model for speculative decoding, interleaving the execution of target and draft models within the offloading pipeline. A planner is introduced to manage tensor placement and select optimal parameters.

Result: SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.

Conclusion: SpecOffload provides a solution to enhance the efficiency of LLM inference on resource-constrained devices, significantly improving GPU core utilization and inference throughput.

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [554] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: The paper introduces Parallel Scaling (ParScale), a new method for scaling language models that increases parallel computation during training and inference, rather than increasing parameters or output tokens. ParScale reuses existing parameters, applies to various model structures, and theoretically offers superior inference efficiency with less memory and latency increase compared to parameter scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find a more inference-efficient way to scale language models beyond the traditional methods of parameter scaling or inference-time scaling by increasing output tokens.

Method: ParScale applies P diverse and learnable transformations to the input, executes forward passes of the model in parallel, and dynamically aggregates the P outputs. This scales parallel computation by reusing existing parameters.

Result: ParScale achieves similar performance improvements as parameter scaling but with up to 22 times less memory increase and 6 times less latency increase. It can also recycle pre-trained models with minimal post-training.

Conclusion: ParScale provides an alternative perspective on the role of computation in machine learning, facilitates the deployment of powerful models in low-resource scenarios, and reduces the training budget significantly.

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [555] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: This paper addresses the charging scheduling problem of Electric Buses (EBs) using a Hierarchical Deep Reinforcement Learning (HDRL) approach, which includes a high-level Semi-MDP and multiple low-level MDPs. The authors propose an HDDQN-HER algorithm to optimize charging strategies for cost reduction while meeting charging targets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the complex charging scheduling problem of EBs, characterized by long-range multi-phase planning with sparse rewards, through the use of deep reinforcement learning techniques.

Method: A Markov Decision Process (MDP) is designed for the charging and operating periods of EBs. This MDP is decoupled into a high-level Semi-MDP and multiple low-level MDPs using Hierarchical DRL (HDRL). An HDDQN-HER algorithm is developed to address decision problems at different temporal resolutions.

Result: The flat policy created by combining the optimal high-level and low-level policies performs as well as the optimal policy of the original MDP. Numerical experiments using real-world data demonstrate the effectiveness of the proposed algorithm in reducing charging costs while achieving charging targets.

Conclusion: The hierarchical DRL framework and HDDQN-HER algorithm successfully tackle the charging scheduling problem for EBs, providing an effective strategy for minimizing charging costs.

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [556] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: This paper proposes a novel router-based architecture for generating high-quality synthetic training data to fine-tune Large Language Models (LLMs) for function calling tasks, which significantly improves function classification accuracy and API parameter selection.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of fine-tuning LLMs for function calling tasks in scenarios where real user interaction data is unavailable due to lack of task-specific data and privacy constraints. This necessitates the generation of synthetic data that can replicate real-world data distributions.

Method: The method involves a router-based architecture that uses domain resources such as content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models, to generate synthetic training data. The flexible routing mechanism ensures that the synthetic data matches observed real-world distributions.

Result: Evaluation on a comprehensive set of real user queries shows significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with the proposed synthetic data outperform traditional approaches.

Conclusion: The proposed router-based architecture for synthetic data generation effectively addresses the limitations of existing approaches, leading to improved performance in function calling tasks and setting new benchmarks.

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [557] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: This paper presents a new data reconstruction attack in Federated Learning that can perfectly recover large data batches without prior knowledge of clients' data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Despite the privacy-preserving nature of Federated Learning (FL), recent studies have shown that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing attacks have limitations regarding assumptions on data distribution and efficiency with larger batch sizes.

Method: The method introduces a novel data reconstruction attack leveraging a geometric perspective on fully connected layers to craft malicious model parameters. This enables the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data.

Result: Through experiments on image and tabular datasets, the attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state-of-the-art.

Conclusion: The proposed data reconstruction attack overcomes limitations of existing methods by enabling perfect recovery of large data batches without prior knowledge of clients' data, highlighting significant vulnerabilities in Federated Learning.

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [558] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV transforms small language models into effective multimodal drafters for vision-language models, increasing accepted length by up to 30% and delivering inference speedups of up to 1.46x.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding accelerates language model inference but applying it to vision-language models is challenging due to architectural mismatches and prediction misalignment between small language models and VLMs.

Method: MASSV uses a two-phase approach: connecting the target VLM's vision encoder to the draft model via a lightweight projector and aligning token predictions through self-distilled visual instruction tuning.

Result: Experiments show MASSV increases accepted length by up to 30% and provides end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

Conclusion: MASSV offers a scalable method for accelerating current and future vision-language models.

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [559] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: This paper presents a deep learning model for high-resolution probabilistic precipitation forecasting in Europe, integrating radar, satellite and NWP data. It surpasses current operational systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of radar-only deep learning models with short forecast lead times and provide accurate forecasts with robust uncertainty quantification.

Method: The model integrates multiple data sources (radar, satellite, and physics-based numerical weather prediction) while capturing long-range interactions, featuring a compact architecture for efficient training and faster inference.

Result: Extensive experiments demonstrate that the model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models.

Conclusion: This model sets a new standard for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency.

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [560] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: The paper connects Hebbian spike-timing-dependent plasticity to noisy gradient descent, proving the learning rule identifies the most active presynaptic neuron and links it to noisy mirror descent.


<details>
  <summary>Details</summary>
Motivation: To explore learning rules accounting for precise spike-timing in biological neural networks beyond well-studied neuronal firing rates-based Hebbian learning.

Method: Relating a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent concerning a natural loss function on the probability simplex.

Result: Proves that the learning rule can identify the presynaptic neuron with the highest activity and uncovers an intrinsic connection to noisy mirror descent.

Conclusion: Hebbian spike-timing-dependent plasticity can be understood through noisy gradient descent and has a connection with noisy mirror descent, advancing understanding of precise spike-timing in neural networks.

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [561] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: The paper proposes a Hierarchical Deep Reinforcement Learning (HDRL) approach, DAC-MAPPO-E, to optimize Electric Bus charging schedules using real-time data and multi-timescale decision-making. It reformulates the MDP into two augmented MDPs and incorporates an attention mechanism and MAPPO algorithm for scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in optimizing Electric Bus (EB) charging schedules due to uncertainties in travel time, energy consumption, electricity prices, and the need for efficient multi-timescale decision-making and scalability for large EB fleets.

Method: The method involves proposing a Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the original Markov Decision Process (MDP) into two augmented MDPs. The novel HDRL algorithm, Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E), is introduced to solve these MDPs and enable multi-timescale decision-making. Enhancements are made at both high and low decision levels through redesigning the decentralized actor network with an attention mechanism and incorporating the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm respectively.

Result: Extensive experiments conducted with real-world data demonstrate the superior performance and scalability of the DAC-MAPPO-E algorithm in optimizing Electric Bus fleet charging schedules.

Conclusion: The proposed DAC-MAPPO-E algorithm successfully addresses the challenges in optimizing Electric Bus charging schedules by enabling multi-timescale decision-making and ensuring scalability for large EB fleets.

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [562] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: In this paper, a novel federated representative-attention-based defense mechanism called FeRA is proposed to address the challenge of detecting backdoor attacks in Federated Learning (FL) systems with non-IID data. FeRA uses cross-client attention over internal feature representations and computes an anomaly score based on representation reconstruction errors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the difficulty in detecting backdoor attacks in FL due to the diverse, non-IID data produced by the heterogeneous nature of edge devices.

Method: FeRA leverages cross-client attention over internal feature representations to distinguish benign from malicious clients by computing an anomaly score based on representation reconstruction errors.

Result: Experimental results indicate that FeRA effectively reduces backdoor attack success rates while maintaining high accuracy on the main task.

Conclusion: FeRA is robust across various FL scenarios, model-agnostic, attack-agnostic, and does not require labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments.

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [563] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: Graph contrastive learning (GCL) often suffers from false negatives, which degrades the performance on downstream tasks. To solve this problem, a novel Negative Metric Learning (NML) enhanced GCL (NML-GCL) is proposed in this paper.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of false negatives in graph contrastive learning (GCL), which leads to suboptimal results in downstream tasks.

Method: The authors propose NML-GCL that uses a learnable Negative Metric Network (NMN) to create a negative metric space. In this space, false negatives can be better distinguished from true negatives based on their distance to an anchor node. A joint training scheme with bi-level optimization objective is also proposed to optimize the encoder and the negative metric network using self-supervision signals.

Result: Theoretical analysis and extensive experiments on widely used benchmarks show the superiority of the proposed method.

Conclusion: NML-GCL effectively addresses the false negative issue in GCL, leading to improved performance on downstream tasks.

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [564] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: The paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) which converges under computation-delay-independent step sizes, making it efficient and suitable for real-world decentralized learning tasks.


<details>
  <summary>Details</summary>
Motivation: Decentralized optimization is crucial for leveraging distributed data without central control. However, practical deployments face challenges due to heterogeneous computation speeds and unpredictable communication delays.

Method: The authors analyze Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool to understand the convergence of ADSGD and show that ADSGD converges under computation-delay-independent step sizes without assuming bounded data heterogeneity.

Result: Empirical experiments reveal that ADSGD outperforms existing methods in wall-clock convergence time across various scenarios.

Conclusion: ADSGD is simple, efficient in memory and communication, and resilient to communication and computation delays, making it well-suited for real-world decentralized learning tasks.

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [565] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: The paper introduces ALERT, a method detecting feature distribution changes in AI models for wireless networks, ensuring model re-training when needed.


<details>
  <summary>Details</summary>
Motivation: AI is crucial for next-gen wireless networks but feature distribution changes can degrade model performance and cause issues. There's a need to detect these changes to ensure model effectiveness.

Method: ALERT includes three components: representation learning (using MLP), statistical testing (Kolmogorov-Smirnov and Population Stability Index tests), and utility assessment (via a new function).

Result: ALERT outperforms ten standard drift detection methods in two wireless network use cases: wireless fingerprinting and link anomaly detection.

Conclusion: ALERT successfully detects feature distribution changes and triggers necessary model re-training in wireless network applications.

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [566] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: This paper addresses the challenge of adapting reinforcement learning (RL) agents to changing environments without forgetting prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Real-world autonomous decision-making systems must operate in environments that change over time, but conventional RL methods struggle to adapt when conditions change.

Method: The dissertation proposes two key capabilities for efficient online adaptation: prioritized exploration and sampling strategies to identify and learn from relevant experiences, and selective preservation of prior knowledge through structured representations.

Result: Demonstrates that with these two capabilities, RL agents can efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge.

Conclusion: Efficient online adaptation in RL requires prioritized exploration/sampling strategies and selective preservation of prior knowledge through structured representations.

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [567] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 在随机分类器集合中，通过采用分类损失作为能量的吉布斯测度对集合进行加权，存在一个有限温度参数可以使分类效果达到最优。此温度对于教师分类器和随机分类器的数量具有普适性。实验表明该现象在高质量、无噪声的数据集中具有重要性。


<details>
  <summary>Details</summary>
Motivation: 许多数据科学和机器学习应用中都存在随机性。由随机组件组成的系统通常表现出确定性的全局行为，这标志着从微观无序到宏观组织的转变。因此，研究随机分类器集合中集体行为的出现是十分有意义的。

Method: 引入理论模型来研究随机分类器集合中集体行为的出现。通过对集合采用由分类损失定义为能量的吉布斯测度进行加权，并分析是否存在使分类效果达到最优的有限温度参数。同时，在样本由高斯分布生成且标签由教师感知机构建的情况下，进行解析证明和数值确认。

Result: 解析证明和数值结果表明，存在一个不依赖于教师分类器和随机分类器数量的最优温度参数，从而揭示了所观察行为的普遍性质。在MNIST数据集上的实验进一步强调了这一现象的重要性。

Conclusion: 通过引入理论模型并结合解析和数值方法，证明了在随机分类器集合中存在一个普适的最优温度参数，使得分类效果达到最优。此发现揭示了系统自组织性质的物理类比。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [568] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: The paper provides a tutorial on discrete variational autoencoders with categorical latent variables, offering a training recipe and example implementation.


<details>
  <summary>Details</summary>
Motivation: To provide a clear and practical introduction to discrete variational autoencoders for those with basic mathematical knowledge.

Method: Introduces VAEs with discrete latent spaces where the latent variables follow a categorical distribution, deriving each step from first principles.

Result: Develops a concrete training recipe and offers an example implementation available on GitHub.

Conclusion: Discrete variational autoencoders with categorical latent variables are presented as a natural choice for certain data modalities, such as text.

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [569] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: Specialized Multi-Task Optimizers (SMTOs) are evaluated on complex multi-task problems, revealing their competitive performance against uniform loss and fixed weights.


<details>
  <summary>Details</summary>
Motivation: To address critiques suggesting that equally weighted tasks can achieve results as good as SMTOs due to poor hyperparameter optimization and lack of regularization in previous studies.

Method: Conduct an extensive empirical evaluation of SMTOs, including recent methods, on more complex multi-task problems.

Result: SMTOs perform well compared to uniform loss; fixed weights can achieve competitive performance compared to SMTOs. Uniform loss performs similarly to SMTOs in some cases.

Conclusion: SMTOs show competitive performance, but fixed weights can also be effective. The reasons for uniform loss performing similarly to SMTOs in certain scenarios are demonstrated.

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [570] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 本论文提出了一种名为FactsR的新方法，用于在医疗咨询期间实时提取重要临床信息（称为Facts），并通过递归使用这些信息生成最终的医疗记录。这种方法通过让医生参与记录生成过程，减少了幻觉、错误表示和过度依赖医生校对的问题，从而提高了记录的准确性和简洁性，同时为实时决策支持开辟了新的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前许多AI书记员解决方案在医疗咨询结束后生成笔记时，依赖于一次性或少量示例提示，缺乏推理能力，可能导致长篇笔记中出现幻觉、误解医生意图以及需要医生校对捕捉错误等问题，这些问题在工作量和疲劳影响警觉性时会对患者安全构成威胁。

Method: 论文介绍了一种名为FactsR的方法，在医疗咨询过程中实时提取重要临床信息（称为Facts），并递归地利用这些信息生成最终的笔记。该方法将医生纳入到笔记生成的过程中，从而提高笔记的质量。

Result: FactsR方法生成的笔记更准确、更简洁，并且为实时决策支持提供了新的应用可能性。

Conclusion: FactsR方法通过实时提取临床信息和递归生成笔记，显著提高了医疗记录的准确性和简洁性，同时增强了患者安全性，为实时决策支持系统的发展奠定了基础。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [571] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: Schrier-Coset Graph Propagation (SCGP) is a new method in GNNs that improves long-range message passing without altering graph topology, showing advantages in hierarchical and modular graphs.


<details>
  <summary>Details</summary>
Motivation: To solve the over-squashing problem in GNNs while avoiding scalability bottlenecks.

Method: Introduces SCGP, which uses Schreier-coset embeddings to enrich node features without changing the input graph topology.

Result: Empirical evaluations show SCGP performs as well as or better than existing methods, particularly excelling with hierarchical and modular graphs.

Conclusion: SCGP offers improved performance, scalability, and efficiency for real-time and resource-constrained applications.

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [572] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: The paper introduces PIF, a new anomaly detection method that merges adaptive isolation techniques with preference embedding, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection by leveraging structured patterns through combining adaptive isolation methods and preference embedding.

Method: PIF embeds data into high dimensional space and uses PI-Forest, a tree-based method, to calculate anomaly scores.

Result: PIF outperforms state-of-the-art anomaly detection methods in experiments on both synthetic and real datasets. PI-Forest is more effective at measuring distances and isolating points in the preference space.

Conclusion: PIF presents a promising approach for anomaly detection by integrating adaptive isolation and preference embedding.

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [573] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG is a two-stage VAE-based IA mesh generator that creates realistic aneurysm shapes and parent vessels, conditioned on specific morphological measurements, to aid in flow simulation studies.


<details>
  <summary>Details</summary>
Motivation: The lack of large IA image datasets makes it difficult to train networks for predicting blood flow forces in real time. Existing shape generation methods fail to capture realistic IA features and ignore the relationship between IA pouches and parent vessels.

Method: AneuG uses a two-stage VAE approach. In the first stage, it generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes with constraints on morphing energy statistics. In the second stage, it generates parent vessels conditioned on GHD tokens by creating vascular centreline and propagating the cross-section.

Result: AneuG can generate IA shapes with specific clinically relevant morphological measurements, improving physiological realism and enabling controlled generation.

Conclusion: AneuG provides a valuable tool for understanding shape variations and the effects of specific clinical shape parameters on fluid dynamics, contributing to both research and potential clinical applications.

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [574] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL is a NAS-based framework designed for data-incremental learning that dynamically adapts model structure, reduces forgetting, and enhances accuracy while maintaining a lower model size.


<details>
  <summary>Details</summary>
Motivation: Incremental learning requires balancing plasticity (learning new tasks) and stability (preserving past knowledge), but existing NAS-based approaches often expand the model at every task, making them impractical in resource-constrained environments.

Method: SEAL uses a capacity estimation metric to determine when to expand the model structure, preserving stability through cross-distillation training after each expansion step. The NAS component searches for both the architecture and the optimal expansion policy.

Result: Experiments show SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods across multiple benchmarks.

Conclusion: Combining NAS and selective expansion holds promise for efficient, adaptive learning in incremental scenarios.

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [575] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: 通过将学习分解为多个不同的机制，可以显著提高数据效率，使其与人类学习相一致。这种方法比单独区分符号和子符号学习对效率的影响更大。研究发现，整合多种专门的学习机制可能是弥合数据驱动的机器学习与人类学习效率差距的关键。


<details>
  <summary>Details</summary>
Motivation: 探讨人类学习者能够从少量例子中快速学习的原因，是否源于我们能够结合使用多种专门的学习机制。

Method: 通过对在线辅导环境中的归纳人类学习模拟进行消融分析，比较强化学习与更高效的数据3机制符号规则归纳方法。

Result: 将学习分解为多个不同机制显著提高了数据效率，与人类学习水平相当，并且这种分解对效率的影响大于符号与子符号学习的区别。

Conclusion: 整合多种专门的学习机制可能是弥合数据驱动的机器学习与人类学习效率差距的关键。

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [576] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: This paper explores the connection between gradient-based optimization in parametric models like neural networks and linear combinations of random features. It demonstrates that if a model can be learned via mini-batch stochastic gradient descent without data distribution assumptions, then with high probability, the target function can be approximated using a polynomial-sized combination of random features.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between gradient-based optimization of parametric models (such as neural networks) and optimization of linear combinations of random features, especially under scenarios where no assumptions about data distribution are made.

Method: The authors prove that if a parametric model can be learned through mini-batch stochastic gradient descent (bSGD) without making assumptions on the data distribution, then the target function can also be approximated by a polynomial-sized combination of random features. They introduce average probabilistic dimension complexity (adc), which extends probabilistic dimension complexity, and demonstrate its polynomial relationship with statistical query dimension.

Result: The size of the combination of random features needed to approximate the target function depends on the number of gradient steps and numerical precision used in bSGD. This reveals fundamental limitations of distribution-free learning in neural networks trained by gradient descent. The study also shows an infinite separation between adc and standard dimension complexity.

Conclusion: This work highlights the importance of making assumptions about data distributions when training neural networks with gradient descent due to the inherent limitations of distribution-free learning. Additionally, it introduces a new theoretical framework, adc, which provides deeper insights into the complexities involved in such optimizations.

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [577] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: The paper introduces Learning to Think (L2T), a framework for fine-tuning large language models (LLMs) to optimize reasoning effectiveness and computational efficiency. L2T uses an information-theoretic reinforcement approach with a novel reward mechanism based on PAC-Bayes bounds and the Fisher information matrix, reducing computational complexity while maintaining estimation accuracy. Empirical results show improvements in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving reasoning abilities in LLMs do not adequately balance reasoning effectiveness with computational efficiency, often leading to unnecessarily long reasoning chains and wasted tokens.

Method: L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward that quantifies episode-wise information gain in parameters without needing extra annotations or task-specific evaluators. A method is proposed to quickly estimate this reward using PAC-Bayes bounds and the Fisher information matrix, theoretically reducing computational complexity with high accuracy. Reinforcement learning optimizes the model by rewarding each episode's contribution and penalizing excessive updates.

Result: Empirical results across various reasoning benchmarks and base models demonstrate L2T's advantage in different tasks, enhancing both reasoning effectiveness and efficiency.

Conclusion: Learning to Think (L2T) provides a promising approach to fine-tune LLMs for achieving optimal reasoning with fewer tokens, thereby improving both the effectiveness and efficiency of reasoning tasks.

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [578] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 本研究探讨了使用基于分数的扩散模型进行云和降水短时预报（0至3小时）的潜力，该方法不仅能够推进现有云层，还能生成和消散云层，包括对流启动。通过实验比较三种扩散模型，发现残差校正扩散模型（CorrDiff）表现最佳，优于传统U-Net和其他模型。此外，扩散模型还具备生成集合预报的能力，有助于误差估计。


<details>
  <summary>Details</summary>
Motivation: 云和降水对于理解天气和气候至关重要，但由于需要次网格参数化，传统的数值天气预报难以准确模拟。尽管机器学习已被用于预测云和降水，但早期方法常产生模糊的预测结果。因此，本研究探索了一种新的机器学习方法——基于分数的扩散模型，以改善短时预报精度。

Method: 研究采用了三种主要类型的扩散模型：标准基于分数的扩散模型（Diff）、残差校正扩散模型（CorrDiff）和潜在扩散模型（LDM）。这些模型利用过去20分钟的地球同步红外卫星图像作为输入，进行云和降水的短时预报。

Result: 扩散模型不仅能够推进现有的云层，还能生成和消散云层，包括对流启动。在实验中，CorrDiff模型表现最佳，其均方根误差比其他扩散模型、传统U-Net以及持续性预报低1至2开尔文。此外，扩散模型还能够自动生成集合预报，其集合离散度与误差相关性良好，显示出有效的校准能力。

Conclusion: 基于分数的扩散模型在云和降水短时预报中表现出色，尤其是CorrDiff模型，其精度和校准能力优于其他方法。这种方法为天气预报提供了新的可能性，并为进一步研究奠定了基础。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [579] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: The paper discusses the limitations of conventional methods for deriving gas turbine engine models and proposes a new approach using Koopman eigenfunction space. The resulting Koopman model was validated against an in-house reference model, and the Koopman-based controller outperformed other benchmark controllers.


<details>
  <summary>Details</summary>
Motivation: Gas turbine engines are complex systems with nonlinear dynamics, making it challenging to derive physics-based models. Conventional experimental methods have limitations when creating component-level and locally linear parameter-varying models.

Method: Employ identification techniques based on data from standard engine operation under closed-loop control. Use sparse identification of nonlinear dynamics for rotor dynamics, map autonomous dynamics into Koopman eigenfunction space via eigenvalue optimization and temporal projection, followed by gradient-based eigenfunction identification.

Result: The Koopman-based controller outperformed classical and gain-scheduled proportional-integral controllers, as well as an internal model control approach, in both reference tracking and disturbance rejection under varying conditions due to its global nature.

Conclusion: Koopman eigenfunction space allows targeting individual modes during optimization, leading to better performance tuning. The proposed method provides a globally optimal nonlinear feedback controller.

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [580] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.LG

TL;DR: Effective communication about breast and cervical cancers is challenging. This study evaluates LLMs' capabilities and limitations in generating cancer-related information. Results show general-purpose LLMs have higher linguistic quality, while medical LLMs have greater accessibility but also more potential harm. There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness.


<details>
  <summary>Details</summary>
Motivation: Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment.

Method: Evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g.

Result: General-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness.

Conclusion: There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness.

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [581] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: NCDPO is a new framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy, enabling efficient likelihood evaluation and gradient backpropagation. It matches MLP+PPO in sample efficiency and surpasses existing methods in performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies can learn diverse skills but suffer from sub-optimal demonstration data which may lead to catastrophic failures. RL-based fine-tuning is promising but adapting PPO to diffusion models is challenging due to computational intractability.

Method: NCDPO reformulates Diffusion Policy as a noise-conditioned deterministic policy, making likelihood evaluation and gradient backpropagation tractable through all diffusion timesteps by treating each denoising step as a differentiable transformation.

Result: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch and outperforms existing methods in both sample efficiency and final performance across various benchmarks. It is also robust to the number of denoising timesteps.

Conclusion: NCDPO addresses the challenges in adapting PPO to diffusion models, providing a more efficient and effective method for training diffusion policies.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [582] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 本论文提出了一种名为QFIX的新方法，它通过简单的'fixing'层扩展了先前模型的表现能力，并在多个实验环境中表现出优越的性能、稳定性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习中的值函数分解方法（如VDN、QMIX）尽管能保证个体-全局最大值（IGM）属性，但表现能力有限；而唯一不受此限制的方法QPLEX又过于复杂。因此，需要一种既能表示完整的IGM值类，又保持简单性的新方法。

Method: 作者提出了一个完整的IGM值类的简单公式化方法，由此推导出QFIX这一新型值函数分解模型家族。QFIX通过一个薄的'fixing'层扩展了先前模型的表现能力。此外，还从该方法中衍生出了多个QFIX变体，并在两个知名的多智能体框架中实现了其中三种变体。

Result: 在SMACv2和Overcooked等多个环境中的实证评估表明：(i) QFIX成功提升了先前方法的性能；(ii) 与主要竞争对手QPLEX相比，QFIX学习更稳定且表现更好；(iii) QFIX在采用最简单和最小的混合模型时达到了上述效果。

Conclusion: QFIX是一种有效的值函数分解模型，它不仅扩展了先前模型的表现能力，而且以更简单和更小的模型实现了更好的性能和稳定性。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [583] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: To overcome the limitations of current XAI frameworks, PnPXAI is introduced as a universal framework that supports diverse data modalities and neural network models in a Plug-and-Play manner.


<details>
  <summary>Details</summary>
Motivation: Current XAI frameworks face challenges such as limited flexibility to diverse model architectures and data modalities, restricted number of supported XAI methods, and sub-optimal recommendations of explanations.

Method: PnPXAI automatically detects model architectures, recommends applicable explanation methods, and optimizes hyperparameters for optimal explanations.

Result: The framework's effectiveness is validated through user surveys and its versatility is showcased across various domains, including medicine and finance.

Conclusion: PnPXAI addresses the limitations of existing XAI frameworks and enhances the adoption of XAI technology in real-world applications.

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [584] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation that integrates an atom-based representation of the 3D pharmacophore into the generative process, offering a powerful and flexible framework for rational drug design.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules is a central challenge in drug discovery, especially for novel targets lacking structural or functional data.

Method: PharmaDiff employs a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses.

Result: PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods and achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures.

Conclusion: By integrating pharmacophore modeling with 3D generative techniques, PharmaDiff offers a powerful and flexible framework for rational drug design.

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [585] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 本研究提出了一种新的工作流程，通过将可穿戴健身设备的生理数据与实时环境暴露相结合，利用AI模型预测个人对污染的健康反应。


<details>
  <summary>Details</summary>
Motivation: 空气污染和气候变化对公共健康构成重大威胁，而个人传感技术和AI技术的进步为监测和预测个人健康结果提供了新机会。

Method: 通过整合可穿戴设备的生理数据和实时环境暴露数据，使用对抗自编码器神经网络训练AI模型，以预测个人对污染暴露的健康反应，并应用迁移学习提高模型的泛化能力。

Result: AI模型能够准确重建时间依赖的健康信号，并捕捉对污染的非线性反应，迁移学习提高了模型的泛化能力。

Conclusion: 该方法展示了在个性化医疗领域的潜力，特别是在预测个人对污染的健康反应方面。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [586] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: The paper introduces Neural Thermodynamic Laws (NTL), a framework providing insights into LLM training dynamics, showing thermodynamic quantities and principles emerge under certain assumptions and offering guidelines for learning rate schedules.


<details>
  <summary>Details</summary>
Motivation: To explore the underlying laws of large language models beyond neural scaling laws.

Method: Demonstrate that key thermodynamic quantities and classical thermodynamic principles naturally emerge under river-valley loss landscape assumptions, and provide intuitive guidelines for designing learning rate schedules from this scientific perspective.

Result: Neural Thermodynamic Laws framework successfully connects thermodynamic concepts with LLM training dynamics, providing theoretical and practical contributions.

Conclusion: NTL offers fresh insights into LLM training dynamics by linking thermodynamic principles with LLMs, contributing both theoretically and practically.

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


### [587] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: 本文旨在通过交叉性理论改进机器学习技术，以实现更公平的结果。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习技术在支持社会身份和体验方面存在不足，因此需要一种能够考虑复杂社会身份并关注社会正义和权力的框架。交叉性理论虽然可以支持开发包容所有社会成员的技术，但其应用方式并不总是符合其基础，从而削弱了其影响力。

Method: 本文通过强调基础的交叉性研究，创建了一个社会相关的初步框架，用于评估和报告机器学习文献中交叉性应用的（不）匹配情况。

Result: 本文提出了一个基于三个C的初步框架，并用它来评估和报告机器学习文献中交叉性应用的（不）匹配情况。

Conclusion: 本文提出了一种基于Crenshaw、Combahee和Collins（三个C）的初步框架，以支持在机器学习中适当采用交叉性理论，从而实现更公平的技术结果。

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [588] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 本文综述了 onboard learning 的各种方法，以解决其面临的挑战，包括优化模型效率、加速推理和在分布式设备上支持协作学习。此外，还探讨了减少模型复杂性、提高推理速度和确保隐私计算的方法，并介绍了增强动态环境中可扩展性和适应性的新兴策略。


<details>
  <summary>Details</summary>
Motivation: Onboard learning 是一种变革性的方法，在边缘 AI 中实现实时数据处理、决策和自适应模型训练，而无需依赖集中式服务器。这种范式对于需要低延迟、增强隐私和能效的应用至关重要。然而，onboard learning 面临着计算资源有限、高推理成本和安全漏洞等挑战。

Method: 本文综述了各种方法，以解决 onboard learning 面临的挑战，包括优化模型效率、加速推理和在分布式设备上支持协作学习。此外，还探讨了减少模型复杂性、提高推理速度和确保隐私计算的方法，并介绍了增强动态环境中可扩展性和适应性的新兴策略。

Result: 本文综述了各种方法，以解决 onboard learning 面临的挑战，包括优化模型效率、加速推理和在分布式设备上支持协作学习。此外，还探讨了减少模型复杂性、提高推理速度和确保隐私计算的方法，并介绍了增强动态环境中可扩展性和适应性的新兴策略。

Conclusion: 本文综述了解决这些挑战的广泛方法，重点是优化模型效率、加速推理和在分布式设备上支持协作学习。同时探讨了减少模型复杂性、提高推理速度和确保隐私计算的方法，并介绍了增强动态环境中可扩展性和适应性的新兴策略。通过结合硬件-软件协同设计、模型压缩和去中心化学习的进展，本文为在边缘进行稳健、高效和安全的AI部署提供了见解。

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [589] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

TL;DR: 本文提出了一种将分层结构嵌入三维闵可夫斯基时空的快速算法，通过因果关系进行层次访问，并在WordNet语料库上取得了完美嵌入的结果。


<details>
  <summary>Details</summary>
Motivation: 我们希望找到一种方法，将数据的层次结构完全编码在几何结构中，并通过因果关系进行访问。

Method: 我们提出了一种快速算法，将分层结构嵌入到三维闵可夫斯基时空。我们的模型仅依赖于定向标记对——局部分层信号——没有访问全局符号结构。我们引入了一种新的检索机制，其中因果关系而不是距离决定了层次访问。

Result: 我们在WordNet语料库上应用了我们的方法，提供了哺乳动物子树的完美嵌入，包括歧义（每个节点有多个层次），并完全编码了层次结构。我们还扩展到了WordNet最大无歧义子集的完美嵌入，包含82,115个名词标记和每个标记一个层次。

Conclusion: 我们的结果表明，所有离散数据都有一个完美的几何表示，它在三维空间中。这些嵌入几乎是共形不变的，表明与广义相对论和场论有深刻的联系。这些结果表明，概念、类别及其相互关系，即层次意义本身，是几何的。

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [590] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

TL;DR: 本文研究了多模态视觉-语言生成系统中的模型崩溃问题，并提出了有效的缓解方法，为构建稳健的多模态合成数据集提供了初步见解和实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有的关于模型崩溃的研究仅限于单一、单模态模型，限制了我们在更现实场景中的理解，例如通过合成数据自主交互的多样化多模态AI代理和持续进化的场景。

Method: 我们将合成数据训练和模型崩溃的研究扩展到多模态视觉-语言生成系统，如视觉-语言模型（VLMs）和文本到图像扩散模型，以及具有多个模型的递归生成-训练循环。

Result: 我们发现，在多模态环境中，模型崩溃表现出不同的特征，例如改进的视觉-语言对齐和VLM图像描述任务中的方差增加。此外，我们发现诸如增加解码预算、更大的模型多样性和使用冻结模型重新标记等通用方法可以有效缓解模型崩溃。

Conclusion: 我们的研究提供了关于减少自我改进的多智能体AI系统中模型崩溃风险以及构建稳健的多模态合成数据集的初步见解和实用指南。

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [591] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过在每个线性投影前插入RMS归一化并应用渐进的、逐层的量化计划，稳定地将全精度检查点微调为三进制LLM。这种方法在标准语言建模基准上与更复杂的知识蒸馏流程相匹配或超越，而没有增加模型复杂度。这些结果表明，仔细的归一化本身就可以弥合二进制和全精度LLM之间的准确度差距，使超低比特推理成为可能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然改变了自然语言处理，但它们的规模使得实际部署成本高昂。后训练量化可以减少内存和计算，但通常会降低准确性，而量化感知训练可以在额外训练的成本下恢复性能。将量化推向三进制（2位）制度可以实现更大的节省，但众所周知这是不稳定的。

Method: 我们在每个线性投影前插入RMS归一化，并应用渐进的、逐层的量化计划，以稳定微调全精度检查点为三进制LLM。

Result: 我们的方法在标准语言建模基准上与更复杂的知识蒸馏流程相匹配或超越，而没有增加模型复杂度。

Conclusion: 这些结果表明，仔细的归一化本身就可以弥合二进制和全精度LLM之间的准确度差距，使超低比特推理成为可能。

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [592] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 本文提出了一种无需参考解决方案即可让大型语言模型通过自我评判实现自我改进的方法，并在多个任务上取得了显著的性能提升，展示了其在强化学习领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统上，强化学习需要参考解决方案来提供奖励信号，但在某些领域中，获取这些解决方案是困难的。我们希望通过自我评判，使模型能够在没有参考解决方案的情况下进行自我改进，从而扩展强化学习的应用范围。

Method: 我们通过自我评判实现模型的自我改进，利用生成和验证解决方案之间的固有不对称性。我们实现了自我评判，并结合合成问题生成，建立了完整的自我改进循环，让模型生成练习题、解决问题并评估自己的表现。

Result: 实验结果显示，模型可以在没有真实答案的情况下提供可靠的奖励信号，从而在Countdown谜题和MIT积分蜜蜂问题上实现显著的性能提升。结合合成问题生成后，模型实现了8%的性能提升，并在积分任务上超越了GPT-4o的表现。

Conclusion: 我们的研究结果表明，LLM裁判可以为训练模型提供有效的奖励信号，从而解锁了许多以前因创建程序化奖励的难度而受限的强化学习环境。这暗示着一种可能的范式转变，即AI系统可以通过自我引导的学习持续改进，而不是依赖于人类指导的训练，这可能加速在数据稀缺或评估要求复杂的领域中的进展。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [593] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: 本文探讨了预测算法在准确性和公平性之间的权衡问题，并提出使用线性组合来衡量其总体价值。


<details>
  <summary>Details</summary>
Motivation: 当前预测算法需要在准确性和公平性之间进行权衡，但缺乏明确的指导原则。

Method: 本文依赖于Harsanyi在偏好聚合文献中的经典结果，提出了一个正式的论点，并应用该结果对COMPAS数据集进行了分析。

Result: 本文论证了使用准确性和公平性度量的线性组合作为预测算法总体价值的衡量方法是合理的。

Conclusion: 本文认为，使用准确性和公平性度量的线性组合来衡量预测算法的总体价值是有道理的。

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [594] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

TL;DR: 本文介绍了评估时间序列简化在TSC解释中使用的度量标准，并发现简化方法在特定类型的时间序列中表现更优。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据不像文本和图像数据那样直观易懂，因此需要简化以提高可解释性。

Method: 引入了评估简化时间序列在TSC解释中的使用的度量标准，包括简化的复杂性和忠诚度。这些度量标准用于评估四种不同的简化算法，并在不同特性的数据集上进行测试。

Result: 研究结果表明，使用简化的时间序列进行TSC的解释效果优于使用原始时间序列，特别是在时间序列具有季节性、非平稳性和/或低熵的情况下。

Conclusion: 使用简化的时间序列在解释TSC方面比使用原始时间序列更好，尤其是在时间序列具有季节性、非平稳性和/或低熵时。

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [595] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [596] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

TL;DR: 本文探讨了在NeurIPS 2024 Ariel数据挑战中，使用机器学习技术从模拟光谱数据中提取大气成分的方法。研究强调了模型简单性、可解释性和泛化能力之间的权衡，并指出在天体物理数据分析中存在一些限制。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习技术从模拟光谱数据中提取大气成分，是解决系外行星大气表征这一复杂挑战的机会。

Method: 我们采用了一种以数据为中心的商业方法，优先考虑泛化而非竞争特定优化。我们探讨了特征提取、信号变换和异方差不确定性建模等多个实验轴。

Result: 我们的实验表明，不确定性估计在高斯对数似然（GLL）分数中起着关键作用，影响性能几个百分点。尽管GLL分数提高了11%，但结果突显了表格建模和特征工程在此任务中的固有局限性，以及在Kaggle式竞赛框架内商业驱动方法的限制。

Conclusion: 我们的研究强调了在天体物理数据分析中模型简单性、可解释性和泛化能力之间的权衡。

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [597] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

TL;DR: ForeCite是一个简单但强大的框架，用于预测学术论文的未来引用率，它在900K+生物医学论文的精心挑选的数据集上实现了测试相关性ρ=0.826，比之前的最先进水平提高了27分。


<details>
  <summary>Details</summary>
Motivation: 预测学术论文的未来引用率是实现研究评估自动化和加速科学进步的重要步骤。

Method: ForeCite是一个简单的但强大的框架，将预训练的因果语言模型附加了一个线性头部，用于平均月引用率预测。适应变压器进行回归任务，ForeCite在900K+生物医学论文的精心挑选的数据集上实现了测试相关性ρ=0.826。

Result: ForeCite在900K+生物医学论文的精心挑选的数据集上实现了测试相关性ρ=0.826，比之前的最先进水平提高了27分。综合尺度定律分析揭示了模型大小和数据量的一致收益，而时间保留实验证实了实际的稳健性。梯度基于显著性热图表明对标题和摘要文本的潜在过度依赖。

Conclusion: 这些结果确立了预测学术研究长期影响的新最先进的水平，并为科学贡献的自动化、高保真评估奠定了基础。

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [598] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

TL;DR: GPML 是一个用于机器学习的图处理库，能够将网络流量转换为图表示，从而帮助检测网络中的异常行为和社区变化。


<details>
  <summary>Details</summary>
Motivation: 由于动态网络中复杂、多步骤和快速演变的攻击显著增加，需要先进的网络威胁检测器。

Method: GPML 库通过将原始网络流量痕迹转换为图表示，从而提供工具来检测交互异常和动态网络中的社区变化。

Result: GPML 支持社区和频谱度量提取，增强了实时检测和历史取证分析。

Conclusion: GPML 提供了一种强大的基于图的方法，以应对现代网络安全挑战。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [599] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文提出了一种通用的状态空间模型构建方法，可以使用任何帧或基，而不仅仅是多项式。


<details>
  <summary>Details</summary>
Motivation: 目前，用于状态空间模型的基函数仅限于少数几种多项式，而最先进的实现基于这些有限选项中的最佳选择。因此，需要一种更通用的方法来扩展状态空间模型的可能性。

Method: 本文提出了一种通用的方法，可以使用任何帧或基来构建状态空间模型（SSM），而不是被限制在多项式上。

Result: 本文提出了一种通用的方法，可以使用任何帧或基来构建状态空间模型（SSM），而不是被限制在多项式上。该框架包括已知的HiPPO方法，还允许在SSM架构中存在无限多样的其他可能的“物种”。

Conclusion: 本文提出了一种通用的方法，可以使用任何帧或基来构建状态空间模型（SSM），而不仅仅局限于多项式。该框架包括已知的HiPPO方法，还允许在SSM架构中存在无限多样的其他可能的“物种”。

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [600] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种通过引入归纳偏差来平衡回归模型的方法，以解决在线预测中的问题。该方法在回归和正则化误差之间取得了更好的权衡，并减少了累积误差。此外，本文还提供了更精确的对数遗憾界O(log^3 N)。


<details>
  <summary>Details</summary>
Motivation: 在已知系统模型的情况下，最优预测器是著名的卡尔曼滤波器。然而，在系统未知的情况下，基于递归最小二乘及其变体的方法可能会因回归模型的高度不平衡而表现不佳，导致过拟合和预测精度下降。

Method: 本文通过在回归模型中注入归纳偏差（即指数遗忘）来解决在线预测中的问题。这种方法不同于传统的数据加权方法，而是专注于平衡回归模型。

Result: 本文的方法在回归和正则化误差之间取得了更好的权衡，并减少了累积误差。此外，本文还提供了更精确的对数遗憾界O(log^3 N)。

Conclusion: 本文提出了一种通过引入归纳偏差来平衡回归模型的方法，以解决在线预测中由于回归模型不平衡导致的过拟合问题。该方法在回归和正则化误差之间取得了更好的权衡，并减少了累积误差。此外，本文还提供了更精确的对数遗憾界O(log^3 N)。

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [601] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

TL;DR: 该研究探索了自动编码器在检测新任务和匹配环境中的有效性，并提出了一个结合策略优化和自动编码器的持续学习系统。


<details>
  <summary>Details</summary>
Motivation: 持续学习对于强化学习代理来说仍然是一个重大挑战，特别是在没有外部信号指示任务或环境变化的情况下。

Method: 该研究结合了策略优化和熟悉度自动编码器，构建了一个端到端的持续学习系统。

Result: 初步结果表明，该方法能够在没有外部信号的情况下成功实现持续学习。

Conclusion: 该研究展示了在没有外部信号的情况下，通过自动编码器实现持续学习的潜力。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [602] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

TL;DR: 本文介绍了一种基于人工智能的软件解决方案，用于从全身扩散加权成像（WB-DWI）扫描中量化骨转移疾病。该软件通过弱监督的残差U-Net模型、统计归一化框架和浅层卷积神经网络实现，具有良好的分割精度和可重复性，并能有效评估治疗反应。


<details>
  <summary>Details</summary>
Motivation: 需要一种可重复的定量方法来监测骨转移疾病的反应，以支持临床决策。

Method: 我们开发了一种基于人工智能的软件解决方案，用于从WB-DWI扫描中量化骨转移疾病。核心技术包括：(i) 一种弱监督的残差U-Net模型生成骨骼概率图以隔离骨骼；(ii) 一种统计框架用于WB-DWI强度归一化，获得信号归一化的b=900s/mm^2（b900）图像；以及(iii) 一个浅层卷积神经网络，处理(i)和(ii)的输出以生成疑似骨病变的掩码，这些病变由于受限的水扩散表现出更高的b900信号强度。此掩码应用于gADC图以提取TDV和gADC统计信息。

Result: 手动和自动分割之间的Dice分数为0.6，平均表面距离为2mm。log-TDV和中位数gADC的相对差异分别低于9%和5%。可重复性分析显示log-TDV和中位数gADC的变异系数分别为4.57%和3.54%，组内相关系数高于0.9。软件在与构建参考标准比较时，在评估治疗反应方面达到了80.5%的准确率、84.3%的灵敏度和85.7%的特异性。生成掩码的计算时间平均为每扫描90秒。

Conclusion: 我们的软件能够从全身扩散加权成像（WB-DWI）扫描中可重复地量化总病灶体积（TDV）和全局表观扩散系数（gADC），从而为监测骨转移疾病的反应提供潜在有用的测量值，这可能对前列腺癌（APC）患者的临床决策有帮助。

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [603] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

TL;DR: 本文提出了一种新的动态图嵌入方法DyGSSM，结合了GCN和GRU进行局部和全局特征提取，并使用交叉注意力机制和基于HiPPO的SSM来处理长期依赖性。实验结果显示该方法在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在每个快照中仅提取局部或全局结构，未能同时提取全局和局部信息，并且在参数更新时未考虑当前快照的模型性能，导致缺乏时间依赖性管理。

Method: 我们提出了一种名为DyGSSM的新方法，结合了图卷积网络（GCN）进行局部特征提取和随机游走与门控循环单元（GRU）进行全局特征提取，并使用交叉注意力机制整合局部和全局特征。此外，我们引入了一个基于HiPPO算法的SSM来处理长期依赖性，在更新模型参数时确保每个快照的模型性能影响后续更新。

Result: 实验结果表明，我们的方法在20个案例中的17个中优于现有的基线和最先进的方法。

Conclusion: 实验结果表明，我们的方法在20个案例中的17个中优于现有的基线和最先进的方法。

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [604] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: 本文分析了Mamba在长序列任务中的问题，并提出了B_2S_6来改进其性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高Mamba的通用性和多功能性，需要理解并解决其在长序列任务中的不足。

Method: 通过三个视角分析Mamba的局限性：表达能力、归纳偏差和训练稳定性，并提出B_2S_6来改进这些方面。

Result: B_2S_6在LRA任务上优于S4和S4D，同时保持了Mamba在语言建模基准上的性能。

Conclusion: 本文分析了Mamba在长序列任务中的局限性，并提出了B_2S_6来改进其性能。

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [605] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

TL;DR: Flexi-VAE 是一种高效的一次性预测非线性参数化偏微分方程（PDEs）的数据驱动框架，通过神经传播器实现潜在表示的时间推进，具有更高的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要迭代时间步进，导致计算成本高昂且效率低下。Flexi-VAE 的目标是提供一种高效、准确且稳定的替代方案，以加速高保真模拟。

Method: Flexi-VAE 引入了一种数据驱动的框架，用于高效的一次性预测非线性参数化偏微分方程（PDEs），消除了迭代时间步进的需要。它结合了一个神经传播器，该传播器在时间上推进潜在表示，并在变分自动编码器设置中对齐潜在演化与物理状态重建。

Result: Flexi-VAE 在经典的 PDE 基准测试中进行了验证，包括 1D 粘性 Burgers 方程和 2D 对流扩散方程，实现了广泛的参数范围内的准确预测。模型在大规模时间偏移情况下比自编码器-LSTM 基线快 50 倍 CPU 和 90 倍 GPU。

Conclusion: Flexi-VAE 是一种可扩展且可解释的代理建模工具，可用于加速计算流体动力学（CFD）和其他参数化偏微分方程（PDE）驱动的应用中的高保真模拟，并且可以扩展到更高维和更复杂的系统。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [606] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: AdaFortiTran is a novel model that improves channel estimation in OFDM systems by combining convolutional layers with a transformer encoder and integrating channel statistics as priors, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for channel estimation in OFDM systems often suffer from performance degradation under fast-fading channels and low-SNR scenarios. The goal is to enhance channel estimation in challenging environments.

Method: AdaFortiTran combines convolutional layers that exploit locality bias with a transformer encoder that applies the global Attention mechanism to channel patches. It also integrates nonlinear representations of channel statistics as priors and uses a residual connection to merge global and local features.

Result: AdaFortiTran achieves up to 6 dB reduction in MSE compared to state-of-the-art models and shows superior robustness across a wide range of Doppler shifts, SNRs, and delay spreads.

Conclusion: AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models and demonstrates superior robustness in high-mobility environments.

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [607] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

TL;DR: 本文提出了一种利用脑信号将人类概念结构转移到深度神经网络的方法，以增强其理解和处理抽象及未见过概念的能力，并在多个任务中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 尽管通过增加训练数据量来扩大模型参数已逐步提高了DNN的能力，但实现复杂的认知能力（如理解抽象概念、推理和适应新场景）仍然是一个重大挑战。

Method: 脑中循环监督学习，利用少量脑信号将人类概念结构转移到深度神经网络（DNNs）中。

Result: 实验结果表明，增强的认知能力在具有挑战性的任务中带来了显著的性能提升，包括少样本/零样本学习和分布外识别，同时产生了高度可解释的概念表示。

Conclusion: 这些发现表明，人机交互监督可以有效增强大型模型的复杂认知能力，为开发更接近人类的认知能力的人工系统提供了有前景的途径。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [608] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

TL;DR: 本文提出了一种时间一致性判别器，使预训练的图像扩散模型能够生成现实的时空动态，无需对模型进行扩展或微调。在理想化的湍流模拟和真实世界的全球降水数据集上进行了比较，结果表明该方法在时间一致性方面表现相当，同时改进了不确定性校准并降低了偏差，并实现了稳定的百年尺度气候模拟。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）是生成高度逼真动态的最先进方法。然而，从头开始训练VDMs可能具有挑战性，并且需要大量的计算资源，限制了它们的广泛应用。

Method: 我们提出了一种时间一致性判别器，使预训练的图像扩散模型能够生成现实的时空动态。该判别器指导采样推理过程，不需要对图像扩散模型进行扩展或微调。

Result: 我们的方法在时间一致性方面表现相当，与VDM相比显示出改进的不确定性校准和较低的偏差，并且在每天的时间步长下实现了稳定的百年尺度气候模拟。

Conclusion: 我们的方法在时间一致性方面表现相当，与VDM相比显示出改进的不确定性校准和较低的偏差，并且在每天的时间步长下实现了稳定的百年尺度气候模拟。

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [609] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的异步算法Argus，用于解决SAGIN中的非凸和非光滑分布式联邦双层学习问题，并通过理论分析和数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式和同步优化算法不适用于SAGIN，因为其具有无基础设施和时间变化的环境。

Method: 提出了一种新的异步算法Argus，用于解决SAGIN中的非凸和非光滑分布式联邦双层学习问题。

Result: 提供了Argus算法的迭代复杂度、通信复杂度和计算复杂度的理论分析，并通过数值实验验证了其有效性。

Conclusion: Argus算法在SAGIN中表现出色，能够有效处理非凸和非光滑的分布式联邦双层学习问题。

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [610] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架DSIV-CFR，用于解决序列治疗效果估计中的潜在混杂偏差问题，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在存在未测量混杂因素的情况下，序列治疗的累积因果效应是一个关键问题。现有的先进因果方法虽然能控制观察到的混杂因素，但仍受未测量混杂因素的影响，因此需要一种有效的方法来调整这种潜在的混杂偏差。

Method: 本文提出了一种基于共同负控制假设的新框架DSIV-CFR，利用工具变量（IV）作为特殊的负控制暴露，同时以前期结果作为负控制结果，从而恢复观察变量中的IV，并通过广义矩条件估计序列治疗效果。

Result: 在4个数据集上的实验表明，DSIV-CFR在单步和多步预测中取得了显著的性能提升，支持了对动态系统中最优治疗方案的识别。

Conclusion: 本文提出了一种新的分解顺序工具变量框架（DSIV-CFR），用于反事实回归，以解决序列治疗效果估计中的潜在混杂偏差问题。实验结果表明，该方法在单步和多步预测中表现出色，能够识别动态系统中的最优治疗方案。

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [611] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平聚类算法 FCA，该算法通过分解公平 K-means 聚类目标函数，能够在不使用复杂约束的情况下理论保证近似最优的聚类效用，并在实验中表现出优越的公平性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平聚类算法由于其固有的复杂性或近似性，可能导致聚类效用不佳或数值不稳定。因此，需要一种新的公平聚类算法来解决这些限制。

Method: FCA 算法通过交替地 (i) 找到一个联合概率分布以对齐不同受保护群体的数据，以及 (ii) 在对齐空间中优化聚类中心来实现公平聚类。

Result: 实验表明，FCA 在 (i) 公平水平和聚类效用之间的权衡方面优于现有方法，以及 (ii) 实现了接近完美的公平性而没有数值不稳定性。

Conclusion: FCA 在实践中实现了高实用性公平聚类，因为它理论上保证了在任何给定的公平水平下近似最优的聚类效用，并且避免了复杂的约束。

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [612] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

TL;DR: 本文提出了CXMArena，一个用于评估AI在操作性客户体验管理（CXM）场景中的大规模合成基准数据集。该数据集涵盖了五个关键操作任务，并展示了当前模型在这些任务上的局限性，强调了需要更复杂的解决方案来克服传统技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试往往缺乏现实性，未能整合深度知识库（KB）集成、现实世界噪声或超出对话流畅性的关键操作任务。因此，需要一个更真实、更全面的基准来评估大型语言模型（LLMs）在复杂操作环境中的实际效用。

Method: 本文介绍了CXMArena，这是一个专门设计用于评估AI在操作性CXM场景中的大规模合成基准数据集。我们开发了一个可扩展的LLM驱动的流程，模拟品牌CXM实体，如知识文章、产品规格、问题分类和客服对话。通过受控噪声注入和严格的自动化验证，这些实体紧密代表了现实世界的分布。

Result: CXMArena 提供了针对五个重要操作任务的专用基准：知识库优化、意图预测、代理质量遵守、文章搜索以及集成工具的多轮RAG。实验结果显示，即使是最先进的嵌入和生成模型在文章搜索任务中也只达到了68%的准确率，而标准嵌入方法在知识库优化任务中的F1分数仅为0.3，这表明当前模型面临重大挑战，需要复杂的管道和解决方案。

Conclusion: CXMArena 是一个用于评估AI在操作性客户体验管理（CXM）场景中的新基准数据集，它提供了针对五个重要操作任务的专用基准。实验结果表明，即使最先进的嵌入和生成模型在文章搜索任务中也只达到了68%的准确率，而标准嵌入方法在知识库优化任务中的F1分数仅为0.3，这突显了当前模型面临的重大挑战，需要复杂的管道和解决方案来克服传统技术的限制。

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [613] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

TL;DR: DSoftKI is a scalable Gaussian Process method that can fit and predict full derivative observations, extending SoftKI to the setting with derivatives and demonstrating accuracy and scalability on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations, which is more accurate and can scale to larger datasets with full derivative observations than previously possible.

Method: DSoftKI extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives. It enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data, enabling the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation.

Result: DSoftKI was evaluated on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating its accuracy and scalability.

Conclusion: DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible.

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [614] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 本文提出了一种新的自监督学习方法，用于无线信道表示，该方法在多个任务中表现出色，并且比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 当前将自监督学习应用于无线信道表示的现有方法往往借鉴了为文本和图像处理开发的范式，但没有充分考虑无线通信的独特特性和约束。我们的工作旨在填补这一空白。

Method: 我们首先提出了WiMAE（Wireless Masked Autoencoder），这是一个基于Transformer的编码器-解码器基础模型，在现实的开源多天线无线信道数据集上进行预训练。在此基础上，我们开发了ContraWiMAE，它通过在统一的多任务框架中结合对比学习目标和重建任务来增强WiMAE。

Result: 通过在未见过的场景上进行广泛的评估，我们证明了两种方法在多个下游任务中的有效性，其中ContraWiMAE在线性可分性和在不同无线环境中的适应性方面表现出进一步的改进。与最先进的无线信道基础模型的比较评估证实了我们模型的优越性能和数据效率。

Conclusion: 我们的模型在自监督无线信道表示学习中展现出优越的性能和数据效率，具有作为未来研究强大基线的潜力。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [615] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

TL;DR: 该研究提出了一种基于商复形的新表示方法，并引入了QCformer模型，用于更准确地预测钙钛矿材料的属性。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）难以捕捉此类系统中的周期结构和高阶相互作用。因此，需要一种新的表示方法和模型来提高材料属性预测的准确性。

Method: 该研究提出了一种基于商复形（QC）的新表示方法，并引入了Quotient Complex Transformer（QCformer）模型。材料结构被建模为商复形，通过不同维度的单形来编码成对和多体相互作用，并通过商操作捕捉材料周期性。模型利用定义在单形上的高阶特征，并使用基于单形的Transformer模块进行处理。

Result: QCformer在HOIP属性预测中优于最先进的模型，证明了其有效性。商复形表示和QCformer模型共同为钙钛矿材料的预测建模提供了一个强大的新工具。

Conclusion: 该研究提出了一个基于商复形（QC）的新表示方法，并引入了Quotient Complex Transformer（QCformer）模型，用于材料属性预测。该方法在HOIP属性预测中表现出色，展示了其有效性。商复形表示和QCformer模型共同为钙钛矿材料的预测建模提供了一个强大的新工具。

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [616] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

TL;DR: 本文提出了一种新框架，用于在德黑兰优先发展城市绿色空间，利用多种数据源和机器学习模型，结果表明该框架有效并具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 由于气象站不足，需要估计1公里分辨率的气温，并且需要确定哪些区域缺乏植被覆盖，以便优先发展城市绿色空间。

Method: 使用多种社会经济、环境和敏感性指数，结合Google Earth Engine、空气污染测量、市政报告和WRF模型，通过机器学习模型（如随机森林）进行二元植被覆盖分类，并生成优先发展城市绿色空间的地图。

Result: 随机森林模型在总体准确率、召回率和F1分数方面均超过94%。微气候模拟显示，在关键区域使用绿色屋顶技术后，空气温度可降低高达0.67°C。

Conclusion: 该框架为城市规划者提供了一个有价值的工具，以开发绿色空间。

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [617] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

TL;DR: 本文研究了一种感知剪枝的LAIM协同推理方案，通过分析和实验验证了其在平衡推理性能、系统延迟和能耗方面的优越性。结果表明，剪枝后的模型参数失真可以作为输出失真的可靠上限，并且分割点在异构和资源受限的边缘环境中对系统性能优化起着关键作用。


<details>
  <summary>Details</summary>
Motivation: 随着对大型人工智能模型（LAIM）服务需求的增长，传统基于云的推理正在向基于边缘的推理转变，以实现低延迟和隐私保护的应用。边缘设备协同推理作为一种资源高效的LAIM执行策略，在无线网络中具有前景。然而，如何在边缘设备和服务器之间合理划分模型以优化性能仍然是一个挑战。

Method: 本文首先证明了LAIM输出失真由参数失真上界限制，然后通过率失真理论推导出参数失真的下界，分析了剪枝比例与协同推理性能之间的关系。接着，基于分析结果，联合优化剪枝比例、传输功率和计算频率，构建了一个最小化LAIM协同推理失真界限的问题。最后，提出了一种高效的算法来解决高度非凸问题。

Result: 实验结果表明，模型参数失真可以作为输出失真的可靠上限。同时，所提出的联合剪枝比例和资源管理设计在平衡推理性能、系统延迟和能耗方面优于基准方案，如完全在设备端或服务器端的推理。此外，分割点在异构和资源受限的边缘环境中对系统性能优化起着关键作用。

Conclusion: 本文提出了一种感知剪枝的LAIM协同推理方案，并通过分析和实验验证了其在平衡推理性能、系统延迟和能耗方面的优越性。此外，结果表明，剪枝后的模型参数失真可以作为输出失真的可靠上限，并且分割点在异构和资源受限的边缘环境中对系统性能优化起着关键作用。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [618] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

TL;DR: 本文提出了一种名为Birch SGD的新框架，用于分析和设计分布式SGD方法。通过将方法表示为加权有向树，我们得出了一个通用的理论结果，并设计了八种新方法。我们的研究提供了对异步和并行优化方法的统一基础。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式SGD方法在分析和设计上缺乏统一的框架，因此需要一种新的方法来统一这些方法，并提供更直观的基础。

Method: 我们提出了一个名为Birch SGD的新框架，用于分析和设计分布式SGD方法。该框架的核心思想是将每种方法表示为加权有向树，称为计算树。

Result: 我们设计了八种新方法，并分析了它们以及已知的方法，其中至少六种新方法被证明具有最优的计算时间复杂度。我们得出两个关键见解：(i) 所有方法共享相同的“迭代率”；(ii) 不同的方法表现出不同的权衡，例如一些方法更新迭代器更频繁，提高了实际性能，而其他方法则更通信高效或关注其他方面。

Conclusion: 我们的研究为理解、分析和设计高效的异步和并行优化方法提供了统一的基础。

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [619] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过符号延续和熵正则化轨迹实现稳定且凸的IB优化。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈（IB）方法在优化过程中经常遇到不稳定的问题，特别是在IB权衡参数beta的关键点附近会出现突然的表示变化。

Method: 本文引入了符号延续和熵正则化轨迹的方法，以实现稳定的IB优化。

Result: 通过分析证明了在包含熵正则化项时，IB解路径具有凸性和唯一性，并展示了这种方法如何在广泛的eta值范围内稳定表示学习。此外，还进行了围绕关键点（beta）的敏感性分析，并提供了统计上稳健的不确定性量化（95%置信区间）。

Conclusion: 本文提出了一种新的方法，通过符号延续和熵正则化轨迹实现稳定且凸的IB优化。实验结果和可重复性框架为实际部署和未来扩展提供了清晰的路径。

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [620] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: SDIFT是一种新的框架，可以从稀疏观测中生成完整的物理动力学演变，并在多个物理系统中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在预定时空分辨率的网格数据上运行，但难以处理现实世界物理动力学的稀疏观测和连续性质。

Method: SDIFT利用功能Tucker模型作为潜在空间表示器，并在功能Tucker空间中构建了一个具有时间增强UNet的顺序扩散模型，通过从高斯过程中抽取噪声来生成核心张量序列。

Result: SDIFT能够从不规则稀疏观测中生成完整的物理动力学演变，并在多个物理系统中表现出色。

Conclusion: SDIFT在三个物理系统上验证，展示了比最先进的方法在重建精度和计算效率上的显著改进。

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [621] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

TL;DR: 该研究提出了一种结合联邦学习和差分特征的方法，以解决隐私问题并提高模型的性能和泛化能力。实验结果表明，该方法在保持隐私的同时，实现了与集中式学习相当的性能，并且在所有评估数据集上都优于非差分方法。


<details>
  <summary>Details</summary>
Motivation: 现有的教育数据挖掘研究面临整合学校间的机密数据（如学术记录和学习日志）的挑战，因为隐私问题导致分析通常局限于单个学校的数据，这使得开发高性能和泛化能力强的模型变得困难。

Method: 该研究结合了联邦学习和差分特征的方法。联邦学习允许在不集中数据的情况下进行模型训练，从而保护学生隐私。差分特征利用相对值而不是绝对值，以提高模型性能和泛化能力。

Result: 实验结果表明，所提出的方法解决了隐私问题，并在Top-n精度、nDCG和PR-AUC方面达到了与集中式学习相当的性能。此外，使用差分特征在所有评估数据集上都提高了预测性能。训练的模型还适用于早期预测，在验证数据集中检测出早期处于风险中的学生时表现出色。

Conclusion: 该研究提出了一种结合联邦学习和差分特征的方法，以解决隐私问题并提高模型的性能和泛化能力。实验结果表明，该方法在保持隐私的同时，实现了与集中式学习相当的性能，并且在所有评估数据集上都优于非差分方法。

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [622] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法LACForest，用于通过森林进行带有增强类的学习。该方法利用增强Gini不纯度来改进决策树的分裂过程，并通过伪标记的增强实例分割森林以提高性能。此外，还开发了深度神经森林，以利用神经网络的表示能力。理论分析和实验结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，决策树和森林通常处理所有测试类别都出现在训练数据中的情况。然而，在本工作中，我们关注于通过森林进行带有增强类的学习，其中增强类可能在测试数据中出现但不在训练数据中。

Method: 我们引入了一种新的分裂准则，称为增强Gini不纯度，并开发了名为LACForest的方法，该方法基于增强Gini不纯度构建浅层森林，并通过伪标记的增强实例分割森林以提高性能。此外，我们还开发了深度神经森林，其优化目标基于我们的增强Gini不纯度。

Result: 我们提出了增强Gini不纯度的收敛分析，并进行了实验以验证我们方法的有效性。

Conclusion: 实验结果验证了我们方法的有效性。代码已发布在https://github.com/nju-xuf/LACForest/。

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [623] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

TL;DR: 该论文利用UFM框架来提供对神经多元回归的定性见解，并验证了多任务模型优于单任务模型以及白化和标准化回归目标可以提高训练性能的理论预测。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决两个关键问题：(1) 多任务模型与多个单任务模型在训练性能方面如何比较？(2) 白化和标准化回归目标是否能提高训练性能？

Method: 该论文利用UFM来提供对神经多元回归的定性见解。

Result: UFM理论预测，当对后者应用相同或更强的正则化时，多任务模型的训练MSE严格小于多个单任务模型，并且我们的实证结果证实了这些发现。关于白化和标准化回归目标，UFM理论预测它们会降低训练MSE，当目标维度的平均方差小于1时，我们的实证结果再次证实了这些发现。

Conclusion: 这些发现强调了UFM作为一个强大的框架，用于推导关于DNN设计和数据预处理策略的可操作见解。

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [624] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

TL;DR: 本文提出了一种多尺度结构-时间链接预测模型（MUST），用于解决UANET中由于高度动态和稀疏性带来的链接预测挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的链接预测方法主要关注单一结构尺度的时间动态性，而忽略了稀疏性的影响，导致信息捕捉不足且适用性有限。

Method: 提出了一种多尺度结构-时间链接预测模型（MUST），首先使用图注意力网络（GATs）捕获多个层次的结构特征，然后使用长短期记忆（LSTM）网络学习这些多尺度结构特征的时间动态性，并通过引入复杂的损失函数来解决稀疏性的影响。

Result: MUST在多个通过模拟生成的UANET数据集上验证了其性能，并且实验结果表明其在高度动态和稀疏的UANET中表现优异。

Conclusion: MUST在高度动态和稀疏的UANET中实现了最先进的链接预测性能。

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [625] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

TL;DR: GreenFactory是一种基于随机森林回归器的零成本代理集成方法，能够直接预测模型测试精度，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要训练和评估每个网络，耗时且资源密集。最近的代理通常缺乏跨不同场景的泛化能力，并且只提供相对排名而不是预测精度。

Method: 提出GreenFactory，这是一个基于随机森林回归器的零成本代理集成方法，旨在直接预测模型测试精度。

Result: GreenFactory在NATS-Bench数据集上取得了稳健的结果，特别是在NATS-Bench-SSS和NATS-Bench-TSS上，与实际性能有高度一致性。

Conclusion: GreenFactory在NATS-Bench数据集上表现出色，具有高Kendall相关性，表明其预测分数与实际性能之间有显著的一致性。

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [626] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

TL;DR: 本文提出了一种新的校准策略CleanSE，利用干净样本来提高部分标签学习中的消歧效果。


<details>
  <summary>Details</summary>
Motivation: 现有的消歧策略主要关注单个部分标签实例的特性，而忽视了数据集中随机存在的干净样本的强监督信息。

Method: 提出了一种新的校准策略称为CleanSE，该策略基于可微计数损失策略和K-近邻算法。

Result: 通过收集干净样本，可以提供指导并增强最可能候选者的置信度。

Conclusion: 实验表明，这种校准策略可以应用于大多数最先进的PLL方法，并提高其性能。

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [627] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: 本文提出了一种新的GNN量化框架MixQ-GNN，通过选择有效的整数位宽来提高效率，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: GNN的计算需求高，需要高效的加速方法。混合精度量化是一种有前景的解决方案，可以提高GNN架构的效率而不影响预测性能。

Method: 引入了一个定理，用于高效量化消息传递以聚合整数消息，并基于此提出了MixQ-GNN框架。

Result: MixQ-GNN在节点分类和图分类任务中分别实现了5.5倍和5.1倍的位操作减少。

Conclusion: MixQ-GNN能够通过选择有效的整数位宽来提高GNN的效率，同时保持预测性能。

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [628] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

TL;DR: 本文研究了可学习激活函数在Kolmogorov-Arnold网络（KANs）中的潜力，用于下肢假肢的个性化控制。结果表明，可学习激活函数在更复杂和大规模数据集中可能具有优势，而深度学习模型的混合训练数据表现与个体特定数据相当。


<details>
  <summary>Details</summary>
Motivation: 本文研究了可学习激活函数在Kolmogorov-Arnold网络（KANs）中的潜力，用于下肢假肢的个性化控制。同时，评估了用户特定与混合训练数据，以提高机器学习（ML）和深度学习（DL）在转弯意图预测中的性能。

Method: 收集了五名下肢截肢者在实验室环境中执行转弯任务的胫骨惯性测量单元（IMU）数据。评估了多层感知器（MLP）、Kolmogorov-Arnold网络（KAN）、卷积神经网络（CNN）和分数Kolmogorov-Arnold网络（FKAN）对即将发生的转弯的分类能力。比较了MLP和KAN（对于ML模型）以及FKAN和CNN（对于DL模型）以评估可学习激活函数的有效性。模型分别使用个体特定和混合数据进行训练，以评估训练数据对其性能的影响。

Result: KAN和FKAN中的可学习激活函数没有显著优于MLP和CNN。对于ML模型，使用个体特定数据训练的结果优于混合数据（p < 0.05）。相反，对于DL模型，个体特定和混合训练之间没有显著差异。

Conclusion: 这些发现表明，可学习的激活函数可能在涉及更复杂任务和更大数据量的数据集中表现出独特的优势。此外，对于深度学习模型，混合训练数据的表现与个体特定训练数据相当，这表明可以利用多个参与者的数据进行假肢控制模型的训练。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [629] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath is a framework that enhances the safety of LLM-based path planning by integrating conformal prediction, reducing planning uncertainty and collision rates significantly.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) show promise in autonomous driving but face issues of overconfidence and hallucinations, which raise safety concerns. SafePath aims to address these issues by providing formal safety guarantees for LLM-based path planning.

Method: SafePath is a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. It operates in three stages: generating diverse candidate paths, filtering out high-risk trajectories with a user-defined probability, and selecting the safest path or delegating control to a human based on uncertainty.

Result: SafePath reduces planning uncertainty by 77% and collision rates by up to 70% on datasets like nuScenes and Highway-env, proving its effectiveness in enhancing the safety of LLM-driven path planning.

Conclusion: SafePath effectively reduces planning uncertainty and collision rates, demonstrating its effectiveness in making LLM-driven path planning safer.

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [630] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: 本文提出了一种新的凸平滑代理损失构造方法，通过Fenchel-Young损失和下确界卷积实现了线性代理遗憾界，从而克服了凸平滑代理损失在遗憾转移中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的凸平滑代理损失在优化和估计方面具有良好的性质，但在进行遗憾转移后可能会恶化。本文旨在克服这一困境，为任意离散目标损失提供一种新的代理损失构造方法。

Method: 本文基于Fenchel-Young损失，利用卷积负熵生成代理损失，并通过求解目标贝叶斯风险的下确界卷积来实现线性代理遗憾界。

Result: 本文构造了一种凸平滑代理损失，该损失具有线性代理遗憾界，并且能够保持一致的类概率估计。

Conclusion: 本文展示了如何通过凸分析在风险最小化中渗透优化和统计效率，为任意离散目标损失提供了具有线性代理遗憾界的凸平滑代理损失。

Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap
between the convergence rates of surrogate and target losses, with linear
bounds favorable for their lossless regret transfer. While convex smooth
surrogate losses are appealing in particular due to the efficient estimation
and optimization, the existence of a trade-off between the smoothness and
linear regret bound has been believed in the community. That being said, the
better optimization and estimation properties of convex smooth surrogate losses
may inevitably deteriorate after undergoing the regret transfer onto a target
loss. We overcome this dilemma for arbitrary discrete target losses by
constructing a convex smooth surrogate loss, which entails a linear surrogate
regret bound composed with a tailored prediction link. The construction is
based on Fenchel-Young losses generated by the convolutional negentropy, which
are equivalent to the infimal convolution of a generalized negentropy and the
target Bayes risk. Consequently, the infimal convolution enables us to derive a
smooth loss while maintaining the surrogate regret bound linear. We
additionally benefit from the infimal convolution to have a consistent
estimator of the underlying class probability. Our results are overall a novel
demonstration of how convex analysis penetrates into optimization and
statistical efficiency in risk minimization.

</details>


### [631] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: VRRAEs combine the advantages of RRAEs and VAEs, leading to improved generative performance and reduced posterior collapse.


<details>
  <summary>Details</summary>
Motivation: The deterministic nature of RRAEs makes them less suitable for generative purposes, while VAEs are known for their generative abilities. The paper aims to combine the strengths of both models.

Method: VRRAEs combine the advantages of RRAEs and VAEs by carefully sampling the latent space of RRAEs and regularizing with the Kullback-Leibler (KL) divergence.

Result: VRRAEs outperform RRAEs and VAEs on many random generation and interpolation tasks based on the FID score, and show robustness against collapse on a synthetic dataset and real-world datasets like MNIST, CelebA, and CIFAR-10.

Conclusion: VRRAEs outperform RRAEs and VAEs, and are better generators than VAEs while reducing the possibility of posterior collapse.

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [632] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

TL;DR: AdaLin is a new approach for mitigating plasticity loss in deep neural networks by dynamically adapting each neuron's activation function.


<details>
  <summary>Details</summary>
Motivation: The loss of plasticity in deep neural networks is a key obstacle to learning in non-stationary problem settings, and recent work has shown that deep linear networks tend to be resilient towards loss of plasticity.

Method: AdaLin is a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss by equipping every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow.

Result: AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. It is also effective in more complex scenarios such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone and mitigating plasticity loss in off-policy reinforcement learning agents.

Conclusion: AdaLin can significantly improve performance on standard benchmarks and is effective in complex scenarios such as class-incremental learning and mitigating plasticity loss in off-policy reinforcement learning agents.

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [633] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

TL;DR: 本文研究了后训练方法如何修改语言模型的行为和表示，并提出了一个名为分层遗忘（LU）的算法，以提高对抗性重新学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 我们想要理解后训练方法如何修改语言模型的行为和表示，并特别关注这些修改的脆弱性，这使得它们容易通过提示工程或重新学习被绕过。

Method: 我们设计了一种称为分层遗忘（LU）的遗忘算法，该算法为数据的一个增长子集创建了不同的抑制机制。通过在第i个阶段遗忘前i个折叠而保留其余k-i个折叠，LU限制了在数据子集上重新学习恢复完整数据集的能力。

Result: 我们发现LU提高了对抗性重新学习的鲁棒性，适用于几种不同的遗忘方法。

Conclusion: 我们的结果为机器遗忘技术的最新进展做出了贡献，并提供了对后训练更新影响的见解。

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [634] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kahou,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: 该研究探讨了表格型ICL的公平性问题，并发现基于不确定性的演示选择策略可以有效提高群体公平性。


<details>
  <summary>Details</summary>
Motivation: 虽然传统机器学习模型中的偏见已被广泛记录，但这些偏见在表格型ICL中的表现仍不清楚。因此，本研究旨在探讨表格型ICL的公平性问题。

Method: 该研究调查了表格型ICL的公平性影响，并探索了三种预处理策略：相关性去除、组平衡演示选择和基于不确定性的演示选择。

Result: 综合实验表明，基于不确定性的演示选择策略能够持续提高群体公平性。

Conclusion: 该研究发现基于不确定性的演示选择策略可以持续提高上下文预测的群体公平性。

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [635] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

TL;DR: 本文研究了全连接前馈神经网络中梯度流的行为，证明了梯度流要么收敛到临界点，要么发散到无穷大。通过理论分析和实验验证，得出初始化良好的梯度流会发散到无穷大的结论。


<details>
  <summary>Details</summary>
Motivation: 研究全连接前馈神经网络的损失景观中的梯度流，以理解其收敛行为和优化过程。

Method: 我们利用o-极小结构的几何特性进行证明，并通过数值实验和现实场景验证了这些理论发现。

Result: 我们证明了梯度流要么收敛到临界点，要么发散到无穷大，同时损失收敛到渐近临界值。此外，我们证明了存在一个阈值ε>0，使得在最优水平以上最多ε初始化的梯度流会收敛到最优值。对于多项式目标函数和足够大的架构和数据集，我们证明了最优损失值为零，只能渐近实现。

Conclusion: 我们的主要结论是，任何初始化良好的梯度流都会发散到无穷大。

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [636] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

TL;DR: 本文提出了一种基于菱形平铺结构的新型聚类方法（RT）和一个基于RT的分层图聚类池化模型（RTPool），用于图分类任务，并在多个基准数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的消息传递框架过度依赖图的连接结构，限制了其捕捉几何图中固有丰富几何特征的能力。

Method: 提出了一种基于菱形平铺结构的新型聚类方法（Rhomboid Tiling, RT），并设计了基于RT聚类的分层图聚类池化模型（RTPool）用于图分类任务。

Result: 所提出的模型在所有7个基准数据集上优于21个最先进的竞争对手。

Conclusion: 所提出的模型在所有7个基准数据集上优于21个最先进的竞争对手，展示了其优越的性能。

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [637] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest 是一种专为流式条件设计的新方法，能够无缝跟踪随时间演变的数据生成过程。实验表明它在效率方面优于所有竞争对手，适用于需要快速识别异常的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的在线异常检测方法通常无法解决这些约束，往往依赖于定期重新训练以适应在线环境。

Method: Online-iForest 是一种专为流式条件设计的新方法，能够无缝跟踪随时间演变的数据生成过程。

Result: 在真实数据集上的实验验证表明，Online-iForest 与在线替代方案相当，并且与定期重新训练的最先进的离线异常检测技术相当。

Conclusion: Online-iForest 是一种有前途的解决方案，特别适用于需要快速识别异常的应用，如网络安全、欺诈和故障检测。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [638] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

TL;DR: This paper introduces ASF, a new defense mechanism against adversarial suffix attacks, which is effective, lightweight, and model-agnostic.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against adversarial suffix attacks are limited by their reliance on internal model architecture, high memory and computation footprints, or vulnerability to simple prompt engineering methods. There is a need for a more effective and efficient defense mechanism.

Method: Adversarial Suffix Filtering (ASF) is introduced as a lightweight, model-agnostic defensive pipeline that acts as an input preprocessor and sanitizer to detect and filter adversarially crafted suffixes in prompts.

Result: ASF effectively neutralizes malicious injections, providing strong defense in both black-box and white-box attack settings, with minimal impact on the model's performance in non-adversarial scenarios.

Conclusion: ASF provides comprehensive defense capabilities against adversarial suffix attacks, reducing attack efficacy to below 4% while minimally affecting the model's normal capabilities.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [639] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: 本文提出了一种无损失的ANN-SNN转换方法（LAS），用于全脉冲驱动的LLMs。LAS通过引入两种新型神经元和定制的脉冲等效Transformer组件，实现了无损失转换，并在多个模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的转换方法在处理基于ANN的LLMs的极端激活异常值和不兼容的非线性操作方面存在困难。因此，需要一种更有效的转换方法来实现无损失的转换。

Method: 提出了一种无损失的ANN-SNN转换方法，称为LAS。LAS引入了两种新型神经元来转换基于ANN的LLMs的激活异常值和非线性操作，并为基于脉冲的LLMs定制了等效于脉冲的Transformer组件，以确保完全脉冲转换而不会有任何性能损失。

Result: 实验结果表明，LAS在六种语言模型和两种视觉语言模型上实现了无损失转换。在OPT-66B上，LAS甚至在WSC任务中提高了2%的准确率。参数和消融研究进一步验证了LAS的有效性。

Conclusion: 实验结果表明，LAS实现了无损失转换，并在OPT-66B上提高了WSC任务的准确率2%。参数和消融研究进一步验证了LAS的有效性。

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [640] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: 本文提出了一种通用且可扩展的方法，以适应大语言模型在噪声和低精度模拟硬件上执行。该方法使最先进的模型在存在模拟噪声和量化约束的情况下，保持与4位权重、8位激活基线相当的性能。此外，模拟基础模型可以被量化以在低精度数字硬件上进行推理，并且在测试时计算扩展方面表现出更好的行为。


<details>
  <summary>Details</summary>
Motivation: 由于这些约束和不精确性，现成的大语言模型无法在基于模拟内存计算的硬件上实现4位级别的性能。之前的研究主要集中在小型、主要是基于视觉的模型上，但尚未存在适用于预训练了数万亿个标记的大语言模型的通用方法。

Method: 我们引入了一种通用且可扩展的方法，以稳健地适应大语言模型在噪声和低精度模拟硬件上执行。

Result: 我们的方法使最先进的模型（包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct）在存在模拟噪声和量化约束的情况下，保持与4位权重、8位激活基线相当的性能。此外，我们展示了作为训练方法的副产品，模拟基础模型可以被量化以在低精度数字硬件上进行推理。最后，我们展示了我们的模型在测试时计算扩展方面也受益，显示出比使用4位权重和8位静态输入量化的模型更好的扩展行为。

Conclusion: 我们的工作弥合了高容量大语言模型和高效模拟硬件之间的差距，为节能的基础模型提供了一条路径。

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [641] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文探讨了图删除是否会导致偏差的问题，并提出了一种公平的图删除方法FGU。FGU通过在分割的子图上训练碎片模型，并采用双层去偏过程来确保隐私和公平性。实验结果表明，FGU在保持隐私和准确性的同时实现了优越的公平性，并且对各种删除请求具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当用户信息从模型中删除时，不同敏感群体之间的预测分布往往会改变。此外，图模型容易放大偏差，使得研究图删除中的公平性尤为重要。这引发了一个问题：图删除是否会引入偏差？

Method: 为了保证隐私，FGU在分割的子图上训练碎片模型，从相应的子图中删除所需的数据，并在修改后的子图上重新训练碎片模型。为了确保公平性，FGU采用了一个双层去偏过程：首先通过在碎片模型重新训练中引入公平正则化来实现碎片级别的公平性，然后通过对齐所有碎片模型以最小化全局差异来实现全局级别的公平性。

Result: 我们的研究结果表明，删除后的模型的预测与敏感属性高度相关，证实了图删除过程中引入了偏差。

Conclusion: 我们的实验表明，FGU在保持隐私和准确性的同时实现了优越的公平性。此外，FGU对各种删除请求具有鲁棒性，确保了在不同数据分布下的公平性和实用性。

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [642] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在人工智能物联网场景中的能耗问题，提出了两种基于聚类的方法来加速模型训练的收敛，并在保持低能耗的同时实现了高收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有文献中常常忽视了联邦学习（FL）在人工智能物联网（AIoT）场景中的能源影响，而设备/客户端选择对于加速分布式AIoT环境中的模型训练收敛至关重要。

Method: 我们提出了两种基于聚类的方法，这些聚类解决方案旨在将具有相似标签分布的AIoT设备分组，从而形成由几乎异构设备组成的集群。

Result: 通过广泛的数值实验，我们证明了我们的聚类策略在与其他文献中可用的其他最新方法相比时，通常能实现高收敛率并保持低能耗。

Conclusion: 我们的方法在保持低能耗的同时，通常能实现较高的收敛速度，优于文献中其他最近的方法。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [643] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 本文研究了深度形态神经网络（DMNNs），提出了几种新的架构，并展示了形态层在加速大规模批量梯度下降中的作用。


<details>
  <summary>Details</summary>
Motivation: 我们研究了深度形态神经网络（DMNNs），并证明尽管它们具有内在的非线性，但层间的激活对于DMNNs是必要的。

Method: 我们提出了几种新的DMNN架构，每种架构都有不同的参数约束。第一（或第二）架构在多数参数（或可学习参数）应属于形态操作的约束下工作。

Result: 我们提出的网络可以成功训练，并且比线性网络更易于剪枝。据我们所知，我们是第一个在这些约束下成功训练DMNNs的团队，尽管我们的网络的泛化能力仍然有限。

Conclusion: 我们提出了一种结合线性和形态层的混合网络架构，实验证明形态层的引入显著加速了大规模批量梯度下降的收敛。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [644] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 本文探讨了如何验证算法是否学习到组合结构，并提出新网络架构以提升OOD性能，但即使有良好表现，也可能未能学到正确的组合特征。


<details>
  <summary>Details</summary>
Motivation: 为了确认算法是否真正从数据中学习到组合结构，仅在OOD设置上测试是不够的，还需要验证所识别的特征是否确实具有组合性。

Method: 通过探索两个具有明确OOD指标的任务，测试三种常用的神经网络（MLP、CNN和Transformer）的OOD性能，并开发两种新的网络架构以提高OOD表现。

Result: 三种常用神经网络在特定任务中无法解决OOD问题，而两种新提出的网络架构在OOD场景中表现成功。

Conclusion: 即使在正确的偏差和几乎完美的OOD性能下，算法仍可能无法学习到正确的特征以实现组合泛化。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [645] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习方法，通过自适应噪声清洗、基于条件GAN的合成数据生成和鲁棒训练来解决数据质量问题，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保持数据隐私的同时提供了有效的协作模型训练方案，但数据质量问题（如噪声标签、缺失类别和不平衡分布）显著影响其效果。因此，需要一种能够系统性解决这些问题的方法。

Method: 本文提出了一种系统性解决数据质量问题的联邦学习方法，包括自适应噪声清洗、基于条件GAN的协作合成数据生成和鲁棒的联邦模型训练。

Result: 在基准数据集（MNIST和Fashion-MNIST）上的实验评估表明，在不同噪声和类别不平衡条件下，联邦模型性能有显著提升，特别是在宏F1分数方面。

Conclusion: 本研究提出的方法有效缓解了常见的数据质量挑战，提供了一个稳健、可扩展且符合隐私要求的解决方案，适用于各种现实世界的联邦学习场景。

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [646] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: 本文提出了一种生成性端到端求解器，用于解决黑盒组合优化问题，强调样本效率和解决方案质量。该方法借鉴了基于退火的算法，通过训练神经网络来模拟相关的玻尔兹曼分布，从而学习能量景观的结构并促进全局优化。在查询昂贵或便宜的情况下，该方法都能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 当查询昂贵时，温度依赖的分布自然地实现了数据增强并提高了样本效率；当查询便宜但问题仍然困难时，模型学习隐式变量交互，有效地'打开'黑盒。

Method: 我们提出了一种生成性的端到端求解器，它借鉴了基于退火的算法，将黑盒目标视为能量函数，并训练神经网络来模拟相关的玻尔兹曼分布。通过条件温度，网络捕捉到了从高温下的近似均匀分布到低温下围绕全局最优值的尖锐分布的连续分布，从而学习了能量景观的结构并促进了全局优化。

Result: 我们在具有挑战性的组合任务上验证了我们的方法，在有限和无限查询预算下都表现出色，与最先进的黑盒优化器相比具有竞争力。

Conclusion: 我们的方法在有限和无限查询预算的具有挑战性的组合任务上验证了其性能，表现优于最先进的黑盒优化器。

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [647] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的多智能体强化学习框架，结合了社区结构、可迁移性和主动学习，并具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在处理动态网络和混合成员结构时存在局限性，因此需要一种更灵活和抽象的协调模式。

Method: 本文提出了一种基于社区的框架，其中每个智能体可以属于多个重叠的社区，每个社区维护共享的策略和价值函数，并通过个性化的成员权重进行聚合。此外，设计了利用这种结构的演员-评论家算法。

Result: 本文提出的框架支持迁移学习和主动学习，并在使用线性函数近似的情况下建立了演员和评论家更新的收敛保证。

Conclusion: 本文提出了一个新颖的多智能体强化学习（MARL）框架，该框架结合了社区结构、可迁移性和主动学习，并具有可证明的保证。

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [648] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [649] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.LG

TL;DR: 本文提出了一种分阶段的超参数优化方法，用于比较利用多阶段学习率调度和优化器参数分组的多任务自然语言模型变体，并在2021年Eberts和Ulges提出的联合实体和关系抽取模型变体上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过分阶段的超参数优化过程，比较不同多任务自然语言模型变体的性能，并利用多阶段学习率调度和优化器参数分组来提升模型效果。

Method: 本文应用了分阶段的超参数优化过程，比较了利用多阶段学习率调度和优化器参数分组的多任务自然语言模型变体。采用短时间的贝叶斯优化会话，利用多保真度、超参数空间剪枝、渐进式减半和一定程度的人类指导。使用Optuna TPE采样器和Hyperband剪枝器以及Scikit-Learn高斯过程最小化。首先使用高效的低保真冲刺来剪枝超参数空间。后续的冲刺逐步提高模型保真度，并使用Hyperband剪枝以提高效率。此外，还使用元学习者来调整阈值以在推理期间解决分类概率。

Result: 本文展示了所提出方法在2021年Eberts和Ulges提出的联合实体和关系抽取模型变体上的有效性。

Conclusion: 本文展示了所提出方法在2021年Eberts和Ulges提出的联合实体和关系抽取模型变体上的有效性。

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [650] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: 本文提出了一种高效的检查点压缩方案LMC，能够显著提高压缩性能并减少CPU资源消耗，从而允许更频繁的检查点操作。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型训练过程中需要频繁地将张量数据保存到持久化存储中，而每次检查点的数据量非常大，因此需要一种优化的检查点解决方案来减少数据量。

Method: 本文通过实验分析了检查点数据，利用字节分组和霍夫曼编码构建了LMC压缩方案，并评估了现有通用压缩引擎的效果。

Result: LMC在压缩性能上优于最佳替代方案（BZ2），并且压缩时间减少了数量级。16核并行实现的LMC可以达到2.78 GiB/s的压缩吞吐量和3.76 GiB/s的解压缩吞吐量。

Conclusion: 本文提出了一个高效的压缩解决方案，称为Language Model Compressor (LMC)，它能够显著提高压缩性能，并减少CPU资源的使用，从而允许更频繁的检查点操作。

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [651] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 本研究评估了机器学习算法在预测中风风险方面的有效性，并提出了改进基于机器学习的中风预测的策略。


<details>
  <summary>Details</summary>
Motivation: 中风仍然是全球最重要的健康挑战之一，排名为第二大死亡原因和第三大残疾原因。需要更可靠和可解释的模型来早期评估中风风险。

Method: 本研究探讨了机器学习算法在预测中风风险方面的有效性，使用了来自中风预测数据集的人口统计、临床和生活方式数据，并评估了多种模型，包括逻辑回归、随机森林和XGBoost。

Result: 虽然这些模型实现了高准确率，但灵敏度仍然是实际临床应用中的限制因素。此外，我们确定了最具影响力的预测特征，并提出了改进基于机器学习的中风预测的策略。

Conclusion: 这些发现有助于开发更可靠和可解释的模型，用于中风风险的早期评估。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [652] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数梯度下降和Bregman投影方法的LLM攻击技术，该技术在多个开源LLM上表现出更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用空间的约束和结构，开发一种有效的LLM攻击技术。

Method: 开发了一种基于指数梯度下降和Bregman投影方法的内在优化技术，以确保优化的one-hot编码始终位于概率单纯形内。

Result: 在四个公开数据集上的五个开源LLM上验证了该技术的有效性，结果表明其成功率和效率均优于其他三种最先进的攻击技术。

Conclusion: 该技术在提高LLM的攻击成功率和效率方面表现出色，优于其他三种最先进的攻击技术。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [653] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 本文研究了从平滑信号中学习Kronecker结构产品图的问题，并提出了一种交替优化方法，具有理论保证，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于多向数据的普及，对产品图的兴趣增加，但现有的图产品类型在建模多样依赖结构方面仍然有限。Kronecker乘积能够更复杂地建模依赖关系，但对图学习问题提出了更高的约束。

Method: 本文提出了一种交替方案来优化每个因子图，并提供了其渐近收敛的理论保证。此外，还修改了算法以学习强积的因子图。

Result: 在合成和真实世界图上进行了实验，并展示了所提出方法的有效性和优越性能。

Conclusion: 本文提出了一种从平滑信号中学习Kronecker结构产品图的方法，并通过实验验证了其有效性和优越性。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [654] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果机器学习、约束优化和生成AI的销售优化方法，并在LinkedIn上成功部署，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 销售过程的优化对于任何B2B业务的成功至关重要。然而，现有的系统在处理销售过程中的复杂性和动态性方面存在局限。

Method: 本文提出了一种包含三个层次的系统：1) 带有因果机器学习的预测层；2) 带有约束优化和上下文带攻击的优化层；3) 带有生成AI和反馈循环的部署层。

Result: 本文在LinkedIn上实现了该系统，并展示了其相对于传统系统的显著优势。

Conclusion: 本文提出了一个原则性的销售优化和商业AI方法，即因果预测优化与生成（CPOG），并在LinkedIn上进行了实现和部署，展示了显著的成果。

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [655] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 本文介绍了一种利用结构MRI图像和基因表达数据进行阿尔茨海默病检测的新方法，该方法通过异构二部图表示学习实现了高精度分类，并有望应用于其他疾病。


<details>
  <summary>Details</summary>
Motivation: 影像和基因组数据的整合可以揭示疾病复杂景观的新见解，但目前的方法在处理这些数据方面存在局限性。

Method: 我们提出了一种新的异构二部图表示学习方法，结合了结构MRI图像和基因表达数据。

Result: 我们的方法能够有效地将阿尔茨海默病（AD）分为三个不同的阶段，并识别出每个分类组中起重要作用的基因。

Conclusion: 我们的方法在小数据集上表现出色，并且可以扩展到其他疾病的基于放射基因组的分类。

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [656] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: 本文介绍了基于zentropy理论的数据驱动机器学习新方法，提出了一种名为ZENN的神经网络，用于有效处理异构数据集，并在多个任务中展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于熵的方法在量化数据中的不确定性和无序性以及开发人工智能算法方面一直是非常重要的工具。然而，各种领域数据的快速增长带来了新的挑战，特别是整合具有内在差异的异构数据集。

Method: 我们通过引入内在熵，将zentropy理论扩展到数据科学领域，并提出了一个zentropy增强型神经网络（ZENN），该网络同时学习能量和内在熵组件，以捕捉多源数据的潜在结构。我们重新设计了神经网络架构，以更好地反映不同数据集中的固有属性和变异性。

Result: 我们在分类任务和能量景观重建中证明了ZENN的有效性，显示了其优越的泛化能力和鲁棒性，尤其是在预测高阶导数方面。作为实际应用，我们使用ZENN重构了Fe3Pt的Helmholtz能量景观，并捕获了关键材料行为，包括负热膨胀和温度-压力空间中的临界点。

Conclusion: 我们的研究引入了一种基于zentropy理论的数据驱动机器学习新方法，突出了ZENN作为处理复杂异构数据集的科学问题的多功能且稳健的深度学习框架。

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [657] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: This paper introduces Chisme, a novel suite of protocols for robust intelligence at the network edge, addressing challenges such as heterogeneous data distributions, episodic connectivity, and lack of infrastructure. It includes synchronous DFL and asynchronous GL variants, along with a data similarity heuristic to improve personalized training while maintaining collaboration.


<details>
  <summary>Details</summary>
Motivation: Distributed learning at the network edge has emerged as a key enabling technology, but existing paradigms like federated learning (FL) and decentralized FL (DFL) face challenges in connectivity and synchronization in resource-constrained and infrastructure-less environments. Gossip learning (GL) algorithms are more robust but have generally been designed for homogeneous data distributions and may not suit all contexts.

Method: Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. A data similarity heuristic is introduced to allow agents to opportunistically infer affinity with each other using the existing communication of model updates in decentralized FL and GL.

Result: Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

Conclusion: Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [658] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型中的两种学习模式（IWL和ICL）之间的相互作用，并通过实验分析了环境稳定性和线索可靠性对它们平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解这两种学习模式之间的相互作用，我们借鉴了进化生物学中的类似适应策略。

Method: 我们从进化生物学中类似的适应策略中获得灵感，实验操作了可预测性的这些维度，并系统地研究了它们对变压器中ICL/IWL平衡的影响。

Result: 高环境稳定性明显有利于IWL，而高线索可靠性则增强了ICL的效果，特别是在稳定性较低时。此外，学习动态显示了任务相关的随时间演变：在某些情况下（例如，具有许多类别的分类），会发生典型的ICL到IWL转变，但在其他情况下（例如，较少类别或较慢的ICL获取）可能会出现初始IWL阶段，随后被ICL主导。

Conclusion: 这些发现支持了关于解释这些学习模式转换的相对成本假设，确立了可预测性作为控制变压器适应策略的关键因素，并提供了新的见解来理解ICL并指导训练方法。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [659] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文介绍了一种基于Transformer的统一归因方法，能够处理成员级数据、汇总级数据以及外部宏观因素的整合，并在LinkedIn上进行了大规模实施，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的归因是现代营销智能的基础，对于任何营销业务和广告平台都至关重要。然而，现有的方法在处理不同级别的数据和外部因素时存在局限性。

Method: 本文提出了一种基于Transformer的统一归因方法，能够处理成员级数据、汇总级数据以及外部宏观因素的整合。

Result: 本文在LinkedIn上展示了该方法的大规模实施效果，并分享了可广泛应用于营销和广告技术领域的学习和见解。

Conclusion: 本文介绍了基于Transformer的统一归因方法，并展示了其在LinkedIn的大规模实施效果，以及对营销和广告技术领域的广泛适用性。

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [660] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA是一种非参数变化检测算法，结合了KQT直方图和EWMA统计量，能够在控制误报率的同时实现高效的在线监测。


<details>
  <summary>Details</summary>
Motivation: 大多数非参数变化检测测试很少能事先控制ARL_0，而KQT-EWMA能够通过操作预定义的ARL_0来控制误报率。

Method: KQT-EWMA结合了Kernel-QuantTree (KQT)直方图和EWMA统计量，用于在线监控多变量数据流。

Result: 实验结果表明，KQT-EWMA可以在控制ARL_0的同时实现与现有先进方法相当或更低的检测延迟。

Conclusion: KQT-EWMA可以控制ARL_0，同时在检测延迟方面与最先进的方法相当或更优。

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [661] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO is a new pruning technique that reduces the computational and environmental costs of training large machine learning models while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and cost of large machine learning models have made them inaccessible to non-wealthy individuals and companies, and have led to higher prices for consumers. Current pruning methods are computationally and environmentally taxing.

Method: BINGO studies specific subsets of a neural network one at a time during the training pass to gauge how significant each weight plays in contributing to a network's accuracy. It generates a significance score for each weight by the end of training, allowing insignificant weights to be pruned in one shot.

Result: BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth.

Conclusion: BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [662] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 本文首次全面总结了多模态世界中的威胁景观，对文本、图像、视频和音频的对抗性攻击进行了综述。


<details>
  <summary>Details</summary>
Motivation: 由于多模态模型在现实世界应用中的普及，需要对威胁景观有清晰的认识以采取预防措施。然而，目前缺乏面向实践者的攻击类型概述。

Method: 本文对针对文本、图像、视频和音频四种模态的对抗性攻击进行了综述。

Result: 本文提供了对抗性攻击景观的视图，并展示了多模态对抗性威胁的演变。

Conclusion: 本文填补了多模态世界中对抗性威胁景观综述的空白，提供了对多模态对抗性威胁演化的见解。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [663] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: 这项研究比较了大型语言模型、人类和MAB算法的探索-利用策略，发现推理使大型语言模型更接近人类行为，但在复杂环境中仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 我们想了解大型语言模型是否表现出与人类相似的决策行为，并能否实现相当（或更好）的性能。

Method: 我们使用可解释的选择模型来捕捉代理的探索-利用策略，并研究通过提示策略和增强推理的模型如何影响大型语言模型的决策。

Result: 我们发现，推理使大型语言模型更接近人类的行为，这种行为由随机和定向探索的混合特征。在简单的静态任务中，推理启用的大型语言模型表现出与人类相似水平的随机和定向探索。然而，在更复杂的非静态环境中，大型语言模型难以匹配人类的适应性，特别是在有效的定向探索方面，尽管在某些情况下达到了相似的遗憾。

Conclusion: 我们的研究结果突显了大型语言模型在模拟人类行为和自动化决策方面的潜力和局限性，并指出了可能的改进领域。

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [664] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: 本研究提出了一种混合深度学习模型TCN-MLP-Attention，用于预测哈斯牛油果的价格波动，并取得了优异的预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着对健康食品需求的增长，农产品价格预测变得越来越重要。哈斯牛油果作为一种高价值作物，其价格波动复杂，受季节性、地区和天气等因素影响。传统预测模型在处理高度非线性和动态数据时往往表现不佳。

Method: 我们提出了一个混合深度学习模型，TCN-MLP-Attention架构，结合了时间卷积网络（TCN）用于序列特征提取，多层感知机（MLP）用于非线性交互，以及注意力机制用于动态特征加权。

Result: 实验结果表明，TCN-MLP-Attention模型具有出色的预测性能，RMSE为1.23，MSE为1.51，优于传统方法。

Conclusion: 本研究提供了一种可扩展且有效的时间序列预测方法，为农业市场的智能供应链管理和价格策略优化提供了有价值的见解。

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [665] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 本文研究了直接采样欧几里得扩散模型用于一般流形约束数据，并提出了两种新方法来减轻得分函数的奇异性和提高采样精度。


<details>
  <summary>Details</summary>
Motivation: 我们研究了直接采样欧几里得扩散模型用于一般流形约束数据，因为以前的工作没有显式利用特殊流形的结构。

Method: 我们提出了两种新方法：(1) Niso-DM，它沿法线方向引入非各向同性噪声以减少尺度差异，(2) Tango-DM，它仅使用仅切向的损失函数训练得分函数的切向分量。

Result: 数值实验表明，我们的方法在各种具有复杂几何结构的流形上的分布上表现优越。

Conclusion: 我们的方法在具有复杂几何结构的流形上的分布上实现了优越的性能。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [666] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文提出了一种新的持续学习框架RiCL，通过处理实时人类反馈中的噪声，提高了AI模型在动态环境中的学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习存在两个主要局限：使用流式实时人工标注数据进行动态模型更新，而不是静态数据集；以及假设标签是干净的，而现实中反馈往往存在噪声。

Method: RiCL框架利用大型语言模型（LLMs）从动态反馈中有效学习新技能，包含三个关键组件：时间一致性感知净化器、交互感知直接偏好优化策略和抗噪声对比学习模块。

Result: 在两个基准数据集（FewRel和TACRED）上进行的广泛实验表明，RiCL方法显著优于现有方法。

Conclusion: RiCL方法在两个基准数据集上表现出色，显著优于现有的在线持续学习和噪声标签学习方法的组合。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [667] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: 本研究利用大型语言模型分析高速公路事故数据，识别事故原因，并提供全面的解释。结果显示，大型语言模型能有效识别主要事故原因，如酒后驾驶、超速等，并结合事件数据提供更深入的见解。研究验证了模型的实际应用价值，并为交通安全措施的改进提供了潜在对策。


<details>
  <summary>Details</summary>
Motivation: 理解导致交通事故的因素并制定策略以减轻其严重性至关重要。传统的统计方法和机器学习模型往往难以捕捉各种因素之间的复杂相互作用以及每次事故的独特特征。因此，本研究旨在利用大型语言模型来分析高速公路事故数据并提供事故原因分析。

Method: 本研究利用大型语言模型（LLM）分析高速公路事故数据，并通过微调Llama3 8B模型进行事故原因分析。使用QLoRA对模型进行微调，以增强其对高速公路事故及其相关因素的理解。然后，该微调后的模型通过零样本分类识别事故原因，无需预标记数据，并提供全面的解释。

Result: 结果表明，大型语言模型能够有效地识别主要事故原因，如酒后驾驶、超速、攻击性驾驶和驾驶员分心。结合事件数据（如道路维护）可以提供更深入的见解。通过交通安全管理领域的研究人员问卷调查（88.89%的高一致性），验证了模型的实际适用性和改善交通安全措施的潜力。

Conclusion: 本研究强调了交通事故的复杂性，并展示了大型语言模型在全面分析事故原因和其他相关因素方面的潜力。此外，它为规划者和政策制定者提供了有价值的见解和潜在的应对措施，以制定更有效和高效的交通安全措施。

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [668] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [669] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为 TransPL 的新方法，用于时间序列的无监督域适应，通过代码转换矩阵建模源域的联合分布，并利用贝叶斯规则生成伪标签。TransPL 在多个基准测试中表现优异，并提供可解释的见解。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签策略无法捕捉时间序列数据中的时间模式和通道间偏移，导致次优的伪标签。因此，需要一种新的方法来解决这些限制。

Method: TransPL 通过代码转换矩阵对源域的联合分布 P(X, y) 进行建模，其中代码是从时间序列片段的向量量化 (VQ) 得到的。该方法从源域构建类和通道相关的代码转换矩阵，并使用贝叶斯规则进行目标域适应，基于通道加权的类条件似然生成伪标签。

Result: TransPL 在四个时间序列 UDA 基准测试中表现出色，优于最先进的伪标签方法，在准确率上提高了 6.1%，在 F1 分数上提高了 4.9%。

Conclusion: TransPL 的有效性已在四个时间序列 UDA 基准测试中得到验证，并且它在伪标签生成方面提供了可解释的见解。

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [670] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: FedRAG 是一种新的联邦强化学习框架，通过共享近似行为度量的状态投影函数，在保护隐私的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦强化学习方法通常共享加密的本地状态或策略信息，但无法有效保护敏感任务特定信息。

Method: 提出了一种基于近似行为度量的状态投影函数共享方法，通过在中央服务器聚合投影函数的参数来实现。

Result: 在 DeepMind Control Suite 上进行了广泛的实验，验证了 FedRAG 的有效性。

Conclusion: FedRAG 是一种有效的联邦强化学习框架，能够在保护敏感信息的同时提升性能。

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [671] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: 该研究提出了一种基于机器学习的心脏病预测框架，使用三种分类器进行评估，其中随机森林表现最佳。研究结果表明该模型在心脏病预测方面具有潜力，但需要更大、更多样化的数据集进行进一步验证。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在利用机器学习方法提高心脏病预测的准确性，从而辅助临床决策。

Method: 该研究采用机器学习框架，包括数据预处理、模型训练和评估，使用了三种分类器：逻辑回归、K-近邻（KNN）和随机森林。通过GridSearchCV和RandomizedSearchCV进行超参数调优以提高模型性能。

Result: 随机森林分类器表现最佳，准确率为91%，F1得分为0.89。评估指标显示各类别之间表现均衡。

Conclusion: 该模型展示了在临床决策中辅助预测心脏病的潜力，但数据集大小和泛化能力存在局限性，需要未来使用更大、更多样化的数据集进行研究。

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [672] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.LG

TL;DR: 本文提出了一种基于sybil的虚拟数据中毒攻击方法，通过生成sybil节点和基于梯度匹配的虚拟数据生成方法，有效降低了计算复杂度，并设计了三种目标模型获取方案，在非独立均匀分布的数据下表现出色。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到恶意对手的中毒攻击。现有的方法通常需要高昂的成本才能实现有效的攻击。

Method: 我们提出了一种基于sybil的虚拟数据中毒攻击，其中恶意客户端生成sybil节点以放大中毒模型的影响。为了降低神经网络计算复杂度，我们开发了一种基于梯度匹配的虚拟数据生成方法。我们还设计了三种针对目标模型获取的方案，适用于在线本地、在线全局和离线场景。

Result: 我们的方法在模拟中优于其他攻击算法，因为我们的方法可以在非独立均匀分布的数据下获得全局目标模型。

Conclusion: 我们的方法在模拟中优于其他攻击算法，因为我们的方法可以在非独立均匀分布的数据下获得全局目标模型。

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


### [673] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为AI2MMUM的可扩展、任务感知的人工智能-空气接口多模态通用模型，该模型能够根据细微的任务指令灵活有效地执行各种物理层任务，并在多个任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 设计一个6G导向的通用模型，能够处理多模态数据并执行多样化的空中接口任务，已成为未来无线系统的一个共同目标。

Method: 提出了一种可扩展、任务感知的人工智能-空气接口多模态通用模型（AI2MMUM），通过固定任务关键词和可学习的隐式前缀提示来增强任务适应性。LLM主干提供了强大的上下文理解和泛化能力，同时采用微调方法融入领域特定知识。轻量级任务特定头部直接输出任务目标。

Result: AI2MMUM在五个代表性物理环境/无线信道下游任务中实现了SOTA性能。

Conclusion: AI2MMUM在基于WAIR-D和DeepMIMO数据集的五个代表性物理环境/无线信道下游任务中实现了SOTA性能。

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [674] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [675] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 该研究提出了ImagineBench，一个用于评估利用LLM生成经验的离线RL算法的基准测试，并发现现有算法在新任务上的表现不佳，强调了需要改进算法以更好地利用这些生成经验。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准基准，该领域的发展受到阻碍。因此，作者提出了ImagineBench来填补这一空白。

Method: 该研究引入了ImagineBench，这是第一个全面的基准测试，用于评估利用真实经验与LLM生成经验的离线RL算法。

Result: 通过系统评估最先进的离线RL算法，发现简单地应用现有算法在未见过的任务上表现不佳，仅达到35.44%的成功率，而使用真实经验训练的方法则达到了64.37%的成功率。

Conclusion: 该研究强调了需要改进算法以更好地利用LLM生成的想象经验，并指出了未来研究的关键机会，包括更好地利用想象经验、快速在线适应和持续学习，以及扩展到多模态任务。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [676] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: 该研究提出了一种新的归一化方法，以提高量子-经典混合机器学习模型在抗癌药物反应预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 量子-经典混合机器学习模型在抗癌药物反应预测中表现出色，但由于数据编码问题容易出现稳定性问题，因此需要改进。

Method: 提出了一种基于修正梯度版本的$	anh$的归一化函数的新策略，以解决量子-经典混合机器学习模型对数据编码敏感的问题。

Result: 实验结果表明，当数据经过最优归一化时，量子-经典混合机器学习模型的表现优于传统深度学习模型。

Conclusion: 该研究为生物医学数据利用量子计算机分析开辟了新的可能性。

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [677] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，用于提高电路发现的完整性和忠实性，并揭示了三种逻辑门的基本属性及其在语言模型中的行为。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法无法保证完整性，导致电路在不同运行中不一致，并且可能遗漏关键机制。OR 门的存在是不完整性的根源，通常在标准电路发现方法中只能部分检测到。

Method: 本文系统地引入了三种逻辑门：AND、OR 和 ADDER 门，并将电路分解为这些逻辑门的组合。此外，还提出了一种结合噪声和去噪干预的框架，可以轻松集成到现有的电路发现方法中，而不会显著增加计算复杂度。

Result: 实验验证了框架在恢复电路的忠实性、完整性和稀疏性方面的能力。此外，通过该框架，揭示了三种逻辑门的基本属性，如它们的比例和对输出的贡献，并探索了它们在语言模型功能中的行为。

Conclusion: 本文提出了一种结合噪声和去噪干预的框架，能够完全识别逻辑门并在电路中区分它们。此外，通过该框架，我们揭示了三种逻辑门的基本属性，并探索了它们在语言模型功能中的行为。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [678] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: 本文提出了一种新的非示例持续图学习范式IPAL，通过原型对比学习（PCL）减少特征漂移，并结合拓扑集成高斯原型（TIGP）和实例-原型亲和力蒸馏（IPAD）来提高模型的可塑性和稳定性。在四个节点分类基准数据集上的评估表明，该方法优于现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNN) 面临灾难性遗忘的问题，这会削弱它们在吸收新信息时保留先前获得知识的能力。基于重放的技术通过回顾历史示例来缓解这一现象，但记忆爆炸和隐私侵犯限制了其效用。非示例方法通过原型重放（PR）避免了这些问题，但特征漂移带来了新的挑战。

Method: 我们提出了Instance-Prototype Affinity Learning (IPAL)，这是一种非示例持续图学习（NECGL）的新范式。我们利用图结构信息，制定了Topology-Integrated Gaussian Prototypes (TIGP)，指导特征分布向高影响节点发展，以增强模型吸收新知识的能力。Instance-Prototype Affinity Distillation (IPAD) 通过规范类关系中的不连续性来保护任务记忆。此外，我们在PCL中嵌入了一个Decision Boundary Perception (DBP)机制，促进更大的类间可区分性。

Result: 我们的实证结果表明，原型对比学习（PCL）比传统的PR表现出更小的漂移。基于PCL，我们提出了IPAL，这是一种新的非示例持续图学习范式。TIGP引导特征分布向高影响节点发展，以增强模型吸收新知识的能力。IPAD通过规范类关系中的不连续性来保护任务记忆。DBP机制增强了类间可区分性。在四个节点分类基准数据集上的评估显示，我们的方法优于现有最先进的方法。

Conclusion: 我们的方法在四个节点分类基准数据集上的评估表明，它优于现有的最先进方法，实现了更好的可塑性和稳定性之间的权衡。

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [679] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: 本文提出了一种结合梯度提升模型和可解释人工智能技术的欺诈检测框架，能够在保持高预测准确性的同时提高模型的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型往往优先考虑预测准确性，而牺牲了模型的透明度和可解释性。缺乏透明度使得组织难以遵守监管要求并获得利益相关者的信任。

Method: 我们提出了一个结合了众所周知的梯度提升模型（XGBoost、LightGBM 和 CatBoost）的堆叠集成的欺诈检测框架。此外，使用了可解释的人工智能（XAI）技术来增强模型决策的透明度和可解释性。SHAP（SHapley Additive Explanations）用于特征选择以确定最重要的特征。进一步的努力是使用 Local Interpretable Model-Agnostic Explanation (LIME)、Partial Dependence Plots (PDP) 和 Permutation Feature Importance (PFI) 来解释模型的预测。

Result: 该模型在 IEEE-CIS 欺诈检测数据集上实现了 99% 的准确率和 0.99 的 AUC-ROC 得分，优于几种最近的相关方法。

Conclusion: 这些结果表明，将高预测准确性与透明的可解释性相结合是可能的，并且可能在金融欺诈检测中带来更道德和值得信赖的解决方案。

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [680] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: 本文提出了一种自适应蒸馏方法，用于改进深度估计和场景分割的联合建模，通过动态调整知识量和避免知识遗忘，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度估计和场景分割是智能交通系统中的重要任务，联合建模可以减少存储和训练需求。现有的解决方案以静态方式传递多个教师的知识，但这种方法可能无法有效避免知识遗忘。

Method: 我们提出了一种自适应蒸馏方法，可以根据学生当前的学习能力动态调整从每个教师获取的知识量，并设计了一种基于轨迹的蒸馏损失来避免知识遗忘。

Result: 我们在Cityscapes和NYU-v2等多个基准数据集上评估了我们的方法，结果表明相比最先进的解决方案，我们的方法取得了明显改进。

Conclusion: 我们的方法在多个基准数据集上取得了明显改进，证明了其有效性。

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [681] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 本文提出了一种多模态时间序列预测框架ChronoSteer，通过结合大型语言模型和时间序列基础模型的优势，有效利用时间和文本信息进行未来推断。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法依赖于单模态时间序列数据，限制了其利用丰富文本信息的能力。最近，大型语言模型和时间序列基础模型分别在文本推理和时间建模方面表现出强大的能力。整合两者的优势以构建一个同时利用时间和文本信息进行未来推断的多模态模型已成为关键的研究挑战。

Method: 提出了一种解耦框架，使用大型语言模型（LLM）将文本事件转换为修订指令，然后用于引导时间序列基础模型（TSFM）的输出。引入了ChronoSteer，这是一种可以通过文本修订指令进行引导的多模态TSFM。还设计了一种基于合成数据的两阶段训练策略，并构建了一个高质量的多模态时间序列预测基准。

Result: ChronoSteer在仅使用合成数据训练的情况下，与单模态基线相比预测准确性提高了25.7%，比之前的最先进多模态方法提高了22.5%。

Conclusion: ChronoSteer在仅使用合成数据训练的情况下，与单模态基线相比预测准确性提高了25.7%，比之前的最先进多模态方法提高了22.5%。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [682] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: This paper introduces MiCo, a hierarchical language agent framework that uses LLMs to solve the Online Dynamic Multidimensional Bin Packing problem in cloud services. MiCo outperforms existing methods by achieving a high competitive ratio and maintaining performance under varying conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods struggle with real-time changes, heuristic approaches have rigid strategies, and learning-based methods lack generalizability and interpretability. The paper aims to address these limitations by introducing a novel framework for solving ODMBP in cloud services.

Method: The paper proposes a hierarchical language agent framework named MiCo, which formulates ODMBP as a Semi-Markov Decision Process with Options (SMDP-Option) and employs a two-stage architecture: Option Miner and Option Composer. Option Miner discovers non-context-aware strategies using LLMs, while Option Composer integrates these strategies with contextual ones.

Result: MiCo achieves a 96.9% competitive ratio in large-scale scenarios involving over 10,000 virtual machines. It maintains high performance under nonstationary request flows and diverse configurations, validating its effectiveness in complex cloud environments.

Conclusion: MiCo demonstrates effectiveness in complex and large-scale cloud environments by achieving a high competitive ratio and maintaining performance under nonstationary request flows and diverse configurations.

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [683] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 本研究提出了一种新的策略，通过在图变换器神经网络上同时训练稀疏多任务分子属性实验目标和从XGBoost模型生成的合成目标，以提高多任务分子属性预测中神经模型的性能。结果表明，这种合成任务增强方法是有效的，无需特征注入或预训练。


<details>
  <summary>Details</summary>
Motivation: 将基于规则的模型如随机森林注入可微分神经网络框架仍然是机器学习中的一个开放性挑战。尽管最近的进展表明预训练模型可以生成高效的分子嵌入，但这些方法通常需要大量的预训练和额外的技术来提高性能。

Method: 我们提出了一种新的策略，将单个图变换器神经网络同时在稀疏多任务分子属性实验目标和从XGBoost模型生成的合成目标上进行联合训练，这些合成任务作为独立的辅助任务。

Result: 我们的结果表明，在所有19个分子属性预测任务中，性能都有持续且显著的提升。在19个目标中的16个中，多任务图变换器优于XGBoost单任务学习者。

Conclusion: 我们的研究展示了合成任务增强是一种有效的方法，可以在不进行特征注入或预训练的情况下提高多任务分子属性预测中神经模型的性能。

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [684] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种提高联邦学习中本地模型适应性的方法，从而提升了全局模型的性能。


<details>
  <summary>Details</summary>
Motivation: 由于客户端上的数据分布异构性和联邦学习中的数据隐私，很难训练本地模型以实现表现良好的全局模型。

Method: 我们引入了本地模型的适应性，并通过提高本地模型的适应性来增强全局模型的性能。我们提供了具有良好适应性的本地模型的属性，并将其形式化为带有约束的本地训练目标，提出了一个可行的解决方案来训练本地模型。

Result: 在联邦学习基准上的广泛实验表明，我们的方法显著提高了本地模型的适应性，并实现了表现良好的全局模型，该模型始终优于基线方法。

Conclusion: 我们的方法显著提高了本地模型的适应性，并实现了表现良好的全局模型，该模型始终优于基线方法。

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [685] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习框架FedAPC，通过原型增强来提高在领域异质性下的全局模型泛化能力。实验结果表明，该框架在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许在分布式边缘设备上进行协作训练，同时确保数据隐私，是隐私敏感应用的流行解决方案。然而，由于统计异质性，特别是领域异质性，FL面临重大挑战，这阻碍了全局模式的收敛。

Method: 我们引入了FedAPC（联邦增强原型对比学习），这是一个基于原型的联邦学习框架，旨在提高特征多样性和模型鲁棒性。FedAPC利用增强数据的平均特征生成原型，以捕获更丰富的表示。通过将本地特征与全局原型对齐，使模型能够学习有意义的语义特征，同时减少对任何特定领域的过拟合。

Result: 在Office-10和Digits数据集上的实验结果表明，我们的框架优于最先进的基线，表现出优越的性能。

Conclusion: 实验结果表明，我们的框架在Office-10和Digits数据集上优于最先进的基线，表现出优越的性能。

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [686] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: 本文研究了多代理多臂老虎机中的最佳臂识别问题，并提出了两种新的算法Cl-BAI和BAI-Cl。这些算法通过聚类和最佳臂识别的结合，在样本复杂性和通信开销方面表现出色，并在某些情况下达到了最优的样本复杂性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多代理多臂老虎机中的最佳臂识别问题，其中代理被分组到不同的聚类中，每个聚类解决一个随机老虎机问题。目标是在最小化样本复杂性和通信开销的同时，为每个代理确定最佳臂。

Method: 本文提出了两种新的算法：Cl-BAI和BAI-Cl。Cl-BAI首先根据代理所学习的老虎机问题对代理进行聚类，然后为每个聚类识别最佳臂。BAI-Cl则先识别最佳臂，再根据这些臂对代理进行聚类。两种算法都利用了连续消除框架以确保计算效率和高准确性。

Result: 本文建立了两种方法的δ-PC保证，推导了它们的样本复杂性界限，并提供了该问题类的下界。当M是小常数时，BAI-Cl的一个变种在顺序意义上达到了最小最大最优性。实验结果表明，所提出的算法在样本和通信效率方面表现优越，特别是在M远小于N的情况下。

Conclusion: 本文提出了两种新的算法Cl-BAI和BAI-Cl，用于解决多代理多臂老虎机中的最佳臂识别问题。这两种算法在样本复杂性和通信开销方面表现出色，并且在某些情况下达到了最优的样本复杂性。

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [687] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: 本研究提出了QuXAI框架，该框架基于Q-MEDLEY，用于解释混合量子-经典机器学习（HQML）模型中的特征重要性。结果表明，Q-MEDLEY能够区分HQML模型中的经典影响因素和噪声，并在经典验证设置中与现有的XAI技术相竞争。此外，消融研究进一步揭示了Q-MEDLEY中复合结构的优点。本研究的成果对于提高HQML模型的可解释性和可靠性具有重要意义，有助于推动更安全和负责任的量子增强人工智能技术的应用。


<details>
  <summary>Details</summary>
Motivation: 当前，混合量子-经典机器学习（HQML）模型在计算智能方面展现出新的前景，但其基本复杂性经常导致黑箱行为，削弱了其应用的透明度和可靠性。尽管可解释人工智能（XAI）在量子系统中仍处于初级阶段，但在针对采用量化特征编码并随后进行经典学习的HQML架构的稳健全局和局部解释方法方面存在明显的研究空白。本研究旨在填补这一空白。

Method: 本研究提出了QuXAI框架，该框架基于Q-MEDLEY，用于解释混合量子-经典机器学习（HQML）模型中的特征重要性。Q-MEDLEY结合了基于特征的推断，保留了量子变换阶段，并可视化了结果属性。

Result: 实验结果表明，Q-MEDLEY能够区分HQML模型中的经典影响因素和噪声，并在经典验证设置中与现有的XAI技术相竞争。此外，消融研究进一步揭示了Q-MEDLEY中复合结构的优点。

Conclusion: 本研究提出了QuXAI框架，该框架基于Q-MEDLEY，用于解释混合量子-经典机器学习（HQML）模型中的特征重要性。结果表明，Q-MEDLEY能够区分HQML模型中的经典影响因素和噪声，并在经典验证设置中与现有的XAI技术相竞争。此外，消融研究进一步揭示了Q-MEDLEY中复合结构的优点。本研究的成果对于提高HQML模型的可解释性和可靠性具有重要意义，有助于推动更安全和负责任的量子增强人工智能技术的应用。

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [688] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: 本文提出了Alinear，一个超轻量级的预测模型，它使用仅k级参数就能实现具有竞争力的性能。实验表明，Alinear在多个基准数据集上优于大规模模型，同时使用不到1%的参数，并提出了一个新的参数感知评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模的快速扩展，时间序列预测面临关键挑战。从早期的Transformer到最近的TimesNet，性能提升往往伴随着参数数量的指数级增长。但这种扩展是否真的必要？为了质疑扩展定律在时间序列预测中的适用性，我们提出了Alinear。

Method: 我们提出了Alinear，一个超轻量级的预测模型，它使用仅k级参数就能实现具有竞争力的性能。我们引入了一种时间范围感知的自适应分解机制，以及一种渐进式频率衰减策略，以在各种预测时间范围内实现稳定的预测，而不会产生注意力机制的计算开销。

Result: 在七个基准数据集上的广泛实验表明，Alinear始终优于大规模模型，同时使用的参数不到它们的1%，并在短时间和超长预测时间范围内保持了强大的准确性。此外，为了更公平地评估模型效率，我们提出了一种新的参数感知评估指标，突出了ALinear在受限模型预算下的优越性。

Conclusion: 本文挑战了大型模型本质上更好的普遍信念，并建议向更高效的时间序列建模范式转变。

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [689] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: 本文研究了使用合成数据训练深度学习模型以检测半导体制造过程中的微小缺陷。结果表明，YOLOv8在检测较小缺陷方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于缺陷标注的质量数据缺乏良好的代表性，特别是较小的缺陷，这阻碍了基于深度学习的缺陷检测模型在制造生产线中的部署。

Method: 我们人工生成具有已知缺陷分布的线图案扫描电子显微镜（SEM）图像，并自主注释它们。然后，我们采用最先进的目标检测模型来研究缺陷检测性能作为缺陷尺寸的函数，比节距宽度小得多。

Result: 实时目标检测器YOLOv8的平均精度为96%，相比之下EfficientNet为83%，SSD为77%。在真实SEM数据上测试时，YOLOv8模型正确检测了84.6%的桥接缺陷和78.3%的断裂缺陷。

Conclusion: 这些有希望的结果表明，合成数据可以作为开发稳健机器学习模型的现实世界数据的替代品。

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [690] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer is a new attention mechanism that improves the integration of semantic and positional information in transformer models, leading to better performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Prior methods often model semantic and positional differences separately or apply uniform positional adjustments across heads, which can limit representational capacity. This paper aims to address these challenges by introducing a more flexible and expressive attention mechanism.

Method: ComplexFormer introduces Complex Multi-Head Attention (CMHA), which allows each head to independently model semantic and positional differences within the complex plane. It includes a per-head Euler transformation and a per-head adaptive differential rotation mechanism.

Result: Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show that ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.

Conclusion: ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [691] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: 本文介绍了一种基于声学和惯性信号融合的深度神经网络模型，用于提高对牛采食行为的识别精度。


<details>
  <summary>Details</summary>
Motivation: 监测牛的采食行为对于高效的牧场管理和资源利用至关重要。自动识别动物的进食活动可以改善饮食配方，并及早发现代谢问题和动物不适的症状。

Method: 本文提出了一种基于声学和惯性信号融合的深度神经网络模型，结合了卷积、循环和密集层。

Result: 特征级融合在F1分数上优于数据级和决策级融合，且提出的模型在F1分数上达到了0.802，比之前的方法提高了14%。

Conclusion: 该模型通过融合声学和惯性信号，提高了对牛采食行为的识别精度，并且在F1分数上比之前的方法提高了14%。

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [692] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 本文提出了一种新的跨领域知识迁移框架，以提高大型语言模型在时间序列预测中的性能。实验结果表明，该方法在预测准确性和泛化方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛采用，需要建立最佳实践，以超越传统的自然语言任务来利用其能力。时间序列预测在能源系统、金融和医疗保健等领域变得越来越重要。

Method: 提出了一种新颖的跨领域知识迁移框架，以增强大型语言模型在时间序列预测中的性能。该方法系统地将结构化的时间信息注入大型语言模型，以提高其预测准确性。

Result: 在真实世界的时间序列数据集上评估了所提出的方法，并将其与没有辅助信息的基线进行了比较。结果表明，知识指导的预测在预测准确性和泛化方面显著优于无信息基线。

Conclusion: 这些发现突显了知识迁移策略在弥合大型语言模型与特定领域预测任务之间的差距方面的潜力。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [693] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 本文通过构建一个玩具模型，研究了损失与模型大小的缩放关系，并发现表示超叠加是神经缩放定律的重要机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的成功依赖于一个观察结果，即更大的模型表现更好。然而，这种神经缩放定律——损失随着模型大小呈幂律下降——的起源仍然不清楚。

Method: 从两个经验原理出发——LLM表示比它们拥有的模型维度（宽度）更多的东西（即表示是叠加的），以及语言中的单词或概念出现的频率不同——我们构建了一个玩具模型来研究损失与模型大小的缩放关系。

Result: 当叠加较弱时，损失与模型大小的缩放取决于基础特征频率；如果特征频率遵循幂律，损失也是如此。相反，在强叠加下，损失与模型维度成反比，这在广泛的特征频率分布中都成立。

Conclusion: 表示超叠加是观察到的神经缩放定律的重要机制。我们预计这些见解将激发新的训练策略和模型架构，以更少的计算和更少的参数实现更好的性能。

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [694] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: This paper proposes SpecOffload, an inference engine that improves GPU utilization and throughput by embedding speculative decoding into offloading.


<details>
  <summary>Details</summary>
Motivation: Efficient LLM inference on resource-constrained devices faces challenges in compute and memory utilization due to limited GPU memory and substantial I/O overhead between CPU and GPU.

Method: SpecOffload embeds speculative decoding into offloading, unlocking latent GPU resources for storing and executing a draft model used for speculative decoding.

Result: SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.

Conclusion: SpecOffload improves GPU core utilization and boosts inference throughput compared to the best baseline.

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [695] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: ParScale 是一种新的模型缩放方法，通过增加并行计算来提高推理效率，并在低资源场景下具有潜力。


<details>
  <summary>Details</summary>
Motivation: 传统上认为扩展语言模型会带来显著的空间或时间成本，而 ParScale 提出了一种更高效的缩放方式。

Method: ParScale 通过在训练和推理过程中增加模型的并行计算来实现缩放，使用 P 种多样且可学习的变换对输入进行处理，并动态聚合 P 个输出。

Result: ParScale 可以使用更少的内存和延迟达到与参数扩展相当的性能提升，并且可以将现成的预训练模型转换为并行扩展模型。

Conclusion: ParScale 提供了一种新的缩放方法，可以提高模型的推理效率，并且在低资源场景下具有潜力。

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [696] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的电动公交车充电调度方法，通过分层结构和新的算法优化充电策略，以最小化充电成本。


<details>
  <summary>Details</summary>
Motivation: 解决长距离多阶段规划中的稀疏奖励挑战，同时优化电动公交车的充电调度以降低充电成本。

Method: 提出了一种分层深度强化学习（HDRL）方法，将原始MDP分解为一个高层半马尔可夫决策过程（SMDP）和多个低层MDP，并结合了分层双深度Q网络（HDDQN）和事后经验回放（HER）算法来解决不同时间分辨率下的决策问题。

Result: 实验结果表明，所提出的算法在实际数据中表现出良好的性能，能够有效降低充电成本并满足充电目标。

Conclusion: 通过将高阶策略和低阶策略叠加，构建的扁平策略在性能上与原始MDP的最优策略相当。

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [697] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于路由器的架构，利用领域资源和语言模型生成高质量的合成数据，以解决在缺乏真实用户数据的情况下微调大型语言模型进行函数调用任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 在数字内容创作工具中，用户通过自然语言查询表达需求，需要映射到API调用。由于缺乏真实世界任务特定数据和隐私限制，需要生成合成数据。现有方法在多样性和复杂性方面不足，无法复制真实数据分布，导致微调后的性能不佳。

Method: 提出了一种基于路由器的架构，利用领域资源（如内容元数据和结构化知识图谱）以及文本到文本和视觉到文本的语言模型生成高质量的合成训练数据。

Result: 在一组真实用户查询上的评估显示，函数分类准确率和API参数选择有显著提高。

Conclusion: 模型在合成数据上微调后表现优于传统方法，为函数调用任务建立了新的基准。

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [698] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据重建攻击方法，克服了现有方法的局限性，并在图像和表格数据集上进行了广泛的实验，证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据重建攻击方法存在重要限制，例如依赖于对客户端数据分布的假设，或者当批次大小超过几十个样本时效率显著下降。因此，需要一种更有效的方法来解决这些问题。

Method: 本文引入了一种新的数据重建攻击方法，利用全连接层的新几何视角来设计恶意模型参数，从而实现任意大的数据批次的完美恢复。

Result: 实验结果表明，本文提出的攻击方法优于现有方法，并且能够实现比现有技术大两个数量级的数据批次的完美重建。

Conclusion: 本文提出了一种新的数据重建攻击方法，克服了现有方法的局限性，并在图像和表格数据集上进行了广泛的实验，证明了其优越性。

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [699] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV is a method that accelerates vision-language models by transforming small language models into effective multimodal drafters through a two-phase approach, achieving significant speedups in inference.


<details>
  <summary>Details</summary>
Motivation: Applying speculative decoding to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context.

Method: MASSV transforms existing small language models into effective multimodal drafters through a two-phase approach. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.

Result: Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

Conclusion: MASSV provides a scalable, architecture-compatible method for accelerating both current and future vision-language models (VLMs).

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [700] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: 本文提出了一种深度学习模型，用于在欧洲进行8小时范围内的高分辨率概率降水预测，该模型有效地整合了多种数据源，并通过一致的概率图实现准确的预测和稳健的不确定性量化。实验表明，该模型超越了现有系统，设定了新的标准。


<details>
  <summary>Details</summary>
Motivation: 克服仅使用雷达的深度学习模型在短预报提前时间上的局限性。

Method: 我们提出了一种深度学习模型，用于在欧洲进行8小时范围内的高分辨率概率降水预测，该模型有效地整合了多种数据源（包括雷达、卫星和基于物理的数值天气预测），同时捕捉长距离交互，通过一致的概率图实现准确的预测和稳健的不确定性量化。

Result: 广泛的实验表明，我们的模型超越了当前的操作性NWP系统、基于外推的方法和深度学习的实时预报模型。

Conclusion: 我们的模型在欧洲高分辨率降水预报中设定了新的标准，确保了准确性、可解释性和计算效率之间的平衡。

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [701] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: 本文研究了基于精确尖峰时间的学习规则，并将其与噪声梯度下降和噪声镜面下降联系起来，证明了该规则可以识别出活动最高的前突触神经元。


<details>
  <summary>Details</summary>
Motivation: 了解基于精确尖峰时间的学习规则，而不是基于神经元放电率的已知规则。

Method: 通过将Hebbian的尖峰时间依赖可塑性规则与概率单纯形上的自然损失函数的噪声梯度下降联系起来，证明了这一点。

Result: 该学习规则最终能够识别出活动最高的前突触神经元，并且与噪声镜面下降有内在联系。

Conclusion: 该学习规则最终能够识别出活动最高的前突触神经元。

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [702] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文提出了一种新的分层深度强化学习算法DAC-MAPPO-E，用于优化电动公交车队的充电计划，并通过实验验证了其优越性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于旅行时间、能耗和电价的不确定性，优化电动公交车（EBs）的充电计划仍然是一个关键挑战。此外，为了应对现实世界的复杂性，充电政策必须在多个时间尺度上高效决策，并且对于大规模的EB车队具有可扩展性。

Method: 本文提出了一种分层深度强化学习（HDRL）方法，将原始马尔可夫决策过程（MDP）重新表述为两个增强的MDP，并引入了一种新的HDRL算法，即双演员-评论家多智能体近端策略优化增强（DAC-MAPPO-E）。

Result: 通过使用真实世界的数据进行广泛实验，证明了DAC-MAPPO-E在优化电动公交车队充电计划方面的优越性能和可扩展性。

Conclusion: 实验结果表明，DAC-MAPPO-E在优化电动公交车队的充电计划方面表现出优越的性能和可扩展性。

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [703] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: 本文提出了一种名为FeRA的新型联邦代表注意力防御机制，用于检测和抵御后门攻击，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 由于边缘设备的异构性导致数据分布不一致，使得检测后门攻击变得更加困难，因此需要一种新的防御机制。

Method: FeRA利用跨客户端注意力机制，通过内部特征表示来区分良性客户端和恶意客户端，并基于表示重构误差计算异常分数。

Result: FeRA在各种联邦学习场景中表现出鲁棒性，能够有效降低后门攻击的成功率，同时保持主任务的高准确率。

Conclusion: FeRA是一种有效的防御机制，能够在各种联邦学习场景中保持高准确率并有效降低后门攻击的成功率。

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [704] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于负度量学习的图对比学习方法NML-GCL，通过构建负度量空间和联合训练方案来解决假负问题，实验结果表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的解决假负问题的方法通常依赖于人工先验知识，导致GCL的结果仍不理想。

Method: NML-GCL采用可学习的负度量网络（NMN）构建负度量空间，以更好地区分假负样本和真负样本，并通过双层优化目标的联合训练方案来优化编码器和负度量网络。

Result: NML-GCL在下游任务中的性能得到了提升，且理论分析和实验验证了该方法的有效性。

Conclusion: NML-GCL在广泛使用的基准测试中通过实验证明了其优越性。

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [705] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: 本文提出了一种改进的异步去中心化随机梯度下降算法（ADSGD），并在实际假设下分析了其收敛性。实验结果表明，ADSGD 在各种场景下的墙钟收敛时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在利用分布式数据的同时避免中央控制，提高可扩展性和隐私性。然而，实际部署面临由于计算速度异构和不可预测的通信延迟带来的挑战。

Method: 本文引入了一个改进的异步去中心化随机梯度下降（ADSGD）模型，在实际假设下分析了其收敛性，并通过实验验证了其性能。

Result: 实验结果表明，ADSGD 在各种场景下的墙钟收敛时间上优于现有方法。

Conclusion: ADSGD 是一种适用于实际去中心化学习任务的算法，具有简单性、内存和通信效率以及对通信和计算延迟的鲁棒性。

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [706] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: ALERT是一种用于检测特征分布变化并触发模型重新训练的方法，在无线网络的两个用例中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，特征分布的变化可能会降低AI模型的性能并导致不良行为。为了应对未被检测到的模型退化，我们提出了ALERT。

Method: ALERT包括三个组件：表示学习、统计测试和效用评估。表示学习使用MLP，统计测试使用Kolmogorov-Smirnov和Population Stability Index测试，效用评估使用新函数。

Result: ALERT在两个无线网络用例（无线指纹识别和链路异常检测）中表现出优于文献中十种标准漂移检测方法的优势。

Conclusion: ALERT是一种有效检测特征分布变化并触发模型重新训练的方法，在无线网络的两个用例中表现良好。

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [707] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: 本文探讨了深度强化学习在动态环境中的适应问题，并提出了两种关键能力：优先探索和采样策略，以及通过结构化表示选择性地保留先前知识。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的自主决策系统必须在不断变化的环境中运行。虽然深度强化学习在静态环境中表现出色，但大多数方法数据密集且假设世界在训练和测试时间之间不会改变。因此，当条件发生变化时，传统RL方法难以适应。这带来了根本性的挑战：如何在不灾难性遗忘有用先验知识的情况下，让RL代理在部署过程中遇到新环境变化时高效适应？

Method: 本文讨论了深度强化学习（RL）在非静态环境中的适应问题，并提出了两种关键能力：优先探索和采样策略，以及通过结构化表示选择性地保留先前知识。

Result: 本文展示了高效的在线适应需要两个关键能力：(1) 优先探索和采样策略，有助于识别和从相关经验中学习，以及 (2) 通过结构化表示选择性地保留先前知识，这些表示可以在不干扰可重用组件的情况下进行更新。

Conclusion: 本文展示了高效的在线适应需要两个关键能力：(1) 优先探索和采样策略，有助于识别和从相关经验中学习，以及 (2) 通过结构化表示选择性地保留先前知识，这些表示可以在不干扰可重用组件的情况下进行更新。

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [708] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 本文研究了随机分类器集合中集体行为的出现，并发现最优温度参数具有普遍性，不受教师分类器和分类器数量的影响。实验表明该现象在高质量数据集中具有重要性，并通过物理类比揭示了其自组织特性。


<details>
  <summary>Details</summary>
Motivation: 随机性在数据科学和机器学习中无处不在，但由随机组件组成的系统往往表现出宏观的确定性行为。本文旨在研究这种从微观无序到宏观有序的转变机制。

Method: 本文引入了一种理论模型，通过将分类损失作为能量定义吉布斯测度，研究随机分类器集合中集体行为的出现。通过数学证明和数值实验验证了最优温度参数的普遍性。

Result: 本文证明了在样本由高斯分布生成且标签由教师感知机构造的情况下，最优温度参数不依赖于教师分类器或随机分类器的数量，展示了该行为的普遍性。实验在MNIST数据集上验证了这一现象的重要性。

Conclusion: 本文提出了一种理论模型来研究随机分类器集合中集体行为的出现，并发现最优温度参数具有普遍性，不受教师分类器和随机分类器数量的影响。实验表明该现象在高质量数据集中具有重要性，并通过物理类比揭示了其自组织特性。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [709] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: 本文介绍了离散变分自编码器的理论基础和实现方法，适用于不同数据模态的应用。


<details>
  <summary>Details</summary>
Motivation: 近年来，离散潜在空间在许多数据模态中变得越来越受欢迎，因此需要一个系统性的介绍来理解其原理和应用。

Method: 本文通过从头开始推导每个步骤，介绍了离散变分自编码器的理论基础，并提供了一个具体的训练方案和示例实现。

Result: 本文为离散变分自编码器提供了严谨但实用的介绍，并提供了一个示例实现以供参考。

Conclusion: 本文提供了对离散变分自编码器的深入介绍，并展示了其在不同数据模态中的应用潜力。

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [710] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: 本文评估了专门的多任务优化器（SMTOs）在多任务学习中的表现，并发现固定权重可以实现与SMTOs相当的性能。


<details>
  <summary>Details</summary>
Motivation: 为了澄清SMTOs在多任务学习中的行为，以及为什么统一损失函数在某些情况下可以与SMTOs相媲美。

Method: 通过广泛的实证评估对SMTOs进行评估，包括最新的方法，在更复杂的多任务问题上验证这些主张。

Result: SMTOs在多任务学习中表现良好，固定权重可以实现与SMTOs相当的性能，并且在某些情况下，统一损失函数的表现与SMTOs相似。

Conclusion: SMTOs在多任务学习中表现良好，固定权重可以与SMTOs竞争性地表现，并且在某些情况下，统一损失函数的表现与SMTOs相似。

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [711] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 本文提出了一种名为FactsR的方法，用于在医疗咨询过程中实时提取关键临床信息，并递归地生成最终的病历记录。该方法提高了病历的准确性和简洁性，并使临床医生参与病历生成过程，同时为实时决策支持开辟了新的用例。


<details>
  <summary>Details</summary>
Motivation: 现有的AI病历记录解决方案依赖于一次或少量提示来生成病历，缺乏足够的推理能力，导致病历冗长且容易出现幻觉和误述临床医生的意图，需要临床医生进行校对，这可能因工作量和疲劳而影响患者安全。

Method: 本文提出了一种名为FactsR的方法，用于在医疗咨询过程中实时提取关键临床信息，并利用这些信息递归地生成最终的病历记录。

Result: FactsR方法通过让临床医生参与病历生成过程，提高了病历的准确性和简洁性，并为实时决策支持开辟了新的用例。

Conclusion: FactsR方法是一种有效的解决方案，可以提高医疗病历的质量和安全性，同时为临床医生提供更好的支持。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [712] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: SCGP是一种基于群论的增强方法，通过Schreier-coset嵌入丰富节点特征，改善长距离信息传递，同时保持计算效率。在标准基准测试中表现优异，尤其适合处理层次化和模块化图结构。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案如图重布线和抗瓶颈架构（如Cayley图和expander图）虽然避免了过压缩问题，但引入了可扩展性瓶颈。Cayley图在SL(2,Z_n)上构建时表现出强大的理论性质，但由于节点增长为O(n^3)，导致高内存使用。

Method: SCGP是一种基于群论的增强方法，通过Schreier-coset嵌入丰富节点特征，而无需改变输入图的拓扑结构。

Result: SCGP在标准节点和图分类基准上的实证评估表明，其性能与expander图和重布线GNN基线相当或更优。此外，SCGP在处理层次化和模块化图结构时具有优势。

Conclusion: SCGP在处理层次化和模块化图结构时表现出色，具有较低的推理延迟、改进的可扩展性和低内存占用，适用于实时和资源受限的应用。

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [713] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文提出了一种名为PIF的新异常检测方法，结合了自适应隔离方法和偏好嵌入的优点。通过在高维空间中使用PI-Forest计算异常分数，实验表明PIF优于现有的异常检测技术。


<details>
  <summary>Details</summary>
Motivation: To address the problem of detecting anomalies with respect to structured patterns.

Method: PIF, which combines adaptive isolation methods with preference embedding, and employs PI-Forest in a high-dimensional space to compute an anomaly score.

Result: Experiments on synthetic and real datasets demonstrate that PIF favorably compares with state-of-the-art anomaly detection techniques.

Conclusion: PI-Forest is better at measuring arbitrary distances and isolating points in the preference space.

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [714] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG是一种基于两阶段变分自编码器的IA网格生成器，能够生成具有特定临床相关形态测量值的IA形状，这对于理解形状变化和流体动力学影响很有用。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大型IA图像数据集，需要一个生成模型来训练网络以实时预测血液流动力，这是影响疾病进展的关键因素。现有的形状生成方法难以捕捉真实的IA特征，并忽略了IA袋与父血管之间的关系，限制了生理现实性和生成控制。

Method: AneuG是一种基于两阶段变分自编码器（VAE）的IA网格生成器。第一阶段生成低维图谐波变形（GHD）标记以编码和重建动脉瘤袋形状，受形态能统计真相的约束。第二阶段根据GHD标记生成父血管，通过生成血管中心线并传播横截面。

Result: AneuG能够生成具有特定临床相关形态测量值的IA形状，这有助于研究形状变化和流体动力学影响。源代码和实现细节可在https://github.com/anonymousaneug/AneuG上获得。

Conclusion: AneuG可以生成具有特定临床相关形态测量值的IA形状，这对于理解形状变化和流体动力学影响很有用。

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [715] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL是一种基于NAS的增量学习框架，通过动态调整模型结构和选择性扩展，在保持模型大小的同时提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于NAS的增量学习方法通常在每个任务中扩展模型，这在资源受限的环境中不切实际。因此，需要一种更有效的框架来平衡模型的可塑性和稳定性。

Method: SEAL是一种基于NAS的框架，专门用于数据增量学习，通过动态调整模型结构并在必要时进行扩展，并通过交叉蒸馏训练保持稳定性。NAS组件共同搜索架构和最佳扩展策略。

Result: 在多个基准测试中，SEAL有效减少了遗忘并提高了准确性，同时保持了比先前方法更低的模型大小。

Conclusion: SEAL展示了结合NAS和选择性扩展在增量场景中高效、自适应学习的潜力。

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [716] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: 本文通过分析在线辅导环境中的归纳人类学习模拟，探讨了人类学习是否可能通过结合多个专门的学习机制实现更高的数据效率。研究发现，分解学习为多个不同机制能显著提高数据效率，使其接近人类学习水平。


<details>
  <summary>Details</summary>
Motivation: 人类学习依赖于专业化——不同的认知机制协同工作以实现快速学习。相比之下，大多数现代神经网络依赖于单一机制：目标函数上的梯度下降。这引发了问题：人类学习者从仅几十个例子而不是数十万个数据进行快速学习是否可能源于我们能够结合使用多个专门的学习机制？

Method: 我们通过在线辅导环境中的归纳人类学习模拟的消融分析来研究这个问题。比较了强化学习与更高效的数据3机制符号规则归纳方法。

Result: 我们发现将学习分解为多个不同的机制可以显著提高数据效率，使其与人类学习相匹配。此外，我们证明这种分解对效率的影响大于符号和非符号学习之间的区别。

Conclusion: 我们的研究结果表明，整合多个专门的学习机制可能是弥合数据驱动机器学习与人类学习之间差距的关键。

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [717] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: 本文研究了参数模型优化与随机特征组合优化之间的关系，揭示了神经网络中分布无关学习的局限性，并引入了新的理论框架平均概率维数复杂度（adc）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨神经网络中分布无关学习的局限性，并为实际应用中数据分布假设的必要性提供理论支持。

Method: 我们通过分析随机特征的线性组合优化与基于梯度的参数模型优化之间的关系，提出了一个新的理论框架——平均概率维数复杂度（adc）。

Result: 结果表明，如果参数模型可以使用小批量随机梯度下降（bSGD）进行学习而不依赖数据分布假设，则目标函数可以用多项式大小的随机特征组合近似。

Conclusion: 我们的研究揭示了神经网络通过梯度下降训练时分布无关学习的根本限制，并强调了在实践中做出数据分布假设的重要性。

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [718] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: L2T is a framework for large language models that improves reasoning effectiveness and efficiency by optimizing the use of each episode in a hierarchical session.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the trade-off between reasoning effectiveness and computational efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens.

Method: L2T is an information-theoretic reinforcement fine-tuning framework that treats each query-response interaction as a hierarchical session of multiple episodes. It proposes a universal dense process reward, which quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. A method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix is also proposed.

Result: Theoretical analyses show that L2T significantly reduces computational complexity with high estimation accuracy. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.

Conclusion: L2T demonstrates advantages across different tasks, boosting both reasoning effectiveness and efficiency.

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [719] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 本文探讨了基于评分的扩散模型在现在预报云和降水中的应用。实验表明，扩散模型能够不仅平流现有的云，还能生成和消散云，包括对流的开始。最佳的扩散模型是CorrDiff方法，在根均方误差上优于其他模型和传统方法。扩散模型还能够进行开箱即用的集合生成，显示出良好的校准。


<details>
  <summary>Details</summary>
Motivation: 传统的数值天气预报在模拟云和降水时具有挑战性，因为需要子网格参数化。机器学习已被用于预测云和降水，但早期的机器学习方法通常会产生模糊的预测。本文旨在探索一种较新的方法，即基于评分的扩散模型，用于现在预报云和降水。

Method: 本文探讨了一种较新的方法，名为基于评分的扩散模型，用于现在预报（零到三小时的预报）云和降水。我们讨论了基于评分的扩散模型的背景和直觉，从而为社区提供了一个起点，同时探索了该方法在现在预报静止轨道红外图像中的应用。我们测试了三种主要类型的扩散模型：标准的基于评分的扩散模型（Diff）；残差校正扩散模型（CorrDiff）；以及潜在扩散模型（LDM）。

Result: 扩散模型能够不仅平流现有的云，还能生成和消散云，包括对流的开始。这些结果令人惊讶，因为预测仅以过去20分钟的红外卫星图像为起点。最佳的扩散模型是CorrDiff方法，在根均方误差上比所有其他扩散模型、传统的U-Net和一个持续性预测好一到两个开尔文。扩散模型还能够进行开箱即用的集合生成，这显示出技能良好的校准，集合的范围与误差有很好的相关性。

Conclusion: 扩散模型能够不仅平流现有的云，还能生成和消散云，包括对流的开始。这些结果令人惊讶，因为预测仅以过去20分钟的红外卫星图像为起点。最佳的扩散模型是CorrDiff方法，在根均方误差上比所有其他扩散模型、传统的U-Net和一个持续性预测好一到两个开尔文。扩散模型还能够进行开箱即用的集合生成，这显示出技能良好的校准，集合的范围与误差有很好的相关性。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [720] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: 本文讨论了传统实验方法在推导组件级和局部线性参数变化模型时的局限性，并通过利用从标准发动机操作中收集的数据进行识别技术来解决这些问题。结果表明，基于Koopman的控制器在参考跟踪和扰动抑制方面优于其他基准控制器。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在推导组件级和局部线性参数变化模型时存在局限性，而物理模型的推导需要性能特性，这些特性并不总是可用的。

Method: 使用从标准发动机操作中收集的数据进行识别技术，估计转子动力学，并将自主动力学映射到最优构造的Koopman特征函数空间中。随后，通过元启发式算法进行特征值优化和时间投影，然后进行基于梯度的特征函数识别。

Result: 基于Koopman的控制器在参考跟踪和扰动抑制方面优于其他基准控制器，特别是在海平面和不同飞行条件下。

Conclusion: 该研究展示了基于Koopman的控制器在参考跟踪和扰动抑制方面优于其他基准控制器，因为它具有全局性质。

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [721] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.LG

TL;DR: 本研究评估了大型语言模型在生成癌症相关信息方面的性能，发现通用LLMs在语言质量和效果上表现更好，而医学LLMs在沟通可及性方面更优，但存在更高的潜在危害和偏见。研究强调了领域专业知识与安全性之间的双重性，并提出了改进AI生成健康内容的建议。


<details>
  <summary>Details</summary>
Motivation: 有效沟通关于乳腺癌和宫颈癌的问题仍然是一个持续的健康挑战，公众对癌症预防、筛查和治疗的理解存在显著差距，可能导致诊断延迟和治疗不足。本研究评估了大型语言模型（LLMs）在生成准确、安全和可访问的癌症相关信息方面的能力与局限性，以支持患者理解。

Method: 本研究使用混合方法评估框架，包括语言质量、安全性和可信度、沟通可及性和效果，评估了五种通用和三种医学LLMs。我们使用定量指标、定性专家评分和统计分析（如Welch的ANOVA、Games-Howell和Hedges' g）进行分析。

Result: 通用LLMs在语言质量和效果方面表现更好，而医学LLMs在沟通可及性方面表现更佳。然而，医学LLMs表现出更高的潜在危害、毒性以及偏见，这降低了它们在安全性和可信度方面的表现。

Conclusion: 本研究提供了对用于癌症沟通的LLMs的全面评估，为改进AI生成的健康内容提供了关键见解，并指导未来开发准确、安全和可访问的数字健康工具。

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [722] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: The paper introduces NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. The experiments show that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives.

Method: We introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.

Result: Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.

Conclusion: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [723] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 本文提出了QFIX，一种新的价值函数分解模型，能够增强先前方法的性能，同时保持简单和高效。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（如VDN、QMIX）在表示能力上有限，无法表示所有IGM值，而唯一的例外QPLEX则过于复杂。因此，需要一种更简单且有效的解决方案。

Method: 提出了一种完整的IGM值的简单公式，自然导致了QFIX的推导，这是一种新的价值函数分解模型家族，通过一个薄的“修复”层扩展了先前模型的表示能力。

Result: QFIX在多个SMACv2和Overcooked环境中进行了实证评估，结果表明它能够提高先前方法的性能，学习更加稳定，并且优于QPLEX。

Conclusion: QFIX成功地增强了先前方法的性能，学习更加稳定，并且在使用最简单和最小的混合模型的情况下表现优于其主要竞争对手QPLEX。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [724] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: 本文提出了一种通用的 XAI 框架 PnPXAI，旨在解决现有 XAI 框架的局限性，支持多种数据模态和神经网络模型，并通过自动检测和优化提供最佳解释。


<details>
  <summary>Details</summary>
Motivation: 现有的 XAI 框架存在灵活性不足、支持的 XAI 方法有限以及解释推荐不优等问题，限制了 XAI 技术在实际应用中的采用。

Method: PnPXAI 通过自动检测模型架构、推荐适用的解释方法和优化超参数来实现其目标。

Result: PnPXAI 在用户调查中验证了其有效性，并展示了其在医学和金融等多个领域的多功能性。

Conclusion: PnPXAI 是一个通用的 XAI 框架，能够支持多种数据模态和神经网络模型，并通过自动检测模型架构、推荐适用的解释方法和优化超参数来提供最佳解释。

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [725] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a pharmacophore-conditioned diffusion model that integrates an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses. It outperforms ligand-based drug design methods and achieves higher docking scores without requiring target protein structures.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules remains a central, time- and cost-heavy challenge in drug discovery, particularly for novel targets lacking structural or functional data. Pharmacophore modeling presents an alternative for capturing the key features required for molecular bioactivity against a biological target.

Method: PharmaDiff is a pharmacophore-conditioned diffusion model that uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.

Result: PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods. It also achieves higher docking scores across a range of proteins in structure-based drug design without the need for target protein structures.

Conclusion: PharmaDiff offers a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques.

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [726] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 本文提出了一种新的工作流程，通过整合可穿戴健身设备的生理数据和实时环境暴露数据，预测个人对污染的健康反应。AI模型在云基础模块化框架中运行，并利用迁移学习提高其泛化能力。结果表明，该方法能够准确重建时间依赖的健康信号并捕捉污染的非线性反应。


<details>
  <summary>Details</summary>
Motivation: 空气污染对公共健康构成重大威胁，导致或加剧许多呼吸系统和心血管疾病。此外，气候变化带来了更多极端天气事件，如野火和热浪，这可能增加污染水平并加重污染暴露的影响。最近的个人传感技术进步改变了行为和生理数据的收集，为医疗保健的新改进提供了潜力。本文旨在利用这些数据以及AI在时间序列预测方面的最新进展，以监测和预测个体的健康结果。

Method: 本文提出了一种新的工作流程，通过整合可穿戴健身设备的生理数据和实时环境暴露数据，预测个人对污染的健康反应。数据以安全和伦理的方式从各种来源收集，并用于训练一个AI模型，该模型在云基础模块化框架中运行。使用对抗自编码器神经网络作为AI模型，并应用迁移学习以提高其泛化能力。

Result: AI模型（在此情况下为对抗自编码器神经网络）能够准确重建时间依赖的健康信号，并捕捉污染的非线性反应。利用个人智能手表数据进行迁移学习，提高了AI模型的泛化能力，并展示了该方法在现实世界用户生成数据中的适应性。

Conclusion: 本文提出了一种新的工作流程，通过整合可穿戴健身设备的生理数据和实时环境暴露数据，预测个人对污染的健康反应。AI模型在云基础模块化框架中训练，能够准确重建时间依赖的健康信号并捕捉污染的非线性反应。利用个人智能手表数据进行迁移学习，提高了AI模型的泛化能力，并展示了该方法在现实世界用户生成数据中的适应性。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [727] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: 本文介绍了神经热力学定律（NTL），这是一个新的框架，能够提供关于大型语言模型训练动态的新见解，并为设计学习率调度提供直观的指导。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型的规律，超越神经缩放定律。

Method: 通过假设河流山谷损失景观，证明了关键热力学量和经典热力学原理自然出现。

Result: NTL框架提供了对LLM训练动态的新见解，并产生了设计学习率调度的直观指南。

Conclusion: NTL框架为理解大型语言模型的训练动态提供了新的视角，并为设计学习率调度提供了直观的指导。

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [728] [Virtual Dosimetrists: A Radiotherapy Training "Flight Simulator"](https://arxiv.org/abs/2505.09796)
*Skylar S. Gay,Tucker Netherton,Barbara Marquez,Raymond Mumme,Mary Gronberg,Brent Parker,Chelsea Pinnix,Sanjay Shete,Carlos Cardenas,Laurence Court*

Main category: physics.med-ph

TL;DR: This paper introduces 'Virtual Dosimetrist' models that generate suboptimal radiotherapy plans and allow trainees to improve them via natural language prompts, addressing limitations in the current clinic-based training paradigm.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective education in radiotherapy plan quality review which requires robust, regularly updated examples and flexibility to demonstrate multiple planning approaches and their consequences. The current clinic-based paradigm does not support these needs.

Method: The researchers developed 'Virtual Dosimetrist' models that can generate training examples of suboptimal treatment plans and enable trainees to improve the plan quality through simple natural language prompts.

Result: The dose generation and modification process is accurate, rapid, and requires only modest resources. This work successfully combines dose distribution prediction with natural language processing.

Conclusion: The 'Virtual Dosimetrist' models provide a robust pipeline for generating suboptimal training plans and allowing trainees to practice their critical plan review and improvement skills, addressing the challenges of the current clinic-based paradigm.

Abstract: Effective education in radiotherapy plan quality review requires a robust,
regularly updated set of examples and the flexibility to demonstrate multiple
possible planning approaches and their consequences. However, the current
clinic-based paradigm does not support these needs. To address this, we have
developed 'Virtual Dosimetrist' models that can both generate training examples
of suboptimal treatment plans and then allow trainees to improve the plan
quality through simple natural language prompts, as if communicating with a
dosimetrist. The dose generation and modification process is accurate, rapid,
and requires only modest resources. This work is the first to combine dose
distribution prediction with natural language processing; providing a robust
pipeline for both generating suboptimal training plans and allowing trainees to
practice their critical plan review and improvement skills that addresses the
challenges of the current clinic-based paradigm.

</details>


### [729] [Virtual Dosimetrists: A Radiotherapy Training "Flight Simulator"](https://arxiv.org/abs/2505.09796)
*Skylar S. Gay,Tucker Netherton,Barbara Marquez,Raymond Mumme,Mary Gronberg,Brent Parker,Chelsea Pinnix,Sanjay Shete,Carlos Cardenas,Laurence Court*

Main category: physics.med-ph

TL;DR: 本研究开发了'虚拟剂量师'模型，结合剂量分布预测和自然语言处理技术，以生成次优训练计划并允许学员进行计划改进，解决了当前基于临床的范式无法满足的需求。


<details>
  <summary>Details</summary>
Motivation: 当前基于临床的范式无法满足有效教育在放射治疗计划质量审查中的需求，需要一种更灵活和更新频繁的示例库。

Method: 开发了'虚拟剂量师'模型，结合了剂量分布预测和自然语言处理技术，以生成次优训练计划并允许学员进行计划改进。

Result: 该工作首次结合了剂量分布预测与自然语言处理，提供了一个强大的管道，用于生成次优训练计划，并让学员练习他们的关键计划审查和改进技能。

Conclusion: 本研究开发了'虚拟剂量师'模型，能够生成次优治疗计划的训练示例，并允许学员通过简单的自然语言提示提高计划质量，从而解决了当前基于临床的范式无法满足的需求。

Abstract: Effective education in radiotherapy plan quality review requires a robust,
regularly updated set of examples and the flexibility to demonstrate multiple
possible planning approaches and their consequences. However, the current
clinic-based paradigm does not support these needs. To address this, we have
developed 'Virtual Dosimetrist' models that can both generate training examples
of suboptimal treatment plans and then allow trainees to improve the plan
quality through simple natural language prompts, as if communicating with a
dosimetrist. The dose generation and modification process is accurate, rapid,
and requires only modest resources. This work is the first to combine dose
distribution prediction with natural language processing; providing a robust
pipeline for both generating suboptimal training plans and allowing trainees to
practice their critical plan review and improvement skills that addresses the
challenges of the current clinic-based paradigm.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [730] [Predictive Models for Chronic Heart Failure](https://arxiv.org/abs/2505.09619)
*Pietro Cassieri,Aiman Faiz,Anna Maria De Roberto,Claudio Pascarelli,Gianvito Mitrano,Gianluca Fimiani,Marina Garofano,Christiancarmine Esposito,Genoveffa Tortora,Alessia Bramanti,Giuseppe Scanniello*

Main category: stat.OT

TL;DR: 研究人员开发了一个基于机器学习的预测模型，用于识别心力衰竭风险患者。该模型结合了临床和超声心动图特征，表现良好，尤其是在高灵敏度方面。它有望成为医疗决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 慢性心力衰竭管理面临巨大挑战，需要持续监测、早期恶化检测和个性化治疗策略。因此，有必要开发一种能够有效识别心力衰竭风险患者的预测模型。

Method: 该模型采用集成学习方法，具体为一种修改后的堆叠技术。它使用两个专门模型分别利用临床和超声心动图特征，然后通过元模型将这两个模型的预测结果结合起来。

Result: 在真实数据集上的初步评估显示，该模型在心力衰竭风险患者分层方面表现出色，灵敏度高达95%，准确率为84%。与三个基线模型相比，该模型性能更优。

Conclusion: 基于机器学习的风险分层模型可以作为有价值的决策支持工具，不仅适用于PrediHealth项目，还对医疗专业人员有益，有助于早期干预和个性化患者管理。

Abstract: The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.

</details>


### [731] [Predictive Models for Chronic Heart Failure](https://arxiv.org/abs/2505.09619)
*Pietro Cassieri,Aiman Faiz,Anna Maria De Roberto,Claudio Pascarelli,Gianvito Mitrano,Gianluca Fimiani,Marina Garofano,Christiancarmine Esposito,Genoveffa Tortora,Alessia Bramanti,Giuseppe Scanniello*

Main category: stat.OT

TL;DR: 本文提出了一种基于机器学习的预测模型，用于识别HF风险患者。该模型采用集成学习方法，利用临床和超声心动图特征，并通过元模型结合预测结果。初步结果显示，该模型在高危患者分层方面表现良好，优于基线模型，可作为有价值的决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 慢性心力衰竭（HF）的管理在现代医疗保健中面临重大挑战，需要持续监测、早期检测加重情况和个性化治疗策略。

Method: 提出了一种基于机器学习的预测模型，该模型采用集成学习方法，修改后的堆叠技术，使用两个专门的模型利用临床和超声心动图特征，然后使用元模型来结合这两个模型的预测。

Result: 模型在真实数据集上的评估结果表明，它在高危患者分层方面表现良好，具有高灵敏度（95%）和中等准确率（84%）。此外，与三种基线模型相比，该模型表现出更好的性能。

Conclusion: ML-based risk stratification models can serve as valuable decision-support tools for healthcare professionals, aiding in early intervention and personalized patient management.

Abstract: The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [732] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

TL;DR: 在统计均值估计问题中，当样本通过记忆消除通道传输时，本文提出了最优的误差指数，并证明了其在广泛的估计精度和通道质量条件下是紧的。相较之下，两种自然的基线方法产生的误差指数严格次优。


<details>
  <summary>Details</summary>
Motivation: 研究者关注的是一个统计均值估计问题，其中样本不是直接被观察到，而是通过一个中继（教师）通过记忆消除通道将信息传递给解码器（学生），学生最终进行估计。目标是在大偏差制度下分析最小最大估计误差，并寻找紧的可实现误差指数。

Method: 首先聚焦于伯努利源和二进制对称通道，然后推广到次高斯和重尾分布设置以及任意离散无记忆通道。通过理论分析，提出了一种方法以获得紧的误差指数，并与两个自然的基线方法进行了比较。

Result: 提出的误差指数在广泛的估计精度和通道质量条件下是紧的，而两种自然的基线方法产生了严格次优的误差指数。

Conclusion: 该研究表明，在特定条件下，通过设计适当的传输机制，可以在样本不直接观察的情况下实现最优的均值估计性能。这对于涉及间接观测的实际应用具有重要意义。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


### [733] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

TL;DR: 本文研究了在样本不直接观察的情况下，通过中继传输信息进行统计均值估计的问题，并建立了可实现的误差指数。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在样本不直接观察的情况下，如何通过中继传输信息来提高统计均值估计的准确性。

Method: 本文首先专注于伯努利源和二进制对称信道，然后扩展到次高斯和重尾设置以及任意离散无记忆信道。

Result: 本文建立了在大偏差范围内最小最大估计误差的可实现误差指数，并证明这些指数在广泛的估计精度和信道质量范围内是紧致的。与两种自然基线方法相比，本文的方法表现更优。

Conclusion: 本文研究了在样本不直接观察的情况下，通过中继（教师）通过无记忆信道传输信息给解码器（学生）进行统计均值估计的问题。我们建立了在大偏差范围内最小最大估计误差的可实现误差指数，并证明这些指数在广泛的估计精度和信道质量范围内是紧致的。与两种自然基线方法相比，本文的方法表现更优。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [734] [Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks](https://arxiv.org/abs/2505.10134)
*Guangjin Pan,Kaixuan Huang,Hui Chen,Shunqing Zhang,Christian Häger,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出了一种基于基础模型的无线定位解决方案LWLM，通过自监督学习框架预训练并优化三个目标：空间频率屏蔽信道建模(SF-MCM)、域变换不变性(DTI)和位置不变对比学习(PICL)，在多个定位任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确且鲁棒的定位对于5G和6G应用至关重要，但现有的数据驱动方法需要大量标注数据且难以泛化到不同部署场景和无线配置中。

Method: 首先分析了不同的自监督学习(SSL)任务如何根据信息瓶颈(IB)理论获取通用和特定任务的语义特征，然后设计了LWLM的预训练方法，包括联合优化三个互补目标：SF-MCM、DTI和PICL，并为关键下游任务设计了轻量级解码器。

Result: 实验结果表明，LWLM在所有定位任务中一致超越基于模型和监督学习的基线方法，相较于无预训练的Transformer模型提升了26.0%-87.5%，并在标注有限和未见过的基站配置下表现出强大的泛化能力。

Conclusion: LWLM作为一种基础模型，具有解决无线定位问题的巨大潜力，能够显著提高定位精度并增强泛化能力。

Abstract: Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.

</details>


### [735] [Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks](https://arxiv.org/abs/2505.10134)
*Guangjin Pan,Kaixuan Huang,Hui Chen,Shunqing Zhang,Christian Häger,Henk Wymeersch*

Main category: eess.SP

TL;DR: 本文提出了一种基于基础模型的无线定位解决方案，通过自监督学习框架优化多个目标，实现了在各种定位任务中的优越性能和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 准确且鲁棒的定位是新兴5G和6G应用的关键使能器，包括自动驾驶、扩展现实（XR）和智能制造。然而，现有模型需要大量标记数据，并且在部署场景和无线配置之间难以泛化。

Method: 我们设计了一种预训练方法，用于提出的大型无线定位模型（LWLM）。具体来说，我们提出了一个联合优化三个互补目标的自监督学习框架：(i) 空间-频率掩码信道建模（SF-MCM），(ii) 域变换不变性（DTI），以及(iii) 位置不变对比学习（PICL）。

Result: 综合实验结果证实，LWLM在所有定位任务中都一致超越了基于模型和监督学习的基线。特别是，LWLM在没有预训练的变压器模型上实现了26.0%--87.5%的改进，并在标签有限的微调和未见过的基站配置下表现出强大的泛化能力。

Conclusion: LWLM在所有定位任务中都优于基于模型和监督学习的基线，展示了其作为无线定位基础模型的潜力。

Abstract: Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [736] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

TL;DR: WavReward is a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. It outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the gap in evaluating spoken dialogue models' conversational performance, which cannot be easily measured using text-based language models like ChatGPT.

Method: 1) Based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, a specialized evaluator tailored to spoken dialogue models is constructed. 2) ChatReward-30K, a preference dataset used to train WavReward, is introduced. This dataset includes comprehension and generation aspects of spoken dialogue models covering various tasks.

Result: WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%.

Conclusion: Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly available after the paper is accepted.

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [737] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/abs/2505.09972)
*Anchen Sun,Tiantian Feng,Gabriela Gutierrez,Juan J Londono,Anfeng Xu,Batya Elbaum,Shrikanth Narayanan,Lynn K Perry,Daniel S Messinger*

Main category: eess.AS

TL;DR: This paper introduces an automated framework WSW2.0 for analyzing vocal interactions in preschool classrooms, enhancing both accuracy and scalability through the integration of wav2vec2-based speaker classification and Whisper speech transcription.


<details>
  <summary>Details</summary>
Motivation: To provide accurate measures of key features of preschool classroom speech using deep learning and natural language processing techniques, ultimately guiding more effective intervention strategies and supporting early childhood language development.

Method: Integration of wav2vec2-based speaker classification and Whisper (large-v2 and large-v3) speech transcription to create WSW2.0 framework. Comparison of system outputs to expert human annotations on 235 minutes of audio recordings.

Result: WSW2.0 achieves a weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of .672 for speaker classification. Transcription quality is moderate to high with word error rates of .119 for teachers and .238 for children. High absolute agreement intraclass correlations (ICC) with expert transcriptions for a range of classroom language features.

Conclusion: The findings highlight the potential of deep learning and natural language processing techniques to revolutionize educational research by providing accurate measures of key features of preschool classroom speech.

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


### [738] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

TL;DR: WavReward is a reward feedback model based on audio language models that evaluates both the IQ and EQ of spoken dialogue systems. It outperforms previous state-of-the-art evaluation models and shows significant improvements in both objective and subjective evaluations.


<details>
  <summary>Details</summary>
Motivation: The evaluation of spoken dialogue models' conversational performance has been overlooked due to the non-textual information conveyed by intelligent chatbots, which cannot be easily measured using text-based language models like ChatGPT.

Method: WavReward is a reward feedback model based on audio language models that evaluates both the IQ and EQ of spoken dialogue systems with speech input. It incorporates deep reasoning processes and nonlinear reward mechanisms for post-training, utilizing multi-sample feedback via reinforcement learning algorithms. Additionally, ChatReward-30K, a preference dataset, is introduced to train WavReward.

Result: WavReward achieves a significant improvement in objective accuracy from 55.1% to 91.5% compared to Qwen2.5-Omni and leads in subjective A/B testing by a margin of 83%. The comprehensive ablation studies confirm the necessity of each component of WavReward.

Conclusion: WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement in objective accuracy and subjective A/B testing. Comprehensive ablation studies confirm the necessity of each component of WavReward.

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [739] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/abs/2505.09972)
*Anchen Sun,Tiantian Feng,Gabriela Gutierrez,Juan J Londono,Anfeng Xu,Batya Elbaum,Shrikanth Narayanan,Lynn K Perry,Daniel S Messinger*

Main category: eess.AS

TL;DR: 本文介绍了一种名为WSW2.0的自动化框架，用于分析幼儿园教室中的语音互动。该框架结合了基于wav2vec2的说话人分类和Whisper语音转录技术，实现了高准确性和可扩展性。实验结果显示，WSW2.0在说话人分类和语音转录方面表现优异，并在多个课堂语言特征上与专家转录具有高度一致性。该框架已成功应用于大规模数据集，展示了其在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了提高幼儿园教室中语音互动分析的准确性和可扩展性，该研究开发了WSW2.0框架。

Method: 该论文引入了一个自动化框架WSW2.0，用于分析幼儿园教室中的语音互动，通过集成wav2vec2-based说话人分类和Whisper（large-v2和large-v3）语音转录来提高准确性和可扩展性。

Result: WSW2.0在说话人分类（儿童与教师）上达到了加权F1分数0.845、准确率0.846和误差校正的kappa值0.672。对于教师和儿童的语音转录质量分别为中等至高，词错误率分别为0.119和0.238。WSW2.0在一系列课堂语言特征方面与专家转录有相对较高的绝对一致性，包括教师和儿童的平均话语长度、词汇多样性、提问以及对问题和其他话语的回应，这些特征的绝对一致性ICC在0.64到0.98之间。此外，该框架被应用于一个涵盖两年时间超过1,592小时课堂音频数据集，证明了其在广泛实际应用中的鲁棒性。

Conclusion: 这些发现突显了深度学习和自然语言处理技术在教育研究中的潜力，通过提供准确的学前班课堂言语关键特征测量，最终指导更有效的干预策略并支持早期儿童语言发展。

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [740] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

TL;DR: Accurate wind power forecasting is crucial for grid stability and renewable energy integration. Conventional GP models with stationary kernels are inadequate for modeling the non-stationary nature of wind speed and power output. This paper proposes a non-stationary GP framework incorporating the GSM kernel to capture time-varying patterns and heteroscedastic behaviors in wind data. Evaluated on real-world SCADA data, the GSM-based model outperforms standard kernels especially in short-term forecasts.


<details>
  <summary>Details</summary>
Motivation: Wind power forecasting is essential for maintaining grid stability and integrating renewable energy sources. However, existing GP models with stationary kernels cannot adequately model the non-stationary characteristics of wind speed and power output.

Method: The paper introduces a non-stationary GP framework that uses the generalized spectral mixture (GSM) kernel. This allows the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data.

Result: The proposed GSM-based model was evaluated on real-world SCADA data across different forecasting horizons. It showed superior performance compared to standard radial basis function and spectral mixture kernels, particularly in short-term forecasts.

Conclusion: Modeling non-stationarity is necessary for accurate wind power forecasting. The proposed non-stationary GP model demonstrates practical value in operational settings.

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


### [741] [Pure Component Property Estimation Framework Using Explainable Machine Learning Methods](https://arxiv.org/abs/2505.09783)
*Jianfeng Jiao,Xi Gao,Jie Li*

Main category: stat.AP

TL;DR: An enhanced framework using explainable machine learning for predicting pure component physiochemical properties is proposed, reducing feature count without compromising accuracy and aligning with mechanistic interpretations.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of pure component physiochemical properties is crucial for process integration, multiscale modeling, and optimization.

Method: The framework uses molecular representation based on connectivity matrix for feature generation, random forest for feature ranking and pooling, and employs Shapley values for feature analysis. Models like Artificial Neural Network and Gaussian Process Regression are used for predictions.

Result: The root-mean-square error on the test set was reduced by up to 83.8% compared to GC based models. The number of features was reduced from 13316 to 100 without loss in model accuracy.

Conclusion: The proposed framework is feasible and provides a solid foundation for mixture component reconstruction and process integration modelling.

Abstract: Accurate prediction of pure component physiochemical properties is crucial
for process integration, multiscale modeling, and optimization. In this work,
an enhanced framework for pure component property prediction by using
explainable machine learning methods is proposed. In this framework, the
molecular representation method based on the connectivity matrix effectively
considers atomic bonding relationships to automatically generate features. The
supervised machine learning model random forest is applied for feature ranking
and pooling. The adjusted R2 is introduced to penalize the inclusion of
additional features, providing an assessment of the true contribution of
features. The prediction results for normal boiling point (Tb), liquid molar
volume, critical temperature (Tc) and critical pressure (Pc) obtained using
Artificial Neural Network and Gaussian Process Regression models confirm the
accuracy of the molecular representation method. Comparison with GC based
models shows that the root-mean-square error on the test set can be reduced by
up to 83.8%. To enhance the interpretability of the model, a feature analysis
method based on Shapley values is employed to determine the contribution of
each feature to the property predictions. The results indicate that using the
feature pooling method reduces the number of features from 13316 to 100 without
compromising model accuracy. The feature analysis results for Tb, Tc, and Pc
confirms that different molecular properties are influenced by different
structural features, aligning with mechanistic interpretations. In conclusion,
the proposed framework is demonstrated to be feasible and provides a solid
foundation for mixture component reconstruction and process integration
modelling.

</details>


### [742] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

TL;DR: 本文提出了一种基于广义谱混合核的非平稳高斯过程框架，用于更准确地预测风力发电，特别是在短期预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确的风力发电概率预测对于保持电网稳定性和实现可再生能源的高效整合至关重要。传统方法依赖于平稳核，这不足以建模风速和功率输出的固有非平稳性。

Method: 我们提出了一种非平稳的高斯过程框架，该框架结合了广义谱混合（GSM）核，使模型能够捕捉风速和风力数据中的时变模式和异方差行为。

Result: 所提出的模型在真实世界的SCADA数据上进行了评估，涵盖了短期、中期和长期预测时间范围。与标准径向基函数和谱混合核相比，基于GSM的模型表现更优，尤其是在短期预测中。

Conclusion: 这些结果强调了在风力发电预测中建模非平稳性的必要性，并展示了非平稳高斯过程模型在实际操作环境中的实用价值。

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


### [743] [Pure Component Property Estimation Framework Using Explainable Machine Learning Methods](https://arxiv.org/abs/2505.09783)
*Jianfeng Jiao,Xi Gao,Jie Li*

Main category: stat.AP

TL;DR: 本文提出了一种基于可解释机器学习方法的纯组分性质预测增强框架，通过分子表示方法和随机森林模型提高了预测准确性，并利用Shapley值进行特征分析以增强模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确预测纯组分的物理化学性质对于过程集成、多尺度建模和优化至关重要。

Method: 提出了一种基于连接矩阵分子表示方法和随机森林监督机器学习模型的增强框架，使用调整后的R2评估特征的真实贡献，并采用基于Shapley值的特征分析方法提高模型的可解释性。

Result: 通过人工神经网络和高斯过程回归模型获得的正常沸点(Tb)、液态摩尔体积、临界温度(Tc)和临界压力(Pc)的预测结果验证了分子表示方法的准确性。与基于GC的模型相比，测试集上的均方根误差减少了高达83.8%。特征池化方法将特征数量从13316减少到100而不影响模型准确性。Tb、Tc和Pc的特征分析结果确认了不同的分子性质由不同的结构特征影响，与机制解释一致。

Conclusion: 提出的框架被证明是可行的，并为混合组分重构和过程集成建模提供了坚实的基础。

Abstract: Accurate prediction of pure component physiochemical properties is crucial
for process integration, multiscale modeling, and optimization. In this work,
an enhanced framework for pure component property prediction by using
explainable machine learning methods is proposed. In this framework, the
molecular representation method based on the connectivity matrix effectively
considers atomic bonding relationships to automatically generate features. The
supervised machine learning model random forest is applied for feature ranking
and pooling. The adjusted R2 is introduced to penalize the inclusion of
additional features, providing an assessment of the true contribution of
features. The prediction results for normal boiling point (Tb), liquid molar
volume, critical temperature (Tc) and critical pressure (Pc) obtained using
Artificial Neural Network and Gaussian Process Regression models confirm the
accuracy of the molecular representation method. Comparison with GC based
models shows that the root-mean-square error on the test set can be reduced by
up to 83.8%. To enhance the interpretability of the model, a feature analysis
method based on Shapley values is employed to determine the contribution of
each feature to the property predictions. The results indicate that using the
feature pooling method reduces the number of features from 13316 to 100 without
compromising model accuracy. The feature analysis results for Tb, Tc, and Pc
confirms that different molecular properties are influenced by different
structural features, aligning with mechanistic interpretations. In conclusion,
the proposed framework is demonstrated to be feasible and provides a solid
foundation for mixture component reconstruction and process integration
modelling.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [744] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

TL;DR: This paper proposes DPN-GAN, a novel GAN architecture for audio sequence generation that addresses the issues of resolution constraints and mode collapse through kernel-based periodic ReLU activation and deformable convolution operations. It demonstrates superior performance and robustness in both speech synthesis and music generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GANs for audio sequence generation rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences and lead to mode collapse during conditional generation.

Method: The proposed method is DPN-GAN, which incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation and utilizes deformable convolution operations for multi-resolution generation with adaptive receptive fields. The discriminator network is also enhanced using deformable convolution to better distinguish between real and generated samples.

Result: DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics across five different datasets covering speech synthesis and music generation tasks. It shows increased robustness in synthesized audio, especially on out-of-distribution and noisy data.

Conclusion: DPN-GAN delivers superior performance and robustness in audio sequence generation compared to existing GAN architectures.

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [745] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: Voice timbre is being studied in the VtaD 2025 challenge, where human impression of voice timbre is verbalized with sensory descriptors and compared between two voices.


<details>
  <summary>Details</summary>
Motivation: To explain voice timbre attribute in a comparative manner using sensory descriptors.

Method: The timbre is explained from the comparison between two voices in their intensity within a specific descriptor dimension.

Result: Not mentioned in the abstract.

Conclusion: VtaD 2025 challenge will have a special proposal at the NCMMSC2025 conference.

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [746] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [747] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/abs/2505.09616)
*Yuqi Li,Yuanzhong Zheng,Zhongtian Guo,Yaoxuan Wang,Jianjun Yin,Haojun Fei*

Main category: cs.SD

TL;DR: SpecWav-Attack is an adversarial model which uses Wav2Vec2 for feature extraction, spectrogram resizing and incremental training to detect speakers in anonymized speech. It performs better than conventional attacks, showing the weaknesses of anonymized speech systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective method to detect speakers in anonymized speech and reveal vulnerabilities in these systems.

Method: Using Wav2Vec2 for feature extraction, incorporating spectrogram resizing and incremental training.

Result: Outperforms conventional attacks on librispeech-dev and librispeech-test datasets.

Conclusion: Anonymized speech systems have significant vulnerabilities which need stronger defenses, as benchmarked against the ICASSP 2025 Attacker Challenge.

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [748] [Introducing voice timbre attribute detection](https://arxiv.org/abs/2505.09661)
*Jinghao He,Zhengyan Sheng,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: This paper introduces voice timbre attribute detection (vTAD) to explain voice timbre using sensory attributes. A framework built on speaker embeddings is proposed and tested on VCTK-RVA dataset. ECAPA-TDNN performs better in seen scenarios, while FACodec shows superior generalization in unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: To explain the timbre conveyed by speech signals through a set of sensory attributes describing its human perception.

Method: Propose a framework based on speaker embeddings extracted from speech utterances for voice timbre attribute detection (vTAD). Investigate using VCTK-RVA dataset with ECAPA-TDNN and FACodec speaker encoders.

Result: ECAPA-TDNN outperforms in seen scenarios where testing speakers are part of training set; FACodec is superior in unseen scenarios indicating better generalization capability.

Conclusion: VCTK-RVA dataset and open-source code are provided for future research on voice timbre attribute detection.

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [749] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/abs/2505.10101)
*Jongmin Jung,Dasaem Jeong*

Main category: cs.SD

TL;DR: LAV系统结合了EnCodec的神经音频压缩与StyleGAN2的生成能力，通过随机初始化线性映射将EnCodec嵌入转换为StyleGAN2的样式潜在空间，从而生成由预录音频驱动的视觉动态输出。此方法保留了语义丰富性，并展示了预训练音频压缩模型在艺术和计算应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖显式特征映射进行音视频转换，而本文旨在探索一种新方法，利用EnCodec的嵌入作为潜在表示，直接转换到StyleGAN2的样式潜在空间，以实现更细致和语义连贯的音视频转换。

Method: LAV系统使用EnCodec的嵌入作为潜在表示，并通过随机初始化的线性映射将其直接转换到StyleGAN2的样式潜在空间中，从而生成视觉动态输出。这种方法无需显式的特征映射，而是保留了转换过程中的语义丰富性。

Result: 该框架成功实现了由预录音频驱动的视觉动态输出，展现了语义连贯且细腻的音视频转换效果。

Conclusion: 研究证明了使用预训练音频压缩模型（如EnCodec）进行艺术和计算应用的潜力，为未来音视频生成技术的发展提供了新的思路。

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [750] [Detecting Musical Deepfakes](https://arxiv.org/abs/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: This study explores the detection of AI-generated songs using the FakeMusicCaps dataset and mel spectrograms with a CNN model, while also discussing ethical and societal implications of TTM platforms.


<details>
  <summary>Details</summary>
Motivation: The rise of Text-to-Music platforms has made music creation more accessible but also poses challenges to musicians and the music industry, necessitating research into detecting AI-generated music.

Method: Using the FakeMusicCaps dataset, tempo stretching and pitch shifting were applied to simulate adversarial conditions. Mel spectrograms were generated from the modified audio for training and evaluating a convolutional neural network.

Result: Technical results on detecting AI-generated songs are presented, showing the effectiveness of the approach under adversarial conditions.

Conclusion: Carefully designed detection systems are crucial for protecting artists and harnessing the positive potential of generative AI in music.

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [751] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/abs/2505.10511)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: The paper explores combining modal decomposition with neural ordinary differential equations to model distributed musical systems, demonstrating its ability to reproduce nonlinear dynamics using synthetic data of a nonlinear string.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling geometric nonlinearities in distributed musical systems, such as pitch glides and brightness changes in high-amplitude string vibrations.

Method: Combine modal decomposition with neural ordinary differential equations. Use an analytical solution for linear vibration modes and a neural network for nonlinear dynamics. Avoid the need for a parameter encoder in the network architecture.

Result: Successfully trained the model on synthetic data of a nonlinear transverse string to reproduce the system's nonlinear dynamics.

Conclusion: This approach provides a proof of concept for modeling nonlinear musical systems while keeping physical parameters accessible.

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


### [752] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

TL;DR: 本文提出了一种新的生成对抗网络（DPN-GAN），通过引入周期性偏差和可变形卷积操作，提高了音频生成的质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成对抗网络（GAN）通常依赖于带宽有限的梅尔频谱图，这限制了生成音频序列的分辨率，并在条件生成过程中导致模式崩溃。为了解决这个问题，提出了DPN-GAN。

Method: 提出了一种基于可变形周期网络的生成对抗网络（DPN-GAN），该网络结合了基于内核的周期ReLU激活函数，以在音频生成中引入周期性偏差。此外，还使用可变形卷积增强了判别器网络，以更好地区分真实和生成的样本。

Result: 实验结果表明，DPN-GAN在分布外和噪声数据上表现优越，展示了其稳健性和适应性。

Conclusion: DPN-GAN在各种数据集上训练后，优于最先进的GAN架构，并在合成音频中表现出更高的鲁棒性。

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [753] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: VtaD 2025挑战旨在通过比较方式解释声音音色属性，并在NCMMSC2025会议上进行特别提案。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和描述声音音色属性，该挑战将人类对声音音色的印象用一系列感官描述符进行表达。

Method: 通过比较两个声音在特定描述符维度中的强度来解释音色属性。

Result: VtaD 2025挑战将于2025年10月在中国镇江的NCMMSC2025会议上举行特别提案。

Conclusion: VtaD 2025挑战旨在通过比较方式解释声音音色属性，并在NCMMSC2025会议上进行特别提案。

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [754] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

TL;DR: 本文提出了一种低计算量的方法，用于连续噪声适应用于KWS分类的预训练神经网络，该方法仅需要1-shot学习和一个epoch。实验结果表明，适应后的模型在所有场景中都优于预训练模型，特别是在SNR ≤ 18 dB的情况下，准确率提高了4.9%到46.0%。这些结果表明了所提出方法的有效性，并且其轻量级特性使其适合在资源受限的设备上部署。


<details>
  <summary>Details</summary>
Motivation: 标准的KWS系统在嵌入式设备上部署时，常常在真实操作条件下性能下降。为了解决这个问题，需要一种能够动态适应的鲁棒KWS系统，例如添加或替换关键词、适应特定用户和提高噪声鲁棒性。然而，在资源受限的设备上部署这种鲁棒的独立KWS系统仍然具有挑战性，因为内存和计算资源有限。

Method: 该研究提出了一种低计算量的方法，用于连续噪声适应用于KWS分类的预训练神经网络，该方法仅需要1-shot学习和一个epoch。

Result: 该方法使用两个预训练模型和三个真实噪声源在信噪比（SNR）范围从24到-3 dB的情况下进行了评估。适应后的模型在所有场景中都优于预训练模型，特别是在SNR ≤ 18 dB的情况下，准确率提高了4.9%到46.0%。

Conclusion: 该研究提出了一个低计算量的方法，用于连续噪声适应用于KWS分类的预训练神经网络，该方法仅需要1-shot学习和一个epoch。实验结果表明，适应后的模型在所有场景中都优于预训练模型，尤其是在SNR ≤ 18 dB的情况下，准确率提高了4.9%到46.0%。这些结果表明了所提出方法的有效性，并且其轻量级特性使其适合在资源受限的设备上部署。

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [755] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/abs/2505.09616)
*Yuqi Li,Yuanzhong Zheng,Zhongtian Guo,Yaoxuan Wang,Jianjun Yin,Haojun Fei*

Main category: cs.SD

TL;DR: 该论文提出了一个名为SpecWav-Attack的对抗模型，用于检测匿名语音中的说话人，结果表明其优于传统攻击方法，揭示了匿名语音系统的漏洞。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在检测匿名语音系统中的漏洞，并提出一种更有效的对抗模型。

Method: 该论文提出了SpecWav-Attack，一种用于检测匿名语音中说话人的对抗模型，利用了Wav2Vec2进行特征提取，并结合了频谱图缩放和增量训练以提高性能。

Result: 在librispeech-dev和librispeech-test上评估，SpecWav-Attack优于传统攻击方法。

Conclusion: 该论文揭示了匿名语音系统中的漏洞，并强调了需要更强的防御措施。

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [756] [Introducing voice timbre attribute detection](https://arxiv.org/abs/2505.09661)
*Jinghao He,Zhengyan Sheng,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: 本文介绍了一种新的语音音色属性检测任务，并提出了基于说话人嵌入的框架。实验结果显示了不同编码器在不同场景下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解释语音信号中的音色，并通过一组描述人类感知的感官属性来表达语音音色。

Method: 本文提出了一种名为语音音色属性检测（vTAD）的任务，通过语音语料对进行处理，并利用说话人嵌入构建框架。

Result: 实验结果表明，ECAPA-TDNN在已知场景中表现更优，而FACodec在未知场景中具有更好的泛化能力。

Conclusion: 实验结果表明，ECAPA-TDNN在已知场景中表现更好，而FACodec在未知场景中表现出更强的泛化能力。VCTK-RVA数据集和开源代码已发布。

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [757] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/abs/2505.10101)
*Jongmin Jung,Dasaem Jeong*

Main category: cs.SD

TL;DR: 本文介绍了LAV，这是一种结合EnCodec和StyleGAN2的系统，能够根据音频生成视觉动态内容，并保留语义信息。


<details>
  <summary>Details</summary>
Motivation: 以往的工作依赖于显式的特征映射，而LAV通过保留语义丰富性来实现更细致和语义连贯的音视频转换。

Method: LAV结合了EnCodec的神经音频压缩和StyleGAN2的生成能力，使用EnCodec嵌入作为潜在表示，并通过随机初始化的线性映射将其直接转换为StyleGAN2的风格潜在空间。

Result: LAV能够生成视觉动态输出，这些输出由预先录制的音频驱动，并且在转换过程中保持语义一致性。

Conclusion: 该框架展示了使用预训练的音频压缩模型在艺术和计算应用中的潜力。

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [758] [Detecting Musical Deepfakes](https://arxiv.org/abs/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: 本研究探讨了如何检测AI生成的歌曲，并提出了技术方法和伦理社会影响的分析。


<details>
  <summary>Details</summary>
Motivation: 随着Text-to-Music（TTM）平台的普及，音乐创作变得更加民主化，但这也给音乐家和整个音乐产业带来了新的挑战。因此，需要研究如何检测AI生成的歌曲。

Method: 本研究使用FakeMusicCaps数据集，通过将音频分类为深度伪造或人类创作来检测AI生成的歌曲。对数据集进行了节奏拉伸和音高转换以模拟现实世界的对抗条件，并从修改后的音频中生成了梅尔频谱图，然后用于训练和评估卷积神经网络。

Result: 本研究通过实验验证了检测AI生成歌曲的有效性，并提出了技术结果。

Conclusion: 本研究认为，精心设计的检测系统对于保护艺术家和释放生成式AI在音乐中的积极潜力至关重要。

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [759] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/abs/2505.10511)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: 本文提出了一种将模态分解与神经微分方程结合的方法，用于建模分布式音乐系统。该方法利用线性振动的解析解，并使用神经网络来处理非线性动态行为。


<details>
  <summary>Details</summary>
Motivation: 现有的模态合成方法在处理几何非线性方面存在局限，因此需要一种新的方法来建模分布式音乐系统。

Method: 本文结合了模态分解和神经微分方程的方法，以建模分布式音乐系统。该方法利用线性振动的解析解，并使用神经网络来处理非线性动态行为。

Result: 本文生成了一个非线性横向弦的合成数据集，并展示了模型可以训练以重现系统的非线性动力学。

Conclusion: 本文提出了一种将模态分解与神经微分方程结合的方法，用于建模分布式音乐系统。该方法利用线性振动的解析解，并使用神经网络来处理非线性动态行为。

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [760] [ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks](https://arxiv.org/abs/2505.10371)
*Kai Sun,Peibo Duan,Levin Kuhlmann,Beilun Wang,Bin Zhang*

Main category: cs.NE

TL;DR: A temporal Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model is proposed to solve the dilemma of gamma in Spiking Neural Networks (SNNs), which improves energy efficiency, stabilizes training and enhances accuracy.


<details>
  <summary>Details</summary>
Motivation: The non-differentiable spike function in SNNs is approximated by surrogate gradients with a narrow range of nonzero derivatives near the firing threshold. This leads to a dilemma: large gamma causes overactivation while small gamma results in vanishing gradients and weakened temporal dependencies.

Method: The ILIF neuron model incorporates interconnected inhibitory units for membrane potential and current, inspired by biological inhibitory mechanisms, to mitigate overactivation while preserving gradient propagation.

Result: Theoretical analysis shows that ILIF effectively overcomes the gamma dilemma. Experiments on multiple datasets demonstrate reduced firing rates, stabilized training, enhanced accuracy, and improved energy efficiency.

Conclusion: The ILIF neuron model successfully addresses the gamma dilemma in SNNs, leading to more energy-efficient, stable, and accurate networks.

Abstract: The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.

</details>


### [761] [ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks](https://arxiv.org/abs/2505.10371)
*Kai Sun,Peibo Duan,Levin Kuhlmann,Beilun Wang,Bin Zhang*

Main category: cs.NE

TL;DR: 本文提出了一种新的ILIF神经元模型，以解决SNN训练中的gamma困境，提高了能量效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于传统方法中代理梯度的支持宽度gamma存在困境，即较大的gamma导致过度激活，而较小的gamma导致梯度消失，因此需要一种新的神经元模型来解决这一问题。

Method: 提出了一种基于生物抑制机制的时序抑制漏电积分-发放（ILIF）神经元模型，该模型结合了膜电位和电流的相互连接的抑制单元，有效缓解了过度激活问题。

Result: ILIF模型在多个数据集上的实验表明，它能够降低脉冲率，提高能量效率，稳定训练并增强准确性。

Conclusion: ILIF模型有效地解决了gamma困境，提高了能量效率，稳定了训练，并增强了准确性。

Abstract: The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [762] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

TL;DR: The paper explores the application of interpretability techniques, specifically LIME, to understand and enhance inverse design optimization in photonic chip design, leading to better-performing components.


<details>
  <summary>Details</summary>
Motivation: Inverse design methodologies have significantly advanced photonic chip design but suffer from a lack of transparency in their optimization processes, similar to machine learning-based methods.

Method: The authors apply the LIME interpretability technique from machine learning to analyze and guide the inverse design optimization process for photonic components, focusing on two-mode multiplexers.

Result: LIME-informed insights led to more effective initial conditions, which directly improved device performance and revealed underlying patterns in the inverse design process.

Conclusion: Interpretability methods can actively guide and enhance the design of photonic components beyond just explaining models.

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


### [763] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

TL;DR: 本文探讨了如何利用可解释性技术（如LIME）来理解并改进逆向设计的光子组件，结果表明这些技术不仅能解释模型，还能指导设计以提升性能。


<details>
  <summary>Details</summary>
Motivation: 逆向设计方法在光子芯片设计中取得了显著进展，但其黑箱性质使得难以理解输出。为了克服这一挑战，我们采用了可解释性技术来提高透明度。

Method: 我们应用了机器学习中的可解释性技术，特别是局部可解释模型无关解释（LIME），以理解逆向设计优化在设计双模复用器中的作用。

Result: LIME提供的见解帮助我们找到了更有效的初始条件，从而直接提高了设备性能。

Conclusion: 我们的结果表明，可解释性技术不仅可以解释模型，还可以主动指导和增强逆向设计的光子组件。

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [764] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

TL;DR: 在肿瘤学中，利用少量标注样本进行上下文学习（ICL）是一种不需要重新训练模型即可适应新诊断任务的方法。本研究首次比较了多个视觉-语言模型在不同肿瘤分类任务上的表现，结果表明ICL具有接近特定任务行为的潜力，尤其适用于稀有癌症和资源有限的情境。


<details>
  <summary>Details</summary>
Motivation: 当前AI在肿瘤学中的应用受限于对大型标注数据集的依赖以及针对特定领域诊断任务需要重新训练模型的需求。为解决这些限制，研究探索了上下文学习作为无需重新训练模型的实用替代方案。

Method: 研究使用四个视觉-语言模型（Paligemma、CLIP、ALIGN和GPT-4o），通过仅使用少量标注样本来评估其在三个肿瘤学数据集（MHIST、PatchCamelyon和HAM10000）上的性能，而无需更新模型参数。

Result: 所有模型在少量样本提示下表现出显著提升，其中GPT-4o在二分类任务中达到F1分数0.81，在多分类任务中达到0.60。尽管这些结果低于完全微调系统的水平，但开源模型如Paligemma和CLIP也展示了竞争力。

Conclusion: 上下文学习在肿瘤学中展现出作为实际解决方案的潜力，特别是在稀有癌症和资源受限的情况下，可以利用少量例子来近似特定任务的行为。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [765] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

TL;DR: The paper presents a unified framework for multi-organ and multilingual ultrasound report generation, which integrates fragment-based multilingual training, curates a bilingual dataset, and leverages standardized US reports.


<details>
  <summary>Details</summary>
Motivation: Ultrasound report generation is challenging due to variability in images, operator dependence, and lack of consistent datasets. Current methods have limitations in generating accurate and standardized reports across different organs and languages.

Method: A unified framework is proposed that includes fragment-based multilingual training, alignment of modular text fragments with imaging data, and curation of a bilingual English-Chinese dataset. Fine-tuning with selective unfreezing of the vision transformer (ViT) is used to improve text-image alignment.

Result: Compared to the previous state-of-the-art KMVE method, the approach achieves relative gains of about 2% in BLEU scores, approximately 3% in ROUGE-L, and about 15% in CIDEr, while significantly reducing errors such as missing or incorrect content.

Conclusion: This work demonstrates strong potential for real-world clinical workflows by unifying multi-organ and multi-language report generation into a single, scalable framework.

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [766] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

TL;DR: Deep learning models for cervical cancer classification often lack reliable uncertainty estimation. Conformal prediction can address this issue, but current methods still have limitations in aligning with human expectations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are overconfident and cannot reliably reflect diagnostic uncertainty. The conventional evaluation of conformal prediction primarily focuses on whether the prediction set includes the true class, ignoring extraneous classes.

Method: Using expert annotation sets collected from multiple annotators to comprehensively validate conformal prediction sets, evaluating three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification.

Result: Conventional coverage-based evaluations overestimate performance, and current conformal prediction methods often produce prediction sets that do not align well with human labels. Additionally, conformal prediction methods show some capabilities in identifying ambiguous and out-of-distribution data.

Conclusion: Conformal prediction shows promise in addressing uncertainty issues in deep learning models for cervical cancer classification, but improvements are needed to better align prediction sets with human expectations.

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [767] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

TL;DR: The paper introduces a new mask pattern for the SparK model, named Mesh Mask-ed SparK model, and investigates the impact of mask patterns on pre-training performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of the SparK model by introducing a new mask pattern.

Method: The method involves proposing a new mask pattern for the SparK model, resulting in the Mesh Mask-ed SparK model.

Result: The effect of the mask pattern on the performance during pre-training is reported.

Conclusion: A new mask pattern for the SparK model has been proposed and its influence on performance has been demonstrated.

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [768] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/abs/2505.08843)
*Marco Corrias,Giada Franceschi,Michele Riva,Alberto Tampieri,Karin Föttinger,Ulrike Diebold,Thomas Pock,Cesare Franchini*

Main category: eess.IV

TL;DR: 实验获取的显微图像不可避免地受到噪声和其他不想要信号的影响，这会降低它们的质量并可能隐藏相关特征。本研究通过基于全变差（TV）的工作流程对显微图像进行分解和降噪，处理了包括原子力显微镜（AFM）、扫描隧道显微镜（STM）和扫描电子显微镜（SEM）在内的各种显微技术获得的图像。


<details>
  <summary>Details</summary>
Motivation: 实验获取的显微图像受噪声和其他不想要信号影响，需要现代降噪和恢复解决方案。

Method: 通过基于全变差（TV）的工作流程对显微图像进行分解和降噪，评估TV-$L^1$、Huber-ROF和TGV-$L^1$在不同研究案例中的性能。

Result: Huber-ROF是最灵活的方法，而TGV-$L^1$最适合降噪。该方法在显微镜中有更广泛的应用潜力。

Conclusion: Python代码公开可用，并可集成到实验工作流程中用于图像获取或降噪已获取的图像。

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


### [769] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/abs/2505.09193)
*Wei Jiang,Junru Li,Kai Zhang,Li Zhang*

Main category: eess.IV

TL;DR: The paper introduces BiECVC, a bidirectional video compression framework that incorporates local and non-local context modeling with adaptive gating, achieving state-of-the-art performance surpassing VTM 13.2 RA.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing bidirectional video compression (BVC) methods which primarily exploit temporal motion while neglecting non-local correlations across frames and lack adaptability to suppress harmful contexts from fast motion or occlusion.

Method: BiECVC reuses high-quality features from lower layers for local context enhancement, adopts a linear attention mechanism for efficient non-local dependency modeling, and introduces Bidirectional Context Gating to dynamically filter contextual information based on conditional coding results.

Result: BiECVC reduces bit-rate by 13.4% and 15.7% compared to VTM 13.2 under Random Access configuration with intra periods of 32 and 64 respectively, making it the first learned video codec to surpass VTM 13.2 RA across all standard test datasets.

Conclusion: BiECVC achieves state-of-the-art performance in bidirectional video compression by effectively modeling local and non-local contexts and adaptively gating contextual information.

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [770] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/abs/2505.09323)
*Pengli Zhu,Yingji Fu,Nanguang Chen,Anqi Qiu*

Main category: eess.IV

TL;DR: This study proposes Q-CATN, a novel network for MS-HARDI synthesis from flexible q-space sampling using structural MRI data. It uses collaborative attention and task-specific constraints to preserve anatomical fidelity in DWI, outperforming existing methods on the HCP dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a method for multi-shell, high-angular resolution DWI synthesis that can work with flexible q-space sampling while preserving anatomical fidelity, improving upon current methods that require fixed sampling schemes.

Method: Q-CATN employs a collaborative attention mechanism to extract complementary information from multiple modalities and adjusts its internal representations based on flexible q-space information. Task-specific constraints are introduced to preserve anatomical fidelity in DWI.

Result: Extensive experiments on the HCP dataset show that Q-CATN outperforms existing methods (1D-qDL, 2D-qDL, MESC-SD, QGAN) in estimating parameter maps and fiber tracts both quantitatively and qualitatively while preserving fine-grained details.

Conclusion: Q-CATN's ability to accommodate flexible q-space sampling makes it a promising toolkit for clinical and research applications.

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [771] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/abs/2505.09334)
*Sadman Sakib Alif,Nasim Anzum Promise,Fiaz Al Abid,Aniqua Nusrat Zereen*

Main category: eess.IV

TL;DR: Lung cancer is a major cause of cancer-related deaths. Deep learning models, while effective, are resource-intensive and lack transparency. Knowledge distillation and explainable AI (XAI) can address these issues. We propose DCSNet, a lightweight student model for lung cancer detection, using ResNet50 as the teacher model among eight evaluated CNNs.


<details>
  <summary>Details</summary>
Motivation: Deep learning models like CNNs have revolutionized medical image analysis but face challenges such as computational expense and lack of transparency which hinder their adoption in healthcare, especially in resource-constrained environments.

Method: The paper evaluates eight CNNs as potential teacher models and uses knowledge distillation to develop a lightweight student model called Distilled Custom Student Network (DCSNet). ResNet50 was selected as the teacher model. The method also incorporates XAI techniques to enhance the transparency of the model.

Result: DCSNet ensures high diagnostic performance even in resource-constrained settings and addresses transparency concerns through the use of XAI techniques.

Conclusion: This approach facilitates the broader adoption of AI-driven diagnostic tools in healthcare by making them more efficient and trustworthy.

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [772] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/abs/2505.09521)
*Dongyi He,Shiyang Li,Bin Jiang,He Yan*

Main category: eess.IV

TL;DR: Spec2VolCAMU-Net是一种轻量级的频谱图到体积生成器，通过多方向时频卷积注意力编码器和Vision-Mamba U-Net解码器，实现了从EEG生成类似fMRI的脑部活动数据，显著提高了效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率功能性磁共振成像（fMRI）对于绘制人类大脑活动至关重要，但成本高昂且操作复杂。如果能直接从广泛使用的头皮脑电图（EEG）生成可比的体积数据，先进的神经成像将变得更加普及。然而，现有的方法要么无法捕捉跨通道的时间频率线索，要么依赖于内存密集和不稳定的变压器/ GAN解码器。

Method: 提出了Spec2VolCAMU-Net，一个轻量级的频谱图到体积生成器。它包括一个多方向时频卷积注意力编码器，该编码器堆叠了时间、光谱和联合卷积与自注意力机制，并使用Vision-Mamba U-Net解码器，其线性时间状态空间块能够有效进行长距离空间建模。整个模型通过混合SSI-MSE损失函数进行端到端训练。

Result: 在三个公开基准测试中，Spec2VolCAMU-Net达到了最先进的保真度：NODDI上的SSIM为0.693，Oddball上为0.725，CN-EPFL上为0.788，分别比之前的最佳SSIM分数提高了14.5%，14.9%和16.9%。此外，在PSNR评分方面表现也具有竞争力，尤其是在CN-EPFL数据集上，比之前的最佳PSNR提高了4.6%。

Conclusion: Spec2VolCAMU-Net不仅在重建质量上取得了更好的平衡，而且由于其轻量级和高效的特点，非常适合用于临床和研究环境中的实时应用。

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [773] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

TL;DR: The paper presents a novel SVR method using implicit neural representations for fast and accurate MRI reconstruction, especially in cases of severe motion and image corruption. It shows improvements in quality and up to 50% reduction in reconstruction time compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing SVR methods struggle with image artifacts and severe subject motion or require slice pre-alignment for good performance.

Method: The proposed method performs motion correction, outlier handling, and super-resolution reconstruction based on implicit neural representations. The model can be initialized with task-specific priors through self-supervised meta-learning.

Result: The method showed improvements in reconstruction quality, particularly in the presence of severe motion, and up to 50% reduction in reconstruction time in experiments with over 480 reconstructions of simulated and clinical MRI brain data.

Conclusion: The novel SVR method using implicit neural representations is effective for fast and accurate MRI reconstruction in cases of severe motion and image corruption.

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [774] [ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions](https://arxiv.org/abs/2505.09831)
*Tushar Kataria,Beatrice Knudsen,Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: The paper introduces ImplicitStainer, a new method using local implicit functions for virtual staining to generate IHC stains from H&E images. It addresses the limitations of existing GAN and diffusion models by improving performance with limited data and shows superior results through extensive validation.


<details>
  <summary>Details</summary>
Motivation: H&E staining is the standard in pathology but lacks comprehensive diagnostic information. Current IHC staining methods are time-consuming and only available at specialized centers, causing delays in patient treatment. Virtual staining via deep learning offers an alternative solution.

Method: ImplicitStainer uses local implicit functions to enhance pixel-level predictions in image translation tasks, specifically focusing on generating high-quality IHC stains from H&E images even when data is limited.

Result: Validated on two datasets using multiple metrics, ImplicitStainer outperforms over fifteen state-of-the-art GAN and diffusion-based models, demonstrating robustness and effectiveness with limited data.

Conclusion: ImplicitStainer provides a promising advancement in virtual staining technology, offering improved performance with less data compared to existing methods. Full code and trained models will be made publicly available.

Abstract: Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.

</details>


### [775] [Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/abs/2505.09985)
*Pengfei Yu,Bin Huang,Minghui Zhang,Weiwen Wu,Shaoyu Wang,Qiegen Liu*

Main category: eess.IV

TL;DR: OSMM is a new model that divides CT projection data into subsets for independent learning, integrates OWDM for global information constraint, and uses an unsupervised learning framework to improve sparse-view CT reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current score-based diffusion models face challenges with large, redundant projection datasets in sparse-view CT reconstruction, leading to lower learning effectiveness, higher difficulty, and reconstructed images lacking fine details.

Method: The OSMM divides CT projection data into equal subsets and applies MSDM to learn from each subset independently. It also integrates OWDM with complete sinogram data as a global information constraint.

Result: Experimental results show that OSMM surpasses traditional diffusion models in image quality and noise resilience for sparse-view CT reconstruction.

Conclusion: OSMM offers a robust and versatile solution for advanced CT imaging in sparse-view scenarios.

Abstract: Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.

</details>


### [776] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/abs/2505.10405)
*Jianhao Huang,Qunsong Zeng,Kaibin Huang*

Main category: eess.IV

TL;DR: The paper proposes a hybrid generative semantic communication (Gen-SemCom) system with critical information embedding framework for 6G networks. It introduces the GVIF metric to evaluate visual quality and designs a channel-adaptive system.


<details>
  <summary>Details</summary>
Motivation: To overcome the issues of losing fine-grained visual details in purely prompt-driven generation and the lack of systematic metrics to evaluate Gen-SemCom systems.

Method: Developed a hybrid Gen-SemCom system using a critical information embedding (CIE) framework which transmits both text prompts and semantically critical features. Proposed the generative visual information fidelity (GVIF) metric to assess visual quality. Designed a channel-adaptive Gen-SemCom system based on maximizing the GVIF metric.

Result: Validated that the GVIF metric is sensitive to visual fidelity, correlating with PSNR and critical information volume. The optimized system outperforms benchmarking schemes with higher PSNR and lower FID scores.

Conclusion: The hybrid Gen-SemCom system with CIE framework and GVIF metric effectively addresses the limitations of current Gen-SemCom systems and shows superior performance.

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


### [777] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/abs/2505.10464)
*Jiaming Liang,Lihuan Dai,Xiaoqi Sheng,Xiangguang Chen,Chun Yao,Guihua Tao,Qibin Leng,Honming Cai,Xi Zhong*

Main category: eess.IV

TL;DR: In the field of gastric cancer lesion analysis, multimodal medical image segmentation encounters challenges due to lack of datasets and difficulty in aligning modalities. To solve these problems, the paper presents GCM 2025, a new large-scale dataset, and HWA-UNETR, a novel 3D segmentation framework. Experiments show that HWA-UNETR outperforms existing methods with a Dice score improvement of up to 1.68%.


<details>
  <summary>Details</summary>
Motivation: Multimodal medical image segmentation for gastric cancer lesion analysis is constrained by the scarcity of independent multimodal datasets and the challenge of amalgamating misaligned modalities, which leads to substantial resource expenditure and potential decline in accuracy.

Method: The paper introduces HWA-UNETR, a 3D segmentation framework incorporating an HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures. It also utilizes a tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies.

Result: Experiments on the GCM 2025 and BraTS 2021 datasets demonstrate that HWA-UNETR surpasses existing methods by up to 1.68% in the Dice score while maintaining robustness.

Conclusion: The introduction of the GCM 2025 dataset and the HWA-UNETR framework provides a valuable resource and effective solution for multimodal medical image segmentation in gastric cancer lesion analysis.

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [778] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/abs/2505.10492)
*Taylor L. Bobrow,Mayank Golhar,Suchapa Arayakarnkul,Anthony A. Song,Saowanee Ngamruengphong,Nicholas J. Durr*

Main category: eess.IV

TL;DR: The paper introduces Multi-contrast Laser Endoscopy (MLE) for better gastrointestinal imaging, showing improved contrast and color difference in detecting polyps compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the detection of diseases in the gastrointestinal tract by enhancing the contrast of visual abnormalities that are often subtle in white light endoscopy.

Method: Developed MLE platform with rapidly tunable spectral, coherent, and directional illumination. Demonstrated capabilities include multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.

Result: MLE achieved a three-fold improvement in contrast and five-fold improvement in color difference when imaging 31 polyps compared to white light and narrow band imaging.

Conclusion: MLE shows potential as an investigative tool to enhance gastrointestinal imaging by revealing multiple types of tissue contrast while integrating into the clinical environment.

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


### [779] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

TL;DR: 本文研究了在不重新训练模型的情况下，通过在推理时使用少量标记示例来适应新的诊断任务，从而替代模型重新训练的潜在方法。


<details>
  <summary>Details</summary>
Motivation: AI在肿瘤学中的应用受到其对大型注释数据集的依赖以及需要重新训练模型以进行领域特定诊断任务的限制。

Method: 我们使用四种视觉语言模型（VLMs）-Paligemma、CLIP、ALIGN和GPT-4o，评估了在三个肿瘤学数据集上的性能：MHIST、PatchCamelyon和HAM10000。

Result: 所有模型在少样本提示下都显示出显著的提升，GPT-4o在二分类设置中达到了0.81的F1分数，在多类分类设置中达到了0.60。开放源代码模型如Paligemma和CLIP尽管规模较小，但表现出有竞争力的提升。

Conclusion: 这些发现突显了ICL在肿瘤学中的潜力，特别是在罕见癌症和资源有限的环境中，其中微调不可行且注释数据难以获得。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [780] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

TL;DR: 本研究提出了一种统一的框架，用于多器官和多语言超声报告生成，通过结合基于片段的多语言训练和标准化报告特性，实现了跨器官和语言的一致且临床准确的文本生成，并在多个评估指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声报告生成是一项具有挑战性的任务，由于超声图像的变异性、操作员依赖性和标准化文本的需求，自动化变得困难。与X射线和CT不同，超声成像缺乏一致的数据集，使得自动化变得困难。

Method: 提出了一种统一的框架，用于多器官和多语言超声报告生成，结合了基于片段的多语言训练，并利用了超声报告的标准化特性。通过将模块化文本片段与多样化的成像数据对齐，并整理了一个双语英语-中文数据集，该方法实现了跨器官部位和语言的一致且临床准确的文本生成。选择性解冻视觉变压器（ViT）的微调进一步提高了文本-图像对齐效果。

Result: 与之前的最先进方法KMVE相比，我们的方法在BLEU分数上相对提升了约2%，ROUGE-L约3%，CIDEr约15%，同时显著减少了缺失或错误内容等错误。

Conclusion: 通过将多器官和多语言报告生成统一到一个可扩展的框架中，本研究展示了在实际临床工作流程中的强大潜力。

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [781] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

TL;DR: 本文研究了共形预测方法在宫颈癌分类中的应用，发现传统评估方法高估了性能，并且共形预测方法在与人类标签对齐方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常过于自信，不能可靠地反映诊断不确定性。此外，它们通常被优化为生成最大似然预测，这无法传达结果中的不确定性和歧义。这些问题可以通过共形预测来解决，这是一种模型无关的框架，用于生成包含训练深度学习模型可能类别的预测集。

Method: 我们使用从多个注释器收集的专家注释集全面验证了共形预测集。我们评估了三种共形预测方法应用于三种用于宫颈非典型性分类的深度学习模型的效果。

Result: 我们的专家注释分析显示，传统的基于覆盖率的评估高估了性能，并且当前的共形预测方法经常产生与人类标签不一致的预测集。此外，我们探索了共形预测方法在识别模糊和分布外数据方面的能力。

Conclusion: 我们的分析表明，传统的基于覆盖率的评估高估了性能，并且当前的共形预测方法经常产生与人类标签不一致的预测集。此外，我们探索了共形预测方法在识别模糊和分布外数据方面的能力。

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [782] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

TL;DR: 本文提出了一种新的遮罩模式，用于改进SparK模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了改进SparK模型的性能，本文提出了一个新的遮罩模式。

Method: 本文提出了一个新的遮罩模式，用于改进SparK模型的性能。

Result: 本文报告了用于图像遮罩的遮罩模式在预训练中的效果。

Conclusion: 本文提出了一种新的遮罩模式，即Mesh Mask-ed SparK模型，以改进SparK模型的性能。

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [783] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/abs/2505.08843)
*Marco Corrias,Giada Franceschi,Michele Riva,Alberto Tampieri,Karin Föttinger,Ulrike Diebold,Thomas Pock,Cesare Franchini*

Main category: eess.IV

TL;DR: 本研究通过基于总变分（TV）的工作流程，评估了TV-L^1、Huber-ROF和TGV-L^1在显微镜图像去噪中的性能。结果显示Huber-ROF最为灵活，TGV-L^1最适用于去噪，并表明该方法在显微镜学中有更广泛的应用前景。


<details>
  <summary>Details</summary>
Motivation: 由于实验获得的显微镜图像不可避免地受到噪声和其他不需要的信号的影响，这会降低其质量并可能隐藏相关特征。随着图像采集速率的增加，现代去噪和修复解决方案变得必要。

Method: 本研究通过基于总变分（TV）的工作流程，专注于显微镜图像的分解和去噪，包括原子力显微镜（AFM）、扫描隧道显微镜（STM）和扫描电子显微镜（SEM）。我们的方法包括提取并从原始图像中减去不需要的信号成分，或者对图像进行去噪。

Result: Huber-ROF被证明是最灵活的方法，而TGV-L^1最适合去噪。

Conclusion: 我们的结果表明，这种方法在显微镜学中有更广泛的应用前景，不仅限于STM、AFM和SEM图像。

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


### [784] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/abs/2505.09193)
*Wei Jiang,Junru Li,Kai Zhang,Li Zhang*

Main category: eess.IV

TL;DR: BiECVC是一种新型的双向视频压缩框架，通过结合多样化局部和非局部上下文建模以及自适应上下文门控，显著提高了视频压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有的BVC方法主要利用时间运动，而忽略了跨帧的非局部相关性，并且缺乏动态抑制因快速运动或遮挡产生的有害上下文的能力。因此，需要一种新的方法来解决这些问题。

Method: BiECVC框架结合了多样化局部和非局部上下文建模以及自适应上下文门控。它重新使用低层的高质量特征，并使用解码的运动矢量进行对齐，以避免引入额外的运动开销。此外，采用线性注意力机制来高效建模非局部依赖关系，并引入双向上下文门控来动态过滤基于条件编码结果的上下文信息。

Result: BiECVC在Random Access (RA)配置下，与VTM 13.2相比，分别在intra周期为32和64的情况下，将比特率降低了13.4%和15.7%。BiECVC是第一个在所有标准测试数据集上超越VTM 13.2 RA的 learned video codec。

Conclusion: BiECVC是第一个在所有标准测试数据集上超越VTM 13.2 RA的 learned video codec。

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [785] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/abs/2505.09323)
*Pengli Zhu,Yingji Fu,Nanguang Chen,Anqi Qiu*

Main category: eess.IV

TL;DR: Q-CATN是一种新的Q空间引导协作注意力翻译网络，用于从灵活的q空间采样中合成多壳高角分辨率扩散加权成像（MS-HARDI），通过协作注意力机制和任务特定约束，提高了参数图和纤维束估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要固定的采样方案，而Q-CATN旨在解决这一限制，通过灵活的q空间采样来提高MS-HARDI合成的准确性。

Method: Q-CATN采用协作注意力机制，从多种模态中有效提取互补信息，并根据灵活的q空间信息动态调整内部表示，同时引入任务特定约束以保持DWI的解剖保真度。

Result: 在HCP数据集上的广泛实验表明，Q-CATN在定量和定性评估中均优于1D-qDL、2D-qDL、MESC-SD和QGAN等现有方法，同时保留了细粒度细节。

Conclusion: Q-CATN在估计参数图和纤维束方面优于现有方法，并且能够适应灵活的q空间采样，显示出其在临床和研究应用中的潜力。

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [786] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/abs/2505.09334)
*Sadman Sakib Alif,Nasim Anzum Promise,Fiaz Al Abid,Aniqua Nusrat Zereen*

Main category: eess.IV

TL;DR: 本文提出了一种基于知识蒸馏的方法用于肺癌检测，结合了可解释AI技术以提高模型的透明度。通过评估多种CNN作为教师模型，并训练了一个轻量级学生模型DCSNet，该方法在资源受限的环境中实现了高诊断性能，并解决了透明度问题，促进了AI驱动诊断工具在医疗领域的应用。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，早期检测和准确诊断对于提高生存率至关重要。尽管深度学习，特别是卷积神经网络（CNN）在医学图像分析中取得了重大进展，但其应用面临挑战，如计算成本高、资源需求大以及缺乏透明度，这阻碍了其在医疗等敏感领域的信任和广泛应用。

Method: 我们提出了一种基于知识蒸馏的方法用于肺癌检测，并结合了可解释AI（XAI）技术以提高模型的透明度。评估了八种CNN作为教师模型，包括ResNet50、EfficientNetB0、EfficientNetB3和VGG16，并使用ResNet50作为教师训练了一个轻量级学生模型DCSNet。

Result: 该方法不仅确保了在资源受限环境中的高诊断性能，还解决了透明度问题，有助于AI驱动诊断工具在医疗领域的采用。

Conclusion: 该方法在资源受限的环境中确保了高诊断性能，并解决了透明度问题，促进了AI驱动诊断工具在医疗领域的采用。

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [787] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/abs/2505.09521)
*Dongyi He,Shiyang Li,Bin Jiang,He Yan*

Main category: eess.IV

TL;DR: 本文提出了一种名为Spec2VolCAMU-Net的轻量级频谱图到体积生成器，通过多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器来解决现有方法的问题。在三个公共基准上实现了最先进的保真度，并在PSNR得分上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 高分辨率功能性磁共振成像（fMRI）在映射人类大脑活动方面至关重要，但成本高昂且物流困难。如果能够直接从广泛可用的头皮脑电图（EEG）生成可比较的体积，先进的神经成像将变得更加可及。

Method: 提出了一种名为Spec2VolCAMU-Net的轻量级频谱图到体积生成器，通过多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器来解决现有方法的问题。

Result: 在三个公共基准上实现了最先进的保真度，分别在NODDI、Oddball和CN-EPFL数据集上获得了0.693、0.725和0.788的SSIM值，分别比之前最好的SSIM分数提高了14.5%、14.9%和16.9%。此外，在CN-EPFL数据集上PSNR得分有4.6%的提升。

Conclusion: 该模型轻量且高效，适合临床和研究环境中的实时应用。

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [788] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

TL;DR: 本文提出了一种新的SVR方法，能够在严重运动和图像伪影的情况下实现快速准确的MRI重建。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案在图像伪影和严重的受试者运动情况下表现不佳，或者需要切片预对齐以获得令人满意的重建性能。

Method: 我们提出了一种新的SVR方法，该方法基于隐式神经表示进行运动校正、异常值处理和超分辨率重建。模型可以通过在模拟或现实世界数据上的完全自监督元学习进行任务特定先验的初始化。

Result: 在包括来自不同中心的480多个模拟和临床MRI脑数据重建的广泛实验中，我们证明了在严重受试者运动和图像伪影情况下方法的有效性。

Conclusion: 我们的方法在严重运动和图像伪影的情况下证明了其有效性，并且与最先进的方法相比，重建质量有所提高，重建时间减少了最多50%。

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [789] [ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions](https://arxiv.org/abs/2505.09831)
*Tushar Kataria,Beatrice Knudsen,Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: ImplicitStainer是一种新的图像翻译方法，通过局部隐函数提升虚拟染色性能，即使在数据有限的情况下也能生成高质量结果。


<details>
  <summary>Details</summary>
Motivation: H&E染色无法捕捉所有诊断信息，而IHC染色虽然重要，但通常只在专业中心可用且耗时，导致患者治疗延迟。虚拟染色提供了一种有前途的替代方案，通过计算生成IHC染色。然而，现有方法需要大量和多样化的数据，限制了其应用。

Method: ImplicitStainer利用局部隐函数来改进图像翻译，专注于像素级预测，以提高虚拟染色性能。

Result: ImplicitStainer在两个数据集上进行了验证，并与超过十五种最先进的GAN和扩散模型进行了比较，展示了其在有限数据下的高质量结果和对数据集大小变化的鲁棒性。

Conclusion: ImplicitStainer是一种改进图像翻译，特别是虚拟染色性能的新方法，通过关注像素级预测来提高对数据集大小变化的鲁棒性，并在有限数据下提供高质量结果。

Abstract: Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.

</details>


### [790] [Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/abs/2505.09985)
*Pengfei Yu,Bin Huang,Minghui Zhang,Weiwen Wu,Shaoyu Wang,Qiegen Liu*

Main category: eess.IV

TL;DR: 本文提出了一种新的稀疏视图CT重建方法——有序子集多扩散模型（OSMM），通过将数据划分为子集并结合全局信息约束，提高了图像质量和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在处理未处理的数据时学习效果较差，难以重建出精细细节，因此需要一种更有效的解决方案来提高稀疏视图CT重建的质量。

Method: 我们提出了有序子集多扩散模型（OSMM），该模型创新性地将CT投影数据分为相等的子集，并使用多子集扩散模型（MSDM）独立学习每个子集。此外，还将一个完整的一体化扩散模型（OWDM）与完整的sinogram数据集成，作为全局信息约束，减少生成错误或不一致的sinogram信息的可能性。OSMM的无监督学习框架提供了强大的鲁棒性和泛化能力，能够无缝适应CT sinogram的不同稀疏水平。

Result: 实验结果表明，OSMM在图像质量和噪声鲁棒性方面优于传统扩散模型，为稀疏视图CT成像提供了强大而通用的解决方案。

Conclusion: 实验结果表明，OSMM在图像质量和噪声鲁棒性方面优于传统扩散模型，为稀疏视图CT成像提供了强大而通用的解决方案。

Abstract: Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.

</details>


### [791] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/abs/2505.10405)
*Jianhao Huang,Qunsong Zeng,Kaibin Huang*

Main category: eess.IV

TL;DR: 本文提出了一种混合Gen-SemCom系统，结合了文本提示和语义关键特征的传输，以提高图像重建的保真度。同时，引入了生成视觉信息保真度（GVIF）度量标准，用于评估生成图像的视觉质量，并设计了一个信道自适应的系统以优化性能。实验结果表明，该系统在PSNR和FID分数方面优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 生成语义通信（Gen-SemCom）利用大型人工智能（AI）模型为6G网络提供了一种变革性的范式，通过传输低维提示而不是原始数据来减少通信成本。然而，纯粹的提示驱动生成会丢失精细的视觉细节。此外，缺乏系统的度量标准来评估Gen-SemCom系统的性能。

Method: 我们开发了一个混合Gen-SemCom系统，其中包含一个关键信息嵌入（CIE）框架，其中提取文本提示和语义关键特征进行传输。首先，提出了一种新的语义过滤方法，以选择和传输与语义标签相关的图像的语义关键特征。通过集成文本提示和关键特征，接收器使用基于扩散的生成模型重建高保真图像。接下来，我们提出了生成视觉信息保真度（GVIF）度量标准来评估生成图像的视觉质量。通过表征图像特征的统计模型，GVIF度量标准量化了失真特征与其原始对应物之间的互信息。通过最大化GVIF度量标准，我们设计了一个信道自适应的Gen-SemCom系统，根据信道状态自适应地控制特征量和压缩率。

Result: 实验结果验证了GVIF度量对视觉保真度的敏感性，并与PSNR和关键信息量相关。此外，优化后的系统在PSNR和FID分数方面优于基准方案。

Conclusion: 实验结果验证了GVIF度量对视觉保真度的敏感性，并与PSNR和关键信息量相关。此外，优化后的系统在PSNR和FID分数方面优于基准方案。

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


### [792] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/abs/2505.10464)
*Jiaming Liang,Lihuan Dai,Xiaoqi Sheng,Xiangguang Chen,Chun Yao,Guihua Tao,Qibin Leng,Honming Cai,Xi Zhong*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [793] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/abs/2505.10492)
*Taylor L. Bobrow,Mayank Golhar,Suchapa Arayakarnkul,Anthony A. Song,Saowanee Ngamruengphong,Nicholas J. Durr*

Main category: eess.IV

TL;DR: MLE improves gastrointestinal imaging by enhancing tissue contrast through multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.


<details>
  <summary>Details</summary>
Motivation: White light endoscopy often fails to detect clinically relevant cases due to subtle contrast in tissue color, texture, and shape.

Method: Multi-contrast Laser Endoscopy (MLE) is introduced, which uses rapidly tunable spectral, coherent, and directional illumination to enhance tissue contrast.

Result: MLE images from 31 polyps show an approximate three-fold improvement in contrast and a five-fold improvement in color difference compared to white light and narrow band imaging.

Conclusion: MLE shows promise as an investigative tool to improve gastrointestinal imaging.

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [794] [Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training](https://arxiv.org/abs/2505.10393)
*Agustin Medina,Marcelo Arlego,Carlos A. Lamas*

Main category: cond-mat.str-el

TL;DR: 本研究探讨了使用人工神经网络高效学习磁相的方法，结合计算简单性和物理信息策略。通过监督分类和非监督检测两种方法，并加入物理信息指导，验证了机器学习预测与数值估计结果的一致性。结果表明，合成、结构化且计算高效的训练方案可以揭示物理意义的相边界，为传统方法提供了低成本且稳健的替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于稀释伊辛模型缺乏精确解析解，因此需要探索有效的学习方法来理解其相结构。

Method: 使用简单的密集神经网络进行监督分类，以及仅基于理想自旋配置训练的卷积自动编码器进行非监督检测。同时，利用架构偏差放大对称性破坏相关特征，并包含显式破坏$\mathbb{Z}_2$对称性的训练配置以增强网络检测有序相的能力。

Result: 机器学习预测与直接数值估计的关键温度和渗透阈值一致，证明了该方法在揭示复杂系统中物理意义相边界的有效性。

Conclusion: 合成、结构化且计算高效的训练方案可以揭示物理意义的相边界，为传统方法提供了低成本且稳健的替代方案，具有广泛的应用前景。

Abstract: We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.

</details>


### [795] [Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training](https://arxiv.org/abs/2505.10393)
*Agustin Medina,Marcelo Arlego,Carlos A. Lamas*

Main category: cond-mat.str-el

TL;DR: 本文探讨了利用人工神经网络高效学习磁相的方法，结合计算简单性和物理指导策略。通过两种互补方法（监督分类和无监督检测）研究稀释伊辛模型的相变行为，并通过引入物理指导增强模型性能。结果表明，这种方法能够揭示复杂系统中的物理相边界，具有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 我们旨在通过人工神经网络高效学习磁相，结合计算简单性和物理指导策略。由于稀释伊辛模型缺乏精确的解析解，因此需要探索新的方法来研究其相变行为。

Method: 我们使用人工神经网络对合成数据进行高效学习，结合计算简单性和物理指导策略。我们专注于稀释伊辛模型，探索两种互补方法：使用简单密集神经网络的监督分类，以及使用仅在理想自旋配置上训练的卷积自编码器的无监督相变检测。为了提高模型性能，我们引入了两种关键的物理指导形式：首先，我们利用架构偏差，优先放大与对称性破缺相关的特征。其次，我们在训练配置中包括明确打破Z2对称性的配置，以增强网络检测有序相的能力。

Result: 我们的结果表明，合成、结构化且计算高效的训练方案可以揭示复杂系统中的物理上有意义的相边界。这种方法提供了一种低成本且稳健的替代传统方法的方案，并具有在更广泛的凝聚态物理和统计物理背景下的应用潜力。

Conclusion: 我们的结果表明，合成、结构化且计算高效的训练方案可以在复杂系统中揭示物理上有意义的相边界。这种框架提供了一种低成本且稳健的替代传统方法的方法，具有在更广泛的凝聚态物理和统计物理背景下的应用潜力。

Abstract: We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [796] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

TL;DR: The paper introduces SALM, a new framework that integrates language models into social network simulations. It features a hierarchical prompting architecture, an attention-based memory system, and formal bounds on personality stability, enabling stable long-term simulations with reduced token usage and efficient memory management.


<details>
  <summary>Details</summary>
Motivation: Current agent-based modeling methods for social systems rely heavily on rule-based behaviors, which limits their ability to capture complex dynamics. The motivation is to move beyond predefined rules by leveraging contextual understanding from language models of human social interaction.

Method: The method involves creating SALM (Social Agent LM Framework), which includes a hierarchical prompting architecture for stable simulation, an attention-based memory system for efficient caching, and formal bounds on personality stability.

Result: SALM achieves unprecedented temporal stability in multi-agent scenarios, reducing token usage by 73% and achieving 80% cache hit rates with sub-linear memory growth. It is validated against SNAP ego networks, demonstrating its capability to model long-term social phenomena accurately.

Conclusion: SALM represents a significant advancement in integrating language models into social network simulations, providing a robust framework for modeling long-term social dynamics with high behavioral fidelity.

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>


### [797] [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
*Sulong Zhou,Qunying Huang,Shaoheng Zhou,Yun Hang,Xinyue Ye,Aodong Mei,Kathryn Phung,Yuning Ye,Uma Govindswamy,Zehan Li*

Main category: cs.SI

TL;DR: This study analyzes Reddit discourse during the 2025 Los Angeles wildfires using topic modeling methods enhanced by LLMs and HITL refinement, identifying two main categories of topics (Situational Awareness and Crisis Narratives) and contributing an annotated social media dataset.


<details>
  <summary>Details</summary>
Motivation: To understand how affected populations perceive and respond during wildfire crises for timely and empathetic disaster response.

Method: Adopting topic modeling methods enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement to analyze Reddit posts and comments related to the Palisades and Eaton fires.

Result: Identified two main categories of latent topics: Situational Awareness (SA) and Crisis Narratives (CN). SA closely aligns with real-world fire progressions and peaks within the first 2-5 days. CN includes grief signals and mental health risks, with the highest volume occurring at night.

Conclusion: Contributed the first annotated social media dataset on the 2025 LA fires and introduced a scalable multi-layer framework for crisis discourse analysis, which can inform more empathetic and adaptive strategies for disaster response.

Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.

</details>


### [798] [Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion](https://arxiv.org/abs/2505.10197)
*Anjali de Silva,Gang Chen,Hui Ma,Seyed Mohammad Nekooei,Xingquan Zuo*

Main category: cs.SI

TL;DR: 社区检测技术对于实际应用至关重要，它通过利用社交网络中的拓扑和属性相似性来发现连贯的节点组（社区）。现有的图卷积网络（GCNs）在最大化模块度时通常收敛到次优解。此外，直接使用人工标注的社区进行训练可能会通过仅基于节点属性对不相连的节点进行分组而削弱拓扑连贯性。为了解决这些问题，我们提出了一个新的基于拓扑和属性相似性的社区检测（TAS-Com）方法。TAS-Com引入了一种新的损失函数，利用高效且可扩展的Leiden算法来检测具有全局最优模块度的社区结构。进一步利用Leiden算法优化人工标注的社区，以确保每个社区内的连通性，使TAS-Com能够检测到在模块度和符合人工标签之间具有良好权衡的社区结构。多个基准网络上的实验结果证实，TAS-Com可以显著优于几种最先进的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积网络（GCNs）在最大化模块度时通常收敛到次优解，并且直接使用人工标注的社区进行训练可能会通过仅基于节点属性对不相连的节点进行分组而削弱拓扑连贯性。

Method: 提出了一种新的基于拓扑和属性相似性的社区检测（TAS-Com）方法。该方法引入了一种新的损失函数，利用高效且可扩展的Leiden算法来检测具有全局最优模块度的社区结构，并进一步利用Leiden算法优化人工标注的社区，以确保每个社区内的连通性。

Result: 多个基准网络上的实验结果证实，TAS-Com可以显著优于几种最先进的算法。

Conclusion: TAS-Com方法能够有效解决现有GCNs收敛到次优解以及人工标注社区可能削弱拓扑连贯性的问题，在社区检测任务中表现出色。

Abstract: Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.

</details>


### [799] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

TL;DR: 本文介绍了SALM框架，这是一种将语言模型整合到社交网络模拟中的新方法，能够在多智能体场景中实现前所未有的时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的行为方法限制了社会系统代理建模的能力，无法超越预定义规则并利用人类社会互动的上下文理解。

Method: 本文提出了一种层次化提示架构和基于注意力的记忆系统，以实现多智能体场景中的时间稳定性。

Result: 通过与SNAP ego networks的广泛验证，证明了SALM框架在长时间尺度上建模社会现象的能力，并实现了80%的缓存命中率和9.5%的次线性内存增长。

Conclusion: 本文提出了SALM框架，这是第一个能够建模长期社会现象并保持经验验证的行为保真度的LLM框架。

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>


### [800] [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
*Sulong Zhou,Qunying Huang,Shaoheng Zhou,Yun Hang,Xinyue Ye,Aodong Mei,Kathryn Phung,Yuning Ye,Uma Govindswamy,Zehan Li*

Main category: cs.SI

TL;DR: 本研究分析了2025年洛杉矶野火期间Reddit上的讨论，使用主题建模方法结合大型语言模型和人工优化，开发了一个分层框架来分类潜在主题。研究发现，情境意识与火灾进展密切相关，而危机叙事则涉及大量心理健康和悲伤信号。


<details>
  <summary>Details</summary>
Motivation: 了解受影响人口在野火危机中如何感知和响应对于及时和富有同理心的灾害响应至关重要。社交媒体平台提供了一种众包渠道来捕捉不断变化的公众讨论，提供了超本地信息和公众情绪的见解。

Method: 本研究采用主题建模方法，结合大型语言模型（LLMs）和人工在环（HITL）优化，开发了一个分层框架来对潜在主题进行分类，包括两个主要类别：情境意识（SA）和危机叙事（CN）。

Result: SA类别的数量与现实中的火灾进展紧密一致，在火灾达到最大范围的前2-5天达到峰值。最常见的共同出现类别是公共卫生和安全、损失和损害以及应急资源，涵盖了广泛的健康相关潜在主题，包括环境健康、职业健康和一个健康。悲伤信号和心理健康风险分别占CN实例的60%和40%，最高总数量出现在夜间。

Conclusion: 本研究贡献了第一个关于2025年洛杉矶火灾的注释社交媒体数据集，并引入了一个可扩展的多层框架，利用主题建模进行危机话语分析。通过识别持续的公共卫生问题，我们的结果可以为更富有同理心和适应性的灾害应对、公共卫生传播和未来类似气候相关灾害事件的研究提供参考。

Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.

</details>


### [801] [Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion](https://arxiv.org/abs/2505.10197)
*Anjali de Silva,Gang Chen,Hui Ma,Seyed Mohammad Nekooei,Xingquan Zuo*

Main category: cs.SI

TL;DR: 本文提出了一种新的社区检测方法TAS-Com，通过结合拓扑和属性相似性，解决了现有方法在模块度优化和人工标签一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积网络（GCNs）在最大化模块度时常常收敛到次优解，而直接使用人工标记的社区进行训练可能会因仅基于节点属性而将不相连的节点分组，从而破坏拓扑紧密性。

Method: 提出了一种基于拓扑和属性相似性的社区检测方法（TAS-Com），引入了新的损失函数，并利用高效的Leiden算法来检测具有全局最优模块度的社区结构。

Result: 实验结果表明，TAS-Com在多个基准网络上能够显著优于几种最先进的算法。

Conclusion: TAS-Com可以显著优于几种最先进的算法。

Abstract: Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [802] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

TL;DR: TensorRL-QAS 是一种结合张量网络方法与强化学习的可扩展框架，用于设计量子电路。通过矩阵乘积态近似目标解来初始化架构搜索，从而缩小搜索空间并加速收敛到理想解。在多个量子化学问题上，该方法显著减少了 CNOT 数量和电路深度，并提高了训练效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法有潜力解决嘈杂中等规模量子硬件上的有意义问题，但设计既符合设备限制又能解决问题的量子电路具有挑战性。基于强化学习的量子架构搜索（QAS）方法虽然有希望，但面临严重的可扩展性问题。

Method: 引入了 TensorRL-QAS 框架，将张量网络方法与强化学习结合，以设计量子电路。使用矩阵乘积态近似目标解来初始化架构搜索，从而缩小搜索空间至物理上有意义的电路。

Result: 在最多 12 量子比特的多个量子化学问题上，TensorRL-QAS 将 CNOT 数量和电路深度减少多达 10 倍，同时保持或超越化学精度。它将函数评估减少多达 100 倍，加速训练回合多达 98%，并在 10 量子比特系统上实现高达 50% 的成功概率。在无噪声和有噪声场景中均表现出鲁棒性和多样性。

Conclusion: TensorRL-QAS 被确立为一个有前途的候选方案，用于在近期量子硬件上实现可扩展和高效的量子电路发现协议。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [803] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: The paper presents Quantum Parameter Adaptation (QPA), a hybrid quantum-classical framework for efficient typhoon forecasting model learning, which significantly reduces trainable parameters while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Typhoon trajectory forecasting is crucial but computationally intensive due to atmospheric dynamics complexity and deep learning resource demands.

Method: Quantum Parameter Adaptation (QPA) is introduced, leveraging quantum neural networks (QNNs) only during training as part of the Quantum-Train (QT) framework, integrated with an Attention-based Multi-ConvGRU model for parameter-efficient training.

Result: QPA reduces the number of trainable parameters significantly while preserving predictive performance.

Conclusion: This work marks the first application of quantum machine learning to large-scale typhoon trajectory prediction, providing a scalable and energy-efficient method for climate modeling.

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [804] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

TL;DR: This paper explores work extraction protocols for transferring maximum energy to a battery from unknown pure qubit states, using sequential access and adaptive strategies inspired by reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve energy harvesting from qubits by balancing the charging of the battery optimally with acquiring more information about the qubit state to enhance future rounds of energy extraction.

Method: The method involves leveraging exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies for optimal energy extraction from qubits.

Result: The result is an exponential improvement over current protocols based on full state tomography, with energy dissipation scaling only poly-logarithmically in $N$ copies of the qubit state.

Conclusion: Adaptive strategies developed using reinforcement learning principles provide a significant enhancement in energy extraction efficiency compared to traditional methods.

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


### [805] [Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation](https://arxiv.org/abs/2505.09653)
*Samuel Yen-Chi Chen,Chen-Yu Liu,Kuan-Cheng Chen,Wei-Jia Huang,Yen-Jui Chang,Wei-Hao Huang*

Main category: quant-ph

TL;DR: The paper proposes an automated solution using differentiable optimization to design quantum neural networks (QNNs) that can generate classical neural network parameters. This method jointly optimizes both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for effective quantum circuit architectures for quantum-enhanced neural programmers, which remains non-trivial and often requires expertise in quantum information science.

Method: The method uses differentiable optimization to jointly optimize both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.

Result: Simulation results show that the proposed method matches or outperforms manually designed QNN architectures on classification, time-series prediction, and reinforcement learning tasks.

Conclusion: This work offers a scalable and automated pathway for designing QNNs that can generate classical neural network parameters across diverse applications.

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have led to the emergence of quantum machine learning (QML), which integrates
the strengths of both fields. Among QML approaches, variational quantum
circuits (VQCs), also known as quantum neural networks (QNNs), have shown
promise both empirically and theoretically. However, their broader adoption is
hindered by reliance on quantum hardware during inference. Hardware
imperfections and limited access to quantum devices pose practical challenges.
To address this, the Quantum-Train (QT) framework leverages the exponential
scaling of quantum amplitudes to generate classical neural network parameters,
enabling inference without quantum hardware and achieving significant parameter
compression. Yet, designing effective quantum circuit architectures for such
quantum-enhanced neural programmers remains non-trivial and often requires
expertise in quantum information science. In this paper, we propose an
automated solution using differentiable optimization. Our method jointly
optimizes both conventional circuit parameters and architectural parameters in
an end-to-end manner via automatic differentiation. We evaluate the proposed
framework on classification, time-series prediction, and reinforcement learning
tasks. Simulation results show that our method matches or outperforms manually
designed QNN architectures. This work offers a scalable and automated pathway
for designing QNNs that can generate classical neural network parameters across
diverse applications.

</details>


### [806] [Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering](https://arxiv.org/abs/2505.10012)
*Tadashi Kadowaki*

Main category: quant-ph

TL;DR: This paper explores the integration of quantum computing and AI in scientific automation, introducing Quantum CAE for tasks like simulation and optimization, with case studies on combinatorial problems. It emphasizes the role of AI agents in quantum algorithm design and questions the future collaboration between humans, AI, and quantum resources.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the recent advances in AI and quantum computing that are reshaping research methodologies, necessitating a framework that can leverage these technologies for enhanced automation in engineering processes.

Method: The method involves using Quantum CAE as a framework which applies quantum algorithms for simulation, optimization, and machine learning within engineering design, supported by practical implementations through case studies focused on combinatorial optimization problems.

Result: The results include successful illustrations of Quantum CAE applications in solving combinatorial optimization problems and discussions on advancements towards higher automation levels, highlighting the importance of specialized AI agents in proficient quantum algorithm design.

Conclusion: The conclusion underscores the transformative potential of integrating quantum computing with AI, posing significant questions about future collaborative dynamics among human scientists, engineers, AI systems, and quantum computational resources.

Abstract: Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.

</details>


### [807] [Role of scrambling and noise in temporal information processing with quantum systems](https://arxiv.org/abs/2505.10080)
*Weijie Xiong,Zoë Holmes,Armando Angrisani,Yudai Suzuki,Thiparat Chotibut,Supanut Thanasilp*

Main category: quant-ph

TL;DR: Quantum scrambling systems are effective for temporal information processing. This paper explores their scalability and memory retention using high-order unitary designs, showing exponential concentration of readouts with reservoir size but without worsening over iterations in noiseless settings. However, scaling problem size deteriorates generalization unless an exponential shot overhead is affordable. Memory of early inputs decays exponentially in both reservoir size and iterations, also proven for noisy channels.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical performance of quantum scrambling systems in temporal tasks, specifically focusing on scalability and memory retention.

Method: A general quantum reservoir processing framework was used to examine models with quantum systems. High-order unitary designs were applied to model reservoirs in both noiseless and noisy settings. New proof techniques were introduced to bound concentration in temporal quantum learning models.

Result: In noiseless settings, measurement readouts concentrate exponentially with increasing reservoir size without worsening over iterations. Memory of early inputs decays exponentially in both reservoir size and iterations. In noisy settings, exponential memory decay with iterations was proven for local noisy channels.

Conclusion: Quantum scrambling systems show potential for temporal information processing but face challenges in scalability and memory retention. Theoretical analysis reveals that while small reservoir reusability is possible, scaling up requires significant resources.

Abstract: Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.

</details>


### [808] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

TL;DR: TensorRL-QAS 是一种结合张量网络和强化学习的可扩展框架，用于设计量子电路，显著提高了性能并减少了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了应对基于强化学习的量子架构搜索（QAS）方法在可扩展性方面的挑战，特别是在量子比特数量、电路深度和噪声增加时计算和训练成本迅速增长的问题。

Method: TensorRL-QAS 结合了张量网络（TN）方法和强化学习（RL）来设计量子电路，并通过矩阵乘积状态近似目标解决方案来启动架构搜索。

Result: TensorRL-QAS 在最多 12 个量子比特的量子化学问题上实现了 CNOT 数量和电路深度减少 10 倍，同时保持或超越化学精度。它减少了功能评估多达 100 倍，加速了训练周期高达 98%，并在 10 量子比特系统中实现了高达 50% 的成功率，远超基线方法的 <1%。

Conclusion: TensorRL-QAS 是一种有前途的可扩展和高效的量子电路发现协议，适用于近期的量子硬件。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [809] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 本文提出了一种名为量子参数适应（QPA）的新方法，用于高效台风轨迹预测。QPA与基于注意力的多卷积门控循环单元模型结合，减少了可训练参数数量，同时保持了预测准确性，为量子机器学习在气候建模中的应用提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对于灾害准备至关重要，但由于大气动力学的复杂性和深度学习模型的资源需求，仍然计算上很耗时。为了提高效率和可持续性，本文提出了QPA方法。

Method: 本文引入了量子参数适应（QPA），这是一种高效的台风预测模型学习方法。QPA与基于注意力的多卷积门控循环单元（Multi-ConvGRU）模型集成，实现了参数高效的训练，同时保持了预测准确性。

Result: QPA显著减少了可训练参数的数量，同时保持了预测性能，使得高性能的台风轨迹预测更加可行和可持续。

Conclusion: 本研究代表了量子机器学习（QML）在大规模台风轨迹预测中的首次应用，提供了一种可扩展且节能的气候建模方法。结果表明，QPA显著减少了可训练参数的数量，同时保持了性能，使高性能预测更加可行和可持续。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [810] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

TL;DR: 本文提出了一种基于强化学习的自适应策略，用于在未知纯量子态副本中提取最大可能的能量，并实现了能量耗散仅随N的多项式对数增长，这比当前基于完整状态层析的协议有指数级的改进。


<details>
  <summary>Details</summary>
Motivation: 本文旨在设计一种工作提取协议，以使用顺序访问N个未知纯量子态副本的最大可能能量传递到电池中，并优化电池充电和获取更多信息之间的平衡。

Method: 本文利用强化学习中的探索与利用权衡来开发自适应策略，以实现能量耗散仅随N的多项式对数增长。

Result: 本文提出的策略实现了能量耗散仅随N的多项式对数增长，这比当前基于完整状态层析的协议有指数级的改进。

Conclusion: 本文提出了一种基于强化学习中探索与利用权衡的自适应策略，实现了能量耗散仅随N的多项式对数增长，这比当前基于完整状态层析的协议有指数级的改进。

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


### [811] [Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation](https://arxiv.org/abs/2505.09653)
*Samuel Yen-Chi Chen,Chen-Yu Liu,Kuan-Cheng Chen,Wei-Jia Huang,Yen-Jui Chang,Wei-Hao Huang*

Main category: quant-ph

TL;DR: 本文提出了一种基于可微分优化的自动化方法，用于设计量子神经网络（QNN），可以在不依赖量子硬件的情况下生成经典神经网络参数，并在多种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管变分量子电路（VQCs）在量子机器学习（QML）中显示出潜力，但其广泛应用受到对量子硬件依赖的限制。因此，需要一种自动化的方法来设计有效的量子电路架构。

Method: 本文提出了一种基于可微分优化的自动化方法，联合优化传统电路参数和架构参数，通过自动微分实现端到端优化。

Result: 实验结果表明，所提出的方法在分类、时间序列预测和强化学习任务中表现与手动设计的QNN架构相当或更优。

Conclusion: 本文提出了一种自动化的方法，通过可微分优化来设计量子神经网络（QNN），该方法能够在不依赖量子硬件的情况下生成经典神经网络参数，并在多种应用中表现出色。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have led to the emergence of quantum machine learning (QML), which integrates
the strengths of both fields. Among QML approaches, variational quantum
circuits (VQCs), also known as quantum neural networks (QNNs), have shown
promise both empirically and theoretically. However, their broader adoption is
hindered by reliance on quantum hardware during inference. Hardware
imperfections and limited access to quantum devices pose practical challenges.
To address this, the Quantum-Train (QT) framework leverages the exponential
scaling of quantum amplitudes to generate classical neural network parameters,
enabling inference without quantum hardware and achieving significant parameter
compression. Yet, designing effective quantum circuit architectures for such
quantum-enhanced neural programmers remains non-trivial and often requires
expertise in quantum information science. In this paper, we propose an
automated solution using differentiable optimization. Our method jointly
optimizes both conventional circuit parameters and architectural parameters in
an end-to-end manner via automatic differentiation. We evaluate the proposed
framework on classification, time-series prediction, and reinforcement learning
tasks. Simulation results show that our method matches or outperforms manually
designed QNN architectures. This work offers a scalable and automated pathway
for designing QNNs that can generate classical neural network parameters across
diverse applications.

</details>


### [812] [Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering](https://arxiv.org/abs/2505.10012)
*Tadashi Kadowaki*

Main category: quant-ph

TL;DR: 本文探讨了量子计算与AI结合的潜力，提出了量子CAE框架，并通过案例研究展示了其在工程设计中的应用，强调了未来自动化发现和创新的可能性。


<details>
  <summary>Details</summary>
Motivation: 近年来，人工智能和量子计算的进步正在加速科学和工程过程的自动化，这促使研究者探索如何将量子计算与AI结合，以提升工程设计的效率和创新能力。

Method: 本文通过案例研究展示了量子CAE的实际应用，并讨论了向更高自动化水平发展的进展。

Result: 本文介绍了量子CAE框架，并通过案例研究展示了其在组合优化问题中的应用。同时，讨论了AI代理在量子算法设计中的关键作用以及人机协作的重要性。

Conclusion: 本文提出了量子CAE作为一种框架，利用量子算法进行模拟、优化和机器学习，以提高工程设计的自动化水平。同时，它强调了人类科学家、工程师、AI系统和量子计算资源之间的协作动态的重要性，展望了自动化发现和创新的未来。

Abstract: Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.

</details>


### [813] [Role of scrambling and noise in temporal information processing with quantum systems](https://arxiv.org/abs/2505.10080)
*Weijie Xiong,Zoë Holmes,Armando Angrisani,Yudai Suzuki,Thiparat Chotibut,Supanut Thanasilp*

Main category: quant-ph

TL;DR: 本文分析了量子储层处理框架在时间任务中的性能，发现其在无噪声环境下具有良好的可扩展性，但在扩大问题规模时需要付出指数级的采样成本。此外，在噪声环境中，早期输入和初始状态的记忆会迅速衰减。


<details>
  <summary>Details</summary>
Motivation: 尽管量子混洗系统在提供丰富的特征映射方面已被广泛研究，但对其在时间任务中的性能的理论理解仍然缺乏。因此，本文旨在分析量子储层处理框架的性能，特别是在可扩展性和记忆保持方面的表现。

Method: 本文提出了一种通用的量子储层处理框架，该框架涵盖了广泛的物理计算模型，并通过高阶酉设计对混洗储层进行了建模。研究了在无噪声和有噪声环境下，模型的可扩展性和记忆保持能力。

Result: 在无噪声环境下，测量读数随着储层大小的增加而呈指数集中，但不会因储层迭代而恶化。然而，扩大问题规模会损害泛化能力，除非能够承担指数级的采样开销。在噪声环境中，早期输入和初始状态的记忆会随着储层大小和迭代次数呈指数衰减。

Conclusion: 本文结论是，虽然重复使用小规模的量子混洗储层在处理量子数据时可能是可行的，但扩大问题规模会损害泛化能力，除非能够承担指数级的采样开销。此外，在噪声环境中，早期输入和初始状态的记忆会随着储层大小和迭代次数呈指数衰减。

Abstract: Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [814] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

TL;DR: The paper presents a Multi-Agent Reinforcement Learning (MARL) algorithm for localizing multiple airborne pollution sources using small uncrewed aerial systems, significantly outperforming conventional approaches.


<details>
  <summary>Details</summary>
Motivation: Industrial catastrophes demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment.

Method: The method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN). It uses full sequences of historical action-observation pairs, effectively approximating latent states. A general-purpose simulation environment based on the Gaussian Plume Model (GPM) is used, incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources.

Result: Extensive simulations show that the algorithm significantly outperforms conventional approaches, with agents exploring only 1.29% of the environment to successfully locate pollution sources.

Conclusion: The MARL algorithm provides an effective solution for localizing multiple airborne pollution sources in complex, partially observable environments.

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [815] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/abs/2505.10387)
*Artem Agafonov,Konstantin Yakovlev*

Main category: cs.MA

TL;DR: 在多智能体路径寻找（MAPF）问题中，考虑智能体大小会使问题变得更加复杂。本文证明了带有大型智能体的MAPF问题是NP难问题，这意味着如果P!=NP，则不存在多项式时间算法可以解决该问题。


<details>
  <summary>Details</summary>
Motivation: 传统的MAPF问题假设智能体没有大小，并仅考虑两种冲突类型：在同一时间步占用同一顶点或使用同一边。然而，在许多实际应用中（如机器人技术），考虑智能体的大小对于确保MAPF解决方案的安全执行至关重要。因此，研究引入大型智能体后对问题复杂性的影响是有意义的。

Method: 作者通过将著名的3SAT问题（已知为NP完全问题）归约到带有大型智能体的MAPF问题来证明其NP难度。具体而言，他们为任意的3SAT公式构造一个专门的图，并展示出给定的3SAT公式是可满足的当且仅当相应的路径寻找实例有解。

Result: 成功证明了带有大型智能体的MAPF问题是NP难问题。这意味着如果P!=NP，则不存在能够解决该问题的完整多项式时间算法。

Conclusion: 带有大型智能体的MAPF问题是NP难问题，这表明我们需要寻求启发式方法或其他近似算法来解决这一问题，而不是期望存在一个通用的多项式时间算法。

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


### [816] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

TL;DR: 本文提出了一种基于多智能体强化学习的算法，用于使用小型无人航空系统定位多个空气污染源，并在模拟中表现出色。


<details>
  <summary>Details</summary>
Motivation: 工业灾难表明，需要快速可靠的烟羽追踪算法来保护公共健康和环境。传统方法在现实的湍流条件下往往失败。

Method: 我们提出了一种基于多智能体强化学习（MARL）的算法，用于使用小型无人航空系统（sUAS）定位多个空气污染源。该方法将问题建模为部分可观测马尔可夫博弈（POMG），并采用基于长短期记忆（LSTM）的特定动作双深度递归Q网络（ADDRQN）。

Result: 我们的算法在广泛的模拟中显著优于传统方法。具体来说，我们的模型允许代理仅探索1.29%的环境即可成功定位污染源。

Conclusion: 我们的算法在复杂、部分可观测的环境中表现出色，能够有效地定位污染源。

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [817] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/abs/2505.10387)
*Artem Agafonov,Konstantin Yakovlev*

Main category: cs.MA

TL;DR: 本文首次证明了考虑大代理的多智能体路径规划问题NP难，表明在P≠NP的情况下，无法为该问题提供多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，如机器人领域，考虑代理的大小对于确保MAPF解决方案能够安全执行至关重要。然而，目前尚不清楚当考虑这些冲突时，问题的难度会增加多少。

Method: 通过将经典的3SAT问题（已知是NP完全问题）归约到该问题，从而证明了该问题的NP难性。

Result: 本文首次证明了考虑大代理的MAPF问题是NP难的。

Conclusion: 本文首次证明了考虑大代理的MAPF问题NP难，因此如果P≠NP，则无法为该问题提供多项式时间算法。

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [818] [Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems](https://arxiv.org/abs/2505.09734)
*Babak Esmaeili,Nariman Niknejad,Hamidreza Modares*

Main category: eess.SY

TL;DR: This paper proposes a risk-aware safe reinforcement learning control design for stochastic discrete-time linear systems, integrating safety and optimality through a novel interpolation technique.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods that rely on myopic interventions or high-fidelity models in ensuring safety for RL-controlled stochastic systems.

Method: Learning a risk-informed safe controller alongside an RL controller and combining them using a data-driven interpolation technique. Utilizing piecewise affine controllers to expand the invariant set and optimizing over a scalar decision variable to minimize safety violations.

Result: Achieves high-confidence safety without needing a high-fidelity model, avoids undesired equilibria, provides efficient solutions, reduces data requirements, and minimizes variance of safety violations.

Conclusion: The proposed approach successfully integrates safety and optimality in RL control for stochastic systems, validated through a simulation example.

Abstract: This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.

</details>


### [819] [A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024](https://arxiv.org/abs/2505.10367)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Songyuan Liu,Jinming Yu*

Main category: eess.SY

TL;DR: This paper presents team GEB's solution that ranked 3rd in trading, 4th in forecasting, and 1st among student teams in HEFTCom2024. They provide accurate probabilistic forecasts for a wind-solar hybrid system and achieve substantial trading revenue.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of obtaining accurate probabilistic energy forecasts and making effective decisions amid diverse uncertainties in future energy systems.

Method: Key components include: (1) a stacking-based approach combining sister forecasts from various NWPs to provide wind power forecasts; (2) an online solar post-processing model to address the distribution shift in the online test set caused by increased solar capacity; (3) a probabilistic aggregation method for accurate quantile forecasts of hybrid generation; (4) a stochastic trading strategy to maximize expected trading revenue considering uncertainties in electricity prices.

Result: Substantial trading revenue in the day-ahead electricity market was achieved.

Conclusion: The proposed methods were validated through detailed case studies, and code is available for reproduction and further research.

Abstract: Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.

</details>


### [820] [Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems](https://arxiv.org/abs/2505.09734)
*Babak Esmaeili,Nariman Niknejad,Hamidreza Modares*

Main category: eess.SY

TL;DR: 本文提出了一种风险感知的安全强化学习控制设计，通过结合RL控制器和安全控制器，实现了高置信度的安全性，并减少了对高保真系统模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的安全认证方法可能过于短视，无法有效避免不期望的平衡点。因此，需要一种更全面的方法来确保系统的安全性，同时减少对高保真系统模型的依赖。

Method: 本文提出了一种风险感知的安全控制器，与RL控制器相结合。通过优化标量决策变量和线性规划多面体集，实现了高效且计算上可行的解决方案。此外，引入了一种新的数据驱动插值技术，以确保RL代理在噪声环境中的安全性。

Result: 本文提出的方法能够减少数据需求，并降低安全违规的方差。通过仿真示例验证了理论结果的有效性。

Conclusion: 本文提出了一种风险感知的安全强化学习（RL）控制设计，用于随机离散时间线性系统。通过结合RL控制器和安全控制器，实现了高置信度的安全性，并减少了对高保真系统模型的依赖。最后，通过仿真示例验证了理论结果。

Abstract: This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.

</details>


### [821] [A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024](https://arxiv.org/abs/2505.10367)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Songyuan Liu,Jinming Yu*

Main category: eess.SY

TL;DR: 本文介绍了团队GEB在IEEE混合能源预测和交易竞赛2024中的解决方案，该方案通过多种方法提高了风-光混合系统的预测准确性和交易收益，并展示了其在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在未来的能源系统中，获取准确的概率能源预测并应对各种不确定性是常规挑战。本文旨在提供一种有效的解决方案，以提高风-光混合系统的预测准确性和交易收益。

Method: 本文提出的方法包括基于堆叠的风力发电预测方法、在线太阳能后处理模型、概率聚合方法以及随机交易策略。此外，还探索了端到端学习以进一步提高交易收益。

Result: 团队GEB在HEFTCom2024比赛中取得了第3名、第4名和学生团队第1名的好成绩。提出的解决方案在风-光混合系统的概率预测和交易收益方面表现出色。

Conclusion: 本文提出了团队GEB的解决方案，该方案在IEEE混合能源预测和交易竞赛2024（HEFTCom2024）中取得了优异的成绩，并展示了其在风-光混合系统中的准确概率预测和显著交易收益的能力。

Abstract: Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [822] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

TL;DR: Open-source AI libraries are crucial for modern AI systems but pose significant risks. LibVulnWatch is a graph-based framework that performs deep evaluations of these libraries, generating reproducible scores across five critical domains and publishing them to a public leaderboard.


<details>
  <summary>Details</summary>
Motivation: Open-source AI libraries pose significant risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance.

Method: LibVulnWatch uses a graph-based agentic assessment framework built on LangGraph. It coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases.

Result: Applied to 20 widely used libraries, LibVulnWatch covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library, including critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability.

Conclusion: LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [823] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

TL;DR: This paper surveys the security and privacy issues in Internet of Agents (IoA) systems, which are composed of autonomous AI agents. It outlines the architecture of IoA, its unique vulnerabilities in aspects like identity authentication and cross-agent trust, reviews defense mechanisms, and points out open research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to provide a comprehensive examination of the security and privacy landscape in IoA systems as these systems become more prevalent and important for coordination among different AI agents.

Method: The method involves reviewing literature on IoA architecture and its vulnerabilities, analyzing existing and emerging defense mechanisms against identified threats, and identifying gaps and challenges in current research.

Result: The result is an overview of the state-of-the-art in securing IoA systems, including insights into the specific threats such as identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. The review also highlights persistent challenges in creating secure and private IoA ecosystems.

Conclusion: The conclusion emphasizes the need for further research in developing resilient and privacy-preserving IoA systems, suggesting several open research directions.

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [824] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

TL;DR: This paper proposes MixBridge, a novel diffusion Schrödinger bridge framework for arbitrary input distributions, and addresses the challenges of implanting multiple heterogeneous backdoor triggers using a Divide-and-Merge strategy.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor formulations are mainly designed for single-attack scenarios and limited to Gaussian noise input models. This paper aims to address these limitations by proposing a framework that can cater to complex and arbitrary input distributions.

Method: The proposed method is called MixBridge, a diffusion Schrödinger bridge (DSB) framework. It allows for the implantation of multiple heterogeneous backdoor triggers by directly training with poisoned image pairs. To handle the performance conflict across backdoor tasks, a Divide-and-Merge strategy is proposed where models are independently pre-trained and then integrated into a unified model. Additionally, a Weight Reallocation Scheme (WRS) is designed to enhance the stealthiness of MixBridge.

Result: Empirical studies across diverse generation tasks demonstrate the efficacy of MixBridge in handling arbitrary input distributions and successfully implanting multiple backdoor triggers while maintaining performance.

Conclusion: MixBridge provides a flexible tool to study backdoor behavior for bridge models and effectively addresses the challenge of training multiple backdoor triggers in a single DSB model through the Divide-and-Merge strategy.

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [825] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

TL;DR: 本研究提出了一种强大且可扩展的框架，用于检测高速公路VANET环境中的DDoS攻击，使用XGBoost和CatBoost实现了96%的F1分数，展示了其在保障紧急通信方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 车辆自组织网络（VANET）在智能交通系统中起着关键作用，特别是在为应急车辆提供实时通信方面。然而，分布式拒绝服务（DDoS）攻击会干扰安全关键的通信信道，严重影响其可靠性，因此需要一种强大的方法来检测这些攻击。

Method: 构建了一个合成数据集，使用NS-3网络模拟器与城市交通模拟（SUMO）结合，并通过OpenStreetMap提取了德国A81高速公路的实际移动轨迹。模拟了三种流量类别：DDoS、VoIP和基于TCP的视频流（VideoTCP）。数据预处理管道包括归一化、信噪比（SNR）特征工程、缺失值填补以及使用合成少数类过采样技术（SMOTE）进行类别平衡。使用SHapley Additive exPlanations (SHAP)评估特征重要性。基准测试了11个分类器，其中包括XGBoost、CatBoost、AdaBoost、GradientBoosting和人工神经网络（ANN）。

Result: XGBoost和CatBoost表现最佳，分别达到了96%的F1分数。

Conclusion: 该框架展示了其在VANET中实时部署的鲁棒性和潜力，以确保关键的应急通信。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [826] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: The paper reviews the advancements and challenges in Federated Large Language Models (FLLM), focusing on feasibility, robustness, security, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of the latest advancements in FLLM and identify key challenges and future research directions.

Method: Conducting an exhaustive survey of existing studies on FLLM feasibility, introducing methods to enhance robustness, analyzing privacy and security risks, reviewing defense mechanisms, and exploring promising future research areas.

Result: Analyzed the challenges from four critical perspectives (feasibility, robustness, security, and future directions) and highlighted the need for further research in enhancing system robustness and security.

Conclusion: FLLM faces significant challenges but holds great potential. Future research should focus on improving robustness, security, and addressing unique integration challenges.

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [827] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

TL;DR: This paper explores the vulnerabilities of AI-based automated checkout systems in unmanned stores through adversarial patch attacks, evaluates their effectiveness digitally and physically, proposes a new color histogram similarity loss function, and suggests defense strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expose and address the security vulnerabilities in AI-based automated checkout systems used in unmanned stores, which can be exploited by adversarial patch attacks leading to significant issues such as theft and inventory discrepancies.

Method: The method involves investigating three types of adversarial patch attacks (Hiding, Creating, and Altering), introducing a novel color histogram similarity loss function, proposing a new bounding-boxes-based metric for analysis, evaluating these attacks both digitally on object detection models and physically in a simulated unmanned store environment, and assessing the robustness of these attacks in black-box scenarios.

Result: The results show that adversarial patches can severely disrupt object detection models in unmanned stores, with high success rates even in physical environments and black-box scenarios. The proposed color histogram similarity loss function and bounding-boxes-based metric provide new ways to evaluate attack impacts.

Conclusion: The study concludes by emphasizing the need for robust defense mechanisms to protect unmanned stores from adversarial threats, highlighting current limitations in real-time detection systems, and suggesting proactive measures to improve model robustness.

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [828] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

TL;DR: The paper explores the trade-offs of model performance, privacy and explainability when using Differential Privacy to counteract model extraction attacks in MLaaS platforms that incorporate explainable AI.


<details>
  <summary>Details</summary>
Motivation: MLaaS platforms are becoming increasingly important for deploying predictive models. However, these platforms face security and privacy challenges such as model extraction attacks which can be facilitated by explainable AI's counterfactual explanations.

Method: Investigates two distinct Differential Privacy strategies - one implemented during classification model training and another at the explainer during counterfactual generation - to evaluate their impact on model performance, privacy, and explainability.

Result: Not explicitly stated in the abstract but expected to provide insights into how each DP strategy affects the balance among model performance, privacy, and explainability.

Conclusion: Differential Privacy shows promise in mitigating counterfactual-facilitated model extraction attacks while managing the trade-offs among model performance, privacy, and explainability.

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [829] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本研究提出了新的隐私保护对齐算法，结合DPO和RLHF技术，在适度的隐私预算下实现最先进的性能。DP-AdamW算法显著提高了对齐质量（最高达15%），并探讨了隐私、效能与计算需求之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的对齐过程虽重要，但涉及敏感用户数据，引发隐私问题。尽管已有工作将差分隐私（DP）与对齐技术结合，但其效果仍有限，需要改进。

Method: 提出新的隐私保护对齐算法，适用于直接偏好优化（DPO）和基于人类反馈的强化学习（RLHF）两种技术。通过系统实验评估这些算法在不同隐私预算下的表现，重点分析DP-AdamW算法的效果。

Result: 所提方法在适度隐私预算下达到最佳性能，其中DP-AdamW与DPO结合可使对齐质量提高多达15%。同时明确了隐私保障、对齐效果和计算需求之间的关系。

Conclusion: 新算法在隐私保护前提下有效提升了语言模型的对齐质量，并为优化隐私与效能间的权衡提供了实用指导。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [830] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 研究人员通过假设检验与辅助信息的视角，分析了一次性文本水印中的基本权衡，并提出了优化的水印设计策略，同时提供了理论和实际效果的对比。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的文本几乎与人类生成的内容无法区分，因此需要一种方法在不显著改变输出质量的情况下检测出这些文本是由LLM生成的。

Method: 通过假设检验和辅助信息的视角，研究者们构建了一个框架，用以分析文本水印中检测能力和生成文本质量之间的权衡。他们提出了一种关键的设计组件，即在提供给水印探测器的辅助信息和LLM词汇表的随机划分之间创建耦合。此外，还确定了在最坏情况下满足最小熵约束的最优耦合和随机化策略。

Result: 研究者给出了在提出的方案下检测率的闭式表达式，并以最大最小的方式量化了成本。数值结果表明，该方案在合成数据和LLM水印上均接近理论最优，并优于现有方案。

Conclusion: 本文通过深入分析和实验验证，提供了一种有效的文本水印设计方案，能够在保持文本生成质量的同时提高水印检测能力。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [831] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

TL;DR: The paper introduces TokenProber, a method for sensitivity-aware differential testing to evaluate the robustness of refusal mechanisms in T2I models by generating adversarial prompts. It distinguishes between dirty words and discrepant words to create prompts that can bypass safety filters while maintaining NSFW content generation. The evaluation shows its superior effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of subtly modifying prompts to preserve sensitive nature while bypassing refusal mechanisms in T2I models.

Method: TokenProber conducts fine-grained analysis of specific words within prompts, distinguishing between dirty words essential for NSFW content and discrepant words showing different sensitivity assessments. Through sensitivity-aware mutation, it generates adversarial prompts balancing NSFW content generation and evading detection.

Result: TokenProber demonstrates superior effectiveness in bypassing safety filters compared to existing methods, with a 54%+ increase on average when evaluated against 5 safety checkers on 3 popular T2I models using 324 NSFW prompts.

Conclusion: TokenProber is effective in uncovering robustness issues in existing refusal mechanisms of T2I models.

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [832] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

TL;DR: The paper introduces a self-supervised contrastive learning method using transformer encoders for intrusion detection, which performs better than traditional NetFlow methods in both intra-dataset and inter-dataset evaluations.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for innovative Intrusion Detection Systems (IDS) due to the increase in zero-day attacks. Traditional machine learning-based IDS face challenges in generalization when encountering unseen traffic patterns.

Method: A novel self-supervised contrastive learning approach based on transformer encoders is proposed. It uses packet-level data augmentation and a transformer-based architecture to automatically learn comprehensive packet sequence representations without relying on handcrafted statistical features.

Result: The transformer-based framework outperforms existing NetFlow self-supervised methods with up to 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. It also shows adaptability when fine-tuned across different datasets.

Conclusion: This approach provides a strong baseline for supervised intrusion detection with limited labeled data and demonstrates strong performance even when lacking benign data from the target domain.

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [833] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

TL;DR: The paper proposes a security policy management framework using reinforcement learning (RL) to adapt dynamically in cloud environments, significantly outperforming static policies.


<details>
  <summary>Details</summary>
Motivation: Static security policies have become inadequate due to the complex and dynamic nature of cloud environments, such as Amazon Web Services (AWS).

Method: The proposed method employs deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, leveraging cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies.

Result: Experimental results demonstrate that the adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and reducing incident detection and response times by 58%.

Conclusion: The findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [834] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

TL;DR: The paper presents two contributions, Prioritized Binary Rounding and the sigma-binary attack, to improve adversarial attacks in binary-constrained domains for Android malware detection. Experiments show that current defenses are highly vulnerable.


<details>
  <summary>Details</summary>
Motivation: Current machine learning-based Android malware detectors are vulnerable to evasion attacks, especially due to lack of comprehensive evaluation frameworks in binary-constrained domains.

Method: Introduced Prioritized Binary Rounding for converting continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Also introduced sigma-binary attack which achieves attack goals with minimal feature changes.

Result: The sigma-binary attack outperforms existing methods by achieving over 90% attack success rate using fewer than 10 feature modifications, reaching 100% with just 20. Even strong defenses like PAD-SMA have a 94.56% success rate under unrestricted perturbations.

Conclusion: The findings emphasize the need for precise adversarial methods like sigma-binary to expose vulnerabilities in existing defenses and support development of more robust malware detection systems.

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [835] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: In this paper, the authors propose SafeFL, a new algorithm that accurately identifies malicious clients in federated learning by generating a synthetic dataset from a series of global models. SafeFL shows better performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of current Byzantine-robust aggregation rules and FL detection techniques in identifying malicious clients within the federated learning framework, particularly against advanced threats.

Method: SafeFL involves the server collecting a series of global models to generate a synthetic dataset, which is used to distinguish between malicious and benign models based on their behavior.

Result: Extensive testing demonstrates that SafeFL outperforms existing methods in terms of efficiency and accuracy for detecting malicious clients.

Conclusion: SafeFL is an effective solution for accurately identifying malicious clients in federated learning.

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [836] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

TL;DR: This paper proposes a new method for identifying sybil addresses in blockchain ecosystems using subgraph feature extraction and lightGBM, which performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Sybil attacks are a significant security threat to blockchain ecosystems, especially in token airdrop events.

Method: The method constructs a two-layer deep transaction subgraph for each address, extracts key event operation features according to the lifecycle of sybil addresses, and also extracts amount and network structure features.

Result: Experiments on a dataset with 193,701 addresses show that this method outperforms existing approaches in precision, recall, F1 score, and AUC, with all metrics exceeding 0.9.

Conclusion: This study's methods and results can be applied to broader blockchain security areas like transaction manipulation identification and token liquidity risk assessment, promoting a more secure and fair blockchain ecosystem.

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


### [837] [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
*Yidan Wang,Yanan Cao,Yubing Ren,Fang Fang,Zheng Lin,Binxing Fang*

Main category: cs.CR

TL;DR: This paper explores the use of jailbreak attacks for extracting sensitive information from Large Language Models (LLMs) and introduces PIG, a novel framework targeting Personally Identifiable Information (PII). Experiments on multiple LLMs demonstrate PIG's superior performance compared to existing methods, highlighting significant privacy risks in LLMs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of jailbreak attacks in extracting sensitive information from LLMs and address the limitations of current methods for evaluating privacy leakage.

Method: The proposed framework, PIG, identifies PII entities and their types in privacy queries, builds a privacy context using in-context learning, and iteratively updates it with three gradient-based strategies to elicit target PII.

Result: PIG outperforms baseline methods when evaluated on four white-box and two black-box LLMs using two privacy-related datasets, achieving state-of-the-art results.

Conclusion: The findings reveal significant privacy risks in LLMs and stress the importance of enhancing safeguards.

Abstract: Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.

</details>


### [838] [Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data](https://arxiv.org/abs/2505.09974)
*Adel ElZemity,Budi Arief,Shujun Li*

Main category: cs.CR

TL;DR: The integration of LLMs in cyber security applications has opportunities and risks. Fine-tuning reduces safety resilience across tested LLMs. A safety alignment approach rewords instruction-response pairs to include safety precautions and ethical considerations, maintaining or improving model safety while preserving technical utility.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the safety risks in fine-tuned LLMs for cyber security applications and propose a method to maintain or improve model safety while preserving technical utility.

Method: Using the OWASP Top 10 for LLM Applications framework, seven open-source LLMs were assessed. A safety alignment approach was proposed and evaluated, which rewords instruction-response pairs to include explicit safety precautions and ethical considerations.

Result: Fine-tuning reduces safety resilience across all tested LLMs. The proposed safety alignment approach can maintain or even improve model safety while preserving technical utility.

Conclusion: This work provides a systematic evaluation of safety risks in LLMs, facilitating safer adoption of generative AI in sensitive domains and contributing to the development of secure, trustworthy, and ethically aligned LLMs.

Abstract: The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.

</details>


### [839] [AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons](https://arxiv.org/abs/2505.10273)
*Hexu Li,Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: Vehicle platooning via V2X communication improves fuel efficiency and road use but is susceptible to insider attacks. This paper introduces AttentionGuard, a transformer-based framework using self-attention to detect anomalies in mobility data, achieving up to 0.95 F1-score with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Vehicle platooning systems are vulnerable to falsification attacks by authenticated insiders, which can destabilize formations and cause collisions. There is a need for effective misbehavior detection mechanisms to ensure safety and stability.

Method: AttentionGuard leverages a multi-head transformer-encoder and the self-attention mechanism to process sequential kinematic information from vehicles. It differentiates between normal patterns and falsification attacks across various platooning scenarios including steady-state operation, join, and exit maneuvers.

Result: AttentionGuard achieves an F1-score of up to 0.95 in detecting attacks within diverse attack vectors and operational parameters. The system maintains robust performance during complex maneuvers and operates with minimal latency (100ms decision intervals).

Conclusion: AttentionGuard demonstrates superior detection capabilities for securing Cooperative Intelligent Transport Systems (C-ITS) against insider threats, making it a promising solution for real-time transportation safety applications.

Abstract: Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.

</details>


### [840] [Private Transformer Inference in MLaaS: A Survey](https://arxiv.org/abs/2505.10315)
*Yang Li,Xinyu Zhou,Yitong Wang,Liangxin Qian,Jun Zhao*

Main category: cs.CR

TL;DR: Transformer models, despite their success in AI applications, pose privacy risks when deployed in MLaaS due to centralized data processing. Private Transformer Inference (PTI) mitigates these risks through cryptographic techniques like secure multi-party computation and homomorphic encryption, allowing for private inference. This paper reviews PTI advancements, provides a taxonomy, and evaluates solutions focusing on balancing resource efficiency with privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the increasing privacy concerns associated with deploying transformer models in MLaaS environments, where sensitive user data is processed centrally.

Method: The paper reviews recent advancements in Private Transformer Inference (PTI), which uses cryptographic techniques such as secure multi-party computation and homomorphic encryption to enable private inference. It also introduces a structured taxonomy and evaluation framework to assess PTI solutions.

Result: The paper highlights state-of-the-art PTI solutions and challenges, providing insights into balancing resource efficiency with privacy.

Conclusion: Private Transformer Inference offers a promising approach to address privacy concerns in transformer model deployments, and the introduced taxonomy and evaluation framework will aid in advancing PTI solutions.

Abstract: Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.

</details>


### [841] [AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents](https://arxiv.org/abs/2505.10321)
*Julius Henke*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.

</details>


### [842] [Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts](https://arxiv.org/abs/2505.09843)
*Melissa Turcotte,François Labrèche,Serge-Olivier Paquette*

Main category: cs.CR

TL;DR: AACT系统通过学习分析师对网络安全警报的分类操作，自动化SOC工作流程，显著减少展示给分析师的警报数量，同时保持低误报率。


<details>
  <summary>Details</summary>
Motivation: 企业网络规模日益扩大，安全警报数量激增，导致SOC分析师面临严重的警报疲劳问题。此外，在管理SOC服务中，上下文切换和业务流程可见性不足进一步加剧了这一问题。

Method: 引入名为AACT的新系统，该系统通过分析分析师对网络安全警报的分类行为，实时预测分类决策，从而实现自动关闭良性警报并优先处理关键警报。

Result: 在真实SOC数据和公开数据集上进行训练和评估后，系统表现出高性能，能有效区分恶意和良性警报。在实际应用中，AACT系统在六个月内减少了61%展示给分析师的警报，同时误报率仅为1.36%。

Conclusion: AACT系统成功减少了SOC分析师的工作负担，提高了工作效率，并且具有高准确性和低误报率。

Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack
surface, increasing the volume of security alerts generated from security
controls. Security Operations Centre (SOC) analysts triage these alerts to
identify malicious activity, but they struggle with alert fatigue due to the
overwhelming number of benign alerts. Organisations are turning to managed SOC
providers, where the problem is amplified by context switching and limited
visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by
learning from analysts' triage actions on cybersecurity alerts. It accurately
predicts triage decisions in real time, allowing benign alerts to be closed
automatically and critical ones prioritised. This reduces the SOC queue
allowing analysts to focus on the most severe, relevant or ambiguous threats.
The system has been trained and evaluated on both real SOC data and an open
dataset, obtaining high performance in identifying malicious alerts from benign
alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC
environment, reducing alerts shown to analysts by 61% over six months, with a
low false negative rate of 1.36% over millions of alerts.

</details>


### [843] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

TL;DR: LibVulnWatch is a framework that evaluates open-source AI libraries for risks and provides scores for governance alignment, helping in informed library selection.


<details>
  <summary>Details</summary>
Motivation: Open-source AI libraries pose significant risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance, which are underexamined.

Method: LibVulnWatch is a graph-based agentic assessment framework that performs deep, source-grounded evaluations of open-source AI libraries using evidence from trusted sources such as repositories, documentation, and vulnerability databases.

Result: LibVulnWatch covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library, including critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and gaps in regulatory documentation and auditability.

Conclusion: LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [844] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

TL;DR: 本文是一篇关于IoA系统中安全和隐私问题的综述，涵盖了架构、漏洞、防御机制和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在虚拟和物理领域中的普及，需要对IoA系统中的安全和隐私问题进行深入研究，以确保其可扩展性和安全性。

Method: 本文通过概述IoA架构及其与传统网络的不同漏洞，回顾了现有的防御机制，并指出了持续的挑战。

Result: 本文全面审视了IoA系统中的安全和隐私景观，提出了四个关键方面：身份认证威胁、跨代理信任问题、实体安全和隐私风险，并讨论了现有的防御机制和持续的挑战。

Conclusion: 本文总结了IoA系统中的安全和隐私问题，并指出了未来研究的方向，以推动具有弹性和隐私保护的IoA生态系统的发展。

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [845] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

TL;DR: 本文提出了MixBridge框架，用于在桥接扩散模型中植入多个异构后门触发器，并通过Divide-and-Merge策略和WRS提升效果与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门方法主要针对单次攻击场景，并且局限于高斯噪声输入模型，无法处理复杂和任意输入分布的情况。本文旨在填补这一空白。

Method: 本文提出了一种新的扩散薛定谔桥（DSB）框架MixBridge，通过直接使用中毒图像对进行训练来注入后门触发器，并采用Divide-and-Merge策略和WRS来增强性能和隐蔽性。

Result: 实验结果表明，MixBridge在各种生成任务中表现出色，能够有效植入多个后门触发器并保持模型性能。

Conclusion: 本文提出了MixBridge框架，以解决在桥接扩散模型中植入多个异构后门触发器的问题，并通过Divide-and-Merge策略和Weight Reallocation Scheme (WRS)来提高其有效性与隐蔽性。

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [846] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

TL;DR: 本研究提出了一个用于检测基于高速公路的VANET环境中DDoS攻击的稳健且可扩展的框架。通过使用NS-3和SUMO构建合成数据集，并结合真实移动轨迹进行丰富，数据预处理包括归一化、SNR特征工程、缺失值插补和SMOTE类别平衡。通过SHAP评估特征重要性，并对多个分类器进行了基准测试。XGB和CB表现最佳，F1分数达到96%。这些结果表明该框架具有良好的性能，适用于VANET中的实时部署，以保护关键的应急通信。


<details>
  <summary>Details</summary>
Motivation: VANETs在ITS中起着关键作用，特别是在为应急车辆提供实时通信方面。然而，DDoS攻击会严重损害其可靠性。因此，需要一种稳健且可扩展的框架来检测DDoS攻击。

Method: 本研究引入了一个稳健且可扩展的框架来检测基于高速公路的VANET环境中的DDoS攻击。使用NS-3和SUMO构建了一个合成数据集，并进一步用德国A81高速公路的真实移动轨迹进行丰富。数据预处理包括归一化、信噪比（SNR）特征工程、缺失值插补和使用SMOTE进行类别平衡。通过SHAP评估特征重要性，并对十一个分类器进行了基准测试。

Result: XGB和CB达到了最佳性能，每个都获得了96%的F1分数。

Conclusion: 这些结果突显了所提出框架的稳健性及其在VANET中实时部署以保护关键应急通信的潜力。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [847] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: 本文对FLLM的最新进展进行了全面回顾，分析了其面临的挑战，并提出了增强系统鲁棒性和安全性的方法。


<details>
  <summary>Details</summary>
Motivation: FLLM面临着通信和计算开销、异构性、隐私和安全问题等重大挑战，当前研究主要集中在FLLM的可行性上，未来趋势将更加注重系统鲁棒性和安全性。

Method: 本文对FLLM的最新进展进行了全面回顾，从可行性、鲁棒性、安全性和未来方向四个关键角度进行了分析。

Result: 本文对FLLM的可行性进行了详尽的调查，介绍了在资源、数据和任务异构性面前增强鲁棒性的方法，并分析了这种集成带来的新风险，包括隐私威胁和安全挑战。此外，还回顾了最新的防御机制，并探讨了有前景的未来研究方向。

Conclusion: 本文强调了进一步研究的必要性，以增强系统鲁棒性和安全性，同时解决FL和LLM集成带来的独特挑战。

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [848] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

TL;DR: 本研究探讨了对抗性补丁攻击对无人商店中对象检测模型的影响，并提出了新的方法来提高系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于无人商店中的对象检测模型容易受到对抗性补丁攻击的影响，因此需要研究这些攻击的有效性以及如何提高系统的鲁棒性。

Method: 本研究调查了三种对抗性补丁攻击（隐藏、创建和修改攻击），并引入了新的颜色直方图相似性损失函数。此外，还引入了一种基于边界框的度量标准来分析这些攻击的实际影响。

Result: 实验结果表明，对抗性补丁可以严重干扰无人商店中使用的对象检测模型，导致盗窃、库存差异和干扰等问题。此外，在黑盒场景中评估了这些攻击的鲁棒性，并展示了阴影攻击可以提高攻击成功率。

Conclusion: 本研究强调了为保护无人商店免受对抗性威胁而需要稳健的防御策略，并提供了改进目标检测模型的鲁棒性和加强无人零售环境的见解。

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [849] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

TL;DR: 本文研究了在机器学习即服务（MLaaS）平台中，使用差分隐私（DP）技术在模型性能、隐私和可解释性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: MLaaS平台需要防范安全和隐私攻击，如模型提取攻击（MEA）。XAI的引入增加了隐私风险，因为攻击者可以利用模型解释，特别是反事实解释（CFs）来促进MEA。

Method: 评估两种不同的DP策略：在分类模型训练期间实施和在生成CF时在解释器中实施。

Result: 研究了DP在缓解CF促成的MEA方面的有效性，并分析了其对模型性能、隐私和可解释性的影响。

Conclusion: DP是一种有前途的技术，可以在MLaaS中平衡模型性能、隐私和可解释性。

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [850] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本文提出了一种隐私保护对齐的新算法，在保持隐私的同时提升了对齐效果。


<details>
  <summary>Details</summary>
Motivation: 语言模型对齐对于确保大型语言模型（LLMs）与人类偏好一致至关重要，但通常涉及敏感用户数据，引发重大隐私问题。尽管之前的工作已将差分隐私（DP）与对齐技术结合，但其性能仍有限。因此，本文旨在提出更有效的隐私保护对齐方法。

Method: 本文提出了新的隐私保护对齐算法，并将其应用于直接偏好优化（DPO）和从人类反馈中强化学习（RLHF）两种著名的对齐技术。通过系统实验，验证了算法的有效性。

Result: 本文提出的算法在大规模语言模型上的实验结果表明，其性能优于现有方法。特别是DP-AdamW算法结合DPO，在中等隐私预算下（ε=2-5）提高了对齐质量达15%。

Conclusion: 本文提出了新的隐私保护对齐算法，并在不同隐私预算和模型上进行了严格的分析。实验表明，所提出的方法在保持隐私的同时实现了最先进的性能，为优化隐私保证、对齐效果和计算需求之间的权衡提供了实用指南。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [851] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 本文分析了文本水印在检测能力和生成文本质量之间的基本权衡，并提出了一个优化的水印设计策略。实验结果表明，该方法在检测率和生成文本质量之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）生成的文本越来越接近人类生成的内容，水印技术变得越来越重要。然而，现有方法在检测能力和生成文本质量之间缺乏理论上的分析和优化。

Method: 本文通过假设检验与辅助信息的视角，分析了文本水印在检测能力和生成文本质量之间的基本权衡。我们提出了一种优化的水印设计策略，考虑了最坏情况下的语言模型下一个词分布。

Result: 本文提出了一种优化的水印设计策略，并通过实验验证了其有效性。结果表明，该方法在检测率和生成文本质量之间取得了良好的平衡。

Conclusion: 本文分析了在一次性设置下文本水印的理论极限，并提出了一个优化的水印设计策略。实验结果表明，该方法在检测率和生成文本质量之间取得了良好的平衡。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [852] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

TL;DR: TokenProber is a method that generates adversarial prompts to evaluate the robustness of safety checkers in T2I models, demonstrating superior effectiveness in bypassing safety filters.


<details>
  <summary>Details</summary>
Motivation: The key challenge is to subtly modify prompts to preserve their sensitive nature while bypassing refusal mechanisms, as T2I models can generate NSFW content.

Method: TokenProber is a method designed for sensitivity-aware differential testing, which generates adversarial prompts by analyzing the impact of specific words within prompts and distinguishing between dirty words and discrepant words.

Result: TokenProber was evaluated against 5 safety checkers on 3 popular T2I models using 324 NSFW prompts, showing a 54%+ increase in bypassing safety filters compared to existing methods.

Conclusion: TokenProber demonstrates superior effectiveness in bypassing safety filters compared to existing methods, highlighting the robustness issues in existing refusal mechanisms.

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [853] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

TL;DR: 本文提出了一种基于Transformer的自监督对比学习方法，用于入侵检测，该方法在异常检测任务和有监督的入侵检测中表现出色，并且在有限标记数据的情况下提供了强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着数字环境的日益互联，零日攻击的频率和严重性显著增加，迫切需要创新的入侵检测系统（IDS）。机器学习-based IDS能够从网络流量特征中学习并识别攻击模式，是传统基于签名的IDS的先进解决方案。然而，它们严重依赖标记数据集，并且在遇到未见过的流量模式时泛化能力仍然存在问题。

Method: 本文提出了一种基于Transformer编码器的自监督对比学习方法，结合了数据增强策略，以提取和生成流量流的有意义表示。与依赖于手工统计特征的传统方法不同，该方法自动学习全面的数据包序列表示。

Result: 与现有的NetFlow自监督方法相比，基于Transformer的框架表现出更好的性能。具体而言，在数据集内评估中，我们的方法在异常检测中的AUC提高了高达3%，在数据集间评估中，AUC分数提高了高达20%。此外，当在相同数据集上预训练和评估时，我们的模型在自监督NetFlow模型上实现了最高1.5%的AUC提升。

Conclusion: 本文提出了一种基于Transformer编码器的新型自监督对比学习方法，专门用于在原始数据包序列上进行通用入侵检测。该方法在异常检测任务和有监督的入侵检测中表现出色，并且在有限标记数据的情况下提供了强大的基线模型。此外，我们的预训练模型在不同数据集上的微调展示了其适应性，并在缺乏目标领域良性数据的情况下表现出色。

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [854] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

TL;DR: 本文提出了一种基于强化学习的安全策略管理框架，以动态适应云环境中的安全需求。


<details>
  <summary>Details</summary>
Motivation: 静态安全策略已不足以应对不断演变的威胁和云资源的弹性。

Method: 我们采用深度强化学习算法，包括深度Q网络和近端策略优化，使控制（如防火墙规则和身份和访问管理（IAM）策略）能够学习和持续调整。

Result: 实验结果表明，我们的自适应RL框架显著优于静态策略，在入侵检测率（92%对比82%）和减少事件检测和响应时间（58%）方面表现更好。

Conclusion: 这些发现验证了自适应强化学习方法在改进云安全策略管理方面的有效性。

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [855] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

TL;DR: 本文介绍了Prioritized Binary Rounding和sigma-binary攻击，以在二进制领域中实现最小特征变化的攻击目标。实验表明，sigma-binary攻击比现有攻击更有效，并揭示了现有防御中的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: ML-based detectors在二进制受限领域中缺乏全面的评估框架，限制了对其鲁棒性的理解。

Method: Prioritized Binary Rounding（优先二进制舍入）和sigma-binary攻击（一种新的对抗性方法）。

Result: sigma-binary攻击在Malscan数据集上优于现有攻击，并揭示了最先进的防御中的关键漏洞。配备对抗检测器的防御措施表现出显著的脆弱性，使用少于10个特征修改的攻击成功率超过90%。对抗训练的防御措施在小预算下提高了鲁棒性，但仍容易受到不受限制的扰动影响。PAD-SMA在对抗基于梯度的攻击方面表现出强大的鲁棒性，但sigma-binary攻击在不受限制的扰动下取得了更高的成功率。

Conclusion: 这些发现突显了像sigma-binary这样的精确方法在暴露现有防御中的隐藏漏洞和支持开发更强大恶意软件检测系统的重要性。

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [856] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: 本文提出了一种名为SafeFL的新算法，用于准确识别联邦学习中的恶意客户端。


<details>
  <summary>Details</summary>
Motivation: 现有的FL检测技术常常错误地将大量良性客户端分类为威胁，或者依赖于对服务器能力的不切实际的假设。

Method: SafeFL方法涉及服务器收集一系列全局模型以生成合成数据集，并基于其行为区分恶意和良性模型。

Result: 实验表明，SafeFL在检测恶意客户端方面表现出色，具有更高的效率和准确性。

Conclusion: SafeFL在检测恶意客户端方面优于现有方法，提供了更高的效率和准确性。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [857] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

TL;DR: 本文提出了一种基于子图特征提取的轻量级GBM（lightGBM）的Sybil地址识别方法，该方法在精确率、召回率、F1分数和AUC方面均优于现有方法，所有指标均超过0.9。


<details>
  <summary>Details</summary>
Motivation: Sybil攻击对区块链生态系统，特别是在代币空投事件中构成重大安全威胁。因此，需要一种有效的Sybil地址识别方法来提高区块链的安全性。

Method: 本文提出了一种基于子图特征提取的轻量级GBM（lightGBM）的Sybil地址识别方法。该方法首先为每个地址构建一个两层的深度交易子图，然后根据Sybil地址的生命周期提取关键事件操作特征，包括首次交易时间、首次获取Gas时间、参与空投活动时间和最后一次交易时间。此外，该方法还提取了金额和网络结构特征，通过特征传播和融合全面描述地址行为模式和网络拓扑结构。

Result: 在包含193,701个地址（其中23,240个是Sybil地址）的数据集上进行的实验表明，该方法在精确率、召回率、F1分数和AUC方面均优于现有方法，所有指标均超过0.9。

Conclusion: 本文的方法和结果可以进一步应用于更广泛的区块链安全领域，如交易操控识别和代币流动性风险评估，有助于构建更加安全和公平的区块链生态系统。

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


### [858] [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
*Yidan Wang,Yanan Cao,Yubing Ren,Fang Fang,Zheng Lin,Binxing Fang*

Main category: cs.CR

TL;DR: 本文研究了Jailbreak攻击在提取敏感信息方面的有效性，并提出了PIG框架来针对PII，实验结果表明PIG在隐私泄露评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的评估LLM隐私泄露的方法容易被对齐良好的模型阻止，而Jailbreak攻击在隐私场景中的作用尚未得到充分研究。

Method: PIG框架通过识别PII实体及其类型，使用上下文学习构建隐私上下文，并通过三种基于梯度的策略迭代更新以引出目标PII。

Result: PIG在两个与隐私相关的数据集上进行了评估，并在四个白盒和两个黑盒LLM上进行了实验，结果显示PIG优于基线方法并达到了最先进的结果。

Conclusion: 实验结果表明，LLM中存在显著的隐私风险，强调了需要更强的保护措施。

Abstract: Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.

</details>


### [859] [Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data](https://arxiv.org/abs/2505.09974)
*Adel ElZemity,Budi Arief,Shujun Li*

Main category: cs.CR

TL;DR: 本文评估了网络安全应用中微调大型语言模型的安全风险，并提出了一种安全对齐方法，以在保持技术效用的同时提高模型安全性。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）集成到网络安全应用中虽然带来了显著的机会，但也可能引入关键风险和安全问题，因此需要评估这些风险并寻找解决方案。

Method: 使用OWASP Top 10 for LLM Applications框架评估了七个开源LLM，并提出了一种安全对齐方法，通过仔细重写指令-响应对来包含显式的安全预防措施和伦理考虑。

Result: 微调会降低所有测试LLM的安全弹性，例如Llama 3.1 8B在对抗提示注入时的安全评分从0.95降至0.15。所提出的安全对齐方法展示了在保持技术效用的同时维持或提高模型安全性的可能性。

Conclusion: 本文提供了对LLM安全风险的系统评估，为在敏感领域安全采用生成式AI提供了可能，并有助于开发安全、可信和符合伦理的LLM。

Abstract: The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.

</details>


### [860] [AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons](https://arxiv.org/abs/2505.10273)
*Hexu Li,Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: This paper presents AttentionGuard, a transformer-based framework for detecting misbehavior in vehicle platooning systems. It uses a multi-head transformer-encoder to process sequential kinematic information and identify anomalous patterns in mobility data. Experimental results show that AttentionGuard achieves high accuracy in attack detection with minimal latency, making it suitable for real-time transportation safety applications.


<details>
  <summary>Details</summary>
Motivation: Vehicle platooning systems are vulnerable to sophisticated falsification attacks by authenticated insiders, which can destabilize the formation and cause catastrophic collisions. This paper addresses the challenge of misbehavior detection in vehicle platooning systems.

Method: AttentionGuard is a transformer-based framework that uses the self-attention mechanism to identify anomalous patterns in mobility data. It employs a multi-head transformer-encoder to process sequential kinematic information, enabling effective differentiation between normal mobility patterns and falsification attacks.

Result: Experimental results demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance maintained during complex maneuvers. It also performs effectively with minimal latency (100ms decision intervals), making it suitable for real-time transportation safety applications.

Conclusion: AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance during complex maneuvers. It performs effectively with minimal latency, making it suitable for real-time transportation safety applications. The transformer-encoder is a promising approach for securing C-ITS against sophisticated insider threats.

Abstract: Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.

</details>


### [861] [Private Transformer Inference in MLaaS: A Survey](https://arxiv.org/abs/2505.10315)
*Yang Li,Xinyu Zhou,Yitong Wang,Liangxin Qian,Jun Zhao*

Main category: cs.CR

TL;DR: 本文探讨了Private Transformer Inference (PTI)的概念，介绍了一种利用密码学技术保护用户数据和模型隐私的方法，并提出了一个结构化的分类法和评估框架。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在AI领域取得了重大突破，但在MLaaS中的部署引发了隐私问题，因为敏感用户数据是集中处理的。因此，需要一种方法来保护用户数据和模型隐私，同时保持高性能推理。

Method: 本文介绍了Private Transformer Inference (PTI)的概念，并利用密码学技术如安全多方计算和同态加密来实现推理，同时保护用户数据和模型隐私。此外，本文提出了一个结构化的分类法和评估框架。

Result: 本文回顾了PTI的最新进展，提出了一个结构化的分类法和评估框架，以平衡资源效率与隐私，并弥合高性能推理与数据隐私之间的差距。

Conclusion: 本文回顾了最近的PTI进展，强调了最先进的解决方案和挑战，并引入了一个结构化的分类法和评估框架，以平衡资源效率与隐私，并弥合高性能推理与数据隐私之间的差距。

Abstract: Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.

</details>


### [862] [AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents](https://arxiv.org/abs/2505.10321)
*Julius Henke*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型在渗透测试中的应用，介绍了一种名为AutoPentest的自动化工具，并通过实验比较了其与传统方法的效果和成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在渗透测试中的使用是一个研究热点，有望降低成本并允许更高的频率。

Method: 我们进行了相关工作的回顾，确定了最佳实践和常见的评估问题。然后我们介绍了AutoPentest，这是一个用于执行高度自主的黑盒渗透测试的应用程序。AutoPentest基于OpenAI的LLM GPT-4o和LLM代理框架LangChain。

Result: 在三个CTF风格的Hack The Box（HTB）机器上进行的研究表明，我们的实现AutoPentest与手动使用ChatGPT-4o用户界面的基线方法都能完成15-25%的子任务，而AutoPentest略胜一筹。使用AutoPentest的总成本为96.20美元，而一个月的ChatGPT Plus订阅费用为20美元。

Conclusion: 结果表明，进一步的实施努力和未来更强大的LLMs的使用可能会使这成为漏洞管理中可行的一部分。

Abstract: A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.

</details>


### [863] [Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts](https://arxiv.org/abs/2505.09843)
*Melissa Turcotte,François Labrèche,Serge-Olivier Paquette*

Main category: cs.CR

TL;DR: AACT系统通过自动化SOC工作流程，减少警报数量并提高效率，显著降低了分析人员的工作负担。


<details>
  <summary>Details</summary>
Motivation: 企业网络不断扩大，安全警报数量增加，导致SOC分析人员面临警报疲劳问题。同时，使用托管SOC服务时，问题因上下文切换和有限的业务流程可见性而加剧。

Method: AACT系统通过学习分析人员对网络安全警报的分类操作来自动化SOC工作流程，实时准确预测分类决策。

Result: AACT系统在真实SOC数据和公开数据集上进行了训练和评估，表现出高性能。在实际SOC环境中，它减少了61%的警报展示给分析人员，且误报率仅为1.36%。

Conclusion: AACT系统在实际SOC环境中表现出色，显著减少了展示给分析人员的警报数量，并保持了低误报率。

Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack
surface, increasing the volume of security alerts generated from security
controls. Security Operations Centre (SOC) analysts triage these alerts to
identify malicious activity, but they struggle with alert fatigue due to the
overwhelming number of benign alerts. Organisations are turning to managed SOC
providers, where the problem is amplified by context switching and limited
visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by
learning from analysts' triage actions on cybersecurity alerts. It accurately
predicts triage decisions in real time, allowing benign alerts to be closed
automatically and critical ones prioritised. This reduces the SOC queue
allowing analysts to focus on the most severe, relevant or ambiguous threats.
The system has been trained and evaluated on both real SOC data and an open
dataset, obtaining high performance in identifying malicious alerts from benign
alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC
environment, reducing alerts shown to analysts by 61% over six months, with a
low false negative rate of 1.36% over millions of alerts.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [864] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

TL;DR: This paper introduces ARCANE, the first framework for early ICME detection in streaming solar wind data under realistic operational constraints. It compares a machine learning-based method (ResUNet++) with a threshold-based baseline and finds that ResUNet++ significantly outperforms the baseline, especially in detecting high-impact events. Using real-time solar wind data leads to only minimal performance degradation compared to high-resolution science data.


<details>
  <summary>Details</summary>
Motivation: Interplanetary coronal mass ejections (ICMEs) are major drivers of space weather disturbances, posing risks to technological infrastructure and human activities. There is a need for robust real-time detection of ICMEs in solar wind in situ data for early warning systems.

Method: The authors developed ARCANE, which evaluates the strengths and limitations of detection models by comparing a machine learning-based method (ResUNet++) to a threshold-based baseline. They use real-time solar wind (RTSW) data instead of high-resolution science data.

Result: The ResUNet++ model significantly outperforms the baseline, particularly in detecting high-impact events, while retaining solid performance on lower-impact cases. The detection pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5% of the event's duration. As more data becomes available, the performance increases significantly.

Conclusion: ARCANE marks a substantial step forward in automated space weather monitoring and lays the groundwork for enhanced real-time forecasting capabilities.

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


### [865] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

TL;DR: 本文介绍了ARCANE框架，用于在实时太阳风数据中早期检测ICMEs。通过比较基于机器学习的方法和基于阈值的基线，发现ResUNet++模型在检测高影响事件方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动检测ICMEs对于早期预警系统至关重要。尽管已经提出了几种方法来识别时间序列数据中的这些结构，但稳健的实时检测仍然是一个重大挑战。

Method: 我们比较了基于机器学习的方法和基于阈值的基线，评估了检测模型的优势和局限性。使用ResUNet++模型，在科学数据上验证过，显著优于基线，特别是在检测高影响事件方面。

Result: 我们的检测流程在F1分数上达到了0.53，平均检测延迟为事件持续时间的21.5%，并且只看到了少量的数据。随着更多数据的可用，性能显著提高。

Conclusion: 这些结果标志着在自动化空间天气监测方面取得了重大进展，并为增强实时预测能力奠定了基础。

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [866] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

TL;DR: 准确的血糖预测对1型糖尿病治疗至关重要。本研究探索了transformer模型在血糖预测中的潜力，使用DCLP3和OhioT1DM数据集进行训练、验证和测试。结果表明，采用patch-wise方法的Crossformer和PatchTST模型分别在短期（30分钟）和中长期（1小时、2小时、4小时）预测中表现最佳，展示了transformer架构通过捕捉多变量时间序列数据中的季节性模式以提高预测精度的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的架构在复杂多变量时间序列预测中展现了强大的注意力机制，但其在血糖预测领域的潜力尚未被充分挖掘。准确的血糖预测可以为1型糖尿病的个性化治疗提供支持，包括胰岛素和饮食调整等干预措施。

Method: 研究采用了公开的DCLP3数据集（n=112）按80%-10%-10%的比例划分训练、验证和测试集，并使用OhioT1DM数据集（n=12）作为外部测试集。训练了具有点式、块式、序列式和混合嵌入方式的网络模型，利用连续血糖监测（CGM）、胰岛素和餐食数据进行多步血糖预测。

Result: 对于短期（30分钟）血糖预测，Crossformer模型表现出色，均方根误差（RMSE）为15.6 mg/dL。而对于较长期预测（1小时、2小时、4小时），PatchTST模型表现最佳，RMSE分别为24.6 mg/dL、36.1 mg/dL和46.5 mg/dL。总体而言，使用分块标记化的模型在更大的输入尺寸下显示出更高的准确性，其中一周的历史数据提供了最佳结果。

Conclusion: 基于transformer的架构在血糖预测中展现出巨大潜力，能够通过捕捉多变量时间序列数据中的季节性模式来提高预测准确性，为1型糖尿病患者的个性化治疗提供了有力支持。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


### [867] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: The study compares LLM-based clustering methods with classical methods on pediatric sepsis data from a low-income country, finding that LLMs, especially Stella-En-400M-V5, outperform classical techniques in capturing richer context and identifying distinct patient subgroups.


<details>
  <summary>Details</summary>
Motivation: Clustering patient subgroups is crucial for personalized care and efficient resource use, but traditional methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding.

Method: Patient records were serialized into text with and without a clustering objective. Embeddings were generated using three LLMs: quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with LoRA, and Stella-En-400M-V5. K-means clustering was applied to these embeddings. Classical methods included K-Medoids clustering on UMAP and FAMD-reduced mixed data.

Result: Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles.

Conclusion: LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features, showing potential for contextual phenotyping and informed decision-making in resource-limited settings.

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


### [868] [Generative diffusion model surrogates for mechanistic agent-based biological models](https://arxiv.org/abs/2505.09630)
*Tien Comlekoglu,J. Quetzalcóatl Toledo-Marín,Douglas W. DeSimone,Shayn M. Peirce,Geoffrey Fox,James A. Glazier*

Main category: q-bio.QM

TL;DR: This paper explores the use of denoising diffusion probabilistic models to create a generative AI surrogate for the Cellular-Potts Model (CPM), which is used to study in vitro vasculogenesis. The surrogate model significantly reduces computational time and aids in model selection and verification.


<details>
  <summary>Details</summary>
Motivation: The Cellular-Potts Model, while powerful for modeling biological systems at single-cell resolution, becomes computationally expensive at large scales. There's a need for accelerated evaluation methods without losing accuracy or detail.

Method: Denoising diffusion probabilistic models are employed to train a generative AI surrogate of the CPM focused on in vitro vasculogenesis. An image classifier is used to understand parameter space characteristics aiding in surrogate model selection and verification.

Result: The surrogate model successfully generates configurations 20,000 timesteps ahead with about a 22x reduction in computational time compared to native code execution.

Conclusion: This work marks progress in applying DDPMs for creating digital twins of stochastic biological systems, offering significant computational advantages.

Abstract: Mechanistic, multicellular, agent-based models are commonly used to
investigate tissue, organ, and organism-scale biology at single-cell
resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework
for developing and interrogating these models. CPMs become computationally
expensive at large space- and time- scales making application and investigation
of developed models difficult. Surrogate models may allow for the accelerated
evaluation of CPMs of complex biological systems. However, the stochastic
nature of these models means each set of parameters may give rise to different
model configurations, complicating surrogate model development. In this work,
we leverage denoising diffusion probabilistic models to train a generative AI
surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We
describe the use of an image classifier to learn the characteristics that
define unique areas of a 2-dimensional parameter space. We then apply this
classifier to aid in surrogate model selection and verification. Our CPM model
surrogate generates model configurations 20,000 timesteps ahead of a reference
configuration and demonstrates approximately a 22x reduction in computational
time as compared to native code execution. Our work represents a step towards
the implementation of DDPMs to develop digital twins of stochastic biological
systems.

</details>


### [869] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

TL;DR: 本文比较了变压器模型在多时间范围血糖预测中的表现，发现块式变压器在短期和长期预测中都表现出色，尤其是在使用一周的历史数据时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 准确的血糖预测可以为1型糖尿病治疗提供新的干预措施，包括个性化的胰岛素和饮食调整。尽管最近基于变压器的架构在复杂多变量时间序列预测中展示了注意力机制的力量，但它们在血糖预测中的潜力仍待探索。

Method: 我们进行了变压器模型的比较分析，以进行多时间范围的血糖预测，检查最多4小时的预测和最多1周的输入历史。我们使用了CGM、胰岛素和餐食数据训练了具有点式、块式、序列式和混合嵌入的网络。

Result: 对于短期血糖预测，Crossformer（一种块式变压器架构）在OhioT1DM上实现了RMSE为15.6 mg/dL的优越30分钟预测。对于更长期的预测（1小时、2小时和4小时），PatchTST（另一种块式变压器）在OhioT1DM上实现了最低的RMSE（24.6 mg/dL、36.1 mg/dL和46.5 mg/dL）。一般来说，通过块进行标记化的模型在输入规模较大时表现出更高的准确性，最佳结果是在一周的历史数据下获得的。

Conclusion: 这些发现表明，基于变压器的架构在血糖预测中具有前景，通过捕捉和利用多变量时间序列数据中的季节性模式来提高准确性。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


### [870] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: 本研究评估了基于大型语言模型的聚类方法，并将其与经典方法进行了比较。结果表明，基于LLM的方法在处理高维、异构的医疗数据时表现更好，能够识别出具有不同特征的患者亚组。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维、异构的医疗数据时存在困难，并且缺乏上下文理解。因此，需要一种更有效的聚类方法来识别患者亚组，以实现个性化护理和高效资源利用。

Method: 本研究评估了基于大型语言模型（LLM）的聚类方法，并将其与经典方法进行了比较，使用了一个来自低收入国家的儿科败血症数据集。患者记录被序列化为文本，并生成了嵌入向量。应用了K均值聚类，并与K-中位数聚类进行了比较。

Result: Stella-En-400M-V5模型获得了最高的轮廓分数（0.86）。LLAMA 3.1 8B在具有聚类目标的情况下表现更好，能够识别出具有不同营养、临床和社会经济特征的亚组。基于LLM的方法优于经典技术，因为它们能够捕捉更丰富的上下文并优先考虑关键特征。

Conclusion: 这些结果突显了大型语言模型在资源有限的环境中进行情境表型和知情决策的潜力。

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


### [871] [Generative diffusion model surrogates for mechanistic agent-based biological models](https://arxiv.org/abs/2505.09630)
*Tien Comlekoglu,J. Quetzalcóatl Toledo-Marín,Douglas W. DeSimone,Shayn M. Peirce,Geoffrey Fox,James A. Glazier*

Main category: q-bio.QM

TL;DR: 本文提出了一种利用去噪扩散概率模型（DDPM）来训练生成AI代理模型的方法，以加速细胞-陈模型（CPM）的评估。该方法通过图像分类器学习参数空间中的特征，并用于代理模型的选择和验证。结果表明，该代理模型能够显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: CPM在大规模空间和时间尺度上计算成本高昂，使得应用和研究开发的模型变得困难。代理模型可能允许加速评估复杂的生物系统CPM。然而，这些模型的随机性意味着每组参数可能会产生不同的模型配置，这使得代理模型的开发变得复杂。

Method: 我们利用去噪扩散概率模型来训练一个用于研究体外血管生成的CPM的生成AI代理模型。我们描述了使用图像分类器来学习定义二维参数空间中独特区域的特征。然后，我们将此分类器用于辅助代理模型的选择和验证。

Result: 我们的CPM模型代理可以生成比参考配置提前20,000个时间步的模型配置，并且与原生代码执行相比，计算时间减少了约22倍。

Conclusion: 我们的工作代表了将DDPM应用于开发随机生物系统的数字孪生体的一个步骤。

Abstract: Mechanistic, multicellular, agent-based models are commonly used to
investigate tissue, organ, and organism-scale biology at single-cell
resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework
for developing and interrogating these models. CPMs become computationally
expensive at large space- and time- scales making application and investigation
of developed models difficult. Surrogate models may allow for the accelerated
evaluation of CPMs of complex biological systems. However, the stochastic
nature of these models means each set of parameters may give rise to different
model configurations, complicating surrogate model development. In this work,
we leverage denoising diffusion probabilistic models to train a generative AI
surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We
describe the use of an image classifier to learn the characteristics that
define unique areas of a 2-dimensional parameter space. We then apply this
classifier to aid in surrogate model selection and verification. Our CPM model
surrogate generates model configurations 20,000 timesteps ahead of a reference
configuration and demonstrates approximately a 22x reduction in computational
time as compared to native code execution. Our work represents a step towards
the implementation of DDPMs to develop digital twins of stochastic biological
systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [872] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/abs/2505.09651)
*Xixuan Hao,Yutian Jiang,Xingchen Zou,Jiabo Liu,Yifang Yin,Yuxuan Liang*

Main category: cs.DB

TL;DR: Location Intelligence (LI) evolves through two revolutions: deep learning and large language models (LLMs). This survey reviews geospatial representation learning across both eras, organizing advancements into data, methodology, and application perspectives. It highlights current progress, limitations, and future research directions in the LLM era.


<details>
  <summary>Details</summary>
Motivation: The field of Location Intelligence is being reshaped by advancements in deep learning and large language models, offering new capabilities for processing structured and unstructured geospatial data.

Method: The paper provides a comprehensive review of geospatial representation learning, categorizing it into a taxonomy based on data perspective, methodological perspective, and application perspective across two technological eras.

Result: The survey outlines current advancements and limitations in geospatial representation learning and suggests potential future research directions, particularly in the context of LLMs.

Conclusion: This work aims to provide a roadmap for further innovation in Location Intelligence, summarizing key developments and pointing towards promising areas for exploration.

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [873] [Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts](https://arxiv.org/abs/2505.09798)
*Bojan Ristov,Stefan Eftimov,Milena Trajanoska,Dimitar Trajanov*

Main category: cs.DB

TL;DR: The paper explores transforming traditional procurement data into a semantic knowledge graph using ontological modeling and automated data transformation techniques, enhancing data transparency, decision-making, and analysis in public procurement.


<details>
  <summary>Details</summary>
Motivation: Traditional procurement data is often stored in rigid, tabular formats which limits its analytical potential and hinders transparency.

Method: Transform structured procurement data into a semantic knowledge graph by leveraging ontological modeling and automated data transformation techniques. Integrate RDF and SPARQL-based querying and incorporate machine learning-driven predictive modeling.

Result: Improved data transparency, support for evidence-based decision-making, and enabled in-depth analysis of procurement activities in North Macedonia.

Conclusion: This work contributes to the field of public procurement intelligence by demonstrating methods to enhance accessibility, interpretability, and analytics of procurement records.

Abstract: Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.

</details>


### [874] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/abs/2505.09651)
*Xixuan Hao,Yutian Jiang,Xingchen Zou,Jiabo Liu,Yifang Yin,Yuxuan Liang*

Main category: cs.DB

TL;DR: 本文综述了地理空间表示学习，讨论了其在深度学习和大语言模型时代的进展、局限性及未来方向。


<details>
  <summary>Details</summary>
Motivation: 位置智能（LI）已成为现代空间决策的核心。随着地理空间表示学习的快速发展，它正在通过两次技术革命重新塑造LI的发展：深度学习突破和新兴的大语言模型（LLM）范式。

Method: 本文对地理空间表示学习进行了全面回顾，并根据完整的流程组织成结构化的分类法，包括：(1) 数据视角，(2) 方法论视角和(3) 应用视角。

Result: 本文总结了最新的论文列表，并提出了在LLM时代可能的未来研究方向。

Conclusion: 本文提供了对位置智能领域的全面探索，并为LI的进一步创新提供了路线图。

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [875] [Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts](https://arxiv.org/abs/2505.09798)
*Bojan Ristov,Stefan Eftimov,Milena Trajanoska,Dimitar Trajanov*

Main category: cs.DB

TL;DR: 本研究提出了一种将结构化采购数据转换为语义知识图谱的方法，利用本体建模、RDF和SPARQL查询以及机器学习技术，以提高数据透明度、支持决策并促进采购活动的深入分析。


<details>
  <summary>Details</summary>
Motivation: 传统采购数据通常存储在刚性的表格格式中，限制了其分析潜力并阻碍了透明度。因此，需要一种更有效的数据处理方法来提高公共采购数据的可用性和分析能力。

Method: 本研究提出了一种方法论框架，将结构化采购数据转换为语义知识图谱，利用本体建模和自动化数据转换技术，并结合RDF和SPARQL查询以增强采购记录的可访问性和可解释性。此外，还引入了机器学习驱动的预测建模，以提供采购趋势和风险评估的见解。

Result: 该系统通过语义知识图谱增强了采购数据的可访问性和可解释性，使复杂语义查询和高级分析成为可能，并提供了采购趋势和风险评估的见解。

Conclusion: 本研究通过构建语义知识图谱，提高了公共采购数据的透明度，支持基于证据的决策，并促进了对北马其顿采购活动的深入分析。

Abstract: Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [876] [LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps](https://arxiv.org/abs/2505.10537)
*Filippo Olimpieri,Noemi Giustini,Andrea Lacava,Salvatore D'Oro,Tommaso Melodia,Francesca Cuomo*

Main category: cs.NI

TL;DR: The paper introduces LibIQ, a library for RF signals that uses dApps concept to enable real-time RF spectrum classification. It processes I/Q samples to detect and classify external RF signals using a CNN, achieving an average accuracy of 97.8%. The authors plan to release LibIQ and the dataset publicly.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current RICs, such as latency overhead in data exchange and inability to access user plain data, which restrict real-time monitoring and certain use cases like beamforming and spectrum classification.

Method: Leverage the dApps concept and develop LibIQ, a novel library for RF signals that provides functionalities to read I/Q samples as time-series, create datasets, visualize data through plots and spectrograms, and classify signals using a CNN.

Result: Achieved an average accuracy of approximately 97.8% in identifying signal types across all scenarios in real-time analysis when deploying LibIQ in heterogeneous scenarios with varying conditions.

Conclusion: LibIQ enables efficient and accurate real-time RF spectrum classification and the authors intend to release it along with the created dataset as a public framework.

Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.

</details>


### [877] [LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps](https://arxiv.org/abs/2505.10537)
*Filippo Olimpieri,Noemi Giustini,Andrea Lacava,Salvatore D'Oro,Tommaso Melodia,Francesca Cuomo*

Main category: cs.NI

TL;DR: 本文提出了一种基于LibIQ库和CNN模型的实时RF频谱分类方法，具有高准确率，并计划公开相关工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的RAN架构在数据交换延迟和用户原始数据访问方面存在限制，阻碍了如波束成形和频谱分类等用例的应用。

Method: 本文利用dApps概念，通过LibIQ库对I/Q样本进行时间序列处理、数据集创建和可视化分析，并使用CNN进行信号分类。

Result: 通过LibIQ库和CNN模型，实现了高效的实时RF频谱分类，准确率达到约97.8%。

Conclusion: 本文提出了LibIQ库和一个用于实时RF频谱分类的CNN模型，实现了约97.8%的平均准确率，并计划公开发布LibIQ和数据集。

Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [878] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

TL;DR: The paper introduces Depth-based Local Center Clustering (DLCC), a new clustering method that uses local data depth to identify multimodal characteristics and varying shape clusters, along with a new internal metric for evaluating non-convex cluster performance.


<details>
  <summary>Details</summary>
Motivation: Existing clustering methods have limitations in practical applications as they are typically designed for specific scenarios. Traditional methods often fail to capture the multimodal characteristics of data and struggle with non-convex clusters.

Method: The proposed method, DLCC, employs a local version of data depth based on subsets of data to identify local centers and clusters of varying shapes. It also proposes a new internal metric based on density-based clustering to evaluate clustering performance on non-convex clusters.

Result: DLCC is presented as a flexible clustering approach that overcomes some limitations of traditional clustering methods, such as capturing multimodal characteristics and handling non-convex clusters effectively.

Conclusion: DLCC enhances data analysis capabilities across a wide range of application scenarios by providing a more versatile and effective clustering solution compared to traditional methods.

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [879] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

TL;DR: Mixed effects models are widely used but slow with high-dimensional crossed random effects. This paper presents Krylov subspace-based methods to solve computational bottlenecks, achieving significant runtime reduction and better scalability compared to Cholesky-based computations.


<details>
  <summary>Details</summary>
Motivation: To address the slowness of current standard computations using Cholesky decompositions for high-dimensional crossed random effects in mixed effects models.

Method: Novel Krylov subspace-based methods are developed. Theoretical analysis and empirical evaluation of preconditioners for conjugate gradient and stochastic Lanczos quadrature methods are performed. New convergence results are derived and computationally efficient methods for calculating predictive variances are developed.

Result: Extensive experiments show that the proposed methods scale much better than Cholesky-based computations, with a runtime reduction of approximately two orders of magnitude for both estimation and prediction. The software implementation is up to 10'000 times faster and more stable than state-of-the-art implementations like lme4 and glmmTMB.

Conclusion: The Krylov subspace-based methods presented in this work provide a significant improvement in computational efficiency and scalability for mixed effects models with high-dimensional crossed random effects.

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


### [880] [Forests for Differences: Robust Causal Inference Beyond Parametric DiD](https://arxiv.org/abs/2505.09706)
*Hugo Gobato Souto,Francisco Louzada Neto*

Main category: stat.ME

TL;DR: This paper introduces DiD-BCF, a novel model for Difference-in-Differences estimation that addresses challenges like staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating various treatment effects and demonstrates superior performance in simulations and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing methods for Difference-in-Differences (DiD) estimation by addressing issues such as staggered adoption of treatments and heterogeneous treatment effects, which traditional methods struggle with.

Method: The paper proposes the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a non-parametric model that incorporates a Parallel Trends Assumption-based reparameterization to enhance estimation accuracy and stability. This model offers a unified framework for estimating Average Treatment Effects (ATE), Group-Average Treatment Effects (GATE), and Conditional Average Treatment Effects (CATE).

Result: Through extensive simulations, the DiD-BCF model shows superior performance compared to established benchmarks, especially in scenarios involving non-linearity, selection biases, and effect heterogeneity. When applied to U.S. minimum wage policy, it reveals significant conditional treatment effect heterogeneity related to county population, insights not captured by traditional methods.

Conclusion: DiD-BCF is presented as a robust and versatile tool for more nuanced causal inference in modern DiD applications, offering improvements over traditional methods in terms of accuracy, stability, and ability to uncover complex treatment effect heterogeneity.

Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest
(DiD-BCF), a novel non-parametric model addressing key challenges in DiD
estimation, such as staggered adoption and heterogeneous treatment effects.
DiD-BCF provides a unified framework for estimating Average (ATE),
Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core
innovation, its Parallel Trends Assumption (PTA)-based reparameterization,
enhances estimation accuracy and stability in complex panel data settings.
Extensive simulations demonstrate DiD-BCF's superior performance over
established benchmarks, particularly under non-linearity, selection biases, and
effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers
significant conditional treatment effect heterogeneity related to county
population, insights obscured by traditional methods. DiD-BCF offers a robust
and versatile tool for more nuanced causal inference in modern DiD
applications.

</details>


### [881] [Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging](https://arxiv.org/abs/2505.10279)
*Gabriel R. Palma,Sally McClean,Brahim Allan,Zeeshan Tariq,Rafael A. Moral*

Main category: stat.ME

TL;DR: TV providers need to offer personalized experiences for customers. A key challenge is understanding group viewing behavior within households. This paper proposes a novel framework using Gaussian mixture model averaging and Bayesian random walk to estimate the number of household TV profiles and their characteristics from real customer data.


<details>
  <summary>Details</summary>
Motivation: To provide better-personalised recommendations for group viewing in households, it is essential to understand the behaviour and preferences of multiple people watching TV together.

Method: The method involves using a Gaussian mixture model averaging to obtain point estimates for the number of household TV profiles and a Bayesian random walk model to introduce uncertainty.

Result: The results show that the framework can estimate the number of household TV profiles and their characteristics, including shifts over time and quantification of uncertainty, when applied to real customer data with approximately half a million observations.

Conclusion: Combining the proposed framework with selected features provides an effective way to understand and predict group viewing behavior.

Abstract: TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.

</details>


### [882] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

TL;DR: 本文介绍了深度基于局部中心聚类（DLCC），该方法利用数据深度来识别局部中心和不同形状的聚类，并提出了一种新的内部度量来评估非凸聚类的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管过去几十年提出了许多聚类方法，但每种方法通常针对特定场景，并在实际应用中存在一定的限制。数据深度通常无法捕捉数据的多模态特性，这对聚类至关重要。

Method: 深度基于局部中心聚类（DLCC）利用数据深度，这可以产生多变量空间中样本点的中心向外排序。DLCC使用基于数据子集的局部数据深度，以识别局部中心和不同形状的聚类。此外，还提出了一种基于密度的内部度量来评估非凸聚类的聚类性能。

Result: DLCC能够识别局部中心和不同形状的聚类，并且提出的新内部度量可以评估非凸聚类的聚类性能。

Conclusion: DLCC是一种灵活的聚类方法，似乎克服了传统聚类方法的一些局限性，从而增强了在各种应用场景中的数据分析能力。

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [883] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

TL;DR: 本文提出了一种基于Krylov子空间的方法，用于提高高维交叉随机效应混合效应模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于Cholesky分解的计算方法在处理高维交叉随机效应时速度过慢，需要更高效的计算方法。

Method: 我们提出了基于Krylov子空间的方法，理论分析和实验评估了共轭梯度和随机Lanczos二次求积方法的各种预处理方法，并推导了新的收敛结果。

Result: 实验表明，我们的方法在估计和预测方面比基于Cholesky的计算方法快两个数量级，并且在默认设置下比最先进的实现如lme4和glmmTMB快多达10,000倍。

Conclusion: 我们的方法在计算上更高效且更稳定，适用于高维交叉随机效应的混合效应模型。

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


### [884] [Forests for Differences: Robust Causal Inference Beyond Parametric DiD](https://arxiv.org/abs/2505.09706)
*Hugo Gobato Souto,Francisco Louzada Neto*

Main category: stat.ME

TL;DR: This paper introduces DiD-BCF, a novel non-parametric model that addresses key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating ATE, GATE, and CATE. The model's PTA-based reparameterization enhances estimation accuracy and stability in complex panel data settings. Extensive simulations show its superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. When applied to U.S. minimum wage policy, it reveals significant conditional treatment effect heterogeneity related to county population, which traditional methods fail to capture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects, by introducing a novel non-parametric model that can provide more accurate and stable estimates.

Method: DiD-BCF is a novel non-parametric model that addresses key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating ATE, GATE, and CATE. Its core innovation is a PTA-based reparameterization that enhances estimation accuracy and stability in complex panel data settings.

Result: Extensive simulations demonstrate DiD-BCF's superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers significant conditional treatment effect heterogeneity related to county population, insights obscured by traditional methods.

Conclusion: DiD-BCF offers a robust and versatile tool for more nuanced causal inference in modern DiD applications.

Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest
(DiD-BCF), a novel non-parametric model addressing key challenges in DiD
estimation, such as staggered adoption and heterogeneous treatment effects.
DiD-BCF provides a unified framework for estimating Average (ATE),
Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core
innovation, its Parallel Trends Assumption (PTA)-based reparameterization,
enhances estimation accuracy and stability in complex panel data settings.
Extensive simulations demonstrate DiD-BCF's superior performance over
established benchmarks, particularly under non-linearity, selection biases, and
effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers
significant conditional treatment effect heterogeneity related to county
population, insights obscured by traditional methods. DiD-BCF offers a robust
and versatile tool for more nuanced causal inference in modern DiD
applications.

</details>


### [885] [Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging](https://arxiv.org/abs/2505.10279)
*Gabriel R. Palma,Sally McClean,Brahim Allan,Zeeshan Tariq,Rafael A. Moral*

Main category: stat.ME

TL;DR: 本文提出了一种新的框架，使用高斯混合模型平均和贝叶斯随机游走模型来估计家庭电视配置文件的数量及其特性。


<details>
  <summary>Details</summary>
Motivation: TV提供商需要提供个性化的体验，以节省客户在发现内容时的时间。然而，可靠地了解他们的行为和偏好是关键。当为电视创建个性化推荐时，最大的挑战是理解家庭中多个人观看时的观看行为。

Method: 我们提出了一种新的框架，使用高斯混合模型平均来获得家庭电视配置文件数量的点估计，并使用贝叶斯随机游走模型引入不确定性。

Result: 我们的结果表明，将我们的框架与选定的特征结合可以估计家庭电视配置文件的数量及其特性，包括随时间的变化和不确定性的量化。

Conclusion: 我们的结果表明，将我们的框架与选定的特征结合可以估计家庭电视配置文件的数量及其特性，包括随时间的变化和不确定性的量化。

Abstract: TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [886] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

TL;DR: The paper proposes spherical attention algorithm and FlashSign kernel using neural circuit diagrams to enable systematic reasoning about deep learning architectures. It overcomes bottlenecks in standard attention mechanisms and achieves high performance.


<details>
  <summary>Details</summary>
Motivation: To enable artificial intelligence models to improve themselves by developing a principled and systematic method for reasoning about deep learning architectures.

Method: Using neural circuit diagrams based in category theory, the authors prove a general theorem related to deep learning algorithms, develop a novel attention algorithm (spherical attention) for gene regulatory networks, and produce an efficient kernel (FlashSign).

Result: The proposed spherical attention algorithm overcomes the special function unit bottleneck of standard attention while retaining essential streaming properties. The FlashSign kernel achieves 3.6x the performance of PyTorch and is comparable to state-of-the-art FlashAttention on an A100 GPU.

Conclusion: Neural circuit diagrams are suitable as a high-level framework for the automated development of efficient, novel artificial intelligence architectures.

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


### [887] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

TL;DR: 本文利用神经电路图来改进人工智能模型，提出了一种新的注意力算法，并展示了其在性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 当前自动化编译方法较差，而高效的算法需要多年的开发。因此，需要一种更有效的方法来改进人工智能模型。

Method: 本文使用基于范畴论的神经电路图，证明了一个与深度学习算法相关的通用定理，指导了针对基因调控网络领域的新型注意力算法的开发，并产生了相应的高效内核。

Result: 所提出的球面注意力算法通过将SoftMax替换为L^2范数，克服了标准注意力的特殊函数单元瓶颈，同时保留了高性能所需的流特性。图示推导出的FlashSign内核在A100上实现了与最先进的微调FlashAttention算法相当的性能，并且比PyTorch快3.6倍。

Conclusion: 本研究展示了神经电路图作为高效、新颖人工智能架构自动开发的高层框架的适用性。

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [888] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

TL;DR: RNA语言模型被分为三类，并与DNA和蛋白质语言模型一起在零样本RNA二级结构预测和功能分类中进行了比较。结果表明，擅长二级结构预测的模型在功能分类上表现较差，反之亦然，提示需要更平衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型在结构和功能推断中的有效性，RNA语言模型在过去几年才开始受到更多关注，但缺乏统一的标准进行比较。

Method: 将RNA语言模型分为三类：在多种RNA类型（特别是非编码RNA）上预训练的模型、特定用途RNA模型以及将RNA与DNA或蛋白质或两者统一起来的模型。然后使用13个RNA语言模型、3个DNA模型和1个蛋白质模型作为对照，在零样本RNA二级结构预测和功能分类任务中进行比较。

Result: 擅长二级结构预测的模型在功能分类任务上的表现往往较差，反之亦然。

Conclusion: 需要更平衡的无监督训练来提高RNA语言模型在不同任务上的综合性能。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


### [889] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

TL;DR: 本文对13种RNA语言模型以及3种DNA和1种蛋白质语言模型进行了比较，发现它们在二级结构预测和功能分类上的表现存在矛盾，暗示需要更平衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: 由于蛋白质语言模型在结构和功能推断中的有用性，近年来RNA语言模型受到了越来越多的关注。然而，这些RNA模型通常没有经过相同的比较标准。

Method: 将RNA语言模型分为三类，并与3个DNA和1个蛋白质语言模型进行零样本预测RNA二级结构和功能分类的比较。

Result: 结果表明，模型在二级结构预测上的表现与功能分类上的表现往往相互矛盾，这表明需要更平衡的无监督训练。

Conclusion: 结果表明，那些在二级结构预测上表现良好的模型在功能分类上可能表现较差，反之亦然，这表明需要更平衡的无监督训练。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [890] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

TL;DR: 本研究比较了通用大型语言模型（GPT-4o）和推理优化模型（o1-preview）在解决德国物理奥林匹克竞赛问题上的表现，并与人类参赛者进行了对比。结果表明，两种LLM在解题能力上优于人类平均水平，其中o1-preview表现尤为突出。研究还探讨了如何在物理教育中设计评估方式以维持其完整性和促进学生对LLM的批判性使用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的普及，它们在教育中的应用引发了对学生学习过程和评估形式完整性的担忧。特别是在物理教育中，理解LLMs在特定问题解决方面的能力至关重要，这将有助于负责任地将LLMs整合到教学和评估中。

Method: 研究选取了德国物理奥林匹克竞赛的一系列明确定义的问题，比较了GPT-4o（使用不同的提示技术）和o1-preview模型与人类参赛者的解题表现。除了评估解题正确性外，还分析了LLM生成解决方案的特征优势和局限性。

Result: 结果显示，GPT-4o和o1-preview在解决奥林匹克类型物理问题上表现出高级别的解题能力，平均而言超越了人类参赛者。不同提示技术对GPT-4o的影响较小，而o1-preview几乎始终优于GPT-4o和人类基准。

Conclusion: 基于这些发现，研究讨论了物理教育中总结性和形成性评估的设计影响，包括如何保持评估的完整性以及支持学生批判性地与LLMs互动。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


### [891] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

TL;DR: 本研究比较了GPT-4o和o1-preview等大型语言模型与德国物理奥林匹克竞赛参与者的解题表现，发现大型语言模型在奥林匹克物理问题上表现出先进能力，平均优于人类参与者。研究讨论了这些发现对物理教育中评估设计的影响。


<details>
  <summary>Details</summary>
Motivation: 在物理教育中，问题解决在教学和评估中起着核心作用，因此了解大型语言模型的物理特定问题解决能力至关重要。这种理解对于制定负责任且符合教学原则的大型语言模型整合方法至关重要。

Method: 本研究比较了通用大型语言模型（GPT-4o，使用不同的提示技术）和优化推理的模型（o1-preview）与德国物理奥林匹克竞赛参与者的解题表现，基于一组定义明确的奥林匹克问题。除了评估生成解决方案的正确性外，还分析了大型语言模型生成解决方案的特点优势和局限性。

Result: 研究结果表明，两种测试的大型语言模型（GPT-4o和o1-preview）在奥林匹克类型的物理问题上表现出先进的问题解决能力，平均表现优于人类参与者。提示技术对GPT-4o的表现影响不大，而o1-preview几乎一直优于GPT-4o和人类基准。

Conclusion: 研究结果表明，GPT-4o和o1-preview这两种大型语言模型在奥林匹克物理问题解决方面表现出先进的能力，平均表现优于人类参与者。研究讨论了这些发现对物理教育中总结性评估和形成性评估设计的影响，包括如何维护评估的完整性并支持学生批判性地使用大型语言模型。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [892] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

TL;DR: CellTypeAgent, a trustworthy large language model (LLM)-agent that integrates LLMs with verification from relevant databases for cell type annotation in single-cell RNA sequencing analysis, achieves higher accuracy than existing methods while mitigating hallucinations.


<details>
  <summary>Details</summary>
Motivation: Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis.

Method: Presented is CellTypeAgent, which integrates large language models (LLMs) with verification from relevant databases.

Result: Evaluated across nine real datasets involving 303 cell types from 36 tissues, CellTypeAgent achieves higher accuracy than existing methods while reducing hallucinations.

Conclusion: The combined approach of CellTypeAgent holds promise for more efficient and reliable cell type annotation.

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [893] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

TL;DR: 研究人员应用字节对编码（BPE）技术于九个端粒到端粒（T2T）灵长类动物基因组，包括三个完整人类基因组。他们发现仅有少量的token在所有基因组中共享，并且基于token重叠构建的系统发育树不能准确反映已知的灵长类动物关系。这归因于物种特异性高拷贝重复元素的影响。研究强调了BPE技术在压缩重复序列方面的有效性，但同时指出其作为比较基因组学通用工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管T2T基因组组装的出现为比较基因组学提供了新途径，但基因组序列的有效标记化策略仍需探索。

Method: 使用自定义工具dnaBPE，训练独立的BPE标记器对九个T2T灵长类动物基因组进行分析，每个标记器具有512,000个固定词汇量。

Result: 只有11,569个token在所有基因组组装中共享，而约991,854个token仅在一个基因组中独有。基于token重叠构建的系统发育树未能重现已知的灵长类动物关系。

Conclusion: BPE标记化技术能有效压缩重复序列，但受高拷贝重复元素影响，不适合作为比较基因组学的通用工具。需要采用混合策略和重复屏蔽方法来改进基因组标记化，开发大规模基因组语言模型时应考虑领域特定的适应性。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


### [894] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

TL;DR: 本文介绍了一种名为CellTypeAgent的可信大型语言模型代理，通过结合大型语言模型和相关数据库的验证，提高了单细胞RNA测序分析中细胞类型注释的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 细胞类型注释是单细胞RNA测序分析中的关键但耗时的步骤。

Method: 提出了一种可信的大型语言模型（LLM）代理，CellTypeAgent，它将LLM与相关数据库的验证相结合。

Result: CellTypeAgent在九个真实数据集上进行了评估，涉及303种细胞类型和36种组织，其准确性高于现有方法，同时减少了幻觉。

Conclusion: 这种结合方法有望实现更高效和可靠的细胞类型注释。

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [895] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

TL;DR: 本研究探讨了BPE分词在基因组序列中的应用，发现其在处理重复序列方面有效，但受高拷贝元件影响较大，限制了其作为通用工具的效用。


<details>
  <summary>Details</summary>
Motivation: 研究BPE分词策略在基因组序列中的应用，以探索其在比较基因组学中的潜力。

Method: 使用自定义工具dnaBPE对九个T2T灵长类基因组（包括三个人类基因组）应用字节对编码（BPE）进行分析。

Result: 只有11,569个标记在所有基因组中共享，而近991,854个标记是单个基因组独有的，表明随着基因组比较的增加，共享词汇迅速减少。此外，基于标记重叠的系统发育树未能再现已知的灵长类关系。

Conclusion: BPE分词在压缩重复序列方面有效，但对高拷贝元件的敏感性限制了其作为比较基因组学通用工具的效用。需要领域特定的适应来开发大规模基因组语言模型。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [896] [$XX^{t}$ Can Be Faster](https://arxiv.org/abs/2505.09814)
*Dmitry Rybin,Yushun Zhang,Zhi-Quan Luo*

Main category: cs.DS

TL;DR: The paper introduces RXTX, a novel algorithm that computes the product of a matrix by its transpose using fewer multiplications and additions than current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithm that can compute the product of a matrix by its transpose more efficiently in terms of reducing the number of multiplications and additions.

Method: By integrating Machine Learning-based search methods with Combinatorial Optimization techniques to discover the new algorithm RXTX.

Result: RXTX uses 5% less multiplications and additions than the State-of-the-Art, providing accelerations even for small sizes of matrix X.

Conclusion: RXTX is a more efficient algorithm for computing the product of a matrix by its transpose.

Abstract: We present a new algorithm RXTX that computes product of matrix by its
transpose $XX^{t}$. RXTX uses $5\%$ less multiplications and additions than
State-of-the-Art and achieves accelerations even for small sizes of matrix $X$.
The algorithm was discovered by combining Machine Learning-based search methods
with Combinatorial Optimization.

</details>


### [897] [On Unbiased Low-Rank Approximation with Minimum Distortion](https://arxiv.org/abs/2505.09647)
*Leighton Pate Barnes,Stephen Cameron,Benjamin Howard*

Main category: cs.DS

TL;DR: The paper presents an algorithm for sampling a low-rank random matrix that optimally approximates a fixed target matrix in terms of expected Frobenius norm error.


<details>
  <summary>Details</summary>
Motivation: To develop an optimal method for approximating a fixed target matrix with a low-rank random matrix, ensuring unbiasedness and minimizing the expected Frobenius norm error.

Method: The algorithm mirrors the solution to the efficient unbiased sparsification problem for vectors but is applied to the singular components of the matrix P. It ensures that Q is unbiased, has a rank less than or equal to r, and minimizes the expected Frobenius norm error.

Result: The algorithm achieves optimality by matching the error from an existing lower bound.

Conclusion: The presented algorithm provides an optimal way to sample a low-rank random matrix that best approximates a given target matrix.

Abstract: We describe an algorithm for sampling a low-rank random matrix $Q$ that best
approximates a fixed target matrix $P\in\mathbb{C}^{n\times m}$ in the
following sense: $Q$ is unbiased, i.e., $\mathbb{E}[Q] = P$;
$\mathsf{rank}(Q)\leq r$; and $Q$ minimizes the expected Frobenius norm error
$\mathbb{E}\|P-Q\|_F^2$. Our algorithm mirrors the solution to the efficient
unbiased sparsification problem for vectors, except applied to the singular
components of the matrix $P$. Optimality is proven by showing that our
algorithm matches the error from an existing lower bound.

</details>


### [898] [$XX^{t}$ Can Be Faster](https://arxiv.org/abs/2505.09814)
*Dmitry Rybin,Yushun Zhang,Zhi-Quan Luo*

Main category: cs.DS

TL;DR: RXTX是一种新的算法，用于计算矩阵与其转置的乘积，它比现有技术更高效，并且在小尺寸矩阵上也能实现加速。


<details>
  <summary>Details</summary>
Motivation: 寻找一种更高效的计算矩阵与其转置乘积的方法，以减少计算中的乘法和加法操作。

Method: 通过将基于机器学习的搜索方法与组合优化相结合，发现了RXTX算法。

Result: RXTX算法在计算矩阵与其转置的乘积时，使用的乘法和加法操作减少了5%。

Conclusion: RXTX算法在计算矩阵与其转置的乘积时比现有技术更高效，且在小尺寸矩阵上也能实现加速。

Abstract: We present a new algorithm RXTX that computes product of matrix by its
transpose $XX^{t}$. RXTX uses $5\%$ less multiplications and additions than
State-of-the-Art and achieves accelerations even for small sizes of matrix $X$.
The algorithm was discovered by combining Machine Learning-based search methods
with Combinatorial Optimization.

</details>


### [899] [On Unbiased Low-Rank Approximation with Minimum Distortion](https://arxiv.org/abs/2505.09647)
*Leighton Pate Barnes,Stephen Cameron,Benjamin Howard*

Main category: cs.DS

TL;DR: 本文提出了一种算法，用于生成一个低秩随机矩阵Q，使其在期望Frobenius范数误差方面最好地逼近固定的目标矩阵P。


<details>
  <summary>Details</summary>
Motivation: 我们需要一种方法来生成一个低秩随机矩阵Q，使其在期望Frobenius范数误差方面最好地逼近固定的目标矩阵P。

Method: 我们提出了一个算法，用于采样一个低秩随机矩阵Q，该矩阵在期望Frobenius范数误差方面最好地逼近固定的目标矩阵P。该算法类似于解决向量高效无偏稀疏化问题的解决方案，但应用于矩阵P的奇异分量。

Result: 我们的算法生成的矩阵Q是无偏的，其秩不超过r，并且最小化了期望Frobenius范数误差。

Conclusion: 我们的算法在期望Frobenius范数误差方面是最优的，因为它与现有的下限相匹配。

Abstract: We describe an algorithm for sampling a low-rank random matrix $Q$ that best
approximates a fixed target matrix $P\in\mathbb{C}^{n\times m}$ in the
following sense: $Q$ is unbiased, i.e., $\mathbb{E}[Q] = P$;
$\mathsf{rank}(Q)\leq r$; and $Q$ minimizes the expected Frobenius norm error
$\mathbb{E}\|P-Q\|_F^2$. Our algorithm mirrors the solution to the efficient
unbiased sparsification problem for vectors, except applied to the singular
components of the matrix $P$. Optimality is proven by showing that our
algorithm matches the error from an existing lower bound.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [900] [Deconstructing Subset Construction -- Reducing While Determinizing](https://arxiv.org/abs/2505.10319)
*John Nicol,Markus Frohme*

Main category: cs.FL

TL;DR: The paper presents a new method for NFA canonization using intermediate minimization steps and equivalence registries, which can be integrated into existing approaches, showing improvements in worst-case scenarios.


<details>
  <summary>Details</summary>
Motivation: To reduce the exploration space during NFA canonization and improve performance, especially in worst-case scenarios.

Method: Introducing intermediate minimization steps and utilizing equivalence registries to manage equivalent states, allowing for additional optimizations like convexity closures or simulation.

Result: Evaluation on real-world examples shows improvement in worst-case scenarios for NFA canonization.

Conclusion: The novel approach is effective and can be embedded in classic methods, with an open-source implementation available for experimentation.

Abstract: We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.

</details>


### [901] [Deconstructing Subset Construction -- Reducing While Determinizing](https://arxiv.org/abs/2505.10319)
*John Nicol,Markus Frohme*

Main category: cs.FL

TL;DR: 本文提出了一种新的NFA规范化方法，通过中间最小化步骤和等价注册表来提高性能，并在实际案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的NFA规范化方法在处理某些情况时效率较低，因此需要一种新的方法来优化性能。

Method: 我们引入了中间最小化步骤和等价注册表，以减少探索空间并提高性能。

Result: 我们在实际案例中测试了该方法，并观察到在最坏情况下有显著的性能提升。

Conclusion: 我们的方法在实际案例中表现出色，特别是在最坏情况下的性能提升。

Abstract: We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [902] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

TL;DR: An innovative training method for dynamical systems governed by Lagrangian mechanics is proposed, which uses Equilibrium Propagation and the principle of action extremization to adjust system trajectories towards desired targets.


<details>
  <summary>Details</summary>
Motivation: Current methods for training dynamical systems may be inefficient or complex, especially when dealing with periodic boundary conditions or fixed initial and final states. There is a need for an approach that can efficiently update parameters without explicit backpropagation through time.

Method: The paper introduces a method extending Equilibrium Propagation to dynamical trajectories by utilizing the principle of action extremization. The system is trained by slightly nudging its trajectories towards desired targets and observing the response of variables conjugate to the parameters being trained.

Result: This method allows efficient parameter updates in systems with periodic boundary conditions or fixed initial and final states, without requiring explicit backpropagation through time. It also yields the semiclassical limit of Quantum Equilibrium Propagation in the case of periodic boundary conditions and has potential applications in systems with dissipation.

Conclusion: The proposed method provides an effective way to train dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation and action extremization, enabling efficient learning in specific types of systems.

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


### [903] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

TL;DR: 本文提出了一种基于平衡传播的方法来训练由拉格朗日力学控制的动力系统，通过作用极值原理实现高效的参数更新，适用于周期性边界条件或固定初始和最终状态的系统。


<details>
  <summary>Details</summary>
Motivation: 传统的反向传播通过时间（BPTT）方法在处理动力系统时存在计算复杂度高的问题。本文旨在提供一种更高效的方法来训练这些系统，特别是在具有周期性边界条件或固定初始和最终状态的情况下。

Method: 本文的方法基于平衡传播，通过作用极值原理将平衡传播扩展到动力轨迹。训练过程中，通过轻微调整轨迹以接近目标，并测量与要训练的参数共轭的变量的响应。

Result: 本文的方法能够在不显式进行反向传播通过时间的情况下实现高效的参数更新。在周期性边界条件的情况下，该方法得到了量子平衡传播的半经典极限。此外，还讨论了在耗散系统中的应用。

Conclusion: 本文提出了一种使用平衡传播训练由拉格朗日力学控制的动力系统的方法。该方法通过利用作用极值原理将平衡传播扩展到动力轨迹，并在具有周期性边界条件或固定初始和最终状态的系统中实现了高效的参数更新。

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [904] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

TL;DR: The paper establishes a novel lower bound for the Neyman-Pearson boundary using $f$-divergence, improves Pinsker's inequality with KL divergence, obtains an upper bound in terms of the Chernoff $\alpha$-coefficient, and presents methods for constructing distribution pairs to realize given boundaries.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of the Neyman-Pearson region by deriving new bounds for its boundary and presenting methods to construct distribution pairs that can realize given boundaries.

Method: Establishing a novel lower bound for the Neyman-Pearson boundary using any $f$-divergence, improving Pinsker's inequality with KL divergence, obtaining a closed-form refined upper bound using the Chernoff $\alpha$-coefficient, and proposing methods for constructing pairs of distributions to approximately or exactly realize given Neyman-Pearson boundaries.

Result: A novel lower bound for the Neyman-Pearson boundary was established in terms of any $f$-divergence, which is best possible when generated by hockey-stick $f$-divergences. An improved Pinsker's inequality was achieved with KL divergence. A closed-form refined upper bound for the Neyman-Pearson boundary was obtained in terms of the Chernoff $\alpha$-coefficient. Methods for constructing pairs of distributions were presented.

Conclusion: The analysis provides new insights into the Neyman-Pearson region by offering tighter bounds on its boundary and demonstrating how to construct distribution pairs to achieve specific boundaries.

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>


### [905] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

TL;DR: The paper extends standard decision theory to include counterfactual losses for better assessing decision quality.


<details>
  <summary>Details</summary>
Motivation: Classical statistical decision theory evaluates treatment choices based solely on observed outcomes, ignoring counterfactual outcomes which limits its ability to assess the quality of decisions relative to feasible alternatives.

Method: Extend standard decision theory to incorporate counterfactual losses under the assumption of strong ignorability. Identify that a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes.

Result: Counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions when more than two treatment options are involved.

Conclusion: Incorporating counterfactual losses into decision theory allows for a more comprehensive evaluation of decision quality.

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


### [906] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

TL;DR: 本文研究了二元假设检验的Neyman-Pearson区域边界，提出了一种新的下界，并改进了现有不等式，同时提供了构造分布对的方法。


<details>
  <summary>Details</summary>
Motivation: 研究Neyman-Pearson区域的边界对于理解二元假设检验的性能具有重要意义。本文旨在提供新的理论结果，以改进现有的不等式并扩展其应用。

Method: 本文利用f-散度和Chernoff α系数来分析Neyman-Pearson区域的边界，并通过构造分布对来实现特定的边界。

Result: 本文提出了一个新的下界，该下界在某些情况下是最佳的，并且在KL散度的情况下改进了Pinsker不等式。同时，得到了一个闭式上界，并提出了构建分布对的方法。

Conclusion: 本文提出了一个新的下界，用于描述二元假设检验的Neyman-Pearson区域边界，并展示了该下界在KL散度情况下的改进。此外，还得到了一个闭式上界，并提出了构建分布对的方法以实现给定的Neyman-Pearson边界。

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>


### [907] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

TL;DR: 本文扩展了标准决策理论，引入反事实损失以更全面地评估治疗选择，并展示了在特定条件下反事实损失函数如何影响治疗建议。


<details>
  <summary>Details</summary>
Motivation: 传统统计决策理论仅基于观察结果评估治疗选择，但忽略了反事实结果，因此无法相对可行替代方案评估决策质量。

Method: 本文通过假设强可忽略性，研究了反事实风险的可识别性，并探讨了在多于两个治疗选项的情况下，反事实损失函数如何影响治疗建议。

Result: 在强可忽略性假设下，反事实风险只有在反事实损失函数是潜在结果的可加函数时才可识别。此外，反事实损失函数可以产生与标准损失函数不同的治疗建议。

Conclusion: 本文提出了一种扩展的标准决策理论，以纳入反事实损失，从而更全面地评估治疗选择的质量。

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [908] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

TL;DR: This paper introduces RT-cache, a novel trajectory memory pipeline that accelerates robot inference via big-data retrieval and learning. It stores successful trajectories, retrieves relevant motions, and adapts to new environments with few samples. Experiments show it outperforms baselines without retrieval.


<details>
  <summary>Details</summary>
Motivation: Modern Vision-Language-Action (VLA) models can handle diverse robotic tasks but suffer from high per-step inference costs and latency.

Method: RT-cache includes a Memory Builder and Trajectory Retrieval system. It stores large-scale memory of successful robot trajectories and retrieves relevant multistep motion snippets for current scenes.

Result: Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks faster and more successfully than a baseline lacking retrieval.

Conclusion: RT-cache provides a practical, data-driven solution for real-time robotic manipulation by reducing inference overhead through experience-based retrieval.

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [909] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: This paper introduces a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) collaborate to accomplish missions specified in natural language, using a Large Language Model (LLM)-enabled planner.


<details>
  <summary>Details</summary>
Motivation: As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification.

Method: The system leverages a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. The system considers task-driven navigation in urban and rural areas and must infer mission-relevant semantics and actively acquire information via semantic mapping.

Result: The system was demonstrated on seven different natural-language specifications at up to kilometer-scale navigation in both ground and air-ground teaming experiments.

Conclusion: The authors have presented a system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly.

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [910] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

TL;DR: The paper introduces ChicGrasp, an end-to-end hardware-software co-design for automating poultry processing. It uses a pneumatic gripper and a conditional diffusion-policy controller trained on 50 teleoperation demonstrations to grasp and lift chicken carcasses with a 40.6% success rate.


<details>
  <summary>Details</summary>
Motivation: To automate the task of lifting slippery chicken carcasses onto a shackle conveyor in poultry processing lines, addressing challenges such as deformability, anatomical variance, and hygiene rules that make conventional methods unreliable.

Method: ChicGrasp employs a dual-jaw pneumatic gripper to clamp chicken legs and a conditional diffusion-policy controller trained from multi-view teleoperation demonstrations (RGB + proprioception) to plan 5 DoF end-effector motion including jaw commands.

Result: Achieves a 40.6% grasp-and-lift success rate and completes the pick-to-shackle cycle in 38 seconds, outperforming state-of-the-art implicit behaviour cloning (IBC) and LSTM-GMM baselines which fail entirely.

Conclusion: ChicGrasp demonstrates that imitation learning can bridge the gap between rigid hardware and variable bio-products, providing a reproducible benchmark and public dataset for agricultural engineering and robot learning research.

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [911] [Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery](https://arxiv.org/abs/2505.08932)
*Mohammad Wasil,Ahmad Drak,Brennan Penfold,Ludovico Scarton,Maximilian Johenneken,Alexander Asteroth,Sebastian Houben*

Main category: cs.RO

TL;DR: The paper adapts the Segment Anything Model (SAM) using parameter-efficient fine-tuning (PEFT) for forest floor object segmentation, achieving high mIoU. LoRA is proposed as a lightweight alternative for UAVs.


<details>
  <summary>Details</summary>
Motivation: Forest floor understanding is challenging due to variability and ambiguous annotations. SAM's strong generalization capabilities make it suitable for this task.

Method: Adapt SAM with PEFT by fine-tuning a subset of parameters and adjusting the mask decoder for automatic segmentation of forest floor objects.

Result: Adapter-based PEFT achieves the highest mIoU. LoRA offers a lightweight solution for UAVs.

Conclusion: SAM adapted with PEFT effectively segments forest floor objects, with LoRA being a viable option for resource-constrained UAV platforms.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.

</details>


### [912] [Multi-step manipulation task and motion planning guided by video demonstration](https://arxiv.org/abs/2505.08949)
*Kateryna Zorina,David Kovar,Mederic Fourmy,Florent Lamiraux,Nicolas Mansard,Justin Carpentier,Josef Sivic,Vladimir Petrik*

Main category: cs.RO

TL;DR: This paper leverages instructional video to solve complex multi-step task-and-motion planning tasks in robotics.


<details>
  <summary>Details</summary>
Motivation: To solve complex multi-step task-and-motion planning tasks in robotics by utilizing instructional video as guidance.

Method: Propose an extension of the RRT planner which simultaneously grows multiple trees around grasp and release states extracted from the guiding video, combining contact states and 3D object poses extracted from the video with a traditional planning algorithm.

Result: Demonstrate the effectiveness of the proposed planning algorithm on several robots through a new benchmark with three challenging tasks.

Conclusion: Developed a trajectory refinement approach formulated as an optimal control problem for a seamless transfer of the obtained plans to the real robot.

Abstract: This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).

</details>


### [913] [FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis](https://arxiv.org/abs/2505.09109)
*Yuxing Chen,Bowen Xiao,He Wang*

Main category: cs.RO

TL;DR: This paper addresses the challenge of generating high-quality data for robotic garment manipulation by presenting a synthetic garment dataset and proposing KG-DAgger, which improves model performance and boosts real-world success rate.


<details>
  <summary>Details</summary>
Motivation: The deformability of garments makes it difficult to generate a large amount of high-quality data for robotic garment manipulation tasks.

Method: The authors construct geometric garment templates based on keypoints and apply generative models to create realistic texture patterns. They use these keypoint annotations to generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. They also propose KG-DAgger, a keypoint-based strategy to generate demonstration data for recovering from failures.

Result: After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75% success rate in the real world, with KG-DAgger boosting the real-world success rate by 25%. Experiments validate the effectiveness of the proposed framework in both simulation and real-world settings.

Conclusion: The synthetic garment dataset and KG-DAgger significantly improve the robustness and success rate of robotic garment folding.

Abstract: Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.

</details>


### [914] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

TL;DR: This paper proposes a customized wearable robotic controller using reinforcement learning and a musculoskeletal model to compensate for specific muscle deficits in hand-object manipulation tasks. The controller is trained with video data of human grasping tasks and fine-tuned for object-specific interactions. When integrated into a virtual exoglove, it provides shared assistance for weakened hand muscles, achieving 90.5% of original manipulation proficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the need for more personalized rehabilitation solutions for patients with hand motor impairments, acknowledging the uniqueness of each patient's muscle loss.

Method: The method involves using reinforcement learning on a biologically accurate musculoskeletal model to create a customized wearable robotic controller. Video data of human grasping tasks is used to train a manipulation model which is then fine-tuned for specific object interaction tasks. Muscle forces in the model are weakened to simulate impairments and compensated by a virtual wearable robotics glove.

Result: The results demonstrate that integrating the virtual wearable robotic glove provides effective shared assistance to support hand manipulators with weakened muscle forces. The learned exoglove controller achieves an average of 90.5% of the original manipulation proficiency.

Conclusion: In conclusion, the proposed customized wearable robotic controller shows promise in addressing specific muscle deficits in hand-object manipulation tasks through the use of reinforcement learning and musculoskeletal models.

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [915] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

TL;DR: TransDiffuser is an encoder-decoder model for generative trajectory planning in autonomous driving, which uses scene information as multi-modal conditional input and introduces a decorrelation optimization mechanism to solve mode collapse dilemma. It achieves PDMS of 94.85 on NAVSIM benchmark without anchor-based prior trajectories.


<details>
  <summary>Details</summary>
Motivation: To transfer the capabilities of diffusion models to modern autonomous driving systems and generate high-quality diverse trajectories.

Method: Propose TransDiffuser, an encoder-decoder based generative trajectory planning model. The encoded scene information serves as the multi-modal conditional input of the denoising decoder and a multi-modal representation decorrelation optimization mechanism is introduced during the training process.

Result: Achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.

Conclusion: TransDiffuser successfully generates high-quality diverse trajectories in autonomous driving systems and outperforms previous methods.

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [916] [APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression](https://arxiv.org/abs/2505.09356)
*Srinivas Ravuri,Yuan Xu,Martin Ludwig Zehetner,Ketan Motlag,Sahin Albayrak*

Main category: cs.RO

TL;DR: APR-Transformer是一种新的模型架构，使用图像或LiDAR数据预测绝对姿态（3D位置和3D方向），在多个数据集上达到最先进的性能，并在GNSS拒绝环境下通过实车验证。


<details>
  <summary>Details</summary>
Motivation: 精确初始化对于定位算法的性能至关重要，特别是在机器人、自动驾驶和计算机视觉领域。不准确的初始姿态会导致定位精度差，尤其是在无法依赖GPS信号的环境中。利用深度神经网络进行姿态回归的最新进展显著提高了准确性和鲁棒性。

Method: 引入了APR-Transformer模型架构，该架构可以使用图像或LiDAR数据来预测绝对姿态（3D位置和3D方向）。该方法在几个基准数据集上进行了测试，并扩展到自定义的复杂数据集APR-BeIntelli。此外，在GNSS拒绝环境下，通过在自主测试车辆上实时部署模型来验证其可靠性。

Result: 在Radar Oxford Robot-Car、DeepLoc和APR-BeIntelli数据集上取得了最先进的性能，并在GNSS拒绝环境下的实车测试中展示了实际可行性和有效性。

Conclusion: APR-Transformer在多种数据集上表现出色，并在GNSS拒绝环境下通过实车验证，证明了其实际可行性和有效性。源代码已公开。

Abstract: Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.

</details>


### [917] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: The paper discusses deploying foundation model (FM)-enabled robots in large-scale and unstructured environments, presenting SPINE - an LLM-enabled autonomy framework. It showcases the first large-scale LLM-based robot planning in such settings and introduces a language-driven UAV planner using onboard language models.


<details>
  <summary>Details</summary>
Motivation: Foundation model (FM)-enabled robots have been primarily used in closed-world settings with full prior maps or complete workspace views. The motivation is to address the challenge of deploying these robots in real-world missions that require operation in large-scale and unstructured environments.

Method: The method involves using SPINE, an LLM-enabled autonomy framework, which is adaptable to different LLMs and can be distilled into smaller models for SWaP-limited platforms. This approach enables effective exploration, navigation through obstacles, handling unexpected sensor inputs, and operation within compute constraints.

Result: The result includes successful deployments of SPINE in field robotic settings, demonstrating the first large-scale LLM-enabled robot planning over several kilometers in unstructured environments. Additionally, a language-driven UAV planner using on-device language models was presented.

Conclusion: The conclusion proposes several promising directions for future research, indicating the potential advancements in FM-enabled robotics for complex, real-world applications.

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [918] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出Past-Token Prediction (PTP) 方法，通过预测过去动作标记来增强策略对未来和过去动作之间依赖关系的捕捉，有效解决了长序列任务中的信息保留问题。实验表明，该方法将长上下文扩散策略性能提升3倍，并加速训练10倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前学习长上下文策略的方法通常通过截断上下文长度来规避挑战，但这种方法会丢弃对后续决策至关重要的历史信息。因此需要一种新方法来显式地规范过去信息的保留。

Method: 作者引入了Past-Token Prediction (PTP) 作为辅助任务，让策略同时学习预测过去和未来动作标记。此外，还提出了一个多阶段训练策略：先用短上下文预训练视觉编码器，再用缓存的长上下文嵌入微调策略头。最后，扩展PTP为自验证机制以在推理时选择与过去动作一致的候选方案。

Result: 在四个真实世界和六个模拟任务上的实验表明，所提方法使长上下文扩散策略的性能提高了3倍，并且将策略训练速度提升了10倍以上。

Conclusion: Past-Token Prediction (PTP) 辅助任务显著改善了策略的时间建模能力，多阶段训练策略大幅降低了内存和计算开销，而自验证机制进一步增强了推理时的一致性。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [919] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

TL;DR: The paper presents Mini-Diffuser, a method for training multi-task vision-language robotic diffusion policies that significantly reduces training time and memory usage while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: To reduce the training time and memory usage for multi-task vision-language robotic diffusion policies by exploiting the dimensional differences between action diffusion and image diffusion techniques.

Method: Mini-Diffuser uses Level-2 minibatching, pairing multiple noised action samples with each vision-language condition. Architectural adaptations to the diffusion transformer prevent information leakage across samples while maintaining full conditioning access.

Result: In RLBench simulations, Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies, using only 5% of the training time and 7% of the memory. Real-world experiments validate its ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs.

Conclusion: Mini-Diffuser effectively reduces resource requirements for training multi-task vision-language robotic diffusion policies without significant loss in performance.

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [920] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: This paper addresses policy distillation under privileged information by introducing two methods to help the student policy learn effectively from a teacher with full-state access.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the problem of information asymmetry in policy distillation, where the student policy has only partial observations and cannot directly access the teacher's state space.

Method: The authors propose two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration.

Result: The proposed methods were validated in both simulated and real-world robotic tasks, showing significant improvements over standard teacher-student baselines in terms of training efficiency and final performance.

Conclusion: The introduced methods provide a more efficient way for the student policy to learn from the teacher policy despite information asymmetry.

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [921] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: DataMIL is a policy-driven data selection framework that optimizes data selection for task success in robotics, showing consistent gains in success rates and superior performance over multiple baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of generalist robot policies which underperform on individual, specialized tasks. The authors aim to improve specialized policies by combining task-specific data with subsets of large prior datasets via co-training, while avoiding naive data selection that may harm downstream performance.

Method: The method introduced is DataMIL, a policy-driven data selection framework based on the datamodels paradigm. It uses an end-to-end approach to select data points that will most improve performance, directly optimizing for task success rather than relying on human notions of quality. A novel surrogate loss function on task-specific data is used to avoid expensive rollouts in the environment during selection.

Result: The approach was validated on over 60 simulation and real-world manipulation tasks, demonstrating consistent gains in success rates and superior performance compared to multiple baselines. Notably, successful data selection was shown from the Open X-Embodiment datasets.

Conclusion: The results highlight the importance of end-to-end, performance-aware data selection for maximizing the potential of large prior datasets in robotics.

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


### [922] [ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation](https://arxiv.org/abs/2505.09698)
*Enyu Zhao,Vedant Raval,Hejia Zhang,Jiageng Mao,Zeyu Shangguan,Stefanos Nikolaidis,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: Vision-Language Models (VLMs) have transformed AI and robotics. While used in high-level planning, recent studies explore their lower-level reasoning ability for precise robot movements. A new benchmark, ManipBench, is proposed to evaluate VLMs' low-level robotic manipulation capabilities across different dimensions. Testing 33 representative VLMs from 10 model families on this benchmark reveals significant performance variation across tasks, correlating with real-world manipulation trends, but a substantial gap remains between these models and human-level understanding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a clear and common benchmark evaluating how well Vision-Language Models (VLMs) can aid low-level reasoning in robotics, particularly in understanding object-object interactions and deformable object manipulation.

Method: Propose a novel benchmark called ManipBench to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions. Extensively test 33 representative VLMs from 10 model families on this benchmark, including variants testing different model sizes.

Result: The evaluation shows significant variation in VLM performance across tasks, strong correlation between this performance and trends in real-world manipulation tasks, and a significant gap between these models and human-level understanding.

Conclusion: ManipBench provides a comprehensive evaluation framework for VLMs' low-level robotic manipulation reasoning capabilities, revealing performance variations, correlations with real-world tasks, and highlighting the need for further advancements to approach human-level understanding.

Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.

</details>


### [923] [FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation](https://arxiv.org/abs/2505.10075)
*Jun Guo,Xiaojian Ma,Yikai Wang,Min Yang,Huaping Liu,Qing Li*

Main category: cs.RO

TL;DR: The paper presents FlowDreamer, a model that uses 3D scene flow for predicting future frames in robot manipulation tasks, showing improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: To develop better visual world models for robot manipulation that can accurately predict future visual observations using past frames and robot actions.

Method: FlowDreamer predicts 3D scene flow using a U-Net and then uses a diffusion model to predict future frames, trained end-to-end despite its modular nature.

Result: FlowDreamer outperforms baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate across various robot manipulation domains.

Conclusion: FlowDreamer, with its explicit motion representation through 3D scene flow, provides enhanced performance in video prediction and visual planning tasks for robot manipulation.

Abstract: This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.

</details>


### [924] [Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering](https://arxiv.org/abs/2505.10073)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: A novel framework for Multi-Robot Task Allocation (MRTA) and collision avoidance is presented, using spatial clustering, K-means, and 2-Opt algorithm to reduce time by up to 93% and eliminate collisions in industrial environments.


<details>
  <summary>Details</summary>
Motivation: To create a combined solution for MRTA and collision avoidance in industrial environments with homogeneous measurement tasks, improving computational efficiency and ensuring operation free from collisions.

Method: The method uses spatial clustering to divide the workspace into operational zones for each robot, applying K-means clustering to divide task sites and the 2-Opt algorithm to schedule robot routes within corresponding clusters.

Result: The framework achieves up to 93% time reduction and a 7% improvement in solution quality compared to the best performing method, while completely eliminating collision points.

Conclusion: The findings indicate that spatial partitioning unifies task allocation and collision avoidance problems, making this approach highly significant for real-world applications requiring both computational efficiency and collision-free operations.

Abstract: In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.

</details>


### [925] [EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation](https://arxiv.org/abs/2505.10105)
*Zibin Dong,Fei Ni,Yifu Yuan,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: The paper introduces EmbodiedMAE, a multi-modal masked autoencoder that learns representations across RGB, depth, and point cloud modalities to improve robot manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information.

Method: Enhance the DROID dataset with high-quality depth maps and point clouds to construct DROID-3D, then develop EmbodiedMAE which is a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion.

Result: EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms.

Conclusion: EmbodiedMAE is established as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly effective in precise tabletop manipulation settings where spatial perception is critical.

Abstract: We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.

</details>


### [926] [Learning Rock Pushability on Rough Planetary Terrain](https://arxiv.org/abs/2505.09833)
*Tuba Girgin,Emre Girgin,Cagri Kilic*

Main category: cs.RO

TL;DR: 在非结构化环境中，通过移动障碍物而非避开它来提高多智能体重复使用的路径效率。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，传统的路径规划算法依赖于避障，这可能导致长期效率低下，并需要持续的路径规划系统支持。为了解决这个问题，提出了一种新的方法。

Method: 利用安装在移动机器人上的机械臂的操作能力，结合外部和本体感觉反馈来评估障碍物的可推性，从而实现障碍物的重新定位。初步视觉估计考虑了障碍物及其所在表面的特性，而推动可负担性评估模块则利用与障碍物交互时通过机械臂获得的力反馈作为引导信号。

Result: 该导航方法旨在通过减少车队在需要自主基础设施发展的环境（如月球或火星表面）中所花费的总体时间，提高多智能体长时间使用路径的效率。

Conclusion: 通过整合机械臂操作和反馈机制，提出的框架可以有效改善非结构化环境中多智能体路径的长期效率。

Abstract: In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.

</details>


### [927] [IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning](https://arxiv.org/abs/2505.10442)
*Dechen Gao,Hang Wang,Hanchu Zhou,Nejib Ammar,Shatadal Mishra,Ahmadreza Moradipari,Iman Soltani,Junshan Zhang*

Main category: cs.RO

TL;DR: IN-RIL是一种将模仿学习（IL）和强化学习（RL）交织进行的策略微调方法，通过周期性注入IL更新来提高稳定性及采样效率，并通过梯度分离机制防止优化冲突。实验表明，IN-RIL在多个机器人操作和运动任务中显著提高了采样效率并缓解了性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人学习方法通常采用IL预训练加RL微调的两步范式，但这种范式在RL微调阶段存在不稳定性和样本效率低的问题。因此需要一种新的方法，在整个微调过程中都能从IL的稳定性和专家数据指导中受益，同时保持RL的探索能力。

Method: 提出了一种名为IN-RIL的方法，该方法通过在多次RL更新后定期注入IL更新，使策略在整个微调过程中既能从IL的稳定性中受益，又能得到专家数据的指导以实现更有效的探索。此外，开发了梯度分离机制，以防止由于IL和RL的不同优化目标而产生的破坏性干扰。

Result: 广泛的实验结果表明，IN-RIL可以在长视域和短视域任务中、稀疏或密集奖励的任务中，显著提高采样效率并减轻在线微调期间的性能崩溃问题。例如，在Robomimic Transport任务上，成功率从12%提升到88%，提升了6.3倍。

Conclusion: IN-RIL作为一种通用插件，与各种最先进的RL算法兼容，能够显著改善RL微调过程中的稳定性和采样效率。

Abstract: Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.

</details>


### [928] [Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests](https://arxiv.org/abs/2505.10033)
*Luis F. W. Batista,Stéphanie Aravecchia,Seth Hutchinson,Cédric Pradalier*

Main category: cs.RO

TL;DR: This paper evaluates the resilience of a DRL-based agent for Autonomous Surface Vehicles (ASVs) under various perturbations, finding that it performs reliably despite significant disturbances.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient exploration of robustness in real-world conditions for DRL-based ASVs, especially under external disturbances.

Method: The agent is trained using domain randomization and its performance is evaluated in both simulation and real-world experiments under unexpected disturbances like asymmetric drag and off-center payload. Performance degradation is quantified and benchmarked against an MPC baseline.

Result: Results show that the DRL agent performs reliably even with significant disturbances.

Conclusion: The study provides insights into effective training strategies, real-world challenges, and practical considerations for deploying DRL-based ASV controllers, alongside releasing the implementation open-source.

Abstract: Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.

</details>


### [929] [Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation](https://arxiv.org/abs/2505.10522)
*Xinrui Wang,Yan Jin*

Main category: cs.RO

TL;DR: An abstract about a Knowledge Capture, Adaptation, and Composition (KCAC) framework to improve reinforcement learning in robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning has shown great potential but is limited by sample inefficiency and lack of interpretability. To address these challenges and enhance the agent's understanding and adaptability, there is a need for strategic knowledge utilization.

Method: The KCAC framework integrates knowledge transfer into RL via cross-task curriculum learning. It involves redesigning the benchmark reward function for flexibility, defining sub-tasks, and implementing a structured curriculum to facilitate efficient learning.

Result: KCAC achieves a 40% reduction in training time and improves task success rates by 10% compared to traditional RL methods. Key curriculum design parameters are identified for optimizing learning efficiency.

Conclusion: This work provides valuable insights into curriculum design in reinforcement learning and robotic learning, offering conceptual guidance for future curriculum-based RL frameworks.

Abstract: Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.

</details>


### [930] [Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning](https://arxiv.org/abs/2505.10547)
*Milan Ganai,Rohan Sinha,Christopher Agia,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures, outperforming slow reasoning models in safety classification accuracy and improving system safety and planning success.


<details>
  <summary>Details</summary>
Motivation: Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, but current methods lack the ability to plan generalizable, semantically safe motions due to high inference latency of Large Vision and Language Models.

Method: FORTRESS uses multi-modal reasoners at a low frequency in nominal operations to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time.

Result: FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.

Conclusion: By bridging open-world, multi-modal reasoning with dynamics-aware planning, FORTRESS eliminates the need for hard-coded fallbacks and human safety interventions.

Abstract: Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.

</details>


### [931] [AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics](https://arxiv.org/abs/2505.10398)
*Alexandre Banks,Randy Moore,Sayem Nazmuz Zaman,Alaa Eldin Abdelaal,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: The paper introduces AutoCam, an automatic auxiliary camera placement method for RAMIS using da Vinci Research Kit. It ensures robust camera tracking with high visibility and low pose error while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To enhance spatial awareness in RAMIS by incorporating an autonomous auxiliary camera that eliminates manual viewpoint control and addresses limitations of existing path planning methods which do not simultaneously account for camera orientation, workspace constraints, and robot joint limits.

Method: AutoCam uses a priority-based, workspace-constrained control algorithm combining heuristic geometric placement with nonlinear optimization. Implemented on the da Vinci Research Kit, it tracks a salient feature autonomously.

Result: Maintained 99.84% visibility of a salient feature with a pose error of 4.36 ± 2.11 degrees and 1.95 ± 5.66 mm. The controller was computationally efficient with a loop time of 6.8 ± 12.8 ms. Novices performed equally well using AutoCam's viewpoint as with the endoscope's during a training task.

Conclusion: An auxiliary camera can be autonomously controlled using the da Vinci patient-side manipulators to track a salient feature, paving the way for new multi-camera visualization methods in RAMIS.

Abstract: Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.

</details>


### [932] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

TL;DR: RT-cache 是一种新型轨迹记忆管道，通过利用大数据检索和经验学习来加速真实世界的机器人推理。


<details>
  <summary>Details</summary>
Motivation: 现代 Vision-Language-Action (VLA) 模型在处理多样化的机器人任务时，通常会产生较高的每步推理成本，导致显著的延迟，有时每个任务需要几分钟。

Method: RT-cache 通过存储大量之前成功的机器人轨迹并检索相关多步骤运动片段，从而减少推理开销。它结合了 Memory Builder 和 Trajectory Retrieval，开发了一个高效的检索过程。

Result: 在 Open-X Embodiment 数据集和其他真实数据上的实验表明，RT-cache 在任务执行速度和成功率上优于缺乏检索的基线。

Conclusion: RT-cache 提供了一种实用的数据驱动解决方案，用于实时操作，能够在任务执行速度和成功率上优于缺乏检索的基线。

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [933] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种基于大型语言模型的系统，使无人机和无人地面车辆能够协作完成自然语言指定的任务，并在城市和农村地区展示了其能力。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人系统的成熟，用户希望以意图级别而非低级细节来指定任务。然而，实现语言引导的机器人团队需要克服重大技术障碍，包括语义推理、异构机器人协调以及在间歇性通信下的稳健策略。

Method: 本文利用基于大型语言模型的规划器，在在线构建并机会性共享的语义-度量地图上进行语义推理，并通过语义映射主动获取信息。

Result: 在地面和空地团队实验中，本文系统在七种不同的自然语言规范下实现了高达公里级的导航。

Conclusion: 本文提出了一种首个实现自然语言指导的无人机和无人地面车辆协作完成任务的系统，展示了在城市和农村地区进行任务驱动导航的能力。

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [934] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

TL;DR: ChicGrasp是一種用於自動化家禽處理的端到端硬體-軟體協同設計，利用模仿學習來解決傳統方法在處理易變生物產品時的可靠性問題。


<details>
  <summary>Details</summary>
Motivation: 自動家禽加工線仍然依賴人類將滑溜、易受傷的家禽 carcasses 放到掛鉤傳送帶上。變形性、解剖學差異和嚴格的衛生規則使傳統吸力和腳本動作不可靠。

Method: ChicGrasp是一種端到端的硬體-軟體協同設計，使用獨立驅動的雙爪氣動夾具夾住雞腿，並使用條件擴散策略控制器進行5自由度末端執行器運動規劃。

Result: 在個別展示的原始肉雞屍體上，系統達到了40.6%的抓取和提升成功率，並在38秒內完成從拾取到掛鉤的週期，而現有的隱式行為克隆(IBC)和LSTM-GMM基線完全失敗。

Conclusion: ChicGrasp展示了模仿學習可以在剛性硬體和變異生物產品之間建立橋樑，為農業工程和機器人學習的研究人員提供了可重現的基準和公開資料集。

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [935] [Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery](https://arxiv.org/abs/2505.08932)
*Mohammad Wasil,Ahmad Drak,Brennan Penfold,Ludovico Scarton,Maximilian Johenneken,Alexander Asteroth,Sebastian Houben*

Main category: cs.RO

TL;DR: 本文研究了如何利用SAM模型和PEFT方法对森林地面对象进行自动分割，并比较了不同方法的性能。


<details>
  <summary>Details</summary>
Motivation: 由于自然变异性高、环境参数变化快以及标注不明确，对森林地面的详细理解仍然是一个挑战。为此，我们适应了具有强大泛化能力的视觉基础模型Segment Anything Model（SAM），以分割森林地面对象，如树桩、植被和木屑。

Method: 我们采用参数高效的微调（PEFT）方法，对SAM模型进行微调，同时保持原始权重不变。我们调整了SAM的掩码解码器，以生成与我们的数据集类别对应的掩码，从而实现无需手动提示的自动分割。

Result: 基于适配器的PEFT方法在mIoU方面表现最佳，而LoRA则提供了资源受限的UAV平台的轻量级替代方案。

Conclusion: 我们的实验结果表明，基于适配器的PEFT方法在mIoU方面表现最佳，而LoRA则提供了资源受限的UAV平台的轻量级替代方案。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.

</details>


### [936] [Multi-step manipulation task and motion planning guided by video demonstration](https://arxiv.org/abs/2505.08949)
*Kateryna Zorina,David Kovar,Mederic Fourmy,Florent Lamiraux,Nicolas Mansard,Justin Carpentier,Josef Sivic,Vladimir Petrik*

Main category: cs.RO

TL;DR: 本文提出了一种基于教学视频的机器人任务与运动规划方法，通过扩展RRT算法来解决复杂的多步骤任务。实验结果表明该方法在多个机器人平台上有效，并且能够将生成的计划无缝转移到真实机器人上。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人任务与运动规划方法在处理复杂多步骤任务时存在局限性，特别是需要顺序依赖的任务。因此，本文旨在利用教学视频来解决这些问题。

Method: 本文提出了一种扩展的RRT规划器，同时在从教学视频中提取的抓取和释放状态周围生长多棵树。结合接触状态和3D物体姿态，利用传统规划算法解决具有顺序依赖性的任务。此外，还设计了一个新的基准测试来验证所提方法的有效性。

Result: 本文的方法在三个具有挑战性的任务上得到了验证：(I) 3D物体在桌子和架子之间的重新排列，(ii) 物体通过隧道的多步骤传输，以及(iii) 使用托盘传输物体。实验结果表明该方法在多个机器人平台上有效。

Conclusion: 本文提出了一种基于教学视频的机器人任务与运动规划方法，通过扩展RRT算法来解决复杂的多步骤任务。实验结果表明该方法在多个机器人平台上有效，并且能够将生成的计划无缝转移到真实机器人上。

Abstract: This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).

</details>


### [937] [FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis](https://arxiv.org/abs/2505.09109)
*Yuxing Chen,Bowen Xiao,He Wang*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人服装折叠的合成服装数据集。通过基于关键点的策略生成演示数据，提高了模型性能，并在真实世界中实现了75%的成功率。


<details>
  <summary>Details</summary>
Motivation: 由于服装的可变形性，为机器人服装操作任务生成大量高质量数据是非常具有挑战性的。

Method: 我们首先基于关键点构建几何服装模板，并应用生成模型来生成逼真的纹理图案。利用这些关键点注释，我们在模拟中生成折叠演示，并通过闭环模仿学习训练折叠策略。为了提高鲁棒性，我们提出了KG-DAgger，它使用基于关键点的策略来生成恢复失败的演示数据。

Result: KG-DAgger显著提高了模型性能，将真实世界的成功率提高了25%。在15K轨迹（约2M图像-动作对）上训练后，模型在真实世界中的成功率为75%。

Conclusion: 实验结果表明，所提出的框架在模拟和现实世界环境中都有效。

Abstract: Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.

</details>


### [938] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习和生物准确骨骼肌模型的定制可穿戴机器人控制器，用于解决手部运动障碍患者的特定肌肉缺陷问题。通过学习演示训练操作模型，并进行微调以执行特定物体的交互任务。结果表明，整合虚拟可穿戴机器人手套可以有效支持肌肉力量减弱的手部操作器，达到原始操作熟练度的平均90.5%。


<details>
  <summary>Details</summary>
Motivation: 由于患者肌肉损失的独特性经常被忽视，因此需要一种能够针对特定肌肉缺陷提供补偿的定制化可穿戴机器人控制器。

Method: 利用强化学习和生物准确的骨骼肌模型进行模拟，提出了一种定制的可穿戴机器人控制器，能够解决特定的肌肉缺陷，并为手-物体操作任务提供补偿。通过学习演示训练了操作模型，并对其进行微调以执行特定物体的交互任务。随后，骨骼肌操作模型中的肌肉力量被削弱以模拟神经运动障碍，并由虚拟可穿戴机器人手套的驱动进行补偿。

Result: 结果表明，整合虚拟可穿戴机器人手套可以提供共享辅助，以支持肌肉力量减弱的手部操作器。学习到的外手套控制器达到了原始操作熟练度的平均90.5%。

Conclusion: 整合虚拟可穿戴机器人手套可以提供共享辅助，以支持肌肉力量减弱的手部操作器。学习到的外手套控制器达到了原始操作熟练度的平均90.5%。

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [939] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

TL;DR: 本文提出了一种基于编码器-解码器的生成轨迹规划模型TransDiffuser，用于端到端的自动驾驶。该模型通过引入一种简单而有效的多模态表示去相关优化机制，解决了生成高质量多样化轨迹的模式崩溃问题，并在NAVSIM基准上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型的能力转移到现代自动驾驶系统中已成为一个有前途的方向。

Method: 我们提出了TransDiffuser，这是一个基于编码器-解码器的生成轨迹规划模型，用于端到端的自动驾驶。编码的场景信息作为去噪解码器的多模态条件输入。为了应对生成高质量多样化轨迹的模式崩溃困境，我们在训练过程中引入了一种简单而有效的多模态表示去相关优化机制。

Result: TransDiffuser在NAVSIM基准上实现了94.85的PDMS，超越了之前最先进的方法，而无需任何基于锚点的先验轨迹。

Conclusion: TransDiffuser在NAVSIM基准上实现了94.85的PDMS，超越了之前最先进的方法，而无需任何基于锚点的先验轨迹。

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [940] [APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression](https://arxiv.org/abs/2505.09356)
*Srinivas Ravuri,Yuan Xu,Martin Ludwig Zehetner,Ketan Motlag,Sahin Albayrak*

Main category: cs.RO

TL;DR: 本文提出了APR-Transformer模型，该模型通过图像或LiDAR数据预测绝对姿态，在多个基准数据集上取得了最先进的性能，并在GNSS-denied环境中验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 精确的初始化在定位算法的性能中起着关键作用，尤其是在机器人、自动驾驶和计算机视觉的背景下。在GNSS-denied环境中，GPS信号主要用于初始化，因此需要更准确的初始姿态以提高定位精度。

Method: 本文引入了APR-Transformer，这是一种受最新方法启发的模型架构，利用图像或LiDAR数据预测绝对姿态（3D位置和3D方向）。

Result: 本文提出的APR-Transformer模型在Radar Oxford Robot-Car和DeepLoc等基准数据集上实现了最先进的性能，并在自定义的APR-BeIntelli数据集上进行了扩展实验。此外，还在真实环境中验证了模型的可靠性。

Conclusion: 本文提出的APR-Transformer模型在GNSS-denied环境中展示了其实际可行性和有效性，并在多个基准数据集上取得了最先进的性能。

Abstract: Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.

</details>


### [941] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文研究了将基础模型集成到机器人中以使其在非结构化环境中运行的挑战，并展示了SPINE框架在实际应用中的成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有的FM启用的机器人主要在封闭世界环境中运行，而本文旨在将FM启用的机器人部署到现场，其中任务通常需要机器人在大规模和非结构化环境中运行。

Method: 本文讨论了SPINE，我们的LLM启用的自主框架在实际机器人环境中的最新部署，并通过初步的模型蒸馏工作，展示了使用设备上语言模型的语言驱动无人机规划器。

Result: 本文展示了第一个在非结构化环境中进行大规模LLM启用机器人规划的演示，并展示了第一个使用设备上语言模型的语言驱动无人机规划器。

Conclusion: 本文提出了未来研究的一些有希望的方向。

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [942] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法，通过预测过去动作标记来改进长上下文策略的学习，从而提高性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 学习有效的长上下文策略仍然具有挑战性，因为随着上下文长度的增加，训练变得昂贵且策略性能通常会因虚假相关性而下降。

Method: 我们提出了Past-Token Prediction (PTP)辅助任务，以及多阶段训练策略，同时在测试时引入了自验证机制。

Result: 我们的方法显著提高了策略头中的时间建模能力，同时减少了内存和计算开销，并在多个任务中提升了性能。

Conclusion: 我们的方法在四个真实世界和六个模拟任务中将长上下文扩散策略的性能提高了3倍，并加速了策略训练超过10倍。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [943] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

TL;DR: Mini-Diffuser is a method for training multi-task vision-language robotic diffusion policies that reduces training time and memory usage by an order of magnitude, achieving high performance while preserving key strengths of diffusion-based policies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce training time and memory usage for multi-task vision-language robotic diffusion policies by exploiting the asymmetry between action diffusion and image diffusion techniques.

Method: Mini-Diffuser introduces Level-2 minibatching, which pairs multiple noised action samples with each vision-language condition, instead of the conventional one-to-one sampling strategy. Architectural adaptations to the diffusion transformer prevent information leakage across samples while maintaining full conditioning access.

Result: In RLBench simulations, Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies, while using only 5% of the training time and 7% of the memory. Real-world experiments validate that it preserves the key strengths of diffusion-based policies.

Conclusion: Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies while using only 5% of the training time and 7% of the memory. It preserves the key strengths of diffusion-based policies, including the ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs.

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [944] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: 本文研究了在特权信息下的策略蒸馏，提出了一种通过策略性交互来提高训练效率和性能的方法。


<details>
  <summary>Details</summary>
Motivation: 我们研究了在特权信息下的策略蒸馏，其中具有部分观察的学生策略必须从具有完整状态访问的教师学习。关键挑战是信息不对称：学生无法直接访问教师的状态空间，导致分布偏移和策略退化。现有的方法要么修改教师以产生可实现但次优的演示，要么依赖学生独立探索缺失信息，这两种方法都是低效的。

Method: 我们引入了两种方法：(i) 一种模仿学习方法，自适应地确定学生何时应查询教师以获得纠正；(ii) 一种强化学习方法，选择在哪里初始化训练以实现高效的探索。

Result: 我们在模拟和现实世界的机器人任务中验证了我们的方法，结果表明在训练效率和最终性能方面有显著改进。

Conclusion: 我们的方法在训练效率和最终性能方面显著优于标准的师生基线。

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [945] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 本文提出了一种名为DataMIL的端到端数据选择框架，用于优化机器人策略的性能。通过直接针对任务成功优化数据选择，而不是依赖人工定义的质量标准，DataMIL能够在不降低性能的情况下实现实时应用。在大量任务中验证了其有效性，展示了其在提升成功率方面的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然这些策略在各种任务上实现了强大的平均性能，但它们在单个、专业任务上的表现往往不佳，并且需要在新获取的任务特定数据上进行进一步调整。将任务特定数据与精心挑选的大规模先验数据集子集通过共同训练结合，可以产生更好的专业化策略，但盲目选择数据可能会损害下游性能。

Method: 我们引入了DataMIL，这是一个基于数据模型范式的政策驱动数据选择框架，它以端到端的方式推理数据选择，使用策略本身来识别最能提高性能的数据点。

Result: 我们在超过60个模拟和现实世界操作任务的套件上验证了我们的方法——最著名的是展示了从Open X-Embodiment数据集中成功选择数据——证明了成功率的一致增长和对多个基线的优越性能。

Conclusion: 我们的结果强调了端到端、性能感知的数据选择在释放机器人领域大型先验数据集潜力中的重要性。

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


### [946] [ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation](https://arxiv.org/abs/2505.09698)
*Enyu Zhao,Vedant Raval,Hejia Zhang,Jiageng Mao,Zeyu Shangguan,Stefanos Nikolaidis,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: 本文提出了一种新的基准测试框架ManipBench，用于评估视觉语言模型在低级机器人操作推理任务中的能力。评估结果显示，视觉语言模型在不同任务中的表现存在显著差异，并且与现实世界操作任务的趋势有很强的相关性。然而，这些模型与人类水平的理解之间仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个明确且通用的基准测试来评估视觉语言模型在机器人低级推理中的能力。因此，提出了ManipBench以填补这一空白。

Method: 提出了一种新的基准测试框架ManipBench，用于评估视觉语言模型在各种维度上的低级机器人操作推理能力，包括对象-对象交互和可变形物体操作。对33个代表性的视觉语言模型进行了广泛测试，并测试了不同模型大小的变体。

Result: 评估结果表明，视觉语言模型在不同任务中的表现存在显著差异，并且与现实世界操作任务的趋势有很强的相关性。同时，这些模型与人类水平的理解之间仍存在显著差距。

Conclusion: 评估结果显示，视觉语言模型在低级机器人操作推理任务中的表现存在显著差异，并且与现实世界操作任务的趋势有很强的相关性。此外，这些模型与人类水平的理解之间仍存在显著差距。

Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.

</details>


### [947] [FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation](https://arxiv.org/abs/2505.10075)
*Jun Guo,Xiaojian Ma,Yikai Wang,Min Yang,Huaping Liu,Qing Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉世界模型FlowDreamer，通过显式运动表示提高机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在训练更好的视觉世界模型，以预测未来的视觉观察结果，从而提高机器人操作的性能。

Method: FlowDreamer通过U-Net预测3D场景流，并利用扩散模型预测未来帧，采用3D场景流作为显式运动表示。

Result: FlowDreamer在4个不同的基准测试中进行了实验，涵盖了视频预测和视觉规划任务，结果表明其性能优于其他基线RGB-D世界模型。

Conclusion: FlowDreamer在各种机器人操作领域中相比其他基线RGB-D世界模型表现出更好的性能，分别在语义相似性上提高了7%，像素质量上提高了11%，成功率提高了6%。

Abstract: This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.

</details>


### [948] [Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering](https://arxiv.org/abs/2505.10073)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架，结合多机器人任务分配和避障，通过空间聚类划分工作区，并使用K-means聚类和2-Opt算法进行任务划分和路径规划。该框架在时间效率和解决方案质量方面表现出色，并完全消除了碰撞点。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，针对同质测量任务，需要同时解决任务分配和碰撞风险问题。

Method: 我们提出了一个基于多机器人任务分配（MRTA）和避障的新框架，通过空间聚类将工作区划分为可区分的操作区域，使用K-means聚类和2-Opt算法来划分任务点并安排机器人路径。

Result: 所提出的框架表现出令人满意的性能，与最佳表现方法相比，时间减少了高达93%（1.24秒对17.62秒），解决方案质量提高了高达7%。此外，我们的方法完全消除了比较方法中持续存在的碰撞点。

Conclusion: 本研究的发现对于需要计算效率和无碰撞操作的实际应用具有重要意义。

Abstract: In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.

</details>


### [949] [EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation](https://arxiv.org/abs/2505.10105)
*Zibin Dong,Fei Ni,Yifu Yuan,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: 本文介绍了 EmbodiedMAE，这是一种用于机器人操作的统一 3D 多模态表示。通过增强 DROID 数据集并构建 DROID-3D，开发了 EmbodiedMAE，该模型在多个任务中表现出色，特别是在需要空间感知的精确桌面操作场景中。


<details>
  <summary>Details</summary>
Motivation: 当前方法在训练数据集和机器人操作任务之间存在显著的领域差距，并且缺乏能够有效结合 3D 信息的模型架构。

Method: 我们增强了 DROID 数据集，加入了高质量的深度图和点云，构建了 DROID-3D，然后开发了 EmbodiedMAE，这是一种多模态掩码自编码器，通过随机掩码和跨模态融合同时学习 RGB、深度和点云模态的表示。

Result: 在 DROID-3D 上训练后，EmbodiedMAE 在 70 个仿真任务和 20 个真实世界机器人操作任务中，无论是训练效率还是最终性能，都优于最先进的视觉基础模型 (VFMs)。

Conclusion: 实验结果表明，EmbodiedMAE 是一种可靠的统一 3D 多模态 VFM，特别适用于需要空间感知的精确桌面操作场景。

Abstract: We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.

</details>


### [950] [Learning Rock Pushability on Rough Planetary Terrain](https://arxiv.org/abs/2505.09833)
*Tuba Girgin,Emre Girgin,Cagri Kilic*

Main category: cs.RO

TL;DR: 本文提出了一种新的移动导航方法，通过重新定位障碍物而不是避开它们，以提高多个代理在长期使用中的路线效率。


<details>
  <summary>Details</summary>
Motivation: 避免障碍物可能会降低长期效率，并导致对主动路径规划系统的持续依赖。因此，我们需要一种更高效的导航方法。

Method: 我们提出了一种替代方法，利用安装在移动机器人顶部的机械臂的操纵能力，通过整合外感受和本体感受反馈来评估障碍物的推动可能性，从而重新定位障碍物而不是避开它们。

Result: 我们的方法通过重新定位障碍物而不是避开它们，提高了多个代理在长期使用中的路线效率。

Conclusion: 我们的导航方法旨在通过减少在需要自主基础设施开发的环境中，舰队花费的总时间，来提高多个代理长期使用的路线效率。

Abstract: In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.

</details>


### [951] [IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning](https://arxiv.org/abs/2505.10442)
*Dechen Gao,Hang Wang,Hanchu Zhou,Nejib Ammar,Shatadal Mishra,Ahmadreza Moradipari,Iman Soltani,Junshan Zhang*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.

</details>


### [952] [Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests](https://arxiv.org/abs/2505.10033)
*Luis F. W. Batista,Stéphanie Aravecchia,Seth Hutchinson,Cédric Pradalier*

Main category: cs.RO

TL;DR: 本文评估了基于DRL的代理在各种扰动下的弹性，包括不对称阻力和偏心载荷。结果显示，DRL代理在面对显著干扰时表现可靠。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习（DRL）在自主水面航行器（ASVs）方面取得了重大进展，但它们在现实条件下的鲁棒性，特别是在外部干扰下，仍缺乏充分探索。

Method: 我们使用领域随机化训练代理，并在真实世界现场测试中评估其性能，评估其处理意外干扰的能力，如不对称阻力和偏心载荷。

Result: 结果表明，DRL代理在面对显著干扰时表现可靠。

Conclusion: 结果表明，尽管存在显著的干扰，DRL代理仍然能够可靠地工作。我们还提供了有效的训练策略、现实世界的挑战和部署DRL基于ASV控制器的实用考虑。

Abstract: Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.

</details>


### [953] [Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation](https://arxiv.org/abs/2505.10522)
*Xinrui Wang,Yan Jin*

Main category: cs.RO

TL;DR: 本文提出了一种新的强化学习框架KCAC，通过跨任务课程学习提高机器人操作的效率和成功率。实验结果显示，该方法在训练时间和任务成功率方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在机器人操作中展现出巨大的潜力，但在样本效率和可解释性方面面临挑战，限制了其在现实场景中的应用。使智能体获得更深入的理解并更高效地适应各种工作场景至关重要，而战略知识利用是这一过程中的关键因素。

Method: 本文提出了一个知识捕获、适应和组合（KCAC）框架，通过跨任务课程学习将知识迁移系统地整合到强化学习（RL）中。同时，重新设计了基准奖励函数，并定义了两个自定义子任务，以实现结构化的跨任务课程学习。

Result: KCAC方法在两块堆叠任务中实现了40%的训练时间减少和10%的任务成功率提升。此外，本文还识别了优化学习效率的关键课程设计参数，如子任务选择、过渡时机和学习率。

Conclusion: 本文提出了一个知识捕获、适应和组合（KCAC）框架，通过跨任务课程学习将知识迁移系统地整合到强化学习（RL）中。实验结果表明，与传统RL方法相比，KCAC方法在训练时间上减少了40%，任务成功率提高了10%。此外，本文还识别了优化学习效率的关键课程设计参数，为基于课程的RL框架提供了概念性指导。

Abstract: Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.

</details>


### [954] [Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning](https://arxiv.org/abs/2505.10547)
*Milan Ganai,Rohan Sinha,Christopher Agia,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. It eliminates the need for hard-coded fallbacks and human safety interventions by bridging open-world, multi-modal reasoning with dynamics-aware planning.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e., out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods are limited.

Method: FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. It uses multi-modal reasoners to identify goals and anticipate failure modes at a low frequency in nominal operations. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time.

Result: FORTRESS eliminates the need for hard-coded fallbacks and human safety interventions by bridging open-world, multi-modal reasoning with dynamics-aware planning. It outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.

Conclusion: FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.

Abstract: Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.

</details>


### [955] [AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics](https://arxiv.org/abs/2505.10398)
*Alexandre Banks,Randy Moore,Sayem Nazmuz Zaman,Alaa Eldin Abdelaal,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: 该研究提出了AutoCam，一种自动辅助摄像头放置方法，用于改善RAMIS中的可视化效果。通过结合启发式几何定位和非线性优化，系统能够高效地跟踪显著特征，并在用户研究中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的辅助摄像头路径规划方法只跟踪二维手术特征，但没有同时考虑摄像头方向、工作空间约束和机器人关节限制。

Method: 该系统在da Vinci Research Kit上实现，使用基于优先级的、受工作空间限制的控制算法，结合启发式几何定位和非线性优化以确保稳健的摄像头跟踪。

Result: 用户研究（N=6）显示，系统保持了99.84%的显著特征可见性，并实现了4.36±2.11度和1.95±5.66毫米的姿态误差。控制器计算效率高，循环时间为6.8±12.8毫秒。一项额外的试点研究（N=6）表明，新手可以从AutoCam的视角进行远程操作，效果与从腹腔镜视角相当，同时仍能受益于AutoCam改进的场景视觉覆盖。

Conclusion: 该研究表明，可以使用da Vinci患者侧机械臂自主控制辅助摄像头来跟踪显著特征，为RAMIS中的新型多摄像头可视化方法奠定了基础。

Abstract: Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [956] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/abs/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

TL;DR: The paper discusses the development of a protocol for real-time content moderation in live video streams, enabling removal of objectionable content and resumption of playback when appropriate.


<details>
  <summary>Details</summary>
Motivation: There is an increased need for robust content moderation in live video streaming due to its popularity on social media platforms. Live streaming imposes restrictions on latency for both analysis and distribution.

Method: Extensions to the Media Over QUIC Transport protocol are presented, which enable real-time content moderation in one-to-many video live streams. The solution removes only the video segments that contain objectionable content and allows playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices.

Result: The system was implemented and evaluated in the context of light strobe removal for photosensitive viewers, resulting in an increased latency of only one group-of-pictures duration for streaming clients.

Conclusion: The proposed extensions to the Media Over QUIC Transport protocol effectively enable real-time content moderation in live video streams with minimal impact on latency.

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


### [957] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/abs/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

TL;DR: 本文提出了一种基于Media Over QUIC Transport协议的实时内容审核方法，能够在直播流中快速删除不当内容，同时保持播放的连续性。


<details>
  <summary>Details</summary>
Motivation: 随着直播流的普及，需要一种高效的实时内容审核方法，以确保内容符合政策要求。

Method: 我们扩展了Media Over QUIC Transport协议，以实现一对多视频直播流中的实时内容审核。内容分析任务可以透明地分配给任意客户端设备。

Result: 我们在光闪烁去除的背景下实现了并评估了我们的系统，发现流媒体客户端的延迟仅增加了一个图像组持续时间。

Conclusion: 我们的解决方案能够在不中断播放的情况下实时删除含有不当内容的视频片段，从而提高直播流的内容审核效率。

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [958] [Inferring entropy production in many-body systems using nonequilibrium MaxEnt](https://arxiv.org/abs/2505.10444)
*Miguel Aguilera,Sosuke Ito,Artemy Kolchinsky*

Main category: cond-mat.stat-mech

TL;DR: A method for inferring entropy production in high-dimensional stochastic systems is proposed, which uses trajectory observables and doesn't require probability distributions or special assumptions. It has intuitive physical interpretation and can be used for hierarchical decomposition of EP.


<details>
  <summary>Details</summary>
Motivation: Standard techniques for estimating entropy production become intractable in high-dimensional stochastic systems due to computational and statistical limitations.

Method: Infer trajectory-level EP and lower bounds on average EP by exploiting a nonequilibrium analogue of the Maximum Entropy principle, along with convex duality, using samples of trajectory observables.

Result: Demonstrated numerical performance on a disordered nonequilibrium spin model with 1000 spins and a large neural spike-train dataset.

Conclusion: The proposed method provides an intuitive physical interpretation as a thermodynamic uncertainty relation and may be used to compute a hierarchical decomposition of EP.

Abstract: We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.

</details>


### [959] [Inferring entropy production in many-body systems using nonequilibrium MaxEnt](https://arxiv.org/abs/2505.10444)
*Miguel Aguilera,Sosuke Ito,Artemy Kolchinsky*

Main category: cond-mat.stat-mech

TL;DR: 本文提出了一种新的方法，用于在高维随机系统中推断熵产生，该方法无需重建高维概率分布或速率矩阵，也无需特殊假设。


<details>
  <summary>Details</summary>
Motivation: 标准的熵产生估计技术在高维系统中变得不可行，因为存在计算和统计上的限制。本文旨在解决这一问题。

Method: 本文利用非平衡系统的最大熵原理和凸对偶性，通过轨迹可观测量（如时空相关函数）来推断轨迹级别的熵产生和平均熵产生的下限。

Result: 本文在具有1000个自旋的无序非平衡自旋模型和大型神经脉冲数据集上验证了该方法的数值性能。

Conclusion: 本文提出了一种在高维随机系统中推断熵产生的方法，该方法具有数值性能，并且可以用于计算熵产生的层次分解。

Abstract: We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [960] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

TL;DR: The paper explores the development of a Large Language Model (LLM) specialized for explaining VHDL code, crucial for organizations involved in high-performance processor design. They created specific test sets and performed extended pretraining (EPT) on a base LLM, increasing expert evaluation ratings from 43% to 69%. An LLM-as-a-judge was also developed, leading to new models with an instruction-tuned EPT model expected to reach a rating of 71%, potentially rising to 85% with advanced base models. The conclusion discusses enhancing hardware design LLMs using developments in Generative AI.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention given to VHDL in the context of Large Language Models (LLMs), despite its industry popularity, and to meet the unique needs of organizations engaged in high-performance processor design.

Method: Developed test sets specific to their needs and conducted extended pretraining (EPT) on a base LLM. Expert evaluations were used to assess the quality of code explanations produced by the EPT model. Also developed an LLM-as-a-judge to evaluate models similarly to expert evaluators.

Result: Expert evaluation ratings increased from 43% for the base model to 69% for the EPT model. The LLM-as-a-judge helped derive new models, including an instruction-tuned version of the EPT model with an expected rating of 71%. Experiments suggest that newer base models could push this rating to 85% or higher.

Conclusion: Discusses further improving the quality of hardware design LLMs using advancements in Generative AI.

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [961] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

TL;DR: This paper presents a method using customized AI tools, particularly GPT-4o and a distilled model, to enhance code comments across different quality axes.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of code comments by rewriting them with customized AI-based tools, ensuring better understanding and maintenance of code.

Method: An empirical study followed by grounded theory qualitative analysis determined the quality axes for improvement. Then, a procedure was proposed that uses a Large Language Model (LLM) like GPT-4o to rewrite existing code comments along these axes. The results were distilled into a smaller model for in-house use.

Result: The evaluation demonstrated that the procedure successfully improves code comments along the defined quality axes.

Conclusion: The approach effectively enhances code comment quality and all data and source code have been made available in an online repository for reproducibility.

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [962] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

TL;DR: The paper introduces WebApp1K, a benchmark for evaluating LLMs in TDD tasks using test cases as prompts and verifications. It highlights instruction following and in-context learning as key capabilities for TDD success.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' ability to interpret and implement functionality directly from test cases, reflecting real-world software development practices.

Method: Created a benchmark named WebApp1K with 1000 diverse challenges across 20 application domains, focusing on evaluating LLMs' abilities in generating compact, functional code under specific constraints.

Result: Found that instruction following and in-context learning are more critical than general coding proficiency or pretraining knowledge for TDD success. Identified performance bottlenecks like instruction loss in long prompts.

Conclusion: WebApp1K underscores the practical value of TDD-specific benchmarks and sets the stage for advancing LLM capabilities in rigorous, application-driven coding scenarios.

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [963] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

TL;DR: Recent advancements use transformer-based models for source code summarization, but they often generate only one summary. This paper introduces Variational Prefix Tuning (VPT), which uses a CVAE framework to generate diverse summaries from pre-trained models, without retraining them. A bi-criteria reranking method is also used to ensure diversity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for source code summarization focus on generating a single high-quality summary, neglecting scenarios where multiple alternatives might be needed.

Method: The paper proposes Variational Prefix Tuning (VPT) that integrates a Conditional Variational Autoencoder (CVAE) as a modular component into pre-trained models. It allows sampling of continuous embeddings used as prefixes to guide the generation of diverse outputs during decoding. The method is parameter-efficient and does not require model retraining. Additionally, a bi-criteria reranking method is employed to select summaries optimizing both diversity and accuracy.

Result: Extensive experimental evaluations using widely recognized datasets and state-of-the-art pre-trained code summarization models demonstrate the effectiveness of VPT and its adaptability across different models.

Conclusion: VPT enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, providing users with more suitable options for source code summaries.

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


### [964] [Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values](https://arxiv.org/abs/2505.09830)
*Martín Rodríguez,Gustavo Rossi,Alejandro Fernandez*

Main category: cs.SE

TL;DR: The study explores the potential of LLMs in generating unit test cases, comparing them with manual tests.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of Large Language Models (LLMs) in automatically generating test cases and compare their effectiveness with manual tests.

Method: An optimized prompt integrating code and requirements was developed to cover critical cases such as equivalence partitions and boundary values. Quantitative metrics and manual qualitative analysis were used to compare LLMs and trained programmers.

Result: The effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. LLMs are flexible and promising but still require human supervision.

Conclusion: This work highlights the importance of manual qualitative analysis as a complement to automation in unit test evaluation.

Abstract: The design and implementation of unit tests is a complex task many
programmers neglect. This research evaluates the potential of Large Language
Models (LLMs) in automatically generating test cases, comparing them with
manual tests. An optimized prompt was developed, that integrates code and
requirements, covering critical cases such as equivalence partitions and
boundary values. The strengths and weaknesses of LLMs versus trained
programmers were compared through quantitative metrics and manual qualitative
analysis. The results show that the effectiveness of LLMs depends on
well-designed prompts, robust implementation, and precise requirements.
Although flexible and promising, LLMs still require human supervision. This
work highlights the importance of manual qualitative analysis as an essential
complement to automation in unit test evaluation.

</details>


### [965] [Are Sparse Autoencoders Useful for Java Function Bug Detection?](https://arxiv.org/abs/2505.10375)
*Rui Melo,Claudia Mamede,Andre Catarino,Rui Abreu,Henrique Lopes Cardoso*

Main category: cs.SE

TL;DR: The paper explores the use of Sparse Autoencoders (SAEs) as a lightweight, interpretable alternative for bug detection in Java functions using representations from pretrained LLMs like GPT-2 Small and Gemma 2B. SAE-derived features achieved an F1 score of up to 89%, outperforming fine-tuned transformer encoder baselines without requiring fine-tuning or task-specific supervision.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities pose significant security risks, and traditional detection methods face challenges such as high false positive rates and scalability issues. AI-based approaches, particularly LLMs, offer new possibilities but encounter interpretability and deployment problems due to their complexity.

Method: The researchers applied Sparse Autoencoders (SAEs) to internal representations from pretrained LLMs (GPT-2 Small and Gemma 2B) to detect bugs in Java functions. The method did not involve fine-tuning the LLMs or applying task-specific supervision.

Result: SAE-derived features achieved an F1 score of up to 89% in bug detection, consistently surpassing fine-tuned transformer encoder baselines.

Conclusion: This study provides the first empirical evidence that SAEs can effectively detect software bugs directly from the internal representations of pretrained LLMs without fine-tuning or additional supervision.

Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.

</details>


### [966] [Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?](https://arxiv.org/abs/2505.10443)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 研究人员通过语义保持的代码变异方法评估了六个大型语言模型（LLM）对Python程序的理解能力，发现如Llama3.2等模型在高达61%的情况下基于有缺陷的推理得出正确预测，并且这些模型在面对代码变异时预测不稳定，表明其语义理解能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型（LLMs）在编程任务中的研究多集中于预测准确性，而忽略了对其推理过程的评估。此外，已知LLMs可能通过错误逻辑得出正确答案，特别是在数学推理任务中。这促使研究者进一步探讨LLMs是否真正能够理解代码逻辑还是仅凭猜测。

Method: 研究者采用了五种语义保持的代码变异技术：变量重命名、镜像比较表达式、交换if-else分支、for循环转换为while循环以及循环展开。通过LiveCodeBench和CruxEval平台，他们评估了六个LLM模型在面对这些变异时的表现，并由人类专家分析正确预测背后的推理依据。

Result: 结果显示，部分LLM（例如Llama3.2）在高达61%的情况下基于有缺陷的推理得出了正确的预测结果。此外，当代码发生变异时，LLM经常改变预测结果，表明它们的语义理解能力较为有限。

Conclusion: 本研究表明，尽管一些LLM可以生成正确的预测结果，但其推理过程可能存在重大缺陷，且模型在代码变异面前表现出不稳定性，说明其语义理解能力尚需改进。

Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.

</details>


### [967] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

TL;DR: 本文介绍了为VHDL代码解释开发LLM的过程，展示了通过扩展预训练和专家评估提高模型性能的方法，并提出了未来利用生成式AI改进硬件设计LLM的建议。


<details>
  <summary>Details</summary>
Motivation: 在高性能处理器设计领域，LLM在VHDL代码解释方面的应用尚未得到充分研究，而VHDL在工业界仍然很受欢迎。

Method: 我们开发了针对特定需求的测试集，并用于评估模型，进行了扩展预训练（EPT），还开发了一个LLM-as-a-judge来评估模型。

Result: 经过EPT模型的专家评估得分从43%提高到了69%，并且通过指令调优的EPT模型预计专家评估得分可达71%。使用更新的基模型可以将得分提升到85%以上。

Conclusion: 我们讨论了如何进一步利用生成式AI领域的最新发展来提高硬件设计LLM的质量。

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [968] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

TL;DR: 本文提出了一种使用人工智能工具来改善代码注释质量的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提高代码注释的质量是软件开发中的一个重要问题，现有的代码注释可能不够准确或不完整，因此需要一种有效的方法来改进它们。

Method: 本文通过实证研究和基于扎根理论的定性分析确定了需要改进的质量维度，然后提出了一种使用大语言模型（LLM）来重写现有代码注释的程序。

Result: 本文通过实验验证了所提出的方法能够有效地提高代码注释在质量维度上的表现，并且还展示了如何将结果压缩到一个较小的模型中，以便用户可以在内部运行。

Conclusion: 本文展示了如何通过定制的人工智能工具重写代码注释来提高代码注释在不同质量维度上的表现，并通过实验验证了方法的有效性。

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [969] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

TL;DR: 本研究介绍了WebApp1K基准测试，用于评估大型语言模型在测试驱动开发任务中的表现，强调了指令遵循和上下文学习的重要性，并揭示了性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于自然语言提示，而本研究旨在强调LLMs直接从测试用例中解释和实现功能的能力，这反映了现实世界软件开发实践。

Method: 引入了WebApp1K，这是一个新的基准测试，用于评估大型语言模型（LLMs）在测试驱动开发（TDD）任务中的表现，其中测试用例既是提示也是代码生成的验证。

Result: 研究发现，指令遵循和上下文学习是TDD成功的关键能力，超过了通用编码熟练度或预训练知识的重要性。通过全面评估19个前沿模型，揭示了性能瓶颈，如长提示中的指令丢失，并提供了详细的错误分析。

Conclusion: 本研究强调了针对TDD的基准测试在实际应用中的价值，并为提升LLM在严格、应用驱动的编码场景中的能力奠定了基础。

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [970] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

TL;DR: 本文提出了一种新的方法，即变分前缀调优（VPT），以增强预训练模型生成多样且准确的代码摘要集的能力，从而允许用户选择最适合给定源代码的摘要。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常专注于为给定的源代码生成一个高质量的摘要，而忽略了生成的摘要可能不充分的情况，需要替代选项。

Method: 本文将条件变分自编码器（CVAE）框架作为模块化组件集成到预训练模型中，以建模观察到的目标摘要分布，并在解码过程中采样连续嵌入作为前缀来引导生成多样化的输出。此外，还采用双标准重排序方法选择生成摘要的子集，优化了提供的选项的多样性和准确性。

Result: 本文通过广泛的实验评估，使用常用数据集和当前最先进的预训练代码摘要模型，证明了所提出方法的有效性及其在不同模型上的适应性。

Conclusion: 本文提出了一种新的方法，即变分前缀调优（VPT），以增强预训练模型生成多样且准确的代码摘要集的能力，从而允许用户选择最适合给定源代码的摘要。

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


### [971] [Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values](https://arxiv.org/abs/2505.09830)
*Martín Rodríguez,Gustavo Rossi,Alejandro Fernandez*

Main category: cs.SE

TL;DR: This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision.


<details>
  <summary>Details</summary>
Motivation: The design and implementation of unit tests is a complex task many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests.

Method: An optimized prompt was developed that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis.

Result: The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements.

Conclusion: LLMs still require human supervision and manual qualitative analysis is essential in unit test evaluation.

Abstract: The design and implementation of unit tests is a complex task many
programmers neglect. This research evaluates the potential of Large Language
Models (LLMs) in automatically generating test cases, comparing them with
manual tests. An optimized prompt was developed, that integrates code and
requirements, covering critical cases such as equivalence partitions and
boundary values. The strengths and weaknesses of LLMs versus trained
programmers were compared through quantitative metrics and manual qualitative
analysis. The results show that the effectiveness of LLMs depends on
well-designed prompts, robust implementation, and precise requirements.
Although flexible and promising, LLMs still require human supervision. This
work highlights the importance of manual qualitative analysis as an essential
complement to automation in unit test evaluation.

</details>


### [972] [Are Sparse Autoencoders Useful for Java Function Bug Detection?](https://arxiv.org/abs/2505.10375)
*Rui Melo,Claudia Mamede,Andre Catarino,Rui Abreu,Henrique Lopes Cardoso*

Main category: cs.SE

TL;DR: 本文探讨了稀疏自编码器（SAEs）作为Java函数中错误检测的轻量级、可解释的替代方案的潜力。结果表明，SAEs可以从预训练大型语言模型的内部表示中直接检测软件错误，而无需任何微调或任务特定的监督。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞如缓冲区溢出和SQL注入是安全漏洞的主要来源。传统的漏洞检测方法虽然仍然重要，但受到高误报率、可扩展性问题和对人工努力的依赖的限制。这些限制促使人们对基于人工智能的方法进行自动化漏洞检测和安全代码生成产生了兴趣。尽管大型语言模型（LLMs）为分类任务开辟了新途径，但它们的复杂性和不透明性对可解释性和部署提出了挑战。

Method: 我们探索了稀疏自编码器（SAEs）是否可以作为Java函数中错误检测的轻量级、可解释的替代方案。我们评估了当应用于GPT-2 Small和Gemma 2B的表示时，SAEs的有效性，检查它们在不微调底层LLMs的情况下突出显示错误行为的能力。

Result: 我们发现，SAE衍生的特征可以实现高达89%的F1分数的错误检测，始终优于微调的变压器编码器基线。

Conclusion: 我们的工作提供了第一个实证证据，表明稀疏自编码器可以直接从预训练大型语言模型的内部表示中检测软件错误，而无需任何微调或任务特定的监督。

Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.

</details>


### [973] [Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?](https://arxiv.org/abs/2505.10443)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 本文评估了最先进的LLMs是否能够推理Python程序或只是猜测。通过应用五种保持语义的代码变异，我们发现一些LLMs在高达61%的情况下基于错误的推理产生正确的预测，且它们在面对代码变异时的预测变化表明其语义理解能力有限。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）的推理和鲁棒性对于其在编程任务中的可靠使用至关重要。然而，现有研究主要关注预测的准确性，而未评估其背后的推理过程。此外，在数学推理任务中观察到LLMs可以通过有缺陷的逻辑得出正确答案，这引发了对代码理解中类似问题的担忧。

Method: 我们应用了五种保持语义的代码变异，包括重命名变量、镜像比较表达式、交换if-else分支、将for循环转换为while循环以及循环展开。我们评估了六种LLMs，并使用LiveCodeBench进行了人工专家分析，以评估正确预测是否基于合理的推理。

Result: 我们的研究发现，一些LLMs，如Llama3.2，在多达61%的情况下基于错误的推理产生正确的预测。此外，LLMs在面对代码变异时经常改变预测，这表明它们在语义理解方面存在有限的鲁棒性。

Conclusion: 一些大型语言模型（LLMs）在多达61%的情况下基于错误的推理产生正确的预测，这表明它们在语义理解方面存在有限的鲁棒性。

Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [974] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了一种基于主动学习策略的新型逆向材料设计生成框架InvDesFlow-AL，用于功能材料的开发。该框架在晶体结构预测方面表现优异，同时在低形成能和低Ehull材料的设计中也得到了成功验证。通过寻找BCS超导体的例子，成功确定了Li₂AuH₆作为一种具有超高转变温度（140 K）的传统BCS超导体。


<details>
  <summary>Details</summary>
Motivation: 开发具有特定性能的功能材料的逆向设计方法对于可再生能源、催化、储能和碳捕获等领域的发展至关重要。现有的生成和预测晶体结构的方法往往成功率较低，因此需要一种更高效的材料生成框架来加速这一过程。

Method: 提出了名为InvDesFlow-AL的逆向材料设计生成框架，该框架基于主动学习策略。通过迭代优化材料生成过程，逐渐引导其达到所需的性能特征。此模型能够系统地生成具有逐步降低形成能的材料，并不断扩展对多样化化学空间的探索。

Result: 在晶体结构预测方面，InvDesFlow-AL模型实现了0.0423 Å的RMSE，相比现有生成模型性能提高了32.96%。此外，在低形成能和低Ehull材料的设计中也取得了成功验证，并成功识别出Li₂AuH₆为一种转变温度为140 K的常规BCS超导体。

Conclusion: InvDesFlow-AL框架显著提升了材料发现和逆向设计的效率，证明了主动学习驱动的生成模型在材料科学中的应用潜力。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [975] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes a synthesizability-driven CSP framework integrating symmetry-guided structure derivation and machine learning, successfully predicting synthesizable structures and demonstrating potential for experimental realization.


<details>
  <summary>Details</summary>
Motivation: To address the gap between theoretical predictions and experimental synthesis in thermodynamic energy-based crystal structure prediction (CSP), especially for metastable materials synthesized through kinetically controlled pathways.

Method: A synthesizability-driven CSP framework that combines symmetry-guided structure derivation with a Wyckoff encode-based machine-learning model to identify promising subspaces. A structure-based synthesizability evaluation model fine-tuned with recently synthesized structures is then used alongside ab initio calculations to predict synthesizable candidates.

Result: Reproduced 13 experimentally known XSe structures, filtered 92,310 structures from 554,054 candidates predicted by GNoME, and identified eight thermodynamically favorable Hf-X-O structures, including three highly synthesizable HfV$_2$O$_7$ candidates.

Conclusion: Establishes a data-driven paradigm for machine-learning-assisted inorganic materials synthesis, bridging the gap between computational predictions and experimental realization, and unlocking opportunities for discovering novel functional materials.

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


### [976] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种基于主动学习的逆向材料设计生成框架InvDesFlow-AL，显著提高了材料设计效率，并成功发现了具有超高转变温度的BCS超导体。


<details>
  <summary>Details</summary>
Motivation: 开发逆向设计方法对于功能材料的特定性能至关重要，但现有方法在生成和预测晶体结构时存在成功率低的问题。

Method: InvDesFlow-AL框架结合了主动学习策略，通过迭代优化材料生成过程，逐步引导其达到期望的性能特征。

Result: InvDesFlow-AL在晶体结构预测中实现了0.0423 Å的RMSE，比现有生成模型提高了32.96%。此外，该方法成功验证了低形成能和低Ehull材料的设计，并发现了Li$_2$AuH$_6$作为具有140 K超高转变温度的BCS超导体。

Conclusion: 该研究提出了一种基于主动学习策略的新型逆向材料设计生成框架InvDesFlow-AL，能够显著加速材料发现和逆向设计过程。该方法在晶体结构预测中表现出色，并成功应用于低形成能和低Ehull材料的设计，还发现了具有超高转变温度的BCS超导体Li$_2$AuH$_6$。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [977] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种基于可合成性的晶体结构预测框架，结合了对称性引导的结构推导与基于Wyckoff编码的机器学习模型，以高效定位可能产生高度可合成结构的子空间。该框架成功再现了13种实验已知的XSe结构，并从大量候选结构中筛选出92,310个具有高可合成性的结构。此外，还发现了8种热力学有利的Hf-X-O结构，其中三种表现出高可合成性，为实验实现提供了可行的候选材料。


<details>
  <summary>Details</summary>
Motivation: 尽管基于热力学能量的晶体结构预测（CSP）已彻底改变了材料发现，但能量驱动的CSP方法往往难以识别通过动力学控制路径合成的实验上可实现的亚稳材料，这在理论预测和实验合成之间造成了关键差距。

Method: 本文提出了一种基于可合成性的晶体结构预测框架，该框架结合了对称性引导的结构推导与基于Wyckoff编码的机器学习模型，以高效定位可能产生高度可合成结构的子空间。此外，还使用最近合成的结构对基于结构的可合成性评估模型进行了微调，以提高预测准确性，并结合第一性原理计算系统地识别可合成候选材料。

Result: 该框架成功再现了13种实验已知的XSe（X = Sc, Ti, Mn, Fe, Ni, Cu, Zn）结构，证明了其在预测可合成结构方面的有效性。从GNoME预测的554,054个候选结构中筛选出92,310个结构，显示出巨大的可合成性潜力。此外，还发现了8种热力学有利的Hf-X-O（X = Ti, V, 和 Mn）结构，其中三种HfV$_2$O$_7$候选结构表现出高可合成性，为实验实现提供了可行的候选材料，并可能与实验观察到的温度诱导相变相关。

Conclusion: 本文建立了一个数据驱动的机器学习辅助无机材料合成范式，展示了其在计算预测和实验实现之间架起桥梁的潜力，并为靶向发现新型功能材料提供了新的机会。

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [978] [Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease](https://arxiv.org/abs/2505.09624)
*Ekaterina Kuzmina,Dmitrii Kriukov,Mikhail Lebedev,Dmitry V. Dylov*

Main category: q-bio.NC

TL;DR: An adaptive deep brain stimulation (aDBS) benchmark is introduced, featuring neurophysiological realism and a structured environment for training and evaluating deep reinforcement learning algorithms to optimize aDBS control strategies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in data collection for optimizing aDBS control offline and to provide a realistic benchmark for comparing synthetic models of Parkinson's Disease (PD) and control algorithms.

Method: The methodology includes conventional basal ganglia circuit dynamics and pathological oscillations, along with 15 previously overlooked physiological attributes such as signal instabilities, noise, neural drift, electrode conductance changes, and individual variability. These are modeled using beta-band activity in the brain and feedback mechanisms. A structured environment is also built for training and evaluating deep reinforcement learning algorithms.

Result: This approach creates a neurophysiologically realistic benchmark that can be used to train and evaluate deep RL algorithms, thereby opening new possibilities for optimizing aDBS control strategies.

Conclusion: This work invites the machine learning community to contribute to the development of intelligent neurostimulation interfaces by providing a comprehensive and realistic benchmark for aDBS.

Abstract: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment
for Parkinson disease (PD). In aDBS, a surgically placed electrode sends
dynamically altered stimuli to the brain based on neurophysiological feedback:
an invasive gadget that limits the amount of data one could collect for
optimizing the control offline. As a consequence, a plethora of synthetic
models of PD and those of the control algorithms have been proposed. Herein, we
introduce the first neurophysiologically realistic benchmark for comparing said
models. Specifically, our methodology covers not only conventional basal
ganglia circuit dynamics and pathological oscillations, but also captures 15
previously dismissed physiological attributes, such as signal instabilities and
noise, neural drift, electrode conductance changes and individual variability -
all modeled as spatially distributed and temporally registered features via
beta-band activity in the brain and a feedback. Furthermore, we purposely built
our framework as a structured environment for training and evaluating deep
reinforcement learning (RL) algorithms, opening new possibilities for
optimizing aDBS control strategies and inviting the machine learning community
to contribute to the emerging field of intelligent neurostimulation interfaces.

</details>


### [979] [Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making](https://arxiv.org/abs/2505.09646)
*Carmel Mary Esther A*

Main category: q-bio.NC

TL;DR: This paper proposes a novel theoretical model for real-time awareness by reducing perceptual delays, through investigating cosmic signal delay, neurological reaction times and the ancient cognitive state of stillness.


<details>
  <summary>Details</summary>
Motivation: To explain how human mind and artificial intelligence can approach real-time awareness by reducing perceptual delays.

Method: Investigating cosmic signal delay, neurological reaction times, and the ancient cognitive state of stillness to shift from reactive perception to a conscious interface with the near future.

Result: Introduction of a physical and cognitive model for perceiving the present as an interference zone where early-arriving cosmic signals and reactive human delays intersect, along with experimental approaches using human neural observation and neuro-receptive extensions.

Conclusion: Proposes a mathematical framework to guide AI systems toward temporally efficient, ethically sound, and internally conscious decision-making processes.

Abstract: This paper proposes a novel theoretical model to explain how the human mind
and artificial intelligence can approach real-time awareness by reducing
perceptual delays. By investigating cosmic signal delay, neurological reaction
times, and the ancient cognitive state of stillness, we explore how one may
shift from reactive perception to a conscious interface with the near future.
This paper introduces both a physical and cognitive model for perceiving the
present not as a linear timestamp, but as an interference zone where
early-arriving cosmic signals and reactive human delays intersect. We propose
experimental approaches to test these ideas using human neural observation and
neuro-receptive extensions. Finally, we propose a mathematical framework to
guide the evolution of AI systems toward temporally efficient, ethically sound,
and internally conscious decision-making processes

</details>


### [980] [A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System](https://arxiv.org/abs/2505.09643)
*Zhixuan Wang*

Main category: q-bio.NC

TL;DR: Epilepsy patients often use alternative medicine due to the limitations of conventional drugs. This study developed an AI-driven system that analyzes natural compounds and RCTs, identifying 17 high-efficacy herbs for epilepsy treatment with significant seizure reduction confirmed in a validation trial.


<details>
  <summary>Details</summary>
Motivation: To optimize herbal epilepsy treatment by leveraging AI-driven analysis of global natural products and statistically validated RCTs, addressing the limitations of conventional antiepileptic drugs.

Method: The intelligent prescription system integrates machine learning algorithms for herb-efficacy characterization, Bayesian optimization for personalized dosing, and meta-analysis of RCTs for evidence-based recommendations. It analyzed 1,872 natural compounds and clinical outcomes from 48 RCTs covering 48 epilepsy conditions.

Result: Identified 17 high-efficacy herbs showing significant seizure reduction (p<0.01, Cohen's d=0.89) with statistical significance confirmed by multiple testing (p<0.001). A validation trial demonstrated 28.5% greater seizure frequency reduction compared to conventional protocols.

Conclusion: AI-optimized herbal prescriptions offer a promising alternative for epilepsy treatment, significantly reducing seizure frequency.

Abstract: Epilepsy is a prevalent neurological disease with millions of patients
worldwide. Many patients have turned to alternative medicine due to the limited
efficacy and side effects of conventional antiepileptic drugs. In this study,
we developed a computational approach to optimize herbal epilepsy treatment
through AI-driven analysis of global natural products and statistically
validated randomized controlled trials (RCTs). Our intelligent prescription
system combines machine learning (ML) algorithms for herb-efficacy
characterization, Bayesian optimization for personalized dosing, and
meta-analysis of RCTs for evidence-based recommendations. The system analyzed
1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and
ethnopharmacological databases, integrating their bioactive properties with
clinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using
LASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs
(e.g., Gastrodia elata [using \'e for accented characters], Withania
somnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)
with statistical significance confirmed by multiple testing (p$<$0.001). A
randomized double-blind validation trial (n=120) demonstrated 28.5\% greater
seizure frequency reduction with AI-optimized herbal prescriptions compared to
conventional protocols (95\% CI: 18.7-37.3\%, p=0.003).

</details>


### [981] [Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease](https://arxiv.org/abs/2505.09624)
*Ekaterina Kuzmina,Dmitrii Kriukov,Mikhail Lebedev,Dmitry V. Dylov*

Main category: q-bio.NC

TL;DR: 本文介绍了第一个神经生理学上现实的基准，用于比较自适应深部脑刺激（aDBS）模型，并为训练和评估深度强化学习算法提供了结构化的环境，从而为优化aDBS控制策略开辟了新可能性。


<details>
  <summary>Details</summary>
Motivation: 由于aDBS需要侵入性设备，限制了可用于离线优化控制的数据量，因此提出了大量合成模型和控制算法。本文旨在引入一个神经生理学上现实的基准，以比较这些模型并促进智能神经刺激接口的发展。

Method: 本文提出了一种方法，涵盖了传统的基底神经节回路动力学和病理振荡，并捕捉了15个以前被忽视的生理特征，如信号不稳定性、噪声、神经漂移、电极电导变化和个体差异，并通过β波段活动和反馈将其建模为时空分布的特征。

Result: 本文构建了一个结构化的环境，用于训练和评估深度强化学习算法，为优化aDBS控制策略提供了新可能，并邀请机器学习社区参与这一新兴领域。

Conclusion: 本文介绍了第一个神经生理学上现实的基准，用于比较自适应深部脑刺激（aDBS）模型，并为训练和评估深度强化学习算法提供了结构化的环境，从而为优化aDBS控制策略开辟了新可能性。

Abstract: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment
for Parkinson disease (PD). In aDBS, a surgically placed electrode sends
dynamically altered stimuli to the brain based on neurophysiological feedback:
an invasive gadget that limits the amount of data one could collect for
optimizing the control offline. As a consequence, a plethora of synthetic
models of PD and those of the control algorithms have been proposed. Herein, we
introduce the first neurophysiologically realistic benchmark for comparing said
models. Specifically, our methodology covers not only conventional basal
ganglia circuit dynamics and pathological oscillations, but also captures 15
previously dismissed physiological attributes, such as signal instabilities and
noise, neural drift, electrode conductance changes and individual variability -
all modeled as spatially distributed and temporally registered features via
beta-band activity in the brain and a feedback. Furthermore, we purposely built
our framework as a structured environment for training and evaluating deep
reinforcement learning (RL) algorithms, opening new possibilities for
optimizing aDBS control strategies and inviting the machine learning community
to contribute to the emerging field of intelligent neurostimulation interfaces.

</details>


### [982] [Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making](https://arxiv.org/abs/2505.09646)
*Carmel Mary Esther A*

Main category: q-bio.NC

TL;DR: 本文提出了一种新的理论模型，用于解释人类大脑和人工智能如何通过减少感知延迟来实现实时意识，并提出了一个数学框架，以指导人工智能系统的发展。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人类大脑和人工智能如何通过减少感知延迟来实现实时意识，并提出一种数学框架，以指导人工智能系统的发展。

Method: 本文通过研究宇宙信号延迟、神经反应时间和古代认知状态的静止状态，探索如何从反应性感知转变为对近未来的有意识界面。此外，本文还提出了使用人类神经观察和神经接受扩展来测试这些想法的实验方法。

Result: 本文提出了一个新的理论模型，用于解释人类大脑和人工智能如何通过减少感知延迟来实现实时意识，并提出了一个数学框架，以指导人工智能系统的发展。

Conclusion: 本文提出了一种新的理论模型，以解释人类大脑和人工智能如何通过减少感知延迟来实现实时意识。同时，本文提出了一个数学框架，指导人工智能系统向时间高效、道德上正当且内部有意识的决策过程发展。

Abstract: This paper proposes a novel theoretical model to explain how the human mind
and artificial intelligence can approach real-time awareness by reducing
perceptual delays. By investigating cosmic signal delay, neurological reaction
times, and the ancient cognitive state of stillness, we explore how one may
shift from reactive perception to a conscious interface with the near future.
This paper introduces both a physical and cognitive model for perceiving the
present not as a linear timestamp, but as an interference zone where
early-arriving cosmic signals and reactive human delays intersect. We propose
experimental approaches to test these ideas using human neural observation and
neuro-receptive extensions. Finally, we propose a mathematical framework to
guide the evolution of AI systems toward temporally efficient, ethically sound,
and internally conscious decision-making processes

</details>


### [983] [A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System](https://arxiv.org/abs/2505.09643)
*Zhixuan Wang*

Main category: q-bio.NC

TL;DR: A study developed an AI system to optimize herbal epilepsy treatment by analyzing natural products and clinical trials, resulting in significant seizure reduction.


<details>
  <summary>Details</summary>
Motivation: Many patients with epilepsy turn to alternative medicine due to the limited efficacy and side effects of conventional antiepileptic drugs.

Method: Developed a computational approach using AI-driven analysis of global natural products and statistically validated RCTs. Combined ML algorithms, Bayesian optimization, and meta-analysis of RCTs.

Result: Identified 17 high-efficacy herbs with significant seizure reduction. A randomized double-blind validation trial showed 28.5% greater seizure frequency reduction with AI-optimized prescriptions.

Conclusion: AI-optimized herbal prescriptions showed significant seizure reduction compared to conventional protocols.

Abstract: Epilepsy is a prevalent neurological disease with millions of patients
worldwide. Many patients have turned to alternative medicine due to the limited
efficacy and side effects of conventional antiepileptic drugs. In this study,
we developed a computational approach to optimize herbal epilepsy treatment
through AI-driven analysis of global natural products and statistically
validated randomized controlled trials (RCTs). Our intelligent prescription
system combines machine learning (ML) algorithms for herb-efficacy
characterization, Bayesian optimization for personalized dosing, and
meta-analysis of RCTs for evidence-based recommendations. The system analyzed
1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and
ethnopharmacological databases, integrating their bioactive properties with
clinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using
LASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs
(e.g., Gastrodia elata [using \'e for accented characters], Withania
somnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)
with statistical significance confirmed by multiple testing (p$<$0.001). A
randomized double-blind validation trial (n=120) demonstrated 28.5\% greater
seizure frequency reduction with AI-optimized herbal prescriptions compared to
conventional protocols (95\% CI: 18.7-37.3\%, p=0.003).

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [984] [A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds](https://arxiv.org/abs/2505.10201)
*Victor Lagerkvist,Mohamed Maizia,Johannes Schmidt*

Main category: cs.CC

TL;DR: The paper explores the complexity of non-monotonic reasoning, specifically abductive reasoning, by analyzing intractable abduction problems with respect to the number of variables in the knowledge base. It presents positive results for certain problem fragments and shows improvements over exhaustive search for a $\Sigma^P_2$-complete problem.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between monotonic and non-monotonic reasoning, especially focusing on abductive reasoning which lacks detailed complexity analysis beyond classic theory.

Method: Analyze the complexity of intractable abduction problems using the parameter n (number of variables in the knowledge base) and provide both positive results and lower bounds for various problem fragments.

Result: Achieved several positive results for different complexity classes including $\Sigma^P_2$, NP, and coNP-complete fragments. Demonstrated an improvement over exhaustive search for a $\Sigma^P_2$-complete problem. Provided lower bounds under the strong exponential-time hypothesis.

Conclusion: This work marks a significant step in understanding the complexity of non-monotonic reasoning, providing both advancements and limitations in solving abductive reasoning problems.

Abstract: The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.

</details>


### [985] [A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds](https://arxiv.org/abs/2505.10201)
*Victor Lagerkvist,Mohamed Maizia,Johannes Schmidt*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [986] [LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting](https://arxiv.org/abs/2505.10191)
*Qingyu Zheng,Qi Shao,Guijun Han,Wei Li,Hong Li,Xuan Wang*

Main category: physics.ao-ph

TL;DR: LanTu, a dynamics-enhanced deep learning regional eddy-resolving ocean forecasting system incorporating cross-scale interactions and multiscale physical constraints, outperforms existing operational numerical and AI-based systems in predicting temperature, salinity, sea level anomaly, and currents with a lead time of over 10 days.


<details>
  <summary>Details</summary>
Motivation: Eddy-resolving ocean forecasting is crucial for fisheries and navigational safety but presents scientific challenges and high computational costs. AI-based systems offer a balance between forecast performance and efficiency, yet still face challenges in mesoscale eddy forecasting.

Method: Developed LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. Incorporated cross-scale interactions and constructed multiscale physical constraints guided by eddy dynamics knowledge to improve mesoscale evolution forecasting skill.

Result: LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly, and current prediction with a lead time of more than 10 days.

Conclusion: Dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting.

Abstract: Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.

</details>


### [987] [LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting](https://arxiv.org/abs/2505.10191)
*Qingyu Zheng,Qi Shao,Guijun Han,Wei Li,Hong Li,Xuan Wang*

Main category: physics.ao-ph

TL;DR: 本文介绍了LanTu，一个基于增强动力学的深度学习的区域涡旋解析海洋预报系统。通过引入跨尺度相互作用和构建多尺度物理约束，LanTu在多个海洋变量的预测上表现出色，优于现有的数值和人工智能预报系统。


<details>
  <summary>Details</summary>
Motivation: Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling).

Method: 我们开发了LanTu，这是一个基于增强动力学的深度学习的区域涡旋解析海洋预报系统。我们将跨尺度相互作用纳入LanTu，并构建多尺度物理约束，以优化LanTu，从而提高其对中尺度演化的预测技能。

Result: 结果表明，LanTu在温度、盐度、海平面异常和电流预测方面优于现有的先进操作数值海洋预报系统（NOFS）和基于人工智能的海洋预报系统（AI-OFS），预测时效超过10天。

Conclusion: 我们的研究强调，增强动力学的深度学习（LanTu）可以成为涡旋解析海洋预报的强大范式。

Abstract: Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [988] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

TL;DR: 为了适应视觉语言模型（VLM）在医疗保健等高风险领域的开发、适配和应用，本文提出需要建立清晰和标准化的报告协议。文章建议重新构建传统的机器学习报告标准和评估指南以适应多阶段的VLM研究，并提出了一个分类框架及相应的报告标准，最后还提供了一个检查列表以确保VLM相关研究发表的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的研究具有多样性和复杂性，从新模型的开发到特定领域的微调，再到直接应用于诊断和预测任务，这些差异使得制定统一的报告标准变得困难。然而，在如医疗保健这样的高风险领域，缺乏明确和标准化的报告协议可能会导致严重的后果。因此，有必要为VLM研究制定专门的报告标准和评估指南。

Method: 作者首先讨论了传统机器学习报告标准的不足，并提出需要根据VLM研究的特点进行调整。接着，文章提出了一种针对VLM研究的分类框架，该框架涵盖了从模型开发到实际应用的不同阶段。然后，根据这一分类框架，详细描述了各个阶段的报告标准，包括性能评估、数据报告协议以及论文撰写建议。最后，提供了一个综合性的检查列表，帮助研究人员确保其研究符合所提出的报告标准。

Result: 通过提出分类框架和具体的报告标准，本文为VLM研究提供了一个全面的指导方案，有助于提高研究的透明度和可重复性。此外，所提供的检查列表可以作为工具，促进社区对这些标准的采纳和使用。

Conclusion: 视觉语言模型的研究需要专门设计的报告标准来适应其多样性和复杂性。本文提出的分类框架和报告标准为未来VLM研究提供了指导方向，同时，通过提供的检查列表，促进了这些标准在研究社区中的应用。这将有助于提升VLM研究的质量和一致性，尤其是在医疗保健等关键领域。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [989] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [990] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

TL;DR: The paper introduces FareShare, a computational tool designed to automate lost wage estimation for deactivated rideshare drivers. Through a partnership with Washington's largest rideshare labor union, the deployment of FareShare showed significant reductions in calculation time and errors, while also highlighting socio-technical challenges.


<details>
  <summary>Details</summary>
Motivation: Deactivation from gig work platforms can severely impact workers' financial stability due to arbitrary AI and algorithmic decisions. Current policies mandate appeals processes and compensation recovery, but lack effective tools to support these workflows.

Method: The authors partnered with the State of Washington's largest rideshare labor union over 6 months to design FareShare. The tool was then deployed in the field for 3 months, registering 178 account signups.

Result: FareShare reduced lost wage calculation time by over 95%, eliminated manual data entry errors, and enabled more efficient report generation for legal teams.

Conclusion: While FareShare demonstrated significant improvements in efficiency and accuracy, the deployment also revealed important socio-technical challenges related to trust, consent, and tool adoption in high-stakes labor contexts.

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [991] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

TL;DR: This paper investigates how socioeconomic factors influence industry clustering and workforce distribution in the transportation cybersecurity ecosystem across the US, presenting a BiTransGCN framework to model visitor flow patterns.


<details>
  <summary>Details</summary>
Motivation: To understand spatiotemporal dynamics of visitor flows and examine how socioeconomic factors shape industry clustering and workforce distribution within evolving sectors related to transportation cybersecurity.

Method: Develops a BiTransGCN framework which integrates an attention-based Transformer architecture with a Graph Convolutional Network backbone for modeling and predicting visitor flow patterns.

Result: Improves the ability to track, interpret, and anticipate changes in industry clustering and mobility trends, providing support for strategic planning in transportation network.

Conclusion: The study provides a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem.

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [992] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

TL;DR: An abstract about a new framework-agnostic method called FedIDA which aims to improve fairness in federated learning for healthcare AI models.


<details>
  <summary>Details</summary>
Motivation: There is a need to ensure fairness in high-stakes domains like healthcare when using AI. Current federated learning methods, while enabling privacy-preserving collaboration across institutions, are still vulnerable to algorithmic bias and subgroup imbalance especially with multiple intersecting sensitive attributes.

Method: Propose FedIDA, a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling to support multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm.

Result: Theoretical analysis shows fairness improvement bounds using Lipschitz continuity and concentration inequalities. Empirical results on benchmark and real-world clinical datasets confirm that FedIDA improves fairness while maintaining competitive predictive performance.

Conclusion: FedIDA demonstrates effectiveness for equitable and privacy-preserving modeling in healthcare.

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [993] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

TL;DR: Since 2022, RLHF technique has been used to fine-tune LLMs, making their outputs more human-like. This convergence of human and machine-written text raises ethical, sociotechnical, and pedagogical concerns. This paper conducts a rhetorical analysis of the procedures reshaped by RLHF-enhanced chatbots, shifting focus from content persuasiveness to underlying persuasive mechanisms, and opens new directions for AI ethics inquiry.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to highlight the severe ethical, sociotechnical, and pedagogical implications brought about by the increasing convergence of human and machine-written text due to the integration of RLHF in LLMs.

Method: The method involves conducting a rhetorical analysis of the central procedures and processes reshaped by RLHF-enhanced generative AI chatbots using Ian Bogost's concept of procedural rhetoric, shifting the site of investigation from content analysis to the underlying mechanisms of persuasion.

Result: The result is a theoretical investigation that uncovers how AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships.

Conclusion: This paper concludes that there is a need for further inquiry in AI ethics considering the implications of procedures rerouted through AI-driven technologies, which will be of interest to educators, researchers, scholars, and users of generative AI chatbots.

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [994] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

TL;DR: 尽管单个查询效率高，但大规模应用导致不成比例的资源消耗。本研究提供了一种标准化、基于实证的方法来衡量LLM部署的可持续性，为AI发展的环境责任和可持续性标准奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各行业的广泛应用，理解其在推理层面的环境足迹变得至关重要。然而，大多数现有研究存在局限性，如排除专有模型、忽视基础设施差异和开销或仅关注训练阶段，而推理对AI环境影响的占比日益增加。因此需要填补这一空白。

Method: 引入了一个新的基础设施感知基准测试框架，用于量化30个最先进的LLM在商业数据中心中的环境足迹。该框架结合了公共API性能数据、区域特定的环境乘数和硬件配置的统计推断，并使用交叉效率的数据包络分析（DEA）方法根据性能与环境成本的比率对模型进行排名。

Result: 结果显示o3和DeepSeek-R1是最耗能的模型，每处理一个长提示消耗超过33瓦时，是GPT-4.1 nano的70多倍；Claude-3.7 Sonnet在生态效率方面排名最高。例如，每天7亿次GPT-4o查询会导致相当于35,000户美国家庭用电量的电力消耗等显著年度环境影响。

Conclusion: 研究揭示了个体查询高效但全球规模应用导致资源过度消耗的矛盾现象，并提供了标准化的实证基准测试方法，以推动未来AI开发中的环境责任和可持续性标准。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


### [995] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: This paper comprehensively reviews the use of AI and GenAI in assessing damage from natural disasters, discussing its strengths, limitations, applications to multimodal data, ethical concerns, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Natural disasters pose significant risks to human lives and infrastructure, necessitating rapid and efficient damage assessment for effective disaster response. The potential of AI and GenAI to revolutionize this process by handling various data types and simulating realistic scenarios drives the motivation for this review.

Method: The authors conducted a comprehensive review of existing literature on the application of AI and GenAI techniques to damage assessment across different types of natural disasters. They analyzed the use of these technologies with multimodal data (text, image, video, audio), identified ethical and security issues, and explored potential misuse threats such as misinformation and adversarial attacks.

Result: The review highlights that AI and GenAI can significantly enhance the speed and accuracy of damage assessment in natural disasters through their ability to process diverse data sources and simulate scenarios. However, challenges remain regarding data privacy, security, and ethical considerations. Misuse of GenAI could lead to misinformation or adversarial attacks.

Conclusion: This work is presented as the first comprehensive survey of GenAI techniques in disaster assessment and response. Future research should focus on developing secure, reliable, and ethical GenAI systems to support disaster management efforts.

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


### [996] [Healthy Distrust in AI systems](https://arxiv.org/abs/2505.09747)
*Benjamin Paaßen,Suzana Alpsancar,Tobias Matzner,Ingrid Scharlau*

Main category: cs.CY

TL;DR: This paper introduces the concept of 'healthy distrust' in AI systems, emphasizing its necessity for building meaningful trust and respecting human autonomy. It investigates prior notions of trust and distrust across multiple disciplines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current AI system designs which focus solely on inspiring trust, without considering the social context that may create tension with personal interests.

Method: The method involves examining previous concepts of trust and distrust from various fields such as computer science, sociology, history, psychology, and philosophy to identify gaps and define 'healthy distrust'.

Result: The result is the conceptualization of 'healthy distrust' as an essential element for AI usage that respects human autonomy, suggesting that distrust can be justified and necessary.

Conclusion: The conclusion is that 'healthy distrust' should be recognized and incorporated into AI design and practice to ensure respect for human autonomy and build meaningful trust.

Abstract: Under the slogan of trustworthy AI, much of contemporary AI research is
focused on designing AI systems and usage practices that inspire human trust
and, thus, enhance adoption of AI systems. However, a person affected by an AI
system may not be convinced by AI system design alone -- neither should they,
if the AI system is embedded in a social context that gives good reason to
believe that it is used in tension with a person's interest. In such cases,
distrust in the system may be justified and necessary to build meaningful trust
in the first place. We propose the term "healthy distrust" to describe such a
justified, careful stance towards certain AI usage practices. We investigate
prior notions of trust and distrust in computer science, sociology, history,
psychology, and philosophy, outline a remaining gap that healthy distrust might
fill and conceptualize healthy distrust as a crucial part for AI usage that
respects human autonomy.

</details>


### [997] [Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?](https://arxiv.org/abs/2505.09868)
*Tin Trung Nguyen,Jiannan Xu,Phuong-Anh Nguyen-Le,Jonathan Lazar,Donald Braman,Hal Daumé III,Zubin Jelveh*

Main category: cs.CY

TL;DR: 尽管“个体公平”标准有其美国宪法基础，但它尚未在州或联邦法规中得到具体化。本文通过人体实验探讨了哪些人口统计特征与个体公平评估再犯风险评估（RRA）工具相关，并得出个体相似性函数应考虑年龄和性别，但不应考虑种族。


<details>
  <summary>Details</summary>
Motivation: 探讨个体公平标准在法律条款中的缺失，并试图明确哪些人口统计学特征对于个体公平评估再犯风险评估工具是相关的。

Method: 进行人体实验来评价不同人口统计特征在个体公平评估再犯风险评估工具中的相关性。

Result: 个体相似性函数应该考虑年龄和性别，但不应考虑种族。

Conclusion: 为了填补个体公平标准在法律条款中的空白，实验结果表明个体相似性函数应考虑年龄和性别，但不应考虑种族。

Abstract: Despite its U.S. constitutional foundation, the technical ``individual
fairness'' criterion has not been operationalized in state or federal
statutes/regulations. We conduct a human subjects experiment to address this
gap, evaluating which demographic features are relevant for individual fairness
evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude
that the individual similarity function should consider age and sex, but it
should ignore race.

</details>


### [998] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

TL;DR: 本文讨论了VLM研究的多样性，并提出了一种分类框架和报告标准，以提高VLM相关研究的出版质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 由于VLM研究的多样性和复杂性，传统机器学习报告标准无法满足需求，因此需要重新构建这些标准，以便更好地适应VLM的研究和应用。

Method: 本文通过分析VLM研究的多样性，提出了一种分类框架，并根据该框架制定了全面的报告标准，包括性能评估、数据报告协议和手稿撰写建议。此外，还提供了一个检查清单，以确保标准化的报告。

Result: 本文提出了一个VLM研究的分类框架和相应的报告标准，并提供了一个检查清单，以确保VLM相关研究的出版一致性与质量。

Conclusion: 本文主张需要重新构建传统的机器学习报告标准和评估指南，以适应多阶段的视觉语言模型（VLM）研究，并提出了一个分类框架和相应的报告标准，以确保VLM相关研究的出版一致性和质量。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [999] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

TL;DR: 本研究分析了拉丁美洲对人工智能和机器人技术的担忧变化，发现教育水平和政治倾向是主要驱动因素，并揭示了时间与国家间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人技术日益重塑全球劳动力市场，了解公众对这些技术的看法变得至关重要。本研究旨在探讨这些看法在拉丁美洲的演变情况，并识别影响担忧的关键结构和意识形态因素。

Method: 我们使用统计建模和潜在类别分析，基于来自拉丁巴罗梅特罗2017、2018、2020和2023年波次的调查数据，分析了因人工智能和机器人技术导致的工作流失恐惧。

Result: 我们的研究发现，在2018年对工作流失的恐惧达到高峰，并通过潜在分段出现了不同的态度特征。教育水平和政治倾向被确定为最一致的驱动因素。

Conclusion: 我们的研究揭示了拉丁美洲对人工智能和机器人技术的担忧在时间和国家间的显著差异，并强调了教育水平和政治倾向作为主要驱动因素的重要性。这些结果为理解新兴经济体中的AI焦虑提供了新的见解，并有助于更广泛地了解全球北方以外的自动化公众态度。

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [1000] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

TL;DR: FareShare is a tool that automates lost wage estimation for deactivated rideshare drivers, reducing calculation time and errors, but also highlighting challenges around trust and adoption in labor contexts.


<details>
  <summary>Details</summary>
Motivation: Labor organizers lack effective tools to support complex, error-prone workflows related to deactivation of gig workers, which can devastate their financial stability.

Method: FareShare is a computational tool designed to automate lost wage estimation for deactivated drivers, developed through a 6-month partnership with the State of Washington's largest rideshare labor union.

Result: FareShare reduced lost wage calculation time by over 95%, eliminated manual data entry errors, and enabled legal teams to generate arbitration-ready reports more efficiently. It registered 178 account signups during its field deployment.

Conclusion: FareShare demonstrated significant improvements in lost wage calculation for deactivated drivers, but also highlighted socio-technical challenges around trust, consent, and tool adoption in high-stakes labor contexts.

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [1001] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

TL;DR: 本文提出了一种新的框架，用于分析交通网络安全生态系统中的访客流动模式，并通过AI和空间分析提高对行业集群和移动趋势的预测能力。


<details>
  <summary>Details</summary>
Motivation: 随着交通网络安全生态系统的快速发展，需要更好地理解其时空动态，以及社会经济因素如何影响行业集群和劳动力分布。

Method: 本文开发了一个BiTransGCN框架，将基于注意力的Transformer架构与图卷积网络（GCN）结合，以建模和预测访客流动模式。

Result: 通过将AI驱动的预测技术与空间分析相结合，本文提高了跟踪、解释和预测行业集群和移动趋势的能力。

Conclusion: 本文提出了一个基于AI的预测技术与空间分析相结合的框架，以改进对行业集群和移动趋势的跟踪、解释和预测能力，从而支持安全和有弹性的交通网络的战略规划。它为经济规划、劳动力发展和交通网络安全生态系统中的针对性投资提供了数据驱动的基础。

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [1002] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

TL;DR: FedIDA is a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling to improve fairness in federated learning for healthcare applications.


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect.

Method: FedIDA (Federated Learning for Imbalance and Disparity Awareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling.

Result: Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance.

Conclusion: FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare.

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [1003] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

TL;DR: 本文分析了RLHF增强的生成式AI聊天机器人对语言规范、信息寻求和社会关系的影响，并探讨了其可能带来的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI聊天机器人（如ChatGPT和Claude）使用RLHF技术来优化输出，人类与机器写作文本之间的界限日益模糊，这带来了严重的伦理、社会技术及教育影响。因此，需要对这些影响进行深入研究。

Method: 本文使用Ian Bogost的程序修辞概念，将修辞调查的焦点从内容分析转移到RLHF增强的LLMs中内置的说服机制。

Result: 本文通过修辞分析揭示了RLHF增强的生成式AI聊天机器人在语言规范、信息寻求和社会关系期望方面的变化，并探讨了其潜在的负面影响。

Conclusion: 本文提出了一个关于RLHF增强的生成式AI聊天机器人如何重新塑造语言规范、信息寻求实践和社会关系期望的修辞分析。此外，本文还指出，这种技术可能强化主流语言使用、延续偏见、去语境化学习，并侵犯人际关系。

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [1004] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

TL;DR: 本文介绍了一个新的基础设施感知基准框架，用于量化LLM推理在商业数据中心中30个最先进的模型的环境足迹。结果表明，某些模型的能耗远高于其他模型，而大规模查询可能导致显著的环境影响。研究提供了评估LLM部署可持续性的标准化方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各行业中传播，了解其推理层面的环境足迹不再可选，而是必不可少的。然而，大多数现有研究排除了专有模型，忽视了基础设施的差异性和开销，或者只关注训练，尽管推理越来越主导AI的环境影响。为了弥合这一差距，我们提出了一个新颖的基础设施感知基准框架。

Method: 我们引入了一个新的基础设施感知基准框架，用于量化LLM推理在商业数据中心中30个最先进的模型的环境足迹。该框架结合了公共API性能数据、特定地区的环境乘数和硬件配置的统计推断。我们还使用交叉效率数据包络分析（DEA）按相对于环境成本的性能对模型进行排名。

Result: 我们的结果显示，o3和DeepSeek-R1是最耗能的模型，每个长提示消耗超过33Wh，比GPT-4.1 nano的消耗多70倍以上，并且Claude-3.7 Sonnet在生态效率方面排名最高。虽然单个短GPT-4o查询消耗0.43Wh，但将其扩展到每天7亿次查询会导致显著的年度环境影响。这些包括电力使用量相当于35,000个美国家庭，淡水蒸发量相当于120万人的年度饮用水需求，以及碳排放量需要一个芝加哥大小的森林来抵消。

Conclusion: 我们的研究提供了一种标准化的、基于实证的方法来评估LLM部署的可持续性，为未来AI开发的环境责任和可持续性标准奠定了基础。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


### [1005] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: 本文综述了AI和GenAI在各种自然灾害中的损害评估前景，强调了其优势和局限性，并讨论了数据隐私、安全性和伦理使用等问题。


<details>
  <summary>Details</summary>
Motivation: 自然灾难对人类生命和基础设施资产构成巨大风险，有效的灾害响应依赖于快速而高效地评估损害程度。AI和GenAI提供了突破性的解决方案，能够结合多种类型和来源的数据，模拟现实的灾害场景，并以前所未有的速度识别新兴趋势。

Method: 本文对AI和GenAI在各种自然灾害中的损害评估前景进行了全面综述，并讨论了其在多模态数据（如文本、图像、视频和音频）中的应用。

Result: 本文突出了AI和GenAI在灾害评估中的优势和局限性，并涵盖了数据隐私、安全性和技术在危机中的伦理使用等主要问题。此外，还指出了GenAI被滥用的威胁，例如传播虚假信息和进行对抗性攻击。

Conclusion: 本文认为，这项工作代表了对灾害评估和响应领域中使用Gen-AI技术的首次全面调查。

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


### [1006] [Healthy Distrust in AI systems](https://arxiv.org/abs/2505.09747)
*Benjamin Paaßen,Suzana Alpsancar,Tobias Matzner,Ingrid Scharlau*

Main category: cs.CY

TL;DR: 本文探讨了在可信AI的口号下，当代AI研究如何关注设计能够激发人类信任并促进AI系统采用的AI系统和使用实践。然而，当AI系统嵌入到一个可能与个人利益相冲突的社会背景中时，仅靠AI系统设计可能无法说服受影响的人。在这种情况下，对系统的不信任可能是合理且必要的，以建立有意义的信任。本文提出了“健康的不信任”这一概念，描述了对某些AI使用实践的合理、谨慎的态度，并探讨了计算机科学、社会学、历史学、心理学和哲学中关于信任和不信任的先验概念，指出了健康不信任可能填补的空白，并将健康不信任概念化为尊重人类自主性的AI使用的重要组成部分。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探讨在AI系统嵌入到可能与个人利益相冲突的社会背景中时，为什么仅靠AI系统设计可能无法说服受影响的人，以及为什么对系统的不信任可能是合理且必要的。

Method: 本文通过探讨计算机科学、社会学、历史学、心理学和哲学中关于信任和不信任的先验概念，分析了健康不信任可能填补的空白，并将其概念化为尊重人类自主性的AI使用的重要组成部分。

Result: 本文提出了“健康的不信任”这一概念，描述了对某些AI使用实践的合理、谨慎的态度，并指出健康不信任是尊重人类自主性的AI使用的重要组成部分。

Conclusion: 本文认为，在AI系统嵌入到可能与个人利益相冲突的社会背景中时，对系统的不信任可能是合理且必要的，以建立有意义的信任。健康不信任是尊重人类自主性的AI使用的重要组成部分。

Abstract: Under the slogan of trustworthy AI, much of contemporary AI research is
focused on designing AI systems and usage practices that inspire human trust
and, thus, enhance adoption of AI systems. However, a person affected by an AI
system may not be convinced by AI system design alone -- neither should they,
if the AI system is embedded in a social context that gives good reason to
believe that it is used in tension with a person's interest. In such cases,
distrust in the system may be justified and necessary to build meaningful trust
in the first place. We propose the term "healthy distrust" to describe such a
justified, careful stance towards certain AI usage practices. We investigate
prior notions of trust and distrust in computer science, sociology, history,
psychology, and philosophy, outline a remaining gap that healthy distrust might
fill and conceptualize healthy distrust as a crucial part for AI usage that
respects human autonomy.

</details>


### [1007] [Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?](https://arxiv.org/abs/2505.09868)
*Tin Trung Nguyen,Jiannan Xu,Phuong-Anh Nguyen-Le,Jonathan Lazar,Donald Braman,Hal Daumé III,Zubin Jelveh*

Main category: cs.CY

TL;DR: 本文通过一项人类受试者实验，研究了哪些人口统计特征对于量刑风险评估工具的个体公平性评估是相关的，并得出结论：个体相似性函数应考虑年龄和性别，但不应考虑种族。


<details>
  <summary>Details</summary>
Motivation: 尽管有美国宪法基础，但技术性的“个体公平”标准尚未在州或联邦法规中实现。

Method: 我们进行了一项人类受试者实验，以评估哪些人口统计特征对于量刑风险评估（RRA）工具的个体公平性评估是相关的。

Result: 我们的分析得出结论，个体相似性函数应考虑年龄和性别，但应忽略种族。

Conclusion: 个体相似性函数应考虑年龄和性别，但不应考虑种族。

Abstract: Despite its U.S. constitutional foundation, the technical ``individual
fairness'' criterion has not been operationalized in state or federal
statutes/regulations. We conduct a human subjects experiment to address this
gap, evaluating which demographic features are relevant for individual fairness
evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude
that the individual similarity function should consider age and sex, but it
should ignore race.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1008] [On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion](https://arxiv.org/abs/2505.09766)
*Roberto Ponciroli*

Main category: math.NA

TL;DR: This paper presents a methodology for reconstructing the spatial distribution of neutron flux in a nuclear reactor using real-time ex-core detector measurements and solving the Kirchhoff-Helmholtz equation as an inverse problem to approximate the Green's function.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable method for reconstructing the spatial distribution of neutron flux within a nuclear reactor, leveraging real-time boundary data from ex-core detectors.

Method: Utilize the Kirchhoff-Helmholtz equation to define the problem, derive the Green's function from the one-speed neutron diffusion model, establish symmetry properties, and demonstrate the procedure for sensor data interpretation and flux reconstruction.

Result: The existence and uniqueness of the Green's function inferred from sampled data are demonstrated, ensuring the reliability of the proposed neutron flux reconstruction method.

Conclusion: The data-driven approximation of the Green's function is well-posed, providing a reliable framework for neutron flux reconstruction in nuclear reactors.

Abstract: This work presents a methodology for reconstructing the spatial distribution
of the neutron flux in a nuclear reactor, leveraging real-time measurements
obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation
inherently defines the problem of estimating a scalar field within a domain
based on boundary data, making it a natural mathematical framework for this
task. The main challenge lies in deriving the Green's function specific to the
domain and the neutron diffusion process. While analytical solutions for
Green's functions exist for simplified geometries, their derivation of complex,
heterogeneous domains-such as a nuclear reactor-requires a numerical approach.
The objective of this work is to demonstrate the well-posedness of the
data-driven Green's function approximation by formulating and solving the K-H
equation as an inverse problem. After establishing the symmetry properties that
the Green's function must satisfy, the K-H equation is derived from the
one-speed neutron diffusion model. This is followed by a comprehensive
description of the procedure for interpreting sensor readings and implementing
the neutron flux reconstruction algorithm. Finally, the existence and
uniqueness of the Green's function inferred from the sampled data are
demonstrated, ensuring the reliability of the proposed method and its
predictions.

</details>


### [1009] [On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion](https://arxiv.org/abs/2505.09766)
*Roberto Ponciroli*

Main category: math.NA

TL;DR: 本文提出了一种基于K-H方程和数据驱动格林函数的中子通量重建方法，适用于复杂核反应堆环境。


<details>
  <summary>Details</summary>
Motivation: 现有的解析解仅适用于简单几何结构，而核反应堆等复杂异质域需要一种数值方法来推导格林函数，以实现中子通量的准确重建。

Method: 本文利用Kirchhoff-Helmholtz（K-H）方程，将中子通量的重建问题作为反问题进行求解，并通过数值方法推导出适用于复杂异质域的格林函数。

Result: 本文成功证明了从采样数据中推导出的格林函数的存在性和唯一性，确保了所提方法及其预测的可靠性。

Conclusion: 本文提出的方法能够可靠地重建核反应堆中的中子通量空间分布，并通过数据驱动的格林函数近似证明了其合理性。

Abstract: This work presents a methodology for reconstructing the spatial distribution
of the neutron flux in a nuclear reactor, leveraging real-time measurements
obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation
inherently defines the problem of estimating a scalar field within a domain
based on boundary data, making it a natural mathematical framework for this
task. The main challenge lies in deriving the Green's function specific to the
domain and the neutron diffusion process. While analytical solutions for
Green's functions exist for simplified geometries, their derivation of complex,
heterogeneous domains-such as a nuclear reactor-requires a numerical approach.
The objective of this work is to demonstrate the well-posedness of the
data-driven Green's function approximation by formulating and solving the K-H
equation as an inverse problem. After establishing the symmetry properties that
the Green's function must satisfy, the K-H equation is derived from the
one-speed neutron diffusion model. This is followed by a comprehensive
description of the procedure for interpreting sensor readings and implementing
the neutron flux reconstruction algorithm. Finally, the existence and
uniqueness of the Green's function inferred from the sampled data are
demonstrated, ensuring the reliability of the proposed method and its
predictions.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [1010] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

TL;DR: An abstract about developing a weakly-supervised textual classification system called Ornithologist that measures the hawkishness and dovishness of central bank text, using taxonomy-guided reasoning to guide a large language model with human-authored decision trees.


<details>
  <summary>Details</summary>
Motivation: To create a more transparent, explainable, and accessible system for measuring the hawkishness and dovishness of central bank text that requires less supervision than traditional classification systems and can be applied to other problems or sources of text (e.g. news) without much modification.

Method: Developing Ornithologist, a weakly-supervised textual classification system that uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees.

Result: Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.

Conclusion: This system is not only more transparent and explainable but also reduces hallucination risk and can be easily applied to other problems or sources of text.

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


### [1011] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

TL;DR: Ornithologist is a weakly-supervised system for classifying central bank texts, using taxonomy-guided reasoning to improve transparency and reduce hallucination risks.


<details>
  <summary>Details</summary>
Motivation: To develop a weakly-supervised textual classification system that increases transparency, explainability, and reduces hallucination risk.

Method: Ornithologist uses 'taxonomy-guided reasoning', guiding a large language model with human-authored decision trees.

Result: Ornithologist can be applied to other problems or sources of text without much modification.

Conclusion: Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [1012] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

TL;DR: The paper introduces EDBench, a large-scale dataset of electron density (ED) data covering 3.3 million molecules to advance learning-based research in molecular machine learning force fields (MLFFs). It provides benchmark tasks for prediction, retrieval, and generation, demonstrating feasible and accurate ED calculation with reduced computational cost compared to DFT.


<details>
  <summary>Details</summary>
Motivation: Existing MLFFs focus on atoms, molecules, and simple quantum chemical properties but overlook the importance of electron density (ED), which is crucial for understanding molecular force fields according to the Hohenberg-Kohn theorem. The lack of large-scale ED data limits its application in MLFFs due to time-consuming DFT calculations.

Method: The authors introduce EDBench, a dataset built upon PCQM4Mv2 that includes accurate ED data for 3.3 million molecules. They design ED-centric benchmark tasks for prediction, retrieval, and generation to evaluate models' ability to understand electronic information.

Result: Evaluation shows that learning from EDBench is feasible and achieves high accuracy. Learning-based methods can calculate ED efficiently with comparable precision while significantly reducing computational cost relative to traditional DFT calculations.

Conclusion: EDBench provides a robust foundation for ED-driven drug discovery and materials science by offering freely available data and benchmarks.

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


### [1013] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

TL;DR: This paper introduces EDBench, a large-scale, high-quality dataset of electron density (ED) designed to advance learning-based research at the electronic scale. The dataset enables efficient and accurate calculation of ED with reduced computational cost compared to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.


<details>
  <summary>Details</summary>
Motivation: Existing molecular machine learning force fields (MLFFs) focus on learning atoms, molecules, and simple quantum chemical properties but ignore the importance of electron density (ED) in accurately understanding molecular force fields (MFFs). The calculation of ED relies on time-consuming first-principles density functional theory (DFT), leading to a lack of large-scale ED data and limiting its application in MLFFs.

Method: The paper introduces EDBench, a large-scale, high-quality dataset of ED built upon the PCQM4Mv2. It also designs a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation to evaluate the ability of models to understand and utilize electronic information.

Result: The evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, learning-based methods can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations.

Conclusion: EDBench provides a large-scale, high-quality dataset of electron density (ED) that can advance learning-based research at the electronic scale. The dataset enables efficient and accurate calculation of ED with reduced computational cost compared to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1014] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

TL;DR: An enhanced Retrieval-Augmented Generation (RAG) application is presented to assist data scientists in accessing precise and contextually relevant academic resources, with substantial improvements shown in Context Relevance.


<details>
  <summary>Details</summary>
Motivation: Efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation in data science.

Method: The AI-powered application integrates advanced techniques including GROBID for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method.

Result: A comprehensive evaluation using the RAGAS framework demonstrates substantial improvements in key metrics, particularly Context Relevance.

Conclusion: The findings highlight the potential of this enhanced RAG system to transform academic exploration within data science and advance the workflow of research and innovation.

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [1015] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

TL;DR: FocusedRetriever是一个基于半结构化知识库（SKB）的模块化框架，用于多跳问答。它整合了多种组件，在STaRK基准测试的所有三个数据集上超越了现有最佳方法，平均首次命中率高出25.7%。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，机器学习模型和交互系统可以访问结构化知识（如知识图谱或表格）和非结构化内容（如自然语言文档），但大多数系统只能依赖其中一种。为了弥合这一差距，本文提出了利用半结构化知识库的方法，以链接非结构化内容与结构化数据，从而实现更有效的知识访问和使用。

Method: FocusedRetriever通过以下步骤进行多跳问答：1) 利用大型语言模型（LLM）从非结构化文本中提取关系事实和实体属性；2) 使用节点集连接过滤答案候选者；3) 采用向量相似性搜索检索和排名相关非结构化内容；4) 最后再次利用LLM的情境能力对前k个答案进行排名。

Result: 实验结果表明，FocusedRetriever在STaRK基准测试的所有三个数据集上表现优于现有方法，平均首次命中率比第二佳方法高出25.7%。此外，中间结果分析揭示了通过微调等方法进一步提升性能的机会。

Conclusion: FocusedRetriever展示了在多跳问答任务中的卓越性能，并为未来的研究提供了改进方向，例如通过微调大型语言模型来增强其性能。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [1016] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

TL;DR: 尽管扩散模型在推荐系统中的应用被广泛报道为显著进步，但实验表明，这些模型在计算复杂度和碳排放较高的情况下，仍然逊色于简单的现有模型。这揭示了方法论问题的持续存在以及对科研严谨性和文化变革的需求。


<details>
  <summary>Details</summary>
Motivation: 调查现代去噪扩散概率模型在推荐系统中的最新进展，并检查早期研究中发现的方法论问题是否仍然存在于当前的研究中。

Method: 通过实验重现SIGIR 2023和2024年发布的四种基于扩散模型的推荐系统，并与调优后的基线模型进行比较，评估其性能。

Result: 发现扩散模型在传统top-n推荐任务上表现不佳，且其生成能力受到限制，同时确认了方法论问题依然存在。

Conclusion: 需要更高的科研严谨性以及研究和发表文化的变革来应对这些问题。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


### [1017] [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
*Alejo Lopez-Avila,Jinhua Du*

Main category: cs.IR

TL;DR: Multimodal recommender systems (MRS) can be enhanced by large language models (LLMs), which offer semantic reasoning, in-context learning, and dynamic input handling. This survey reviews the integration of LLMs into MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models (LLMs) can enhance multimodal recommender systems (MRS) compared to pre-trained language models (PLMs), and address challenges related to scalability and model accessibility.

Method: The survey proposes a taxonomy for integration patterns of LLMs in MRS, identifies transferable techniques from related domains, provides an overview of evaluation metrics and datasets, and suggests future research directions.

Result: Provides insights into prompting strategies, fine-tuning methods, and data adaptation techniques when integrating LLMs into MRS, supporting future research advancements.

Conclusion: LLMs introduce significant opportunities for enhancing MRS performance but also present challenges that need addressing. The proposed taxonomy and identified techniques aim to guide future research.

Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.

</details>


### [1018] [Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights](https://arxiv.org/abs/2505.10043)
*Yifan Wu,Lutao Yan,Yizhang Zhu,Yinan Mei,Jiannan Wang,Nan Tang,Yuyu Luo*

Main category: cs.IR

TL;DR: 为了提升文本到图表检索的性能，本文提出了一种生成层次语义信息的训练数据管道，并基于此训练了名为ChartFinder的模型。该模型在精确查询和模糊查询任务中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图表检索解决方案往往无法捕捉图表的语义内容和上下文信息，主要是由于缺乏全面的元数据（或语义洞见）。

Method: 提出了一个训练数据开发管道，自动合成图表的层次语义洞见，包括视觉模式、统计特性和实际应用，并基于此训练了一个名为ChartFinder的CLIP-based模型。

Result: 实验表明，ChartFinder在文本到图表检索任务中显著优于现有方法。对于精确查询，ChartFinder的NDCG@10达到66.9%，比最先进的模型高出11.58%。在模糊查询任务中，该方法也显示出一致的改进，几乎所有指标平均提高了5%。

Conclusion: 本文提出的ChartFinder模型通过利用丰富的语义洞见，在训练阶段学习到了更好的图表表示，从而显著提升了文本到图表检索的性能。

Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.

</details>


### [1019] [Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M](https://arxiv.org/abs/2505.10212)
*Dario Di Palma,Felice Antonio Merra,Maurizio Sfilio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: Large Language Models (LLMs) may have memorized public recommendation datasets, which affects their generalizability and fairness. This study investigates the extent of memorization in LLMs using MovieLens-1M dataset.


<details>
  <summary>Details</summary>
Motivation: To verify whether Large Language Models (LLMs) have memorized public recommendation datasets as part of their training data, which is crucial for ensuring the generalizability of research findings and preventing bias amplification.

Method: Define dataset memorization as the ability to retrieve item attributes, user profiles, and user-item interactions by prompting LLMs. Analyze two model families (GPT and Llama) across multiple sizes on the MovieLens-1M dataset. Examine the impact of memorization on recommendation performance and whether it varies across model families and sizes.

Result: All examined models exhibit some degree of memorization of the MovieLens-1M dataset. Recommendation performance is related to the extent of memorization.

Conclusion: LLMs have memorized parts of the MovieLens-1M dataset, impacting recommendation performance. The code for this study has been made publicly available.

Abstract: Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector

</details>


### [1020] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

TL;DR: 本文提出了一种增强型检索增强生成（RAG）系统，用于帮助数据科学家更有效地访问相关的学术资源，并通过实验验证了其在提高信息检索准确性和减少信息过载方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据科学领域，高效地导航庞大的学术文献是做出明智决策和创新的关键。然而，当前的系统在检索相关学术资源方面存在不足，因此需要一种更有效的解决方案。

Method: 本文提出了一种基于人工智能的增强型RAG应用，集成了GROBID技术、微调嵌入模型、语义分块和以摘要优先的检索方法，以提高检索信息的相关性和准确性。

Result: 通过RAGAS框架进行的全面评估显示，关键指标如上下文相关性有显著提升，证明了该系统的有效性。

Conclusion: 本文展示了增强型检索增强生成（RAG）系统在数据科学领域学术探索中的潜力，表明该系统能够有效减少信息过载并提高决策过程的效率。

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [1021] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

TL;DR: FocusedRetriever是一个基于半结构化知识库的多跳问答框架，利用大型语言模型和其他技术，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大多数机器学习模型和交互系统仅依赖结构化知识或非结构化内容，而半结构化知识库（SKB）能够将非结构化内容与结构化数据中的节点连接起来，从而提供新的知识访问和使用策略。

Method: FocusedRetriever是一个基于半结构化知识库（SKB）的框架，整合了基于VSS的实体搜索、基于LLM的Cypher查询生成以及成对重新排序组件。

Result: FocusedRetriever在所有三个STaRK基准测试集上均优于最先进的方法，平均首次命中率比第二好的方法高出25.7%。

Conclusion: FocusedRetriever利用大型语言模型（LLMs）的能力，结合节点集连接、向量相似性搜索和上下文理解，实现了在多跳问答任务中的优越性能。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [1022] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

TL;DR: 本研究分析了最近应用扩散模型到推荐系统的论文，发现存在方法论问题，且最新的推荐技术并不优于更简单的模型。


<details>
  <summary>Details</summary>
Motivation: 由于早期的可重复性研究表明，这一领域的进展可能相当有限，因此我们希望检查这些问题是否仍然存在于当今的研究中。

Method: 我们试图重现最新报道的将现代去噪扩散概率模型应用于推荐系统的优势，重点关注在顶级排名的SIGIR会议2023年和2024年发表的四个模型。

Result: 我们的发现令人担忧，揭示了持续的方法论问题。通过实验，我们发现基于扩散模型的最新推荐技术，尽管计算复杂性和大量的碳足迹，但始终被更简单的现有模型所超越。此外，我们发现了扩散模型的特性与传统top-n推荐任务之间的关键不匹配，这引发了它们适用于推荐的疑问。

Conclusion: 我们的结果和持续的方法论问题呼吁在该领域提高科学严谨性，并对研究和出版文化进行颠覆性的改变。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


### [1023] [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
*Alejo Lopez-Avila,Jinhua Du*

Main category: cs.IR

TL;DR: 本文对大型语言模型（LLMs）和多模态推荐系统（MRS）交叉领域的最新工作进行了全面综述，重点是提示策略、微调方法和数据适应技术。我们提出了一个新颖的分类法来表征集成模式，识别来自相关推荐领域的可转移技术，概述评估指标和数据集，并指出可能的未来方向。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统（MRS）整合异构用户和项目数据，如文本、图像和结构化信息，以提高推荐性能。大型语言模型（LLMs）通过启用语义推理、上下文学习和动态输入处理，为MRS带来了新的机遇。与早期预训练语言模型（PLMs）相比，LLMs提供了更大的灵活性和泛化能力，但也带来了可扩展性和模型可访问性的挑战。

Method: 本文对LLMs和MRS交叉领域的最新工作进行了全面综述，重点是提示策略、微调方法和数据适应技术。我们提出了一个新颖的分类法来表征集成模式，识别来自相关推荐领域的可转移技术，概述评估指标和数据集，并指出可能的未来方向。

Result: 本文提出了一个新颖的分类法来表征集成模式，识别来自相关推荐领域的可转移技术，概述评估指标和数据集，并指出可能的未来方向。

Conclusion: 本文旨在阐明大型语言模型（LLMs）在多模态推荐中的新兴作用，并支持该快速发展的领域的未来研究。

Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.

</details>


### [1024] [Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights](https://arxiv.org/abs/2505.10043)
*Yifan Wu,Lutao Yan,Yizhang Zhu,Yinan Mei,Jiannan Wang,Nan Tang,Yuyu Luo*

Main category: cs.IR

TL;DR: 本文提出了一种新的文本到图表检索方法，称为ChartFinder，通过自动生成图表的层次化语义洞察来提高检索性能。实验结果表明，ChartFinder在各种设置下的文本到图表检索任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图表检索解决方案往往无法捕捉图表的语义内容和上下文信息，主要是由于缺乏全面的元数据（或语义洞察）。为了解决这一限制，我们提出了一个训练数据开发管道，以自动生成图表的层次化语义洞察。

Method: 我们提出了一个训练数据开发管道，该管道可以自动生成图表的层次化语义洞察，涵盖视觉模式（以视觉为导向）、统计属性（以统计为导向）和实际应用（以任务为导向），并生成了207,498个语义洞察，用于69,166个图表。基于这些，我们训练了一个名为ChartFinder的基于CLIP的模型，以学习更好的图表表示用于文本到图表的检索。

Result: 实验结果表明，ChartFinder在文本到图表的检索任务中显著优于现有方法。对于精确查询，ChartFinder在NDCG@10上达到了66.9%，比最先进的模型高出11.58%。在模糊查询任务中，我们的方法也展示了稳定的改进，几乎所有指标的平均增加约为5%。

Conclusion: 实验结果表明，ChartFinder在文本到图表的检索任务中显著优于现有方法。对于精确查询，ChartFinder在NDCG@10上达到了66.9%，比最先进的模型高出11.58%。在模糊查询任务中，我们的方法也展示了稳定的改进，几乎所有指标的平均增加约为5%。

Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.

</details>


### [1025] [Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M](https://arxiv.org/abs/2505.10212)
*Dario Di Palma,Felice Antonio Merra,Maurizio Sfilio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 本文研究了大型语言模型（LLMs）是否记忆了公共推荐数据集MovieLens-1M，发现所有模型都有一定程度的记忆，并且推荐性能与记忆程度相关。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否将公共推荐数据集作为训练数据的一部分进行记忆，因为记忆会降低研究结果的泛化性，并可能放大偏差。

Method: 定义数据集记忆为通过提示LLMs检索物品属性、用户资料和用户-物品交互的程度，并分析记忆对推荐性能的影响，以及不同模型家族和模型大小的记忆差异。

Result: 所有模型都表现出一定程度的MovieLens-1M记忆，且推荐性能与记忆程度相关。

Conclusion: 所有模型都表现出一定程度的MovieLens-1M记忆，且推荐性能与记忆程度相关。

Abstract: Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1026] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

TL;DR: The paper proposes a new extension of EL with weakened negation to represent negative knowledge while maintaining tractability.


<details>
  <summary>Details</summary>
Motivation: Biomedical ontologies contain numerous concept or role names involving negative knowledge such as lacks_part, absence_of. Current representation with labels rather than logical constructors do not allow a reasoner to interpret these as a kind of negation. Adding full negation makes the logic intractable.

Method: Introduce categorical semantics of all logical constructors of the DL SH including EL with disjunction, negation, universal restriction, role inclusion and transitive roles. Weaken semantics of disjunction and universal restriction by identifying independent categorical properties that cause intractability and dropping them.

Result: The resulting logic from weakening semantics is more expressive than EL with the bottom concept, transitive roles and role inclusion.

Conclusion: A new extension of EL with weakened negation is proposed which allows to represent negative knowledge while retaining tractability.

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


### [1027] [Inconsistency Handling in DatalogMTL](https://arxiv.org/abs/2505.10394)
*Meghyn Bienvenu,Camille Bourgaux,Atefe Khodadaditaghanaki*

Main category: cs.LO

TL;DR: The paper explores inconsistency handling in DatalogMTL, defines conflict and repair notions, and analyzes data complexity for generating conflicts/repairs and query entailment.


<details>
  <summary>Details</summary>
Motivation: Inconsistency handling is crucial in DatalogMTL as facts associated with time intervals may contradict rules; thus, understanding how to restore consistency through various methods like removing facts or altering time intervals is important.

Method: The authors define relevant notions of conflicts (minimal explanations for inconsistency) and repairs (ways to restore consistency) specific to the DatalogMTL setting. They also study the properties of these notions and their inconsistency-tolerant semantics.

Result: The first contribution includes definitions and analysis of conflicts and repairs in DatalogMTL. The second contribution involves a detailed data complexity analysis for tasks such as generating a single conflict/repair and query entailment under repair-based semantics.

Conclusion: This work provides essential insights into inconsistency handling in DatalogMTL by defining key concepts and analyzing computational complexities, which will help advance the field.

Abstract: In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.

</details>


### [1028] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

TL;DR: 本文研究了如何在保持可解性的同时扩展EL以表示否定知识，并通过范畴语义的方法实现了这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学本体中包含许多涉及否定知识的概念或角色名称，但传统的逻辑构造无法让推理器正确解释这些否定知识。

Method: 引入了SH逻辑的所有逻辑构造的范畴语义，通过识别并删除导致不可解性的独立范畴属性来恢复可解性。

Result: 提出的逻辑比带有底概念、传递角色和角色包含的EL更具表达能力。

Conclusion: 本文提出了一种新的EL扩展，通过弱化否定来表示负面知识，同时保持可解性。

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


### [1029] [Inconsistency Handling in DatalogMTL](https://arxiv.org/abs/2505.10394)
*Meghyn Bienvenu,Camille Bourgaux,Atefe Khodadaditaghanaki*

Main category: cs.LO

TL;DR: 本文研究了DatalogMTL中处理不一致性的方法，并分析了生成冲突/修复和查询蕴含的数据复杂性。


<details>
  <summary>Details</summary>
Motivation: 由于事实与时间区间相关联，当它们与规则矛盾时，有多种方式可以恢复一致性，例如删除事实或修改其时间区间。

Method: 定义了冲突和修复的概念，并分析了生成单个冲突/修复和查询蕴含的数据复杂性。

Result: 提出了冲突和修复的概念，并分析了生成单个冲突/修复和查询蕴含的数据复杂性。

Conclusion: 本文探讨了在DatalogMTL中处理不一致性的方法，并研究了相关概念和属性，以及基于修复的语义下的查询蕴含问题。

Abstract: In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1030] [Neural models for prediction of spatially patterned phase transitions: methods and challenges](https://arxiv.org/abs/2505.09718)
*Daniel Dylewsky,Sonia Kéfi,Madhur Anand,Chris T. Bauch*

Main category: physics.comp-ph

TL;DR: Dryland vegetation ecosystems are susceptible to critical transitions between alternative stable states when subjected to external forcing. Recent methodological developments in Early Warning Signal (EWS) detection have shown promise in identifying dynamical signatures of oncoming critical transitions, with particularly strong predictive capabilities being demonstrated by deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Dryland vegetation ecosystems are known to be susceptible to critical transitions between alternative stable states when subjected to external forcing.

Method: This paper explores the successes and shortcomings of neural EWS detection for spatially patterned phase transitions, and shows how these models can be used to gain insight into where and how EWS-relevant information is encoded in spatiotemporal dynamics.

Result: Results reveal that model performance often changes dramatically when training and test data sources are interchanged, which offers new insight into the criteria for model generalization.

Conclusion: The study reveals the potential and limitations of using neural networks for detecting early warning signals in dryland vegetation ecosystems.

Abstract: Dryland vegetation ecosystems are known to be susceptible to critical
transitions between alternative stable states when subjected to external
forcing. Such transitions are often discussed through the framework of
bifurcation theory, but the spatial patterning of vegetation, which is
characteristic of drylands, leads to dynamics that are much more complex and
diverse than local bifurcations. Recent methodological developments in Early
Warning Signal (EWS) detection have shown promise in identifying dynamical
signatures of oncoming critical transitions, with particularly strong
predictive capabilities being demonstrated by deep neural networks. However, a
machine learning model trained on synthetic examples is only useful if it can
effectively transfer to a test case of practical interest. These models'
capacity to generalize in this manner has been demonstrated for bifurcation
transitions, but it is not as well characterized for high-dimensional phase
transitions. This paper explores the successes and shortcomings of neural EWS
detection for spatially patterned phase transitions, and shows how these models
can be used to gain insight into where and how EWS-relevant information is
encoded in spatiotemporal dynamics. A few paradigmatic test systems are used to
illustrate how the capabilities of such models can be probed in a number of
ways, with particular attention to the performances of a number of proposed
statistical indicators for EWS and to the supplementary task of distinguishing
between abrupt and continuous transitions. Results reveal that model
performance often changes dramatically when training and test data sources are
interchanged, which offers new insight into the criteria for model
generalization.

</details>


### [1031] [Neural models for prediction of spatially patterned phase transitions: methods and challenges](https://arxiv.org/abs/2505.09718)
*Daniel Dylewsky,Sonia Kéfi,Madhur Anand,Chris T. Bauch*

Main category: physics.comp-ph

TL;DR: 本文研究了神经网络在检测空间模式相变的早期预警信号（EWS）中的应用，发现模型性能在训练和测试数据源互换时可能显著变化，这为模型泛化标准提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 干地植被生态系统在外部扰动下容易发生临界转变，而这种转变通常通过分岔理论框架进行讨论。然而，干地植被的空间模式导致的动力学比局部分岔更为复杂和多样。近年来，早期预警信号（EWS）检测的方法进展表明，深度神经网络在识别即将发生的临界转变方面具有很强的预测能力。但机器学习模型仅在能有效转移到实际感兴趣案例的情况下才有用。虽然这些模型在分岔转变中表现出良好的泛化能力，但在高维相变中的泛化能力尚未得到充分描述。因此，本文旨在探索神经网络EWS检测在空间模式相变中的成功与不足。

Method: 本文探讨了神经网络在检测空间模式相变的早期预警信号（EWS）中的应用，通过几个典型的测试系统来分析这些模型的能力，并特别关注了几种提出的统计指标在EWS检测中的表现以及区分突然和连续相变的任务。

Result: 研究结果表明，当训练和测试数据源互换时，模型性能往往会发生显著变化，这为模型泛化标准提供了新的见解。此外，研究还展示了神经网络EWS检测在不同测试系统中的表现，并分析了统计指标在EWS检测中的有效性。

Conclusion: 研究揭示了神经网络在检测空间模式相变的早期预警信号方面的成功与局限性，并展示了这些模型如何帮助理解EWS相关信息在时空动态中的编码方式。此外，研究还发现当训练和测试数据源互换时，模型性能可能会发生显著变化，这为模型泛化标准提供了新的见解。

Abstract: Dryland vegetation ecosystems are known to be susceptible to critical
transitions between alternative stable states when subjected to external
forcing. Such transitions are often discussed through the framework of
bifurcation theory, but the spatial patterning of vegetation, which is
characteristic of drylands, leads to dynamics that are much more complex and
diverse than local bifurcations. Recent methodological developments in Early
Warning Signal (EWS) detection have shown promise in identifying dynamical
signatures of oncoming critical transitions, with particularly strong
predictive capabilities being demonstrated by deep neural networks. However, a
machine learning model trained on synthetic examples is only useful if it can
effectively transfer to a test case of practical interest. These models'
capacity to generalize in this manner has been demonstrated for bifurcation
transitions, but it is not as well characterized for high-dimensional phase
transitions. This paper explores the successes and shortcomings of neural EWS
detection for spatially patterned phase transitions, and shows how these models
can be used to gain insight into where and how EWS-relevant information is
encoded in spatiotemporal dynamics. A few paradigmatic test systems are used to
illustrate how the capabilities of such models can be probed in a number of
ways, with particular attention to the performances of a number of proposed
statistical indicators for EWS and to the supplementary task of distinguishing
between abrupt and continuous transitions. Results reveal that model
performance often changes dramatically when training and test data sources are
interchanged, which offers new insight into the criteria for model
generalization.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1032] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

TL;DR: 提出了一种基于神经隐式函数的方法，用于学习3D表面以实现解剖学感知的精确肺段重建，并引入了两个临床相关的评估指标。此外，还开发了一个名为Lung3D的数据集，包含800个标记的肺段及其对应的气道、动脉、静脉和段间静脉的3D模型。实验结果表明，该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的3D肺段重建在肺段切除术和肺癌手术治疗计划中起着至关重要的作用。然而，传统的深度学习方法由于目标重建的分辨率要求，常常受到计算资源限制或粒度有限的问题。而隐式建模因其计算效率和连续表示的优势受到青睐。

Method: 提出了一种基于神经隐式函数的方法，通过学习3D表面来实现解剖学感知的精确肺段重建。该方法通过变形一个可学习的模板来表示形状。同时，引入了两个临床相关的评估指标来全面评估重建效果。

Result: 实验结果表明，所提出的方法优于现有方法，为肺段重建提供了新的视角。

Conclusion: 本研究提出了一种新的基于神经隐式函数的肺段重建方法，解决了传统方法的局限性，并提供了临床相关的评估指标和数据集，推动了肺段重建领域的发展。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [1033] [IntrinsicEdit: Precise generative image manipulation in intrinsic space](https://arxiv.org/abs/2505.08889)
*Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Miloš Hašan,Jae Shin Yoon,Thomas Leimkühler,Christian Theobalt,Iliyan Georgiev*

Main category: cs.GR

TL;DR: Generative diffusion models have improved image editing, but lack precise control and specialization in single tasks. This paper introduces a versatile generative workflow operating in intrinsic-image latent space for semantic, local manipulation with pixel precision across various editing operations without additional data collection or model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current generative diffusion models provide high-quality image editing results with intuitive interfaces, but these interfaces lack precise control and the methods typically specialize on a single editing task.

Method: The method involves building a workflow atop the RGB-X diffusion framework to address identity preservation and intrinsic-channel entanglement by incorporating exact diffusion inversion and disentangled channel manipulation.

Result: This approach enables precise, efficient editing with automatic resolution of global illumination effects and demonstrates state-of-the-art performance across a variety of complex image editing tasks.

Conclusion: The introduced generative workflow operates effectively in an intrinsic-image latent space, enabling a range of editing operations with pixel precision and without the need for additional data collection or model fine-tuning.

Abstract: Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.

</details>


### [1034] [Neural BRDF Importance Sampling by Reparameterization](https://arxiv.org/abs/2505.08998)
*Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: This paper presents a reparameterization-based formulation for neural BRDF importance sampling that improves efficiency and flexibility in the rendering pipeline, achieving superior variance reduction and high inference speeds.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the significant challenge of importance sampling in neural bidirectional reflectance distribution functions (BRDFs), which are crucial for enhancing realism in physically-based rendering.

Method: The authors introduce a reparameterization-based formulation of neural BRDF importance sampling. This method transfers the distribution learning task into identifying BRDF integral substitutions, removing constraints such as invertible networks and multi-step inference used in previous methods. It seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples.

Result: Through variance and performance analysis, the proposed reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.

Conclusion: The reparameterization-based formulation of neural BRDF importance sampling offers greater flexibility and efficiency in the rendering process, effectively reducing variance and improving rendering performance.

Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged
as popular material representations for enhancing realism in physically-based
rendering. Yet their importance sampling remains a significant challenge. In
this paper, we introduce a reparameterization-based formulation of neural BRDF
importance sampling that seamlessly integrates into the standard rendering
pipeline with precise generation of BRDF samples. The reparameterization-based
formulation transfers the distribution learning task to a problem of
identifying BRDF integral substitutions. In contrast to previous methods that
rely on invertible networks and multi-step inference to reconstruct BRDF
distributions, our model removes these constraints, which offers greater
flexibility and efficiency. Our variance and performance analysis demonstrates
that our reparameterization method achieves the best variance reduction in
neural BRDF renderings while maintaining high inference speeds compared to
existing baselines.

</details>


### [1035] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

TL;DR: UMotion is an uncertainty-driven, online framework that combines IMUs and UWB sensors with a UKF for accurate 3D human shape and pose estimation.


<details>
  <summary>Details</summary>
Motivation: Sparse wearable inertial measurement units (IMUs) are popular for estimating 3D human motion but face challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies.

Method: UMotion uses six integrated body-worn ultra-wideband (UWB) distance sensors with IMUs. A tightly coupled Unscented Kalman Filter (UKF) framework fuses uncertainties from sensor data and estimated human motion based on individual body shape.

Result: Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and improving pose accuracy over state-of-the-art methods.

Conclusion: UMotion addresses the challenges faced by sparse wearable IMUs through its innovative use of UWB sensors and UKF, leading to more accurate and stable 3D human shape and pose estimation.

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


### [1036] [VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality](https://arxiv.org/abs/2505.10144)
*Xuechang Tu,Lukas Radl,Michael Steiner,Markus Steinberger,Bernhard Kerbl,Fernando de la Torre*

Main category: cs.GR

TL;DR: 3DGS在VR中存在时间伪影、投影失真和帧率降低等问题。本文提出VRSplat方法，结合并扩展了3DGS的最新进展，通过高效的注视点光栅化器和优化步骤解决了这些问题，实现了72+ FPS的同时消除了伪影和干扰漂浮物。


<details>
  <summary>Details</summary>
Motivation: 3DGS技术在新视图合成方面表现出色，但在虚拟现实中存在时间伪影、投影失真和帧率降低等问题，这些问题在头戴式显示器中被放大。

Method: 结合并扩展了Mini-Splatting、StopThePop和Optimal Projection等技术，修改了个别技术和核心3DGS光栅化器；提出了一个高效的注视点光栅化器，可以一次性处理焦点和外围区域，避免冗余计算；还包含一个微调步骤，根据StopThePop深度评估和Optimal Projection优化高斯参数。

Result: 通过25名参与者的对照用户研究验证了该方法，显示出对其他Mini-Splatting配置的强烈偏好。VRSplat是第一个系统评估的3DGS方法，能够支持现代VR应用，达到72+ FPS，同时消除弹出和立体破坏漂浮物。

Conclusion: VRSplat是首个经过系统评估且能够支持现代VR应用的3DGS方法，解决了VR中的关键挑战，显著提升了用户体验。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

</details>


### [1037] [Style Customization of Text-to-Vector Generation with Image Diffusion Priors](https://arxiv.org/abs/2505.10558)
*Peiying Zhang,Nanxuan Zhao,Jing Liao*

Main category: cs.GR

TL;DR: This paper proposes a two-stage style customization pipeline for SVG generation that leverages both feed-forward T2V models and T2I image priors, ensuring structural regularity while enabling diverse and high-quality custom styles based on text prompts.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-vector (T2V) methods for creating SVGs from text prompts often lack the ability to perform style customization, which is crucial for practical applications requiring consistent visual appearance and coherent aesthetics in vector graphics collections.

Method: The method involves a novel two-stage pipeline. In stage one, a T2V diffusion model with path-level representation is trained to ensure structural regularity of SVGs. In stage two, this model is customized to different styles through knowledge distillation from customized T2I models.

Result: Through extensive experiments, the proposed pipeline has been shown to generate high-quality and diverse SVGs in custom styles efficiently using text prompts.

Conclusion: The authors conclude that their two-stage style customization pipeline successfully addresses the challenges faced by existing T2V methods, providing an effective solution for generating SVGs with both structural regularity and customizable styles.

Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.

</details>


### [1038] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

TL;DR: 本文提出了一种基于神经隐函数的方法，用于实现解剖感知、精确的肺段重建，并开发了一个名为Lung3D的数据集，以评估重建效果。


<details>
  <summary>Details</summary>
Motivation: 高质量的肺段3D重建在肺段切除术和肺癌手术治疗计划中起着至关重要的作用。传统基于深度学习的方法由于目标重建的分辨率要求，常常面临计算资源限制或粒度有限的问题。相反，隐式建模因其计算效率和任何分辨率的连续表示而受到青睐。

Method: 我们提出了一种基于神经隐函数的方法，通过变形可学习的模板来实现解剖感知、精确的肺段重建。

Result: 我们展示了所提出的方法优于现有方法，并开发了一个名为Lung3D的形状数据集，包括800个标记的肺段3D模型及其相应的气道、动脉、静脉和段间静脉。

Conclusion: 我们提出的这种方法优于现有方法，为肺段重建提供了新的视角。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [1039] [IntrinsicEdit: Precise generative image manipulation in intrinsic space](https://arxiv.org/abs/2505.08889)
*Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Miloš Hašan,Jae Shin Yoon,Thomas Leimkühler,Christian Theobalt,Iliyan Georgiev*

Main category: cs.GR

TL;DR: 本文提出了一种新的生成工作流，可以在内在图像潜在空间中进行精确的像素级编辑，无需额外的数据收集或模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的接口缺乏精确控制，相关方法通常专注于单一编辑任务。我们需要一种更灵活、精确的图像编辑方法。

Method: 我们引入了一种通用的生成工作流，该工作流在内在图像潜在空间中运行，能够对各种编辑操作进行语义和局部的像素级操作。我们基于RGB-X扩散框架解决了身份保留和内在通道纠缠的关键挑战，并通过精确的扩散反演和解缠通道操作实现了精确高效的编辑。

Result: 我们展示了在复杂图像上的多种任务中的最先进的性能，包括颜色和纹理调整、对象插入和删除、全局重新照明及其组合。

Conclusion: 我们展示了在复杂图像上多种任务的最先进性能，包括颜色和纹理调整、对象插入和删除、全局重新照明及其组合。

Abstract: Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.

</details>


### [1040] [Neural BRDF Importance Sampling by Reparameterization](https://arxiv.org/abs/2505.08998)
*Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: 本文提出了一种基于重新参数化的神经BRDF重要性采样方法，该方法能够无缝集成到标准渲染流程中，并在保持高推理速度的同时实现最佳的方差减少。


<details>
  <summary>Details</summary>
Motivation: 神经双向反射分布函数（BRDF）作为增强物理基础渲染真实感的流行材料表示形式，但其重要性采样仍然是一个重大挑战。

Method: 我们引入了一种基于重新参数化的神经BRDF重要性采样公式，该公式可以无缝集成到标准渲染管道中，并精确生成BRDF样本。

Result: 我们的方差和性能分析表明，与现有基线相比，我们的重新参数化方法在神经BRDF渲染中实现了最佳的方差减少。

Conclusion: 我们的重新参数化方法在神经BRDF渲染中实现了最佳的方差减少，同时保持了高推理速度。

Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged
as popular material representations for enhancing realism in physically-based
rendering. Yet their importance sampling remains a significant challenge. In
this paper, we introduce a reparameterization-based formulation of neural BRDF
importance sampling that seamlessly integrates into the standard rendering
pipeline with precise generation of BRDF samples. The reparameterization-based
formulation transfers the distribution learning task to a problem of
identifying BRDF integral substitutions. In contrast to previous methods that
rely on invertible networks and multi-step inference to reconstruct BRDF
distributions, our model removes these constraints, which offers greater
flexibility and efficiency. Our variance and performance analysis demonstrates
that our reparameterization method achieves the best variance reduction in
neural BRDF renderings while maintaining high inference speeds compared to
existing baselines.

</details>


### [1041] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

TL;DR: UMotion是一种基于不确定性的在线融合框架，用于3D人体形状和姿态估计，结合了UWB传感器和IMUs，通过UKF优化估计，提高了姿态准确性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏可穿戴惯性测量单元（IMUs）在估计3D人体运动时面临的姿态歧义、数据漂移和适应性有限的问题。

Method: 提出了一种基于不确定性的在线融合所有状态估计框架UMotion，结合了六个集成的体穿戴超宽带（UWB）距离传感器和IMUs。开发了一个紧密耦合的无迹卡尔曼滤波器（UKF）框架，通过实时对齐IMU和UWB测量与不确定的人体运动约束来优化估计。

Result: 实验表明，UMotion在稳定传感器数据和提高姿态准确性方面有效，并优于现有技术。

Conclusion: UMotion在稳定传感器数据和提高姿态准确性方面表现出色，优于现有技术。

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


### [1042] [VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality](https://arxiv.org/abs/2505.10144)
*Xuechang Tu,Lukas Radl,Michael Steiner,Markus Steinberger,Bernhard Kerbl,Fernando de la Torre*

Main category: cs.GR

TL;DR: VRSplat是一种改进的3DGS方法，旨在解决VR中的时间伪影、投影失真和低帧率问题，通过结合多种技术并优化光栅化器，实现了高效且高质量的VR体验。


<details>
  <summary>Details</summary>
Motivation: 3DGS在虚拟现实（VR）中面临关键挑战，包括时间伪影、基于投影的失真以及在渲染大量高斯函数时帧率降低的问题。这些问题是由于大视场角、持续的头部运动和高分辨率头戴显示器（HMDs）而被放大。

Method: VRSplat结合并扩展了3DGS的最新进展，以全面解决VR中的挑战。它通过修改单个技术及核心3DGS光栅化器，展示了Mini-Splatting、StopThePop和Optimal Projection等想法如何相互补充。此外，还提出了一种高效的中央视网膜光栅化器，能够在一次GPU调用中处理焦点和周边区域，避免冗余计算并提高GPU利用率。

Result: 通过一个有25名参与者的受控用户研究验证了VRSplat方法，结果显示VRSplat在Mini-Splatting的其他配置上具有明显优势。VRSplat能够消除弹跳和立体视觉干扰的浮点物，并实现72+ FPS的帧率。

Conclusion: VRSplat是第一个经过系统评估的3DGS方法，能够支持现代VR应用，实现了72+ FPS的帧率，同时消除了弹跳和立体视觉干扰的浮点物。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

</details>


### [1043] [Style Customization of Text-to-Vector Generation with Image Diffusion Priors](https://arxiv.org/abs/2505.10558)
*Peiying Zhang,Nanxuan Zhao,Jing Liao*

Main category: cs.GR

TL;DR: 本文提出了一种新的两阶段风格定制管道，用于生成具有结构规律性和多样化表达能力的自定义风格SVG。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到向量（T2V）生成方法在创建SVG时往往忽略了实际应用中的一个重要需求：风格定制，这对于生成具有统一视觉外观和连贯美学的一组矢量图形至关重要。

Method: 我们提出了一种新的两阶段风格定制管道，利用前馈T2V模型和T2I图像先验的优势。第一阶段，我们训练了一个具有路径级表示的T2V扩散模型，以确保SVG的结构规律性并保留多样的表达能力。第二阶段，我们通过蒸馏定制的T2I模型来定制T2V扩散模型以适应不同风格。

Result: 我们的方法在广泛的实验中得到了验证，能够基于文本提示以高效的前馈方式生成高质量且多样化的自定义风格SVG。

Conclusion: 我们的方法通过结合前馈T2V模型和T2I图像先验，有效地解决了风格定制中的挑战，能够在高效前馈方式下生成高质量且多样化的自定义风格SVG。

Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1044] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

TL;DR: The paper argues that instead of comparing LLMs to human experts, research should focus on developing strategies for humans to work efficiently with LLMs in clinical settings to ensure patient safety amidst rapid LLM advancements.


<details>
  <summary>Details</summary>
Motivation: There is a growing concern about the potential harm LLMs could cause to patient care if not properly safeguarded. The current trend of comparing LLMs to human experts is seen as counterproductive.

Method: The authors advocate for shifting research focus from comparison to collaboration between humans and LLMs in clinical settings.

Result: Demonstrates the need for strategies that enable efficient, almost symbiotic work of humans with LLMs rather than simply comparing their capabilities.

Conclusion: Future research efforts must prioritize characterizing the safe use of LLMs in clinical settings and fostering human-LLM collaboration.

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [1045] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

TL;DR: Recent advances in generative AI have transformed information access, but the digital divide persists in many developing regions. To address this, WaLLM - a custom AI chatbot on WhatsApp - was developed. It has features beyond answering queries and has been operational for over 6 months with significant user engagement. Analysis of user interactions shows that most queries seek factual information, especially on 'Health and well-being'. User activity is concentrated around the daily top question and those accessing the 'Leaderboard' feature interact more. The paper concludes with implications for customization, UI design, and trust calibration.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in AI access in developing regions due to the persistent digital divide by creating an accessible AI chatbot on a widely used platform.

Method: Development of WaLLM, a custom AI chatbot on WhatsApp, with features such as answering queries, daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Logs were analyzed systematically to understand user interactions.

Result: WaLLM amassed over 14.7K queries from approximately 100 users over 6 months. 55% of queries sought factual information, with 'Health and well-being' being the most popular topic (28%). Two-thirds of user activity occurred within 24 hours of the daily top question, and users who accessed the 'Leaderboard' interacted 3x as much.

Conclusion: The findings suggest implications for culture-based customization, user interface design, and appropriate calibration of users' trust in AI systems in developing regions.

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [1046] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

TL;DR: This study analyzes reflections from 33 student teams using AI tools in an HCI design course, identifying new types of design judgments.


<details>
  <summary>Details</summary>
Motivation: To understand the kinds of judgments students make when engaging with AI tools as collaborators rather than just aids.

Method: Analyzed reflections from 33 student teams in an HCI design course who used AI tools.

Result: Identified established and emergent types of design judgment including agency-distribution and reliability judgment.

Conclusion: Generative AI adds complexity to design reasoning, prompting reflection on AI's outputs and when to rely on it; a conceptual lens is provided for understanding co-creative sensemaking with AI.

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [1047] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

TL;DR: PreCare is a website with AI-driven assistants designed to guide users through Advance Care Planning (ACP). It significantly improved personal value exploration, knowledge and decisional confidence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap of online ACP which lacks personalized value exploration and immediate clarification of decision consequences.

Method: Conducted two formative studies and designed PreCare in collaboration with ACP professionals. PreCare has three AI-driven assistants to help users explore personal values, gain ACP knowledge, and support informed decision-making.

Result: A usability study showed that PreCare achieved an excellent System Usability Scale (SUS) rating. A comparative evaluation showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.

Conclusion: PreCare successfully bridges the gap between online ACP and clinical consultations.

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [1048] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

TL;DR: This paper explores 'default images' in text-to-image generation using Midjourney, providing insights into their impact and suggesting ways to improve the technology.


<details>
  <summary>Details</summary>
Motivation: To understand why and how default images are generated in text-to-image models, especially when prompts contain unknown terms, to enhance TTI solutions and prompt engineering.

Method: A systematic approach was developed to create input prompts that trigger default images on Midjourney. Initial experiments, small-scale ablation studies, and a survey were conducted to investigate their characteristics and effects on user satisfaction.

Result: The study reveals insights into default images, including how they affect user satisfaction, and identifies challenges for improving TTI models.

Conclusion: Investigating default images is crucial for advancing text-to-image generation technology, offering a foundation for future research and improvements.

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [1049] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

TL;DR: The study explores the impact of generative AI on engineering students' learning experience in China, highlighting opportunities and challenges.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI affects engineering students' learning experiences and to identify the opportunities and challenges it presents in higher education.

Method: Surveyed 148 students from diverse engineering disciplines across China regarding their use of generative AI, its impact on learning, and associated challenges.

Result: More than half of the participants reported positive effects on learning efficiency, initiative, and creativity, with nearly half also noting improved independent thinking. However, many felt academic performance was largely unchanged and expressed concerns about AI accuracy and reliability.

Conclusion: Generative AI offers significant benefits to engineering students but also poses challenges. Recommendations are provided for integrating AI effectively into engineering education.

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


### [1050] [Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents](https://arxiv.org/abs/2505.09757)
*Botao Amber Hu,Yuhan Liu,Helena Rong*

Main category: cs.HC

TL;DR: The study explores the challenges and governance issues in Decentralized AI Agents (DeAgents) that combine AI with blockchain technologies, interviewing stakeholders to understand motivations, benefits, and dilemmas for guiding future design and discussions.


<details>
  <summary>Details</summary>
Motivation: To address the empirical research gap concerning the paradoxical tension between trustlessness and unreliable autonomy in DeAgents, which combines LLM-based AI agents with decentralization technologies.

Method: Through interviews with DeAgents stakeholders including experts, founders, and developers to examine their motivations, perceived benefits, and governance challenges.

Result: The findings will provide insights to guide the future design of DeAgents systems and protocols, as well as inform discussions on governance within sociotechnical AI systems.

Conclusion: This research aims to navigate the complex landscape of decentralized AI agents by offering guidance for system design and fostering discussions on governance mechanisms.

Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents)
combines Large Language Model (LLM)-based AI agents with decentralization
technologies such as blockchain smart contracts and trusted execution
environments (TEEs). These tamper-resistant trustless substrates allow agents
to achieve self-sovereignty through ownership of cryptowallet private keys and
control of digital assets and social media accounts. DeAgent eliminates
centralized control and reduces human intervention, addressing key trust
concerns inherent in centralized AI systems. However, given ongoing challenges
in LLM reliability such as hallucinations, this creates paradoxical tension
between trustlessness and unreliable autonomy. This study addresses this
empirical research gap through interviews with DeAgents stakeholders-experts,
founders, and developers-to examine their motivations, benefits, and governance
dilemmas. The findings will guide future DeAgents system and protocol design
and inform discussions about governance in sociotechnical AI systems in the
future agentic web.

</details>


### [1051] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)
*Ruichen Yang,György M. Lévay,Christopher L. Hunt,Dániel Czeiner,Megan C. Hodgson,Damini Agarwal,Rahul R. Kaliki,Nitish V. Thakor*

Main category: cs.HC

TL;DR: The paper introduces the Reviewer, a 3D visual interface for EMG signal translation that enhances myoelectric prosthesis control through structured feedback and mutual adaptation. Study results show improved performance in completion rates, path efficiency, and throughput compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge users face in producing distinct EMG patterns for reliable classification as prosthesis movement complexity increases, and to improve upon existing training methods that rely on heuristic, trial-and-error adjustments.

Method: A 10-session study with 12 able-bodied participants comparing PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task.

Result: Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group.

Conclusion: The 3D visual feedback provided by the Reviewer significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments.

Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.

</details>


### [1052] [SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition](https://arxiv.org/abs/2505.10312)
*Anh Tuan Ha,Hoang Khang Phan,Thai Minh Tien Ngo,Anh Phan Truong,Nhat Tan Le*

Main category: cs.HC

TL;DR: In Human Activity Recognition (HAR), generating high-quality data and handling data heterogeneity are challenges. This study uses deep learning methods (Attention Autoencoder and conditional GANs) to create a generation dataset, and shuffles data sequences to homogenize distribution. Experiments show this random sequence strategy improves classification performance with accuracy up to 0.70 ± 0.03 and macro F1 score of 0.64 ± 0.01. Disrupting temporal dependencies forces the model to focus on instant recognition, improving robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of obtaining high-quality and diverse data in HAR, as well as dealing with data heterogeneity.

Method: Used Attention Autoencoder and conditional Generative Adversarial Networks to generate a dataset, and applied a random sequence strategy to shuffle data for homogenizing distribution.

Result: Classification performance significantly improved with accuracy up to 0.70 ± 0.03 and macro F1 score of 0.64 ± 0.01.

Conclusion: This approach not only broadens the effective training dataset but also enhances HAR systems' robustness in complex real-world scenarios.

Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.

</details>


### [1053] [AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages](https://arxiv.org/abs/2505.10300)
*Muzhe Wu,Yanzhi Zhao,Shuyi Han,Michael Xieyang Liu,Hong Shen*

Main category: cs.HC

TL;DR: To address the challenge of transferring technical design rationales for ethical evaluation, the authors developed AI LEGO, a web-based prototype that facilitates knowledge handoff and helps identify harmful design choices in early AI development stages. It uses interactive blocks and persona simulations, leading to more effective harm identification compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the persistent knowledge handoff challenge in cross-functional industry teams working on Responsible AI (RAI). This involves transferring high-level technical design rationales from technical experts to non-technical roles for ethical evaluation and harm identification early in the AI development lifecycle.

Method: The method involved literature review and a co-design study with 8 practitioners to understand the current challenges. Based on insights, they developed AI LEGO, a web-based prototype. Technical roles use interactive blocks to draft development plans, while non-technical roles engage through checklists and LLM-driven persona simulations to identify potential harms.

Result: In a study with 18 cross-functional practitioners, AI LEGO increased the volume and likelihood of harms identified compared to baseline worksheets. Participants found its modular structure and persona prompts made harm identification more accessible and fostered clearer RAI practices.

Conclusion: AI LEGO effectively supports cross-functional AI practitioners in facilitating knowledge handoff and identifying harmful design choices early in the development process, leading to more collaborative RAI practices.

Abstract: Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.

</details>


### [1054] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

TL;DR: 本文强调了在临床环境中，应关注人类与LLM的协作而非比较，以确保安全使用。


<details>
  <summary>Details</summary>
Motivation: 当前的研究努力主要集中在将LLM与一组人类专家进行比较，但'专家'这一概念往往不明确或不断变化。如果没有适当的保障措施，LLM可能会威胁到安全交付患者护理的既定结构。

Method: 本文提出了一种策略，旨在促进人类与LLM在临床环境中的高效协作，而不是将LLM与人类进行比较。

Result: 本文展示了需要开发策略，使人类与LLM能够以几乎共生的方式高效工作。

Conclusion: 研究应专注于在临床环境中有效表征LLM的安全使用，这种使用方式能适应新型LLM模型的快速发展。

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [1055] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [1056] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

TL;DR: 本研究分析了33个学生团队在人机交互设计课程中的反思，以探讨学生使用AI工具时所做的判断类型。研究发现，除了传统的设计判断外，还出现了代理分配判断和可靠性判断。这些新形式反映了学生如何与AI协商创造性责任并评估其输出的可信度。研究结果表明，生成式AI在设计推理中引入了新的复杂性，促使学生不仅反思AI的产出，还反思何时以及如何依赖它。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具融入设计工作流程，学生越来越多地将这些工具视为合作者而非仅仅是辅助工具。因此，需要理解学生在与AI合作时所做出的判断类型。

Method: 本研究分析了33个学生团队在人机交互设计课程中的反思，以探讨学生使用AI工具时所做的判断类型。

Result: 研究发现了既有的设计判断形式（例如，工具性、欣赏性和质量判断）以及新兴类型：代理分配判断和可靠性判断。这些新形式捕捉了学生如何与AI协商创造性责任并评估其输出的可信度。

Conclusion: 我们的研究结果表明，生成式AI在设计推理中引入了新的复杂性，促使学生不仅反思AI的产出，还反思何时以及如何依赖它。通过突出这些判断，我们提供了一个概念性视角，以理解学生在设计环境中与AI进行协作创造意义的过程。

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [1057] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

TL;DR: This paper presents PreCare, an online platform with AI assistants that helps individuals plan for end-of-life care by exploring personal values, gaining knowledge, and making informed decisions. It shows excellent usability and was preferred by most participants.


<details>
  <summary>Details</summary>
Motivation: Online ACP lacks key benefits of clinical consultations, including personalized value exploration and immediate clarification of decision consequences.

Method: We conducted two formative studies: 1) shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and social workers (18 patients total), and 2) interviewed 14 users of ACP websites. Building on these insights, we designed PreCare in collaboration with 6 ACP professionals.

Result: A usability study (n=12) showed that PreCare achieved a System Usability Scale (SUS) rating of excellent. A comparative evaluation (n=12) showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.

Conclusion: PreCare is a website with 3 AI-driven assistants designed to guide users through exploring personal values, gaining ACP knowledge, and supporting informed decision-making. It achieved excellent usability and was preferred by most participants.

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [1058] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

TL;DR: 本文研究了文本到图像生成模型中的默认图像问题，并提出了系统性的方法来创建触发这些图像的输入提示。通过实验和调查，我们分析了默认图像对用户满意度的影响，并为未来的研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 我们认为研究默认图像对于设计更好的TTI解决方案和提示工程是有价值的。

Method: 我们描述了系统性方法来创建触发默认图像的输入提示，并展示了初步实验和几个小规模消融研究的结果。我们还报告了一项调查研究，探讨了默认图像如何影响用户满意度。

Result: 我们提供了对Midjourney中默认图像的首次调查，并展示了我们的初始实验结果和小规模消融研究。

Conclusion: 我们的工作为理解TTI中的默认图像奠定了基础，并突出了挑战和未来的研究方向。

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [1059] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

TL;DR: 本研究探讨了生成式AI在工程教育中的使用情况及其对学习体验的影响，发现其在提高学习效率和创造力方面有积极作用，但也存在准确性、可靠性和学术表现未提升的问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，其在高等教育中的潜在应用引起了广泛关注。本研究旨在了解生成式AI在工程教育中的使用情况及其影响。

Method: 本研究调查了来自中国不同工程学科和地区的148名学生如何使用生成式AI，并探讨了其对学习体验的影响以及在工程教育中的机遇与挑战。

Result: 超过一半的参与者报告称生成式AI对他们的学习效率、主动性和创造力有积极影响，近一半的人认为它增强了他们的独立思考能力。然而，尽管承认学习效率提高，许多人认为他们的实际学术成绩基本未变，并对生成式AI的准确性和领域特定可靠性表示担忧。

Conclusion: 研究发现，生成式AI对工程教育带来了积极影响，但也存在准确性、领域特定可靠性和学术表现未明显提升等问题。研究提供了将生成式AI有效整合到工程教育中的建议。

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


### [1060] [Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents](https://arxiv.org/abs/2505.09757)
*Botao Amber Hu,Yuhan Liu,Helena Rong*

Main category: cs.HC

TL;DR: 本文研究了去中心化AI代理（DeAgent）的动机、好处和治理困境，以解决信任和不可靠自主性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决DeAgent在信任和不可靠自主性之间的矛盾，以及在去中心化AI系统中存在的关键信任问题。

Method: 本研究通过访谈DeAgent的利益相关者（专家、创始人和开发者）来探讨他们的动机、好处和治理困境。

Result: 研究结果揭示了DeAgent利益相关者的动机、好处和治理困境，为未来DeAgent系统和协议设计提供了指导。

Conclusion: 本研究通过访谈DeAgent的利益相关者，揭示了他们的动机、好处和治理困境，为未来DeAgent系统和协议设计提供了指导，并为未来社会技术AI系统的治理讨论提供了信息。

Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents)
combines Large Language Model (LLM)-based AI agents with decentralization
technologies such as blockchain smart contracts and trusted execution
environments (TEEs). These tamper-resistant trustless substrates allow agents
to achieve self-sovereignty through ownership of cryptowallet private keys and
control of digital assets and social media accounts. DeAgent eliminates
centralized control and reduces human intervention, addressing key trust
concerns inherent in centralized AI systems. However, given ongoing challenges
in LLM reliability such as hallucinations, this creates paradoxical tension
between trustlessness and unreliable autonomy. This study addresses this
empirical research gap through interviews with DeAgents stakeholders-experts,
founders, and developers-to examine their motivations, benefits, and governance
dilemmas. The findings will guide future DeAgents system and protocol design
and inform discussions about governance in sociotechnical AI systems in the
future agentic web.

</details>


### [1061] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)
*Ruichen Yang,György M. Lévay,Christopher L. Hunt,Dániel Czeiner,Megan C. Hodgson,Damini Agarwal,Rahul R. Kaliki,Nitish V. Thakor*

Main category: cs.HC

TL;DR: 本文介绍了一种名为Reviewer的3D可视化界面，用于实时提供PR算法行为的直观见解，从而减少认知负荷并促进用户生成的EMG模式和解码器边界之间的相互数据驱动适应。


<details>
  <summary>Details</summary>
Motivation: 现有的训练通常涉及静态解码器边界的手动、试错用户调整，随着假肢运动复杂性的增加，用户往往难以产生足够不同的EMG模式以进行可靠分类。

Method: 一项10次会话的研究，涉及12名身体健全的参与者，比较了基于运动的训练和使用Reviewer更新与传统虚拟手臂可视化后的PR性能。

Result: 使用Reviewer训练的参与者在完成率、减少超调以及提高路径效率和吞吐量方面优于标准可视化组。

Conclusion: 3D视觉反馈通过结构化训练显著提高了新手操作员的PR控制，使反馈驱动的适应成为可能，并减少了对大量启发式调整的依赖。

Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.

</details>


### [1062] [SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition](https://arxiv.org/abs/2505.10312)
*Anh Tuan Ha,Hoang Khang Phan,Thai Minh Tien Ngo,Anh Phan Truong,Nhat Tan Le*

Main category: cs.HC

TL;DR: 本研究通过深度学习方法生成数据集，并采用随机序列策略来提高HAR系统的分类性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在人类活动识别（HAR）领域，获取高质量和多样化的数据仍然是一项持续的挑战，因为成本高昂且真实世界活动具有内在的变异性。此外，数据异质性是一个关键挑战。

Method: 通过深度学习方法（注意力自编码器和条件生成对抗网络）生成数据集，并采用随机序列策略来打乱时间依赖性，以提高模型的鲁棒性。

Result: 实验结果表明，随机序列策略显著提高了分类性能，达到了0.70±0.03的准确率和0.64±0.01的宏F1分数。

Conclusion: 该方法不仅扩大了有效的训练数据集，还为在复杂现实场景中增强HAR系统提供了有希望的途径。

Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.

</details>


### [1063] [AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages](https://arxiv.org/abs/2505.10300)
*Muzhe Wu,Yanzhi Zhao,Shuyi Han,Michael Xieyang Liu,Hong Shen*

Main category: cs.HC

TL;DR: 本文提出AI LEGO，一个基于网络的原型，用于支持跨职能AI从业者有效促进知识交接并识别早期设计阶段的有害设计选择。


<details>
  <summary>Details</summary>
Motivation: 在跨职能行业团队中，由于难以将高层次、早期阶段的技术设计理由从技术专家转移到非技术或用户面向的角色，以进行伦理评估和危害识别，因此这项工作经常受阻。

Method: 通过文献综述和与8名从业者的共同设计研究，我们解构了这一挑战的表现形式，并开发了AI LEGO，这是一个基于网络的原型，支持跨职能AI从业者有效促进知识交接并识别早期设计阶段的有害设计选择。

Result: 在与18名跨职能从业者的研究中，AI LEGO 比基线工作表增加了危害识别的数量和可能性。参与者发现其模块化结构和角色提示使危害识别更加容易，促进了早期设计中的更清晰和协作的RAI实践。

Conclusion: AI LEGO 通过模块化结构和角色提示，使危害识别更加容易，促进了早期设计中的更清晰和协作的RAI实践。

Abstract: Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1064] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

TL;DR: 提出了一种基于最小均方误差（MMSE）估计的对抗性评估框架，用于敏感特征推断，并通过理论下界和实证评估展示了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个有效的评估框架来衡量从其他相关特征的噪声观测中推断敏感特征的准确性，并理解有限样本和受限假设类对推断的影响。

Method: 提出了基于MMSE估计的对抗性评估框架，该框架包括理论下界的推导、闭式界表达以及在不同特征关系下的逼近误差分析。还引入了一个基于验证数据集的新下界。

Result: 推导出的理论下界能够准确反映敏感特征推断的性能限制，并且通过实证评估证明了框架的有效性与实用性。

Conclusion: 提出的框架为敏感特征推断提供了平衡理论保证与实际效率的有效工具，适用于线性预测模型及多种特征关系。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [1065] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

TL;DR: This paper examines risk bounds for nonparametric distributional regression estimators, establishing upper bounds for CRPS and MSE under convex constraints, and deriving a general bound for non-convex constraints with neural network-based estimators. Experiments validate the theoretical contributions.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of the performance guarantees (risk bounds) for nonparametric distributional regression estimators, particularly focusing on CRPS and MSE metrics under different types of constraints.

Method: The authors develop general upper bounds for the CRPS and worst-case MSE in the context of convex-constrained distributional regression. They also explore isotonic and trend filtering distributional regression, and derive a general upper bound for non-convex constrained distributional regression, including neural network-based methods.

Result: Established upper bounds match convergence rates seen in mean estimation problems. Theoretical results were validated through comprehensive experiments on both simulated and real data, showing practical effectiveness.

Conclusion: The study successfully provides risk bounds for various forms of distributional regression, extending to non-convex settings such as neural networks, and demonstrates their practical relevance through empirical validation.

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [1066] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

TL;DR: 研究了具有符号激活函数的前馈神经网络的在线学习，探讨了不同条件下的错误界限以及如何通过增加网络限制来减少对维度d的依赖。


<details>
  <summary>Details</summary>
Motivation: 研究具有符号激活函数的前馈神经网络的在线学习能力，特别是分析其在分类任务中的错误界限及影响因素。

Method: 1. 定义并分析了一个边距条件，该条件对于在线学习神经网络是充分且在某些情况下是必要的。
2. 证明了任何网络的最优错误界限大约为$(d,\gamma)$-完全可分离包装数$\mathtt{TS}(d,\gamma)$。
3. 构造了一个网络实例，展示任何学习算法都会犯$\mathtt{TS}(d,\gamma)$次错误。
4. 提出了两个额外的网络限制：多指标模型和扩展边距假设，以减少对维度d的依赖，并分别给出了相应的错误界限。

Result: 1. 在一般情况下，错误界限与维度d呈指数关系。
2. 在多指标模型中，错误界限与输入方向数量k有关，而与维度d无关。
3. 在扩展边距假设下，错误界限与网络深度L和标签数量Y有关，减少了对维度d的依赖。

Conclusion: 通过引入额外的自然限制（如多指标模型和扩展边距假设），可以有效减少错误界限对输入维度d的依赖，从而改善高维数据上的在线学习性能。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [1067] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

TL;DR: Optimal Transport (OT) is used for domain adaptation in supervised settings with linear regression models, especially when domains differ by rotations. The paper proposes an algorithm combining K-means, OT, and SVD to estimate the rotation angle and adapt the model, demonstrating effectiveness in sparsely sampled target domains.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is crucial when source and target domains have differing statistical properties, such as rotations. Current methods lack a specific approach for adapting linear regression models under rotational shifts.

Method: The method involves using Optimal Transport theory to recover the underlying rotation between domains in $\mathbb{R}^2$ with p-norm cost ($p \ge 2$). An algorithm is proposed that combines K-means clustering, OT, and SVD to estimate the rotation angle and adapt the regression model.

Result: The proposed algorithm effectively estimates the rotation angle and adapts the regression model, particularly excelling in scenarios where the target domain is sparsely sampled. This leverages the abundant source data to improve generalization.

Conclusion: This work provides both theoretical understanding of OT-based model adaptation under geometric transformations and practical solutions for domain adaptation in supervised settings involving rotations.

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [1068] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

TL;DR: This paper explores the classification of functional data with fairness constraints, proposing a unified framework and a post-processing algorithm called Fair-FLDA. It provides theoretical guarantees on fairness and excess risk controls, supported by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically study fair classification in functional data while controlling disparity levels below a pre-specified threshold, addressing challenges like absence of density ratios and intractability of posterior probabilities.

Method: A unified framework for fairness-aware functional classification is proposed. A post-processing algorithm named Fair-FLDA is designed for homoscedastic Gaussian processes using group-wise thresholding.

Result: Theoretical guarantees on fairness and excess risk controls are established under weak structural assumptions. The results also cover the excess risk control of standard FLDA as a special case.

Conclusion: The findings are supported by extensive numerical experiments on both synthetic and real datasets, demonstrating the practicality of the Fair-FLDA algorithm.

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [1069] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

TL;DR: Offline reinforcement learning using pre-collected data faces challenges when dealing with heterogeneous data. This paper proposes an individualized offline policy optimization framework for heterogeneous MDPs, introducing a model with individual latent variables and the P4L algorithm to efficiently estimate individual Q-functions and guarantee a fast rate on average regret.


<details>
  <summary>Details</summary>
Motivation: Traditional methods in offline RL focus on learning optimal policies from homogeneous data, which may lead to suboptimal results for heterogeneous populations.

Method: The authors propose an individualized offline policy optimization framework for heterogeneous time-stationary MDPs that includes a heterogeneous model with individual latent variables and the Penalized Pessimistic Personalized Policy Learning (P4L) algorithm.

Result: Simulation studies and real data application show superior numerical performance of the proposed method compared to existing methods under weak partial coverage assumption on behavior policies.

Conclusion: The proposed individualized offline policy optimization framework effectively addresses the challenge of learning from heterogeneous data in offline RL.

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [1070] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

TL;DR: The paper introduces Deep-SITAR, a supervised deep learning framework combining autoencoder architecture with the SITAR model to predict growth trajectories.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing methods for capturing the complexity and nonlinearity of human growth by integrating deep learning techniques with traditional SITAR models.

Method: Development of the Deep-SITAR model which uses an autoencoder architecture that includes a deep neural network and B-spline model. The encoder estimates random effects for individuals, while the decoder fits data using B-splines.

Result: Deep-SITAR allows for predicting random effects of new individuals without re-estimating the full model, providing a flexible and efficient approach.

Conclusion: Deep-SITAR is a powerful method for predicting growth trajectories, merging deep learning flexibility with the interpretability of mixed-effects models.

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [1071] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

TL;DR: This paper introduces AWNN, an adaptively weighted nearest neighbor method for matrix completion. It balances bias and variance without needing cross-validation, providing theoretical guarantees and synthetic experiment support.


<details>
  <summary>Details</summary>
Motivation: Existing nearest neighbor methods lack a systematic approach to choosing radii and weights without using cross-validation.

Method: AWNN method which judiciously balances the bias variance trade off in weighted nearest-neighbor regression.

Result: Theoretical guarantees for AWNN are provided under minimal assumptions, supported by synthetic experiments.

Conclusion: AWNN is an effective method for matrix completion that addresses the challenge of choosing radii and weights systematically.

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


### [1072] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha,Dhruv Vansraj Rathore,Soumadeep Saha,Utpal Garain,David Doermann*

Main category: stat.ML

TL;DR: The paper explores intrinsic causal contributions (ICC) in neural networks using a generative post-hoc framework, showing ICC provides more intuitive and reliable explanations than existing methods.


<details>
  <summary>Details</summary>
Motivation: Quantifying the causal influence of input features within neural networks has become a topic of increasing interest.

Method: Treat NNs as structural causal models (SCMs) and extend focus to include intrinsic causal contributions (ICC), proposing an identifiable generative post-hoc framework for quantifying ICC and drawing a relationship between ICC and Sobol' indices.

Result: Experiments on synthetic and real-world datasets demonstrate that ICC generates more intuitive and reliable explanations compared to existing global explanation techniques.

Conclusion: ICC offers a new perspective in understanding causal influences within neural networks.

Abstract: Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [1073] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: The paper proposes a method for estimating differences in two multi-attribute Gaussian graphical models (GGMs) using a penalized D-trace loss function with non-convex penalties, providing theoretical analysis and numerical examples.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-attribute differential graph estimation rely on group lasso penalized loss functions, but this paper aims to improve upon that by introducing a penalized D-trace loss function with non-convex penalties.

Method: The method involves using a penalized D-trace loss function with non-convex penalties such as log-sum and SCAD. Two proximal gradient descent methods are used to optimize the objective function.

Result: Theoretical analysis is provided that establishes sufficient conditions for consistency in support recovery, convexity, and estimation in high-dimensional settings. The approaches are demonstrated through numerical examples based on synthetic and real data.

Conclusion: This approach offers an alternative to existing methods for multi-attribute differential graph estimation, potentially providing better performance due to the use of non-convex penalties.

Abstract: We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [1074] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski,Michael Ivanitskiy,Nathan Lenssen,Douglas Nychka,Daniel McKenzie*

Main category: stat.ML

TL;DR: The paper explores the use of image-to-image (I2I) networks for parameter estimation in spatially autoregressive (SAR) models, offering faster and more accurate results compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: In many applications, acquiring a large ensemble of data instances is expensive or impractical. Statistical emulation using SAR models can generate synthetic fields, but parameter inference via MLE is computationally prohibitive for large, non-stationary fields.

Method: By recognizing that SAR parameters can be arranged on a regular grid, both inputs (spatial fields) and outputs (model parameters) are treated as images. This allows the application of image-to-image (I2I) networks for parameter estimation.

Result: I2I networks enable faster and more accurate parameter estimation for non-stationary SAR models with high complexity.

Conclusion: Image-to-image networks provide an efficient alternative for parameter estimation in complex SAR models, outperforming traditional methods in terms of speed and accuracy.

Abstract: In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [1075] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka,Matias Quiroz,Vali Asimit,Samuel Muller*

Main category: stat.ML

TL;DR: The paper proposes a gradient-based approach for sparse portfolio selection that transforms the combinatorial problem into a continuous optimization task, providing a fast and scalable solution with negligible error compared to commercial solvers.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of current methods for sparse portfolio selection which rely on mixed-integer quadratic programming and become slow as the number of assets and constraints increase.

Method: A gradient-based approach is introduced, converting the sparse selection problem into a constrained continuous optimization task through Boolean relaxation. A tunable parameter is used to transition the objective function from convex to concave, enabling a stable starting point and progressing towards a sparse binary solution.

Result: The method matches commercial solvers in asset selection for most instances, and when it differs, the solution varies by only a few assets with negligible error in portfolio variance.

Conclusion: The proposed gradient-based method offers a fast, scalable alternative to commercial solvers for sparse portfolio selection with comparable accuracy.

Abstract: Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [1076] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl,Leon Klein*

Main category: stat.ML

TL;DR: Boltzmann Generators利用Normalizing Flows和重要性加权从分子系统的平衡分布生成样本。Flow Matching加速了Continuous Normalizing Flows（CNFs），使其能够扩展到更复杂的分子系统，并最小化流积分轨迹的长度。本文研究了在已知目标能量的情况下，使用路径梯度微调通过Flow Matching初步训练的CNFs的好处。实验表明，这种混合方法在不增加模型复杂性和计算预算的情况下，将分子系统的采样效率提高了三倍。此外，通过测量微调期间流轨迹的长度，证明路径梯度在很大程度上保留了流的学习结构。


<details>
  <summary>Details</summary>
Motivation: 尽管Boltzmann Generators已经可以通过Normalizing Flows和重要性加权生成分子系统的平衡分布样本，但为了进一步提高采样效率，需要探索新的优化方法。Flow Matching虽然加速了CNFs并扩展到更复杂的分子系统，但在已知目标能量的情况下，如何进一步优化这些初步训练的CNFs仍需研究。

Method: 使用路径梯度对通过Flow Matching初步训练的Continuous Normalizing Flows（CNFs）进行微调。这种方法结合了Flow Matching和路径梯度的优势，在不需要额外采样的情况下，优化CNFs以提高采样效率。同时，通过测量微调过程中流轨迹的长度，评估路径梯度对流学习结构的保留程度。

Result: 实验结果表明，与仅使用Flow Matching相比，该混合方法将分子系统的采样效率提高了三倍。此外，路径梯度在微调过程中能够很大程度上保留流的学习结构，这说明该方法具有良好的稳定性和有效性。

Conclusion: 使用路径梯度微调由Flow Matching初步训练的CNFs是一种有效的混合方法，可以在不增加模型复杂性和计算预算的情况下显著提高分子系统的采样效率。这种方法为分子模拟和其他相关领域提供了潜在的应用价值。

Abstract: Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [1077] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: stat.ML

TL;DR: 本文提出了一种新的单阶段Top-k学习框架，统一了预测和延迟决策，通过共享的基于得分的模型选择每个输入的k个最具成本效益的实体（标签或专家）。该方法在单一端到端目标下联合优化预测和延迟决策，并定义了一个与k无关的成本敏感损失函数及其凸替代函数。此外，还引入了一个自适应变体Top-k(x)，动态选择每个输入咨询的实体数量以平衡预测准确性和咨询成本。实验表明，该方法优于传统的Top-1延迟策略，而Top-k(x)通过根据输入复杂性调整分配实现了更好的准确性-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的单阶段学习到延迟（L2D）方法仅限于将决策延迟给单一专家，缺乏灵活性和泛化能力，无法有效处理多实体情况下的预测和延迟决策问题。因此，需要一种能够同时优化预测和延迟并适用于多个实体的新方法。

Method: 1. 提出了一种单阶段Top-k Learning-to-Defer框架，使用共享的基于得分的模型选择k个最具成本效益的实体（标签或专家）。2. 定义了一个成本敏感损失函数，并推导出一个不依赖于k的新型凸替代函数，从而实现跨不同Top-k场景的泛化而无需重新训练。3. 引入了自适应变体Top-k(x)，动态调整每个输入咨询的实体数量，以平衡预测准确性和咨询成本。

Result: 1. 实验结果表明，单阶段Top-k方法在CIFAR-10和SVHN数据集上严格优于Top-1延迟策略。2. Top-k(x)通过根据输入复杂性调整分配，在准确性-成本权衡方面表现出色。

Conclusion: 本文提出的单阶段Top-k Learning-to-Defer框架及其自适应变体Top-k(x)为多实体情况下的预测和延迟决策提供了一种灵活且高效的解决方案，显著提升了性能并在准确性与成本之间实现了更好的权衡。

Abstract: We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [1078] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato,Harvinder Lehal,Simon Maskell,Lee Devlin,Malcolm Strens*

Main category: stat.ML

TL;DR: The paper investigates sampling algorithms using subset evaluations to reduce computational overhead in Bayesian inference with MCMC for irregular and expensive likelihood functions. An improved version of HINTS with adaptive proposals and a data-driven proxy shows the best performance in a fixed computational budget.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when dealing with irregular and costly likelihood functions.

Method: Adapt subset samplers without gradient information, introduce data-driven proxies instead of Taylor expansions, and define a novel computation-cost aware adaptive controller. Use hierarchical delayed acceptance for efficient exact sampling.

Result: Improved HINTS obtains the best sampling error within a fixed computational budget. Subset evaluations provide cost-effective exploration and data-driven proxies successfully pre-screen proposals.

Conclusion: Subset evaluations can offer cheap and tempered exploration while data-driven proxies can effectively pre-screen proposals leading to efficient exact sampling through hierarchical delayed acceptance.

Abstract: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [1079] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin,Shixiao Liang,Christopher Tunnell*

Main category: stat.ML

TL;DR: FlowVAT is a new method for normalizing flow variational inference that tempers both base and target distributions simultaneously, overcoming mode-seeking behavior and posterior collapse in multi-modal and high-dimensional problems. It outperforms traditional methods in experiments with multi-modal distributions and moves toward fully-automatic black-box variational inference.


<details>
  <summary>Details</summary>
Motivation: Variational inference faces challenges with multi-modal and high-dimensional posteriors, leading to mode-seeking behavior and posterior collapse. Traditional annealing methods require temperature schedules and hyperparameter tuning, which hinders the goal of truly black-box variational inference.

Method: FlowVAT uses a conditional tempering approach where both the base and target distributions are tempered simultaneously while maintaining affine-invariance. The normalizing flow is conditioned on temperature, allowing it to generalize across a range of temperatures using overparameterized neural networks. This single flow represents the posterior across different temperatures.

Result: In experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT finds more modes and achieves better ELBO values compared to traditional and adaptive annealing methods, particularly excelling in higher dimensions where existing approaches fail.

Conclusion: FlowVAT advances the field towards fully-automatic black-box variational inference for complicated posteriors by requiring minimal hyperparameter tuning and no annealing schedule.

Abstract: Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [1080] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

Main category: stat.ML

TL;DR: The paper introduces BaNk-UCB, a nonparametric algorithm combining k-NN regression with UCB principle for batched contextual bandits, achieving near-optimal regret and outperforming baselines in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of sequential decision-making in contexts like medicine and marketing where online feedback is limited, especially when actions are taken over a finite horizon divided into a small number of batches.

Method: Proposes BaNk-UCB, which uses adaptive k-nearest neighbor regression combined with the upper confidence bound principle. The method leverages local geometry for reward estimation and adaptively balances exploration and exploitation.

Result: Provides near-optimal regret guarantees under standard assumptions and demonstrates superior performance compared to binning-based methods through empirical evaluations on both synthetic and real-world datasets.

Conclusion: BaNk-UCB is a fully nonparametric algorithm that adapts to context dimensions, achieves minimax-optimal rates, and performs well across different datasets.

Abstract: We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


### [1081] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

TL;DR: 本文提出了一种基于MMSE的对抗评估框架，用于敏感特征推断，能够提供理论下界并兼顾实际效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在敏感特征推断中缺乏理论保障与实际效率之间的平衡，因此需要一种新的评估框架。

Method: 我们提出了一个基于最小均方误差（MMSE）估计的对抗评估框架，通过理论下界分析敏感特征推断的准确性，并结合有限样本和线性预测模型进行推导。

Result: 我们推导了线性预测模型下的闭式边界，并通过实验验证了框架的有效性，证明其能够有效评估敏感特征推断。

Conclusion: 我们的框架为基于最小均方误差（MMSE）估计的敏感特征推断提供了一种有效的对抗评估工具，能够在理论保证和实际效率之间取得平衡。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [1082] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

TL;DR: 本文研究了非参数分布回归估计器的风险界限，并通过实验验证了理论结果的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究非参数分布回归估计器的风险界限，并验证其在实际数据中的有效性。

Method: 本文研究了非参数分布回归估计器的风险界限，建立了凸约束分布回归的CRPS和最坏情况MSE的上界，并将其应用于等距和趋势过滤分布回归。此外，还为非凸约束下的分布回归建立了上界，并应用于基于神经网络的估计器。

Result: 本文建立了凸约束和非凸约束下分布回归的上界，并通过实验验证了这些理论结果的有效性。

Conclusion: 本文通过实验验证了理论贡献，并展示了其实际效果。

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [1083] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

TL;DR: 本文研究了具有符号激活函数的前馈神经网络在在线学习中的可学习性，通过边界条件和包装数分析，得出不同限制条件下的错误界限。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在在线学习中的可学习性，特别是如何通过添加额外限制来减少维度依赖性。

Method: 本文通过分析神经网络的边界条件和包装数，结合多指标模型和扩展边界假设，推导出不同的错误界限。

Result: 本文证明了神经网络的最优错误界限与(d, γ)-完全可分离包装数有关，并在多指标模型和扩展边界假设下得到了不同的错误界限。

Conclusion: 本文研究了具有符号激活函数的前馈神经网络在在线学习中的可学习性。通过引入一个足够且在某些情况下必要的边界条件，证明了最优错误界限与(d, γ)-完全可分离包装数有关。此外，还讨论了两种限制条件：多指标模型和扩展边界假设，分别给出了相应的错误界限。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [1084] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [1085] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

TL;DR: 本文研究了在公平性约束下的功能数据分类问题，提出了一种统一的公平感知功能分类框架，并设计了一个后处理算法Fair-FLDA，实现了公平性控制。理论分析和实验验证表明该方法在公平性和风险控制方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 算法公平性已成为机器学习的核心话题，减轻不同子群体之间的差异已成为快速增长的研究领域。本文旨在研究在公平性约束下的功能数据分类问题，确保分类器的差异水平低于预设阈值。

Method: 本文提出了一种统一的公平感知功能分类框架，解决了无限维功能空间中的关键挑战，并设计了一个后处理算法Fair-FLDA，通过组内阈值调整实现公平性。

Result: 本文提出的Fair-FLDA算法在同方差高斯过程中实现了公平性，并在弱结构假设下建立了公平性和额外风险控制的理论保证。实验结果表明该方法在合成和真实数据集上具有实用性。

Conclusion: 本文提出了一个统一的公平感知功能分类框架，并设计了一个后处理算法Fair-FLDA，实现了公平性控制。理论分析和实验验证表明该方法在公平性和风险控制方面具有优势。

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [1086] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

TL;DR: 本文提出了一种针对异构时间平稳马尔可夫决策过程的个性化离线策略优化框架，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理异构数据时可能无法为异构人群提供最优策略，因此需要一种能够处理异构数据的个性化离线策略优化框架。

Method: 本文提出了一个带有个体潜在变量的异构模型，以有效估计个体Q函数，并引入了惩罚悲观个性化策略学习（P4L）算法，以在行为策略的弱部分覆盖假设下保证平均遗憾的快速率。

Result: 模拟研究和实际数据应用表明，所提出的方法在数值性能上优于现有方法。

Conclusion: 本文提出了一种针对异构时间平稳马尔可夫决策过程（MDPs）的个性化离线策略优化框架，并通过模拟研究和实际数据应用验证了该方法的优越性。

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [1087] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

TL;DR: This paper introduces Deep-SITAR, a deep learning framework that combines the flexibility of deep learning with the interpretability of traditional mixed-effects models to predict growth trajectories efficiently.


<details>
  <summary>Details</summary>
Motivation: Several approaches have been developed to capture the complexity and nonlinearity of human growth. The SITAR model is widely used but has limitations in predicting random effects for new individuals without full model re-estimation.

Method: We introduce a supervised deep learning framework based on an autoencoder architecture that integrates a deep neural network with a B-spline model to estimate the SITAR model. The encoder estimates the random effects for each individual, while the decoder performs a fitting based on B-splines similar to the classic SITAR model.

Result: Deep-SITAR enables the prediction of the random effects of new individuals entering a population without requiring a full model re-estimation, offering a powerful approach to predicting growth trajectories.

Conclusion: Deep-SITAR offers a powerful approach to predicting growth trajectories, combining the flexibility and efficiency of deep learning with the interpretability of traditional mixed-effects models.

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [1088] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

TL;DR: 本文提出了AWNN，一种自适应加权最近邻方法，用于矩阵补全，通过平衡偏差-方差权衡来系统地选择半径和权重。


<details>
  <summary>Details</summary>
Motivation: 传统最近邻方法在选择半径和权重时缺乏系统性方法，通常依赖于交叉验证，这限制了其应用。AWNN旨在解决这一挑战。

Method: AWNN是一种自适应加权最近邻方法，用于执行矩阵补全。它通过平衡加权最近邻回归中的偏差-方差权衡来解决传统方法中参数选择的问题。

Result: AWNN在理论上有保证，并通过合成实验得到了支持。

Conclusion: AWNN方法通过平衡加权最近邻回归中的偏差-方差权衡，提供了一种系统选择半径和权重的方法，从而解决了传统最近邻方法在选择这些参数时依赖交叉验证的问题。

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


### [1089] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha,Dhruv Vansraj Rathore,Soumadeep Saha,Utpal Garain,David Doermann*

Main category: stat.ML

TL;DR: 本文将神经网络视为结构因果模型，并提出了一种可识别的生成后框架来量化内在因果贡献（ICC）。实验表明，ICC生成了比现有全局解释技术更直观和可靠的解释。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络中输入特征的因果影响变得越来越重要，现有的方法通常评估直接、间接和总因果效应。

Method: 将神经网络视为结构因果模型（SCMs），并提出了一种可识别的生成后框架来量化内在因果贡献（ICC）。

Result: 在合成和真实世界数据集上的实验表明，ICC比现有全局解释技术生成了更直观和可靠的解释。

Conclusion: ICC生成了比现有全局解释技术更直观和可靠的解释。

Abstract: Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [1090] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 本文提出了一种基于非凸惩罚的D-trace损失函数方法，用于估计两个相似结构的多属性高斯图模型之间的差异，并通过理论分析和数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多属性差分图估计方法基于组Lasso惩罚损失函数，但本文旨在通过非凸惩罚提高估计的准确性。

Method: 本文提出了基于非凸惩罚（log-sum和SCAD）的D-trace损失函数，并采用两种近端梯度下降方法优化目标函数。

Result: 本文提供了支持恢复、凸性和高维设置下的估计的一致性条件，并通过合成和真实数据的数值例子验证了方法的有效性。

Conclusion: 本文提出了使用非凸惩罚的D-trace损失函数来估计两个具有相似结构的多属性高斯图模型之间的差异，并通过理论分析和数值实验验证了方法的有效性。

Abstract: We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [1091] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski,Michael Ivanitskiy,Nathan Lenssen,Douglas Nychka,Daniel McKenzie*

Main category: stat.ML

TL;DR: 本文提出了一种基于图像到图像网络的空间自回归模型参数估计方法，该方法在非平稳SAR模型上表现出更快和更准确的性能。


<details>
  <summary>Details</summary>
Motivation: 由于最大似然估计(MLE)在计算上是不可行的，特别是对于大型、非平稳的场，因此需要一种更高效的方法来估计SAR模型的参数。

Method: 本文利用空间自回归(SAR)模型的参数可以排列在规则网格上的特性，将输入（空间场）和输出（模型参数）视为图像，并使用图像到图像(I2I)网络进行参数估计。

Result: 实验结果表明，使用图像到图像网络进行参数估计在非平稳SAR模型上具有更高的速度和准确性。

Conclusion: 本文提出了一种利用图像到图像网络进行空间自回归模型参数估计的新方法，该方法在非平稳SAR模型上表现出更快和更准确的性能。

Abstract: In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [1092] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka,Matias Quiroz,Vali Asimit,Samuel Muller*

Main category: stat.ML

TL;DR: 本文提出了一种快速且可扩展的基于梯度的方法，用于稀疏投资组合选择，通过布尔松弛将组合稀疏选择问题转化为受约束的连续优化任务，同时保持与原问题的等价性。


<details>
  <summary>Details</summary>
Motivation: 传统的混合整数二次规划方法在计算成本上随着k和p的增加而呈指数级增长，使得它们对于中等规模的问题来说太慢了。

Method: 我们提出了一种快速且可扩展的基于梯度的方法，通过布尔松弛将组合稀疏选择问题转化为受约束的连续优化任务，同时在二进制点集上保持与原问题的等价性。该算法采用了一个可调参数，将辅助目标从凸函数转换为凹函数。

Result: 我们的方法在大多数情况下与商业求解器在资产选择上相匹配，并且在极少数情况下，解决方案仅相差几个资产，同时显示出可忽略的组合方差误差。

Conclusion: 我们的方法在大多数情况下与商业求解器在资产选择上相匹配，并且在极少数情况下，解决方案仅相差几个资产，同时显示出可忽略的组合方差误差。

Abstract: Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [1093] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl,Leon Klein*

Main category: stat.ML

TL;DR: 本文研究了使用路径梯度对通过流匹配初始训练的连续归一化流（CNF）进行微调的好处。实验结果表明，这种混合方法在分子系统中将采样效率提高了三倍，同时使用相同的模型、相似的计算预算，并且不需要额外的采样。


<details>
  <summary>Details</summary>
Motivation: Boltzmann生成器作为一种有前途的机器学习工具，用于生成分子系统的平衡分布样本。最近，流匹配帮助加快了连续归一化流（CNF），将其扩展到更复杂的分子系统，并减少了流积分轨迹的长度。

Method: 我们研究了在已知目标能量的情况下，使用路径梯度对通过流匹配初始训练的CNFs进行微调的好处。

Result: 我们的实验表明，这种混合方法在分子系统中将采样效率提高了三倍，同时使用相同的模型、相似的计算预算，并且不需要额外的采样。此外，通过在微调过程中测量流轨迹的长度，我们证明路径梯度在很大程度上保留了流的学习结构。

Conclusion: 我们的实验表明，这种混合方法在分子系统中将采样效率提高了三倍，同时使用相同的模型、相似的计算预算，并且不需要额外的采样。此外，通过在微调过程中测量流轨迹的长度，我们证明路径梯度在很大程度上保留了流的学习结构。

Abstract: Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [1094] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: stat.ML

TL;DR: 本文提出了一种新的单阶段Top-k Learning-to-Defer框架，能够统一预测和defer，并通过成本敏感的损失和凸面代理实现跨Top-k制度的泛化。此外，还引入了自适应变体Top-k(x)，以更好地平衡准确率和成本。实验结果表明，该方法在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶段L2D方法仅限于向单一专家defer，我们的方法通过单一端到端目标联合优化多个实体的预测和defer。

Method: 我们引入了第一个单阶段Top-k Learning-to-Defer框架，通过学习一个基于得分的模型来统一预测和defer，该模型为每个输入选择k个最经济有效的实体-标签或专家。我们定义了一个成本敏感的损失，并推导出一个新颖的凸面代理，独立于基数参数k，使泛化跨Top-k制度无需重新训练。我们的公式恢复了先前基于得分的方法的Top-1 defer策略作为特例，并证明了我们的代理在温和假设下是Bayes一致和H一致的。我们进一步引入了一个自适应变体Top-k(x)，它动态地为每个输入选择咨询的实体数量以平衡预测准确性和咨询成本。

Result: 实验表明，我们的单阶段Top-k方法在CIFAR-10和SVHN上严格优于Top-1 defer，而Top-k(x)通过根据输入复杂性定制分配实现了更好的准确率-成本权衡。

Conclusion: 我们的方法在CIFAR-10和SVHN数据集上严格优于Top-1 deferral，而Top-k(x)通过根据输入复杂性定制分配实现了更好的准确率-成本权衡。

Abstract: We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [1095] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato,Harvinder Lehal,Simon Maskell,Lee Devlin,Malcolm Strens*

Main category: stat.ML

TL;DR: 本文研究了在似然函数不规则且计算成本高的情况下，如何通过子集评估和数据驱动的代理来优化贝叶斯推断中的采样算法。


<details>
  <summary>Details</summary>
Motivation: 当似然函数不规则且计算成本高时，使用马尔可夫链蒙特卡洛（MCMC）进行贝叶斯推断具有挑战性。我们探索了几种利用子集评估来减少计算开销的采样算法。

Method: 我们适应了子集采样器，以在没有梯度信息或梯度信息不可靠的情况下进行采样。我们引入了数据驱动的代理代替泰勒展开，并定义了一个新颖的计算成本意识自适应控制器。

Result: 我们在一个具有挑战性的疾病建模任务和一个具有类似不规则似然表面的可配置任务上进行了广泛的评估。我们发现改进的分层重要性嵌套训练样本（HINTS）版本，在固定计算预算下获得了最佳的采样误差。

Conclusion: 子集评估可以提供廉价且自然调节的探索，而数据驱动的代理可以在状态空间的探索区域成功地预筛选提议。这两个元素通过分层延迟接受结合，实现了高效的精确采样。

Abstract: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [1096] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin,Shixiao Liang,Christopher Tunnell*

Main category: stat.ML

TL;DR: FlowVAT 是一种用于正则化流变分推断的条件温度方法，能够在多模态和高维后验分布中找到更多模式并获得更好的 ELBO 值，而无需温度调度和最小的超参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统温度方法需要温度调度和超参数调整，无法实现真正的黑盒变分推断。多模态和高维后验分布对变分推断提出了重大挑战，导致模式搜索行为和崩溃。

Method: FlowVAT 是一种条件温度方法，用于正则化流变分推断。它同时对基础分布和目标分布进行温度调节，并通过将正则化流条件化为温度，利用过参数化神经网络的泛化能力来训练一个代表一系列温度的流。

Result: 在 2、10 和 20 维多模态分布的实验中，FlowVAT 表现出色，比传统和自适应温度方法更好地找到模式并获得更好的 ELBO 值，特别是在高维情况下。

Conclusion: FlowVAT 方法在多模态和高维后验分布中表现出色，能够找到更多模式并获得更好的 ELBO 值，特别是在高维情况下。该方法不需要温度调度和最小的超参数调整，推动了全自动黑盒变分推断的发展。

Abstract: Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [1097] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

Main category: stat.ML

TL;DR: 研究提出了一种非参数算法BaNk-UCB，结合自适应k近邻回归和上置信界原则，用于批处理非参数上下文老虎机问题。该方法在合成和现实数据集上表现出色，优于基于分箱的基线。


<details>
  <summary>Details</summary>
Motivation: The study is motivated by constraints in domains such as medicine and marketing, where online feedback is limited, requiring a nonparametric algorithm that adapts to context dimension and balances exploration and exploitation.

Method: BaNk-UCB combines adaptive k-nearest neighbor (k-NN) regression with the upper confidence bound (UCB) principle to address sequential decision-making in batched nonparametric contextual bandits.

Result: BaNk-UCB provides near-optimal regret guarantees under standard Lipschitz smoothness and margin assumptions, achieving minimax-optimal rates with a theoretically motivated batch schedule.

Conclusion: BaNk-UCB consistently outperforms binning-based baselines in empirical evaluations on synthetic and real-world datasets.

Abstract: We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1098] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

TL;DR: 提出了一种名为RICA的鲁棒独立成分分析方法，通过最小化多变量随机变量之间的鲁棒依赖性度量（距离相关性dCor）来估计成分。为了增强鲁棒性，引入了碗变换（bowl transform）。RICA具有强一致性、参数收敛速度，并在模拟研究中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 大多数ICA方法对离群值不鲁棒，需要一种能够有效处理离群值的ICA方法。

Method: 使用距离相关性（dCor）作为依赖性度量，并通过碗变换（bowl transform）增强鲁棒性，然后通过寻找与剩余部分具有最小dCor的成分来顺序估计独立源。

Result: RICA在模拟研究中表现出色，通常优于竞争对手，并成功应用于三个实际问题，包括鸡尾酒会问题。

Conclusion: RICA是一种鲁棒的ICA方法，具有强一致性和参数收敛速度，适用于存在离群值的数据集。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


### [1099] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

TL;DR: 本文提出了一种名为RICA的鲁棒ICA方法，通过最小化距离相关性来估计独立源，并使用碗变换提高鲁棒性。RICA在模拟和实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大多数ICA方法对异常值不鲁棒，因此需要一种更鲁棒的ICA方法。

Method: RICA通过最小化多变量随机变量之间的鲁棒依赖度量（距离相关性）来估计成分，并使用了新的转换方法（碗变换）以提高鲁棒性。

Result: RICA在模拟研究中通常优于其竞争对手，并在三个应用中得到了验证，包括著名的鸡尾酒会问题。

Conclusion: RICA是一种强大的ICA方法，具有良好的鲁棒性，并在模拟研究和实际应用中表现出色。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1100] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

TL;DR: The paper introduces ELIS, a serving system for LLMs with an ISRTF scheduler that cuts average job completion time by up to 19.6%.


<details>
  <summary>Details</summary>
Motivation: Current LLM serving systems use first-come-first-served scheduling, causing the 'head-of-line blocking' problem.

Method: ELIS uses a trained response length predictor and an ISRTF scheduling strategy to efficiently manage inference tasks.

Result: ELIS reduces the average job completion time by up to 19.6% in experimental results.

Conclusion: ELIS is a cloud-native scheduler system on Kubernetes that overcomes limitations of current LLM serving systems.

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [1101] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

TL;DR: DeepSeek-V3通过硬件感知模型协同设计解决了当前硬件架构中的关键限制，如内存容量、计算效率和互连带宽。其创新包括多头潜在注意力（MLA）、专家混合（MoE）架构、FP8混合精度训练和多平面网络拓扑。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展揭示了当前硬件架构的关键限制，需要一种新的方法来实现成本效益的训练和推理。

Method: 使用Multi-head Latent Attention提高内存效率，Mixture of Experts优化计算-通信权衡，FP8 mixed-precision training充分利用硬件能力，以及Multi-Plane Network Topology减少集群级别的网络开销。

Result: DeepSeek-V3在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型协同设计的有效性，能够实现大规模的成本效益训练和推理。

Conclusion: 硬件与模型的协同设计对于满足日益增长的人工智能工作负载需求至关重要，为下一代人工智能系统提供了实际的创新蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


### [1102] [AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron](https://arxiv.org/abs/2505.09989)
*Tella Rajashekhar Reddy,Palak,Rohan Gandhi,Anjaly Parayil,Chaojie Zhang,Mike Shepperd,Liangcheng Yu,Jayashree Mohan,Srinivasan Iyengar,Shivkumar Kalyanaraman,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: This paper proposes to bring AI workload to modular compute clusters co-located in wind farms, and introduces Heron, a cross-site software router that routes AI inferencing workload according to power generation, improving the aggregate goodput of AI compute by up to 80%.


<details>
  <summary>Details</summary>
Motivation: AI power demand is growing unprecedentedly while abundant wind power awaits grid access. This drives the motivation to explore bringing AI workloads closer to renewable energy sources.

Method: Deploying AI workloads in modular compute clusters co-located in wind farms, using a deployment right-sizing strategy. Developing Heron, a cross-site software router, to leverage complementarity of power generation across wind farms by routing AI inferencing workload around power drops.

Result: Heron improves aggregate goodput of AI compute by up to 80% compared to the state-of-the-art when tested with 1-week production traces from Azure and real variable wind power traces.

Conclusion: Bringing AI workloads to wind farms through modular compute clusters and leveraging Heron for efficient workload routing can significantly improve AI compute goodput while utilizing cheap, green power.

Abstract: AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.

</details>


### [1103] [KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems](https://arxiv.org/abs/2505.10183)
*Jieke Lin,Wanyu Wang,Longxiang Yin,Yinhe Han*

Main category: cs.DC

TL;DR: KAITIAN is a new distributed communication framework that improves resource utilization and scalability in heterogeneous AI systems.


<details>
  <summary>Details</summary>
Motivation: Embodied AI systems require diverse accelerators to meet real-time and energy-efficiency demands, but vendor-specific libraries create interoperability barriers hindering performance.

Method: KAITIAN provides a unified abstraction layer integrating vendor-optimized libraries for intra-group efficiency and general-purpose protocols for inter-group interoperability, with a load-adaptive scheduling mechanism.

Result: KAITIAN accelerates training time by up to 42% compared to baseline homogeneous systems with minimal communication overhead (2.8--4.3%) and maintained model accuracy.

Conclusion: KAITIAN enhances flexible and powerful heterogeneous computing for complex embodied AI applications.

Abstract: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.

</details>


### [1104] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

TL;DR: ELIS是一种用于大型语言模型的服务器系统，采用ISRTF调度策略，通过预测响应长度和优化调度策略，显著提高了推理任务的效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统通常采用先到先服务的调度策略，导致“队头阻塞”问题。为了克服这一限制，需要预测LLM推理时间并应用最短作业优先策略，但由于LLM的自回归特性，预测推理延迟具有挑战性。

Method: ELIS采用了一种基于BGE模型的响应长度预测器，并设计了ISRTF调度策略，以优化现有LLM迭代批处理。此外，还实现了基于Kubernetes的云原生调度系统来评估性能。

Result: 实验结果表明，ISRTF将平均作业完成时间减少了高达19.6%。

Conclusion: ELIS通过ISRTF调度策略有效减少了平均作业完成时间，证明了其在工业环境中的有效性。

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [1105] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

TL;DR: 本文分析了DeepSeek-V3模型架构及其AI基础设施，提出了硬件和模型协同设计的方法，以解决大规模语言模型训练中的硬件瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前硬件架构在内存容量、计算效率和互连带宽方面存在限制，需要通过硬件感知的模型协同设计来解决这些问题。

Method: 本文分析了DeepSeek-V3/R1模型架构及其AI基础设施，重点介绍了多头潜在注意力（MLA）、专家混合（MoE）架构、FP8混合精度训练和多平面网络拓扑等关键技术。

Result: DeepSeek-V3展示了如何通过硬件感知的模型协同设计有效应对这些挑战，实现了成本效益高的大规模训练和推理。

Conclusion: 本文强调了硬件和模型协同设计在满足AI工作负载日益增长需求中的关键作用，并为下一代AI系统提供了实用的创新蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


### [1106] [AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron](https://arxiv.org/abs/2505.09989)
*Tella Rajashekhar Reddy,Palak,Rohan Gandhi,Anjaly Parayil,Chaojie Zhang,Mike Shepperd,Liangcheng Yu,Jayashree Mohan,Srinivasan Iyengar,Shivkumar Kalyanaraman,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: 本文提出了一种将AI工作负载转移到风力发电场中的模块化计算集群的方法，并构建了一个名为Heron的软件路由器，以提高AI计算的效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着AI算力需求的快速增长和新兴的推理工作负载，需要寻找一种经济且环保的方式来满足AI计算的需求。同时，电网接入队列中存在大量未使用的风力发电资源。

Method: 本文提出了一种部署优化策略，并构建了Heron，这是一个跨站点的软件路由器，能够有效地利用风力发电场之间的电力生成互补性。

Result: 通过使用Azure的一周编码和对话生产痕迹以及真实的可变风力发电痕迹，Heron展示了其在提高AI计算整体吞吐量方面的效果，最高可达80%。

Conclusion: 本文认为将AI工作负载带到与风力发电场共置的模块化计算集群中是经济上可行的，并展示了Heron如何通过路由AI推理工作负载来利用风力发电场之间的互补性，从而提高AI计算的整体吞吐量。

Abstract: AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.

</details>


### [1107] [KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems](https://arxiv.org/abs/2505.10183)
*Jieke Lin,Wanyu Wang,Longxiang Yin,Yinhe Han*

Main category: cs.DC

TL;DR: 本文介绍了KAITIAN，这是一种新的分布式通信框架，旨在解决异构加速器之间的互操作性问题。KAITIAN提供了一个统一的抽象层，结合了厂商优化的通信库和通用通信协议，并采用负载自适应调度机制，显著提高了分布式训练任务的资源利用率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads.

Method: KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. It incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics.

Result: KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy.

Conclusion: KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications.

Abstract: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.

</details>
