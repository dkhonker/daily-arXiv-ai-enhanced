<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 126]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.LG](#cs.LG) [Total: 142]
- [stat.ME](#stat.ME) [Total: 4]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.FL](#cs.FL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 15]
- [math.ST](#math.ST) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.HC](#cs.HC) [Total: 10]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [math.CT](#math.CT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.DS](#cs.DS) [Total: 2]
- [stat.ML](#stat.ML) [Total: 17]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: 研究使用作者验证技术量化AI在学术写作中的协助，通过三个阶段的调查，开发并评估了一种改进的特征向量差异AV方法，以支持学术诚信调查。


<details>
  <summary>Details</summary>
Motivation: 随着人-AI合作在教育环境中日益普遍，理解和衡量这种互动的程度和性质面临重大挑战。研究旨在通过作者验证技术量化AI在学术写作中的协助，以促进透明度、可解释性和学生发展。

Method: 研究分为三个阶段：数据集选择和扩展、AV方法开发和系统评估。使用三个数据集，包括公共数据集（PAN-14）和墨尔本大学学生的两个数据集，扩展数据以包括LLM生成的文本，总计1,889篇文档和540个作者问题。开发了一种改进的特征向量差异AV方法，构建学生学术写作的稳健档案，捕捉其写作的有意义的个体特征。

Result: 结果表明，改进的AV分类器能够识别文体差异，并在单词和句子级别衡量人-AI合作，同时为教育工作者提供透明的工具，以支持学术诚信调查。

Conclusion: 这项工作推进了AV技术，为AI驱动时代学术写作的动态提供了可操作的见解。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [2] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: 研究AI驱动的动态叙事在教育游戏中的潜力，发现动态叙事对学习者动机有积极影响，但需平衡教学目标与叙事动态性。


<details>
  <summary>Details</summary>
Motivation: 动机是成功学习的重要因素，以往研究证明静态互动叙事游戏对动机有积极影响，AI技术的发展使动态和自适应互动叙事越来越可行，但动态叙事对学习者动机的影响研究有限。

Method: 比较两种版本的教育互动叙事游戏《Academical》，一种是传统的手工编写分支剧情（静态叙事），另一种在游戏过程中动态排序剧情（动态叙事）。

Result: 结果强调了响应性内容和多种选择对玩家参与度的重要性，同时也展示了在平衡教学目标与叙事动态性方面的挑战。

Conclusion: 这项工作为AI驱动的动态叙事在教育游戏中的潜力提供了初步的启示。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [3] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: 重新评估了大型语言模型在理解复杂英语语句上的表现，发现人类表现被高估，模型能力被低估。在限制重读的条件下，人类准确率低于某些大型语言模型。研究结果表明，人类和模型在涉及潜在互惠行为的查询上都面临挑战，这表明存在共享的语用敏感性，而非模型特定的缺陷。此外，通过额外分析发现模型性能被系统性低估。这些发现强调了在评估大型语言模型时需要更谨慎的实验设计和编码实践，并挑战了当前模型在语言理解上天生弱于人类的假设。


<details>
  <summary>Details</summary>
Motivation: 重新评估大型语言模型在理解复杂英语语句上的表现，纠正之前研究中可能存在的对人类表现的高估和对模型能力的低估。

Method: 使用相同的刺激材料，进行预注册研究，比较人类在允许重读和限制重读两种条件下的反应，同时对比不同大型语言模型的表现。通过额外分析模型的对数概率、开放性回答的重新编码以及对其他句子的语法性评分，进一步评估模型性能。

Result: 人类在限制重读条件下的准确率（73%）低于Falcon-180B-Chat（76%）和GPT-4（81%），而新的GPT-o1模型达到完美准确率。人类和模型在涉及潜在互惠行为的查询上都面临挑战，表明存在共享的语用敏感性。通过额外分析发现模型性能被系统性低估，GPT-4o可以根据提示框架与新手或专家的语法性判断一致。

Conclusion: 需要更谨慎的实验设计和编码实践来评估大型语言模型，当前模型在语言理解上并不天生弱于人类。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [4] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: 研究探讨了GPT-4对英语信息结构和长距离依赖结构的判断能力，发现其与人类判断相似，表明自然语言和GPT-4生成英语之间存在紧密联系。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型（LM）对自然语言的理解和生成能力，特别是对英语信息结构和长距离依赖（LDD）结构的判断能力。

Method: 通过实验，让GPT-4完成与人类相同的任务，包括信息结构和接受度任务，并通过操纵信息结构来验证因果关系。

Result: GPT-4在信息结构和接受度任务上表现出可靠的元语言技能，复制了人类的交互效应。操纵信息结构可以增加后续LDD结构的接受度。

Conclusion: GPT-4生成的英语与自然英语之间存在紧密联系，信息结构和句法之间也存在紧密关系，需要进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [5] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: This research investigates the use of authorship verification (AV) techniques to quantify AI assistance in academic writing, promoting transparency and student development. The study is structured into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the challenges posed by human-AI collaboration in educational contexts, particularly the need to understand and measure such interactions effectively. The authors aim to promote transparency, interpretability, and student development through the application of AV techniques.

Method: The method involves structuring the investigation into three stages: 1) Dataset selection and expansion - using three datasets including LLM-generated texts; 2) AV method development - adapting a Feature Vector Difference methodology to capture individual writing characteristics; 3) Systematic evaluation - evaluating effectiveness across multiple scenarios including distinguishing between student-authored and LLM-generated texts.

Result: The results show that the enhanced AV classifier can successfully identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels. It also demonstrates resilience against LLMs' attempts to mimic student writing styles.

Conclusion: This work advances AV technology, providing actionable insights into the dynamics of academic writing in an AI-driven era. It offers educators a transparent tool to support academic integrity investigations.

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [6] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: The paper compares two versions of an educational game, one with a static narrative and the other with a dynamic narrative, to explore their impact on learner motivation.


<details>
  <summary>Details</summary>
Motivation: Previous research has shown that static interactive narrative games can positively affect motivation, and advances in AI have made dynamic narratives more accessible. However, there is limited exploration into how dynamic narratives impact learner motivation.

Method: The study compares two versions of Academical, an educational game about research ethics: one version uses a traditional hand-authored branching plot (static narrative), while the other dynamically sequences plots during play (dynamic narrative).

Result: Results show that responsive content and a variety of choices are important for player engagement. It also highlights the challenge of balancing pedagogical goals with the dynamic aspects of the narrative.

Conclusion: This work provides initial insights into the potential of AI-driven dynamic narratives in educational games, discussing design implications from the findings.

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [7] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: Reanalysis of language model comprehension shows that previous studies overestimated human performance and underestimated LLMs, especially in naturalistic conditions without rereading. Models like Falcon-180B-Chat and GPT-4 outperform humans in certain tasks, and newer models achieve perfect accuracy.


<details>
  <summary>Details</summary>
Motivation: To reassess the claim that large language models underperform humans in comprehending minimally complex English statements by revisiting the experimental setup and considering different conditions.

Method: Conducted a preregistered study comparing human responses in two conditions: one allowing rereading (replicating an original study) and another restricting rereading (a more naturalistic test). Evaluated LLMs' performance using the same stimuli and additional analyses such as log probabilities, recoding of open-ended responses, and grammaticality ratings.

Result: Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieved perfect accuracy. Both humans and models struggled with queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities.

Conclusion: The findings challenge the assumption that current models are inherently weaker than humans at language comprehension and highlight the need for more careful experimental design and coding practices in evaluating LLMs.

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [8] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: GPT-4展示了与人类相似的元语言技能，能够理解信息结构和可接受性任务之间的关系，并且研究发现信息结构对长距离依赖构造的可接受性有因果影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LM）是否能够捕捉到英语使用者在信息结构和长距离依赖（LDD）结构之间的关系，这种关系已经在人类中被验证。

Method: 通过让GPT-4完成与人类相同的任务，研究其在信息结构和可接受性判断上的表现；并通过操纵基础句子的信息结构来确认因果关系。

Result: GPT-4在零样本条件下表现出可靠的元语言技能，复制了信息结构和可接受性任务之间的显著交互作用；信息结构的突出程度影响后续LDD构造的可接受性评分。

Conclusion: GPT-4生成的英语与自然英语之间存在紧密联系，信息结构与句法之间也有密切关系，需要进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [9] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）常产生事实性幻觉，为解决此问题，本文提出了一种名为ACPO的自监督偏好调整方法，该方法无需外部监督即可提高事实准确性。实验结果表明，ACPO在LongFact和BioGen数据集上比FactAlign高出1.95分。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解LLMs事实性幻觉的方法通常依赖于更强的模型或外部知识库来评估事实正确性，这在实际应用中可能并不总是可行的。因此，需要一种不依赖外部监督的方法来提高LLMs的事实准确性。

Method: 提出了Atomic Consistency Preference Optimization (ACPO) 方法，这是一种自监督的偏好调整方法。ACPO利用原子一致性信号，即多个随机响应中单个事实的一致性，来识别高质量和低质量的数据对，以进行模型对齐。通过消除对外部模型或知识库的需求，ACPO提供了一种可扩展且高效的改进事实性问答的方法。

Result: 尽管是自监督方法，实证结果表明，ACPO在LongFact和BioGen数据集上比强监督对齐基线FactAlign高出1.95分。

Conclusion: ACPO是一种有效的增强事实可靠性的方法，无需依赖外部模型或知识库。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [10] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在技术交互方面有显著进步，但其输出相似性、多样性和伦理影响仍存疑问。研究通过5000个提示词和12个LLMs生成约3百万文本发现：同模型输出更相似；WizardLM-2-8x22b输出相似度高，GPT-4则更多样；不同模型写作风格差异大；LLM内容有独特语言特征；部分模型性别平衡更好、偏见减少。这些结果为LLM未来开发和伦理评估提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现出色，但对其输出相似性、多样性和伦理影响尚存疑问，因此需要深入研究以了解不同模型的表现及特性。

Method: 使用5000个涵盖多种任务的提示词，从包括OpenAI、Google、Microsoft、Meta和Mistral在内的12个LLMs（包含专有和开源系统）生成约3百万文本进行分析。

Result: 1. 同一LLM的输出比与人类文本更相似；2. WizardLM-2-8x22b输出高度相似，GPT-4输出更多样；3. 不同LLM写作风格差异大，Llama 3和Mistral相似度高，GPT-4独具特色；4. LLM词汇和语调差异体现其独特性；5. 部分LLM性别平衡更好且偏见减少。

Conclusion: 研究结果揭示了LLM输出的行为和多样性，有助于指导未来LLM的开发和伦理评估。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [11] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: This paper presents S-DAT, a scalable multilingual framework for assessing divergent thinking using language models and embeddings, showing robust scoring across 11 languages and providing a fairer evaluation tool.


<details>
  <summary>Details</summary>
Motivation: Traditional creativity assessments are labor-intensive, language-specific, and subjective, limiting scalability and cross-cultural applicability.

Method: S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance as a proxy for divergent thinking, evaluated across 11 diverse languages.

Result: S-DAT demonstrates robust and consistent scoring across linguistic contexts, shows convergent validity with other DT measures, and correct discriminant validity with convergent thinking.

Conclusion: S-DAT addresses limitations of earlier approaches by offering cross-linguistic flexibility for inclusive, global-scale creativity research.

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [12] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

TL;DR: Recent advancements in large language models (LLMs) have shown excellent Chinese text processing capabilities, especially in Chinese Spelling Correction (CSC). However, there are still challenges in reliability and generalization. This paper proposes CEC-Zero, a new reinforcement learning (RL) framework that allows LLMs to self-correct without external supervision. Experiments show that RL-enhanced LLMs achieve high accuracy and good cross-domain generalization, providing a scalable solution for optimizing reliability in Chinese NLP applications.


<details>
  <summary>Details</summary>
Motivation: Although LLMs outperform traditional models in accuracy and robustness in CSC tasks, issues with reliability and generalization remain unresolved.

Method: Propose CEC-Zero, a reinforcement learning framework that enables LLMs to self-correct by learning error strategies autonomously without external supervision.

Result: Experiments demonstrate that RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, reducing reliance on annotated data or auxiliary models.

Conclusion: CEC-Zero offers a scalable solution to optimize reliability in Chinese NLP applications and establishes a new paradigm for self-improving language models.

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [13] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

TL;DR: This paper introduces a new UML modeling tool which integrates class diagrams and object diagrams, executes objects, and provides a stimulating learning experience for students. It stems from an international research project focused on multi-level architecture.


<details>
  <summary>Details</summary>
Motivation: To create an advanced UML modeling tool that goes beyond conventional tools by integrating class and object diagrams, executing objects, and providing a valuable teaching aid.

Method: Design, implement, and utilize a new UML modeling tool emerging from an international research project aimed at a comprehensive multi-level architecture.

Result: The new tool allows integration of class and object diagrams as well as execution of objects, leading to new software architectures and an engaging educational tool.

Conclusion: This project exemplifies how research can yield valuable results as side effects of other work.

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [14] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

TL;DR: Effectively analyzing online review data is essential across industries. This study proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection, demonstrating its effectiveness through various experiments.


<details>
  <summary>Details</summary>
Motivation: Existing studies on analyzing online review data are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets.

Method: The method involves applying automatic labeling to Korean and English review datasets spanning various domains. Aspect category candidates are extracted through clustering, and each review is represented as an aspect-aware embedding vector using negative sampling.

Result: The models achieve high performance in multi-aspect labeling, showing that the automatically generated labels are suitable for training. The framework exhibits superior consistency and scalability compared to publicly available large language models. Human evaluation confirms the quality of the automatic labels.

Conclusion: This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments.

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [15] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: The paper introduces InForage, a reinforcement learning framework that improves LLMs' information retrieval through dynamic and adaptive search behaviors.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static retrieval strategies in complex tasks involving ambiguous, multi-step, or evolving information needs.

Method: Proposes InForage, inspired by Information Foraging Theory (IFT), which formalizes retrieval-augmented reasoning as a dynamic information-seeking process. It rewards intermediate retrieval quality to encourage iterative gathering and integration of information.

Result: InForage outperforms baseline methods in general question answering, multi-hop reasoning tasks, and a new real-time web QA dataset.

Conclusion: InForage is effective in building robust, adaptive, and efficient reasoning agents for LLMs.

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [16] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

TL;DR: This paper observes a novel phenomenon called contextual entrainment in language models, where LMs are distracted by irrelevant contextual information. The authors hypothesize the existence of 'entrainment heads' within the model's attention mechanism and use differentiable masking to identify these heads. Turning off these heads reduces the effect of contextual entrainment.


<details>
  <summary>Details</summary>
Motivation: To understand why and how language models become distracted by irrelevant contextual information, introducing the concept of contextual entrainment.

Method: Observing the logits assigned by LMs to tokens that previously appeared in context prompts, using counterfactual vs factual prompts, and employing differentiable masking to discover 'entrainment heads'.

Result: Statistically significant evidence shows that contextual entrainment is influenced by semantic factors, and turning off identified 'entrainment heads' significantly reduces the effect of contextual entrainment.

Conclusion: The discovery of contextual entrainment and the investigation into LM distraction via entrainment heads marks a key step towards mechanistic analysis and mitigation of the distraction problem.

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [17] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

TL;DR: Qwen3 is the latest version in the Qwen model family, featuring advanced performance, efficiency, and multilingual capabilities. It integrates thinking and non-thinking modes for dynamic responses, introduces a thinking budget mechanism, and expands multilingual support from 29 to 119 languages. Qwen3 achieves state-of-the-art results across various benchmarks and is publicly accessible under Apache 2.0.


<details>
  <summary>Details</summary>
Motivation: To advance the capabilities of large language models in terms of performance, efficiency, and multilingual support while reducing the need for switching between different models for various tasks.

Method: Development of Qwen3 series with dense and MoE architectures, integration of thinking and non-thinking modes, introduction of a thinking budget mechanism, and expansion of multilingual support from 29 to 119 languages.

Result: Qwen3 achieves state-of-the-art results in diverse benchmarks including code generation, mathematical reasoning, and agent tasks, competitive against larger MoE models and proprietary models.

Conclusion: Qwen3 successfully advances the state of the art in large language models, enhancing global accessibility through improved cross-lingual understanding and generation capabilities, and is publicly accessible under Apache 2.0.

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [18] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: QEDACVC is a quantum computing solution for multilingual machine translation, achieving 82% accuracy on the OPUS dataset.


<details>
  <summary>Details</summary>
Motivation: Current cloud-based multilingual translation services use large multilingual language models with classical computing as the backend. There is potential to explore quantum computing for this task.

Method: QEDACVC introduces a quantum encoder-decoder architecture that uses quantum convolution, quantum pooling, quantum variational circuit, and quantum attention mechanisms. It is designed to run on quantum computing hardware.

Result: QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.

Conclusion: QEDACVC demonstrates the potential of quantum computing in multilingual machine translation.

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [19] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: Parameter-efficient fine-tuning (PEFT) methods for adapting large language models have shown promise, but existing approaches exhibit counter-intuitive phenomena. This paper proposes PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. It achieves state-of-the-art performance in both question answering and mathematical problem solving tasks while using 25% fewer parameters than LoRA.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods show counter-intuitive phenomena: integrating router into prompt tuning increases training efficiency but does not universally improve performance; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of prompt tuning, the authors propose PT-MoE.

Method: PT-MoE integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. Matrix decomposition enables efficient parameter sharing across experts, while MoE provides dynamic adaptation.

Result: PT-MoE achieves state-of-the-art performance in both question answering and mathematical problem solving tasks. It improves F1 score by 1.49 points over prompt tuning and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over prompt tuning and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA.

Conclusion: The integration of matrix decomposition and MoE in PT-MoE yields complementary benefits, enabling cross-task consistency and generalization abilities. These findings provide insights for future PEFT methods.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [20] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

TL;DR: WorldView-Bench is a new benchmark designed to evaluate Global Cultural Inclusivity in LLMs. It uses free-form generative evaluation and two intervention strategies to significantly increase cultural inclusivity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarking frameworks fail to adequately capture the bias in LLMs towards Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality.

Method: The method involves introducing WorldView-Bench, a benchmark for evaluating Global Cultural Inclusivity in LLMs, using free-form generative evaluation instead of rigid, closed-form assessments. Two intervention strategies are implemented: Contextually-Implemented Multiplex LLMs and Multi-Agent System (MAS)-Implemented Multiplex LLMs.

Result: Results show a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance.

Conclusion: These findings indicate the potential of multiplex-aware AI evaluation in reducing cultural bias in LLMs, promoting more inclusive and ethically aligned AI systems.

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [21] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: The paper explores an alternative method for language modeling by focusing on context embedding, using Graph Convolution in GNNs with LSTMs to predict the next word with limited resources.


<details>
  <summary>Details</summary>
Motivation: To address the high resource demands of current state-of-the-art language models, the study focuses on a sub-task of language modeling - context embedding.

Method: The approach uses Graph Convolution operation in GNNs to encode context and combines it with LSTMs to predict the next word based on preceding words' local context.

Result: The method was tested on a custom Wikipedia text corpus with limited resources and showed promising results in predicting the next word.

Conclusion: This novel approach provides a feasible alternative for language modeling tasks with significantly fewer resources.

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [22] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Xuanzhao Dong,Hao Wang,Haiyu Wu,Huayu Li,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi*

Main category: cs.CL

TL;DR: An abstract about a new method called Diversity-aware Reward Adjustment (DRA) which incorporates semantic diversity into reward computation for reinforcement learning in language model post-training. It outperforms recent baselines achieving state-of-the-art performance with an average accuracy of 58.2%.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of Group Relative Policy Optimization (GRPO) that fails to capture semantic diversity among sampled completions leading to a diversity-quality inconsistency.

Method: The proposed method, Diversity-aware Reward Adjustment (DRA), uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning while maintaining stable exploitation of high-quality samples.

Result: Evaluated on five mathematical reasoning benchmarks, DRA outperforms recent strong baselines achieving state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55.

Conclusion: DRA integrates seamlessly with GRPO and its variant DR.~GRPO, resulting in DRA-GRPO and DGA-DR.~GRPO, improving performance in low-resource settings.

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [23] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger,Francesco Salvi,Jiacheng Liu,Xiaoli Nan,Ramit Debnath,Barbara Fasolo,Evelina Leivada,Gabriel Recchia,Fritz Günther,Ali Zarifhonarvar,Joe Kwon,Zahoor Ul Islam,Marco Dehnert,Daryl Y. H. Lee,Madeline G. Reinecke,David G. Kamper,Mert Kobaş,Adam Sandford,Jonas Kgomo,Luke Hewitt,Shreya Kapoor,Kerem Oktar,Eyup Engin Kucuk,Bo Feng,Cameron R. Jones,Izzy Gainsburg,Sebastian Olschewski,Nora Heinzelmann,Francisco Cruz,Ben M. Tappin,Tao Ma,Peter S. Park,Rayan Onyonka,Arthur Hjorth,Peter Slattery,Qingcheng Zeng,Lennart Finke,Igor Grossmann,Alessandro Salatiello,Ezra Karger*

Main category: cs.CL

TL;DR: In a large-scale experiment, an advanced LLM (Claude Sonnet 3.5) outperformed incentivized human persuaders in persuading participants towards correct or incorrect answers in a real-time conversational quiz setting.


<details>
  <summary>Details</summary>
Motivation: To directly compare the persuasion capabilities of a frontier large language model against incentivized human persuaders and evaluate its impact on participants' accuracy and earnings in a quiz setting.

Method: A preregistered, large-scale incentivized experiment where participants completed an online quiz while being persuaded by either humans or the LLM towards correct or incorrect answers.

Result: The LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than humans, improving participants' accuracy and earnings when guiding towards correct answers and decreasing them when steering towards incorrect answers.

Conclusion: AI's persuasion capabilities already surpass those of humans with real-money bonuses tied to performance, highlighting the need for emerging alignment and governance frameworks.

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [24] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance.


<details>
  <summary>Details</summary>
Motivation: Existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains.

Method: To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them.

Result: We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts.

Conclusion: Our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [25] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu,Lechen Zhang,Sheza Munir,Yiyang Gu,Lu Wang*

Main category: cs.CL

TL;DR: VeriFact is a new framework for factuality evaluation that enhances fact extraction and verification by identifying incomplete or missing facts, while FactRBench is a benchmark to evaluate both precision and recall in long-form model responses.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating the factuality of LLMs often fail to capture essential context and miss key relational facts due to their decompose-decontextualize-verify pipeline.

Method: Introduced VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts. Also introduced FactRBench, a benchmark that evaluates both precision and recall in long-form model responses.

Result: Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information. Benchmarking various LLMs on FactRBench indicate larger models within same model family improve precision and recall, but high precision does not always correlate with high recall.

Conclusion: VeriFact improves factuality evaluation by enhancing fact completeness and preserving complex facts. FactRBench provides a comprehensive way to assess both precision and recall in long-form model responses.

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [26] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: This paper presents a tutorial for developing, testing and applying taxonomies to analyze unstructured data using LLMs, demonstrating the process with personal goals as an example.


<details>
  <summary>Details</summary>
Motivation: To provide a methodological guide for efficiently analyzing unstructured text data while reducing bias and maintaining quality.

Method: An iterative and collaborative process involving researchers and LLMs to develop, test, and apply taxonomies for text analysis, including writing prompts, evaluating and refining taxonomies, assessing intercoder agreements, and categorizing datasets.

Result: Demonstrated a successful application of LLMs in generating a taxonomy of life domains from personal goals, achieving high intercoder reliability.

Conclusion: LLMs offer promising possibilities for text analysis with predefined or data-driven taxonomies, though there are limitations that need to be considered.

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [27] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: Pretrained language models (LLMs) suffer from fixed tokenization schemes which limit their efficiency and performance, especially in multilingual or specialized scenarios. To address this issue, the authors propose TokenAdapt, a model-agnostic tokenizer transplantation method, and novel pre-tokenization learning for multi-word Supertokens. TokenAdapt uses a hybrid heuristic to initialize new unique token embeddings by combining subword decomposition with semantic similarity estimates. This approach preserves semantics while reducing retraining needs. Empirical results show that TokenAdapt outperforms existing methods like Retok and Transtokenizer, achieving significant improvements in zero-shot perplexity scores.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the limitations imposed by fixed tokenization schemes in pretrained language models. Current methods to adapt tokenizers are either computationally expensive or fail to fully preserve semantic nuances and address compression inefficiencies.

Method: The paper introduces two innovations: 1) TokenAdapt - a model-agnostic tokenizer transplantation method that initializes new token embeddings using a hybrid heuristic combining subword decomposition and semantic similarity; 2) Pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation.

Result: Empirical investigations demonstrate that TokenAdapt successfully initializes unique tokens, outperforming conventional baselines and sophisticated methods such as Transtokenizer and ReTok. The Supertokens achieve notable compression gains. Additionally, TokenAdapt shows consistent lower perplexity ratios compared to ReTok and Transtokenizer across different base models and newly trained target tokenizers, with at least a 2-fold improvement in aggregate scores.

Conclusion: TokenAdapt offers an effective solution to the tokenizer lock-in problem in pretrained language models by providing a model-agnostic tokenizer transplantation method that preserves semantics and reduces retraining requirements. It outperforms existing methods in terms of zero-shot perplexity and provides significant improvements in compression and performance.

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [28] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: The paper explores the application of NLP techniques for automating data extraction from EHRs related to lung and breast cancer, using GMV's uQuery tool and fine-tuning a RoBERTa-based model. It reports strong performance in identifying certain clinical entities.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of information from clinical reports is time-consuming and error-prone, limiting the efficiency of data-driven approaches in healthcare.

Method: Utilized GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model (RoBERTa-based) for Named Entity Recognition tasks on a dataset of annotated breast and lung cancer reports.

Result: Demonstrated strong overall performance in entity recognition, especially for common entities like MET and PAT, with challenges remaining for less frequent entities like EVOL.

Conclusion: NLP techniques can significantly enhance the accuracy and efficiency of data extraction from EHRs in oncology, contributing to better patient outcomes.

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [29] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 尽管线性探针在检测短对话中的谎言方面表现出良好的泛化能力，但在处理更长格式的对话时效果较差。本文提出了一种通过在每段对话末尾添加固定关键短语来显著改善这种泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）中普遍存在的真理方向，探讨线性探针是否能够在线性激活空间内区分真实和虚假陈述，并进一步探索其在不同对话形式中的泛化能力。

Method: 分析线性探针在不同对话格式（尤其是包含谎言的对话）中的表现；提出在每段对话的末尾添加固定关键短语以改善泛化性能的方法。

Result: 发现线性探针在短对话中具有良好泛化能力，但在较长对话中表现不佳；提出的方法显著提高了对长对话的泛化能力。

Conclusion: 虽然在短对话中可以实现较好的谎言检测，但要构建可靠的、能适应新环境的LLM谎言检测器仍面临挑战。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [30] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui,Juan Diego Rodriguez,Philippe Laban,Dean Murphy,Joseph P. Dexter,Richard Jean So,Samuel Baker,Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 尽管密切阅读是大学课程的重要组成部分，但尚未在大规模语言模型（LLM）上进行评估。为填补这一空白，本文提出了KRISTEVA，这是第一个包含1331个多项选择题的密切阅读基准测试。通过三个逐步复杂的任务集来评估LLM对文学作品的理解和推理能力，结果显示最先进的LLM虽然具备一定程度的大学水平密切阅读能力（准确率49.7%-69.7%），但在11项任务中的10项上仍不及有经验的人类评估者。


<details>
  <summary>Details</summary>
Motivation: 密切阅读作为培养批判性思维的基础，在大学课程中被广泛采用，然而尚未针对大型语言模型（LLMs）进行评估，且多学科基准如MMLU未涵盖文学领域。因此需要一个专门的基准来评估LLMs在文学文本分析方面的能力。

Method: 提出KRISTEVA——一个由1331道课堂数据改编的多项选择题组成的基准测试，并设计了三个逐步复杂的任务集：1) 提取风格特征；2) 从参数化知识中检索相关上下文信息；3) 在风格与外部上下文之间进行多跳推理。以此来测试LLMs在密切阅读过程不同元素上的表现。

Result: 实验结果表明，最先进的LLMs在密切阅读任务上表现出一定的大学水平能力，准确率范围为49.7%-69.7%，但在11项任务中的10项上，其表现仍逊色于有经验的人类评估者。

Conclusion: 当前最先进的LLMs在文学作品的理解和推理方面已展现出一定能力，但仍显著落后于人类专家的表现，特别是在涉及复杂推理的任务上。这表明在提升LLMs的密切阅读能力方面还有很大改进空间。

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [31] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）在仅使用预训练权重的情况下预测暴力冲突升级和伤亡的能力，并评估了结合外部数据（如冲突数据集和新闻报告）对模型性能的提升。研究通过参数化和非参数化两种设置，在非洲之角和中东地区2020-2024年的冲突预测中进行实验，发现外部知识能有效增强LLMs的冲突预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言任务上表现出色，但其预测暴力冲突的能力尚未得到充分探索。这对于早期预警系统、人道主义规划和政策制定至关重要。

Method: 研究比较了LLMs在参数化（仅依靠预训练权重）和非参数化（结合冲突数据集和新闻报告等外部信息）两种设置下的冲突预测能力。采用两部分评估框架，覆盖2020-2024年非洲之角和中东地区的冲突多发区域。

Result: 在参数化设置下，LLMs能够仅依靠预训练知识预测冲突趋势和伤亡；在非参数化设置下，结合外部信息显著提升了模型性能。

Conclusion: LLMs具备一定的冲突预测能力，但结合结构化的外部知识可以进一步提高其预测效果。这为冲突预测领域提供了新的思路和方法。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [32] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila,Esteban Villa Turek,Ellen Karina Chumbe Fernandez,Luis Felipe Polo Galvez,Luis Cadavid,Andrea Marroquin,Rebeca Vargas Quesada,Johanna Crew,Nicole Vallejo Galarraga,Christopher Rodriguez,Diego Gutierrez,Radhi Datla*

Main category: cs.CL

TL;DR: This paper explores the differences in written Spanish across Latin America and Spain, emphasizing the need for locale-sensitive AI models to bridge sociolinguistic gaps and improve inclusivity.


<details>
  <summary>Details</summary>
Motivation: To highlight the critical need for regional localized models by examining the primary differences between variants of written Spanish across Latin America and Spain.

Method: An in-depth sociocultural and linguistic contextualization of the differences in written Spanish among dialectal groups.

Result: Locale-sensitive AI models can significantly bridge sociolinguistic divides and inform better localization strategies that meet inclusivity goals and secure sustainable user growth.

Conclusion: Implementing at least five sub variants of Spanish addresses key actions such as fostering user trust and reliance on AI language models while demonstrating cultural, historical, and sociolinguistic awareness.

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [33] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang,Yubing Ren,Yanan Cao,Binxing Fang*

Main category: cs.CL

TL;DR: The paper introduces a symbiotic watermarking framework with serial, parallel, and hybrid strategies for Large Language Models (LLMs), which optimizes the balance between detectability, robustness, text quality, and security.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs among robustness, text quality, and security in current watermarking schemes for LLMs.

Method: Proposes a versatile symbiotic watermarking framework with three strategies - serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy.

Result: Experimental results indicate that the method outperforms existing baselines and achieves state-of-the-art (SOTA) performance.

Conclusion: This framework provides novel insights into diverse watermarking paradigms for LLMs.

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [34] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu,Hanzhang Zhou,Zijian Feng,Tianjiao Li,Chua Jia Jim Deryl,Mak Lee Onn,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: The paper introduces MePO, a lightweight prompt optimizer that enhances the quality of prompts for both large-scale and lightweight language models without relying on online optimization, reducing costs and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To address the issue where optimized prompts from advanced large language models (LLMs) can overwhelm lightweight inference models and degrade response quality.

Method: MePO is trained on a preference dataset built from merit-aligned prompts generated by a lightweight LLM. It focuses on model-agnostic prompt quality merits to improve prompt and response quality.

Result: Experiments show that MePO achieves better results across various tasks and model types, providing a scalable and robust solution for real-world deployment.

Conclusion: MePO offers an effective, interpretable, and locally deployable solution for prompt optimization, enhancing performance across diverse tasks and model sizes.

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [35] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 通过使用知识图谱增强检索生成（RAG）的方法，解决大语言模型（LLM）在生成个性化回应时出现的幻觉问题，相较于仅用文本输入个人数据的基础LLM，该方法能更好地理解个人信息并生成精确回应，但响应时间略有减少。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已经被应用于多种场景，但由于训练数据庞大，常常发生过度拟合，导致生成额外和不正确的数据，从而在输出中产生幻觉。其中一个根本原因是缺乏及时、准确和个性化的信息输入到LLM中。

Method: 提出了一种利用知识图谱（KGs）进行检索增强生成（RAG）的方法，以帮助LLM生成个性化的回应。此方法中的知识图谱可以储存持续更新的事实信息，并且作者将重点放在日历数据上。

Result: 实验结果表明，与使用个人数据作为文本输入的基础LLMs相比，该方法能够显著更好地理解和生成准确的回应，不过响应时间有适度的减少。

Conclusion: 引入了知识图谱辅助的检索增强生成方法，有助于改善LLM在个性化回应生成中的表现，同时提高了对个人信息的理解能力。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [36] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin,Fan Huang*

Main category: cs.CL

TL;DR: 研究人员开发了一种名为DIF（人口统计隐式公平）的方法，用于衡量大型语言模型（LLMs）中的隐式偏差。通过使用带有社会人口特征的角色评估现有的逻辑和数学问题数据集，他们发现回答准确性与隐式偏差之间存在反比趋势。这表明LLMs的技术局限性，即无法很好地处理额外信息。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在近年来变得越来越重要，但人们对其从训练数据中继承的潜在偏见表示担忧。先前的研究已经探讨了LLMs如何表现出隐式偏见，例如当引入不同的社会背景时，响应生成会发生变化。本文作者认为这种隐式偏见不仅是伦理问题，也是技术问题，因为这揭示了LLMs无法适应额外信息的能力。然而，与其他LLM智能度量标准不同，目前尚无标准方法来衡量这一特定子集的LLM偏见。

Method: 为了填补这一空白，研究人员开发了一种可以计算出易于解释的基准的方法——DIF（Demographic Implicit Fairness），通过使用社会人口特征角色来评估现有的LLM逻辑和数学问题数据集。

Result: 研究结果表明，该方法可以在统计上验证LLM行为中隐式偏见的存在，并且发现了问题回答准确性和隐式偏见之间的反向趋势。

Conclusion: 这一发现支持了作者关于隐式偏见是技术问题的观点，并揭示了LLMs在处理额外信息方面的不足。

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [37] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng,Jinhao Jiang,Zican Dong,Wayne Xin Zhao,Lei Fang*

Main category: cs.CL

TL;DR: The paper introduces CAFE, a two-stage coarse-to-fine method for enhancing multi-document question-answering by filtering and steering relevant documents, leading to significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing retrieval precision and recall in long-context inputs for LLMs, affecting their ability to answer questions effectively.

Method: CAFE uses a coarse-grained filtering method to identify and rank relevant documents followed by a fine-grained steering method to guide attention to the most relevant content.

Result: Experiments demonstrate that CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model respectively.

Conclusion: CAFE successfully mitigates issues related to background and distracting documents, improving reliance on evidence documents and enhancing multi-document question-answering capabilities.

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [38] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: The paper explores the vulnerability of Large Language Models (LLMs) to jailbreak attacks, revealing a universal attack that compromises multiple models and enables harmful outputs. Despite disclosure, industry responses have been inadequate, raising concerns about AI safety practices as LLMs proliferate.


<details>
  <summary>Details</summary>
Motivation: To highlight the growing threat posed by dark LLMs—models either deliberately designed without ethical guardrails or modified through jailbreak techniques—and to address the inadequacies in current industry practices regarding AI safety.

Method: Identification of a universal jailbreak attack that effectively compromises state-of-the-art LLMs, enabling them to produce harmful outputs upon request. Responsible disclosure of this attack to major LLM providers followed by analysis of their responses.

Result: Discovery of a universal jailbreak attack that many tested LLMs were still vulnerable to, indicating significant gaps in current AI safety measures. Inadequate responses from major LLM providers further emphasize these gaps.

Conclusion: As model training becomes more accessible and cheaper, the risk of misuse escalates. Decisive intervention is needed to prevent LLMs from democratizing access to dangerous knowledge.

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [39] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah,Francois Meyer*

Main category: cs.CL

TL;DR: Pretrained language models (PLMs) for African languages encode more linguistic information when adapted specifically for African languages, compared to massively multilingual PLMs. Token-level syntactic info is in middle-to-last layers and sentence-level semantic info across all layers.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate how pretrained language models capture linguistic knowledge specific to African languages.

Method: Trained layer-wise probes on six typologically diverse African languages using the MasakhaPOS dataset with control tasks designed to interpret probe performance.

Result: Adapted PLMs for African languages encode more linguistic information about target languages than massively multilingual PLMs. Token-level syntactic information is concentrated in middle-to-last layers while sentence-level semantic information is distributed across all layers.

Conclusion: The study confirms that probe performance reflects internal knowledge of PLMs rather than memorization, applying established interpretability techniques to African-language PLMs and highlighting mechanisms behind successful strategies like active learning and multilingual adaptation.

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [40] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu,Sony Trenous,Leonardo F. R. Ribeiro,Bill Byrne,Felix Hieber*

Main category: cs.CL

TL;DR: A new benchmark named XRAG is proposed to evaluate LLMs' generation abilities in cross-lingual RAG settings. It highlights two challenges: response language correctness in monolingual retrieval and reasoning over retrieved information across languages in multilingual retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the evaluation of LLMs' generation abilities in scenarios where user language doesn't match the retrieval results, especially focusing on cross-lingual complexity.

Method: XRAG is constructed from recent news articles ensuring questions require external knowledge. It covers both monolingual and multilingual retrieval scenarios with relevancy annotations.

Result: Experimental results on five LLMs reveal two main challenges: 1) response language correctness in monolingual retrieval; 2) reasoning over retrieved information across languages in multilingual retrieval.

Conclusion: XRAG serves as a valuable benchmark for studying LLM reasoning abilities, highlighting significant gaps between human and LLM performance in cross-lingual RAG settings.

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [41] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan,Di Wu,Yibin Lei,Christof Monz,Iacer Calixto*

Main category: cs.CL

TL;DR: The paper introduces S-MedQA, an English medical QA dataset for evaluating large language models in clinical specialties. It challenges the hypothesis that training on specialty data leads to best performance and suggests domain shifting is more impactful than knowledge injection.


<details>
  <summary>Details</summary>
Motivation: To benchmark large language models' performance in fine-grained clinical specialties and evaluate the hypothesis related to knowledge injection in medical QA.

Method: Introduced S-MedQA dataset and used it to test model performance across different clinical specialties after fine-tuning.

Result: 1) Training on specialty data does not necessarily lead to the best performance on that specialty; 2) Token probabilities of clinically relevant terms increase consistently regardless of the specialty fine-tuned on.

Conclusion: Improvement gains come mostly from domain shifting rather than knowledge injection, suggesting a rethinking of the role of fine-tuning data in the medical domain.

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [42] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da,Parth Mitesh Shah,Kuan-Ru Liou,Jiaxing Zhang,Hua Wei*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are essential in human decision-making, but their outputs can be unreliable. This paper introduces GE-Chat, a framework that uses knowledge graphs and retrieval-augmented generation to provide evidence-based responses, improving the reliability of LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of untrustworthy responses from LLMs that may contain mistakes or plausible yet incorrect information, leading to complications and trust issues among users.

Method: GE-Chat utilizes a knowledge graph enhanced retrieval-augmented generation framework. When a user uploads a document, a knowledge graph is created to support a retrieval-augmented agent. The method employs Chain-of-Thought logic generation, n-hop sub-graph searching, and entailment-based sentence generation for accurate evidence retrieval.

Result: The proposed method enhances the ability to identify exact evidence in a free-form context, providing a reliable way to examine the resources behind LLM conclusions and aiding in assessing their trustworthiness.

Conclusion: GE-Chat offers an effective solution to improve the reliability of LLMs by generating evidence-based responses, thereby helping users make more informed decisions.

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [43] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: Reasoning CPT uses synthetic data to reconstruct hidden thought processes, improving model performance across domains and problem difficulties.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of synthesizing training data for reasoning and its impact on a wide range of domains, addressing the limitations of task-specific signals in supervised fine-tuning and reinforcement learning.

Method: Apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark.

Result: Reasoning CPT consistently improves performance across all evaluated domains, with notable gains in challenging problems and effective transfer of reasoning skills between domains.

Conclusion: Reasoning CPT is an effective method for enhancing reasoning capabilities in language models, allowing them to adjust reasoning depth according to problem difficulty.

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [44] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: The paper introduces CoT Encyclopedia, a framework that automatically extracts and clusters reasoning criteria from model-generated CoTs to produce more interpretable analyses. It also shows performance gains by guiding models toward better strategies.


<details>
  <summary>Details</summary>
Motivation: Current understanding of reasoning strategies in large language models is limited, and predefined categorization methods are constrained by human intuition, failing to capture the full diversity of model behaviors.

Method: The method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior.

Result: Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods, can predict which strategy a model is likely to use, and guide it toward more effective alternatives.

Conclusion: The CoT Encyclopedia provides practical insights, such as the significant impact of training data format on reasoning behavior compared to data domain.

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [45] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao,Hongyi Huang,Jiayi Wu,YiMing Cheng,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: VQ-Logits is a new method using Vector Quantization to reduce parameters and computation in LLM output layers, achieving significant speedup with minor performance loss.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have large output vocabularies leading to high computational and memory costs, especially in the final linear projection layer. Current solutions like adaptive softmax add structural complexities.

Method: The VQ-Logits approach replaces the large output embedding matrix with a small codebook of K vectors. Tokens are mapped to these vectors, and logits over this compact space are 'scattered' back to full vocabulary space.

Result: Experiments on benchmarks like WikiText-103 and C4 show up to 99% parameter reduction and 6x speedup in logit computation, with only a 4% increase in perplexity compared to full softmax baselines.

Conclusion: VQ-Logits effectively reduces parameters and computation in LLM output layers while maintaining strong performance, demonstrated through extensive experiments and ablation studies.

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [46] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang,Kaili Sun,Bowen Wu,Qun Yu,Ying Li,Baoxun Wang*

Main category: cs.CL

TL;DR: The paper proposes RAIDEN-R1, a reinforcement learning framework with Verifiable Role-Awareness Reward (VRAR) to improve role consistency in role-playing conversational agents. It uses singular and multi-term mining strategies for rewards and develops a Chain-of-Thought dataset via multi-LLM collaboration. The 14B-GRPO model shows superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Role-playing conversational agents often struggle with maintaining role consistency, which this research aims to address.

Method: RAIDEN-R1 framework integrates VRAR using singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. A high-quality Chain-of-Thought dataset is constructed through multi-LLM collaboration.

Result: The 14B-GRPO model achieved 88.04% accuracy on Script-Based Knowledge and 88.65% on Conversation Memory metrics, outperforming baseline models while maintaining robustness.

Conclusion: RAIDEN-R1 bridges the gap of non-quantifiability in RPCA training and offers insights into role-aware reasoning patterns, thus advancing the development of RPCAs.

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [47] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 在多语言背景下，评估多个最先进大语言模型在零样本和少样本情况下对复杂文本数据集（包括俄语和乌克兰语社交媒体帖子）进行标注的能力，特别是识别涉及人权侵犯的二分类任务。通过与人工双重标注的黄金标准比较，研究探讨了不同提示条件下的模型表现、错误模式及跨语言适应性，为LLM在敏感领域应用提供了可靠性见解。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理系统的日益复杂，大型语言模型在多种任务中展现出巨大潜力，本研究旨在探索这些模型在多语言环境下处理敏感、特定领域任务的能力，特别是在零样本和少样本学习场景下标注涉及人权侵犯的社交媒体帖子。

Method: 研究选取了GPT-3.5、GPT-4、LLAMA3、Mistral 7B和Claude-2等最先进的大语言模型，通过提供英语和俄语两种语言的提示，对包含俄语和乌克兰语的社交媒体帖子数据集进行二分类任务（识别涉及人权侵犯的参考）。模型的表现通过与1000个样本的人工双重标注黄金标准进行比较来评估，并分析了不同提示条件下的性能和错误模式。

Result: 各模型在不同提示条件下的表现各异，展现了各自的优势、局限性和跨语言适应能力。通过对模型输出与人工标注的对比，揭示了模型在处理主观性强且依赖上下文判断的任务中的独特错误和分歧模式。

Conclusion: 大型语言模型在多语言背景下的敏感领域任务中表现出一定的可靠性和适用性，但其在处理主观性和上下文依赖性判断时仍存在挑战。这为未来在实际场景中部署语言模型提供了重要考虑因素。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [48] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: This paper analyzed 19,123 studies and found that generative LLMs have advantages in open-ended tasks, while traditional NLP is better for information extraction and analysis tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the differences between traditional NLP and generative LLMs across different medical tasks.

Method: Analyzed 19,123 studies related to the application of NLP and LLMs in medicine.

Result: Generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks.

Conclusion: As these technologies advance, ethical use of them is essential to ensure their potential in medical applications.

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [49] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: Quicker is an evidence-based clinical decision support system powered by LLMs designed to automate evidence synthesis and generate clinical recommendations. It has strong performance in question decomposition, retrieval sensitivities, literature screening, and evidence assessment. Collaboration with Quicker reduces the time required for recommendation development.


<details>
  <summary>Details</summary>
Motivation: Integrating clinical evidence into real-time practice is challenging due to workload, complex processes, and time constraints. There is a need for tools that can automate evidence synthesis for more efficient decision making.

Method: Quicker implements a fully automated chain covering all phases from questions to clinical recommendations. It uses large language models and includes integrated tools and interactive user interfaces. A benchmark dataset Q2CRBench-3 was developed based on clinical guideline development records for evaluation.

Result: Experimental results showed Quicker's strong performance in fine-grained question decomposition, retrieval sensitivities comparable to human experts, literature screening approaching comprehensive inclusion of relevant studies, effective evidence assessment support, and more comprehensive and coherent recommendations than clinicians.

Conclusion: The findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [50] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: A new reinforcement learning method named J1 is introduced to train LLM-as-a-Judge models, which converts various prompts to judgment tasks with verifiable rewards. This approach outperforms other existing models in certain sizes and provides detailed analysis on different training aspects.


<details>
  <summary>Details</summary>
Motivation: The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models are crucial solutions.

Method: J1 is a reinforcement learning approach that converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards, incentivizing thinking and mitigating judgment bias.

Result: J1 outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. It also surpasses o1-mini and even R1 on some benchmarks despite training a smaller model.

Conclusion: The study provides analysis and ablations comparing different training recipes, reward strategies, and variations in thought length and content, concluding that the models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [51] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang,Zhanyu Shen,Hui Huang*

Main category: cs.CL

TL;DR: An abstract about a new method called LDIR for creating low-dimensional dense and interpretable text embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods either lack interpretability or perform poorly. Recent work on interpretable embeddings using large language models results in high-dimensional embeddings.

Method: LDIR creates low-dimensional (under 500) dense and interpretable text embeddings. It uses numerical values indicating semantic relatedness to anchor texts via farthest point sampling.

Result: LDIR performs similarly to black-box baseline models and surpasses interpretable embedding baselines, despite having significantly fewer dimensions.

Conclusion: LDIR provides a novel approach for generating interpretable text embeddings with strong performance and reduced dimensionality.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [52] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye,Shaonan Wang*

Main category: cs.CL

TL;DR: 提出了一种统一且灵活的框架，利用视觉-语言模型（VLMs）和特定模态专家，从由视觉、听觉和文本输入引发的大脑记录中重建连贯的语言。该方法性能与现有最佳系统相当，同时更具适应性和可扩展性，推动了更生态有效和可泛化的思维解码研究。


<details>
  <summary>Details</summary>
Motivation: 以往的研究在从fMRI数据重建语言方面取得了一定进展，但通常局限于单一模态输入（如图像或音频），而人类思维本质上是多模态的，因此需要一种能够处理多模态输入的方法来更真实地反映人类认知过程。

Method: 通过构建一个统一且灵活的框架，结合视觉-语言模型（VLMs）和特定模态专家，跨多种模态（视觉、听觉和文本）解释信息，从而实现从大脑活动记录中重建连贯语言的目标。

Result: 实验表明，该方法在性能上可与现有最佳系统媲美，并且具有更高的适应性和可扩展性。

Conclusion: 这项工作为更加生态有效和可泛化的思维解码技术奠定了基础，进一步推动了人类认知研究和脑机交互应用的发展。

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [53] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: This paper explores the design of an aspect-based sentiment analysis system using LLMs for real-world use, focusing on quadruple opinion extraction and demonstrating that a combined multi-domain model achieves performance comparable to specialized single-domain models.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of a single fine-tuned model in handling multiple domain-specific taxonomies simultaneously for aspect-based sentiment analysis.

Method: Designing an aspect-based sentiment analysis system using large language models with focus on quadruple opinion extraction including aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages.

Result: A combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity.

Conclusion: Lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks are shared.

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [54] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 随着神经语言模型的发展，代码生成性能得到了显著提升，但重复问题依然存在。本文正式定义了结构重复问题，并提出了基于语法的重复惩罚解码方法RPG，通过识别和减少重复模式来改善代码生成质量。实验表明，RPG在多个数据集上有效减少了重复并提升了生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 尽管神经语言模型在代码生成方面取得了显著进展，但在生成过程中仍存在重复问题。以往研究主要关注内容重复，而更普遍且具有挑战性的是结构重复，即以固定结构出现在不同模式中的重复代码。

Method: 提出了一种名为RPG（Repetition Penalization based on Grammar）的解码方法，首先利用语法规则识别代码生成中的重复问题，然后通过降低导致重复的关键标记的概率来缓解重复现象。同时构建了一个新数据集CodeRepetEval以全面评估解决重复问题的方法。

Result: 大量实验结果表明，RPG在CodeRepetEval数据集以及HumanEval和MBPP基准测试中显著优于最佳基线方法，有效减少了重复现象并提高了生成代码的质量。

Conclusion: RPG方法能够有效缓解代码生成中的重复问题，提高生成代码的质量，为未来的研究提供了新的方向和评估工具。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [55] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo,Jae Ho Sohn,Gondy Leroy,Trevor Cohen*

Main category: cs.CL

TL;DR: This paper evaluates large language models (LLMs) in generating plain language summaries (PLSs) of health information using subjective and objective measures. Results show LLM-generated PLSs match human-written ones in subjective evaluations but lag behind in improving reader comprehension. Automated metrics poorly align with human judgment.


<details>
  <summary>Details</summary>
Motivation: To address the unclear effectiveness of LLMs in supporting health information comprehension and the limitations of prior evaluations that either rely on automated scores or subjective Likert-scale ratings with limited generalizability.

Method: Conduct a large-scale crowdsourced evaluation using Amazon Mechanical Turk with 150 participants. Assess PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, faithfulness, and objective multiple-choice comprehension and recall measures. Examine the alignment between 10 automated evaluation metrics and human judgments.

Result: LLM-generated PLSs appear indistinguishable from human-written ones in subjective evaluations but lead to significantly worse comprehension outcomes. Automated evaluation metrics fail to reflect human judgment accurately.

Conclusion: There is a need for evaluation frameworks that go beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension.

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [56] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yongkang Wu,Zhonghua Li,Qi Ye,Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner is an efficient plug-and-play refiner designed for long-context input scenarios in RAG applications, which reduces inference costs and enhances performance.


<details>
  <summary>Details</summary>
Motivation: Real-world RAG applications face challenges with long-context inputs due to redundant information and noise, leading to higher inference costs and reduced performance.

Method: LongRefiner uses dual-level query analysis, hierarchical document structuring, and adaptive refinement via multi-task learning on a single foundation model.

Result: Experiments on seven QA datasets show that LongRefiner performs competitively while using 10x fewer computational resources and latency compared to the best baseline.

Conclusion: LongRefiner is scalable, efficient, and effective, offering valuable insights for practical long-text RAG applications.

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [57] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang,Zhiyang Chen,Zijun Wang,Tiancheng Li,Guo-Jun Qi*

Main category: cs.CL

TL;DR: The paper introduces DCoLT, a reasoning framework for diffusion language models which optimizes the entire reasoning trajectory using Reinforcement Learning. It differs from traditional methods by allowing bidirectional, non-linear reasoning. The framework is implemented on two models - SEDD and LLaDA. Experiments show that DCoLT-reinforced models outperform others.


<details>
  <summary>Details</summary>
Motivation: To create a more effective reasoning framework for diffusion language models that can optimize the entire reasoning process and not just individual steps.

Method: DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and uses outcome-based Reinforcement Learning to optimize the entire reasoning trajectory. It is applied to two diffusion language models: SEDD and LLaDA.

Result: Experiments on math and code generation tasks show that DCoLT-reinforced Diffusion Language Models outperform other models trained by SFT or RL. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy significantly on various benchmarks.

Conclusion: DCoLT provides a novel approach to reasoning in diffusion language models, demonstrating superior performance compared to existing methods.

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [58] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang,Licheng Zhang,Zheren Fu,Zhendong Mao*

Main category: cs.CL

TL;DR: A new framework called CL-RAG is proposed, which uses multi-stage curriculum learning to train RAG systems more effectively. It constructs training data with multiple difficulty levels and optimizes the overall performance and generalization of the RAG system.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing the retriever or generator in the RAG system do not consider the varying effectiveness of documents across user queries, hindering adaptation during training.

Method: Propose a multi-stage Curriculum Learning based RAG system training framework named CL-RAG. Construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution and train the model in stages based on the curriculum learning approach.

Result: CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.

Conclusion: The CL-RAG framework can optimize the overall performance and generalization of the RAG system more effectively.

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [59] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou,Xiao Deng,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: Code security and usability are crucial for coding assistant applications driven by LLMs. Current benchmarks lack comprehensive assessment across dimensions. This paper proposes CoV-Eval, a multi-task benchmark for evaluating LLM code security, and VC-Judge, an improved judgment model. Experiments reveal challenges and offer insights for future research.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current code security benchmarks that focus solely on single evaluation tasks and lack comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination.

Method: Propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, detection and classification. Develop VC-Judge, an improved judgment model aligning closely with human experts to review LLM-generated programs for vulnerabilities.

Result: Most LLMs identify vulnerable codes well but tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs.

Conclusion: Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security.

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [60] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing,Goran Glavaš*

Main category: cs.CL

TL;DR: 通过优化词对齐工具(WA)的设计选择，跨语言迁移(XLT)在标记投影中的表现可与基于标记的方法相媲美，并且提出了一种新的集成策略，该策略结合了translate-train和translate-test预测，显著优于基于标记的投影，同时降低了对WA低级设计选择的敏感性，从而提高了XLT在分词任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管词对齐工具(WA)常用于标记投影，但将其应用于基于翻译的XLT的低级设计决策尚未得到系统研究；此外，最近基于标记的方法声称在XLT的标记投影中优于WA，因此有必要重新审视WA并探索其潜力。

Method: 系统研究影响基于翻译的XLT性能的低级设计决策，包括：标签在(多)标记跨度之间的投影算法、减少噪声映射标签数量的过滤策略以及翻译句子的预分词处理；引入一种新的投影策略，集成translate-train和translate-test预测。

Result: 优化WA设计选择后，基于WA的XLT性能至少可与基于标记的方法相媲美；新提出的集成策略显著优于基于标记的投影，并降低了对WA低级设计选择的敏感性，提高了XLT在分词任务中的鲁棒性。

Conclusion: 经过优化的WA可以提供与基于标记方法相当甚至更优的XLT性能；集成translate-train和translate-test预测的新策略进一步提升了性能并增强了鲁棒性，为XLT在分词任务中的应用提供了改进方向。

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [61] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: The paper introduces MuToR, a new method for multi-token prediction that integrates learnable register tokens into the input sequence to predict future targets. It has minimal additional parameters, requires no architectural changes, and works well with pre-trained language models. The method is effective for supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining across language and vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-token prediction methods have not consistently shown benefits when applied to fine-tuning scenarios. There is a need for an approach that can effectively leverage multi-token prediction while remaining compatible with pretrained models and suitable for various fine-tuning tasks.

Method: MuToR interleaves learnable register tokens into the input sequence. Each register token predicts future targets in the sequence. This approach introduces only a small number of additional parameters, does not require any changes to the model architecture, and remains aligned with the next-token pretraining objective. It also supports scalable prediction horizons.

Result: MuToR demonstrates effectiveness and versatility across multiple use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining. It performs well on challenging generative tasks in both language and vision domains.

Conclusion: MuToR is a simple yet powerful approach for multi-token prediction that enhances performance in various fine-tuning and pretraining scenarios without requiring significant modifications to existing models.

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [62] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: The paper explores scaling laws in preference modeling akin to those in language modeling, proposing World Preference Modeling (WorldPM) which shows scalability potential and effectiveness in improving generalization performance across human preference datasets.


<details>
  <summary>Details</summary>
Motivation: Scaling laws in language modeling inspired the exploration of similar patterns in preference modeling.

Method: Preference data was collected from public forums, with extensive training on 15M-scale data using models ranging from 1.5B to 72B parameters. Three types of evaluation metrics were analyzed: adversarial, objective, and subjective.

Result: Adversarial metrics improve with more training data and larger models; objective metrics show emergent behavior in larger models; subjective metrics do not scale. WorldPM improves generalization performance across datasets of varying sizes with gains exceeding 5% on key subtasks and 4%-8% in in-house evaluations.

Conclusion: WorldPM is effective as a foundation for preference fine-tuning and significantly enhances performance when integrated into the RLHF pipeline.

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [63] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu,Yibo Wang,Hanze Dong,Yuhui Xu,Amrita Saha,Caiming Xiong,Bryan Hooi,Junnan Li*

Main category: cs.CL

TL;DR: Large reasoning models (LRMs) have a hidden ability for long-term reasoning. Although reinforcement learning (RL) can sometimes bring out advanced reasoning behaviors, these are unpredictable and unreliable. This paper proposes aligning models with three meta-abilities (deduction, induction, and abduction) through automatically generated tasks to improve performance by over 10% compared to instruction-tuned baselines.


<details>
  <summary>Details</summary>
Motivation: LRMs already have the potential for long-term reasoning, but the advanced reasoning behaviors elicited by RL are not consistent or controllable, limiting scalability and reliability.

Method: The method involves a three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning, using automatically generated self-verifiable tasks aligned with deduction, induction, and abduction.

Result: This approach boosts performance by more than 10% relative to instruction-tuned baselines. Domain-specific RL from the aligned checkpoint yields an additional average gain of 2% in performance across math, coding, and science benchmarks.

Conclusion: Explicit alignment with meta-abilities provides a scalable and reliable foundation for reasoning in LRMs.

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: 本研究提出了一种基于行为的在线监测系统，使用定制的有向图神经网络（DGNN）对列车司机的状态进行分类，结合面部和骨骼特征的模型在三分类模型中准确率最高（80.88%），在二元警觉性分类中准确率超过99%，并引入了包含模拟病理条件的新数据集，以扩大对疲劳和健康相关风险的评估范围。


<details>
  <summary>Details</summary>
Motivation: 传统的列车司机疲劳监测系统（如死人开关）功能有限，无法有效监测司机的警觉性状态，因此需要一种更先进的在线监测系统来提高铁路安全。

Method: 本研究开发了一种基于行为的在线监测系统，使用定制的有向图神经网络（DGNN）对列车司机的状态进行分类。通过消融研究比较了三种特征配置：仅骨骼、仅面部和两者的组合，以优化模型的输入表示。

Result: 实验结果表明，结合面部和骨骼特征的模型在三分类模型中准确率最高（80.88%），在二元警觉性分类中准确率超过99%。此外，本研究还引入了一个新的数据集，首次将模拟病理条件纳入列车司机监测中，扩大了对疲劳和健康相关风险的评估范围。

Conclusion: 本研究通过使用先进的视觉技术进行在线监测，为提高铁路安全迈出了重要一步。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [65] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: This paper presents an online monitoring system for train drivers using a customized DGNN, achieving high accuracy in classifying driver states and introducing a novel dataset with simulated pathological conditions.


<details>
  <summary>Details</summary>
Motivation: Driver fatigue is a significant challenge to railway safety, requiring more advanced systems than traditional ones like the dead-man switch.

Method: The study utilized a Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories. An ablation study compared three feature configurations: skeletal-only, facial-only, and a combination of both.

Result: Combining facial and skeletal features yielded the highest accuracy (80.88%) in the three-class model and over 99% accuracy in binary alertness classification. A novel dataset incorporating simulated pathological conditions was also introduced.

Conclusion: This work enhances railway safety through advanced online monitoring using vision-based technologies.

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [66] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

TL;DR: The paper proposes an OptiGait-LGBM model for person re-identification using skeletal data, introduces the RUET-GAIT dataset, and demonstrates superior performance in accuracy, memory usage, and training time compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in real-world gait recognition such as uncontrolled environments, varying illumination, non-overlapping camera views, and computational efficiency, which current datasets and models fail to handle simultaneously.

Method: The method involves constructing a dataset from landmark positions using a skeletal model approach, introducing the RUET-GAIT benchmark dataset, extracting skeletal joint landmarks, generating numerical datasets, and developing the OptiGait-LGBM gait classification model.

Result: The proposed OptiGait-LGBM model outperforms ensemble techniques like Random Forest and CatBoost in terms of accuracy, memory usage, and training time.

Conclusion: The paper presents a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [67] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的稀疏表示方法SparseMeXt，在高精地图构建任务中超越了密集表示方法，显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏表示在高精地图构建中具有更高的计算效率潜力，但现有方法由于缺乏定制设计而表现不佳，难以与密集表示竞争。因此需要重新审视并改进稀疏表示技术以缩小与密集方法的差距。

Method: 作者提出了专用的网络架构、稀疏-密集分割辅助任务以及基于物理先验的去噪模块，这些技术共同优化了稀疏地图特征提取，并充分利用了几何和语义信息。

Result: SparseMeXt-Tiny、SparseMeXt-Base和SparseMeXt-Large分别达到了55.5%、65.2%和68.9%的mAP，同时保持较高的帧率（32 fps、20 fps以上）。这在nuScenes数据集上展示了其优越性能。

Conclusion: 该研究表明稀疏方法具有未开发的潜力，挑战了对密集表示的传统依赖，重新定义了高精地图构建领域中的效率-性能权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [68] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/abs/2505.08811)
*Shijie Lian,Ziyi Zhang,Laurence Tianruo Yang and,Mengyu Ren,Debin Liu,Hua Li*

Main category: cs.CV

TL;DR: An underwater 3D scene reconstruction method named Tensorized Underwater Gaussian Splatting (TUGS) is proposed, which can accurately simulate light attenuation and backscatter effects in underwater environments with less memory usage and faster rendering speeds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for underwater 3D scene reconstruction are unable to model the interactions between light propagation, water medium, and object surfaces accurately. Moreover, these methods have expensive training and rendering costs that limit their practical application in underwater robotic systems.

Method: The proposed method, TUGS, employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module. This enables accurate simulation of both light attenuation and backscatter effects in underwater environments.

Result: Compared to other NeRF-based and GS-based methods designed for underwater, TUGS can render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated its superior reconstruction quality using a limited number of parameters.

Conclusion: TUGS can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction, making it particularly suitable for memory-constrained underwater UAV applications.

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [69] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

TL;DR: 这篇论文通过一系列实证实验，研究了四种神经网络覆盖度量（主要功能、边界、层次和结构覆盖）之间的关系和模式，使用LeNet、VGG和ResNet作为不同的深度神经网络架构，并分析了模型深度、配置信息和覆盖度量之间的关系。此外，还探讨了修改后的决策/条件覆盖率与数据集大小之间的关系，并提出了三个潜在的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNNs）的广泛应用和进步，不同类型的神经行为引起了关注，出现了各种神经网络覆盖度量。然而，目前缺乏关于这些覆盖度量的经验研究，特别是在分析模型深度、配置信息和神经网络覆盖之间的关系和模式方面。

Method: 选择LeNet、VGG和ResNet作为不同的DNN架构，选取10个深度从5到54层不等的模型，比较和研究不同深度、配置信息和各种神经网络覆盖度量之间的关系。同时，对修改后的决策/条件覆盖率与数据集大小之间的关系进行调查。

Result: 研究揭示了四种覆盖度量之间的关系和模式，以及模型深度、配置信息和覆盖度量之间的联系。此外，还发现了修改后的决策/条件覆盖率与数据集大小之间的关系。

Conclusion: 本研究为理解神经网络覆盖度量之间的关系提供了重要见解，并为未来在DNN模型安全测试方面的研究指明了方向。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [70] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

TL;DR: Score-based generative models (SBM) are leading in image synthesis but can generate NSFW content. Current prevention methods use model knowledge and fine-tuning. This paper explores using external sources for safe outputs in SBMs, presenting an SFW sampler with Conditional Trajectory Correction that uses multimodal models and CLIP for user-defined NSFW classes. Experiments on Stable Diffusion show the SFW sampler reduces explicit content generation while maintaining competitive performance with minimal impact on image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of SBMs generating NSFW content and explore methods to ensure safe outputs without relying solely on the model's own knowledge or requiring fine-tuning.

Method: The method involves implementing a Conditional Trajectory Correction step in the SFW sampler, which uses multimodal models to guide samples away from undesired regions. CLIP is used to allow user-defined NSFW classes, providing flexibility in different settings.

Result: The experiments demonstrate that the SFW sampler effectively reduces the generation of explicit content while performing competitively with fine-tuning-based approaches. The correction scheme has a minor cost with negligible effect on image quality for samples not needing correction.

Conclusion: The study concludes that the SFW sampler is suitable for aligned SBM models and highlights the potential of model-agnostic conditioning for preventing unwanted images.

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [71] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

TL;DR: The paper adapts a Stable Diffusion model with ControlNet to generate high-fidelity satellite imagery for urban planning, conditioned on land use descriptions, infrastructure, and natural environments. It achieves realistic urban landscapes, high FID/KID scores, and positive feedback from planners.


<details>
  <summary>Details</summary>
Motivation: Existing generative AI approaches struggle to produce realistic and practical urban designs at scale.

Method: Adapted a state-of-the-art Stable Diffusion model extended with ControlNet, spatially linking satellite imagery with structured land use and constraint information from OpenStreetMap using data from three major U.S. cities.

Result: Generated realistic and diverse urban landscapes, achieved high FID and KID scores, showed robustness across urban contexts, and received positive qualitative assessments from urban planners and the public.

Conclusion: Established a benchmark for controlled urban imagery generation and demonstrated the potential of generative AI in enhancing planning workflows and public engagement.

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [72] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: This paper proposes a combination of self-supervised training and Multi-Column CNN for crowd counting, and a spatiotemporal model based on VGG19 for crowd anomaly detection, both showing superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in crowd scene analysis including the need for large annotated datasets and the difficulties posed by occluded scenes, nonuniform density, complex backgrounds, and scale invariance in crowd counting; and lighting, environmental conditions, unexpected objects, and scalability in crowd anomaly detection.

Method: For crowd counting, the method combines self-supervised training with Multi-Column CNN. For crowd anomaly detection, a spatiotemporal model based on VGG19 is used where spatial features are learned using CNN and temporal features using LSTM blocks, with dense residual blocks replacing fully connected layers.

Result: The proposed crowd counting model achieves better results on ShanghaiTech and UCF-QNRF datasets in terms of MAE and MSE. The crowd anomaly detection model outperforms other state-of-the-art approaches on the Hockey Fight dataset and SCVD dataset.

Conclusion: The proposed models effectively tackle the challenges in crowd counting and crowd anomaly detection, achieving superior performance over existing state-of-the-art methods.

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [73] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Generative Artificial Intelligence (GenAI) is a transformative force that can potentially solve the challenge of achieving fully autonomous driving, especially Level 5 autonomy. This survey explores the role of GenAI across the autonomous driving stack, including its principles, applications, obstacles and possibilities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive synthesis of the emerging role of Generative Artificial Intelligence (GenAI) in achieving reliable, fully autonomous driving, particularly focusing on Level 5 autonomy.

Method: The paper reviews modern generative modeling techniques such as VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). It maps their applications in areas like image, LiDAR, trajectory, occupancy, video generation, LLM-guided reasoning and decision making. Practical applications include synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI.

Result: Identifies key obstacles such as generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects. Proposes research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence.

Conclusion: This survey serves as a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility.

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [74] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/abs/2505.08882)
*Ali Almakhluk,Uthman Baroudi,Yasser El-Alfy*

Main category: cs.CV

TL;DR: This study develops a system using Raspberry Pi, a camera module, deep learning model, laptop, and cloud service to detect road damages like potholes and cracks, classify their sizes, broadcast warning signals to nearby vehicles, and transmit data to the cloud for improving transportation safety.


<details>
  <summary>Details</summary>
Motivation: Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. The study aims to improve transportation safety by detecting these anomalies and notifying authorities and drivers.

Method: The system uses Raspberry Pi, a camera module, a deep learning model, a laptop, and cloud services. It detects potholes and cracks, classifies their sizes, broadcasts warning signals to nearby vehicles, counts anomalies in real-time, and transmits data to the cloud.

Result: The developed system can successfully detect road anomalies, classify their severity, warn nearby vehicles about severe anomalies, and send data to the cloud for further action by authorities.

Conclusion: By deploying this innovative solution, the study aims to proactively enhance road safety and mitigate potential accidents arising from road hazards, leading to safer road conditions for the whole community.

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [75] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

TL;DR: This study combines image processing and machine learning (neuro-fuzzy and colonial competition algorithms) to improve skin cancer diagnostics using dermoscopic images, achieving 94% accuracy on a dataset of 560 images from the ISIC database.


<details>
  <summary>Details</summary>
Motivation: The increasing incidence of skin cancer, lack of public awareness, and shortage of clinical expertise necessitate advanced diagnostic aids. AI shows promise in distinguishing between malignant and benign skin lesions.

Method: The study utilized a fusion of image processing techniques with machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches, applied to dermoscopic images from the ISIC database.

Result: The method achieved an accuracy of 94% when tested on a dataset of 560 images.

Conclusion: This approach holds significant potential in assisting clinicians with the early detection of melanoma, contributing to advancements in skin cancer diagnostics.

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [76] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

TL;DR: The paper proposes a cocoercive conservative (CoCo) denoiser for Poisson inverse problems in imaging, which can be expansive and improves denoising performance. It introduces a new training strategy using Hamiltonian and spectral regularization, proves the CoCo denoiser's properties, establishes global convergence of PnP methods with this denoiser, and shows superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing Plug-and-play (PnP) methods with deep denoisers require strong convexity or smoothness of the fidelity term and a non-expansive denoiser for convergence, but these assumptions are violated in Poisson inverse problems and can hinder denoising performance.

Method: The authors propose a cocoercive conservative (CoCo) denoiser that may be expansive, leading to improved denoising performance. They leverage the generalized Helmholtz decomposition to introduce a novel training strategy combining Hamiltonian regularization for conservativeness and spectral regularization for cocoerciveness. The CoCo denoiser is proven to be a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior.

Result: The proposed approach outperforms closely related methods in both visual quality and quantitative metrics through extensive experimental results.

Conclusion: The introduction of the cocoercive conservative (CoCo) denoiser addresses challenges in Poisson inverse problems by allowing expansiveness for better denoising, providing theoretical guarantees, and demonstrating superior performance in imaging tasks.

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [77] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

TL;DR: The paper presents Maya, an open-source Multilingual Vision-Language Model (VLM) to improve performance in low-resource languages and varied cultural contexts. It includes a pretraining dataset in eight languages based on LLaVA and a model enhancing cultural and linguistic comprehension.


<details>
  <summary>Details</summary>
Motivation: There is a lack of performance in existing large Vision-Language Models (VLMs) for low-resource languages and varied cultural contexts despite their impressive results on academic benchmarks mainly in widely spoken languages.

Method: The authors introduce Maya, which consists of a multilingual image-text pretraining dataset in eight languages derived from the LLaVA pretraining dataset and a multilingual image-text model supporting these languages to enhance cultural and linguistic understanding.

Result: Maya addresses the limitations of current VLMs by providing support for low-resource languages and improving cultural and linguistic comprehension in vision-language tasks.

Conclusion: The development of Maya contributes towards more inclusive Vision-Language Models capable of handling multiple languages and cultural contexts, with the code being available for further research and development.

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [78] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

TL;DR: The paper introduces DCS-Attention, a novel attention module that selects informative channels for feature extraction in DNNs, improving performance on person Re-ID tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the feature extraction capabilities of deep neural networks by developing an attention mechanism that focuses on selecting the most informative channels. This is motivated by the principle of Information Bottleneck (IB) and aims to improve the accuracy of person re-identification (Re-ID) tasks.

Method: The proposed method involves creating the Differentiable Channel Selection Attention (DCS-Attention) module which performs channel selection in a differentiable manner. It can be integrated with either fixed neural network backbones (DCS-FB) or learnable backbones using Differentiable Neural Architecture Search (DCS-DNAS). The authors derive a novel variational upper bound for the IB loss that can be optimized by SGD and incorporated into the training loss.

Result: Experiments on multiple person Re-ID benchmarks show that DCS-Attention significantly improves the prediction accuracy of DNNs for person Re-ID, demonstrating its effectiveness in learning discriminative features critical to identifying person identities.

Conclusion: The DCS-Attention module successfully enhances the performance of DNNs in person Re-ID tasks by selecting the most informative channels for feature extraction. This approach represents a state-of-the-art advancement in the field.

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [79] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

TL;DR: The paper introduces PRIOR, a vision-language pre-training method that prioritizes image-related tokens through differential weighting in the NTP loss using a reference text-only LLM. This approach improves performance and scaling properties compared to standard NTP.


<details>
  <summary>Details</summary>
Motivation: Standard large vision-language models (LVLMs) pre-training uses next-token prediction (NTP), but since only a small subset of caption tokens directly relate to the visual content, this can unintentionally fit the model to noise and increase hallucination risk.

Method: PRIOR introduces a reference model - a text-only large language model (LLM) trained on captions without image inputs - to weight each token based on its probability for LVLMs training. Tokens directly related to visual inputs receive lower probabilities from the text-only reference LLM and thus are re-weighted during training.

Result: PRIOR shows 19% and 8% average relative improvement in two distinct settings of LVLMs compared to NTP. It also exhibits superior scaling properties with significantly higher scaling coefficients.

Conclusion: PRIOR is a simple yet effective approach for vision-language pre-training that addresses the issue of noise fitting in standard NTP by prioritizing image-related tokens.

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [80] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/abs/2505.08999)
*Wei-Long Tian,Peng Gao,Xiao Liu,Long Xu,Hamido Fujita,Hanan Aljuai,Mao-Li Wang*

Main category: cs.CV

TL;DR: The paper proposes an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking that enhances the transferability and attack effectiveness of adversarial examples.


<details>
  <summary>Details</summary>
Motivation: To reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks.

Method: AMGA integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing. It randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model.

Result: Extensive experimental results on large-scale datasets demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples.

Conclusion: AMGA is an effective method to enhance the transferability and attack effectiveness of adversarial examples in visual tracking.

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [81] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: A multimodal deep learning framework that integrates CGM time-series data, Demographic/Microbiome info, and pre-meal food images to improve caloric intake estimation for Type 2 diabetes management is presented. It reduces RMSRE to 0.2544, surpassing baseline models by over 50%.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of caloric intake is essential for managing Type 2 diabetes but remains challenging. Current methods using Continuous Glucose Monitors (CGMs) lack comprehensive nutritional profiling due to variability among individuals and meals.

Method: The introduced framework uses a combination of attention-based encoding for meal imagery, convolutional feature extraction, multi-layer perceptrons for CGM and Microbiome data, followed by a late fusion strategy for joint reasoning. This allows integration of diverse data types such as CGM time-series, demographic/microbiome information, and pre-meal food images.

Result: Evaluated on a dataset with over 40 participants, the model achieved an RMSRE of 0.2544, showing more than 50% improvement compared to baseline models.

Conclusion: The findings highlight the potential of multimodal sensing in enhancing automated dietary assessment tools, which could be beneficial for chronic disease management like Type 2 diabetes.

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [82] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/abs/2505.09073)
*J. Brennan Peace,Shuowen Hu,Benjamin S. Riggan*

Main category: cs.CV

TL;DR: The paper proposes a novel domain adaptive framework for improving facial recognition performance across large pose differences.


<details>
  <summary>Details</summary>
Motivation: To address the degradation in performance of facial recognition due to substantial perspective (pose) differences between enrollment and query imagery.

Method: The framework uses a shared (joint) attention mapping to emphasize common patterns correlated between 2D facial images and 3D facial data, along with a joint entropy regularizing loss to promote consistency by enhancing correlations among the intersecting 2D and 3D representations.

Result: Evaluated on FaceScape and ARL-VTF datasets, the framework outperforms competitive methods with improvements of at least 7.1% and 1.57% in profile (90°+) TAR @ 1% FAR respectively.

Conclusion: The proposed domain adaptive framework facilitates improved performances across large discrepancies in pose for facial recognition.

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [83] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/abs/2505.09092)
*Yuhang Wang,Abdulaziz Alhuraish,Shengming Yuan,Hao Zhou*

Main category: cs.CV

TL;DR: The paper introduces OpenLKA, the first open large-scale dataset for LKA evaluation and improvement, containing 400 hours of driving data from over 50 vehicle models. It includes CAN bus streams, dash-cam video, Openpilot outputs, and scene annotations.


<details>
  <summary>Details</summary>
Motivation: Lane Keeping Assist (LKA) systems' real-world performance remains underexplored due to proprietary systems and limited data access.

Method: OpenLKA is a dataset that integrates vehicle-internal signals with high-fidelity perception and rich semantic context, collected through extensive road testing and global contributions.

Result: OpenLKA provides a comprehensive platform for benchmarking production LKA systems, identifying safety-critical operational scenarios, and assessing road infrastructure readiness for autonomous driving.

Conclusion: OpenLKA is publicly available and aims to improve the understanding and performance of LKA systems.

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [84] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/abs/2505.09118)
*Dayong Liang,Changmeng Zheng,Zhiyuan Wen,Yi Cai,Xiao-Yong Wei,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces Interaction-augmented Scene Graph Reasoning (ISGR), a framework enhancing vision-language models' ability to reason about complex interactions in visual scenes through three components: dual-stream graph constructor, targeted interaction queries, and long-term memory reinforcement learning. It outperforms baselines on interaction-heavy benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional scene graphs focus only on spatial relationships, limiting vision-language models' ability to reason about complex interactions in visual scenes. Conventional methods produce unfocused relationship sets and fail to form persistent memories for generalizing interaction reasoning.

Method: The ISGR framework includes: 1) A dual-stream graph constructor combining spatial relation extraction with interaction-aware captioning; 2) Targeted interaction queries activating VLMs' latent knowledge of object functionalities; 3) A long-term memory reinforcement learning strategy with an interaction-focused reward function.

Result: Extensive experiments show that ISGR significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, especially improving complex scene understanding tasks.

Conclusion: ISGR enhances VLMs' interactional reasoning capabilities by addressing limitations in conventional detection-to-construction methods and persistent memory formation.

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [85] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/abs/2505.09123)
*Guoying Liang,Su Yang*

Main category: cs.CV

TL;DR: This study successfully applies the Segment Anything Model (SAM) to Camouflaged Object Detection (COD), showing that with appropriate promotion, SAM can work effectively. It introduces a new framework involving Promotion Point Targeting Network (PPT-net) and key point selection (KPS) algorithm to guide segmentation.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of using big models like SAM for COD, overcoming previous claims that SAM is not workable for this task.

Method: Devised a new framework including PPT-net for predicting camouflaged object presences and KPS algorithm for deploying point promotions contrastively to SAM.

Result: Achieves plausible results experimentally over existing methods on 3 data sets under 6 metrics.

Conclusion: Demonstrates an effective off-the-shelf methodology for COD by leveraging SAM, which performs better than designing professional models from scratch.

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [86] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

TL;DR: This paper proposes a lightweight anomaly detection framework based on color features for surveillance video clips in high-risk security tasks, fusing unsupervised KMeans clustering with RGB channel histogram modeling. It successfully identifies anomalous frames in an African country's operation surveillance video without access to original data, showing strong deployability and tactical interpretation value.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models face significant challenges when deployed in high-risk security tasks within unlabeled and data-non-exploitable video intelligence environments.

Method: The method uses a lightweight anomaly detection framework that combines unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutations in key frames of surveillance video clips.

Result: The experiment successfully identified multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference in an African country's operation surveillance video, without needing the original data.

Conclusion: This method is effective for tactical assassination warning, suspicious object screening, and environmental change monitoring. The study highlights the importance of color features as low semantic battlefield signal carriers and suggests future work involving graph neural networks and temporal modeling.

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [87] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/abs/2505.09139)
*Lucas Choi,Ross Greer*

Main category: cs.CV

TL;DR: This paper presents a method for automated prompt refinement in vision-language models (VLMs) using the Contrastive Class Alignment Score (CCAS). It improves object detection accuracy by selecting high-precision prompts without extra model training or labeled data.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) provide flexible object detection through natural language prompts but have performance variability depending on prompt phrasing.

Method: The method generates diverse prompt candidates via a large language model and filters them through CCAS, which is computed using prompt embeddings from a sentence transformer. CCAS ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes.

Result: The approach improves object detection accuracy when evaluated on challenging object categories.

Conclusion: The scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [88] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/abs/2505.09140)
*Zechao Guan,Feng Yan,Shuai Du,Lin Ma,Qingshan Liu*

Main category: cs.CV

TL;DR: A new model TopoDiT-3D is proposed, which integrates topological information for better 3D point cloud generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on local feature extraction but ignore global topological information, such as voids, which are essential for shape consistency and complex geometries.

Method: The method involves designing a bottleneck structure using Perceiver Resampler that incorporates topological information from persistent homology into feature learning and filters out redundant local features.

Result: TopoDiT-3D surpasses state-of-the-art models in visual quality, diversity, and training efficiency.

Conclusion: TopoDiT-3D highlights the significance of rich topological information in 3D point cloud generation and its cooperation with traditional local feature learning.

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [89] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/abs/2505.09155)
*Yichen Shi,Zhuofu Tao,Yuhao Gao,Li Huang,Hongyang Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.CV

TL;DR: Current MLLMs have difficulty understanding circuit schematics due to limited recognition capabilities. This is partly because of the lack of high-quality schematic-netlist training data. The proposed method introduces a novel net detection mechanism based on segmentation with high robustness, which recovers positional information and allows digital reconstruction of schematics. Additionally, AMSnet dataset is expanded to create AMSnet 2.0.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current MLLMs in understanding circuit schematics and overcome the challenges posed by insufficient high-quality schematic-netlist training data.

Method: The proposed method employs a novel net detection mechanism based on segmentation with high robustness. It also recovers positional information for circuit components and nets, enabling digital reconstruction of schematics.

Result: The expansion of AMSnet leads to AMSnet 2.0, which contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information. In contrast, AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.

Conclusion: The novel net detection mechanism and the creation of AMSnet 2.0 aim to improve the ability of MLLMs to understand circuit schematics.

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [90] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为DRRNet的四阶段架构，通过全局上下文特征提取模块、局部细节提取模块以及反向精炼模块等设计，在伪装目标检测任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的伪装目标检测方法在处理与背景颜色、纹理和形状相似的目标时存在困难，容易丢失边缘细节或受类似背景干扰。

Method: 设计了DRRNet，包含四个阶段：全场景特征提取模块获取全局伪装模式；局部细节提取模块补充微观结构信息；双表示生成模块融合全景和局部特征；解码器中的反向精炼模块进行两阶段逆向精炼以增强目标边界连续性和抑制背景干扰。

Result: 实验结果表明，DRRNet在基准数据集上显著超越了当前最先进的方法。

Conclusion: DRRNet有效地解决了伪装目标检测中的挑战，显著提升了检测性能，并且代码已开源。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [91] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/abs/2505.09178)
*Yitao Zhu,Yuan Yin,Zhenrong Shen,Zihao Zhao,Haiyu Song,Sheng Wang,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: The paper introduces UniCAD, a unified architecture that uses pre-trained vision models to efficiently handle 2D and 3D medical images with minimal parameters. It includes low-rank adaptation for efficiency and a plug-and-play modular design for task expansion. Experiments show superior performance and efficiency across 12 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of developing multi-task CAD systems due to growing complexity and lack of an open-source CAD platform in the medical imaging community.

Method: UniCAD leverages pre-trained vision models with two key innovations: (1) Efficiency - Low-rank adaptation strategy introducing only 0.17% trainable parameters; (2) Plug-and-Play - Modular architecture combining frozen foundation model with multiple experts for diverse tasks.

Result: Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency.

Conclusion: UniCAD establishes an open-source platform for sharing lightweight CAD experts, promoting equitable and efficient research ecosystem.

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [92] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/abs/2505.09188)
*Minjun Kim,Jaehyeon Choi,Jongkeun Lee,Wonjin Cho,U Kang*

Main category: cs.CV

TL;DR: 本文提供了一个关于零样本量化（ZSQ）方法的全面概述，定义了ZSQ问题，分类了现有方法，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法需要训练数据，但在许多实际场景中由于隐私、安全或法规限制而不可行，因此需要一种无需真实数据的量化方法。

Method: 作者首先正式定义了ZSQ问题并强调了关键挑战，然后根据数据生成策略将现有的ZSQ方法分为几类，并分析了它们的动机、核心思想和主要收获。

Result: 提供了对ZSQ方法及其最新进展的全面了解，指出了当前方法的局限性，并为未来研究提供了方向。

Conclusion: 本文是迄今为止首个深入调查ZSQ的论文，旨在推动该领域的发展。

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [93] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/abs/2505.09196)
*Tong Li,Lizhi Wang,Hansen Feng,Lin Zhu,Hua Huang*

Main category: cs.CV

TL;DR: In this paper, the authors explore a phenomenon in low-light image enhancement (LLIE) called the gene effect, where resetting certain parameters to random values can improve enhancement performance. They propose a solution named parameter dynamic evolution (PDE), inspired by biological evolution, which employs techniques analogous to gene mutation and recombination.


<details>
  <summary>Details</summary>
Motivation: The motivation is the observation of a peculiar phenomenon in LLIE models, termed the gene effect, where random parameter resettings can outperform learned parameters for some images, limiting model performance.

Method: The proposed method, parameter dynamic evolution (PDE), addresses the gene effect by simulating biological evolution processes. It uses a parameter orthogonal generation technique to implement concepts similar to gene recombination and mutation, allowing adaptation to different images.

Result: Experiments validate the effectiveness of PDE in mitigating the gene effect and improving enhancement performance.

Conclusion: The authors conclude that PDE successfully adapts to varying images and alleviates the gene effect, with plans to release the code publicly.

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [94] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/abs/2505.09251)
*Vineetha Joy,Aditya Anand,Nidhi,Anshuman Kumar,Amit Sethi,Hema Singh*

Main category: cs.CV

TL;DR: A surrogate model using CNN with Huber loss function is proposed for predicting EM responses of metasurface-based RAS, achieving high accuracy and significant reduction in computational time.


<details>
  <summary>Details</summary>
Motivation: Metasurface-based radar absorbing structures (RAS) are crucial for applications like stealth technology and EM shielding. However, conventional design and optimization methods using full wave simulation tools are computationally intensive, time consuming, and require exploration of large design spaces.

Method: The authors propose a surrogate model based on convolutional neural network (CNN) with Huber loss function to predict the reflection characteristics of multi-layered metasurface-based RAS. The model is trained within 1000 epochs.

Result: The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001. It significantly reduced computational time while maintaining high predictive accuracy, as demonstrated by full wave simulations and experiments.

Conclusion: The surrogate model using CNN with Huber loss function successfully accelerates the prediction of EM responses of metasurface-based RAS, offering an efficient alternative to conventional methods.

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [95] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/abs/2505.09252)
*Yinuo Wang,Yue Zeng,Kai Chen,Cai Meng,Chao Pan,Zhouping Tang*

Main category: cs.CV

TL;DR: This study compares zero-shot multi-modal large language models (MLLMs) with traditional deep learning methods in identifying intracranial hemorrhage (ICH) subtypes from non-contrast computed tomography scans. Although MLLMs are inferior in accuracy, they offer enhanced interpretability through interactive capabilities.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely identification of ICH subtypes on non-contrast computed tomography is crucial for prognosis prediction and therapeutic decision-making, but it remains challenging due to low contrast and blurring boundaries.

Method: The study utilized a dataset provided by RSNA, comprising 192 NCCT volumes. It compared various MLLMs (GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2) with conventional deep learning models (ResNet50 and Vision Transformer). Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation.

Result: Traditional deep learning models outperformed MLLMs comprehensively in the ICH binary classification task. For subtype classification, MLLMs also exhibited inferior performance compared to traditional deep learning models.

Conclusion: MLLMs have lower overall accuracy in ICH subtyping compared to deep networks, but they enhance interpretability through language interactions, showing potential in medical imaging analysis.

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [96] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/abs/2505.09256)
*Jaemin Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

TL;DR: This paper introduces Pose-TTA, a novel approach that improves face recognition by aligning faces during the inference phase without additional training. It uses a portrait animator to transfer identities and proposes a weighted feature aggregation strategy to reduce distortions. Experiments show consistent performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing face recognition either rely on training with frontalised images or learning pose-invariant representations, both of which require re-training and testing for each dataset and involve significant effort.

Method: Pose-TTA employs a portrait animator to transfer the identity from a source image to the pose of a driving image at inference time. It also proposes a weighted feature aggregation strategy to handle distortions from synthetic data.

Result: Extensive experiments across diverse datasets and pre-trained models demonstrate that Pose-TTA consistently enhances inference performance in face recognition tasks.

Conclusion: Pose-TTA is an effective method to improve face recognition performance without the need for retraining or fine-tuning underlying models, making it easy to integrate into existing pipelines.

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [97] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

TL;DR: Anomaly detection faces challenges due to scarce anomaly samples. Existing methods using synthetic anomalies have a semantic gap with real-world ones. This paper proposes AnoGen, a few-shot Anomaly-driven Generation method that uses a diffusion model and only a few real anomalies to generate realistic anomalies. It involves three stages: learning anomaly distribution, guiding the diffusion model for generation, and training an anomaly detection model. Experiments on MVTec dataset show improved performance in both classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is crucial but difficult because of the lack of anomaly samples. Current approaches to synthesize anomalies do not bridge the semantic gap with real-world anomalies effectively.

Method: The AnoGen method consists of three stages: (1) Learning the anomaly distribution from a few real anomalies and embedding the knowledge; (2) Guiding the diffusion model using the embedding and bounding boxes to generate realistic anomalies on specific objects/textures; (3) Training a weakly-supervised anomaly detection model using the generated anomalies.

Result: Experiments on the MVTec dataset demonstrate simultaneous improvements in both anomaly classification and segmentation tasks. Notably, DRAEM and DesTSeg achieved 5.8% and 1.5% improvements in AU-PR metric on the segmentation task respectively.

Conclusion: The proposed AnoGen method successfully generates realistic and diverse anomalies using a few real samples, improving the performance of anomaly detection models in both classification and segmentation tasks.

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [98] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: The paper presents OneNIP, a method for anomaly detection that reconstructs normal features and restores anomaly features using one normal image prompt. It also introduces a supervised refiner to improve pixel-level anomaly segmentation.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised reconstruction models may perfectly reconstruct both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Also, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space.

Method: OneNIP reconstructs normal features and restores anomaly features with just one normal image prompt. Additionally, a supervised refiner is proposed that regresses reconstruction errors by using both real normal and synthesized anomalous images.

Result: OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA.

Conclusion: OneNIP effectively boosts unified anomaly detection performance and significantly improves pixel-level anomaly segmentation.

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [99] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: This paper proposes MetaUAS, a one-prompt Meta-learning framework for universal anomaly segmentation using a pure vision model without pre-trained visual-language models. It significantly outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: Current zero- and few-shot visual anomaly segmentation methods rely on powerful vision-language models that use manually designed textual prompts, but visual representations are inherently independent of language.

Method: The authors present a novel paradigm unifying anomaly segmentation into change segmentation, propose the MetaUAS framework trained on large-scale synthetic image pairs, and introduce a soft feature alignment module to handle geometrical variations.

Result: MetaUAS effectively and efficiently segments any anomalies with only one normal image prompt and outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods.

Conclusion: This work demonstrates the potential of a pure visual foundation model for universal visual anomaly segmentation and provides an alternative to widely used vision-language models.

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [100] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/abs/2505.09274)
*Fares Bougourzi,Abdenour Hadid*

Main category: cs.CV

TL;DR: Medical imaging segmentation faces challenges like data accessibility, annotation complexity, and domain adaptation. Recent advancements in Generative AI, Few-Shot Learning, Foundation Models, and Universal Models provide promising solutions. This survey reviews these methodologies, their applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in medical image segmentation such as data accessibility, annotation complexity, structural variability, and privacy constraints.

Method: Exploration of cutting-edge advancements including Generative AI, Few-Shot Learning, Foundation Models, and Universal Models for medical image segmentation.

Result: Provided a comprehensive overview of theoretical foundations, state-of-the-art techniques, and recent applications of these methods.

Conclusion: Discussed inherent limitations, unresolved issues, and outlined future research directions to enhance practicality and accessibility of segmentation models.

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [101] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

TL;DR: This paper presents a new dataset for predicting butterfly species presence from satellite data in the UK and develops a soft, supervised contrastive regularisation loss to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing availability of large-scale citizen-science wildlife observation data has led to interest in predicting multi-species presence directly from satellite images for scalable biodiversity monitoring.

Method: The authors experimentally optimize a Resnet-based model to predict multi-species presence from 4-band satellite images and develop a soft, supervised contrastive regularisation loss tailored to probabilistic labels like species-presence data.

Result: The Resnet-based model outperforms the mean rate baseline particularly for locations with high species biodiversity and the contrastive regularisation method improves prediction accuracy.

Conclusion: The new dataset and contrastive regularisation method contribute towards accurately predicting species biodiversity from remote sensing data which is crucial for efficient biodiversity monitoring.

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [102] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

TL;DR: An ROI-based neural video compression model using 2D Gaussian Splatting is proposed, which speeds up encoding time by 88% through a content-aware initialization strategy and a novel Gaussian inter-frame redundancy-reduction mechanism.


<details>
  <summary>Details</summary>
Motivation: Traditional video codecs have been used for decades, but recent advancements in deep learning-based techniques offer better adaptability and higher compression efficiency. However, the computational demands of neural video codecs limit their use in real-time applications like video conferencing.

Method: The researchers developed a region-of-interest (ROI) based neural video compression model leveraging 2D Gaussian Splatting. They introduced a content-aware initialization strategy and a new Gaussian inter-frame redundancy-reduction mechanism to speed up the encoding process.

Result: This approach reduces encoding time by 88% compared to previous Gaussian splatting-based image codecs, making it feasible for video codec solutions.

Conclusion: This work presents an innovative solution in the neural video codec field, offering significant improvements in encoding speed while maintaining quality, thus expanding potential applications in video streaming platforms.

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [103] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

TL;DR: 通过自监督学习开发可扩展的医学视觉基础模型，研究了模型大小、训练算法、数据大小和成像方式的扩展行为，并引入了包含2100万张生物医学图像的大规模数据集BioVFM-21M。提出的大规模医学视觉基础模型BioVFM在12个医学基准上超越了先前的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管在通用任务的扩展行为上有广泛的研究，但医学图像与自然数据有显著差异，缺乏对医学领域扩展行为的广泛理解，因此需要探索开发可扩展医学视觉基础模型的关键因素。

Method: 通过自监督学习探索跨模型大小、训练算法、数据大小和成像模态的扩展行为，介绍了一个大规模生物医学图像数据集BioVFM-21M，涵盖了广泛的生物医学图像模态和解剖结构。

Result: 观察到扩展确实提供了好处，但因任务而异；提出了一个预训练的大规模医学视觉基础模型BioVFM，它在12个医学基准上超越了以前最先进的基础模型。

Conclusion: 扩展对于追求更好的性能是有益的，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医学基础模型的关键考虑因素。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [104] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: The paper introduces MultiviewVLM, a vision-language model for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. It outperforms existing methods and can be adapted to various real-world applications.


<details>
  <summary>Details</summary>
Motivation: To create a model that can effectively learn representations of facial emotions from 3D/4D data in an unsupervised manner, capturing shared information across multi-views without explicit supervision.

Method: The method integrates pseudo-labels from generated textual prompts, proposes a joint embedding space for multiview alignment, employs a novel multiview contrastive learning strategy with stable positive-negative pair sampling, and uses a gradient-friendly loss function for optimization.

Result: Extensive experiments show that MultiviewVLM outperforms existing state-of-the-art methods in unsupervised contrastive multiview representation learning of facial emotions.

Conclusion: MultiviewVLM is a successful vision-language model for unsupervised learning of facial emotion representations from 3D/4D data, and it can be easily adapted to various real-world applications.

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [105] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

TL;DR: Marigold，一种从预训练的潜在扩散模型中提取知识并将其适应于密集图像分析任务的方法，具有最少的架构修改和强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在计算机视觉方面取得了成功，但在数据稀缺的情况下，高质量的预训练模型对于有效的迁移学习至关重要。当前主要依赖图像分类和自监督学习进行预训练，但文本到图像生成模型展现出了对视觉世界的深刻理解，这为新的预训练方法提供了可能性。

Method: 提出了一种名为Marigold的条件生成模型家族及其微调协议，该方法从预训练的潜在扩散模型（如Stable Diffusion）中提取知识，并将其应用于密集图像分析任务（如单目深度估计、表面法线预测和内在分解）。此方法仅需对预训练模型的架构进行少量修改，并使用小型合成数据集在单个GPU上进行训练。

Result: Marigold展示了最先进的零样本泛化能力，在密集图像分析任务中表现出色。

Conclusion: Marigold提供了一种有效利用预训练文本到图像生成模型知识的方法，适用于多种密集图像分析任务，且具有高效的训练需求和出色的零样本泛化性能。

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [106] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

TL;DR: The paper introduces RobustSpring, a new dataset and benchmark for evaluating the robustness of optical flow, scene flow, and stereo models to image corruptions. It applies 20 different corruptions to the Spring dataset, creating 20,000 challenging images. The benchmark includes a new metric for measuring corruption robustness and allows public evaluation of both accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for optical flow, scene flow, and stereo vision algorithms focus on model accuracy rather than robustness to real-world image corruptions like noise or rain.

Method: RobustSpring applies 20 different image corruptions in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, generating a suite of 20,000 corrupted images. A new corruption robustness metric is introduced for comparing model robustness.

Result: RobustSpring enables public two-axis evaluations of both accuracy and robustness via integration with the Spring benchmark. Initial model benchmarking shows that accurate models are not necessarily robust and robustness varies widely by corruption type.

Conclusion: RobustSpring is a new computer vision benchmark treating robustness as a first-class citizen, fostering the development of models that combine accuracy with resilience.

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [107] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/abs/2505.09372)
*Siyuan Yan,Xieji Li,Ming Hu,Yiwen Jiang,Zhen Yu,Zongyuan Ge*

Main category: cs.CV

TL;DR: Dermatological diagnosis is complex and requires combining visual features with clinical knowledge. Despite the advancements of vision-language pretraining (VLP) in medical AI, its application in dermatology is restricted by text length limitations and lack of structured texts. This paper presents MAKE, a Multi-Aspect Knowledge-Enhanced VLP framework for zero-shot dermatological tasks. It includes a multi-aspect contrastive learning strategy, a fine-grained alignment mechanism, and a diagnosis-guided weighting scheme. Pretrained on 403,563 image-text pairs, MAKE surpasses existing VLP models on various tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to overcome the limitations of current VLP models in dermatology, particularly addressing the issues of text length constraints and unstructured texts, which hinder effective dermatological diagnosis.

Method: The method involves introducing MAKE, a framework that incorporates three key components: a multi-aspect contrastive learning strategy that breaks down clinical narratives into sub-texts using large language models, a fine-grained alignment mechanism that links subcaptions with diagnostic image features, and a diagnosis-guided weighting scheme that prioritizes sub-captions based on their clinical significance.

Result: MAKE significantly outperforms state-of-the-art VLP models across eight datasets on tasks such as zero-shot skin disease classification, concept annotation, and cross-modal retrieval.

Conclusion: The authors conclude that MAKE effectively addresses the challenges posed by dermatological diagnosis through its innovative approach, setting a new benchmark for zero-shot dermatological tasks.

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [108] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/abs/2505.09379)
*Ali Rida Sahili,Najett Neji,Hedi Tabia*

Main category: cs.CV

TL;DR: 文本驱动的运动生成提供了一种强大且直观的方法，可以直接从自然语言创建人类动作。本文回顾了传统的运动合成视角，并对现代文本到运动生成方法进行了全面和结构化的调查。此外，还探讨了最常用的数据库、评估方法和最近的基准测试。


<details>
  <summary>Details</summary>
Motivation: 通过文本驱动的运动生成，可以提供一种强大且直观的方法来直接从自然语言创建人类动作，这种方法不需要预定义的运动输入，提供了灵活且易访问的动画角色控制方式。

Method: 从架构和运动表示两个互补的角度对现代文本到运动生成方法进行分类和调查，包括基于VAE、扩散和混合模型的方法，以及离散和连续运动生成策略的区别。

Result: 本文对文本到运动生成领域进行了全面的回顾，明确了当前的研究状态，指出了关键挑战和局限性，并强调了未来探索的有希望的方向。

Conclusion: 这项工作为研究人员和从业者提供了一个有价值的起点，以推动语言驱动的人类运动合成的边界。

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [109] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

TL;DR: NeoMedSys, a radiology software platform, was evaluated for three months in real-world clinical settings. It enabled iterative improvements in an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage detection, significantly enhancing its diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the feasibility and effectiveness of running NeoMedSys in real-world clinical settings and to improve the performance of VIOLA-AI, an in-house developed AI model for intracranial hemorrhage detection.

Method: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. The study used clinical cases from two sites in Norway to assess ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining.

Result: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. The classification sensitivity rose to 90.3% (from 79.2%), and specificity reached 89.3% (from 80.7%). The AUC for bleed detection improved to 0.949 (from 0.873).

Conclusion: The study concludes that NeoMedSys can enable efficient deployment and refinements of AI models in radiology, highlighting the value of real-time radiologist feedback.

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [110] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

TL;DR: Federated semantic segmentation is improved by a new framework FedSaaS which addresses class-consistency representation problem, enhancing average segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing federated semantic segmentation research often neglects fine-grained class relationships within the semantic space, leading to ambiguities in class representation when dealing with heterogeneous problems like domain shift.

Method: The paper proposes FedSaaS, a novel federated segmentation framework that uses class exemplars as a criterion for both local- and global-level class representations. On the server side, class prototypes are modeled from uploaded exemplars to supervise the global branch of clients. On the client side, an adversarial mechanism harmonizes contributions of global and local branches. Multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space.

Result: Extensive experiments on driving scene segmentation datasets show that the proposed framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy.

Conclusion: FedSaaS effectively addresses the class-consistency representation problem in federated semantic segmentation, offering a significant improvement in performance.

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [111] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/abs/2505.09406)
*Yue Wen,Liang Song,Yijia Liu,Siting Zhu,Yanzi Miao,Lijun Han,Hesheng Wang*

Main category: cs.CV

TL;DR: This paper proposes FreeDriveRF, a method for reconstructing dynamic driving scenes using only sequential RGB images without requiring poses inputs. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss to better constrain the dynamic modeling process.


<details>
  <summary>Details</summary>
Motivation: Dynamic scene reconstruction for autonomous driving is crucial for vehicles to perceive and interpret complex scene changes more precisely. However, existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity.

Method: The proposed method, FreeDriveRF, reconstructs dynamic driving scenes using only sequential RGB images. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss utilizing optical flow. Additionally, it incorporates estimated dynamic flow to constrain the pose optimization process.

Result: Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of FreeDriveRF in dynamic scene modeling for autonomous driving.

Conclusion: FreeDriveRF successfully reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs, showing superior performance in dynamic scene modeling for autonomous driving.

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [112] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/abs/2505.09413)
*Ma Changfeng,Bi Ran,Guo Jie,Wang Chongjun,Guo Yanwen*

Main category: cs.CV

TL;DR: This paper introduces a new point cloud rendering method that predicts 2D Gaussians from point clouds, which can generalize to multiple datasets and categories without needing categorical priors, dense point clouds, or additional refinements.


<details>
  <summary>Details</summary>
Motivation: Current learning-based methods for rendering rely on categorical priors, dense point clouds, or extra refinements, which limits their flexibility and generalization ability.

Method: The method uses two identical modules with an entire-patch architecture. These modules normalize and initialize 2D Gaussians based on point cloud information (normals, colors, distances). Splitting decoders then refine these initial Gaussians by duplicating them for more accurate predictions. This approach accommodates sparse point clouds effectively.

Result: Extensive experiments on various datasets show that this method outperforms existing techniques in terms of superiority and generalization, achieving state-of-the-art (SOTA) performance.

Conclusion: The proposed method provides a novel way to render point clouds using 2D Gaussian prediction, offering direct generalization across different categories without requiring dense point clouds or additional image refinements.

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [113] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/abs/2505.09415)
*Hongyang Wang,Yichen Shi,Zhuofu Tao,Yuhao Gao,Liepiao Zhang,Xun Lin,Jun Feng,Xiaochen Yuan,Zitong Yu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出FaceShield，一种用于面部防欺骗的多模态大语言模型，包括预训练和监督微调数据集。该模型在四个面部防欺骗任务中显著优于以前的深度学习模型和通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有的面部防欺骗方法将任务视为分类问题，缺乏可解释性和预测结果背后的推理。虽然多模态大语言模型（MLLMs）在视觉任务中表现出强大的感知、推理和决策能力，但目前还没有专门为面部防欺骗任务设计的通用和全面的MLLM和数据集。

Method: 提出了FaceShield，一个专门用于面部防欺骗的MLLM，以及相应的预训练和监督微调数据集FaceShield-pre10K和FaceShield-sft45K。使用spoof-aware vision perception (SAVP)，结合原始图像和基于先验知识的辅助信息，并采用prompt-guided vision token masking (PVTM)策略来随机遮蔽视觉token，从而提高模型的泛化能力。

Result: 在三个基准数据集上进行了广泛的实验，结果表明FaceShield在四个面部防欺骗任务（即粗粒度分类、细粒度分类、推理和攻击定位）中显著优于以前的深度学习模型和通用MLLMs。

Conclusion: FaceShield为面部防欺骗任务提供了一个新的解决方案，具有判断面部真实性、识别欺骗攻击类型、提供推理依据和检测攻击区域的能力。

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [114] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2505.09422)
*Xiangyuan Peng,Yu Wang,Miao Tang,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 提出了一种名为MoRAL的运动感知多帧4D雷达和LiDAR融合框架，通过Motion-aware Radar Encoder（MRE）补偿移动物体的帧间雷达错位，并利用Motion Attention Gated Fusion（MAGF）模块将雷达运动特征与LiDAR特征结合以聚焦动态前景物体。实验结果表明，MoRAL在View-of-Delft数据集上优于现有方法，在整体区域和驾驶走廊中的最高mAP分别为73.30%和88.68%，并对行人和骑自行车者有最佳检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多帧雷达点云的4D雷达和LiDAR融合方法虽然有效缩小了点密度差距，但常常忽略由于物体运动导致的帧间雷达点云错位问题，且未充分利用4D雷达提供的物体动态信息。

Method: 设计了一个运动感知雷达编码器（MRE），用于补偿来自移动物体的帧间雷达错位；然后采用运动注意门控融合（MAGF）模块，将雷达运动特征整合进来，引导LiDAR特征关注动态前景物体。

Result: 在View-of-Delft数据集上的广泛评估表明，MoRAL的表现优于现有方法，整体区域的最高mAP为73.30%，驾驶走廊为88.68%。此外，对于行人的AP为69.67%，驾驶走廊中骑自行车者的AP为96.25%。

Conclusion: 所提出的MoRAL框架显著提高了对动态前景物体的检测性能，特别是在自动驾驶系统所需的交通参与者准确检测方面表现出色。

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [115] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/abs/2505.09433)
*Jiahao Zhu,Kang You,Dandan Ding,Zhan Ma*

Main category: cs.CV

TL;DR: SerLiC is a serialization-based neural compression framework that transforms 3D LiDAR point clouds into 1D sequences for effective reflectance analysis, achieving significant volume reduction and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods.

Method: SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization. Each point is tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance. Mamba is incorporated with a dual parallelization scheme for efficient sequential modeling.

Result: Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. A lightweight version of SerLiC achieves > 10 fps with just 111K parameters.

Conclusion: SerLiC provides an effective approach to compress LiDAR reflectance data with significant volume reduction and high efficiency, making it attractive for real-world applications.

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [116] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

TL;DR: Endo-CLIP是一种新颖的自监督框架，通过三阶段方法（清理、调谐和统一）改进了对比语言-图像预训练（CLIP），以应对内窥镜图像分析中的挑战。该模型在零样本和少样本息肉检测和分类任务中显著优于现有最先进的预训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前的图像-文本结肠镜记录预训练方法存在非信息背景图像、复杂的医学术语和多病变描述模糊等问题，这些问题限制了内窥镜图像分析的性能提升。

Method: Endo-CLIP采用三阶段框架：1) 清理阶段 - 移除背景帧；2) 调谐阶段 - 利用大型语言模型提取临床属性用于细粒度对比学习；3) 统一阶段 - 使用患者级别的交叉注意力解决多息肉模糊问题。

Result: 实验表明，Endo-CLIP在零样本和少样本息肉检测与分类任务中显著超越了现有的最先进的预训练方法。

Conclusion: Endo-CLIP为更准确和临床上更相关的内窥镜分析铺平了道路。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [117] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/abs/2505.09450)
*Yuelin Zhang,Qingpeng Ding,Long Lei,Yongxuan Feng,Raymond Shing-Yan Tang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: MrTrack, an aspiration needle tracker with a mamba-based register mechanism for ultrasound-guided FNA biopsy, outperforms state-of-the-art trackers in accuracy, robustness, and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Ultrasound-guided fine needle aspiration (FNA) biopsy lacks an effective tracker for needles with rapid reciprocating motion. To address this gap, MrTrack is proposed.

Method: MrTrack uses a Mamba-based register extractor to distill global context from historical search maps and stores them in a register bank. A Mamba-based register retriever retrieves these temporal cues when current vision features are unusable due to rapid motion or imaging degradation. Additionally, a self-supervised register diversify loss encourages feature diversity and dimension independence within the learned register.

Result: Comprehensive experiments on motorized and manual aspiration datasets show that MrTrack surpasses state-of-the-art trackers in terms of accuracy, robustness, and inference efficiency.

Conclusion: MrTrack successfully addresses the challenge of tracking aspiration needles with rapid reciprocating motion in ultrasound-guided FNA biopsies, demonstrating superior performance.

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [118] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/abs/2505.09455)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.CV

TL;DR: This paper enhances spatio-temporal action detection (STAD) in soccer analytics by adding a denoising sequence transduction task, using game-level reasoning and Transformer-based models to improve precision and recall.


<details>
  <summary>Details</summary>
Motivation: Current STAD methods lack contextual understanding when operated in high-recall, low-precision regimes for soccer event extraction from broadcast videos, leading to many false positives that could be resolved by considering broader sequences of actions and game-state information.

Method: The method involves processing sequences of noisy, context-free player-centric predictions alongside clean game state information using a Transformer-based encoder-decoder model, modeling extended temporal context and reasoning jointly over team-level dynamics.

Result: This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video.

Conclusion: The enhancement through the addition of a denoising sequence transduction task complements existing pixel-based methods and generates 'denoised' sequences of actions.

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [119] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

TL;DR: Vision transformers use self-attention to capture long-range dependencies but existing position encoding techniques fail to capture semantic-aware positional relationships. This paper proposes 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) which dynamically adapts position representations leveraging local content, enhancing model's generalization and translation equivariance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing position encoding techniques that primarily focus on 1D linear position relationship and neglect semantic similarity between distant patches in images.

Method: Propose a novel position encoding method called 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates.

Result: Enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches.

Conclusion: Integrating SaPE^2 into vision transformers bridges the gap between position encoding and perceptual similarity, improving performance on computer vision tasks.

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [120] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2505.09484)
*Yingjie Ma,Xun Lin,Zitong Yu,Xin Liu,Xiaochen Yuan,Weicheng Xie,Linlin Shen*

Main category: cs.CV

TL;DR: This paper presents MMDA framework for Face Anti-Spoofing (FAS) which enhances cross-modal alignment generalization and multimodal detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current multimodal FAS methods face challenges in effective generalization due to modality-specific biases and domain shifts.

Method: The MMDA framework uses CLIP's zero-shot generalization capability, MD2A module for mitigating domain and modality noise, RS2 strategy for aligning multi-domain multimodal data into a generalized representation space, and U-DSA module for enhancing adaptability of representations while maintaining generalization performance.

Result: Experimental results on four benchmark datasets under different evaluation protocols show that the MMDA framework surpasses existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy.

Conclusion: The MMDA framework significantly improves generalization capabilities and complex representation abilities for FAS.

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [121] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Flash-VL 2B的新方法，用于优化视觉-语言模型(VLMs)，使其在不影响准确性的情况下实现超低延迟和高吞吐量。通过多项技术改进，Flash-VL 2B在多个基准测试中表现出色，是实时应用的理想选择。


<details>
  <summary>Details</summary>
Motivation: 随着对实时视觉-语言处理需求的增加，传统的VLMs无法满足超低延迟和高吞吐量的要求，因此需要一种新的优化方法来解决这一问题。

Method: Flash-VL 2B采用了一系列优化策略，包括定制架构设计、令牌压缩机制、数据整理、训练方案以及一种称为隐式语义拼接的新型图像处理技术。这些技术共同作用以减少处理时间并保持模型性能。

Result: 通过对11个标准VLM基准的广泛评估，Flash-VL 2B在速度和准确性上均达到了最先进的水平。

Conclusion: Flash-VL 2B为资源受限环境和大规模实时应用提供了一个有前景的解决方案，能够同时实现超低延迟和高吞吐量。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [122] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/abs/2505.09528)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: The paper integrates conformal prediction with approximate posterior sampling to establish bounds on FRIQ metrics for imaging inverse problems, ensuring safety in applications like medical imaging.


<details>
  <summary>Details</summary>
Motivation: In imaging inverse problems, it's crucial to assess the closeness of the recovered image to the true image using FRIQ metrics. This is particularly vital in safety-critical domains such as medical imaging where misdiagnosis must be avoided.

Method: Combine conformal prediction with approximate posterior sampling to create bounds on FRIQ that hold up to a user-specified error probability.

Result: Successfully demonstrated the approach on image denoising and accelerated MRI problems.

Conclusion: This method provides reliable FRIQ bounds, contributing to safer imaging applications.

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [123] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: This paper explores the use of time event cameras for contact-free heart rate monitoring by extracting cardiac pulse signals from facial recordings using a CNN model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the advantages of event cameras, such as low latency and high temporal resolution, for remote physiological signal extraction, specifically heart rate monitoring.

Method: A supervised CNN model is trained end-to-end to extract cardiac signals from a 2D representation of the event stream generated by an event camera recording faces. Model performance is assessed based on heart rate accuracy.

Result: The model achieved an RMSE of 3.32 bpm when compared to a baseline model with an RMSE of 2.92 bpm using standard camera frames. Event frames at higher FPS (60 and 120) outperformed standard 30 FPS camera frames with RMSEs of 2.54 and 2.13 bpm respectively.

Conclusion: Event cameras effectively preserve physiological cardiac information in the facial region, showing potential for remote heart rate monitoring with improved accuracy at higher frame rates.

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [124] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/abs/2505.09562)
*Nicola Marinello,Simen Cassiman,Jonas Heylen,Marc Proesmans,Luc Van Gool*

Main category: cs.CV

TL;DR: This paper presents a new framework for 3D panoptic scene completion which includes Object Module and Panoptic Module to improve existing models.


<details>
  <summary>Details</summary>
Motivation: 3D panoptic scene completion is underexplored despite its importance in autonomous vehicles for path planning and decision-making.

Method: The authors propose a novel framework that integrates Object Module and Panoptic Module with existing 3D occupancy and scene completion methods. This approach leverages annotations in occupancy benchmarks to learn individual object shapes as a differentiable problem.

Result: The proposed method aims to advance the state of the art in 3D panoptic scene completion, although specific experimental results are not provided in the abstract.

Conclusion: This work introduces a new framework for 3D panoptic scene completion that extends current models using Object and Panoptic Modules.

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [125] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/abs/2505.09564)
*Anne-Marie Rickmann,Stephanie L. Thorn,Shawn S. Ahn,Supum Lee,Selen Uman,Taras Lysyy,Rachel Burns,Nicole Guerrera,Francis G. Spinale,Jason A. Burdick,Albert J. Sinusas,James S. Duncan*

Main category: cs.CV

TL;DR: This paper explores the use of foundation models to create accurate pseudo-labels for porcine cardiac CT segmentation without manual annotations, using a self-training approach that improves segmentation accuracy and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Cardiac image segmentation is crucial in many cardiac analysis tasks but deep learning advancements have been limited in pre-clinical imaging, particularly with porcine models. The anatomical differences between species lead to domain shifts complicating model transfer from human to pig data.

Method: The researchers investigate whether foundation models can generate accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels without manually annotated pig data.

Result: The self-training process enhances segmentation accuracy and smooths out temporal inconsistencies across consecutive frames.

Conclusion: While the results are encouraging, there is still room for improvement by incorporating more sophisticated self-training strategies and exploring additional foundation models and other cardiac imaging technologies.

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [126] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: 研究统一图像理解与生成的框架，探索自回归和扩散模型在多模态设置中的应用，提出使用扩散Transformer生成CLIP图像特征的新方法，以及分阶段预训练策略，并开发了高质量数据集BLIP3o-60k和模型BLIP3-o，开源所有资源。


<details>
  <summary>Details</summary>
Motivation: 尽管图像理解的设计选择已被广泛研究，但结合图像生成的统一框架的最佳模型架构和训练方法仍需深入探索。自回归和扩散模型因其高质量生成能力和可扩展性成为研究重点。

Method: 1. 使用扩散Transformer生成语义丰富的CLIP图像特征，而非传统的VAE-based表示。
2. 采用顺序预训练策略：先进行图像理解训练，再进行图像生成训练。
3. 创建高质量指令调优数据集BLIP3o-60k，涵盖多种场景、对象和人类姿态等。
4. 基于上述方法开发BLIP3-o系列多模态模型。

Result: BLIP3-o在多个流行基准测试中表现出色，涵盖图像理解和生成任务。证明了新方法在提高训练效率和生成质量方面的有效性。

Conclusion: 本研究通过创新的模型设计、训练方法和数据集，推动了统一多模态模型的发展，并全面开放源代码和资源以促进未来研究。

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [127] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/abs/2505.09571)
*Guillermo Gomez-Trenado,Pablo Mesejo,Oscar Cordón,Stéphane Lathuilière*

Main category: cs.CV

TL;DR: The paper presents SAGE(Self-Attention Guidance for image Editing), a new technique using pre-trained diffusion models to edit images more efficiently and accurately. It solves the problem of poor reconstructions or high computation in current methods by incorporating a guidance mechanism based on self-attention layers. Through evaluations and user studies, SAGE proves superior in image editing.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods either require heavy computation or lead to poor reconstructions, making it difficult to achieve efficient and high-quality image editing.

Method: SAGE is built upon DDIM algorithm and introduces a novel guidance mechanism that uses the self-attention layers of the diffusion U-Net. This mechanism computes reconstruction objectives from attention maps during the inverse DDIM process, allowing efficient reconstruction of unedited regions without needing to reconstruct the entire input image.

Result: SAGE outperforms other methods in both quantitative and qualitative evaluations. In a user study with 47 participants, all preferred SAGE over competing methods. Additionally, SAGE ranks first in seven, second in two, and third in one out of ten quantitative analyses.

Conclusion: SAGE successfully addresses the challenges in image editing with its innovative use of self-attention guidance, demonstrating superior performance compared to existing methods.

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [128] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

TL;DR: The paper proposes a Variational VQA approach to improve calibration and reliability of multimodal models in Visual Question Answering without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Multimodal models for Visual Question Answering (VQA) have major reliability concerns as they can be overconfident and miscalibrated, particularly in out-of-distribution settings. Existing solutions mainly focus on unimodal models.

Method: The authors employ a variational algorithm called IVON instead of AdamW for fine-tuning vision-language models, yielding a posterior distribution over model parameters.

Result: The Variational VQA approach improves calibration and abstentions without losing the accuracy of AdamW. It reduces Expected Calibration Error by more than 50% compared to AdamW baseline and raises Coverage by 4% vs. SOTA (for a fixed risk of 1%). In OOD settings with 50% test cases being OOD, it achieves 8% Coverage improvement vs. SOTA (@ 1% risk).

Conclusion: Variational learning is presented as a feasible method to enhance the reliability of multimodal models.

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [129] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/abs/2505.09608)
*Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: This paper introduces a diffusion-based method for precise control over light sources in an image, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing relighting methods either rely on multiple input views or fail to provide explicit control over light changes.

Method: The method fine-tunes a diffusion model on real raw photograph pairs and synthetically rendered images, leveraging the linearity of light to synthesize image pairs depicting controlled light changes.

Result: The model can achieve precise illumination changes with explicit control over light intensity and color, and achieves compelling light editing results that outperform existing methods based on user preference.

Conclusion: The presented diffusion-based method provides fine-grained, parametric control over light sources in an image, showing superior performance compared to existing methods.

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [130] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/abs/2505.09615)
*Yung-Hsuan Lai,Janek Ebbers,Yu-Chiang Frank Wang,François Germain,Michael Jeffrey Jones,Moitreya Chatterjee*

Main category: cs.CV

TL;DR: This paper addresses the challenge of Audio-Visual Video Parsing (AVVP) in a weakly-supervised setting, where training data lacks detailed annotations. It proposes a novel method called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV), which improves upon existing methods by considering inter-segment dependencies and reducing bias towards absent labels. The approach incorporates uncertainty estimation for pseudo-labels and uses feature mixup based training regularization. Empirical results demonstrate that UWAV outperforms state-of-the-art methods on two datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current weakly-supervised AVVP techniques, particularly their inability to effectively capture inter-segment dependencies and their bias towards predicting absent labels. This limits their performance in localizing both uni-modal and multi-modal events in videos.

Method: The proposed method, UWAV, introduces an innovative approach to generate segment-level pseudo-labels while factoring in the uncertainty associated with these labels. It also incorporates a feature mixup based training regularization technique to improve the training process.

Result: Empirical evaluations show that UWAV surpasses state-of-the-art methods in the AVVP task across multiple metrics on two different datasets, demonstrating its effectiveness and generalizability.

Conclusion: The paper concludes that the proposed UWAV method successfully overcomes the limitations of previous approaches by considering inter-segment dependencies and reducing bias. The incorporation of uncertainty estimation and feature mixup based training regularization leads to improved performance in weakly-supervised AVVP.

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [131] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales,Ayah Elsayed,Debbie Zhao,Filip Loncaric,Ainhoa Aguado,Mireia Masias,Gina Quill,Marc Ramos,Ada Doltra,Ana Garcia,Marta Sitges,David Marlevi,Alistair Young,Martyn Nash,Bart Bijnens,Oscar Camara*

Main category: cs.CV

TL;DR: The study introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium (LA), providing robust automated segmentations and conducting a comprehensive assessment of hemodynamic parameters.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional ultrasound analysis and improve understanding of left atrium hemodynamics using 4D Flow MRI, despite challenges such as low velocities, limited spatial resolution, and lack of dedicated computational frameworks.

Method: Development of an open-source computational framework tailored for 4D Flow MRI analysis in the LA, enabling qualitative and quantitative analysis of advanced hemodynamic parameters. The framework is tested on data from different centers and used to assess energy, vorticity, and pressure parameters across various disorders.

Result: The framework produces high-accuracy automated segmentations (Dice > 0.9 and Hausdorff 95 < 3 mm) even with limited training data. A comprehensive assessment of hemodynamic parameters reveals their potential as prognostic biomarkers.

Conclusion: The introduced computational framework offers a robust solution for analyzing 4D Flow MRI in the LA, advancing the study of hemodynamic parameters as potential prognostic biomarkers.

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [132] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke,Takashi Shibuya,Kengo Uchida,Koichi Saito,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 生成逼真的双人人体运动是具有挑战性的，尤其是对于超过典型训练序列长度的长时间交互。本文提出了Dyadic Mamba方法，利用状态空间模型（SSM）生成高质量的任意长度的双人人体运动，解决了长序列生成问题，并提出新的长期运动合成质量评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer的方法在短期双人运动合成方面表现良好，但在处理更长的序列时受到限制，因为位置编码方案存在固有的局限性。

Method: 引入了Dyadic Mamba方法，利用状态空间模型（SSM），通过简单有效的架构促进个体运动序列之间的信息流动，无需复杂的交叉注意力机制。

Result: Dyadic Mamba在标准短期基准上表现出色，在较长序列上显著优于基于transformer的方法，并且提出了一个新的长期运动合成质量评估基准。

Conclusion: 基于SSM的架构为从文本描述中进行长期双人人体运动合成提供了有希望的方向。

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [133] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: An effective and efficient medical image segmentation approach, BoundarySeg, is proposed to improve segmentation accuracy without unannotated data or extra computation.


<details>
  <summary>Details</summary>
Motivation: Obtaining large-scale medical data is challenging due to privacy regulations and data protection policies. Moreover, annotating medical images is time-consuming and costly as it requires domain experts to manually delineate anatomical structures. Semi-supervised methods have gained popularity for reducing annotation costs, but they heavily depend on the availability of unannotated data.

Method: BoundarySeg, a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision.

Result: This strategy improves segmentation accuracy, especially in low data regimes, achieving performance comparable to or exceeding state-of-the-art semi supervised approaches.

Conclusion: BoundarySeg can achieve better performance without relying on unannotated data or increasing computational demands.

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [134] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh,Isabel Funke,Micha Pfeiffer,Fiona Kolbinger,Hanna Maria Schmeiser,Juergen Weitz,Marius Distler,Stefanie Speidel*

Main category: cs.CV

TL;DR: The paper proposes a two-stage, text-conditioned diffusion-based method to synthesize surgical videos for under-represented classes in surgical video datasets, aiming to overcome data imbalance. The approach uses a 2D latent diffusion model and temporal attention layers to ensure spatial and temporal consistency respectively. A rejection sampling strategy is introduced to select the most suitable synthetic samples. The method enhances model performance on surgical action recognition and intra-operative event prediction.


<details>
  <summary>Details</summary>
Motivation: To address the issue of severe data imbalance in surgical video datasets that hinders the development of high-performing models.

Method: A unique two-stage, text-conditioned diffusion-based method is proposed. This method generates high-fidelity surgical videos by conditioning on text prompts and decoupling spatial and temporal modeling using a 2D latent diffusion model and temporal attention layers. Additionally, a rejection sampling strategy is used to select the most suitable synthetic samples.

Result: Incorporating synthetic videos from the proposed approach significantly improves model performance on downstream tasks such as surgical action recognition and intra-operative event prediction.

Conclusion: The authors successfully demonstrate that their method effectively addresses class imbalance in surgical video datasets and improves model performance on relevant tasks.

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [135] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee,Taylor Webb,Trevor Bihl,Keith Holyoak,Hongjing Lu*

Main category: cs.CV

TL;DR: This paper presents Probabilistic Schema Induction (PSI), a model that uses deep learning to perform analogical mapping over structured representations of visual concepts from limited examples, forming schemas. PSI outperforms controls and mimics human-like learning.


<details>
  <summary>Details</summary>
Motivation: To understand and replicate the human ability to learn new visual concepts from limited examples using structured representations and analogical mapping.

Method: Introduced PSI, a prototype model employing deep learning for analogical mapping over structured representations to form compositional concepts called schemas, with mechanisms weighing similarity and amplifying relevant relations.

Result: PSI produces human-like learning performance, outperforming models using unstructured feature vectors or weaker structured representations. Its success is attributed to increasing relational similarity and emphasizing distinguishing relations.

Conclusion: Structured representations and analogical mapping are crucial for modeling rapid human-like learning of compositional visual concepts, demonstrating how deep learning can be used in psychological models.

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [136] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin,Chenyang Wu,Penghui Huang,Yanyong Zhang,Yinian Mao,Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM is a large-scale 3DGS-based visual SLAM method with stereo cameras that introduces multi-modality pose estimation, feature-alignment warping constraints, continuous Gaussian Splatting submaps, and a structure refinement module for superior performance in large-scale outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Current methods using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) for visual SLAM require RGBD sensors and are mostly effective in indoor environments. The robustness of these methods in large-scale outdoor scenarios has not been adequately explored.

Method: The proposed LSG-SLAM uses a multi-modality strategy to estimate prior poses under large view changes, feature-alignment warping constraints to mitigate appearance similarity issues in rendering losses, continuous Gaussian Splatting submaps for scalability in large-scale scenarios, place recognition for loop detection between GS submaps, and a structure refinement module to enhance reconstruction quality after global optimization of camera poses and Gaussian points.

Result: LSG-SLAM demonstrates superior performance over existing Neural, 3DGS-based, and traditional approaches through extensive evaluations on the EuRoc and KITTI datasets.

Conclusion: LSG-SLAM effectively addresses the challenges of large-scale outdoor visual SLAM by integrating advanced techniques such as multi-modality pose estimation, feature alignment, scalable submaps, and structure refinement.

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [137] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP is a method for universal visual anomaly detection that builds on CLIP models with added adapters for visual, textual, and prompt-query processing, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for universal visual anomaly detection struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning which limits their flexibility.

Method: AdaptCLIP learns adaptive visual and textual representations alternately rather than jointly, incorporates comparative learning between query and normal image prompts using both contextual and aligned residual features, and adds three simple adapters to CLIP models: visual adapter, textual adapter, and prompt-query adapter.

Result: AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods.

Conclusion: AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset.

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [138] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin,Shaolei Liu,Manning Wang*

Main category: cs.CV

TL;DR: The paper proposes a new source-free domain adaptation (SFDA) framework that includes preadaptation, frequency prompt, and style-related layer fine-tuning to improve pseudo-label quality and model performance.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods have limitations in producing high-quality pseudo-labels and translating image styles effectively, leading to inefficiencies in model training under limited supervision.

Method: The proposed method introduces three key components: preadaptation for generating a preadapted model to enhance pseudo-labels, data-dependent frequency prompt for effective image style translation, and style-related layer fine-tuning for efficient target model training.

Result: Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks show that the proposed method outperforms existing state-of-the-art techniques.

Conclusion: The novel SFDA framework with preadaptation, frequency prompt, and style-related layer fine-tuning successfully addresses current challenges in SFDA, offering superior performance in medical imaging tasks.

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [139] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: This paper proposes VRU-CIPI, a framework using GRU and Transformer for predicting crossing intentions of Vulnerable Road Users (VRUs) at intersections. It achieves 96.45% accuracy and can be integrated with Infrastructure-to-Vehicles (I2V) communication to enhance intersection safety.


<details>
  <summary>Details</summary>
Motivation: To improve the interaction safety between road users, especially focusing on understanding and predicting the crossing intentions of Vulnerable Road Users (VRUs) at urban intersections.

Method: The VRU-CIPI framework uses Gated Recurrent Unit (GRU) for capturing temporal dynamics in VRU movements and a multi-head Transformer self-attention mechanism for encoding contextual and spatial dependencies.

Result: Evaluated on the UCF-VRU dataset, the proposed framework achieves state-of-the-art performance with an accuracy of 96.45% and real-time inference speed of 33 frames per second.

Conclusion: By integrating with Infrastructure-to-Vehicles (I2V) communication, the approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles.

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [140] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan,Lei Zhou,Liu Mao,Shaofan Chen,Chuanqiu Ren,Xia Xie*

Main category: cs.CV

TL;DR: The paper introduces a new remote sensing change detection task called non-registration change detection, outlines eight real-world scenarios leading to non-registration problems, devises image transformation schemes for these scenarios, and shows the negative impact on current methods.


<details>
  <summary>Details</summary>
Motivation: To tackle emergencies like natural disasters, anthropogenic accidents, and military strikes by proposing a novel remote sensing change detection task.

Method: Propose eight real-world scenarios for non-registration problems, develop tailored image transformation schemes, and demonstrate the catastrophic damage to state-of-the-art methods.

Result: Non-registration change detection causes significant issues to existing advanced methods.

Conclusion: This study highlights the challenges of non-registration change detection and provides resources (code and dataset) for further research.

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [141] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng,Kexuan Li,Xingye Cui,Jiaxuan Li,Chang Long,Tian Pu,Zhenming Peng*

Main category: cs.CV

TL;DR: 提出了一种新的网络CSPENet用于红外小目标检测，该网络通过提取和嵌入轮廓感知和显著性先验来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的红外小目标检测方法在定位微弱目标和在密集杂波环境中感知轮廓信息方面存在不足，限制了检测性能。

Method: 设计了一个包围收敛先验提取模块（SCPEM），提取增强的显著性先验和多尺度结构先验；提出了双分支先验嵌入架构（DBPEA），以最优网络位置嵌入这两种先验；开发了注意力引导特征增强模块（AGFEM）来细化特征表示并提高显著性估计的准确性。

Result: 在NUDT-SIRST、IRSTD-1k和NUAA-SIRST公共数据集上的实验结果表明，CSPENet的检测性能优于其他最先进的方法。

Conclusion: CSPENet通过结合轮廓感知和显著性先验，在红外小目标检测任务中取得了优异的表现，并且代码已公开。

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [142] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang,Tao Tan,Shuai Tan,Weiqin Yang,Kunyan Cai,Calvin Chen,Yue Sun*

Main category: cs.CV

TL;DR: The paper presents MambaControl, a new framework that integrates selective state-space modelling with diffusion processes for predicting medical image trajectories in disease progression. It combines Mamba-based long-range modelling with graph-guided anatomical control and introduces Fourier-enhanced spectral graph representations to achieve high accuracy in Alzheimer's disease prediction.


<details>
  <summary>Details</summary>
Motivation: Current methods for modelling disease progression have difficulty handling longitudinal dependencies and maintaining structural consistency in progressive disorders. This necessitates a more effective approach to capture complex spatio-temporal dynamics while preserving anatomical integrity.

Method: MambaControl integrates selective state-space modelling with diffusion processes. It uses Mamba-based long-range modelling combined with graph-guided anatomical control to represent anatomical correlations. Additionally, it incorporates Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail.

Result: Quantitative and regional evaluations show improved progression prediction quality and anatomical fidelity in the context of Alzheimer's disease prediction, demonstrating state-of-the-art performance.

Conclusion: MambaControl offers significant improvements in predicting disease progression with high anatomical fidelity, showcasing potential for personalized prognosis and clinical decision support.

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [143] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: A novel framework focusing on Texture Key Driver Factors (TKDF) for Facial Expression Recognition (FER) is introduced, which includes a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF). This method achieves state-of-the-art performance in FER tasks.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition in the wild remains challenging due to subtle expression-related features and complex variations in facial appearance.

Method: The proposed framework identifies Texture Key Driver Factors (TKDF), localized texture regions with strong discriminative power across emotional categories. It uses a Texture-Aware Feature Extractor (TAFE) with ResNet-based backbone enhanced with multi-branch attention to extract fine-grained texture representations, and Dual Contextual Information Filtering (DCIF) to refine these features through adaptive pooling and attention mechanisms.

Result: Experimental results on RAF-DB and KDEF datasets show that the method achieves state-of-the-art performance.

Conclusion: The incorporation of TKDFs into FER pipelines proves effective and robust, significantly improving FER performance.

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [144] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao,Shaobo Xia,Sheng Nie,Cheng Wang,Xiaohuan Xi,Bisheng Yang*

Main category: cs.CV

TL;DR: APCoTTA是一种针对ALS点云语义分割的连续测试时适应(CTTA)方法，通过动态可训练层选择、熵基一致性损失和随机参数插值机制等创新手段，缓解了灾难性遗忘和误差累积问题。在新构建的ISPRSC和H3DC基准数据集上，APCoTTA相较于直接推理分别提升了约9%和14%的mIoU性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA研究在ALS点云领域有限，面临无标准化数据集、灾难性遗忘和误差累积等问题。为解决这些问题并提升模型在不同域上的适应能力，需要一种专门针对ALS点云语义分割的CTTA方法。

Method: 提出了一种名为APCoTTA的方法，包括：1）动态可训练层选择模块，利用梯度信息选择低置信度层进行训练以缓解灾难性遗忘；2）熵基一致性损失，仅对可靠样本施加一致性损失以减少误差累积；3）随机参数插值机制，将目标域适应与源知识保留平衡；4）构建了两个新基准数据集ISPRSC和H3DC。

Result: 实验结果表明，APCoTTA在ISPRSC和H3DC两个基准数据集上表现最佳，相较于直接推理分别实现了约9%和14%的mIoU提升。

Conclusion: APCoTTA是首个专为ALS点云语义分割设计的CTTA方法，有效解决了灾难性遗忘和误差累积问题，并在新建的ISPRSC和H3DC基准数据集上展现出优越性能。

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [145] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou,Yichong Xia,Sicheng Pan,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: An underwater image compression algorithm HQUIC is developed, which leverages underwater-image-specific features for better performance.


<details>
  <summary>Details</summary>
Motivation: Current underwater image compression algorithms do not fully utilize the unique characteristics of underwater images, leading to suboptimal performance.

Method: HQUIC uses an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamically weights multi-scale frequency components.

Result: HQUIC shows superior performance compared to state-of-the-art compression methods on various underwater datasets.

Conclusion: HQUIC is an effective solution for underwater image compression that takes advantage of underwater-image-specific features.

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [146] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng,Jiafei Duan,Yi Ru Wang,Haoquan Fang,Boyang Li,Yushan Huang,Elvis Wang,Ainaz Eftekhar,Jason Lee,Wentao Yuan,Rose Hendrix,Noah A. Smith,Fei Xia,Dieter Fox,Ranjay Krishna*

Main category: cs.CV

TL;DR: Pointing is crucial for grounding language in visual contexts. PointArena evaluates multimodal pointing through Point-Bench, Point-Battle, and Point-Act. Molmo-72B leads but proprietary models are catching up. Supervised training boosts performance.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive platform for evaluating multimodal pointing capabilities across diverse reasoning scenarios, addressing the limitation of existing benchmarks focusing only on referential object localization tasks.

Method: Introduced PointArena comprising Point-Bench (1,000 pointing tasks), Point-Battle (interactive pairwise model comparisons with 4,500 votes), and Point-Act (real-world robotic manipulation system). Evaluated state-of-the-art open-source and proprietary multimodal models.

Result: Molmo-72B outperforms other models, proprietary models show comparable performance. Supervised training improves pointing task performance. Strong correlations observed across evaluation stages.

Conclusion: Precise pointing capabilities are essential for enabling multimodal models to bridge abstract reasoning with real-world actions.

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [147] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang,Jiyeong Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: The paper introduces DITM, a method for descriptive image-text matching that explores language flexibility to improve learning of graded contextual similarity.


<details>
  <summary>Details</summary>
Motivation: Most existing image-text matching approaches use sparse binary supervision which limits their ability to cover the many-to-many relationships between images and texts. They also neglect implicit connections from general to specific descriptions.

Method: DITM formulates descriptiveness score with cumulative term frequency-inverse document frequency (TF-IDF) and leverages sentence descriptiveness in two ways: refining false negative labeling and building more precise matching by aligning sentences in a generic-to-specific order.

Result: Experiments on MS-COCO, Flickr30K, and CxC datasets show DITM's effectiveness in representing complex image-text relationships compared to state-of-the-art methods. It also enhances hierarchical reasoning as shown by analysis on HierarCaps benchmark.

Conclusion: DITM successfully moves beyond rigid binary supervision to enhance discovery of optimal matches and potential positive pairs, improving both image-text matching and hierarchical reasoning.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [148] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang,Yuanqi Hu,Xinyu Chen,Yuxia Xu,Suhui Wang,Chunan Yu,Lanyun Zhu,Deyi Ji,Xin Xu,Tianrun Chen*

Main category: cs.CV

TL;DR: In the age of immersive electronics, there's a growing interest in virtual fashion for identity expression. However, current 3D garment design tools are difficult for everyday users due to technical challenges and limited data. This paper presents a 3D sketch-driven framework that allows even inexperienced users to generate high-quality digital clothing via simple 3D sketches in AR/VR environments. The system uses a conditional diffusion model, a sketch encoder, and adaptive curriculum learning to interpret rough inputs and produce realistic garments. Additionally, they introduce KO3DClothes, a new dataset to tackle the lack of training data. Experiments show that this method surpasses existing ones in fidelity and usability, promoting accessible fashion design on future platforms.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to make 3D garment design more accessible to everyday users who want to express their identity through virtual fashion, overcoming the steep technical barriers and limited data that hinder current tools.

Method: The method involves creating a 3D sketch-driven 3D garment generation framework. It combines a conditional diffusion model with a sketch encoder trained in a shared latent space and an adaptive curriculum learning strategy to translate imprecise, free-hand input into realistic, personalized garments. A new dataset, KO3DClothes, consisting of paired 3D garments and user-created sketches, was also introduced to address the scarcity of training data.

Result: The results from extensive experiments and user studies indicate that the proposed method significantly outperforms existing baselines in terms of both fidelity and usability. This highlights the potential of the method to democratize fashion design on next-generation consumer platforms.

Conclusion: This work concludes by demonstrating the effectiveness of the 3D sketch-driven 3D garment generation framework in empowering ordinary users to create high-quality digital clothing. By introducing KO3DClothes and showing superior performance compared to existing methods, the study paves the way for more inclusive and accessible fashion design experiences.

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [149] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: An improved autonomous target detection network based on YOLOv8 is presented to overcome challenges in object detection for autonomous driving, achieving 65% detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current technologies for environmental perception in autonomous driving face challenges such as high costs, vulnerability to weather and lighting conditions, and limited resolution.

Method: The paper integrates structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework.

Result: The enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showing significant improvements over traditional methods.

Conclusion: This improved model has substantial potential for real-world applications, particularly excelling in scenarios involving single-target and small-object detection.

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [150] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution, demonstrating significant improvements in PSNR, SSIM, and LPIPS metrics.


<details>
  <summary>Details</summary>
Motivation: Super-resolution image reconstruction is crucial with the development of remote sensing technology. Current deep learning methods struggle with complex scenes and detail preservation.

Method: A reinforcement learning-based latent diffusion model (LDM) fine-tuning method is proposed. It constructs a reinforcement learning environment optimized by proximal policy optimization (PPO) during the reverse denoising process of the LDM model.

Result: Experiments on the RESISC45 dataset showed improvements over the baseline model: PSNR increased by 3-4dB, SSIM improved by 0.08-0.11, and LPIPS reduced by 0.06-0.10, especially in structured and complex natural scenes.

Conclusion: The proposed method effectively enhances super-resolution quality and demonstrates adaptability across various scenes.

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [151] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: This paper proposes a domain-adaptive one-shot landmark detection framework named Universal One-shot Detection (UOD) to handle multi-domain medical images. It consists of two stages and corresponding universal models, achieving state-of-the-art performances on three public X-ray datasets.


<details>
  <summary>Details</summary>
Motivation: One-shot medical landmark detection has label-efficient training process but existing methods suffer from domain preference when dealing with multi-domain unlabeled data and are not robust for sub-optimal image annotation.

Method: The UOD framework includes two stages: 1) A domain-adaptive convolution model is self-supervised learned to generate pseudo landmark labels; 2) A domain-adaptive transformer is designed to eliminate domain preference and build the global context for multi-domain data. The framework uses domain-specific and domain-shared modules.

Result: UOD was investigated both qualitatively and quantitatively on three widely-used public X-ray datasets in different anatomical domains (head, hand, chest) and achieved state-of-the-art performances in each domain.

Conclusion: The proposed UOD framework can effectively handle multi-domain medical images with only one annotated sample from each domain, providing more robust and accurate landmark detection.

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [152] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: 研究人员开发了DeepSeqCoco，一种基于深度学习的模型，用于从椰子树图像中准确且自动地识别疾病。该模型在不同的优化器设置下进行测试，结果表明其准确性高达99.5%，比现有模型高出5%。混合SGD-Adam优化器显示出最低的验证损失2.81%，同时训练和预测时间显著减少。这证明了通过AI为基础的可扩展且高效的疾病监测系统改进精确农业的潜力。


<details>
  <summary>Details</summary>
Motivation: 椰子树疾病对农业产量构成严重威胁，特别是在传统耕作方式限制早期诊断和干预的发展中国家。目前的疾病识别方法是手动的、劳动密集型的且不可扩展。

Method: 提出了DeepSeqCoco，这是一种基于深度学习的模型，可以从椰子树图像中准确且自动地识别疾病。该模型在各种优化器设置（如SGD、Adam和混合配置）下进行了测试，以找到准确性和计算成本之间的最佳平衡。

Result: 实验结果表明，DeepSeqCoco可以达到高达99.5%的准确率（比现有模型高出多达5%），其中混合SGD-Adam优化器显示出最低的验证损失2.81%。此外，与现有模型相比，训练时间减少了多达18%，预测时间减少了多达85%。

Conclusion: DeepSeqCoco模型展示了通过AI为基础的、可扩展且高效的疾病监测系统改进精确农业的潜力。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [153] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang,Boyang Zheng,Xichen Pan,Sayak Paul,Saining Xie*

Main category: cs.CV

TL;DR: This paper explores the design space of fusing LLMs and DiTs for text-to-image generation, offering comparisons, design insights, and a reproducible training recipe.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding the potential of deep fusion between LLMs and DiTs for multi-modal generation due to lack of detailed comparisons and undisclosed design details in previous studies.

Method: Conduct empirical study on text-to-image generation with controlled comparisons to established baselines, analysis of design choices, and provision of a reproducible large-scale training recipe.

Result: Provides meaningful data points and practical guidelines for future research in multi-modal generation.

Conclusion: This work aims to offer valuable insights and reproducible methods for advancing multi-modal generation research.

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [154] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan,Xuepu Zeng,Jing Zhang,Mingming Gong,Yuxiang Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: Dynamic scene representation and reconstruction have advanced significantly due to developments in neural radiance fields and 3D Gaussian splatting techniques, transitioning from static to dynamic environments. This survey reviews over 200 papers on this topic, categorizing approaches and identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is the transformative advances in dynamic scene representation and reconstruction, driven by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques, which have evolved from addressing static to dynamic environments.

Method: The method involves a systematic analysis of over 200 papers focused on dynamic scene representation using radiance fields, categorized through multiple critical lenses such as motion representation paradigms, reconstruction techniques, auxiliary information integration strategies, and regularization approaches.

Result: The result is a comprehensive overview of the field, organizing diverse methodological approaches under a unified representational framework, and providing insights into persistent challenges and promising research directions.

Conclusion: This survey concludes by establishing a definitive reference for researchers entering the field of dynamic scene reconstruction, offering both experienced practitioners and newcomers a systematic understanding of conceptual principles and practical frontiers.

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [155] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 本文评估了大型多模态模型在低资源Pashto语言上的OCR性能，开发了一个名为PsOCR的合成数据集，并对多个开源和闭源模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: Pashto语言的自然语言处理面临许多挑战，包括其连笔字体和缺乏结构化数据集。为了解决这些问题并评估现有模型的能力和局限性，作者创建了一个新的合成Pashto OCR数据集。

Method: 创建了一个包含一百万张图像的合成Pashto OCR数据集PsOCR，这些图像标注有单词、行和文档级别的边界框。该数据集涵盖了1000种独特的字体家族、颜色、图像大小和布局的变化。使用一个包含10K张图像的基准子集来评估几个LMMs的性能，包括七个开源模型和四个闭源模型。

Result: 实验结果显示Gemini在所有模型中表现最佳，而在开源模型中，Qwen-7B表现出色。

Conclusion: 这项工作提供了关于当前LMMs在Pashto OCR任务中的能力和局限性的深入评估，并为未来在Pashto OCR及其他类似脚本（如阿拉伯语、波斯语和乌尔都语）的研究奠定了基础。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [156] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju,Sheng-Yen Huang,Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB is an efficient two-stage framework that extends Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes. It generates stylized video in Stage 1 and learns stylized neutral head model and expression blendshapes in Stage 2.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to extend the widely used Toonify framework for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, overcoming the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN.

Method: The method involves a two-stage framework called ToonifyGB. In Stage 1, an improved StyleGAN is employed to generate stylized video from input video frames. In Stage 2, a stylized neutral head model and a set of expression blendshapes are learned from the generated video.

Result: ToonifyGB efficiently renders stylized avatars with arbitrary expressions and its effectiveness is validated on the benchmark dataset using two styles: Arcane and Pixar.

Conclusion: ToonifyGB provides an efficient way to synthesize diverse stylized 3D head avatars using Gaussian blendshapes.

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [157] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo,Xiaodong Gu*

Main category: cs.CV

TL;DR: The paper introduces MMRL and MMRL++, methods that enhance cross-modal interactions in Vision-Language Models using a shared representation space, reducing overfitting and improving generalization. Extensive experiments show they outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale pre-trained Vision-Language Models often suffer from overfitting when adapted with limited few-shot data, which undermines their ability to generalize to new tasks.

Method: MMRL proposes a shared, learnable, modality-agnostic representation space by inserting representation tokens into higher encoder layers. A regularization term aligns class and text features with the frozen VLM's zero-shot features. MMRL++ enhances this by significantly reducing trainable parameters and strengthening intra-modal interactions.

Result: Extensive experiments on 15 datasets demonstrate that both MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.

Conclusion: MMRL and MMRL++ are effective methods for multi-modal representation learning, promoting better generalization and reducing overfitting in Vision-Language Models.

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [158] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: MoB is a new method for visual token pruning which provides a closed-form error bound based on Hausdorff distance and uses ϵ-covering theory to balance two objectives, achieving high performance preservation with minimal tokens.


<details>
  <summary>Details</summary>
Motivation: Current visual token pruning methods use static strategies that do not account for the varying importance of objectives across different tasks, resulting in inconsistent performance.

Method: The method derives a closed-form error bound using Hausdorff distance to characterize both objectives of prompt alignment and visual preservation. It leverages ϵ-covering theory to reveal a trade-off between these objectives and proposes MoB (Multi-Objective Balanced Covering) to reformulate visual token pruning as a bi-objective covering problem.

Result: Experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B with only 11.1% of original visual tokens, and accelerates LLaVA-Next-7B by 1.3-1.5× with negligible performance loss. It also works well with Qwen2-VL and Video-LLaVA.

Conclusion: MoB offers a provable performance bound and linear scalability, making it adaptable to challenging pruning scenarios and integrating well into advanced MLLMs and diverse vision-language tasks.

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [159] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil,Lucas Robinet,Laurent Risser,Soleakhena Ken*

Main category: cs.CV

TL;DR: The paper proposes a novel image registration method using a conditional U-Net architecture to estimate unknown condition-related images in real-time without reconstruction artefacts for radiotherapy treatment.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of estimating unknown condition-related images based on known images and their conditions, particularly in the context of moving tumors for radiotherapy treatment with 4D-CT scans.

Method: A new conditional U-Net architecture is used to model the image registration formalism, fully incorporating conditional information without needing any fixed image. This approach is applied to stitch sequential 2D slices into several 3D volumes at different organ positions.

Result: The method successfully generates artefact-free volumes through real-time latencies when applied to 4D-CT clinical data in thoracoabdominal regions.

Conclusion: The proposed method offers a solution for complex image registration tasks, such as those encountered in radiotherapy treatment planning, providing accurate and real-time results.

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [160] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: 提出了一种多源协同风格增强与领域不变学习方法（MCSAD），通过交替进行协同风格增强和领域不变学习，使模型在未见目标领域上具有良好的泛化能力，并且在多个领域泛化数据集上的实验表明该方法显著优于现有的联邦领域泛化方法。


<details>
  <summary>Details</summary>
Motivation: 联合领域泛化的目标是从多个去中心化的源领域中学习一个可泛化的模型，用于部署到未见的目标领域。尽管风格增强方法在领域泛化方面取得了很大进展，但现有的风格增强方法要么探索孤立源领域的数据风格，要么在数据去中心化场景下跨现有源领域插值风格信息，这导致了风格空间有限。

Method: 为了解决上述问题，提出了多源协同风格增强与领域不变学习方法（MCSAD）。具体来说，提出了一种多源协同风格增强模块，以生成更广泛风格空间的数据。此外，通过跨域特征对齐和类别关系集成蒸馏，在原始数据和增强数据之间进行领域不变学习，从而学习到一个领域不变的模型。

Result: 通过交替进行协同风格增强和领域不变学习，该模型能够在未见目标领域上表现良好。广泛的实验结果表明，该方法在多个领域泛化数据集上显著优于现有的联合领域泛化方法。

Conclusion: 提出的MCSAD方法通过扩展风格空间和领域不变学习，有效提高了模型在未见目标领域的泛化性能，是一种先进的联合领域泛化方法。

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [161] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 尽管在图像显著性预测方面取得成功，但由于数据集偏差，跨多个显著性数据集预测注视点仍然具有挑战性。本文提出了一种新的架构，通过扩展几乎与数据集无关的编码器-解码器结构，并使用少于20个数据集特定参数来控制可解释机制，如多尺度结构、中心偏差和注视点分布，从而解决了这一问题。该模型在MIT/Tuebingen显著性基准的所有三个数据集上达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管当前图像显著性预测方法在现有基准上接近黄金标准性能水平，但在不同数据集之间应用模型时存在显著性能下降的问题（约40%），并且增加数据集多样性并不能解决这一跨数据集差距，约60%的差距归因于数据集特定偏差。因此，需要一种能够解决剩余泛化差距的方法。

Method: 提出了一种新型架构，该架构扩展了一个几乎与数据集无关的编码器-解码器结构，并引入了少于20个数据集特定参数，这些参数控制诸如多尺度结构、中心偏差和注视点分布等可解释机制。仅通过适应这些参数到新数据，可以弥补超过75%的泛化差距，且即使只有50个样本也能实现大部分改进。

Result: 该模型在MIT/Tuebingen显著性基准的所有三个数据集（MIT300、CAT2000和COCO-Freeview）上达到了新的最先进水平，即使在从不相关数据集纯泛化的情况下也是如此，但在适应各自训练数据集时性能有显著提升。此外，该模型还提供了有关空间显著性特性的宝贵见解，揭示了结合绝对和相对大小的复杂多尺度效应。

Conclusion: 所提出的模型通过引入少量数据集特定参数解决了显著性预测中的跨数据集泛化问题，并在多个基准数据集上取得了最佳性能，同时提供了对空间显著性特性的深入理解。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [162] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon,Ahmad AlMughrabi,Thanasis Zoumpekas,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: VolE is a novel framework leveraging mobile device-driven 3D reconstruction to accurately estimate food volume without reference or depth information, achieving 2.22% MAPE and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Accurate food volume estimation is essential for medical nutrition management and health monitoring but current methods are limited by mononuclear data, single-purpose hardware, sensor-oriented information, or reliance on camera calibration using a reference object.

Method: VolE captures images and camera locations in free motion using AR-capable mobile devices to generate precise 3D models. It uses food video segmentation for food mask generation and is a reference- and depth-free framework.

Result: VolE outperforms existing volume estimation techniques across multiple datasets with a 2.22% MAPE.

Conclusion: VolE presents a new approach to food volume estimation that is more accurate and less reliant on specific hardware or conditions.

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [163] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 通过MixUp和辅助傅里叶增强等方法，可以提高医学图像分割模型在真实临床环境中的泛化能力和鲁棒性。这些方法能够显著改善心脏电影MRI和前列腺MRI分割中对各种变换的鲁棒性，并通过提升特征表示的可分性和紧凑性来增强nnU-Net训练管道的可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型通常在精心策划的数据集上进行训练，在实际临床环境中部署时，由于训练和测试分布之间的差异，性能会下降。传统的数据增强技术缺乏应对多样化现实场景所需的鲁棒性。

Method: 系统评估了MixUp和辅助傅里叶增强等替代增强策略，这些方法不明确针对特定的分布变化来源，而是缓解多种变化的影响。将这些方法集成到nnU-Net训练管道中。

Result: 定量研究发现，这些增强方法通过促进可分性和紧凑性提高了学习到的特征表示的质量，从而显著改善了模型在各种变换下的泛化能力和鲁棒性。

Conclusion: MixUp和辅助傅里叶增强等方法为提高医学分割模型在实际应用中的可靠性提供了一个易于实现且有效的解决方案。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [164] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: Deep neural networks in medical imaging can have biases causing fairness gaps. This study explores Human-AI alignment and fairness, finding that human insights reduce these gaps and improve generalization, but excessive alignment may compromise performance. A balanced strategy is needed for fair and robust medical AI systems.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the impact of Human-AI alignment on fairness and out-of-domain generalization in medical imaging, where deep neural networks are effective but prone to biases leading to fairness gaps across demographic groups.

Method: Incorporating human insights into AI models in the medical imaging domain to examine how it affects fairness gaps and out-of-domain generalization. The study also investigates potential trade-offs from excessive alignment.

Result: Human insights consistently reduce fairness gaps and enhance out-of-domain generalization in medical AI systems. However, excessive alignment can introduce performance trade-offs, indicating the importance of calibrated strategies.

Conclusion: Human-AI alignment offers a promising approach for developing fair, robust, and generalizable medical AI systems, requiring a balance between expert guidance and automated efficiency.

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [165] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: The paper introduces MTVCrafter, a framework using 4D motion tokens for human image animation that surpasses existing methods relying on 2D-rendered pose images.


<details>
  <summary>Details</summary>
Motivation: Existing human image animation methods depend heavily on 2D-rendered pose images, limiting generalization and losing essential 3D information.

Method: MTVCrafter proposes the use of 4DMoT to quantize 3D motion sequences into 4D motion tokens and MV-DiT with unique motion attention for effective leverage of these tokens as context for animation.

Result: MTVCrafter achieves state-of-the-art results with an FID-VID score of 6.98, outperforming the second-best method by 65%. It also shows strong generalization across diverse characters and scenarios.

Conclusion: MTVCrafter marks a significant advancement in human image animation by directly modeling raw 3D motion sequences and opens new possibilities for pose-guided human video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [166] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen,Wanqi Yin,Xiaofeng Yang,Cheng Chen,Chaoyue Song,Zhongang Cai,Lei Yang,Hao Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper presents ADHMR, a framework that improves human mesh recovery by aligning a diffusion-based HMR model through preference optimization. It uses an assessment model (HMR-Scorer) to evaluate predictions and create a preference dataset for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Human mesh recovery from a single image is challenging due to depth ambiguity and occlusions. Current probabilistic methods produce numerous plausible 3D human mesh predictions but often misalign with 2D image observations and lack robustness on in-the-wild images.

Method: The authors propose ADHMR, which includes training a human mesh prediction assessment model called HMR-Scorer. This model can evaluate predictions for in-the-wild images without 3D annotations. They use HMR-Scorer to create a preference dataset consisting of winner and loser mesh predictions for each input image. This dataset is used to fine-tune the base model via direct preference optimization. Additionally, HMR-Scorer helps improve existing HMR models through data cleaning.

Result: Extensive experiments demonstrate that ADHMR outperforms current state-of-the-art methods in human mesh recovery.

Conclusion: ADHMR addresses the limitations of existing probabilistic methods by using preference optimization to better align 3D human mesh predictions with 2D image observations and enhance robustness for in-the-wild images.

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [167] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu,Jiaqi Tang,Jiyao Wang,Yunfan LU,Xu Cao,Qingyong Hu,Yin Wang,Yuting Zhang,Tianxin Xie,Yunpeng Zhang,Yong Chen,Jiayu. Gao,Bin Huang,Dengbo He,Shuiguang Deng,Hao Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: The paper introduces SAGE DeeR, a driving agent with super alignment, generalist capabilities and self-eliciting features.


<details>
  <summary>Details</summary>
Motivation: To meet different users' comfort, interaction, and safety needs in intelligent driving cockpits.

Method: SAGE DeeR achieves super alignment by reacting to individual preferences and biases, generalist by understanding multi-view and multi-mode inputs, and self-eliciting by revealing implicit thought chains in the language space.

Result: Collected multiple datasets and built a large-scale benchmark for measuring perceptual decision-making ability and super alignment accuracy.

Conclusion: SAGE DeeR aims to fulfill diverse user requirements in the intelligent driving cockpit through its unique abilities.

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [168] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz,Pascal Colling,Qi Han,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper presents a new offline mapping approach for autonomous driving that integrates driver trails into HD map creation using transformer-based deep learning models, showing better generalization and performance than online methods.


<details>
  <summary>Details</summary>
Motivation: Current online mapping approaches for autonomous driving lack in temporal consistency, sensor occlusion handling, runtime efficiency, and generalization. There is a need for an improved method that can create accurate and updatable maps with less manual effort.

Method: The proposed method aggregates trail data from the ego vehicle and other traffic participants to construct a comprehensive global map. It employs transformer-based deep learning models, enabling continuous updates and being sensor-agnostic.

Result: The method outperforms state-of-the-art online mapping approaches in terms of generalization to unseen environments and sensor configurations. It has been validated on two benchmark datasets, proving its robustness and applicability.

Conclusion: This novel offline mapping approach offers a promising solution for creating high-definition maps in autonomous driving systems, providing superior performance and adaptability compared to existing online mapping techniques.

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [169] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: The paper presents HandReader, a set of three architectures for fingerspelling recognition in Sign Language, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Fingerspelling is an important part of Sign Language and previous works have room for improvement in accuracy.

Method: HandReader_RGB uses Temporal Shift-Adaptive Module (TSAM) to process RGB features. HandReader_KP uses Temporal Pose Encoder (TPE) on keypoints as tensors. HandReader_RGB+KP combines both modalities with a joint encoder.

Result: All HandReader models achieve state-of-the-art results on ChicagoFSWild and ChicagoFSWild+ datasets, and high performance on the new Znaki dataset for Russian fingerspelling.

Conclusion: The HandReader models are effective for fingerspelling recognition and the Znaki dataset and pre-trained models are publicly available.

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [170] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu,Kaixin Chen,Heng Guo,Yixiang Huang,Ming Wu,Zhenwei Shi,Chuang Zhang,Jun Guo*

Main category: cs.CV

TL;DR: The paper introduces MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting that includes over 68,000 high-resolution samples. It addresses limitations of existing datasets by enabling evaluation across diverse conditions and supporting development of scalable fog prediction techniques.


<details>
  <summary>Details</summary>
Motivation: Deep learning approaches have shown promise in marine fog detection and forecasting, but the lack of open-source datasets with diverse regional and satellite data limits model evaluation and exploration of marine fog characteristics.

Method: MFogHub integrates annotated marine fog observations from 15 coastal fog-prone regions and six geostationary satellites, providing over 68,000 high-resolution samples to facilitate rigorous evaluation of detection and forecasting methods under varying conditions.

Result: Experiments with 16 baseline models show that MFogHub reveals generalization fluctuations due to regional and satellite discrepancies and serves as a valuable resource for developing targeted and scalable fog prediction techniques.

Conclusion: MFogHub aims to advance practical monitoring and scientific understanding of marine fog dynamics globally.

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [171] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang,Shuai Xu,Xuelin Zhu,Yicong Li*

Main category: cs.CV

TL;DR: The paper proposes a Multi-Stage Cross-modal Interaction (MSCI) model to enhance Compositional Zero-Shot Learning (CZSL) by effectively utilizing intermediate-layer information from CLIP's visual encoder, designing self-adaptive aggregators for local and global information extraction, and dynamically adjusting attention weights. Experiments on three datasets validate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing CZSL studies rely on CLIP but overlook its limitations in capturing fine-grained local features due to architectural and training paradigm constraints.

Method: Propose MSCI model with two self-adaptive aggregators to extract and integrate local and global visual features, progressively incorporating them into textual representations through stage-by-stage interaction mechanism, and dynamically adjusting attention weights based on different combinations.

Result: Experiments on three widely used datasets fully validate the effectiveness and superiority of the proposed model.

Conclusion: MSCI model significantly enhances perception capability for fine-grained local visual information and flexibly adapts to diverse scenarios.

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [172] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: The paper introduces StoryReasoning, a dataset with grounded stories and structured scene analyses to maintain character and object consistency across frames in visual storytelling. By fine-tuning Qwen2.5-VL 7B into Qwen Storyteller, they achieve a 12.3% reduction in hallucinations per story.


<details>
  <summary>Details</summary>
Motivation: Visual storytelling systems often fail to maintain character identity and link actions appropriately due to referential hallucinations. This motivates the need for grounding characters, objects, and entities on visual elements.

Method: Proposed StoryReasoning dataset includes 4,178 stories from 52,016 movie images with structured scene analyses and grounded stories. The approach uses cross-frame object re-identification, chain-of-thought reasoning, and a grounding scheme linking textual elements to visual entities.

Result: Fine-tuned Qwen2.5-VL 7B (Qwen Storyteller) shows a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story compared to non-fine-tuned model.

Conclusion: StoryReasoning dataset and Qwen Storyteller model provide effective solutions for reducing referential hallucinations in visual storytelling.

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [173] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo,Roger Trullo,Albert Pla Planas,Etienne Decenciere,Thomas Walter*

Main category: cs.CV

TL;DR: This paper introduces MIPHEI, a model that predicts multiplex immunofluorescence signals from H&E images using U-Net and ViT architecture. It is trained on the ORION dataset and validated on two independent datasets, achieving high F1 scores for cell-type classification.


<details>
  <summary>Details</summary>
Motivation: Histopathological analysis with H&E staining is standard in cancer diagnosis, but multiplex immunofluorescence (mIF) provides more precise cell type identification which is not widely adopted due to cost and constraints. The motivation is to bridge this gap by predicting mIF signals from H&E images.

Method: The method involves developing MIPHEI, a U-Net-inspired architecture that uses ViT foundation models as encoders to predict mIF signals from H&E images. It targets various markers and is trained on the ORION dataset, then validated on two other datasets.

Result: MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming state-of-the-art baselines and random classifiers for most markers.

Conclusion: MIPHEI effectively captures relationships between nuclear morphologies and molecular markers, offering a promising approach for cell-type-aware analysis of large-scale H&E datasets to uncover relationships between spatial cellular organization and patient outcomes.

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [174] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu,Jirong Zha,Ding Li,Leye Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PartCrop的统一成员推断方法，用于在更真实的环境下对视觉自监督模型进行成员推断攻击，并研究了其有效性和泛化性。此外，还评估了两种常见的防御方法并提出了改进的PartCrop-v2。


<details>
  <summary>Details</summary>
Motivation: 自我监督学习在利用大量未标记数据方面显示出潜力，但在视觉领域也面临着显著的隐私问题。因此，在对手通常面对黑盒系统的实际情况下，研究如何在未知自我监督训练方法和细节的情况下进行成员推断。

Method: 提出了一种名为PartCrop的方法，该方法通过裁剪图像中的对象部分来查询表示空间内的响应。此外，还评估了两种常见的防御方法：提前停止和差分隐私，并提出了改进方法：缩小裁剪比例范围。最后，提出了可扩展的PartCrop-v2，引入了两个结构上的改进。

Result: 广泛的攻击实验验证了PartCrop的有效性和泛化性。防御实验表明所有方法都是有效的。定量研究表明，从数据和模型方面进行扩展的影响，并提出了可扩展的PartCrop-v2。

Conclusion: PartCrop是一种有效的成员推断方法，具有良好的泛化性能。防御方法如提前停止、差分隐私和缩小裁剪比例范围均能有效抵御攻击。此外，提出的PartCrop-v2在实际场景中表现出良好的可扩展性。

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [175] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: The paper introduces SpikeVideoFormer, an efficient spike-driven video Transformer with linear temporal complexity. It uses spike-driven Hamming attention and performs well on video tasks like classification, human pose tracking, and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing SNN-based Transformers mainly focus on single-image tasks and do not effectively utilize SNNs' efficiency in video-based vision tasks.

Method: The authors design a spike-driven Hamming attention (SDHA) for adapting traditional real-valued attention to spike-driven attention. They also analyze various spike-driven space-time attention designs and identify an optimal scheme with linear temporal complexity.

Result: SpikeVideoFormer achieves state-of-the-art performance compared to existing SNN approaches, with over 15% improvement on human pose tracking and semantic segmentation tasks. It matches the performance of recent ANN-based methods while providing significant efficiency gains (x16, x10, and x5 improvements on three tasks).

Conclusion: SpikeVideoFormer is an efficient spike-driven video Transformer that demonstrates strong generalization ability and efficiency across diverse downstream video tasks.

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [176] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: This paper presents a novel unpaired training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data. It employs a multi-term loss function guided by adversarial training with multiple discriminators, using lightweight neural network architectures suitable for mobile devices as backbones.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the challenge of acquiring pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images when developing a learned ISP.

Method: The method proposed in this paper is an unpaired approach which employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks. This approach maintains content structure while learning color and texture characteristics from the target RGB dataset.

Result: The method was evaluated on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, the unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics.

Conclusion: The authors conclude that their unpaired training method for a learnable ISP has strong potential and can achieve high fidelity without needing direct correspondences between raw images and ground-truth data.

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [177] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: Vision language models (VLMs) are AI systems that can process multimodal input, but their comprehension of visuospatial properties needs improvement.


<details>
  <summary>Details</summary>
Motivation: To test the scene comprehension ability of VLMs by using descriptions of virtual objects in images.

Method: Systematically evaluate state-of-the-art VLMs with prompts involving virtual objects and analyze their responses.

Result: The results indicate that VLMs have inadequate ability to process virtual objects and reason about spatial relations.

Conclusion: VLMs need further development to better comprehend visuospatial properties of scenes depicted in images.

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [178] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS是一种优化的3D高斯点绘方法，通过单次训练和用户指定的超参数，实现语义上有意义且跨场景一致的数量-质量控制，同时保持强大的数量-质量性能。相比基线方法，它能以更少的高斯点达到更高的渲染质量，并支持广泛的调整范围和无级调控。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点绘方法虽然追求更好的数量-质量性能，但缺乏让用户直观调整这种权衡以适应实际需求（如在不同硬件和通信限制下的模型部署）的能力。

Method: ControlGS通过一次固定设置的训练运行和一个反映数量-质量偏好的用户指定超参数，能够自动在各种场景中找到理想的数量-质量权衡点，包括紧凑物体到大型室外场景。该方法实现了语义上有意义且跨场景一致的数量-质量控制。

Result: ControlGS不仅能在减少高斯点数量的同时提高渲染质量，还支持广泛调整范围内的无级控制，优于基线方法。

Conclusion: ControlGS提供了一种有效的解决方案，使得用户可以在不同场景下根据实际需求灵活地调整3D高斯点绘的数量-质量权衡，同时保持高性能。

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [179] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov,Petr Surovtsev,Karina Kvanchiani,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CV

TL;DR: This paper addresses two key aspects of isolated sign language recognition (ISLR): limited data for individual sign languages and ambiguity in labeling similar signs. It introduces Logos, a large Russian Sign Language dataset, demonstrating its utility for cross-language ISLR model training, including transfer learning. By explicitly annotating visually similar sign groups, the study improves model quality and outperforms state-of-the-art results on the WLASL dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in tackling two main challenges in ISLR: the scarcity of data for most individual sign languages and the ambiguity caused by similar signs having different semantic meanings.

Method: The method involves creating and utilizing the Logos dataset, a comprehensive Russian Sign Language dataset. The approach includes pre-training models on Logos for universal encoding in other SLR tasks, exploring cross-language transfer learning, and using explicit annotations for visually similar signs to enhance model performance.

Result: Results show that pre-trained models on the Logos dataset can effectively serve as universal encoders, improving accuracy especially in low-resource datasets through joint training with multiple classification heads. Explicit labeling of visually similar signs also enhances model quality.

Conclusion: The study concludes by surpassing current state-of-the-art results on the WLASL dataset and achieving competitive results on the AUTSL dataset, all while using a single stream RGB video model. The source code, dataset, and pre-trained models have been made publicly available.

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [180] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper presents UniEval, a new framework for evaluating unified multimodal models without extra models, images, or annotations. It includes UniBench and UniScore which provide more challenging benchmarks and align closely with human evaluations respectively.


<details>
  <summary>Details</summary>
Motivation: There is currently no unified evaluation framework for multimodal models that can provide an overall evaluation without redundancy.

Method: Introduced UniEval, which consists of UniBench (a holistic benchmark supporting unified and visual generation models) and UniScore (a corresponding metric). UniBench includes 81 fine-grained tags for high diversity.

Result: Experimental results show that UniBench is more challenging than existing benchmarks and UniScore closely aligns with human evaluations, surpassing current metrics.

Conclusion: UniEval provides a simplified and unified way to evaluate multimodal models, offering new insights into their unique values.

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [181] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt,Pedro Sanchez,Yongchen Yao,Steven McDonagh,Sotirios A. Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: The paper introduces CheXGenBench, an evaluation framework for synthetic chest radiograph generation that assesses fidelity, privacy risks, and clinical utility across 11 text-to-image models. It highlights inefficiencies in current protocols, establishes a new benchmark, and releases SynthCheX-75K dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in medical domain evaluations of generative AI, such as methodological inconsistencies, outdated comparisons, and lack of practical clinical value assessment.

Method: Developed CheXGenBench with standardized data partitioning and a unified evaluation protocol including over 20 quantitative metrics to evaluate generation quality, privacy vulnerabilities, and clinical applicability.

Result: Revealed critical inefficiencies in existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent comparisons. Established a new benchmark for medical AI community.

Conclusion: Introduced CheXGenBench as a rigorous evaluation framework, released SynthCheX-75K dataset, and provided resources at https://raman1121.github.io/CheXGenBench/.

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [182] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev,Nuno Goncalves*

Main category: cs.CV

TL;DR: A novel dual-branch classification strategy is proposed to enhance the robustness of face recognition systems against face morphing attacks.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to presentation attacks, especially face morphing, which allows one identity to impersonate another. Thus, there is a need for more robust face recognition systems.

Method: The method modifies the classification task by introducing a dual-branch classification strategy that effectively handles the ambiguity in the labeling of face morphs, incorporating morph images into the training process.

Result: The strategy has been validated on public benchmarks and proved effective in enhancing robustness against face morphing attacks.

Conclusion: This approach can be universally applied and integrated into existing face recognition training pipelines to improve classification-based recognition methods.

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [183] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 提出了一种基于子模子集选择技术的检索框架增强方法，用于处理多图像问答任务，该方法在大规模数据中表现出更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在处理单张图像相关的视觉-语言任务时表现良好，但在面对多张图像集合时（如多图像问答场景），其可扩展性和检索性能面临挑战。

Method: 通过引入子模子集选择技术，利用GraphCut等查询感知子模函数，在主要检索组件之前预选语义相关的图像子集，同时采用基于锚点的查询和数据增强策略来提升检索管道的效果。

Result: 实验表明，该方法在大规模数据集中显著提高了子模检索管道的有效性。

Conclusion: 所提出的增强方法可以有效应对多图像任务中的可扩展性和检索性能问题，特别是在大规模数据环境中。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [184] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang,Guohai Xu,Weinong Wang,Junjie Yang,Jie Lou,Yunhua Xue*

Main category: cs.CV

TL;DR: Recent advancements in MLLMs have improved multi-image comprehension, but existing benchmarks overlook whether models truly understand visual input. This paper defines IVM, where MLLMs give correct answers without full visual comprehension. By analyzing causal attention modules, the authors introduce 'attention accuracy' as a new metric and benchmark for quantifying IVMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of current benchmarks that focus on answer correctness rather than genuine visual comprehension by MLLMs.

Method: The method involves decoupling visual and textual modalities within the causal attention module to analyze how attention distribution converges on images associated with correct answers as network layers deepen. A scale-agnostic metric called 'attention accuracy' is introduced to evaluate visual understanding directly via internal mechanisms.

Result: The result is the development of a novel metric and benchmark for quantifying implicit visual misunderstandings (IVMs) in MLLMs, demonstrating effectiveness in both multimodal and unimodal scenarios.

Conclusion: The conclusion is that 'attention accuracy' provides a reliable way to assess MLLMs' visual understanding, highlighting its versatility and generalizability.

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [185] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: This paper explores the concept of feasibility in synthetic images and its impact on CLIP-based classifier performance. The authors introduce VariReal, a pipeline that edits source images with feasible or infeasible attributes. Experiments reveal minimal effect of feasibility on LoRA-fine-tuned CLIP performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture.

Method: Introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model.

Result: Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.

Conclusion: The study concludes that feasibility has minimal impact on CLIP-based classifier performance, suggesting that it may not be crucial to enforce feasibility in synthetic data generation for such models.

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [186] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses the limitation of current multimodal models in mathematical reasoning by introducing a new approach using code as supervision for cross-modal alignment, resulting in improved performance.


<details>
  <summary>Details</summary>
Motivation: Natural language image-caption datasets used for training Large Multimodal Models focus on natural scenarios and neglect important details of mathematical figures, which are crucial for problem-solving. This limits the progress of LMMs in multimodal mathematical reasoning.

Method: The authors propose leveraging code as supervision for cross-modal alignment because code contains all information needed to generate corresponding figures. They co-develop an image-to-code model (FigCodifier) and a large dataset (ImgCode-8.6M). Then, they use FigCodifier to synthesize new mathematical figures and create MM-MathInstruct-3M, a high-quality fine-tuning dataset. Finally, they introduce MathCoder-VL, trained with ImgCode-8.6M and fine-tuned on MM-MathInstruct-3M for solving multimodal math problems.

Result: MathCoder-VL achieves a new open-source state-of-the-art across six metrics. It outperforms GPT-4o and Claude 3.5 Sonnet in geometry problem-solving, improving scores by 8.9% and 9.2%, respectively.

Conclusion: The proposed method significantly enhances the ability of multimodal models in mathematical reasoning, particularly in geometry problem-solving. The authors will release their datasets and models at https://github.com/mathllm/MathCoder.

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [187] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang,Fan Zhang,Yufeng Cui,Haiwen Diao,Zhuoyan Luo,Huchuan Lu,Jing Liu,Xinlong Wang*

Main category: cs.CV

TL;DR: 现有的视觉标记化方法在下游训练中独立优化视觉标记器，假设视觉标记在不同任务中具有良好的通用性。然而，这种分离范式导致了表示瓶颈。为了解决这个问题，本文提出了ETT，一种端到端的视觉标记器调整方法，通过联合优化视觉标记化和目标自回归任务来解决这一问题。实验表明，与冻结的标记器基线相比，ETT在多模态理解和视觉生成任务上带来了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉标记化方法在下游训练中独立优化视觉标记器，假设视觉标记在不同任务中具有良好的通用性。然而，这种分离范式导致了表示瓶颈，即视觉标记化的损失可能成为目标任务的表示瓶颈。

Method: 提出了一种名为ETT的端到端视觉标记器调整方法，该方法通过联合优化视觉标记化和目标自回归任务来解决这一问题。ETT利用标记器词典的视觉嵌入，并通过重建和标题目标对视觉标记器进行端到端优化。

Result: 广泛的实验证明，与冻结的标记器基线相比，所提出的端到端视觉标记器调整方法在多模态理解和视觉生成任务上带来了2-6%的显著性能提升，同时保留了原有的重建能力。

Conclusion: 本文提出了一种简单的端到端视觉标记器调整方法ETT，该方法可以显著提高多模态理解和视觉生成任务的性能，同时保留原有的重建能力。希望这种方法能够为多模态基础模型赋能。

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [188] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang,Siyu Chen,Lihe Yang,Jialei Wang,Ziang Zhang,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene.


<details>
  <summary>Details</summary>
Motivation: To generate accurate, dense, and detailed metric depth maps by combining incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction.

Method: Design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. Second, develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors.

Result: The model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods.

Conclusion: This framework performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [189] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alex Schwing,Liangyan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为3D-Fixup的新框架，通过学习到的3D先验知识来引导2D图像编辑。该框架支持诸如物体平移和3D旋转等复杂的编辑情况。利用扩散模型的生成能力，并从视频数据中生成训练数据对（源帧和目标帧），同时结合Image-to-3D模型提供的3D指导，以确保高质量的3D引导。实验结果表明，3D-Fixup能够有效地支持复杂的、身份连贯的3D感知编辑，推动了扩散模型在真实图像操作中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管通过扩散模型对图像先验进行建模取得了显著进展，但由于对象仅通过单个图像指定，因此具有3D意识的图像编辑仍然具有挑战性。

Method: 提出了3D-Fixup框架，该框架使用基于训练的方法，利用扩散模型的生成能力，并从视频数据中生成源帧和目标帧的数据对。此外，还结合了一个Image-to-3D模型，以提供3D指导，将2D信息显式投影到3D空间。设计了一个数据生成管道，以确保在整个训练过程中高质量的3D引导。

Result: 通过整合这些3D先验，3D-Fixup有效地支持复杂的、身份连贯的3D感知编辑，实现了高质量的结果，推动了扩散模型在真实图像操作中的应用。

Conclusion: 3D-Fixup是一种有效的框架，可以支持复杂的3D感知图像编辑，其成功在于结合了高质量的3D先验和扩散模型的强大生成能力。

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [190] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: 研究提出了一种基于深度强化学习的信号灯交叉口纵向车辆控制策略，通过综合奖励函数和DRL算法（DDPG和SAC）训练模型，结果表明该策略能提高交通安全性、效率和舒适性。


<details>
  <summary>Details</summary>
Motivation: 开发信号灯交叉口的自动驾驶车辆控制策略是一项挑战，因为其决策过程复杂。该研究旨在提出一种基于深度强化学习的纵向车辆控制策略，以应对这一挑战。

Method: 研究提出了一个综合奖励函数，重点关注距离间隔效率奖励、黄灯决策标准和非对称加减速响应，同时考虑传统安全和舒适性标准。将该奖励函数与两种流行的DRL算法（DDPG和SAC）结合，处理连续的加减速动作空间。使用真实世界前车轨迹和基于Ornstein-Uhlenbeck过程生成的模拟轨迹训练模型。

Result: 通过累积分布函数（CDF）图与真实世界轨迹数据比较，结果表明RL模型成功保持了较低的距离间隔（即更高效率）和抖动，同时不降低安全性。在多种安全关键场景（包括跟车和交通信号遵守）中评估模型的鲁棒性，DDPG和SAC模型均成功处理了这些场景，DDPG模型的动作曲线比SAC模型更平滑。

Conclusion: 总体而言，结果证实基于DRL的信号灯交叉口纵向车辆控制策略有助于提高交通安全、效率和舒适性。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [191] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: This paper explores a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy for autonomous vehicles at signalised intersections, incorporating a comprehensive reward function. It evaluates two DRL algorithms, showing improved traffic safety, efficiency, and comfort.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling autonomous vehicles at signalised intersections may not fully address the complex decision-making processes involved, prompting the need for more efficient and safer strategies.

Method: The method involves developing a comprehensive reward function focused on distance headway-based efficiency, decision-making during amber light, and asymmetric acceleration/deceleration response. This is combined with two DRL algorithms: DDPG and SAC, which handle continuous action spaces of acceleration and deceleration. The models are trained using real-world and simulated trajectories.

Result: The DRL models successfully maintained lower distance headway and jerk compared to human-driven vehicles without compromising safety. Both DDPG and SAC models handled safety-critical scenarios well, with DDPG showing smoother action profiles.

Conclusion: A DRL-based longitudinal vehicle control strategy at signalised intersections can enhance traffic safety, efficiency, and comfort.

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [192] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

TL;DR: Language Models are advancing rapidly, but human-made benchmarks can't keep up. This paper proposes automating fact-based synthetic data model evaluations using the LMs themselves, with strong correlations to human-curated questions.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of humans creating evaluation benchmarks for every domain due to the rapid advancement and scale of Language Models.

Method: Proposing a methodology that leverages Language Models to automatically evaluate domain-specific knowledge using grounding documents as input, generating both multiple choice and open-ended questions.

Result: The synthetic data benchmarking approach shows strong correlation with human-curated questions (Spearman ranking correlation of 0.96 and Pearson accuracy correlation of 0.79). When applied to an arXiv preprint, it revealed strong performance from Gemma3 models.

Conclusion: This novel tool supports gaining diagnostic insight into Language Model capabilities across various domains using automated, fact-based synthetic data evaluations.

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [193] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

TL;DR: 在强化学习中，奖励通常总是可观测的。然而，在许多现实场景中，奖励并不总是可观测的，这可以通过监控马尔可夫决策过程（Mon-MDP）来建模。本文探讨了使用函数逼近（FA）解决Mon-MDP的方法，并研究了其中的挑战。我们展示了结合函数逼近与学习到的奖励模型可以使智能体从具有可观测奖励的监控状态泛化到具有不可观测奖励的非监控环境状态。这样的泛化可以实现接近最优的策略，即使在形式上定义为不可解的环境中也是如此。然而，我们也发现了一个关键限制：由于过拟合，智能体会错误地推断奖励，从而导致不期望的行为。为了解决过拟合问题，我们提出了一种基于奖励不确定性的谨慎策略优化方法。这项工作是弥合Mon-MDP理论与实际应用之间差距的一步。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，奖励并不总是可观测的，而传统的强化学习假设奖励总是可观测的。因此，需要一种新的方法来处理这种不可观测奖励的情况，以使强化学习能够更好地应用于实际问题。

Method: 本文使用函数逼近（FA）和学习到的奖励模型来解决监控马尔可夫决策过程（Mon-MDP）。通过这种方式，智能体可以从具有可观测奖励的监控状态泛化到具有不可观测奖励的非监控环境状态。此外，为了防止过拟合，本文还提出了一种基于奖励不确定性的谨慎策略优化方法。

Result: 实验结果表明，结合函数逼近与学习到的奖励模型可以使智能体在具有不可观测奖励的状态下实现接近最优的策略。然而，也发现了由于过拟合导致的奖励错误推断的问题，但提出的谨慎策略优化方法可以在一定程度上缓解这一问题。

Conclusion: 本文展示了如何使用函数逼近和学习到的奖励模型来解决监控马尔可夫决策过程（Mon-MDP），并揭示了其中的关键挑战，即过拟合问题。提出的谨慎策略优化方法为缓解过拟合提供了一种可能的解决方案，这是弥合Mon-MDP理论与实际应用之间差距的重要一步。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [194] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

TL;DR: This paper proposes a Hierarchical Multi-Agent Reinforcement Learning framework for simulated air combat scenarios, which divides decision-making into low-level and high-level policies to address challenges such as complex flight dynamics and large state/action spaces.


<details>
  <summary>Details</summary>
Motivation: To identify effective Courses of Action in air combat simulations leading to mission success, allowing real-world defense exploration at low cost and in a safe environment.

Method: A two-level abstraction is used where low-level policies control individual units and high-level commander policy issues macro commands aligned with mission targets. Low-level policies are trained through increasing complexity curriculums while high-level commander is trained on mission targets given pre-trained control policies.

Result: Empirical validation confirms the advantages of the proposed framework.

Conclusion: The Hierarchical Multi-Agent Reinforcement Learning framework successfully addresses the challenges of applying deep reinforcement learning in air combat simulations by exploiting policy symmetries and separating control/command tasks.

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [195] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

TL;DR: The paper addresses multi-stage cascading failures in power grids using reinforcement learning, validating the method on IEEE bus systems.


<details>
  <summary>Details</summary>
Motivation: Cascading failures in power grids can lead to severe disruptions, and current mitigation strategies often overlook the complexity of multi-stage scenarios.

Method: The multi-stage cascading failure problem is treated as a reinforcement learning task, with a simulation environment developed and an agent trained via deterministic policy gradient algorithm for continuous actions.

Result: The effectiveness of the approach is demonstrated through validation on the IEEE 14-bus and IEEE 118-bus systems.

Conclusion: Reinforcement learning offers a promising approach to mitigating multi-stage cascading failures in power grids.

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [196] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: This paper presents a meta-prompting method using agentic reinforcement learning where an LLM as a Judge (LLMaaJ) teaches another LLM to produce content aligning with human mental expectations. Experiments at the US Open 2024 showed 53.8% alignment with human reviewers after an average of 4.38 iterations, enhancing content quality and coverage.


<details>
  <summary>Details</summary>
Motivation: To optimize the similarity between human mental expectations and the neural processing states of Large Language Models when producing complex texts.

Method: Meta-prompting method combined with agentic reinforcement learning where an LLM (LLMaaJ) instructs another LLM through in-context learning to generate text considering both intended and unintended traits.

Result: Human content reviewer expectations aligned with AI 53.8% of the time, requiring an average of 4.38 iterations. Content quality improved with extended coverage of tennis action.

Conclusion: The deployment at the US Open 2024 demonstrated the effectiveness of the method in aligning AI-generated content with human expectations, which has been applied successfully in other sports and entertainment events.

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [197] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

TL;DR: MCBS是一种结合了TD3、束搜索和蒙特卡洛rollouts的新方法，用于提升策略学习中的探索和动作选择。在多个连续控制基准测试中，相较于TD3、SAC、PPO和A2C等方法，MCBS展示了更高的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于噪声的探索方法（如TD3）可能导致次优的策略收敛。为了改进探索和动作选择，需要一种新的方法来提高强化学习算法的性能和效率。

Method: 引入了一种名为Monte Carlo Beam Search (MCBS)的混合方法，该方法将束搜索和蒙特卡洛rollouts与TD3结合。MCBS通过生成候选动作并进行短视域rollouts评估，使智能体能够做出更明智的选择。此外，还对关键超参数（如束宽和rollout深度）进行了详细分析，并探讨了适应性策略以优化复杂控制任务中的MCBS表现。

Result: 实验结果表明，MCBS在包括HalfCheetah-v4、Walker2d-v5和Swimmer-v5在内的多个连续控制基准上表现出比TD3、SAC、PPO和A2C更好的样本效率和性能。例如，在约20万步内达到了最大可实现奖励的90%，而第二好的方法则需要约40万步。

Conclusion: MCBS通过结构化的前瞻搜索增强了策略学习，同时保持了计算效率。它在不同环境中展示了更高的收敛速度，为复杂控制任务提供了一种有效的解决方案。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [198] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

TL;DR: 结合链式思维(CoT)与检索增强生成(RAG)，以及自我一致性与自我验证策略，可以减少大型语言模型的幻觉问题并提高事实准确性。本文对比了基础LLM与CoT、CoT+RAG、自我一致性及自我验证技术的效果，找出了在保持流畅性和推理深度的同时，最小化幻觉的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维(CoT)能改善多步骤推理，但它单独无法完全解决大型语言模型在复杂开放任务中的幻觉问题。因此，需要探索其他方法来进一步减少幻觉并提高回答的事实准确性。

Method: 将链式思维(CoT)与检索增强生成(RAG)相结合，并应用自我一致性和自我验证策略。通过引入外部知识源和让模型验证或修正自身输出，生成更准确连贯的响应。

Result: 每种方法都有其效果，研究确定了在保持语言流畅性和推理深度的同时，能够最大程度减少幻觉的最稳健方法。

Conclusion: 结合CoT与RAG，同时使用自我一致性和自我验证策略，是减少大型语言模型幻觉的有效途径。这为未来设计更精确和可靠的模型提供了方向。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [199] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

TL;DR: CRDT is a new framework that improves Decision Transformers by using counterfactual reasoning, leading to better decision-making in unseen scenarios and outperforming conventional DT approaches.


<details>
  <summary>Details</summary>
Motivation: Decision Transformers need high-quality data for optimal performance but face challenges due to lack of training data and scarcity of optimal behaviours. The authors aim to enhance DT's ability to reason beyond known data.

Method: Proposed the Counterfactual Reasoning Decision Transformer (CRDT), which generates and utilizes counterfactual experiences to improve decision-making in unseen scenarios.

Result: Experiments on Atari and D4RL benchmarks show CRDT outperforms conventional DT approaches, especially in scenarios with limited data and altered dynamics. Also, it enables the agent to combine suboptimal trajectories without architectural modifications.

Conclusion: Counterfactual reasoning can significantly enhance reinforcement learning agents' performance and generalization capabilities.

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [200] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

TL;DR: This study replicates and extends Piatti et al.'s GovSim framework, confirming large language models' cooperative decision-making capabilities generalize across models, scenarios, and languages. Notably, high-performing models can influence lower-performing ones in heterogeneous multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To validate the findings of Piatti et al. on the cooperative decision-making capabilities of large language models (LLMs) using the GovSim framework, and to explore its applicability to new settings, languages, and model architectures.

Method: Replicate key experiments from Piatti et al., evaluate additional LLMs (e.g., DeepSeek-V3, GPT-4o-mini), introduce new settings (heterogeneous multi-agent environment, Japanese instructions, inverse environment), and assess the impact of the universalization principle.

Result: Large models like GPT-4-turbo can achieve sustainable cooperation with or without the universalization principle, while smaller models fail without it. Cooperative behavior generalizes across different architectures and model sizes. High-performing models can influence lower-performing ones in heterogeneous environments.

Conclusion: The GovSim benchmark is applicable to various models, scenarios, and languages, providing insights into LLM adaptability in complex cooperative tasks. Findings suggest potential improvements in computational resource efficiency and cooperative AI system development.

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [201] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

TL;DR: The paper proposes a conceptual access control framework to solve the dual-use dilemma in AI safety systems by using verified user credentials and risk category classifiers.


<details>
  <summary>Details</summary>
Motivation: AI safety systems encounter the dual-use dilemma where the same request can be harmless or harmful based on context, leading to potential refusal of legitimate queries and approval of harmful ones.

Method: A conceptual access control framework is proposed. It uses verified user credentials and model output classifiers assigning outputs to risk categories. Responses are permitted only when user credentials match category requirements. For implementing classifiers, a theoretical approach with small, gated expert modules integrated into the generator model via gradient routing is introduced.

Result: This framework enables granular governance of AI capabilities, allowing verified users access to specialized knowledge without arbitrary restrictions while blocking adversaries.

Conclusion: The proposed contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma in AI safety systems.

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [202] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 研究了基于大型语言模型（LLM）的代理在博弈论场景中的战略性推理能力，通过三种不同的代理设计评估了代理复杂度对性能的影响，并发现人类启发的认知结构能提升LLM代理与人类战略行为的一致性，但这种关系是非线性的，受限于底层LLM能力和简单架构增强的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的迅速发展，AI研究转向更具代理特性的系统，因此需要探讨这些基于LLM的代理在多大程度上能够复制人类的战略性推理，特别是在博弈论背景下。

Method: 研究采用了三种代理设计：简单的博弈论模型、无结构的LLM作为代理模型以及整合到传统代理框架中的LLM。通过猜数字游戏作为测试平台，对这些代理进行了基准测试，涵盖了总体推理模式和基于个体角色的目标。此外，还引入了混淆的游戏场景以评估代理在训练分布之外的泛化能力。

Result: 通过对超过2000个推理样本和25种代理配置的分析，发现人类启发的认知结构可以增强LLM代理与人类战略行为的一致性。然而，代理设计复杂度与人类相似性之间的关系是非线性的，这取决于基础LLM的能力，并表明简单架构增强存在局限性。

Conclusion: 尽管人类启发的认知结构可以改善LLM代理的表现，但其效果受到基础LLM能力和架构增强方式的限制，因此提高LLM代理的人类相似性需要更深入的研究和更复杂的架构设计。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [203] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

TL;DR: This paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to address sequential decision-making tasks, encoding them as solutions to non-linear optimization problems and demonstrating practical viability.


<details>
  <summary>Details</summary>
Motivation: Counterfactuals are useful in AI for explaining changes leading to different outputs, but established methods focus on one-step decision-making and aren't directly applicable to sequential decision-making tasks.

Method: The paper encodes counterfactual strategies as solutions to non-linear optimization problems for MDPs, identifying minimal changes to an initial strategy that leads to an undesired outcome. It further extends the encoding to synthesize diverse counterfactual strategies.

Result: The approach is evaluated on four real-world datasets, showing its practical viability in sophisticated sequential decision-making tasks.

Conclusion: The introduction of counterfactual strategies for MDPs fills a gap in addressing sequential decision-making tasks.

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [204] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

TL;DR: The paper introduces a method to compute robust policies for HM-POMDPs, combining formal verification and subgradient ascent, showing better generalization and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in optimal policies for POMDPs against environmental perturbations, the paper explores HM-POMDPs which encompass multiple potential environment models.

Method: The approach involves two techniques: (1) deductive formal verification to evaluate robust policy by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for the worst-case POMDP.

Result: Empirical results demonstrate that the computed policies are more robust and generalize better to unseen POMDPs compared to various baselines. Additionally, the method scales well to HM-POMDPs with over a hundred thousand environments.

Conclusion: The proposed method effectively computes robust policies for HM-POMDPs, providing enhanced generalization capabilities and scalability.

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [205] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

TL;DR: Language model agents' ability to explore and infer causal relationships was examined using the Blicket Test paradigm. They reliably infer disjunctive causal relationships but struggle with conjunctive ones, a bias also seen in human adults. A test-time sampling method was proposed to reduce this bias.


<details>
  <summary>Details</summary>
Motivation: To determine whether language model agents possess the capability to efficiently explore and understand the causal structure of the world, which is crucial for robust reasoning. Also, to investigate if these models exhibit systematic biases leading to erroneous conclusions.

Method: Using the Blicket Test paradigm from developmental psychology to examine the LMs' ability to infer causal relationships. Quantifying similarities between LMs and humans in terms of inference profiles. Proposing a test-time sampling method to eliminate hypotheses about causal relationships.

Result: LMs reliably infer disjunctive causal relationships but struggle with conjunctive ones. This bias persists across different models and strategies, and performance decreases with task complexity. LMs exhibit adult-like inference profiles, not children-like. The proposed sampling method significantly reduces the disjunctive bias.

Conclusion: Language models show a disjunctive bias in causal reasoning similar to human adults, likely inherited from training data. The proposed test-time sampling method can help move LMs towards more causally rigorous reasoning.

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [206] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 在数学意义上，游戏无处不在。游戏中的搜索算法是人工智能方法。但目前尚无对这些算法性能普遍性的评估研究。本文以双人零和完美信息博弈为例填补这一空白，并提出一种新的搜索算法，在短时间搜索时全面优于现有算法，在中等搜索时间内于22个研究游戏中有17个表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对游戏搜索算法性能普遍性的评估研究，特别是针对双人零和完美信息博弈的场景。

Method: 提出了一种新的搜索算法，并在大规模实验中将其与现有算法进行比较。

Result: 新算法在短时间内搜索时全面优于所有已研究算法；在中等搜索时间内，在22个研究游戏中有17个表现最佳。

Conclusion: 提出的搜索算法在双人零和完美信息博弈中表现出色，为评估搜索算法的普遍性提供了一个新视角。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [207] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: This paper enhances techniques for identifying relevant and necessary features in classification models, introduces the concept of usefulness, and provides efficient algorithms for its detection.


<details>
  <summary>Details</summary>
Motivation: To improve existing strategies for ranking features according to their importance in predictions from classification models, especially focusing on the concepts of relevancy and necessity.

Method: Generalize the notion of relevancy, introduce a global concept of usefulness, develop efficient algorithms for detecting necessity and usefulness in complex models like neural networks and decision trees.

Result: Showed that necessity can be efficiently detected in complex models, established relationships between usefulness, relevancy and necessity, and demonstrated practical utility through experiments on three datasets.

Conclusion: The introduced concept of usefulness is related to relevancy and necessity, and efficient algorithms were developed for its detection, contributing to understanding feature importance in model behavior.

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [208] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper introduces the General Dynamic GR problem and uses a model-free goal-conditioned RL approach for real-time Goal Recognition in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Traditional Goal Recognition methods are not suitable for dynamic environments with numerous and constantly evolving goals.

Method: Employing a model-free goal-conditioned Reinforcement Learning approach to facilitate rapid adaptation for Goal Recognition in diverse, changing tasks.

Result: Enables real-time Goal Recognition systems suitable for dynamic scenarios.

Conclusion: Introduces a broader definition of Goal Recognition and promotes further research in this area.

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [209] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI is an expert-driven model that extends the ClinicXAI approach, focusing on interpretable multi-pathology detection from chest X-rays. It outperforms other methods in predictive accuracy and provides concept-level explanations aligning with expert reasoning in lung cancer detection.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of deep learning models for detecting lung pathologies, their lack of transparency limits clinical adoption. The authors aim to address this by creating a generalizable, human-interpretable model that can scale to multiple lung pathologies while maintaining interpretability.

Method: The authors developed XpertXAI, which builds on their previous work with ClinicXAI. It uses a high-performing InceptionV3-based classifier and compares against post-hoc explainability methods and unsupervised CBMs using a public dataset of chest X-rays with radiology reports.

Result: XpertXAI surpasses existing techniques in both predictive accuracy and providing clinically meaningful explanations that align closely with expert radiologist annotations and judgments.

Conclusion: XpertXAI demonstrates the effectiveness of human-centric model design in extending interpretability to broader diagnostic contexts, offering a scalable solution for clinically meaningful explainable AI in medical diagnostics.

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [210] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: The paper introduces a multimodal multi-agent framework for radiology report generation (RRG) that surpasses current methods by producing more accurate, structured, and interpretable reports through task-specific agents.


<details>
  <summary>Details</summary>
Motivation: Radiology report generation has challenges like factual inconsistency, hallucination, and cross-modal misalignment which need to be addressed to improve clinical workflows and reduce workload.

Method: A multimodal multi-agent framework is proposed where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis aligning with the stepwise clinical reasoning workflow.

Result: Experimental results show that this approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations.

Conclusion: This work emphasizes the potential of clinically aligned multi-agent frameworks in supporting explainable and trustworthy clinical AI applications.

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [211] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: This paper explores the use of various offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration, demonstrating its feasibility and effectiveness on different offline datasets including low-quality experience.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of microgrid voltage regulation in environments where online interaction is not feasible due to technical or safety reasons.

Method: The study employs different offline reinforcement learning algorithms trained on previously collected datasets to create a model for microgrid voltage regulation with solar power penetration.

Result: Experiment results on the IEEE 33-bus system show that the proposed approach is both feasible and effective across various offline datasets, even those with low-quality experience.

Conclusion: Offline reinforcement learning algorithms can successfully be used for microgrid voltage regulation when online environment interactions are not possible.

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [212] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本研究定义了好问题，并提出了一个系统性的评估框架，通过适配性与有效性两个维度进行评价，开发了基于评分表的系统，并在数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前对于问题质量的全面评估研究有限，而提问对于人类和人工智能都变得越来越重要。

Method: 提出适配性和有效性作为两个关键评估维度，构建基于评分表的打分系统，结合动态上下文变量，形成半自适应标准的评估框架，并使用CAUS和SQUARE数据集进行验证。

Result: 该框架能够评估结构良好和存在问题的问题，同时适应不同的上下文情境。

Conclusion: 本研究为问题评估提供了一个灵活且全面的框架，向将提问行为与基于问题本质的结构化分析方法整合迈出了重要的一步。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [213] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: The paper discusses the evolution of AI from basic rule-based systems to advanced autonomous agents, like ChatGPT and Grok, potentially representing the 'final generation' of AI as we know it. It highlights key milestones in AI development, societal implications, and the rapid pace of progress, while emphasizing the need for wisdom in navigating this new era.


<details>
  <summary>Details</summary>
Motivation: To chronicle the remarkable journey of AI evolution and understand the capabilities and underlying technologies of current AI agents, exemplified by systems like ChatGPT and Grok.

Method: Exploring practical examples and examining advancements in prompting, training methodologies, hardware capabilities, and architectural innovations that have led to today's AI agents.

Result: AI has reached a culminating phase with sophisticated agents capable of complex reasoning and interaction, with intelligence doubling approximately every six months.

Conclusion: There is a critical need for wisdom and foresight in addressing the opportunities and challenges brought about by this powerful new era of intelligence.

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [214] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: Pre-Act 是一种新方法，通过创建多步骤执行计划和详细推理来增强代理性能。在 Almita 数据集上，Pre-Act 在动作召回率上比 ReAct 高 70%，并且经过微调的小型模型（如 Llama 3.1）在动作准确性和目标完成率上也有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型 (LLMs) 强调通过生成大量中间标记进行推理，但小型模型在实际应用中因延迟和成本限制而难以处理复杂的推理任务。为了改进这一点，提出了一种新的方法 Pre-Act。

Method: Pre-Act 方法通过为给定的用户输入创建一个多步骤执行计划和详细的推理过程来增强代理性能。该计划逐步结合之前的步骤和工具输出，并在每次步骤执行后进行自我完善，直到获得最终响应。此外，还提出了一个两层评估框架：回合级别和端到端。

Result: 实验结果表明，在 Almita 数据集上，Pre-Act 的动作召回率比 ReAct 高 70%。经过微调的 70B 模型在动作准确性（回合级别）上比 GPT-4 提高了 69.5%，在目标完成率（端到端）上提高了 28%。

Conclusion: Pre-Act 方法在提高代理性能方面表现良好，特别是在动作召回率、动作准确性和目标完成率方面。此方法适用于各种规模的模型，包括对延迟和成本敏感的小型模型。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [215] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: An analysis of the abstract of a paper on depression detection, focusing on the MPDD Challenge.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on diverse age groups and individual differences in existing depression datasets and detection methods.

Method: The MPDD Challenge consists of two tracks using specific datasets (MPDD-Elderly and MPDD-Young) for detecting depression in older adults and younger participants. It incorporates multimodal data and individual difference factors.

Result: Provides a baseline model that fuses audio and video modalities with individual difference information to detect depression manifestations in diverse populations.

Conclusion: Aims to promote the development of more personalized and accurate depression detection methods, advancing mental health research.

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [216] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: In this paper, the authors tackle the challenge of learners understanding new knowledge concepts in MOOCs by proposing a Graph RAG pipeline that uses EduKGs and PKGs. This pipeline includes personalized question generation and concept-relation based question answering. The evaluation with expert instructors on different MOOCs shows its potential to enhance personalized learning.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction between learners and instructors, making it hard for learners to understand new knowledge concepts. While LLMs can support knowledge acquisition, they suffer from hallucinations. RAG addresses hallucinations but is limited by unstructured MOOC materials and lacks active guidance for learners.

Method: The authors propose a Graph RAG pipeline which incorporates (1) a PKG-based Question Generation method for personalized questions and (2) an EduKG-based Question Answering method utilizing relationships between knowledge concepts. These methods aim to guide learners effectively within the CourseMapper platform.

Result: A study involving 3 expert instructors across 3 different MOOCs demonstrated the potential of the Graph RAG approach to empower learners with a personalized learning experience for understanding new knowledge concepts.

Conclusion: The Graph RAG pipeline using EduKGs and PKGs has shown promise in enhancing learner understanding of knowledge concepts through personalized questions and answers in MOOC platforms.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [217] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: This study proposes an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations for Taiwanese China Studies (CS) scholarship. By applying generative AI techniques and large language models, the system extracts and standardizes entity relation triples from peer reviewed CS articles, visualizes them through a D3.js based system, and forms a domain specific knowledge graph and vector database. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.


<details>
  <summary>Details</summary>
Motivation: Taiwanese China Studies has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. There is a growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship.

Method: The study applies generative AI techniques and large language models to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field.

Result: The infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. It enables a paradigm shift from linear text consumption to network based knowledge navigation.

Conclusion: This work demonstrates how generative AI can augment area studies and digital humanities, offering a scalable, data driven alternative to traditional ontology construction and supporting a reimagined scholarly infrastructure for regional knowledge systems.

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [218] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: In this paper, researchers aim to identify the most effective AI-generated explanations for diagnostic decision support in healthcare by conducting a user study with physicians.


<details>
  <summary>Details</summary>
Motivation: With the increasing adoption of artificial intelligence in healthcare, there is a need to understand which types of explanations can increase transparency and trust in ML systems' predictions. Establishing mutual trust between doctors and ML systems is crucial in shared decision-making scenarios.

Method: The researchers explored different approaches to generating explanations in XAI and conducted a user study with physicians. The physicians filled out a survey to assess various types of AI-generated explanations and participated in interviews post-survey for qualitative insights.

Result: The study provided insights into the types of explanations that are most effective for enhancing the diagnostic process. These findings contribute to understanding how to empower users with confidence and trust in ML predictions.

Conclusion: Effective AI-generated explanations play an important role in empowering medical professionals to develop trust in ML systems, ultimately improving the diagnostic decision-making process.

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [219] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: MASS is a multi-agent system for portfolio construction that achieves stable excess returns through large-scale simulations and end-to-end optimization, outperforming 6 state-of-the-art baselines in various experiments.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing multi-agent systems which are either pure simulations or constrained by predefined workflows, thus restricting their applicability and effectiveness.

Method: Introduce MASS, which progressively increases the number of agents for large-scale simulations to enhance market understanding and optimizes agent distribution via a reverse optimization process instead of a fixed workflow.

Result: Demonstrates superiority through multiple types of experiments compared with 6 state-of-the-art baselines on 3 challenging A-share stock pools.

Conclusion: The paradigm established by MASS has the potential to be applied to other tasks with similar characteristics and has been open-sourced.

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [220] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: The paper proposes a new method to evaluate common sense in AI, specifically LLMs, by incorporating human heterogeneity. It finds that most LLMs are below human median in commonsense competence and smaller models are more competitive than larger ones.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for assessing machine commonsense intelligence assume homogeneous human common sense, which is not accurate as humans vary greatly in what they consider commonsensical.

Method: The method evaluates correspondence between model's judgment and that of a human population, treating LLMs both as independent survey respondents and as simulators of a hypothetical population.

Result: Most LLMs remain below the human median in individual commonsense competence. LLMs correlate modestly with real humans when agreeing on statements. Smaller, open-weight models are more competitive than larger, proprietary models.

Conclusion: The evaluation framework ties commonsense intelligence to its cultural basis and supports adapting AI models to human collectivities with different social knowledge.

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [221] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: The paper explores the application of Satisfiability Modulo Theories (SMT) in healthcare personnel scheduling, comparing it with Mixed Integer Linear Programming (MILP). It highlights the strengths and weaknesses of both methods based on problem constraints and concludes that SMT is a promising approach for future research.


<details>
  <summary>Details</summary>
Motivation: Despite thorough documentation on how personnel scheduling affects care quality and working conditions, healthcare scheduling remains challenging due to high demand and constraint variability. While studied for decades, limited research has focused on applying SMT techniques.

Method: Generic constraint formulations are created to model real-world scheduling constraints. These are then formulated as SMT and MILP problems to compare state-of-the-art solvers Z3 (SMT) and Gurobi (MILP) on academic and real-world inspired rostering problems.

Result: Experimental results indicate that MILP performs better for highly constrained or infeasible problems, while SMT excels otherwise. On real-world problems with varied shifts and personnel, SMT outperforms MILP. However, SMT's performance is sensitive to how generic constraints are formulated.

Conclusion: SMT-based methods offer a promising direction for future research in personnel scheduling.

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [222] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 在本文中，作者提出了一种新的信息论度量——广义定向信息，定义了智能体的可塑性，并揭示了其与授权之间的基本联系。研究发现可塑性是授权的镜像：智能体的可塑性等同于环境的授权，反之亦然。同时，还存在智能体可塑性和授权之间的张力，这意味着在设计智能体时需要兼顾这两个特性。


<details>
  <summary>Details</summary>
Motivation: 目前对于智能体的研究大多关注其影响未来观察的能力（即授权），但对智能体受过去观察影响的能力（即可塑性）尚未有系统性的研究。为了填补这一空白，作者试图定义和量化智能体的可塑性，并探索其与授权的关系。

Method: 作者基于一系列期望的性质定义了可塑性，使用一种新的信息论量——广义定向信息来衡量。该量严格推广了Massey（1990）引入的定向信息，同时保留了其所有良好性质。通过数学推导，作者揭示了可塑性和授权之间的关系。

Result: 研究发现了两个重要结果：一是可塑性是授权的镜像，即智能体的可塑性等于环境的授权，反之亦然；二是智能体的可塑性和授权之间存在张力，这表明在设计智能体时需要权衡这两者。

Conclusion: 可塑性和授权及其关系对于理解智能体的本质至关重要。在未来的智能体设计中，应充分考虑这两种特性以实现更优的性能。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [223] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: The paper identifies limitations in current model explanation evaluation methods and proposes AXE, a ground-truth Agnostic eXplanation Evaluation framework that independently measures explanation quality without needing ideal ground-truth explanations or model sensitivity.


<details>
  <summary>Details</summary>
Motivation: There are many competing and contradictory explanations for a single model prediction, making it hard to select the appropriate one. Current frameworks evaluate explanation quality by comparing against ideal 'ground-truth' explanations or verifying model sensitivity, but these approaches have limitations.

Method: The authors propose three principles for future explanation evaluation strategies and introduce AXE (ground-truth Agnostic eXplanation Evaluation), which evaluates and compares model explanations while not requiring access to ideal ground-truth explanations or relying on model sensitivity.

Result: AXE is verified by comparison with baselines and demonstrates its ability to detect explanation fairwashing.

Conclusion: AXE provides an independent measure of explanation quality and does not need ideal ground-truth explanations or model sensitivity.

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [224] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: This study distinguishes AI Agents from Agentic AI, offering a taxonomy and analysis of their design philosophies, capabilities, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To clarify the differences between AI Agents and Agentic AI in terms of design philosophies and capabilities by providing a structured conceptual taxonomy, application mapping, and challenge analysis.

Method: Outlining search strategy and foundational definitions; characterizing AI Agents as modular systems driven by LLMs and LIMs for task-specific automation; positioning generative AI as precursor to AI Agents advancing through tool integration, prompt engineering, reasoning enhancements; presenting comparative analysis across both paradigms including architectural evolution, operational mechanisms, interaction styles, autonomy levels.

Result: A clear distinction between AI Agents and Agentic AI systems was established with Agentic AI representing a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Challenges such as hallucination, brittleness, emergent behavior, and coordination failure were examined and targeted solutions proposed.

Conclusion: The work provides a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems.

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [225] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 尽管大语言模型在静态基准测试中表现出色，但作为动态环境中自学习和推理代理的真正潜力尚不清楚。本研究评估了自我反思、启发式变异和规划等提示技术的效果。实验表明，较大的模型通常优于较小的模型，但策略性提示可以缩小这一差距。过长的提示对小型模型的基本反应任务有负面影响，而大型模型表现更稳健。高级提示技术主要使小型模型在复杂游戏中受益，但对已经高性能的大模型改进较少。先进的推理方法结果高度可变，虽然能够显著提高性能，但也引入不稳定性。与人类表现相比，当前大语言模型在规划、推理和空间协调等关键领域存在持续的局限性，表明仅通过自我反思提示可能无法完全克服这些根本缺陷。推理是一个多方面的任务，需要超越静态基准来捕捉推理的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为动态环境中自学习和推理代理的潜力，以及不同提示技术对其适应能力的影响。

Method: 系统评估自我反思、启发式变异和规划等提示技术对不同规模语言模型在动态环境中的适应能力的影响，并比较其与人类的表现。

Result: 较大的模型通常优于较小的模型，但策略性提示可以缩小差距；过长的提示对小型模型不利；高级提示技术主要使小型模型在复杂游戏中受益；先进的推理方法结果可变，可能带来性能提升或下降。

Conclusion: 当前一代大语言模型在规划、推理和空间协调等关键领域存在根本性缺陷，仅通过自我反思提示可能无法完全克服这些限制。需要超越静态基准以更好地理解和评估模型的推理能力。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [226] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: Intersectionality framework can support the development of technologies that acknowledge and support all members of society, but it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact.


<details>
  <summary>Details</summary>
Motivation: Machine learning technologies may not provide adequate support for societal identities and experiences.

Method: Amplify the foundational intersectionality scholarship to create a socially relevant preliminary framework in developing machine-learning solutions.

Result: Evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.

Conclusion: Intersectionality framework can support the development of technologies that acknowledge and support all members of society.

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [227] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: This paper discusses the importance of intersectionality in machine learning to support societal identities and experiences, proposing a framework based on Crenshaw, Combahee, and Collins' scholarship to evaluate its application in ML literature.


<details>
  <summary>Details</summary>
Motivation: Studies have shown that machine learning technologies may not adequately support societal identities and experiences. Intersectionality is a sociological framework that can support the development of inclusive technologies but has been adopted in ways that do not always align with its original foundations.

Method: The authors amplify the foundational intersectionality scholarship of Crenshaw, Combahee, and Collins to create a socially relevant preliminary framework for developing machine-learning solutions. This framework is then used to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.

Result: The proposed framework provides a mechanism for explicitly considering complex social identities in machine learning, focusing on social justice and power. It highlights misalignments in how intersectionality has been applied in machine learning literature.

Conclusion: Adopting intersectionality through the lens of Crenshaw, Combahee, and Collins can help create more equitable technological outcomes. The framework offers a path towards ensuring that intersectionality is appropriately adopted in machine learning.

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [228] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: This paper is a survey that explores methodologies to overcome challenges in onboard learning for edge AI, such as limited resources and security issues, by focusing on model optimization, inference acceleration, and collaborative learning.


<details>
  <summary>Details</summary>
Motivation: Onboard learning in edge AI holds significant promise for real-time data processing, decision-making, and adaptive model training on resource-constrained devices without centralized servers. However, it faces challenges like limited computational resources, high inference costs, and security vulnerabilities.

Method: The survey examines techniques that optimize model efficiency, accelerate inference, support collaborative learning, reduce model complexity, improve inference speed, ensure privacy-preserving computation, and enhance scalability and adaptability through advancements in hardware-software co-design, model compression, and decentralized learning.

Result: The analysis provides insights into the current state of onboard learning, highlighting methods that enable robust, efficient, and secure AI deployment at the edge.

Conclusion: By addressing key challenges through various optimization strategies, onboard learning can achieve more effective and secure AI deployment on edge devices.

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [229] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

TL;DR: An fast algorithm for embedding hierarchical structures in 3D Minkowski spacetime is presented, which perfectly encodes data correlations in causal structures and applies to datasets like WordNet, suggesting that all discrete data may have a perfect 3D geometric representation.


<details>
  <summary>Details</summary>
Motivation: To find a method that can efficiently embed hierarchical structures into 3D Minkowski spacetime and encode the correlation of data purely within the causal structure without needing global symbolic structure.

Method: Using oriented token pairs as local hierarchical signals, the model embeds hierarchical structures such as the mammal sub-tree of WordNet with ambiguities. It also extends to a maximal unambiguous subset of WordNet nouns.

Result: The model successfully provides a perfect embedding of the mammal sub-tree of WordNet including ambiguities and also achieves a perfect embedding of the maximal unambiguous subset of WordNet nouns. A novel retrieval mechanism based on causality rather than distance is introduced.

Conclusion: All discrete data might have a perfect three-dimensional geometrical representation. The embeddings are nearly conformally invariant, linking to general relativity and field theory. Hierarchical meaning itself appears to be geometric.

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [230] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

TL;DR: This paper investigates model collapse in multi-modal vision-language generative systems and recursive generate-train loops, providing insights and guidelines for mitigating collapse.


<details>
  <summary>Details</summary>
Motivation: To understand model collapse in more realistic scenarios such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving.

Method: Expand the study of synthetic data training and model collapse to multi-modal vision-language generative systems including VLMs and text-to-image diffusion models, and explore recursive generate-train loops with multiple models.

Result: Model collapse in multi-modal context shows distinct characteristics like improved vision-language alignment and increased variance in VLM image-captioning task. General approaches can effectively mitigate model collapse.

Conclusion: The findings provide initial insights and practical guidelines for reducing model collapse risk in self-improving multi-agent AI systems and creating robust multi-modal synthetic datasets.

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [231] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

TL;DR: Large language models can be effectively quantized to 2-bit precision using RMS normalization and a layer-wise quantization schedule, maintaining accuracy without added model complexity.


<details>
  <summary>Details</summary>
Motivation: To reduce the cost of deploying large language models by achieving stable ternary (2-bit) quantization without significant accuracy degradation or added model complexity.

Method: Insert RMS normalization before every linear projection and apply a gradual, layer-wise quantization schedule to fine-tune full-precision checkpoints into ternary LLMs.

Result: This method matches or surpasses more complex knowledge-distillation pipelines on language-modeling benchmarks.

Conclusion: Careful normalization can significantly close the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [232] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 通过自我评判，大语言模型能够在没有参考答案的情况下有效自我提升。实验表明，在 Countdown 拼图和 MIT 积分蜂问题上，模型可以提供可靠的奖励信号，从而在以前不可能的领域实现强化学习。结合合成问题生成，建立了一个完整的自我改进循环，使Qwen 2.5 7B比基线提高了8%，并在积分任务上超过了GPT-4o的表现。这表明LLM评判者能够为训练模型提供有效的奖励信号，可能带来向自我导向学习的范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在许多领域受到缺乏编程奖励的限制，难以进行强化学习。因此，探索一种无需参考答案的自我改进方法成为重要的研究方向。

Method: 利用生成和验证解决方案之间的固有不对称性，通过自我评判实现模型的自我改进。具体地，在 Countdown 拼图和 MIT 积分蜂问题上测试模型，并结合合成问题生成技术，形成一个完整的自我改进循环：生成练习问题、解决问题并评估自身性能。

Result: Qwen 2.5 7B实现了8%的性能提升，超过基线并在积分任务上超越了GPT-4o的表现。证明了LLM评判者能够提供有效的奖励信号以训练模型。

Conclusion: 这项研究表明，通过自我评判和自我导向学习，AI系统能够持续改进，可能加速那些训练数据稀缺或评估要求复杂的领域的进步。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [233] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: This paper argues for using a linear combination of accuracy and fairness metrics to measure the overall value of predictive algorithms, based on Harsanyi's result in preference aggregation. It also analyzes accuracy-fairness trade-offs using the COMPAS dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the conceptual challenges of balancing accuracy and fairness in predictive algorithms, as well as determining how to aggregate preferences for different measures of these qualities.

Method: The method involves making a formal argument based on Harsanyi's classic result in preference aggregation literature to justify the use of a linear combination of accuracy and fairness metrics. This approach is then applied to analyze accuracy-fairness trade-offs using the COMPAS dataset.

Result: The result is a justification for using a linear combination of accuracy and fairness metrics to evaluate the overall value of predictive algorithms, with an application to the COMPAS dataset illustrating the analysis of such trade-offs.

Conclusion: The conclusion emphasizes that there are good reasons to use a linear combination of accuracy and fairness metrics when evaluating predictive algorithms, providing a normative guideline for managing trade-offs between accuracy and fairness.

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [234] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

TL;DR: This paper introduces metrics to evaluate simplified time series for interpretability of Time Series Classifier (TSC), showing that simplifications are better than original time series in certain conditions.


<details>
  <summary>Details</summary>
Motivation: Time series data is not intuitively understandable to humans, unlike text and image data. Simplified time series could improve interpretability of TSC.

Method: Introduced metrics related to complexity (number of segments) and loyalty (likelihood to maintain classification). Evaluated four distinct simplification algorithms across several TSC algorithms and datasets with varying characteristics.

Result: Simplifications for interpretability of TSC are much better than using the original time series, especially when the time series are seasonal, non-stationary and/or with low entropy.

Conclusion: Simplified time series can significantly enhance the interpretability of TSCs under specific conditions.

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [235] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

TL;DR: Recent experiments indicate that training trajectories of various deep neural networks evolve on a low-dimensional 'hyper-ribbon-like' manifold. Inspired by this, the paper analytically characterizes this phenomenon for linear networks using dynamical systems theory.


<details>
  <summary>Details</summary>
Motivation: To understand why training trajectories of different neural networks evolve on a low-dimensional manifold.

Method: Using tools in dynamical systems theory, analyze how factors such as eigenvalue decay rate, scale of ground-truth output to initial weights, and number of gradient descent steps control the geometry of this manifold. Also extend analysis to kernel machines and linear models trained with stochastic gradient descent.

Result: The geometry of the low-dimensional manifold is controlled by three key factors and phase boundaries of the region where hyper-ribbons are expected can be characterized by analytically computing and bounding these factors.

Conclusion: This study provides analytical insights into why training trajectories evolve on low-dimensional manifolds for linear networks, kernel machines, and linear models trained with stochastic gradient descent.

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [236] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

TL;DR: The paper explores machine learning methods for analyzing exoplanet atmospheres from spectral data, emphasizing uncertainty estimation's role in performance and highlighting the limitations of a business-driven approach.


<details>
  <summary>Details</summary>
Motivation: To address the complex challenge of characterizing exoplanetary atmospheres through spectral analysis by exploring machine learning techniques provided by the NeurIPS 2024 Ariel Data Challenge.

Method: Focused on a data-centric business approach with prioritization of generalization. Explored multiple experimental axes such as feature extraction, signal transformation, and heteroskedastic uncertainty modeling.

Result: Uncertainty estimation significantly impacts the Gaussian Log-Likelihood (GLL) score, improving it by 11%. However, limitations were found in tabular modeling and feature engineering for this task.

Conclusion: The study reveals trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis under a competition framework.

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [237] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

TL;DR: The paper introduces ForeCite, a framework using pre-trained causal language models with a linear head to predict average monthly citation rates of biomedical papers, achieving high test correlation and establishing a new state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To automate research evaluation and accelerate scientific progress by predicting future citation rates of academic papers.

Method: ForeCite adapts transformers for regression tasks by appending pre-trained causal language models with a linear head. It uses gradient-based saliency heatmaps.

Result: Achieves a test correlation of ρ = 0.826 on a dataset of 900K+ biomedical papers, a 27-point improvement over previous methods.

Conclusion: ForeCite establishes a new state-of-the-art in forecasting long-term influence of academic research, providing groundwork for automated evaluation of scientific contributions.

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [238] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

TL;DR: GPML库通过将原始网络流量跟踪转换为图形表示，支持社区和光谱指标提取，实现实时检测和历史取证分析，应对动态网络中的复杂攻击。


<details>
  <summary>Details</summary>
Motivation: 在网络中复杂、多步骤和快速演变的攻击增加的情况下，需要先进的网络威胁检测器来应对这些挑战。

Method: GPML库采用基于图的方法，将原始网络流量跟踪转换为图表示，提供工具以检测动态网络中的交互异常和社区变化，并支持社区和光谱度量的提取。

Result: 该方法可以增强实时检测和历史取证分析，支持现代网络安全挑战。

Conclusion: GPML库通过其强大的、基于图的方法，为现代网络安全问题提供了有效的解决方案。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [239] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: State-Space Models (SSMs) have been limited to a few polynomial bases, but this paper introduces SaFARi, a generalized method allowing any frame or basis in SSMs, providing infinite diversity in SSM architecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of using only a few polynomial bases in State-Space Models for online function approximation and machine learning models dealing with long-range dependent data.

Method: The method involves creating a generalized framework called SaFARi which allows building an SSM with any frame or basis, not restricted to polynomials. This includes the HiPPO approach as a subset and opens up many new possibilities within the SSM architecture.

Result: This approach potentially leads to more versatile and effective SSMs that can be applied to a wider range of problems.

Conclusion: SaFARi provides a flexible and powerful tool for enhancing the capabilities of State-Space Models by enabling the use of diverse frames and bases.

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [240] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: The paper proposes an online prediction method for unknown, non-explosive linear stochastic systems by incorporating exponential forgetting to balance the regression model and reduce accumulation error. A sharper logarithmic regret bound of O(log^3 N) is provided.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for predicting unknown linear stochastic systems may suffer from degraded performance due to imbalanced regression models leading to overfitting and reduced prediction accuracy.

Method: Inject an inductive bias into the regression model via exponential forgetting to better balance between regression and regularization errors, reducing accumulation error as well.

Result: Achieves a better trade-off between regression and regularization errors and provides a sharper logarithmic regret bound of O(log^3 N).

Conclusion: The proposed method effectively tackles the problem of imbalanced regression models in online prediction for unknown linear stochastic systems.

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [241] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

TL;DR: 研究提出了一种结合策略优化与熟悉度自编码器的端到端持续学习系统，该系统无需外部信号即可成功实现持续学习，能够识别新任务、匹配已知环境，并在重新遇到已知环境时选择性检索相关知识。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理中的持续学习面临重大挑战，尤其是在没有外部信号指示任务或环境变化的情况下，如何保存和利用现有信息成为关键问题。

Method: 通过将策略优化与熟悉度自编码器集成到一个端到端持续学习系统中，该方法能够在不依赖外部信号的情况下，识别和学习新任务或环境，同时保留先前的经验知识，并在重新遇到已知环境时选择性检索相关知识。

Result: 初步结果显示，该系统能够在没有外部信号指示任务变化或重新遇到任务的情况下成功实现持续学习，证明了该方法的有效性。

Conclusion: 所提出的结合策略优化与熟悉度自编码器的持续学习系统为解决强化学习中的持续学习挑战提供了有希望的方法。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [242] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

TL;DR: The paper presents an AI-driven software solution for quantifying metastatic bone disease from WB-DWI scans, demonstrating high accuracy, sensitivity, and specificity compared to a reference standard.


<details>
  <summary>Details</summary>
Motivation: To develop a reproducible and automated method for quantifying metastatic bone disease from WB-DWI scans, providing useful measurements for clinical decision-making in APC patients.

Method: The method includes a weakly-supervised Residual U-Net model for generating a skeleton probability map, a statistical framework for WB-DWI intensity normalisation, and a shallow convolutional neural network for processing outputs to generate a mask of suspected bone lesions. This mask is applied to the gADC map to extract TDV and gADC statistics.

Result: The tool achieved a Dice score of 0.6 for lesions within pelvis and spine, relative differences for log-TDV and median gADC below 9% and 5%, respectively, coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, and intraclass correlation coefficients above 0.9. It also achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing treatment response.

Conclusion: The developed software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, offering potentially valuable measurements for clinical decision-making.

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [243] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

TL;DR: The paper proposes DyGSSM, a novel method for dynamic graph representation learning that integrates local and global features using GCN, random walk with GRU, and cross-attention mechanism. It also incorporates SSM based on HiPPO algorithm to manage long-term dependencies in parameter updates. Experiments show its superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic graph representation learning methods fail to simultaneously extract global and local information within each snapshot and lack effective temporal dependency management during model parameter updates.

Method: DyGSSM combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. A cross-attention mechanism integrates these features, while an SSM based on the HiPPO algorithm manages long-term dependencies in parameter updates.

Result: Experiments on five public datasets demonstrate that DyGSSM outperforms existing baseline and state-of-the-art methods in 17 out of 20 cases.

Conclusion: DyGSSM addresses the limitations of existing methods by effectively capturing both local and global structures in dynamic graphs and managing temporal dependencies in parameter updates.

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [244] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: Mamba model has weakness in long-range sequential tasks despite being designed for long-range dependencies. This paper analyzes Mamba's limitations and proposes an extension called B2S6 which improves its performance.


<details>
  <summary>Details</summary>
Motivation: To understand and address the gap in Mamba's performance on long-range sequential tasks, improving its universality and versatility.

Method: Analyze Mamba's limitations through three perspectives - expressiveness, inductive bias, and training stability; Propose B2S6, an extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias.

Result: Theoretically proves that B2S6 equips the model with a better-suited inductive bias and improves its expressiveness and stability. Empirically, B2S6 outperforms S4 and S4D on Long-Range Arena tasks while maintaining Mamba's performance on language modeling benchmarks.

Conclusion: B2S6 is a successful extension to Mamba that addresses its limitations in long-range sequential tasks.

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [245] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

TL;DR: 提出了一种名为Flexi-VAE的数据驱动框架，用于高效单次预测非线性参数偏微分方程（PDEs），在保持高精度和稳定性的同时，消除了迭代时间步长的需要。该模型在1D粘性Burgers方程和2D对流扩散方程的经典PDE基准测试中表现良好，比自编码器-LSTM基线模型快50倍（CPU）和90倍（GPU）。


<details>
  <summary>Details</summary>
Motivation: 当前非线性参数偏微分方程的预测方法通常依赖于迭代时间步长，这可能影响效率和准确性。因此，研究者希望开发一种能够消除迭代时间步长需求、同时保持高准确性和稳定性的新方法。

Method: Flexi-VAE框架包含一个神经传播器，该传播器将潜在表示向前推进时间，结合变分自动编码器设置中的潜在演化与物理状态重建。文中评估了两种传播策略：直接连接传播器（DCP）和位置编码传播器（PEP）。通过表示理论分析表明，DCP通过促进解耦和物理意义明确的潜在空间，提供更好的长期泛化能力。几何诊断（包括雅可比谱分析）显示，传播的潜在状态位于解码器敏感性较低和局部几何更稳定的区域，增强了长时间预测的鲁棒性。

Result: 在1D粘性Burgers方程和2D对流扩散方程的经典PDE基准测试中，Flexi-VAE实现了广泛的参数范围内的精确预测，并且相比自编码器-LSTM基线模型，在大时间步长移动时提供了超过50倍（CPU）和90倍（GPU）的速度提升。

Conclusion: Flexi-VAE作为一种可扩展且可解释的代理建模工具，适用于加速计算流体动力学（CFD）和其他由参数PDE驱动的应用中的高保真模拟，并且具有扩展到更高维度和更复杂系统的能力。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [246] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: The paper presents AdaFortiTran, a new model improving channel estimation in OFDM systems under fast-fading channels and low-SNR scenarios by combining convolutional layers and transformer encoder, achieving up to 6 dB MSE reduction compared to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for channel estimation in OFDM systems have limitations in performance under fast-fading channels and low-SNR scenarios.

Method: The method uses convolutional layers to capture correlations between neighboring channel elements and a transformer encoder with global Attention mechanism to model long-range dependencies and spectro-temporal interactions. Nonlinear representations of channel statistics are integrated as priors, and residual connections merge global and local features.

Result: AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB), and delay spreads (50-300 ns).

Conclusion: AdaFortiTran demonstrates superior robustness in high-mobility environments, making it a significant advancement in channel estimation for OFDM systems.

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [247] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

TL;DR: 通过脑信号进行监督学习，可以显著提高深度神经网络对抽象概念的理解能力，并提升其在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模参数和训练数据提升了DNN的能力，但实现复杂的认知能力（如理解抽象概念、推理和适应新场景）仍然是一个重大挑战。

Method: 使用脑-环路监督学习方法，利用少量脑信号将人类的概念结构转移到DNN中，从而增强其对抽象概念的理解能力。

Result: 实验结果表明，该方法不仅提高了DNN在小样本/零样本学习和分布外识别等任务中的性能，还生成了高度可解释的概念表示。

Conclusion: 人类-环路监督可以有效增强大型模型的复杂认知能力，为开发更像人类认知能力的人工系统提供了有希望的方向。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [248] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

TL;DR: This paper proposes a time-consistency discriminator that allows pretrained image diffusion models to generate realistic spatiotemporal dynamics, which performs equally well as Video Diffusion Models (VDMs) in temporal consistency while showing advantages in uncertainty calibration and bias reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of training Video Diffusion Models (VDMs) from scratch, which demands large computational resources and limits their application. The authors aim to leverage pretrained image diffusion models to generate realistic spatiotemporal dynamics with less computational burden.

Method: The method involves introducing a time-consistency discriminator that guides the sampling inference process of pretrained image diffusion models without requiring extensions or fine-tuning of these models. This approach is compared against VDMs trained from scratch on turbulence simulation and precipitation datasets.

Result: The results indicate that the proposed approach matches VDMs in terms of temporal consistency, provides better uncertainty calibration, reduces biases, and successfully achieves stable long-term climate simulations at daily time steps.

Conclusion: The conclusion is that the time-consistency discriminator offers an effective alternative to VDMs for generating realistic spatiotemporal dynamics, with advantages in computational efficiency, uncertainty handling, and reduced bias.

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [249] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

TL;DR: In this paper, a new asynchronous algorithm named Argus is developed for non-convex and non-smooth decentralized federated bilevel learning in SAGIN. Argus allows networked agents to tackle bilevel learning problems asynchronously and avoids the problem of stragglers. Theoretical analysis and numerical experiments demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized and synchronous optimization algorithms are not suitable for space-air-ground integrated networks (SAGIN) due to infrastructureless and time-varying environments.

Method: The paper proposes an asynchronous algorithm called Argus to solve non-convex and non-smooth decentralized federated bilevel learning over SAGIN. This algorithm enables networked agents to handle bilevel learning problems asynchronously in time-varying networks.

Result: The theoretical analysis of iteration complexity, communication complexity, and computational complexity of Argus is provided. Numerical experiments further confirm the effectiveness of the proposed algorithm.

Conclusion: Argus is an effective asynchronous algorithm for addressing non-convex and non-smooth decentralized federated bilevel learning in SAGIN, allowing networked agents to deal with bilevel learning problems without being hindered by stragglers.

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [250] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

TL;DR: This paper proposes DSIV-CFR framework to address the challenge of unmeasured confounders in estimating cumulative causal effects of sequential treatments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the problem of latent confounding bias in sequential treatment effect estimation, which cannot be addressed by existing advanced causal methods even though they use transformers to model time sequences.

Method: The method proposed is Decomposing Sequential Instrumental Variable framework for CounterFactual Regression (DSIV-CFR), which relies on a common negative control assumption. It utilizes an instrumental variable as a special negative control exposure and previous outcome as a negative control outcome to recover latent IVs in observation variables.

Result: Experiments on 4 datasets showed significant performance improvement in one- and multi-step prediction.

Conclusion: The DSIV-CFR framework can effectively adjust latent confounding bias and identify optimal treatments for dynamic systems.

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [251] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

TL;DR: A new fair clustering algorithm, Fair Clustering via Alignment (FCA), is proposed to balance proportions of instances assigned to each cluster with respect to a given sensitive attribute. It guarantees approximately optimal clustering utility for any given fairness level without complex constraints.


<details>
  <summary>Details</summary>
Motivation: Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice.

Method: The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.

Result: Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.

Conclusion: FCA theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice.

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [252] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) have great potential in Customer Experience Management (CXM), but their evaluation is restricted by data scarcity and benchmark limitations. This paper introduces CXMArena, a new large-scale synthetic benchmark dataset for evaluating AI in operational CXM contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of evaluating LLMs' practical utility in complex operational environments due to data scarcity and inadequate benchmarks.

Method: Developed a scalable LLM-powered pipeline that simulates brand's CXM entities forming the foundation of datasets including knowledge articles, product specifications, issue taxonomies, and contact center conversations with controlled noise injection and automated validation. Introduced CXMArena which provides benchmarks targeting five important operational tasks.

Result: Baseline experiments showed the difficulty of the benchmark: state-of-the-art models achieved only 68% accuracy on article search and a low F1 score of 0.3 for knowledge base refinement.

Conclusion: Current models face significant challenges necessitating complex pipelines and solutions over conventional techniques.

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [253] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

TL;DR: The paper introduces DSoftKI, a scalable GP method for fitting and predicting full derivative observations. It enhances SoftKI's interpolation scheme to incorporate directional orientation, enabling accurate high-dimensional predictions with larger datasets.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable Gaussian Process method capable of handling full derivative observations in high-dimensional settings.

Method: DSoftKI extends SoftKI by enhancing its interpolation scheme to include the directional orientation of interpolation points relative to the data, allowing construction of a scalable approximate kernel with first and second-order derivatives through interpolation.

Result: DSoftKI shows accuracy in synthetic function benchmarks and high-dimensional molecular force field prediction (100-1000 dimensions), scaling to larger datasets with full derivative observations than previously possible.

Conclusion: DSoftKI is an effective and scalable method for making accurate predictions with full derivative observations in high-dimensional spaces.

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [254] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 当前自监督学习在无线信道表示中的应用通常借用为文本和图像处理开发的范式，而没有充分解决无线通信的独特特性和约束。为了填补这一空白，本文提出了WiMAE（无线掩码自动编码器），这是一种基于变压器的编码器-解码器基础模型，预训练在一个现实的开源多天线无线信道数据集上。在此基础上，我们开发了ContraWiMAE，通过在统一的多任务框架中结合对比学习目标和重建任务来增强WiMAE。通过对未见过的场景进行广泛的评估，我们展示了这两种方法在多个下游任务中的有效性，ContraWiMAE在无线环境中的线性可分性和适应性方面表现出进一步的改进。与最先进的无线信道基础模型的比较评估证实了我们模型的优越性能和数据效率，突显了它们作为未来自监督无线信道表示学习研究的强大基线的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习在无线信道表示中的应用，通常借用文本和图像处理的范式，但未充分考虑无线通信的独特特性和约束。因此需要一种新的方法来更好地满足无线通信的需求。

Method: 提出了一种名为WiMAE的基于变压器的编码器-解码器基础模型，并在此基础上开发了ContraWiMAE，通过结合对比学习目标和重建任务来增强表示质量。

Result: 通过广泛的评估，证明了WiMAE和ContraWiMAE在多个下游任务中的有效性，尤其是在线性可分性和适应性方面有显著提升。

Conclusion: WiMAE和ContraWiMAE在无线信道表示学习中表现出了优越的性能和数据效率，可以作为未来研究的强大基线。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [255] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

TL;DR: The paper proposes Quotient Complex Transformer (QCformer) based on quotient complexes for predicting properties of hybrid organic-inorganic perovskites (HOIPs). It outperforms state-of-the-art models in HOIP property prediction.


<details>
  <summary>Details</summary>
Motivation: Novel functional materials are crucial for sustainable energy and climate change challenges. Hybrid organic-inorganic perovskites (HOIPs) have exceptional optoelectronic properties but traditional graph neural networks (GNNs) fail to capture their periodic structures and higher-order interactions effectively.

Method: A material structure is modeled as a quotient complex that encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. The model uses a simplex-based Transformer module to leverage higher-order features defined on simplices. QCformer is pretrained on benchmark datasets like Materials Project and JARVIS, then fine-tuned on HOIP datasets.

Result: QCformer outperforms state-of-the-art models in predicting properties of HOIPs.

Conclusion: Quotient complex representation and QCformer provide a powerful new tool for predictive modeling of perovskite materials.

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [256] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

TL;DR: The paper proposes a framework for prioritizing urban green space development in Tehran, using socio-economic, environmental, and sensitivity indices. Machine learning models were used for vegetation cover classification, with Random Forest showing the best performance. The framework was validated through microclimate simulation, demonstrating a reduction in air temperature after implementing green roof technology.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient meteorological stations and provide a valuable tool for urban planners to develop green spaces in urban areas.

Method: Using diverse socio-economic, environmental, and sensitivity indices derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the WRF model. Several machine learning models (XGBoost, LightGBM, Random Forest, Extra Trees) were employed for binary vegetation cover classification.

Result: Random Forest achieved the highest performance exceeding 94% in Overall Accuracy, Recall, and F1-score. The framework's performance was validated through microclimate simulation which demonstrated reducing air temperature by up to 0.67°C after utilizing the green roof technology.

Conclusion: This framework provides a valuable tool for urban planners to prioritize and develop green spaces effectively.

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [257] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

TL;DR: 本研究提出了一种剪枝感知的大规模人工智能模型（LAIM）协同推理方案，通过优化剪枝比例、传输功率和计算频率，在系统延迟、能量和资源约束下，最小化推理失真。实验表明该设计在推理性能、系统延迟和能耗之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着对大规模人工智能模型服务需求的增长，为了实现低延迟和保护隐私的应用，从传统的基于云的推理转向基于边缘的推理成为趋势。特别地，边缘设备协同推理作为一种资源高效的LAIM执行策略在无线网络中崭露头角。

Method: 研究首先证明了LAIM输出失真与其参数失真之间的关系，并通过率失真理论推导出参数失真的下界，从而捕捉剪枝比例与协同推理性能之间的关系。接着，研究将LAIM协同推理失真最小化问题公式化，联合优化剪枝比例、传输功率和计算频率，同时考虑系统延迟、能量和可用资源的约束。最后，提出了一种高效算法来解决这一高度非凸问题。

Result: 广泛的模拟验证了所提设计的有效性。结果表明模型参数失真能够可靠地限定输出失真。此外，所提出的联合剪枝比例和资源管理设计相比基准方案（如完全基于设备或服务器的推理），在推理性能、系统延迟和能耗之间的权衡上表现出优越性能。此外，分割点在异构和资源受限的边缘环境中对系统性能优化起着关键作用。

Conclusion: 剪枝感知的LAIM协同推理方案可以有效降低推理失真，并在推理性能、系统延迟和能耗之间取得良好的平衡。分割点的选择对于优化系统性能至关重要，特别是在异构和资源受限的边缘环境中。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [258] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

TL;DR: The paper proposes Birch SGD, a unifying framework for analyzing and designing distributed SGD methods using computation trees. It introduces eight new methods with optimal computational time complexity and reveals common iteration rates and trade-offs among methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for a unified approach to analyze and design efficient asynchronous and parallel optimization methods, specifically distributed SGD techniques.

Method: Birch SGD represents each method as a weighted directed tree (computation tree) and uses this representation to reduce convergence analysis to studying the geometry of these trees. This graph-based interpretation allows for the development of new methods and analysis of existing ones.

Result: Eight new methods were designed with at least six having optimal computational time complexity. All methods share the same iteration rate, but exhibit different trade-offs in terms of update frequency, communication efficiency, etc.

Conclusion: Birch SGD provides a unifying framework for understanding, analyzing, and designing efficient distributed SGD methods by navigating trade-offs among different approaches.

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [259] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

TL;DR: The paper presents a novel method for stable and convex Information Bottleneck optimization using symbolic continuation and entropy-regularized trajectories, proving solution path uniqueness and providing sensitivity analyses.


<details>
  <summary>Details</summary>
Motivation: The Information Bottleneck method often experiences unstable optimization with sudden representation shifts near critical points of the trade-off parameter beta.

Method: A new approach achieving stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories is introduced. Analytical proof of convexity and uniqueness of the IB solution path with an entropy regularization term is provided.

Result: This method stabilizes representation learning across a wide range of beta values and provides extensive sensitivity analyses around critical points with uncertainty quantification.

Conclusion: The open-source implementation, experimental results, and reproducibility framework offer a practical path for deploying and extending the proposed method.

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [260] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: Analyze the abstract of a paper introducing SDIFT, Sequential DIffusion in Functional Tucker space.


<details>
  <summary>Details</summary>
Motivation: Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations is crucial in scientific research but challenging. Current diffusion-based generative modeling approaches work well with on-grid data but struggle with sparsely observed real-world physical dynamics.

Method: The authors introduce SDIFT (Sequential DIffusion in Functional Tucker space), which uses functional Tucker model as latent space representer for universal approximation property. It represents observations as latent functions and Tucker core sequences. A sequential diffusion model with temporally augmented UNet in functional Tucker space denoises noise from Gaussian process to generate core tensor sequence. Additionally, a Message-Passing Posterior Sampling mechanism allows conditional generation guided by limited time step observations.

Result: SDIFT was validated on three physical systems across astronomical, environmental, and molecular domains. It showed significant improvements in reconstruction accuracy and computational efficiency compared to state-of-the-art methods.

Conclusion: SDIFT provides a novel framework for generating full-field evolution of physical dynamics from irregular sparse observations, outperforming existing methods in both accuracy and efficiency.

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [261] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

TL;DR: Digital textbooks produce learning log data used in EDM studies, but privacy concerns limit data integration across schools. This study proposes a method combining federated learning and differential features to overcome these challenges, demonstrating comparable performance to centralized learning while addressing privacy issues.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating confidential educational data across schools due to privacy concerns, which limits the development of high-performing and generalizable models.

Method: The study proposes a method that combines federated learning, which allows model training without centralizing data thus preserving student privacy, and differential features, which use relative values instead of absolute values to enhance model performance and generalizability.

Result: Experimental results showed that the proposed method resolves privacy concerns while achieving performance on par with centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Using differential features improved prediction performance across all datasets compared to non-differential approaches. The models were also effective for early prediction of at-risk students.

Conclusion: The proposed method successfully integrates data across schools while maintaining student privacy, leading to models with performance comparable to those trained via centralized learning, and offering improved prediction capabilities with differential features.

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [262] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

TL;DR: This paper introduces augmented Gini impurity and LACForest approach to handle the situation where an augmented class appears in testing data but not in training data. It also develops deep neural forests with a new optimization objective.


<details>
  <summary>Details</summary>
Motivation: Decision trees and forests have been successful in various applications, but most work with all testing classes known in training data. This research aims to address the challenge when an augmented class may appear in testing data yet not in training data.

Method: A new splitting criterion called augmented Gini impurity is introduced to incorporate information of the augmented class into trees' splitting. The LACForest approach constructs shallow forests based on this impurity and splits forests with pseudo-labeled augmented instances. Additionally, deep neural forests with a novel optimization objective are developed.

Result: Theoretical convergence analysis for augmented Gini impurity is presented. Experiments verify the effectiveness of the approaches proposed in the paper.

Conclusion: The paper successfully addresses learning with augmented class via forests by introducing augmented Gini impurity and developing the LACForest approach as well as deep neural forests.

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [263] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

TL;DR: The paper uses the Unconstrained Feature Model (UFM) to explore two questions in neural multivariate regression: the comparison between multi-task and single-task models, and the effect of whitening and normalizing regression targets. The UFM predicts, and empirical results confirm, that multi-task models have smaller training MSE with same or stronger regularization on single-task models. Whitening and normalizing reduce training MSE when the average variance across target dimensions is less than one.


<details>
  <summary>Details</summary>
Motivation: To provide qualitative insights into neural multivariate regression using the UFM framework, addressing two key questions about model performance and data preprocessing.

Method: Leveraging the UFM to theoretically predict and empirically validate the impact of multi-task versus single-task models and the effects of whitening and normalizing regression targets on training MSE.

Result: Empirical results confirm UFM predictions: multi-task models achieve smaller training MSE under certain regularization conditions, and whitening/normalizing reduces MSE when the average variance across target dimensions is less than one.

Conclusion: The UFM is a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [264] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

TL;DR: The paper proposes a multi-scale structural-temporal link prediction model (MUST) for unmanned aerial vehicle (UAV) ad hoc networks (UANETs). It uses graph attention networks (GATs) and long short-term memory (LSTM) networks to capture structural and temporal patterns, while addressing sparsity via a custom loss function. Experiments show MUST outperforms existing methods in dynamic and sparse UANETs.


<details>
  <summary>Details</summary>
Motivation: Existing link prediction methods focus on temporal dynamics at a single structural scale and neglect the effects of sparsity in UANETs, leading to insufficient information capture and limited applicability.

Method: The proposed model, MUST, first employs GATs to capture structural features at multiple levels (individual UAV, UAV community, and overall network). Then it uses LSTM networks to learn the temporal dynamics of these multi-scale structural features. A sophisticated loss function is introduced to address the impact of sparsity during model optimization.

Result: Extensive experimental results using simulated UANET datasets demonstrate that MUST achieves state-of-the-art performance in link prediction for highly dynamic and sparse UANETs.

Conclusion: MUST effectively captures meaningful structural and temporal patterns in UANETs, overcoming the challenges posed by their highly dynamic and sparse nature. It outperforms existing methods in link prediction tasks.

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [265] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

TL;DR: GreenFactory is an ensemble of zero-cost proxies that uses a random forest regressor to predict model test accuracy without training. Evaluated on NATS-Bench, it shows high Kendall correlations across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing zero-cost proxies for estimating the performance of deep neural networks during neural architecture search (NAS). Current proxies lack generalization and provide only relative rankings rather than predicted accuracies.

Method: The proposed method, GreenFactory, is an ensemble of zero-cost proxies that leverages a random forest regressor. It combines the strengths of multiple predictors to directly predict model test accuracy without the need for training.

Result: GreenFactory achieves robust results on NATS-Bench across multiple datasets. Specifically, it achieves high Kendall correlations: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120 on NATS-Bench-SSS; and 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120 on NATS-Bench-TSS.

Conclusion: GreenFactory demonstrates substantial agreement between its predicted scores and actual performance, showcasing its reliability in both search spaces.

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [266] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

TL;DR: This paper proposes a new calibration strategy called CleanSE to diminish the impact of false-positive labels in partial label learning by utilizing clean samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve disambiguation in partial label learning by leveraging the strong supervision information of clean samples within datasets, which existing strategies neglect.

Method: The method involves collecting clean samples to guide and enhance confidence in most possible candidates. It uses a differentiable count loss strategy and K-Nearest-Neighbor algorithm to attribute higher significance to reliable candidates. Clean samples also help characterize sample distributions by restricting label counts to a specific interval.

Result: Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL datasets show that this calibration strategy can be applied to most state-of-the-art PLL methods and enhances their performance.

Conclusion: CleanSE is an effective calibration strategy for partial label learning that leverages clean samples to improve disambiguation and overall performance.

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [267] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) are crucial for large-scale graph applications, but their computational demands require efficient methods. Mixed precision quantization offers a solution by enhancing GNN efficiency without sacrificing prediction performance. This paper presents a theorem for quantized message passing and introduces the MixQ-GNN framework, which selects optimal integer bit-widths for GNN components. MixQ-GNN integrates with existing methods and achieves significant reductions in bit operations.


<details>
  <summary>Details</summary>
Motivation: To address the high computational demands of GNNs and improve inference efficiency without affecting prediction performance.

Method: Developed a theorem for efficient quantized message passing that ensures numerical equality with full precision. Introduced MixQ-GNN, a framework that flexibly selects integer bit-widths for all components within GNN layers, optimizing efficiency while maintaining performance.

Result: MixQ-GNN achieved 5.5x reduction in bit operations for node classification and 5.1x reduction for graph classification compared to FP32 precision architectures.

Conclusion: Mixed precision quantization via MixQ-GNN effectively accelerates GNN inference while preserving prediction performance.

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [268] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

TL;DR: 研究了KANs中可学习激活函数在下肢假肢个性化控制中的潜力，并评估了用户特定数据与汇总数据对ML和DL模型性能的影响。结果表明，可学习激活函数在KAN和FKAN中没有显著优势，而用户特定数据在ML模型中表现更好，但在DL模型中无显著差异。这表明可学习激活函数可能在更复杂任务和更大数据集中有优势，同时DL模型可以利用汇总数据进行训练。


<details>
  <summary>Details</summary>
Motivation: 探索可学习激活函数在KANs中的应用潜力，以及用户特定数据与汇总数据对ML和DL模型在预测转向意图时的性能影响。目的是提高下肢假肢个性化控制的效果。

Method: 从五名下肢截肢者在实验室环境中执行转向任务时收集胫骨的IMU数据。使用MLP、KAN、CNN和FKAN四种模型进行即将发生转向的分类能力评估。通过比较MLP和KAN（针对ML模型）以及FKAN和CNN（针对DL模型），评估可学习激活函数的有效性。模型分别在用户特定数据和汇总数据上进行训练，以评估训练数据对性能的影响。

Result: 可学习激活函数在KAN和FKAN中并未显著优于MLP和CNN。对于ML模型，用户特定数据训练的结果显著优于汇总数据（p<0.05）。而对于DL模型，未观察到用户特定数据与汇总数据之间的显著差异。

Conclusion: 可学习激活函数可能在更复杂任务和更大数据集中具有明显优势。此外，在DL模型中，汇总训练表现出与用户特定训练相当的性能，表明假肢控制的模型训练可以利用来自多个参与者的数据。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [269] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath is a framework that enhances LLM-based path planning with safety guarantees, reducing uncertainty and collisions in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To address the safety concerns of overconfidence and hallucinations in Large Language Models (LLMs) when applied to autonomous driving path planning.

Method: SafePath operates in three stages: generating diverse candidate paths using an LLM, filtering out high-risk trajectories with conformal prediction while ensuring at least one safe option, and selecting the safest path or delegating to a human if uncertainty is too high.

Result: Theoretically proven to guarantee a safe trajectory with a user-defined probability. Experiments on nuScenes and Highway-env show a 77% reduction in planning uncertainty and up to a 70% reduction in collision rates.

Conclusion: SafePath effectively makes LLM-driven path planning safer by integrating formal safety guarantees, balancing autonomy and safety.

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [270] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: The paper constructs a convex smooth surrogate loss with linear surrogate regret bound for arbitrary discrete target losses.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between smoothness and linear regret bound for convex smooth surrogate losses.

Method: Constructing a convex smooth surrogate loss based on Fenchel-Young losses generated by the convolutional negentropy, equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk.

Result: Achieves a smooth loss while maintaining the surrogate regret bound linear, and provides a consistent estimator of the underlying class probability.

Conclusion: Demonstrates how convex analysis contributes to optimization and statistical efficiency in risk minimization.

Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap
between the convergence rates of surrogate and target losses, with linear
bounds favorable for their lossless regret transfer. While convex smooth
surrogate losses are appealing in particular due to the efficient estimation
and optimization, the existence of a trade-off between the smoothness and
linear regret bound has been believed in the community. That being said, the
better optimization and estimation properties of convex smooth surrogate losses
may inevitably deteriorate after undergoing the regret transfer onto a target
loss. We overcome this dilemma for arbitrary discrete target losses by
constructing a convex smooth surrogate loss, which entails a linear surrogate
regret bound composed with a tailored prediction link. The construction is
based on Fenchel-Young losses generated by the convolutional negentropy, which
are equivalent to the infimal convolution of a generalized negentropy and the
target Bayes risk. Consequently, the infimal convolution enables us to derive a
smooth loss while maintaining the surrogate regret bound linear. We
additionally benefit from the infimal convolution to have a consistent
estimator of the underlying class probability. Our results are overall a novel
demonstration of how convex analysis penetrates into optimization and
statistical efficiency in risk minimization.

</details>


### [271] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: The paper introduces Variational Rank Reduction Autoencoders (VRRAEs), which combines the strengths of Deterministic Rank Reduction Autoencoders (RRAEs) and Variational Autoencoders (VAEs). By applying careful sampling and KL divergence regularization to RRAEs' latent space, VRRAEs outperform both RRAEs and VAEs in random generation and interpolation tasks on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Autoencoders can be regularized by using a truncated Singular Value Decomposition (SVD) to limit the rank of their latent space. This method is powerful but deterministic, making it counter-intuitive for generative purposes. VAEs, however, are known for their generative capabilities due to their probabilistic latent space. The motivation is to create a model that leverages the advantages of both RRAEs and VAEs.

Method: The authors propose VRRAEs, which combine the latent space regularization of RRAEs with the probabilistic approach of VAEs. They apply a truncated SVD for rank reduction and use KL divergence to further regularize the latent space, allowing for better generative performance.

Result: VRRAEs outperform both RRAEs and VAEs in various tasks including random generation and interpolation, as measured by the FID score. The model shows robustness against posterior collapse, even on small synthetic datasets, and performs well on real-world datasets such as MNIST, CelebA, and CIFAR-10.

Conclusion: By integrating the strengths of RRAEs and VAEs, VRRAEs provide superior generative performance and reduce the risk of posterior collapse. The findings indicate that VRRAEs are a promising advancement in the field of autoencoders.

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [272] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

TL;DR: 提出了一种名为AdaLin的新方法，通过动态调整每个神经元的激活函数来缓解深度神经网络中的塑性损失问题。该方法在多种任务和场景中表现出色，并且不需要额外的超参数或明确的任务边界。


<details>
  <summary>Details</summary>
Motivation: 研究动机来源于近期发现深层线性网络对塑性损失具有较强的抵抗力。为了在非平稳问题设定下改善模型的增量学习能力，需要一种新方法来减轻神经网络中的塑性损失。

Method: 提出了Adaptive Linearization（AdaLin）方法，为每个神经元配备一个可学习参数和门控机制，基于梯度流动将线性注入到激活函数中，从而实现自适应调节，确保足够的梯度信号并持续进行连续学习。

Result: AdaLin与传统激活函数（如ReLU、Tanh和GeLU）结合使用时，在标准基准测试（如Random Label和Permuted MNIST等）上显著提高了性能。此外，在更复杂的场景（如CIFAR-100上的类增量学习和离策略强化学习代理中）也验证了其有效性。

Conclusion: AdaLin是一种通用的方法，能够在不增加额外超参数或无需明确任务边界的情况下有效缓解塑性损失，提升模型在多种任务和场景中的表现。系统性的消融实验表明，神经元级别的适应对于良好性能至关重要。

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [273] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

TL;DR: The paper explores how post-training methods modify language models, focusing on their brittleness. It proposes Layered Unlearning (LU), an unlearning algorithm that enhances robustness to adversarial relearning in language models.


<details>
  <summary>Details</summary>
Motivation: To understand the modifications and brittleness caused by post-training methods such as fine-tuning, alignment, and unlearning in language models.

Method: Design an unlearning algorithm called Layered Unlearning (LU) which creates distinct inhibitory mechanisms for subsets of data during different stages, limiting relearning abilities on parts of the dataset.

Result: LU improves robustness to adversarial relearning for several different unlearning methods when evaluated through synthetic and large language model experiments.

Conclusion: The study contributes to the state-of-the-art in machine unlearning and offers insights into the effects of post-training updates.

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [274] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kahou,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: This paper explores the fairness issues in tabular in-context learning (ICL) and proposes three preprocessing strategies to mitigate bias, finding that uncertainty-based demonstration selection improves group fairness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to understand and address potential biases in tabular in-context learning (ICL), which has emerged as a powerful method for making predictions on structured data without updating model parameters.

Method: The authors investigate fairness implications of tabular ICL and explore three preprocessing strategies: correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection.

Result: Experiments show that uncertainty-based demonstration selection consistently enhances the group fairness of in-context predictions.

Conclusion: The study concludes that uncertainty-based demonstration selection is an effective strategy to improve fairness in tabular ICL.

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [275] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

TL;DR: The paper investigates gradient flows in fully connected feedforward neural networks with smooth activation functions, showing that the gradient flow either converges to a critical point or diverges to infinity while the loss approaches an asymptotic critical value. For well-initialized models, the loss can converge to zero for polynomial target functions given sufficient architecture and data. However, in practical settings, gradient flows often diverge to infinity.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of gradient flows in the context of loss landscapes for neural networks with smooth activation functions.

Method: Analysis of gradient flows in fully connected feedforward neural networks using continuously differentiable activation functions. Theoretical proofs regarding convergence or divergence of gradient flows, and numerical experiments confirming these findings in real-world scenarios.

Result: Gradient flows either converge to a critical point or diverge to infinity, with the loss approaching an asymptotic critical value. For polynomial target functions, optimal loss is zero but only achievable asymptotically. In practice, gradient flows with good initialization tend to diverge to infinity.

Conclusion: Gradient flows in neural networks exhibit specific convergence or divergence behaviors, depending on initialization and target function complexity. The theoretical results are supported by numerical experiments.

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [276] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

TL;DR: The paper introduces Rhomboid Tiling (RT) clustering and RTPool, a hierarchical graph clustering pooling model, which leverages complex geometric information to extract higher-order geometric structures. It outperforms 21 state-of-the-art competitors on 7 benchmark datasets for graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current message passing frameworks in Graph Neural Networks rely heavily on the connectivity structure of graphs, limiting their ability to capture rich geometric features inherent in geometric graphs.

Method: The authors propose Rhomboid Tiling (RT) clustering, a novel method based on the rhomboid tiling structure that clusters data leveraging its complex geometric information. They also design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks.

Result: RTPool demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

Conclusion: Rhomboid Tiling (RT) clustering and RTPool effectively extract higher-order geometric structures and perform exceptionally well in graph classification tasks.

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [277] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper presents Online-iForest, a new method for anomaly detection in streaming data that performs well compared to other online methods and offline techniques with periodic retraining, while being more efficient.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods, both offline and online, are not fully suitable for streaming contexts due to impractical assumptions or the need for periodic retraining.

Method: Online-iForest is a novel anomaly detection method designed specifically for streaming data conditions. It tracks the evolving data generating process over time without requiring periodic retraining.

Result: Experimental results on real-world datasets show that Online-iForest performs comparably to other online methods and offline techniques with periodic retraining, but it outperforms them in terms of efficiency.

Conclusion: Online-iForest is a promising solution for applications requiring fast anomaly detection, such as cybersecurity, fraud, and fault detection.

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [278] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

TL;DR: The paper presents Adversarial Suffix Filtering (ASF), a lightweight and model-agnostic defensive pipeline to protect LLMs against adversarial suffix attacks, reducing attack efficacy below 4% without significantly affecting non-adversarial performance.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are increasingly deployed in various environments but remain vulnerable to jailbreak attacks. Current defense mechanisms either require access to internal model architecture, increase computational costs, or can be bypassed through simple methods.

Method: ASF is an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes from prompts. It is designed as a lightweight, model-agnostic pipeline that can operate effectively in both black-box and white-box settings.

Result: ASF reduces the success rate of state-of-the-art adversarial suffix attacks to below 4%, while having minimal impact on the performance of LLMs in non-adversarial scenarios.

Conclusion: ASF offers a comprehensive defense solution for protecting LLMs against adversarial suffix attacks, enhancing their security and trustworthiness.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [279] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: The paper proposes LAS, a loss-less ANN-SNN conversion method for fully spike-driven LLMs that introduces novel neurons and tailored spike-equivalent Transformer components. Experiments show its effectiveness with no performance loss and even accuracy improvement on certain tasks.


<details>
  <summary>Details</summary>
Motivation: To effectively obtain spiking LLMs with energy efficiency while overcoming the challenges of activation outliers and incompatible nonlinear operations in existing ANN-to-SNN conversion methods.

Method: LAS proposes two novel neurons to handle activation outlier and nonlinear operation issues in ANN-based LLMs. It also tailors spike-equivalent Transformer components ensuring full spiking conversion without any performance loss.

Result: Experimental results on six language models and two vision-language models demonstrate loss-less conversion. Notably, on OPT-66B, LAS improves the accuracy by 2% on the WSC task.

Conclusion: LAS achieves loss-less ANN-SNN conversion for fully spike-driven LLMs. The parameter and ablation studies verify its effectiveness.

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [280] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: This paper presents a method to adapt large language models (LLMs) for execution on analog in-memory computing (AIMC) hardware, overcoming challenges like noise and quantization constraints. The approach allows state-of-the-art LLMs to retain performance comparable to digital 4-bit weight, 8-bit activation baselines while being executed on AIMC hardware.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing (AIMC) offers potential improvements in speed and power efficiency for neural network inference compared to traditional von Neumann-based architectures. However, it introduces challenges such as noisy computations and strict input/output quantization constraints that hinder the deployment of LLMs on AIMC-based hardware.

Method: The researchers developed a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. This involves training methodologies that enable LLMs to overcome the accuracy gap when deployed on AIMC hardware.

Result: The approach successfully enables state-of-the-art LLMs, such as Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct, to retain performance comparable to 4-bit weight, 8-bit activation baselines despite analog noise and quantization constraints. Additionally, the models can be quantized for inference on low-precision digital hardware and benefit from test-time compute scaling.

Conclusion: This work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models.

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [281] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: Graph unlearning is essential for user privacy protection. However, it may introduce bias when deleting user information. This paper proposes FGU, a fair graph unlearning method that ensures fairness and maintains privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of bias introduction in the process of graph unlearning while protecting user privacy.

Method: FGU trains shard models on partitioned subgraphs, unlearns the requested data from the corresponding subgraphs, and retrains the shard models on the modified subgraphs. It employs a bi-level debiasing process to ensure fairness at both shard-level and global-level.

Result: Experiments show that FGU achieves superior fairness while maintaining privacy and accuracy. It is also robust to diverse unlearning requests.

Conclusion: FGU is an effective method for fair graph unlearning that can protect user privacy, maintain model performance, and reduce bias.

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [282] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 研究了联邦学习在AIoT场景中的能耗问题，提出两种基于聚类的设备选择方法以优化模型训练的收敛速度和能源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习研究多关注模型性能、收敛速度和通信效率，而忽视了其在AIoT场景中的能耗影响。

Method: 通过观察发现设备/客户端选择对分布式AIoT环境下的模型训练收敛速度至关重要，进而提出两种基于聚类的方法，将具有相似标签分布的AIoT设备分组，形成几乎异构的设备集群，从而缓解实际分布式学习应用中的异构性问题。

Result: 通过广泛的数值实验表明，所提出的聚类策略通常能够在保持低能耗的同时实现高收敛速度，优于其他近期文献中的方法。

Conclusion: 该研究表明，在AIoT场景中，通过合理的设备选择可以有效降低联邦学习的能耗并提高训练效率。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [283] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 研究了深度形态神经网络（DMNNs），提出了几种新架构并在参数上施加不同约束，证明所提网络可成功训练且比线性网络更易剪枝，还提出一种结合线性和形态层的混合架构以加速大批次梯度下降收敛。


<details>
  <summary>Details</summary>
Motivation: 探索深度形态神经网络（DMNNs）的工作机制及其优化潜力，尤其是激活函数在线性与非线性层间的作用。

Method: 提出多种DMNNs架构并分别在参数和可学习参数上设置不同的约束条件，其中要求大部分参数参与形态学操作；同时设计一种混合网络架构将线性和形态层相结合。

Result: 实验表明所提出的网络能够成功训练，比线性网络更容易剪枝，且形态层的加入可以显著加速大批次梯度下降的收敛速度。但网络的泛化能力仍然有限。

Conclusion: 首次成功训练了受特定约束的DMNNs，并验证了其可训练性和易于剪枝的特点，同时也展示了形态层对加速收敛的效果，但仍需改进网络的泛化性能。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [284] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 为了实现OOD，智能系统需要识别适当的任务不变和可组合的输入特征及组合方法。我们提出仅仅测试OOD设置不足以确认算法确实从数据中学习了组合结构，还需要确认识别的特征确实是组合性的。通过两个任务展示了三个常用神经网络无法解决OOD问题，并开发了两种新型网络架构以在OOD场景中取得成功。即使有正确的偏差和接近完美的OOD性能，算法仍可能无法学习到正确的组合泛化特征。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络在处理OOD问题时存在局限性，需要探索新的方法来实现真正的组合泛化。

Method: 设计两个具有明确OOD度量的任务，测试三种常见神经网络（MLP、CNN、Transformer）在OOD场景中的表现；开发两种新型网络架构，引入偏差使其在OOD场景中表现良好；分析算法是否能够学习到正确的组合特征。

Result: 三种常见神经网络无法解决特定OOD任务；两种新型网络架构在OOD场景中表现出色，但仍可能存在未能学习到正确组合特征的情况。

Conclusion: 仅仅测试OOD性能不足以验证算法是否真正学习了组合结构，还需确认其识别的特征是否具有组合性。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [285] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: This paper presents a federated learning methodology that addresses data quality issues such as noisy labels, missing classes, and imbalanced distributions. It enhances data integrity through adaptive noise cleaning, synthetic data generation, and robust model training. Experiments on MNIST and Fashion-MNIST show significant improvements in macro-F1 Score under varying conditions. The framework is practical for edge devices while maintaining data privacy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges due to data quality issues like noisy labels, missing classes, and imbalanced distributions which affect its effectiveness.

Method: The proposed method includes adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation for class imbalance and missing labels, and robust federated model training.

Result: Experimental evaluations on benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant improvements in the macro-F1 Score under different noise and class imbalance conditions.

Conclusion: The proposed methodology effectively mitigates common data quality challenges providing a robust, scalable, and privacy-compliant solution suitable for real-world federated learning scenarios.

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [286] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: We propose a generative end-to-end solver for black-box combinatorial optimization which learns the structure of energy landscape and facilitates global optimization.


<details>
  <summary>Details</summary>
Motivation: Current methods for black-box combinatorial optimization may not perform well in terms of sample efficiency and solution quality simultaneously, especially on NP problems.

Method: The method treats the black-box objective as an energy function and trains a neural network to model the associated Boltzmann distribution. By conditioning on temperature, the network captures different distributions that help in learning the structure of the energy landscape.

Result: Validated on challenging combinatorial tasks with both limited and unlimited query budgets, showing competitive performance against state-of-the-art black-box optimizers.

Conclusion: This approach can improve sample efficiency when queries are expensive and learn implicit variable interactions when queries are cheap but the problem is hard.

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [287] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: A new MARL framework based on community structures is proposed, enabling flexible coordination patterns and structured information sharing.


<details>
  <summary>Details</summary>
Motivation: Existing MARL frameworks often rely on neighbor-based or fixed interaction graphs, which may not capture complex coordination patterns effectively.

Method: The method involves creating a community-based framework where agents can belong to multiple overlapping communities. Each community maintains shared policy and value functions that agents aggregate according to membership weights. Actor-critic algorithms are designed to exploit this structure for policy updates and value learning.

Result: The approach supports transfer learning and active learning, with theoretical convergence guarantees under linear function approximation for both actor and critic updates.

Conclusion: This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees.

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [288] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: 本研究探讨了在存在噪声和对抗性数据管理的情况下，生成模型在自我消耗再训练循环中的演变，并设计了针对竞争对抗场景的攻击算法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的进步，区分真实数据和合成数据变得越来越困难。使用合成数据进行后续训练可能导致模型崩溃或训练不稳定。此外，合成数据通常受到人为反馈的影响并根据用户偏好进行管理，这可能使得模型收敛到优化这些偏好的分布。然而，在实践中，数据管理往往是嘈杂的或被对抗性地操控。

Method: 作者对生成模型进行了理论分析，以评估这种嘈杂的数据管理对其影响，并确定再训练过程稳健性的条件。基于这一分析，设计了用于竞争对抗场景的攻击算法，其中平台通过恶意用户干扰对手模型与实际用户偏好的一致性。

Result: 实验结果表明，所提出的算法在合成和真实世界数据集上都具有有效性。

Conclusion: 本研究揭示了嘈杂和对抗性数据管理对生成模型的影响，并提供了确保再训练过程稳健性的条件。同时，所设计的攻击算法在竞争对抗场景中展现了其有效性。

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [289] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.LG

TL;DR: This case study applies a phased hyperparameter optimization process to compare multitask natural language model variants.


<details>
  <summary>Details</summary>
Motivation: To optimize hyperparameters in multitask natural language models utilizing multiphase learning rate scheduling and optimizer parameter grouping.

Method: Employ short, Bayesian optimization sessions that leverage multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance using the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn Gaussian process minimization. Initially use low-fidelity sprints to prune the hyperparameter space and progressively increase fidelity. Use a meta-learner to tune threshold values for classification probabilities during inference.

Result: Demonstrates the method on variants of the 2021 Joint Entity and Relation Extraction model proposed by Eberts and Ulges.

Conclusion: The phased hyperparameter optimization process is successfully applied to compare multitask natural language model variants.

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [290] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: This paper analyzes checkpoint data in Large Language Models (LLMs) training and proposes a optimized compression solution, Language Model Compressor (LMC).


<details>
  <summary>Details</summary>
Motivation: The large volume of checkpoint data during LLMs training leads to significant resource consumption. Reducing the data volume can improve efficiency.

Method: Analyze the tensor data evolution during model training, evaluate existing compression engines and optimization techniques, then build a new compression solution named LMC based on byte-grouping and Huffman encoding.

Result: LMC outperforms BZ2 with significantly less compression time. A 16-core parallel implementation of LMC achieves high compression and decompression throughput, reducing CPU resources needed and allowing higher-frequency checkpoints.

Conclusion: The proposed LMC is an effective compression solution for checkpoint data in LLMs training, improving efficiency and enabling more frequent checkpoints.

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [291] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 中风是全球第二大死亡原因和第三大致残原因。本研究探讨了使用人口统计学、临床和生活方式数据的机器学习算法在预测中风风险方面的有效性。通过解决类别不平衡和缺失数据等方法论挑战，评估了包括逻辑回归、随机森林和XGBoost在内的多种模型的性能。结果表明，尽管这些模型具有高准确性，但敏感性仍然是实际临床应用中的限制因素。此外，确定了最有影响力的预测特征，并提出了改进基于机器学习的中风预测的策略。这些发现有助于开发更可靠和可解释的早期中风风险评估模型。


<details>
  <summary>Details</summary>
Motivation: 中风是一个关键的全球健康挑战，迫切需要有效的预测工具来降低其影响。现有的预测方法可能无法充分处理数据中的复杂性和挑战，例如类别不平衡和缺失数据。因此，有必要探索和优化机器学习算法在中风风险预测中的应用。

Method: 使用来自中风预测数据集的人口统计学、临床和生活方式数据，评估了逻辑回归、随机森林和XGBoost等机器学习模型的性能。解决了类别不平衡和缺失数据等方法论问题。

Result: 尽管所评估的模型具有高准确性，但敏感性仍然是一个限制因素，可能影响其在临床环境中的实际应用。同时识别出了最有影响力的预测特征。

Conclusion: 为了提高中风风险预测的可靠性和可解释性，需要进一步改进机器学习模型。确定的关键特征和提出的策略为未来的研究提供了方向。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [292] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: This paper presents an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method for jailbreaking Large Language Models (LLMs). It ensures one-hot encoding stays within the probability simplex, proves its convergence, and demonstrates higher success rates and efficiency compared to other techniques.


<details>
  <summary>Details</summary>
Motivation: To systematically understand LLMs and enhance their safety by developing a more effective method to conduct adversarial attacks or 'jailbreak' these models.

Method: The method involves using exponentiated gradient descent combined with the Bregman projection method to optimize within the probability simplex. This approach aims to overcome limitations of discrete token search and continuous token embedding optimizations.

Result: The proposed technique successfully jailbreaks several widely used LLMs with higher success rates and great efficiency on five open-source LLMs across four datasets, outperforming three state-of-the-art methods.

Conclusion: The developed intrinsic optimization technique is proven to be effective in adversarial attacks on LLMs, demonstrating superior performance and efficiency.

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [293] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 本研究提出了一种用于从平滑信号中学习Kronecker结构化乘积图的交替优化方案，并提供了理论收敛保证，实验表明该方法在合成和真实世界图上的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近对能够自然分解不同方式间依赖关系的乘积图的兴趣日益浓厚，但可用于建模多样化依赖结构的图乘积类型仍然有限。因此，研究如何学习Kronecker结构化的乘积图变得尤为重要，因为Kronecker乘积以更复杂、不可分的方式建模依赖关系。

Method: 为了解决这个非凸问题，作者提出了一种交替方案来优化每个因子图，并为该方案的渐近收敛提供了理论保证。此外，还修改了该算法以学习强乘积的因子图。

Result: 通过在合成和真实世界的图上进行实验，证明了所提出方法的有效性和优于现有方法的性能。

Conclusion: 该研究表明，所提出的交替优化方案可以成功地学习Kronecker结构化乘积图，并且在处理复杂依赖关系方面表现出色，具有理论支持和实证验证。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [294] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: This paper presents Causal Predictive Optimization and Generation, a three-layered approach to optimize the sales process in B2B businesses. It includes prediction with causal ML, optimization with constraint optimization and contextual bandit, and serving with Generative AI. The system was deployed in LinkedIn, showing significant improvements over legacy systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to introduce a principled approach to optimize the sales process, which is crucial for the success of any B2B business.

Method: The method involves a three-layered system: 1) Prediction layer using causal ML, 2) Optimization layer using constraint optimization and contextual bandit, 3) Serving layer using Generative AI and a feedback-loop for system enhancement.

Result: The deployment of this system in LinkedIn showed significant wins over legacy systems.

Conclusion: This work details the implementation and deployment of the Causal Predictive Optimization and Generation system in LinkedIn, demonstrating its effectiveness and sharing insights broadly applicable to the field of sales optimization and business AI.

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [295] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: This paper presents a novel approach for Alzheimer's disease detection using radiogenomic data, including structural MRI images and gene expression data, with a heterogeneous bipartite graph representation learning framework that can classify AD into three stages and identify significant genes.


<details>
  <summary>Details</summary>
Motivation: To integrate imaging and genomic data to gain new insights into diseases and demonstrate the potential of radiogenomic-based classification for other diseases.

Method: A heterogeneous bipartite graph representation learning framework featuring two distinct node types (genes and images) is used to classify Alzheimer's disease into three stages (AD, MCI, CN) with a small dataset.

Result: The network effectively classifies Alzheimer's disease into three distinct stages and identifies significant genes in each classification group, evaluated by metrics such as accuracy, recall, precision, and F1 score.

Conclusion: The proposed technique shows potential for extending radiogenomic-based classification to other diseases.

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [296] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: The paper introduces intrinsic entropy via zentropy theory and proposes a ZENN model for handling heterogeneous data, demonstrating its effectiveness in classification tasks, energy landscape reconstructions, and material behavior predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating heterogeneous datasets with intrinsic disparities in various domains.

Method: Extending zentropy theory by introducing intrinsic entropy and proposing a zentropy-enhanced neural network (ZENN) that learns both energy and intrinsic entropy components. The neural network architecture is redesigned to reflect the intrinsic properties and variability in diverse datasets.

Result: ZENN shows superior generalization capabilities and robustness in classification tasks and energy landscape reconstructions, especially in predicting high-order derivatives. It successfully reconstructs the Helmholtz energy landscape of Fe3Pt and captures key material behaviors.

Conclusion: The study presents a novel data-driven machine learning approach based on zentropy theory, showcasing ZENN as a versatile deep learning framework for scientific problems involving complex, heterogeneous datasets.

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [297] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Chisme is a new suite of protocols for robust intelligence at the network edge, including Chisme-DFL and Chisme-GL, which outperform standard methods in distributed and heterogeneous data model training.


<details>
  <summary>Details</summary>
Motivation: To address challenges in implementing robust intelligence at the network edge, such as heterogeneous data distributions, episodic connectivity, and lack of infrastructure.

Method: Introduced Chisme-DFL (synchronous decentralized FL) and Chisme-GL (asynchronous gossip learning), along with a data similarity heuristic that allows agents to infer affinity with each other using model updates. The heuristic extends DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration.

Result: Chisme methods outperform standard counterparts in model training over distributed and heterogeneous data across various network scenarios.

Conclusion: Chisme provides effective solutions for privacy-preserving distributed learning at the network edge under challenging conditions.

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [298] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: Transformer模型通过权重内学习(IWL)和上下文中学习(ICL)两种模式获取知识。受进化生物学启发，研究探讨了环境可预测性对这两种学习模式平衡的影响，揭示了任务相关的时间演化规律，并提出了相对成本假说来解释这些转变。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解Transformer模型中IWL与ICL之间的相互作用，研究从进化生物学中获得灵感，将IWL类比为遗传编码，ICL类比为表型可塑性，并探索环境可预测性如何影响这两种学习模式的平衡。

Method: 通过回归和分类任务实验操作环境可预测性的维度，系统地研究其对IWL/ICL平衡的影响。分析高环境稳定性对IWL的偏好以及高线索可靠性对ICL效能的提升，并观察不同任务条件下的时间演化规律。

Result: 高环境稳定性显著促进IWL，而低稳定性下高线索可靠性增强ICL效能；任务难易度和学习动态决定了IWL与ICL的转换过程，支持了相对成本假说。

Conclusion: 环境可预测性是决定Transformer模型适应策略的关键因素，研究结果为理解ICL提供了新视角，并指导了训练方法的发展。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [299] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: The paper presents a unified transformer-based approach for data driven attribution that can process different types of data and external factors, with significant impact shown through its implementation at LinkedIn.


<details>
  <summary>Details</summary>
Motivation: Data Driven Attribution is crucial for modern marketing intelligence, requiring advanced methods to assign conversion credits based on causal patterns learned from data.

Method: A unified transformer-based attribution approach is introduced, capable of handling member-level data, aggregate-level data, and integrating external macro factors.

Result: The approach was successfully implemented at large scale at LinkedIn, demonstrating significant impact, with learnings applicable broadly to marketing and ad tech fields.

Conclusion: The transformer-based attribution method offers an effective solution for data driven attribution, showcasing potential for broad application in marketing and advertising platforms.

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [300] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper introduces KQT-EWMA, a non-parametric change-detection algorithm that merges Kernel-QuantTree histogram and EWMA statistic for online multivariate data stream monitoring. It allows controlling false alarms using a pre-determined ARL_0 and demonstrates performance comparable to state-of-the-art methods in experiments.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective change-detection algorithms that can monitor multivariate data streams online with controlled false alarm rates, particularly those that can operate at a pre-determined Average Run Length (ARL_0). Most existing non-parametric methods lack the ability to control ARL_0 a priori.

Method: The method involves combining the Kernel-QuantTree (KQT) histogram with the Exponentially Weighted Moving Average (EWMA) statistic to create the KQT-EWMA algorithm. This algorithm is designed to monitor multivariate data streams in a flexible and practical manner, allowing for non-parametric monitoring without dependence on the data distribution in stationary conditions.

Result: Experiments conducted on both synthetic and real-world datasets show that KQT-EWMA is capable of controlling ARL_0 while achieving detection delays that are either comparable to or lower than other advanced methods designed for similar conditions.

Conclusion: KQT-EWMA represents an advancement in non-parametric change-detection by providing a way to control false alarms through the use of a pre-determined ARL_0. It offers competitive performance in terms of detection delay compared to state-of-the-art techniques.

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [301] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO is a new method for pruning neural network models that reduces computational costs while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the high computational and financial costs associated with training and operating large machine learning models, as well as the limitations of current pruning methods that are computationally and environmentally taxing.

Method: BINGO studies specific subsets of a neural network during the training pass to determine the significance of each weight in contributing to the network's accuracy. It then generates a significance score for each weight, enabling the pruning of insignificant weights in one step.

Result: BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than existing methods.

Conclusion: BINGO offers a solution where AI advancement does not necessarily lead to increased model sizes and computational demands.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [302] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: The paper surveys adversarial attacks in multimodal models encompassing text, image, video, and audio, highlighting the evolution of threats and providing a comprehensive view of the threat landscape.


<details>
  <summary>Details</summary>
Motivation: Multimodal models are vulnerable to adversarial attacks across different modalities. With practitioners increasingly adopting these models for real-world applications, there is a need for a focused view on attack types and preventive actions.

Method: The paper conducts a survey of adversarial attacks targeting all four modalities: text, image, video, and audio, to provide an understanding of the threat landscape and its evolution.

Result: A comprehensive summarization of the threat landscape in the multimodal world is presented, which includes adversarial attacks on text, image, video, and audio modalities.

Conclusion: This survey is the first of its kind to outline the adversarial attack landscape in multimodal models, helping practitioners understand potential threats and take necessary actions.

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [303] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: Large language models (LLMs) show human-like exploration-exploitation behavior in simple tasks but struggle to adapt in complex environments.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs can simulate human decision-making behavior and achieve comparable or superior performance, particularly focusing on the exploration-exploitation tradeoff.

Method: Using multi-armed bandit tasks from cognitive science and psychiatry, the study compares the E&E strategies of LLMs, humans, and MAB algorithms through interpretable choice models. It also examines how reasoning, prompted by strategies or enhanced models, influences LLM decision-making.

Result: Reasoning makes LLMs more human-like in their decision-making, with similar levels of random and directed exploration in simple tasks. However, in complex environments, LLMs lack adaptability compared to humans, especially in directed exploration.

Conclusion: LLMs have potential as tools for simulating human behavior and automated decision-making, but they face limitations, especially in adapting to non-stationary environments.

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [304] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: The paper proposes a hybrid deep learning model TCN-MLP-Attention for Hass avocado price forecasting, achieving RMSE of 1.23 and MSE of 1.51.


<details>
  <summary>Details</summary>
Motivation: Agricultural product price forecasting has become increasingly important with the growing demand for healthy foods. Traditional prediction models struggle with highly nonlinear and dynamic data like Hass avocado prices which are influenced by seasonality, region, and weather.

Method: The proposed method is a hybrid deep learning model named TCN-MLP-Attention Architecture that combines Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting.

Result: The model was trained on a dataset with over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018. The experimental results show excellent predictive performance with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.

Conclusion: This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [305] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 本研究探讨了在流形约束数据上直接采样欧几里得扩散模型的方法，揭示了分数函数的多尺度奇异性，并提出了两种改进采样精度的新方法：Niso-DM和Tango-DM。


<details>
  <summary>Details</summary>
Motivation: 尽管欧几里得扩散模型在生成建模领域取得了显著成功，并已被扩展到流形情况，但现有方法通常显式利用特定流形的结构。本文旨在研究如何对一般流形约束数据直接进行采样。

Method: 1. 分析分数函数在流形嵌入空间中的多尺度奇异性；2. 提出Niso-DM方法，通过在法线方向引入非各向同性噪声来减少尺度差异；3. 提出Tango-DM方法，仅训练分数函数的切线分量并使用切线损失函数。

Result: 数值实验表明，所提出的方法在具有复杂几何形状的各种流形分布上表现出优越的性能。

Conclusion: 本文揭示了分数函数的奇异性问题，并通过提出的两种方法有效提高了流形约束数据上的采样精度。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [306] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: This paper introduces RiCL, a framework for interactive continual learning that addresses limitations of traditional methods by handling dynamic real-time feedback and noisy labels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon traditional continual learning methods which struggle with static datasets and the assumption of clean labels, by enabling AI models to dynamically learn new skills from real-time human feedback while retaining prior knowledge.

Method: RiCL incorporates three key components: a temporal consistency-aware purifier to discern clean from noisy samples; an interaction-aware direct preference optimization strategy to align model behavior with human intent; and a noise-resistant contrastive learning module that captures robust representations.

Result: Extensive experiments on two benchmark datasets (FewRel and TACRED) demonstrate that RiCL substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods.

Conclusion: RiCL effectively addresses the limitations of traditional continual learning paradigms by incorporating mechanisms to handle dynamic feedback and noisy labels.

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [307] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: This paper explores the use of large language models (LLMs), specifically a fine-tuned Llama3 8B model, for analyzing freeway crash data and identifying causation factors without pre-labeled data through zero-shot classification. The research compiles 226 traffic safety studies to create a training dataset covering various factors related to crashes. Results show that LLMs effectively identify primary causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. The model's practical applicability is validated by high agreement among researchers.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors contributing to traffic crashes and their unique characteristics. Thus, there is a need for more advanced tools that can provide comprehensive analysis.

Method: The research leverages a large language model (LLM) to analyze freeway crash data. A training dataset was created from 226 traffic safety studies encompassing environmental, driver, traffic, and geometric design factors. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors. Zero-shot classification was used to identify crash causation without pre-labeled data.

Result: Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%.

Conclusion: This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. It provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices.

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [308] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: This paper focuses on long-term continual learning (CL), proposing a novel framework called Long-CL inspired by human memory mechanisms to mitigate catastrophic forgetting and presenting two benchmarks for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods struggle with the challenges of long-term CL, particularly with handling a significantly larger number of tasks and avoiding catastrophic forgetting.

Method: The authors introduce a task-core memory management strategy and a long-term memory consolidation mechanism. The former indexes crucial memories and adaptively updates them, while the latter selectively retains hard and discriminative samples. Two benchmarks, MMLongCL-Bench and TextLongCL-Bench, are also introduced.

Result: Experimental results indicate that Long-CL surpasses the previous state-of-the-art methods by 7.4% and 6.5% AP on the two benchmarks respectively.

Conclusion: The proposed Long-CL framework effectively addresses the issues in long-term CL, providing a significant improvement over existing methods.

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [309] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: The paper introduces TransPL, a novel approach for unsupervised domain adaptation (UDA) in time series data. It uses code transition matrices derived from vector quantization to model temporal transitions and channel-wise shifts between domains, generating explainable pseudo-labels using Bayes' rule.


<details>
  <summary>Details</summary>
Motivation: Traditional pseudo-labeling strategies fail to capture temporal patterns and channel-wise shifts in UDA for time series data, resulting in sub-optimal pseudo-labels.

Method: TransPL models the joint distribution of the source domain through code transition matrices derived from vector quantization of time series patches. It constructs class- and channel-wise code transition matrices and applies Bayes' rule for target domain adaptation, producing pseudo-labels based on channel-wise weighted class-conditional likelihoods.

Result: TransPL outperforms state-of-the-art pseudo-labeling methods by a significant margin (6.1% accuracy improvement, 4.9% F1 improvement) on four time series UDA benchmarks.

Conclusion: TransPL provides a robust solution for UDA in time series data, offering explicit modeling of temporal transitions and channel-wise shifts, versatility towards different UDA scenarios, and explainable pseudo-label generation.

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [310] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: In this paper, the authors propose FedRAG, a federated reinforcement learning (FRL) framework that enhances performance and protects privacy by sharing an approximated behavior metric-based state projection function rather than sensitive information.


<details>
  <summary>Details</summary>
Motivation: Current FRL methods share encrypted local state or policy information to help clients learn from each other while preserving privacy. The authors aim to explore a new way to enhance FRL performance and protect sensitive information more effectively.

Method: The authors introduce FedRAG, which learns a computationally practical projection function of states for each client and aggregates the parameters of these functions at a central server. This approach shares no task-specific sensitive information but still provides information gain for each client.

Result: Extensive experiments on the DeepMind Control Suite demonstrate insightful results, indicating that the proposed method is effective in enhancing FRL performance while protecting privacy.

Conclusion: FedRAG is a promising FRL framework that enhances performance and protects privacy by sharing approximated behavior metric-based state projection functions.

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [311] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: A machine learning framework for heart disease prediction using 303 samples and 14 features was developed. Random Forest classifier achieved the best performance with 91% accuracy, showing potential in clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: To develop an effective predictive model for heart disease using machine learning techniques to aid clinical decision-making.

Method: The study utilized a heart-disease dataset with 303 samples and 14 features. Data preprocessing was performed followed by model training and evaluation using Logistic Regression, KNN, and Random Forest classifiers. Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV.

Result: Random Forest classifier outperformed other models with an accuracy of 91% and F1-score of 0.89. Evaluation metrics indicated balanced performance across classes.

Conclusion: The proposed machine learning-based framework shows strong potential for aiding clinical decision-making in predicting heart disease. However, limitations such as dataset size and generalizability highlight the need for future studies.

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [312] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.LG

TL;DR: In order to reduce the cost of poisoning attacks in federated learning, this paper proposes a sybil-based virtual data poisoning attack method, which can generate sybil nodes to amplify the impact of poisoning model and uses gradient matching to reduce the computational complexity. In addition, three schemes for target model acquisition are designed for different scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to poisoning attacks by malicious adversaries, but existing methods often involve high costs to achieve effective attacks.

Method: The authors propose a sybil-based virtual data poisoning attack, where a malicious client generates sybil nodes to amplify the poisoning model's impact. They also develop a virtual data generation method based on gradient matching to reduce neural network computational complexity and design three schemes for target model acquisition applicable to online local, online global, and offline scenarios.

Result: In simulation, the proposed method outperforms other attack algorithms because it can obtain a global target model under non-independent uniformly distributed data.

Conclusion: The sybil-based virtual data poisoning attack with gradient matching can effectively reduce the cost of poisoning attacks in federated learning and perform well in different scenarios.

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


### [313] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: The paper introduces AI2MMUM, a scalable model for executing diverse 6G air interface tasks by processing multi-modal data, showing SOTA performance in physical layer tasks.


<details>
  <summary>Details</summary>
Motivation: To design a universal model for future wireless systems that can handle multi-modal data and various air interface tasks.

Method: Proposes AI2MMUM with LLM backbone for contextual comprehension, fine-tuning for domain-specific knowledge, task instructions with keywords and prompts, frozen radio modality encoders, adapter layers, and lightweight task-specific heads.

Result: Achieves state-of-the-art performance in five representative physical environment/wireless channel-based downstream tasks using WAIR-D and DeepMIMO datasets.

Conclusion: AI2MMUM is a promising approach for flexible and effective execution of various physical layer tasks in 6G-oriented wireless systems.

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [314] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [315] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: A new benchmark, ImagineBench, is introduced to evaluate offline RL algorithms using real and LLM-imaginary rollouts. Current algorithms perform suboptimally on unseen tasks, with a 35.44% success rate on hard tasks compared to 64.37% when trained only on real data.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation in reinforcement learning (RL) which heavily relies on extensive real-world interaction data. The authors aim to create a standard benchmark for evaluating offline RL algorithms that use both real and synthetic (LLM-imaginary) rollouts to overcome this issue.

Method: The method involves introducing ImagineBench, a comprehensive benchmark that includes datasets with environment-collected and LLM-imaginary rollouts, diverse domains covering locomotion, robotic manipulation, and navigation tasks, and natural language task instructions with varying complexity levels. State-of-the-art offline RL algorithms are systematically evaluated using this benchmark.

Result: The result shows that applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving only a 35.44% success rate on hard tasks compared to 64.37% when methods are trained on real rollouts for hard tasks.

Conclusion: The conclusion highlights the need for advancements in algorithms to better utilize LLM-imaginary rollouts. Future research opportunities include better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks.

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [316] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: Quantum-classical Hybrid Machine Learning (QHML) models show better performance in anti-cancer drug response prediction when data is optimally normalized.


<details>
  <summary>Details</summary>
Motivation: Anti-cancer drug response prediction needs robust models that work well with small datasets. Quantum-classical Hybrid Machine Learning (QHML) models have this potential but are sensitive to data encoding.

Method: Propose a normalization function based on a moderated gradient version of the $\tanh$ to transform neural network outputs without concentrating them at extreme value ranges.

Result: Evaluated on a dataset of gene expression and drug response measurements, QHML models performed better than classical deep learning models when data was optimally normalized.

Conclusion: This study shows new possibilities for biomedical data analysis using quantum computers.

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [317] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 通过引入AND、OR和ADDER逻辑门的概念，提出了一种结合噪声和去噪干预的框架，以实现电路发现方法的完整性与保真性，并揭示了这些逻辑门在语言模型功能中的基本特性。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现方法无法保证完整性和保真性，容易遗漏关键机制，特别是对OR门的检测不完全。

Method: 系统地引入了三种逻辑门（AND、OR、ADDER），将电路分解为这些逻辑门的组合；提出了一个结合噪声和去噪干预的框架，可以集成到现有电路发现方法中，用于全面识别和区分电路中的逻辑门。

Result: 实验验证了该框架能够恢复电路的保真性、完整性和稀疏性，并揭示了三种逻辑门的比例和对输出的贡献等基本属性。

Conclusion: 提出的框架有助于提高电路发现方法的可靠性和解释性，并为进一步研究语言模型的功能提供了新的视角。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [318] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNN) suffer from catastrophic forgetting. This paper proposes Instance-Prototype Affinity Learning (IPAL), a new method for Non-Exemplar Continual Graph Learning, which integrates graph structural information and decision boundary perception to improve knowledge retention and acquisition.


<details>
  <summary>Details</summary>
Motivation: To address the issue of catastrophic forgetting in GNNs while overcoming limitations such as memory explosion and privacy concerns that arise with rehearsal-based techniques.

Method: The proposed method, Instance-Prototype Affinity Learning (IPAL), builds on Prototype Contrastive Learning (PCL). It incorporates Topology-Integrated Gaussian Prototypes (TIGP) to leverage graph structure and enhance knowledge assimilation, Instance-Prototype Affinity Distillation (IPAD) to preserve task memory, and Decision Boundary Perception (DBP) to improve class discriminability.

Result: Evaluations on four node classification benchmark datasets show that IPAL outperforms existing state-of-the-art methods, achieving a better balance between plasticity (learning new information) and stability (retaining old knowledge).

Conclusion: IPAL is an effective approach for Non-Exemplar Continual Graph Learning that mitigates catastrophic forgetting in GNNs while addressing challenges like feature drift and maintaining high performance.

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [319] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: The paper proposes a fraud detection framework using a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and XAI techniques (SHAP, LIME, PDP, PFI) to improve both accuracy and interpretability. Evaluated on the IEEE-CIS dataset, it achieved 99% accuracy and 0.99 AUC-ROC score.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning models focus on predictive accuracy but lack transparency and interpretability, which is problematic for regulatory compliance and trust in sectors like financial fraud detection.

Method: A stacking ensemble of XGBoost, LightGBM, and CatBoost is used for fraud detection. SHAP is employed for feature selection, while LIME, PDP, and PFI are used to explain model predictions. The IEEE-CIS Fraud Detection dataset with over 590,000 transaction records is utilized for evaluation.

Result: The model achieves 99% accuracy and an AUC-ROC score of 0.99, surpassing several recent related approaches.

Conclusion: It's possible to combine high prediction accuracy with transparent interpretability, leading to more ethical and trustworthy solutions in financial fraud detection.

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [320] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: This paper proposes a self-adaptive distillation method and a trajectory-based distillation loss for multi-task learning in depth estimation and scene segmentation, achieving better performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for joint modeling of depth estimation and scene segmentation transfer knowledge from multiple teachers in a static way, which may not be optimal for the student's current learning ability and could lead to knowledge forgetting.

Method: The proposed method includes a self-adaptive distillation approach that dynamically adjusts knowledge transfer based on the student's learning ability and a knowledge trajectory with a trajectory-based distillation loss to prevent erroneous gradient updates and knowledge forgetting.

Result: The method shows clear improvements over state-of-the-art solutions when evaluated on benchmark datasets like Cityscapes and NYU-v2.

Conclusion: The self-adaptive distillation method and trajectory-based distillation loss effectively improve multi-task learning in depth estimation and scene segmentation.

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [321] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 通过结合大型语言模型（LLM）和时间序列基础模型（TSFM），提出了一种名为ChronoSteer的多模态TSFM框架，该框架能够通过文本修订指令进行引导，有效连接LLM和TSFM。此外，还设计了基于合成数据的两阶段训练策略，并构建了一个高质量的多模态时间序列预测基准。实验结果表明，仅在合成数据上训练的ChronoSteer与单模态主干相比，预测精度提高了25.7%，比之前的多模态最佳方法高出22.5%。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法依赖于单模态的时间序列数据，限制了对丰富文本信息的利用。而大型语言模型（LLM）和时间序列基础模型（TSFM）分别在文本推理和时间建模方面表现出强大的能力，因此将两者的优势结合起来，构建一个多模态模型以同时利用时间和文本信息进行未来推断成为一个重要的研究挑战。

Method: 为了解决事件-序列配对数据的稀缺性，提出了一个解耦框架：使用LLM将文本事件转换为修订指令，这些指令再用于引导TSFM的输出。具体来说，引入了ChronoSteer，一种可以通过文本修订指令进行引导的多模态TSFM，从而有效地连接了LLM和TSFM。为了缓解跨模态指令-序列配对数据的短缺问题，设计了基于合成数据的两阶段训练策略。此外，还构建了一个高质量的多模态时间序列预测基准，以解决评估过程中的信息泄露问题。

Result: 经过整合LLM后，仅在合成数据上训练的ChronoSteer相较于单模态主干提升了25.7%的预测准确性，并且比之前最先进的多模态方法高出22.5%。

Conclusion: ChronoSteer作为一种多模态TSFM，通过结合LLM和TSFM的优点，在预测准确性上取得了显著提升，尤其是在合成数据上的表现优异，为多模态时间序列预测提供了新的方向和解决方案。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [322] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: MiCo is a hierarchical language agent framework that solves ODMBP by formulating it as SMDP-Option, achieving 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for VM scheduling in cloud services have limitations such as inability to adapt to real-time changes, rigid strategies, and lack of generalizability and interpretability.

Method: MiCo uses LLMs in a two-stage architecture - Option Miner discovers non-context-aware strategies, and Option Composer integrates these with contextual ones.

Result: MiCo achieves 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.

Conclusion: MiCo provides an effective solution for complex and large-scale cloud environments.

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [323] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Researchers have found a way to improve the performance of neural networks in predicting molecular properties without needing extensive pretraining or extra techniques. They did this by creating a new method that trains a Graph Transformer on both real and synthetic tasks, leading to better results in most cases.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge of integrating rule-based models into differentiable neural network frameworks effectively. Current methods often rely heavily on pretraining and supplementary techniques to achieve good performance.

Method: The researchers proposed a strategy that jointly trains a single Graph Transformer neural network. This network is trained on both sparse multitask molecular property experimental targets and synthetic targets derived from XGBoost models trained on Osmordred molecular descriptors. These synthetic tasks act as auxiliary tasks.

Result: This approach led to consistent and significant improvements in performance across all 19 molecular property prediction tasks. Specifically, for 16 out of the 19 targets, the multitask Graph Transformer performed better than the XGBoost single-task learner.

Conclusion: The study concludes that synthetic task augmentation is an effective method to enhance the performance of neural models in multitask molecular property prediction without requiring feature injection or pretraining.

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [324] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: 在联邦学习中，由于客户端数据分布的异构性和数据隐私问题，训练出高性能的全局模型具有挑战性。本文通过提高本地模型的适应性来增强全局模型的性能，并提出了一种可行的解决方案以优化本地模型的训练目标。实验表明，该方法显著提高了本地模型的适应性，并实现了优于基线方法的全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的客户端数据分布异构性和数据隐私问题导致难以训练出高性能的全局模型。为了解决这一问题，需要提升本地模型的适应性。

Method: 1. 提出适当本地模型的性质，这些模型在客户端的数据分布上具有良好的适应性。
2. 将该性质形式化为带有约束的本地训练目标。
3. 提出一种可行的解决方案以训练本地模型。

Result: 广泛的实验表明，该方法显著提高了本地模型的适应性，并实现了一个性能优越的全局模型，其表现始终优于基线方法。

Conclusion: 通过改进本地模型的适应性，可以有效提升联邦学习中全局模型的性能。

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [325] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: Federated Learning (FL) is a popular solution for privacy-sensitive applications but faces challenges due to statistical heterogeneity. This study introduces FedAPC, a prototype-based FL framework that enhances feature diversity and model robustness, outperforming SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of domain heterogeneity in Federated Learning which impedes the global model's convergence.

Method: Introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a prototype-based FL framework. It leverages prototypes derived from the mean features of augmented data to capture richer representations and aligns local features with global prototypes.

Result: Experimental results on the Office-10 and Digits datasets show that the framework outperforms SOTA baselines.

Conclusion: FedAPC enhances feature diversity and model robustness in FL under domain heterogeneity.

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [326] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: This paper investigates the best arm identification problem for multi-agent multi-armed bandits, proposing two algorithms Cl-BAI and BAI-Cl that achieve high accuracy and efficiency in terms of sample complexity and communication overhead.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying the best arm for each agent in a multi-agent multi-armed bandit setting where agents are grouped into clusters and the mapping between agents and bandits is unknown.

Method: Two algorithms, Cl-BAI and BAI-Cl, are proposed. Cl-BAI clusters agents first then identifies the best arm, while BAI-Cl identifies the best arms first then clusters agents. Both use the successive elimination framework.

Result: Both algorithms provide δ-PC guarantees with bounds on sample complexity. BAI-Cl's variant is minimax optimal when the number of clusters M is small. Experiments show superior performance in sample and communication efficiency especially when M≪N.

Conclusion: The proposed algorithms effectively solve the best arm identification problem in multi-agent multi-armed bandits with unknown agent-bandit mappings, achieving high accuracy and efficiency.

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [327] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: The paper introduces QuXAI, a framework featuring Q-MEDLEY to explain feature importance in hybrid quantum-classical machine learning (HQML) models. Q-MEDLEY highlights influential classical aspects, separates noise, and performs well against established XAI techniques. Ablation studies validate its composite structure, improving HQML interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust global and local explainability approaches for HQML architectures that use quantized feature encoding followed by classical learning.

Method: Creation of HQML models with quantum feature maps and utilization of Q-MEDLEY explainer which combines feature-based inferences, preserves the quantum transformation stage, and visualizes resulting attributions.

Result: Q-MEDLEY delineates influential classical aspects in HQML models, separates their noise, and performs well against established XAI techniques in classical validation settings. Ablation studies further expose the virtues of the composite structure used in Q-MEDLEY.

Conclusion: This work provides a way to improve the interpretability and reliability of HQML models, promoting greater confidence and safer use of quantum-enhanced AI technology.

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [328] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: Alinear is an ultra-lightweight time series forecasting model that achieves competitive performance with significantly fewer parameters, challenging the necessity of scaling laws in this domain.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of model size in time series forecasting has led to exponentially increasing parameter counts, raising questions about the necessity of such scaling for performance gains.

Method: A horizon-aware adaptive decomposition mechanism dynamically rebalances component emphasis across different forecast lengths, and a progressive frequency attenuation strategy ensures stable prediction without attention mechanisms.

Result: Alinear consistently outperforms large-scale models while using less than 1% of their parameters, across seven benchmark datasets. A new parameter-aware evaluation metric highlights its efficiency under constrained model budgets.

Conclusion: This work challenges the belief that larger models are inherently better, suggesting a paradigm shift towards more efficient time series modeling.

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [329] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: In semiconductor manufacturing, detecting small defects during EUV patterning is challenging due to lack of quality data. Researchers generate synthetic SEM images with known defect distributions and annotations, then compare object detection models (YOLOv8, EfficientNet, SSD). YOLOv8 shows best performance in detecting smaller defects, suggesting synthetic data can replace real-world data for robust machine-learning models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting extremely small defects in semiconductor manufacturing which cause false or missed detections due to lack of defect-annotated quality data.

Method: Artificially generate scanning electron microscopy (SEM) images of line patterns with known defect distributions and autonomously annotate them. Then employ state-of-the-art object detection models like YOLOv8, EfficientNet, and SSD to investigate defect detection performance as a function of defect size.

Result: YOLOv8 has the best mean average precision of 96% compared to EfficientNet (83%) and SSD (77%). It can reliably detect the smallest defect sizes. Tested on real SEM data, YOLOv8 correctly detected 84.6% of Bridge defects and 78.3% of Break defects.

Conclusion: Synthetic data can be used as an alternative to real-world data to develop robust machine-learning models for defect detection in semiconductor manufacturing.

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [330] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer with Complex Multi-Head Attention (CMHA) improves modeling of semantic and positional information, leading to superior performance in various tasks compared to strong baselines.


<details>
  <summary>Details</summary>
Motivation: Transformer models face challenges in effectively integrating positional information while allowing multi-head attention flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity.

Method: Introduced ComplexFormer featuring Complex Multi-Head Attention (CMHA). CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. Two key improvements are incorporated: per-head Euler transformation and per-head adaptive differential rotation mechanism.

Result: Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.

Conclusion: ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [331] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: The paper presents a deep neural network model that fuses acoustic and inertial signals for monitoring cattle feeding behavior using multiple sensors. This model, based on convolutional, recurrent, and dense layers, improves precision through automatic feature extraction. Feature-level fusion outperforms other methods by 0.14 in F1-score and shows a 14% increase compared to state-of-the-art methods. An ablation study and post-training quantization evaluation are also provided.


<details>
  <summary>Details</summary>
Motivation: Efficient herd management requires effective monitoring of feeding behaviors in cattle, which can lead to better diet formulation and early detection of health issues. Existing sensor-based approaches have limitations, prompting the exploration of simultaneous multi-sensor use to enhance estimation precision.

Method: A deep neural network combining convolutional, recurrent, and dense layers is introduced to fuse acoustic and inertial signals. The model explores different levels of information fusion (feature, data, and decision) and selects feature-level fusion as the optimal approach.

Result: The proposed model achieved an F1-score of 0.802, surpassing existing methods by 14%. Feature-level fusion outperformed data and decision-level fusion by at least 0.14 in F1-score. Additional evaluations include an ablation study and post-training quantization.

Conclusion: The deep neural network model successfully enhances the precision of cattle feeding behavior monitoring through the fusion of acoustic and inertial signals. Feature-level fusion was identified as the most effective strategy, and the model outperforms current state-of-the-art methods.

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [332] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的跨域知识迁移框架，以提高大型语言模型在时间序列预测中的性能。通过将结构化的时序信息注入LLMs，可以显著提高预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，除了传统的自然语言任务外，还需要利用其能力来解决其他领域的问题，例如时间序列预测。这在能源系统、金融和医疗等领域变得越来越重要。

Method: 提出了一种新的跨域知识迁移框架，该框架系统地将结构化的时序信息注入到LLMs中，以提高其在时间序列预测任务中的准确性。

Result: 实验结果表明，在真实世界的时间序列数据集上，与未提供辅助信息的基线方法相比，知识引导的预测方法在预测准确性和泛化能力方面显著优于基线。

Conclusion: 研究结果表明，知识迁移策略有潜力弥合大型语言模型与特定领域预测任务之间的差距，从而提高预测性能。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [333] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: The paper explores why larger language models perform better, focusing on the role of representation superposition in the neural scaling law.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of the neural scaling law where loss decreases with model size.

Method: Constructed a toy model based on two empirical principles: LLMs represent more things than their dimensions and language features occur with varying frequencies. Analyzed loss scaling under weak and strong superposition scenarios.

Result: Under weak superposition, loss scales with feature frequency; under strong superposition, loss is inversely proportional to model dimension. Open-sourced LLMs exhibit strong superposition and match the toy model predictions.

Conclusion: Representation superposition is crucial for the observed neural scaling laws and can inspire new training strategies and model architectures.

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [334] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload is a new inference engine that combines speculative decoding with offloading to improve GPU core utilization and inference throughput for LLMs on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Efficient LLM inference on devices with limited resources is challenging due to compute and memory constraints. Current systems offload model weights to CPU memory, leading to inefficiencies in GPU usage and performance.

Method: Propose SpecOffload, which embeds speculative decoding into offloading. It uses latent GPU resources to store and execute a draft model for speculative decoding, interleaving the execution of target and draft models within the offloading pipeline. A planner is introduced to manage tensor placement and select optimal parameters.

Result: SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.

Conclusion: SpecOffload provides a solution to enhance the efficiency of LLM inference on resource-constrained devices, significantly improving GPU core utilization and inference throughput.

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [335] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: The paper introduces Parallel Scaling (ParScale), a new method for scaling language models that increases parallel computation during training and inference, rather than increasing parameters or output tokens. ParScale reuses existing parameters, applies to various model structures, and theoretically offers superior inference efficiency with less memory and latency increase compared to parameter scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find a more inference-efficient way to scale language models beyond the traditional methods of parameter scaling or inference-time scaling by increasing output tokens.

Method: ParScale applies P diverse and learnable transformations to the input, executes forward passes of the model in parallel, and dynamically aggregates the P outputs. This scales parallel computation by reusing existing parameters.

Result: ParScale achieves similar performance improvements as parameter scaling but with up to 22 times less memory increase and 6 times less latency increase. It can also recycle pre-trained models with minimal post-training.

Conclusion: ParScale provides an alternative perspective on the role of computation in machine learning, facilitates the deployment of powerful models in low-resource scenarios, and reduces the training budget significantly.

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [336] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: This paper addresses the charging scheduling problem of Electric Buses (EBs) using a Hierarchical Deep Reinforcement Learning (HDRL) approach, which includes a high-level Semi-MDP and multiple low-level MDPs. The authors propose an HDDQN-HER algorithm to optimize charging strategies for cost reduction while meeting charging targets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the complex charging scheduling problem of EBs, characterized by long-range multi-phase planning with sparse rewards, through the use of deep reinforcement learning techniques.

Method: A Markov Decision Process (MDP) is designed for the charging and operating periods of EBs. This MDP is decoupled into a high-level Semi-MDP and multiple low-level MDPs using Hierarchical DRL (HDRL). An HDDQN-HER algorithm is developed to address decision problems at different temporal resolutions.

Result: The flat policy created by combining the optimal high-level and low-level policies performs as well as the optimal policy of the original MDP. Numerical experiments using real-world data demonstrate the effectiveness of the proposed algorithm in reducing charging costs while achieving charging targets.

Conclusion: The hierarchical DRL framework and HDDQN-HER algorithm successfully tackle the charging scheduling problem for EBs, providing an effective strategy for minimizing charging costs.

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [337] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: This paper proposes a novel router-based architecture for generating high-quality synthetic training data to fine-tune Large Language Models (LLMs) for function calling tasks, which significantly improves function classification accuracy and API parameter selection.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of fine-tuning LLMs for function calling tasks in scenarios where real user interaction data is unavailable due to lack of task-specific data and privacy constraints. This necessitates the generation of synthetic data that can replicate real-world data distributions.

Method: The method involves a router-based architecture that uses domain resources such as content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models, to generate synthetic training data. The flexible routing mechanism ensures that the synthetic data matches observed real-world distributions.

Result: Evaluation on a comprehensive set of real user queries shows significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with the proposed synthetic data outperform traditional approaches.

Conclusion: The proposed router-based architecture for synthetic data generation effectively addresses the limitations of existing approaches, leading to improved performance in function calling tasks and setting new benchmarks.

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [338] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: This paper presents a new data reconstruction attack in Federated Learning that can perfectly recover large data batches without prior knowledge of clients' data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Despite the privacy-preserving nature of Federated Learning (FL), recent studies have shown that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing attacks have limitations regarding assumptions on data distribution and efficiency with larger batch sizes.

Method: The method introduces a novel data reconstruction attack leveraging a geometric perspective on fully connected layers to craft malicious model parameters. This enables the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data.

Result: Through experiments on image and tabular datasets, the attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state-of-the-art.

Conclusion: The proposed data reconstruction attack overcomes limitations of existing methods by enabling perfect recovery of large data batches without prior knowledge of clients' data, highlighting significant vulnerabilities in Federated Learning.

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [339] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV transforms small language models into effective multimodal drafters for vision-language models, increasing accepted length by up to 30% and delivering inference speedups of up to 1.46x.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding accelerates language model inference but applying it to vision-language models is challenging due to architectural mismatches and prediction misalignment between small language models and VLMs.

Method: MASSV uses a two-phase approach: connecting the target VLM's vision encoder to the draft model via a lightweight projector and aligning token predictions through self-distilled visual instruction tuning.

Result: Experiments show MASSV increases accepted length by up to 30% and provides end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

Conclusion: MASSV offers a scalable method for accelerating current and future vision-language models.

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [340] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: This paper presents a deep learning model for high-resolution probabilistic precipitation forecasting in Europe, integrating radar, satellite and NWP data. It surpasses current operational systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of radar-only deep learning models with short forecast lead times and provide accurate forecasts with robust uncertainty quantification.

Method: The model integrates multiple data sources (radar, satellite, and physics-based numerical weather prediction) while capturing long-range interactions, featuring a compact architecture for efficient training and faster inference.

Result: Extensive experiments demonstrate that the model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models.

Conclusion: This model sets a new standard for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency.

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [341] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: The paper connects Hebbian spike-timing-dependent plasticity to noisy gradient descent, proving the learning rule identifies the most active presynaptic neuron and links it to noisy mirror descent.


<details>
  <summary>Details</summary>
Motivation: To explore learning rules accounting for precise spike-timing in biological neural networks beyond well-studied neuronal firing rates-based Hebbian learning.

Method: Relating a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent concerning a natural loss function on the probability simplex.

Result: Proves that the learning rule can identify the presynaptic neuron with the highest activity and uncovers an intrinsic connection to noisy mirror descent.

Conclusion: Hebbian spike-timing-dependent plasticity can be understood through noisy gradient descent and has a connection with noisy mirror descent, advancing understanding of precise spike-timing in neural networks.

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [342] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: The paper proposes a Hierarchical Deep Reinforcement Learning (HDRL) approach, DAC-MAPPO-E, to optimize Electric Bus charging schedules using real-time data and multi-timescale decision-making. It reformulates the MDP into two augmented MDPs and incorporates an attention mechanism and MAPPO algorithm for scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in optimizing Electric Bus (EB) charging schedules due to uncertainties in travel time, energy consumption, electricity prices, and the need for efficient multi-timescale decision-making and scalability for large EB fleets.

Method: The method involves proposing a Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the original Markov Decision Process (MDP) into two augmented MDPs. The novel HDRL algorithm, Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E), is introduced to solve these MDPs and enable multi-timescale decision-making. Enhancements are made at both high and low decision levels through redesigning the decentralized actor network with an attention mechanism and incorporating the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm respectively.

Result: Extensive experiments conducted with real-world data demonstrate the superior performance and scalability of the DAC-MAPPO-E algorithm in optimizing Electric Bus fleet charging schedules.

Conclusion: The proposed DAC-MAPPO-E algorithm successfully addresses the challenges in optimizing Electric Bus charging schedules by enabling multi-timescale decision-making and ensuring scalability for large EB fleets.

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [343] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: In this paper, a novel federated representative-attention-based defense mechanism called FeRA is proposed to address the challenge of detecting backdoor attacks in Federated Learning (FL) systems with non-IID data. FeRA uses cross-client attention over internal feature representations and computes an anomaly score based on representation reconstruction errors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the difficulty in detecting backdoor attacks in FL due to the diverse, non-IID data produced by the heterogeneous nature of edge devices.

Method: FeRA leverages cross-client attention over internal feature representations to distinguish benign from malicious clients by computing an anomaly score based on representation reconstruction errors.

Result: Experimental results indicate that FeRA effectively reduces backdoor attack success rates while maintaining high accuracy on the main task.

Conclusion: FeRA is robust across various FL scenarios, model-agnostic, attack-agnostic, and does not require labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments.

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [344] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: Graph contrastive learning (GCL) often suffers from false negatives, which degrades the performance on downstream tasks. To solve this problem, a novel Negative Metric Learning (NML) enhanced GCL (NML-GCL) is proposed in this paper.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of false negatives in graph contrastive learning (GCL), which leads to suboptimal results in downstream tasks.

Method: The authors propose NML-GCL that uses a learnable Negative Metric Network (NMN) to create a negative metric space. In this space, false negatives can be better distinguished from true negatives based on their distance to an anchor node. A joint training scheme with bi-level optimization objective is also proposed to optimize the encoder and the negative metric network using self-supervision signals.

Result: Theoretical analysis and extensive experiments on widely used benchmarks show the superiority of the proposed method.

Conclusion: NML-GCL effectively addresses the false negative issue in GCL, leading to improved performance on downstream tasks.

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [345] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: The paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) which converges under computation-delay-independent step sizes, making it efficient and suitable for real-world decentralized learning tasks.


<details>
  <summary>Details</summary>
Motivation: Decentralized optimization is crucial for leveraging distributed data without central control. However, practical deployments face challenges due to heterogeneous computation speeds and unpredictable communication delays.

Method: The authors analyze Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool to understand the convergence of ADSGD and show that ADSGD converges under computation-delay-independent step sizes without assuming bounded data heterogeneity.

Result: Empirical experiments reveal that ADSGD outperforms existing methods in wall-clock convergence time across various scenarios.

Conclusion: ADSGD is simple, efficient in memory and communication, and resilient to communication and computation delays, making it well-suited for real-world decentralized learning tasks.

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [346] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: The paper introduces ALERT, a method detecting feature distribution changes in AI models for wireless networks, ensuring model re-training when needed.


<details>
  <summary>Details</summary>
Motivation: AI is crucial for next-gen wireless networks but feature distribution changes can degrade model performance and cause issues. There's a need to detect these changes to ensure model effectiveness.

Method: ALERT includes three components: representation learning (using MLP), statistical testing (Kolmogorov-Smirnov and Population Stability Index tests), and utility assessment (via a new function).

Result: ALERT outperforms ten standard drift detection methods in two wireless network use cases: wireless fingerprinting and link anomaly detection.

Conclusion: ALERT successfully detects feature distribution changes and triggers necessary model re-training in wireless network applications.

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [347] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: This paper addresses the challenge of adapting reinforcement learning (RL) agents to changing environments without forgetting prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Real-world autonomous decision-making systems must operate in environments that change over time, but conventional RL methods struggle to adapt when conditions change.

Method: The dissertation proposes two key capabilities for efficient online adaptation: prioritized exploration and sampling strategies to identify and learn from relevant experiences, and selective preservation of prior knowledge through structured representations.

Result: Demonstrates that with these two capabilities, RL agents can efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge.

Conclusion: Efficient online adaptation in RL requires prioritized exploration/sampling strategies and selective preservation of prior knowledge through structured representations.

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [348] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 在随机分类器集合中，通过采用分类损失作为能量的吉布斯测度对集合进行加权，存在一个有限温度参数可以使分类效果达到最优。此温度对于教师分类器和随机分类器的数量具有普适性。实验表明该现象在高质量、无噪声的数据集中具有重要性。


<details>
  <summary>Details</summary>
Motivation: 许多数据科学和机器学习应用中都存在随机性。由随机组件组成的系统通常表现出确定性的全局行为，这标志着从微观无序到宏观组织的转变。因此，研究随机分类器集合中集体行为的出现是十分有意义的。

Method: 引入理论模型来研究随机分类器集合中集体行为的出现。通过对集合采用由分类损失定义为能量的吉布斯测度进行加权，并分析是否存在使分类效果达到最优的有限温度参数。同时，在样本由高斯分布生成且标签由教师感知机构建的情况下，进行解析证明和数值确认。

Result: 解析证明和数值结果表明，存在一个不依赖于教师分类器和随机分类器数量的最优温度参数，从而揭示了所观察行为的普遍性质。在MNIST数据集上的实验进一步强调了这一现象的重要性。

Conclusion: 通过引入理论模型并结合解析和数值方法，证明了在随机分类器集合中存在一个普适的最优温度参数，使得分类效果达到最优。此发现揭示了系统自组织性质的物理类比。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [349] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: The paper provides a tutorial on discrete variational autoencoders with categorical latent variables, offering a training recipe and example implementation.


<details>
  <summary>Details</summary>
Motivation: To provide a clear and practical introduction to discrete variational autoencoders for those with basic mathematical knowledge.

Method: Introduces VAEs with discrete latent spaces where the latent variables follow a categorical distribution, deriving each step from first principles.

Result: Develops a concrete training recipe and offers an example implementation available on GitHub.

Conclusion: Discrete variational autoencoders with categorical latent variables are presented as a natural choice for certain data modalities, such as text.

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [350] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: Specialized Multi-Task Optimizers (SMTOs) are evaluated on complex multi-task problems, revealing their competitive performance against uniform loss and fixed weights.


<details>
  <summary>Details</summary>
Motivation: To address critiques suggesting that equally weighted tasks can achieve results as good as SMTOs due to poor hyperparameter optimization and lack of regularization in previous studies.

Method: Conduct an extensive empirical evaluation of SMTOs, including recent methods, on more complex multi-task problems.

Result: SMTOs perform well compared to uniform loss; fixed weights can achieve competitive performance compared to SMTOs. Uniform loss performs similarly to SMTOs in some cases.

Conclusion: SMTOs show competitive performance, but fixed weights can also be effective. The reasons for uniform loss performing similarly to SMTOs in certain scenarios are demonstrated.

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [351] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 本论文提出了一种名为FactsR的新方法，用于在医疗咨询期间实时提取重要临床信息（称为Facts），并通过递归使用这些信息生成最终的医疗记录。这种方法通过让医生参与记录生成过程，减少了幻觉、错误表示和过度依赖医生校对的问题，从而提高了记录的准确性和简洁性，同时为实时决策支持开辟了新的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前许多AI书记员解决方案在医疗咨询结束后生成笔记时，依赖于一次性或少量示例提示，缺乏推理能力，可能导致长篇笔记中出现幻觉、误解医生意图以及需要医生校对捕捉错误等问题，这些问题在工作量和疲劳影响警觉性时会对患者安全构成威胁。

Method: 论文介绍了一种名为FactsR的方法，在医疗咨询过程中实时提取重要临床信息（称为Facts），并递归地利用这些信息生成最终的笔记。该方法将医生纳入到笔记生成的过程中，从而提高笔记的质量。

Result: FactsR方法生成的笔记更准确、更简洁，并且为实时决策支持提供了新的应用可能性。

Conclusion: FactsR方法通过实时提取临床信息和递归生成笔记，显著提高了医疗记录的准确性和简洁性，同时增强了患者安全性，为实时决策支持系统的发展奠定了基础。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [352] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: Schrier-Coset Graph Propagation (SCGP) is a new method in GNNs that improves long-range message passing without altering graph topology, showing advantages in hierarchical and modular graphs.


<details>
  <summary>Details</summary>
Motivation: To solve the over-squashing problem in GNNs while avoiding scalability bottlenecks.

Method: Introduces SCGP, which uses Schreier-coset embeddings to enrich node features without changing the input graph topology.

Result: Empirical evaluations show SCGP performs as well as or better than existing methods, particularly excelling with hierarchical and modular graphs.

Conclusion: SCGP offers improved performance, scalability, and efficiency for real-time and resource-constrained applications.

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [353] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: The paper introduces PIF, a new anomaly detection method that merges adaptive isolation techniques with preference embedding, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection by leveraging structured patterns through combining adaptive isolation methods and preference embedding.

Method: PIF embeds data into high dimensional space and uses PI-Forest, a tree-based method, to calculate anomaly scores.

Result: PIF outperforms state-of-the-art anomaly detection methods in experiments on both synthetic and real datasets. PI-Forest is more effective at measuring distances and isolating points in the preference space.

Conclusion: PIF presents a promising approach for anomaly detection by integrating adaptive isolation and preference embedding.

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [354] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG is a two-stage VAE-based IA mesh generator that creates realistic aneurysm shapes and parent vessels, conditioned on specific morphological measurements, to aid in flow simulation studies.


<details>
  <summary>Details</summary>
Motivation: The lack of large IA image datasets makes it difficult to train networks for predicting blood flow forces in real time. Existing shape generation methods fail to capture realistic IA features and ignore the relationship between IA pouches and parent vessels.

Method: AneuG uses a two-stage VAE approach. In the first stage, it generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes with constraints on morphing energy statistics. In the second stage, it generates parent vessels conditioned on GHD tokens by creating vascular centreline and propagating the cross-section.

Result: AneuG can generate IA shapes with specific clinically relevant morphological measurements, improving physiological realism and enabling controlled generation.

Conclusion: AneuG provides a valuable tool for understanding shape variations and the effects of specific clinical shape parameters on fluid dynamics, contributing to both research and potential clinical applications.

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [355] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL is a NAS-based framework designed for data-incremental learning that dynamically adapts model structure, reduces forgetting, and enhances accuracy while maintaining a lower model size.


<details>
  <summary>Details</summary>
Motivation: Incremental learning requires balancing plasticity (learning new tasks) and stability (preserving past knowledge), but existing NAS-based approaches often expand the model at every task, making them impractical in resource-constrained environments.

Method: SEAL uses a capacity estimation metric to determine when to expand the model structure, preserving stability through cross-distillation training after each expansion step. The NAS component searches for both the architecture and the optimal expansion policy.

Result: Experiments show SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods across multiple benchmarks.

Conclusion: Combining NAS and selective expansion holds promise for efficient, adaptive learning in incremental scenarios.

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [356] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: 通过将学习分解为多个不同的机制，可以显著提高数据效率，使其与人类学习相一致。这种方法比单独区分符号和子符号学习对效率的影响更大。研究发现，整合多种专门的学习机制可能是弥合数据驱动的机器学习与人类学习效率差距的关键。


<details>
  <summary>Details</summary>
Motivation: 探讨人类学习者能够从少量例子中快速学习的原因，是否源于我们能够结合使用多种专门的学习机制。

Method: 通过对在线辅导环境中的归纳人类学习模拟进行消融分析，比较强化学习与更高效的数据3机制符号规则归纳方法。

Result: 将学习分解为多个不同机制显著提高了数据效率，与人类学习水平相当，并且这种分解对效率的影响大于符号与子符号学习的区别。

Conclusion: 整合多种专门的学习机制可能是弥合数据驱动的机器学习与人类学习效率差距的关键。

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [357] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: This paper explores the connection between gradient-based optimization in parametric models like neural networks and linear combinations of random features. It demonstrates that if a model can be learned via mini-batch stochastic gradient descent without data distribution assumptions, then with high probability, the target function can be approximated using a polynomial-sized combination of random features.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between gradient-based optimization of parametric models (such as neural networks) and optimization of linear combinations of random features, especially under scenarios where no assumptions about data distribution are made.

Method: The authors prove that if a parametric model can be learned through mini-batch stochastic gradient descent (bSGD) without making assumptions on the data distribution, then the target function can also be approximated by a polynomial-sized combination of random features. They introduce average probabilistic dimension complexity (adc), which extends probabilistic dimension complexity, and demonstrate its polynomial relationship with statistical query dimension.

Result: The size of the combination of random features needed to approximate the target function depends on the number of gradient steps and numerical precision used in bSGD. This reveals fundamental limitations of distribution-free learning in neural networks trained by gradient descent. The study also shows an infinite separation between adc and standard dimension complexity.

Conclusion: This work highlights the importance of making assumptions about data distributions when training neural networks with gradient descent due to the inherent limitations of distribution-free learning. Additionally, it introduces a new theoretical framework, adc, which provides deeper insights into the complexities involved in such optimizations.

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [358] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: The paper introduces Learning to Think (L2T), a framework for fine-tuning large language models (LLMs) to optimize reasoning effectiveness and computational efficiency. L2T uses an information-theoretic reinforcement approach with a novel reward mechanism based on PAC-Bayes bounds and the Fisher information matrix, reducing computational complexity while maintaining estimation accuracy. Empirical results show improvements in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving reasoning abilities in LLMs do not adequately balance reasoning effectiveness with computational efficiency, often leading to unnecessarily long reasoning chains and wasted tokens.

Method: L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward that quantifies episode-wise information gain in parameters without needing extra annotations or task-specific evaluators. A method is proposed to quickly estimate this reward using PAC-Bayes bounds and the Fisher information matrix, theoretically reducing computational complexity with high accuracy. Reinforcement learning optimizes the model by rewarding each episode's contribution and penalizing excessive updates.

Result: Empirical results across various reasoning benchmarks and base models demonstrate L2T's advantage in different tasks, enhancing both reasoning effectiveness and efficiency.

Conclusion: Learning to Think (L2T) provides a promising approach to fine-tune LLMs for achieving optimal reasoning with fewer tokens, thereby improving both the effectiveness and efficiency of reasoning tasks.

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [359] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 本研究探讨了使用基于分数的扩散模型进行云和降水短时预报（0至3小时）的潜力，该方法不仅能够推进现有云层，还能生成和消散云层，包括对流启动。通过实验比较三种扩散模型，发现残差校正扩散模型（CorrDiff）表现最佳，优于传统U-Net和其他模型。此外，扩散模型还具备生成集合预报的能力，有助于误差估计。


<details>
  <summary>Details</summary>
Motivation: 云和降水对于理解天气和气候至关重要，但由于需要次网格参数化，传统的数值天气预报难以准确模拟。尽管机器学习已被用于预测云和降水，但早期方法常产生模糊的预测结果。因此，本研究探索了一种新的机器学习方法——基于分数的扩散模型，以改善短时预报精度。

Method: 研究采用了三种主要类型的扩散模型：标准基于分数的扩散模型（Diff）、残差校正扩散模型（CorrDiff）和潜在扩散模型（LDM）。这些模型利用过去20分钟的地球同步红外卫星图像作为输入，进行云和降水的短时预报。

Result: 扩散模型不仅能够推进现有的云层，还能生成和消散云层，包括对流启动。在实验中，CorrDiff模型表现最佳，其均方根误差比其他扩散模型、传统U-Net以及持续性预报低1至2开尔文。此外，扩散模型还能够自动生成集合预报，其集合离散度与误差相关性良好，显示出有效的校准能力。

Conclusion: 基于分数的扩散模型在云和降水短时预报中表现出色，尤其是CorrDiff模型，其精度和校准能力优于其他方法。这种方法为天气预报提供了新的可能性，并为进一步研究奠定了基础。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [360] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: The paper discusses the limitations of conventional methods for deriving gas turbine engine models and proposes a new approach using Koopman eigenfunction space. The resulting Koopman model was validated against an in-house reference model, and the Koopman-based controller outperformed other benchmark controllers.


<details>
  <summary>Details</summary>
Motivation: Gas turbine engines are complex systems with nonlinear dynamics, making it challenging to derive physics-based models. Conventional experimental methods have limitations when creating component-level and locally linear parameter-varying models.

Method: Employ identification techniques based on data from standard engine operation under closed-loop control. Use sparse identification of nonlinear dynamics for rotor dynamics, map autonomous dynamics into Koopman eigenfunction space via eigenvalue optimization and temporal projection, followed by gradient-based eigenfunction identification.

Result: The Koopman-based controller outperformed classical and gain-scheduled proportional-integral controllers, as well as an internal model control approach, in both reference tracking and disturbance rejection under varying conditions due to its global nature.

Conclusion: Koopman eigenfunction space allows targeting individual modes during optimization, leading to better performance tuning. The proposed method provides a globally optimal nonlinear feedback controller.

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [361] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.LG

TL;DR: Effective communication about breast and cervical cancers is challenging. This study evaluates LLMs' capabilities and limitations in generating cancer-related information. Results show general-purpose LLMs have higher linguistic quality, while medical LLMs have greater accessibility but also more potential harm. There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness.


<details>
  <summary>Details</summary>
Motivation: Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment.

Method: Evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g.

Result: General-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness.

Conclusion: There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness.

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [362] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: NCDPO is a new framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy, enabling efficient likelihood evaluation and gradient backpropagation. It matches MLP+PPO in sample efficiency and surpasses existing methods in performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies can learn diverse skills but suffer from sub-optimal demonstration data which may lead to catastrophic failures. RL-based fine-tuning is promising but adapting PPO to diffusion models is challenging due to computational intractability.

Method: NCDPO reformulates Diffusion Policy as a noise-conditioned deterministic policy, making likelihood evaluation and gradient backpropagation tractable through all diffusion timesteps by treating each denoising step as a differentiable transformation.

Result: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch and outperforms existing methods in both sample efficiency and final performance across various benchmarks. It is also robust to the number of denoising timesteps.

Conclusion: NCDPO addresses the challenges in adapting PPO to diffusion models, providing a more efficient and effective method for training diffusion policies.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [363] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 本论文提出了一种名为QFIX的新方法，它通过简单的'fixing'层扩展了先前模型的表现能力，并在多个实验环境中表现出优越的性能、稳定性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习中的值函数分解方法（如VDN、QMIX）尽管能保证个体-全局最大值（IGM）属性，但表现能力有限；而唯一不受此限制的方法QPLEX又过于复杂。因此，需要一种既能表示完整的IGM值类，又保持简单性的新方法。

Method: 作者提出了一个完整的IGM值类的简单公式化方法，由此推导出QFIX这一新型值函数分解模型家族。QFIX通过一个薄的'fixing'层扩展了先前模型的表现能力。此外，还从该方法中衍生出了多个QFIX变体，并在两个知名的多智能体框架中实现了其中三种变体。

Result: 在SMACv2和Overcooked等多个环境中的实证评估表明：(i) QFIX成功提升了先前方法的性能；(ii) 与主要竞争对手QPLEX相比，QFIX学习更稳定且表现更好；(iii) QFIX在采用最简单和最小的混合模型时达到了上述效果。

Conclusion: QFIX是一种有效的值函数分解模型，它不仅扩展了先前模型的表现能力，而且以更简单和更小的模型实现了更好的性能和稳定性。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [364] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: To overcome the limitations of current XAI frameworks, PnPXAI is introduced as a universal framework that supports diverse data modalities and neural network models in a Plug-and-Play manner.


<details>
  <summary>Details</summary>
Motivation: Current XAI frameworks face challenges such as limited flexibility to diverse model architectures and data modalities, restricted number of supported XAI methods, and sub-optimal recommendations of explanations.

Method: PnPXAI automatically detects model architectures, recommends applicable explanation methods, and optimizes hyperparameters for optimal explanations.

Result: The framework's effectiveness is validated through user surveys and its versatility is showcased across various domains, including medicine and finance.

Conclusion: PnPXAI addresses the limitations of existing XAI frameworks and enhances the adoption of XAI technology in real-world applications.

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [365] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation that integrates an atom-based representation of the 3D pharmacophore into the generative process, offering a powerful and flexible framework for rational drug design.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules is a central challenge in drug discovery, especially for novel targets lacking structural or functional data.

Method: PharmaDiff employs a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses.

Result: PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods and achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures.

Conclusion: By integrating pharmacophore modeling with 3D generative techniques, PharmaDiff offers a powerful and flexible framework for rational drug design.

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [366] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 本研究提出了一种新的工作流程，通过将可穿戴健身设备的生理数据与实时环境暴露相结合，利用AI模型预测个人对污染的健康反应。


<details>
  <summary>Details</summary>
Motivation: 空气污染和气候变化对公共健康构成重大威胁，而个人传感技术和AI技术的进步为监测和预测个人健康结果提供了新机会。

Method: 通过整合可穿戴设备的生理数据和实时环境暴露数据，使用对抗自编码器神经网络训练AI模型，以预测个人对污染暴露的健康反应，并应用迁移学习提高模型的泛化能力。

Result: AI模型能够准确重建时间依赖的健康信号，并捕捉对污染的非线性反应，迁移学习提高了模型的泛化能力。

Conclusion: 该方法展示了在个性化医疗领域的潜力，特别是在预测个人对污染的健康反应方面。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [367] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: The paper introduces Neural Thermodynamic Laws (NTL), a framework providing insights into LLM training dynamics, showing thermodynamic quantities and principles emerge under certain assumptions and offering guidelines for learning rate schedules.


<details>
  <summary>Details</summary>
Motivation: To explore the underlying laws of large language models beyond neural scaling laws.

Method: Demonstrate that key thermodynamic quantities and classical thermodynamic principles naturally emerge under river-valley loss landscape assumptions, and provide intuitive guidelines for designing learning rate schedules from this scientific perspective.

Result: Neural Thermodynamic Laws framework successfully connects thermodynamic concepts with LLM training dynamics, providing theoretical and practical contributions.

Conclusion: NTL offers fresh insights into LLM training dynamics by linking thermodynamic principles with LLMs, contributing both theoretically and practically.

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [368] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

TL;DR: The paper introduces Depth-based Local Center Clustering (DLCC), a new clustering method that uses local data depth to identify multimodal characteristics and varying shape clusters, along with a new internal metric for evaluating non-convex cluster performance.


<details>
  <summary>Details</summary>
Motivation: Existing clustering methods have limitations in practical applications as they are typically designed for specific scenarios. Traditional methods often fail to capture the multimodal characteristics of data and struggle with non-convex clusters.

Method: The proposed method, DLCC, employs a local version of data depth based on subsets of data to identify local centers and clusters of varying shapes. It also proposes a new internal metric based on density-based clustering to evaluate clustering performance on non-convex clusters.

Result: DLCC is presented as a flexible clustering approach that overcomes some limitations of traditional clustering methods, such as capturing multimodal characteristics and handling non-convex clusters effectively.

Conclusion: DLCC enhances data analysis capabilities across a wide range of application scenarios by providing a more versatile and effective clustering solution compared to traditional methods.

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [369] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

TL;DR: Mixed effects models are widely used but slow with high-dimensional crossed random effects. This paper presents Krylov subspace-based methods to solve computational bottlenecks, achieving significant runtime reduction and better scalability compared to Cholesky-based computations.


<details>
  <summary>Details</summary>
Motivation: To address the slowness of current standard computations using Cholesky decompositions for high-dimensional crossed random effects in mixed effects models.

Method: Novel Krylov subspace-based methods are developed. Theoretical analysis and empirical evaluation of preconditioners for conjugate gradient and stochastic Lanczos quadrature methods are performed. New convergence results are derived and computationally efficient methods for calculating predictive variances are developed.

Result: Extensive experiments show that the proposed methods scale much better than Cholesky-based computations, with a runtime reduction of approximately two orders of magnitude for both estimation and prediction. The software implementation is up to 10'000 times faster and more stable than state-of-the-art implementations like lme4 and glmmTMB.

Conclusion: The Krylov subspace-based methods presented in this work provide a significant improvement in computational efficiency and scalability for mixed effects models with high-dimensional crossed random effects.

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


### [370] [Forests for Differences: Robust Causal Inference Beyond Parametric DiD](https://arxiv.org/abs/2505.09706)
*Hugo Gobato Souto,Francisco Louzada Neto*

Main category: stat.ME

TL;DR: This paper introduces DiD-BCF, a novel model for Difference-in-Differences estimation that addresses challenges like staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating various treatment effects and demonstrates superior performance in simulations and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing methods for Difference-in-Differences (DiD) estimation by addressing issues such as staggered adoption of treatments and heterogeneous treatment effects, which traditional methods struggle with.

Method: The paper proposes the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a non-parametric model that incorporates a Parallel Trends Assumption-based reparameterization to enhance estimation accuracy and stability. This model offers a unified framework for estimating Average Treatment Effects (ATE), Group-Average Treatment Effects (GATE), and Conditional Average Treatment Effects (CATE).

Result: Through extensive simulations, the DiD-BCF model shows superior performance compared to established benchmarks, especially in scenarios involving non-linearity, selection biases, and effect heterogeneity. When applied to U.S. minimum wage policy, it reveals significant conditional treatment effect heterogeneity related to county population, insights not captured by traditional methods.

Conclusion: DiD-BCF is presented as a robust and versatile tool for more nuanced causal inference in modern DiD applications, offering improvements over traditional methods in terms of accuracy, stability, and ability to uncover complex treatment effect heterogeneity.

Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest
(DiD-BCF), a novel non-parametric model addressing key challenges in DiD
estimation, such as staggered adoption and heterogeneous treatment effects.
DiD-BCF provides a unified framework for estimating Average (ATE),
Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core
innovation, its Parallel Trends Assumption (PTA)-based reparameterization,
enhances estimation accuracy and stability in complex panel data settings.
Extensive simulations demonstrate DiD-BCF's superior performance over
established benchmarks, particularly under non-linearity, selection biases, and
effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers
significant conditional treatment effect heterogeneity related to county
population, insights obscured by traditional methods. DiD-BCF offers a robust
and versatile tool for more nuanced causal inference in modern DiD
applications.

</details>


### [371] [Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging](https://arxiv.org/abs/2505.10279)
*Gabriel R. Palma,Sally McClean,Brahim Allan,Zeeshan Tariq,Rafael A. Moral*

Main category: stat.ME

TL;DR: TV providers need to offer personalized experiences for customers. A key challenge is understanding group viewing behavior within households. This paper proposes a novel framework using Gaussian mixture model averaging and Bayesian random walk to estimate the number of household TV profiles and their characteristics from real customer data.


<details>
  <summary>Details</summary>
Motivation: To provide better-personalised recommendations for group viewing in households, it is essential to understand the behaviour and preferences of multiple people watching TV together.

Method: The method involves using a Gaussian mixture model averaging to obtain point estimates for the number of household TV profiles and a Bayesian random walk model to introduce uncertainty.

Result: The results show that the framework can estimate the number of household TV profiles and their characteristics, including shifts over time and quantification of uncertainty, when applied to real customer data with approximately half a million observations.

Conclusion: Combining the proposed framework with selected features provides an effective way to understand and predict group viewing behavior.

Abstract: TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [372] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

TL;DR: 提出了一种名为RICA的鲁棒独立成分分析方法，通过最小化多变量随机变量之间的鲁棒依赖性度量（距离相关性dCor）来估计成分。为了增强鲁棒性，引入了碗变换（bowl transform）。RICA具有强一致性、参数收敛速度，并在模拟研究中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 大多数ICA方法对离群值不鲁棒，需要一种能够有效处理离群值的ICA方法。

Method: 使用距离相关性（dCor）作为依赖性度量，并通过碗变换（bowl transform）增强鲁棒性，然后通过寻找与剩余部分具有最小dCor的成分来顺序估计独立源。

Result: RICA在模拟研究中表现出色，通常优于竞争对手，并成功应用于三个实际问题，包括鸡尾酒会问题。

Conclusion: RICA是一种鲁棒的ICA方法，具有强一致性和参数收敛速度，适用于存在离群值的数据集。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [373] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

TL;DR: The paper explores the application of interpretability techniques, specifically LIME, to understand and enhance inverse design optimization in photonic chip design, leading to better-performing components.


<details>
  <summary>Details</summary>
Motivation: Inverse design methodologies have significantly advanced photonic chip design but suffer from a lack of transparency in their optimization processes, similar to machine learning-based methods.

Method: The authors apply the LIME interpretability technique from machine learning to analyze and guide the inverse design optimization process for photonic components, focusing on two-mode multiplexers.

Result: LIME-informed insights led to more effective initial conditions, which directly improved device performance and revealed underlying patterns in the inverse design process.

Conclusion: Interpretability methods can actively guide and enhance the design of photonic components beyond just explaining models.

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [374] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

TL;DR: 提出了一种基于神经隐式函数的方法，用于学习3D表面以实现解剖学感知的精确肺段重建，并引入了两个临床相关的评估指标。此外，还开发了一个名为Lung3D的数据集，包含800个标记的肺段及其对应的气道、动脉、静脉和段间静脉的3D模型。实验结果表明，该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的3D肺段重建在肺段切除术和肺癌手术治疗计划中起着至关重要的作用。然而，传统的深度学习方法由于目标重建的分辨率要求，常常受到计算资源限制或粒度有限的问题。而隐式建模因其计算效率和连续表示的优势受到青睐。

Method: 提出了一种基于神经隐式函数的方法，通过学习3D表面来实现解剖学感知的精确肺段重建。该方法通过变形一个可学习的模板来表示形状。同时，引入了两个临床相关的评估指标来全面评估重建效果。

Result: 实验结果表明，所提出的方法优于现有方法，为肺段重建提供了新的视角。

Conclusion: 本研究提出了一种新的基于神经隐式函数的肺段重建方法，解决了传统方法的局限性，并提供了临床相关的评估指标和数据集，推动了肺段重建领域的发展。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [375] [IntrinsicEdit: Precise generative image manipulation in intrinsic space](https://arxiv.org/abs/2505.08889)
*Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Miloš Hašan,Jae Shin Yoon,Thomas Leimkühler,Christian Theobalt,Iliyan Georgiev*

Main category: cs.GR

TL;DR: Generative diffusion models have improved image editing, but lack precise control and specialization in single tasks. This paper introduces a versatile generative workflow operating in intrinsic-image latent space for semantic, local manipulation with pixel precision across various editing operations without additional data collection or model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current generative diffusion models provide high-quality image editing results with intuitive interfaces, but these interfaces lack precise control and the methods typically specialize on a single editing task.

Method: The method involves building a workflow atop the RGB-X diffusion framework to address identity preservation and intrinsic-channel entanglement by incorporating exact diffusion inversion and disentangled channel manipulation.

Result: This approach enables precise, efficient editing with automatic resolution of global illumination effects and demonstrates state-of-the-art performance across a variety of complex image editing tasks.

Conclusion: The introduced generative workflow operates effectively in an intrinsic-image latent space, enabling a range of editing operations with pixel precision and without the need for additional data collection or model fine-tuning.

Abstract: Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.

</details>


### [376] [Neural BRDF Importance Sampling by Reparameterization](https://arxiv.org/abs/2505.08998)
*Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: This paper presents a reparameterization-based formulation for neural BRDF importance sampling that improves efficiency and flexibility in the rendering pipeline, achieving superior variance reduction and high inference speeds.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the significant challenge of importance sampling in neural bidirectional reflectance distribution functions (BRDFs), which are crucial for enhancing realism in physically-based rendering.

Method: The authors introduce a reparameterization-based formulation of neural BRDF importance sampling. This method transfers the distribution learning task into identifying BRDF integral substitutions, removing constraints such as invertible networks and multi-step inference used in previous methods. It seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples.

Result: Through variance and performance analysis, the proposed reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.

Conclusion: The reparameterization-based formulation of neural BRDF importance sampling offers greater flexibility and efficiency in the rendering process, effectively reducing variance and improving rendering performance.

Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged
as popular material representations for enhancing realism in physically-based
rendering. Yet their importance sampling remains a significant challenge. In
this paper, we introduce a reparameterization-based formulation of neural BRDF
importance sampling that seamlessly integrates into the standard rendering
pipeline with precise generation of BRDF samples. The reparameterization-based
formulation transfers the distribution learning task to a problem of
identifying BRDF integral substitutions. In contrast to previous methods that
rely on invertible networks and multi-step inference to reconstruct BRDF
distributions, our model removes these constraints, which offers greater
flexibility and efficiency. Our variance and performance analysis demonstrates
that our reparameterization method achieves the best variance reduction in
neural BRDF renderings while maintaining high inference speeds compared to
existing baselines.

</details>


### [377] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

TL;DR: UMotion is an uncertainty-driven, online framework that combines IMUs and UWB sensors with a UKF for accurate 3D human shape and pose estimation.


<details>
  <summary>Details</summary>
Motivation: Sparse wearable inertial measurement units (IMUs) are popular for estimating 3D human motion but face challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies.

Method: UMotion uses six integrated body-worn ultra-wideband (UWB) distance sensors with IMUs. A tightly coupled Unscented Kalman Filter (UKF) framework fuses uncertainties from sensor data and estimated human motion based on individual body shape.

Result: Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and improving pose accuracy over state-of-the-art methods.

Conclusion: UMotion addresses the challenges faced by sparse wearable IMUs through its innovative use of UWB sensors and UKF, leading to more accurate and stable 3D human shape and pose estimation.

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


### [378] [VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality](https://arxiv.org/abs/2505.10144)
*Xuechang Tu,Lukas Radl,Michael Steiner,Markus Steinberger,Bernhard Kerbl,Fernando de la Torre*

Main category: cs.GR

TL;DR: 3DGS在VR中存在时间伪影、投影失真和帧率降低等问题。本文提出VRSplat方法，结合并扩展了3DGS的最新进展，通过高效的注视点光栅化器和优化步骤解决了这些问题，实现了72+ FPS的同时消除了伪影和干扰漂浮物。


<details>
  <summary>Details</summary>
Motivation: 3DGS技术在新视图合成方面表现出色，但在虚拟现实中存在时间伪影、投影失真和帧率降低等问题，这些问题在头戴式显示器中被放大。

Method: 结合并扩展了Mini-Splatting、StopThePop和Optimal Projection等技术，修改了个别技术和核心3DGS光栅化器；提出了一个高效的注视点光栅化器，可以一次性处理焦点和外围区域，避免冗余计算；还包含一个微调步骤，根据StopThePop深度评估和Optimal Projection优化高斯参数。

Result: 通过25名参与者的对照用户研究验证了该方法，显示出对其他Mini-Splatting配置的强烈偏好。VRSplat是第一个系统评估的3DGS方法，能够支持现代VR应用，达到72+ FPS，同时消除弹出和立体破坏漂浮物。

Conclusion: VRSplat是首个经过系统评估且能够支持现代VR应用的3DGS方法，解决了VR中的关键挑战，显著提升了用户体验。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

</details>


### [379] [Style Customization of Text-to-Vector Generation with Image Diffusion Priors](https://arxiv.org/abs/2505.10558)
*Peiying Zhang,Nanxuan Zhao,Jing Liao*

Main category: cs.GR

TL;DR: This paper proposes a two-stage style customization pipeline for SVG generation that leverages both feed-forward T2V models and T2I image priors, ensuring structural regularity while enabling diverse and high-quality custom styles based on text prompts.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-vector (T2V) methods for creating SVGs from text prompts often lack the ability to perform style customization, which is crucial for practical applications requiring consistent visual appearance and coherent aesthetics in vector graphics collections.

Method: The method involves a novel two-stage pipeline. In stage one, a T2V diffusion model with path-level representation is trained to ensure structural regularity of SVGs. In stage two, this model is customized to different styles through knowledge distillation from customized T2I models.

Result: Through extensive experiments, the proposed pipeline has been shown to generate high-quality and diverse SVGs in custom styles efficiently using text prompts.

Conclusion: The authors conclude that their two-stage style customization pipeline successfully addresses the challenges faced by existing T2V methods, providing an effective solution for generating SVGs with both structural regularity and customizable styles.

Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [380] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

TL;DR: The paper explores the development of a Large Language Model (LLM) specialized for explaining VHDL code, crucial for organizations involved in high-performance processor design. They created specific test sets and performed extended pretraining (EPT) on a base LLM, increasing expert evaluation ratings from 43% to 69%. An LLM-as-a-judge was also developed, leading to new models with an instruction-tuned EPT model expected to reach a rating of 71%, potentially rising to 85% with advanced base models. The conclusion discusses enhancing hardware design LLMs using developments in Generative AI.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention given to VHDL in the context of Large Language Models (LLMs), despite its industry popularity, and to meet the unique needs of organizations engaged in high-performance processor design.

Method: Developed test sets specific to their needs and conducted extended pretraining (EPT) on a base LLM. Expert evaluations were used to assess the quality of code explanations produced by the EPT model. Also developed an LLM-as-a-judge to evaluate models similarly to expert evaluators.

Result: Expert evaluation ratings increased from 43% for the base model to 69% for the EPT model. The LLM-as-a-judge helped derive new models, including an instruction-tuned version of the EPT model with an expected rating of 71%. Experiments suggest that newer base models could push this rating to 85% or higher.

Conclusion: Discusses further improving the quality of hardware design LLMs using advancements in Generative AI.

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [381] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

TL;DR: This paper presents a method using customized AI tools, particularly GPT-4o and a distilled model, to enhance code comments across different quality axes.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of code comments by rewriting them with customized AI-based tools, ensuring better understanding and maintenance of code.

Method: An empirical study followed by grounded theory qualitative analysis determined the quality axes for improvement. Then, a procedure was proposed that uses a Large Language Model (LLM) like GPT-4o to rewrite existing code comments along these axes. The results were distilled into a smaller model for in-house use.

Result: The evaluation demonstrated that the procedure successfully improves code comments along the defined quality axes.

Conclusion: The approach effectively enhances code comment quality and all data and source code have been made available in an online repository for reproducibility.

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [382] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

TL;DR: The paper introduces WebApp1K, a benchmark for evaluating LLMs in TDD tasks using test cases as prompts and verifications. It highlights instruction following and in-context learning as key capabilities for TDD success.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' ability to interpret and implement functionality directly from test cases, reflecting real-world software development practices.

Method: Created a benchmark named WebApp1K with 1000 diverse challenges across 20 application domains, focusing on evaluating LLMs' abilities in generating compact, functional code under specific constraints.

Result: Found that instruction following and in-context learning are more critical than general coding proficiency or pretraining knowledge for TDD success. Identified performance bottlenecks like instruction loss in long prompts.

Conclusion: WebApp1K underscores the practical value of TDD-specific benchmarks and sets the stage for advancing LLM capabilities in rigorous, application-driven coding scenarios.

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [383] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

TL;DR: Recent advancements use transformer-based models for source code summarization, but they often generate only one summary. This paper introduces Variational Prefix Tuning (VPT), which uses a CVAE framework to generate diverse summaries from pre-trained models, without retraining them. A bi-criteria reranking method is also used to ensure diversity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for source code summarization focus on generating a single high-quality summary, neglecting scenarios where multiple alternatives might be needed.

Method: The paper proposes Variational Prefix Tuning (VPT) that integrates a Conditional Variational Autoencoder (CVAE) as a modular component into pre-trained models. It allows sampling of continuous embeddings used as prefixes to guide the generation of diverse outputs during decoding. The method is parameter-efficient and does not require model retraining. Additionally, a bi-criteria reranking method is employed to select summaries optimizing both diversity and accuracy.

Result: Extensive experimental evaluations using widely recognized datasets and state-of-the-art pre-trained code summarization models demonstrate the effectiveness of VPT and its adaptability across different models.

Conclusion: VPT enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, providing users with more suitable options for source code summaries.

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


### [384] [Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values](https://arxiv.org/abs/2505.09830)
*Martín Rodríguez,Gustavo Rossi,Alejandro Fernandez*

Main category: cs.SE

TL;DR: The study explores the potential of LLMs in generating unit test cases, comparing them with manual tests.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of Large Language Models (LLMs) in automatically generating test cases and compare their effectiveness with manual tests.

Method: An optimized prompt integrating code and requirements was developed to cover critical cases such as equivalence partitions and boundary values. Quantitative metrics and manual qualitative analysis were used to compare LLMs and trained programmers.

Result: The effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. LLMs are flexible and promising but still require human supervision.

Conclusion: This work highlights the importance of manual qualitative analysis as a complement to automation in unit test evaluation.

Abstract: The design and implementation of unit tests is a complex task many
programmers neglect. This research evaluates the potential of Large Language
Models (LLMs) in automatically generating test cases, comparing them with
manual tests. An optimized prompt was developed, that integrates code and
requirements, covering critical cases such as equivalence partitions and
boundary values. The strengths and weaknesses of LLMs versus trained
programmers were compared through quantitative metrics and manual qualitative
analysis. The results show that the effectiveness of LLMs depends on
well-designed prompts, robust implementation, and precise requirements.
Although flexible and promising, LLMs still require human supervision. This
work highlights the importance of manual qualitative analysis as an essential
complement to automation in unit test evaluation.

</details>


### [385] [Are Sparse Autoencoders Useful for Java Function Bug Detection?](https://arxiv.org/abs/2505.10375)
*Rui Melo,Claudia Mamede,Andre Catarino,Rui Abreu,Henrique Lopes Cardoso*

Main category: cs.SE

TL;DR: The paper explores the use of Sparse Autoencoders (SAEs) as a lightweight, interpretable alternative for bug detection in Java functions using representations from pretrained LLMs like GPT-2 Small and Gemma 2B. SAE-derived features achieved an F1 score of up to 89%, outperforming fine-tuned transformer encoder baselines without requiring fine-tuning or task-specific supervision.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities pose significant security risks, and traditional detection methods face challenges such as high false positive rates and scalability issues. AI-based approaches, particularly LLMs, offer new possibilities but encounter interpretability and deployment problems due to their complexity.

Method: The researchers applied Sparse Autoencoders (SAEs) to internal representations from pretrained LLMs (GPT-2 Small and Gemma 2B) to detect bugs in Java functions. The method did not involve fine-tuning the LLMs or applying task-specific supervision.

Result: SAE-derived features achieved an F1 score of up to 89% in bug detection, consistently surpassing fine-tuned transformer encoder baselines.

Conclusion: This study provides the first empirical evidence that SAEs can effectively detect software bugs directly from the internal representations of pretrained LLMs without fine-tuning or additional supervision.

Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.

</details>


### [386] [Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?](https://arxiv.org/abs/2505.10443)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 研究人员通过语义保持的代码变异方法评估了六个大型语言模型（LLM）对Python程序的理解能力，发现如Llama3.2等模型在高达61%的情况下基于有缺陷的推理得出正确预测，并且这些模型在面对代码变异时预测不稳定，表明其语义理解能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型（LLMs）在编程任务中的研究多集中于预测准确性，而忽略了对其推理过程的评估。此外，已知LLMs可能通过错误逻辑得出正确答案，特别是在数学推理任务中。这促使研究者进一步探讨LLMs是否真正能够理解代码逻辑还是仅凭猜测。

Method: 研究者采用了五种语义保持的代码变异技术：变量重命名、镜像比较表达式、交换if-else分支、for循环转换为while循环以及循环展开。通过LiveCodeBench和CruxEval平台，他们评估了六个LLM模型在面对这些变异时的表现，并由人类专家分析正确预测背后的推理依据。

Result: 结果显示，部分LLM（例如Llama3.2）在高达61%的情况下基于有缺陷的推理得出了正确的预测结果。此外，当代码发生变异时，LLM经常改变预测结果，表明它们的语义理解能力较为有限。

Conclusion: 本研究表明，尽管一些LLM可以生成正确的预测结果，但其推理过程可能存在重大缺陷，且模型在代码变异面前表现出不稳定性，说明其语义理解能力尚需改进。

Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [387] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了一种基于主动学习策略的新型逆向材料设计生成框架InvDesFlow-AL，用于功能材料的开发。该框架在晶体结构预测方面表现优异，同时在低形成能和低Ehull材料的设计中也得到了成功验证。通过寻找BCS超导体的例子，成功确定了Li₂AuH₆作为一种具有超高转变温度（140 K）的传统BCS超导体。


<details>
  <summary>Details</summary>
Motivation: 开发具有特定性能的功能材料的逆向设计方法对于可再生能源、催化、储能和碳捕获等领域的发展至关重要。现有的生成和预测晶体结构的方法往往成功率较低，因此需要一种更高效的材料生成框架来加速这一过程。

Method: 提出了名为InvDesFlow-AL的逆向材料设计生成框架，该框架基于主动学习策略。通过迭代优化材料生成过程，逐渐引导其达到所需的性能特征。此模型能够系统地生成具有逐步降低形成能的材料，并不断扩展对多样化化学空间的探索。

Result: 在晶体结构预测方面，InvDesFlow-AL模型实现了0.0423 Å的RMSE，相比现有生成模型性能提高了32.96%。此外，在低形成能和低Ehull材料的设计中也取得了成功验证，并成功识别出Li₂AuH₆为一种转变温度为140 K的常规BCS超导体。

Conclusion: InvDesFlow-AL框架显著提升了材料发现和逆向设计的效率，证明了主动学习驱动的生成模型在材料科学中的应用潜力。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [388] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes a synthesizability-driven CSP framework integrating symmetry-guided structure derivation and machine learning, successfully predicting synthesizable structures and demonstrating potential for experimental realization.


<details>
  <summary>Details</summary>
Motivation: To address the gap between theoretical predictions and experimental synthesis in thermodynamic energy-based crystal structure prediction (CSP), especially for metastable materials synthesized through kinetically controlled pathways.

Method: A synthesizability-driven CSP framework that combines symmetry-guided structure derivation with a Wyckoff encode-based machine-learning model to identify promising subspaces. A structure-based synthesizability evaluation model fine-tuned with recently synthesized structures is then used alongside ab initio calculations to predict synthesizable candidates.

Result: Reproduced 13 experimentally known XSe structures, filtered 92,310 structures from 554,054 candidates predicted by GNoME, and identified eight thermodynamically favorable Hf-X-O structures, including three highly synthesizable HfV$_2$O$_7$ candidates.

Conclusion: Establishes a data-driven paradigm for machine-learning-assisted inorganic materials synthesis, bridging the gap between computational predictions and experimental realization, and unlocking opportunities for discovering novel functional materials.

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [389] [Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training](https://arxiv.org/abs/2505.10393)
*Agustin Medina,Marcelo Arlego,Carlos A. Lamas*

Main category: cond-mat.str-el

TL;DR: 本研究探讨了使用人工神经网络高效学习磁相的方法，结合计算简单性和物理信息策略。通过监督分类和非监督检测两种方法，并加入物理信息指导，验证了机器学习预测与数值估计结果的一致性。结果表明，合成、结构化且计算高效的训练方案可以揭示物理意义的相边界，为传统方法提供了低成本且稳健的替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于稀释伊辛模型缺乏精确解析解，因此需要探索有效的学习方法来理解其相结构。

Method: 使用简单的密集神经网络进行监督分类，以及仅基于理想自旋配置训练的卷积自动编码器进行非监督检测。同时，利用架构偏差放大对称性破坏相关特征，并包含显式破坏$\mathbb{Z}_2$对称性的训练配置以增强网络检测有序相的能力。

Result: 机器学习预测与直接数值估计的关键温度和渗透阈值一致，证明了该方法在揭示复杂系统中物理意义相边界的有效性。

Conclusion: 合成、结构化且计算高效的训练方案可以揭示物理意义的相边界，为传统方法提供了低成本且稳健的替代方案，具有广泛的应用前景。

Abstract: We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [390] [LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps](https://arxiv.org/abs/2505.10537)
*Filippo Olimpieri,Noemi Giustini,Andrea Lacava,Salvatore D'Oro,Tommaso Melodia,Francesca Cuomo*

Main category: cs.NI

TL;DR: The paper introduces LibIQ, a library for RF signals that uses dApps concept to enable real-time RF spectrum classification. It processes I/Q samples to detect and classify external RF signals using a CNN, achieving an average accuracy of 97.8%. The authors plan to release LibIQ and the dataset publicly.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current RICs, such as latency overhead in data exchange and inability to access user plain data, which restrict real-time monitoring and certain use cases like beamforming and spectrum classification.

Method: Leverage the dApps concept and develop LibIQ, a novel library for RF signals that provides functionalities to read I/Q samples as time-series, create datasets, visualize data through plots and spectrograms, and classify signals using a CNN.

Result: Achieved an average accuracy of approximately 97.8% in identifying signal types across all scenarios in real-time analysis when deploying LibIQ in heterogeneous scenarios with varying conditions.

Conclusion: LibIQ enables efficient and accurate real-time RF spectrum classification and the authors intend to release it along with the created dataset as a public framework.

Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [391] [Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease](https://arxiv.org/abs/2505.09624)
*Ekaterina Kuzmina,Dmitrii Kriukov,Mikhail Lebedev,Dmitry V. Dylov*

Main category: q-bio.NC

TL;DR: An adaptive deep brain stimulation (aDBS) benchmark is introduced, featuring neurophysiological realism and a structured environment for training and evaluating deep reinforcement learning algorithms to optimize aDBS control strategies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in data collection for optimizing aDBS control offline and to provide a realistic benchmark for comparing synthetic models of Parkinson's Disease (PD) and control algorithms.

Method: The methodology includes conventional basal ganglia circuit dynamics and pathological oscillations, along with 15 previously overlooked physiological attributes such as signal instabilities, noise, neural drift, electrode conductance changes, and individual variability. These are modeled using beta-band activity in the brain and feedback mechanisms. A structured environment is also built for training and evaluating deep reinforcement learning algorithms.

Result: This approach creates a neurophysiologically realistic benchmark that can be used to train and evaluate deep RL algorithms, thereby opening new possibilities for optimizing aDBS control strategies.

Conclusion: This work invites the machine learning community to contribute to the development of intelligent neurostimulation interfaces by providing a comprehensive and realistic benchmark for aDBS.

Abstract: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment
for Parkinson disease (PD). In aDBS, a surgically placed electrode sends
dynamically altered stimuli to the brain based on neurophysiological feedback:
an invasive gadget that limits the amount of data one could collect for
optimizing the control offline. As a consequence, a plethora of synthetic
models of PD and those of the control algorithms have been proposed. Herein, we
introduce the first neurophysiologically realistic benchmark for comparing said
models. Specifically, our methodology covers not only conventional basal
ganglia circuit dynamics and pathological oscillations, but also captures 15
previously dismissed physiological attributes, such as signal instabilities and
noise, neural drift, electrode conductance changes and individual variability -
all modeled as spatially distributed and temporally registered features via
beta-band activity in the brain and a feedback. Furthermore, we purposely built
our framework as a structured environment for training and evaluating deep
reinforcement learning (RL) algorithms, opening new possibilities for
optimizing aDBS control strategies and inviting the machine learning community
to contribute to the emerging field of intelligent neurostimulation interfaces.

</details>


### [392] [Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making](https://arxiv.org/abs/2505.09646)
*Carmel Mary Esther A*

Main category: q-bio.NC

TL;DR: This paper proposes a novel theoretical model for real-time awareness by reducing perceptual delays, through investigating cosmic signal delay, neurological reaction times and the ancient cognitive state of stillness.


<details>
  <summary>Details</summary>
Motivation: To explain how human mind and artificial intelligence can approach real-time awareness by reducing perceptual delays.

Method: Investigating cosmic signal delay, neurological reaction times, and the ancient cognitive state of stillness to shift from reactive perception to a conscious interface with the near future.

Result: Introduction of a physical and cognitive model for perceiving the present as an interference zone where early-arriving cosmic signals and reactive human delays intersect, along with experimental approaches using human neural observation and neuro-receptive extensions.

Conclusion: Proposes a mathematical framework to guide AI systems toward temporally efficient, ethically sound, and internally conscious decision-making processes.

Abstract: This paper proposes a novel theoretical model to explain how the human mind
and artificial intelligence can approach real-time awareness by reducing
perceptual delays. By investigating cosmic signal delay, neurological reaction
times, and the ancient cognitive state of stillness, we explore how one may
shift from reactive perception to a conscious interface with the near future.
This paper introduces both a physical and cognitive model for perceiving the
present not as a linear timestamp, but as an interference zone where
early-arriving cosmic signals and reactive human delays intersect. We propose
experimental approaches to test these ideas using human neural observation and
neuro-receptive extensions. Finally, we propose a mathematical framework to
guide the evolution of AI systems toward temporally efficient, ethically sound,
and internally conscious decision-making processes

</details>


### [393] [A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System](https://arxiv.org/abs/2505.09643)
*Zhixuan Wang*

Main category: q-bio.NC

TL;DR: Epilepsy patients often use alternative medicine due to the limitations of conventional drugs. This study developed an AI-driven system that analyzes natural compounds and RCTs, identifying 17 high-efficacy herbs for epilepsy treatment with significant seizure reduction confirmed in a validation trial.


<details>
  <summary>Details</summary>
Motivation: To optimize herbal epilepsy treatment by leveraging AI-driven analysis of global natural products and statistically validated RCTs, addressing the limitations of conventional antiepileptic drugs.

Method: The intelligent prescription system integrates machine learning algorithms for herb-efficacy characterization, Bayesian optimization for personalized dosing, and meta-analysis of RCTs for evidence-based recommendations. It analyzed 1,872 natural compounds and clinical outcomes from 48 RCTs covering 48 epilepsy conditions.

Result: Identified 17 high-efficacy herbs showing significant seizure reduction (p<0.01, Cohen's d=0.89) with statistical significance confirmed by multiple testing (p<0.001). A validation trial demonstrated 28.5% greater seizure frequency reduction compared to conventional protocols.

Conclusion: AI-optimized herbal prescriptions offer a promising alternative for epilepsy treatment, significantly reducing seizure frequency.

Abstract: Epilepsy is a prevalent neurological disease with millions of patients
worldwide. Many patients have turned to alternative medicine due to the limited
efficacy and side effects of conventional antiepileptic drugs. In this study,
we developed a computational approach to optimize herbal epilepsy treatment
through AI-driven analysis of global natural products and statistically
validated randomized controlled trials (RCTs). Our intelligent prescription
system combines machine learning (ML) algorithms for herb-efficacy
characterization, Bayesian optimization for personalized dosing, and
meta-analysis of RCTs for evidence-based recommendations. The system analyzed
1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and
ethnopharmacological databases, integrating their bioactive properties with
clinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using
LASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs
(e.g., Gastrodia elata [using \'e for accented characters], Withania
somnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)
with statistical significance confirmed by multiple testing (p$<$0.001). A
randomized double-blind validation trial (n=120) demonstrated 28.5\% greater
seizure frequency reduction with AI-optimized herbal prescriptions compared to
conventional protocols (95\% CI: 18.7-37.3\%, p=0.003).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [394] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

TL;DR: 在统计均值估计问题中，当样本通过记忆消除通道传输时，本文提出了最优的误差指数，并证明了其在广泛的估计精度和通道质量条件下是紧的。相较之下，两种自然的基线方法产生的误差指数严格次优。


<details>
  <summary>Details</summary>
Motivation: 研究者关注的是一个统计均值估计问题，其中样本不是直接被观察到，而是通过一个中继（教师）通过记忆消除通道将信息传递给解码器（学生），学生最终进行估计。目标是在大偏差制度下分析最小最大估计误差，并寻找紧的可实现误差指数。

Method: 首先聚焦于伯努利源和二进制对称通道，然后推广到次高斯和重尾分布设置以及任意离散无记忆通道。通过理论分析，提出了一种方法以获得紧的误差指数，并与两个自然的基线方法进行了比较。

Result: 提出的误差指数在广泛的估计精度和通道质量条件下是紧的，而两种自然的基线方法产生了严格次优的误差指数。

Conclusion: 该研究表明，在特定条件下，通过设计适当的传输机制，可以在样本不直接观察的情况下实现最优的均值估计性能。这对于涉及间接观测的实际应用具有重要意义。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [395] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

TL;DR: An innovative training method for dynamical systems governed by Lagrangian mechanics is proposed, which uses Equilibrium Propagation and the principle of action extremization to adjust system trajectories towards desired targets.


<details>
  <summary>Details</summary>
Motivation: Current methods for training dynamical systems may be inefficient or complex, especially when dealing with periodic boundary conditions or fixed initial and final states. There is a need for an approach that can efficiently update parameters without explicit backpropagation through time.

Method: The paper introduces a method extending Equilibrium Propagation to dynamical trajectories by utilizing the principle of action extremization. The system is trained by slightly nudging its trajectories towards desired targets and observing the response of variables conjugate to the parameters being trained.

Result: This method allows efficient parameter updates in systems with periodic boundary conditions or fixed initial and final states, without requiring explicit backpropagation through time. It also yields the semiclassical limit of Quantum Equilibrium Propagation in the case of periodic boundary conditions and has potential applications in systems with dissipation.

Conclusion: The proposed method provides an effective way to train dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation and action extremization, enabling efficient learning in specific types of systems.

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [396] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

TL;DR: WavReward is a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. It outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the gap in evaluating spoken dialogue models' conversational performance, which cannot be easily measured using text-based language models like ChatGPT.

Method: 1) Based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, a specialized evaluator tailored to spoken dialogue models is constructed. 2) ChatReward-30K, a preference dataset used to train WavReward, is introduced. This dataset includes comprehension and generation aspects of spoken dialogue models covering various tasks.

Result: WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%.

Conclusion: Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly available after the paper is accepted.

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [397] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/abs/2505.09972)
*Anchen Sun,Tiantian Feng,Gabriela Gutierrez,Juan J Londono,Anfeng Xu,Batya Elbaum,Shrikanth Narayanan,Lynn K Perry,Daniel S Messinger*

Main category: eess.AS

TL;DR: This paper introduces an automated framework WSW2.0 for analyzing vocal interactions in preschool classrooms, enhancing both accuracy and scalability through the integration of wav2vec2-based speaker classification and Whisper speech transcription.


<details>
  <summary>Details</summary>
Motivation: To provide accurate measures of key features of preschool classroom speech using deep learning and natural language processing techniques, ultimately guiding more effective intervention strategies and supporting early childhood language development.

Method: Integration of wav2vec2-based speaker classification and Whisper (large-v2 and large-v3) speech transcription to create WSW2.0 framework. Comparison of system outputs to expert human annotations on 235 minutes of audio recordings.

Result: WSW2.0 achieves a weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of .672 for speaker classification. Transcription quality is moderate to high with word error rates of .119 for teachers and .238 for children. High absolute agreement intraclass correlations (ICC) with expert transcriptions for a range of classroom language features.

Conclusion: The findings highlight the potential of deep learning and natural language processing techniques to revolutionize educational research by providing accurate measures of key features of preschool classroom speech.

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [398] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

TL;DR: The paper presents a Multi-Agent Reinforcement Learning (MARL) algorithm for localizing multiple airborne pollution sources using small uncrewed aerial systems, significantly outperforming conventional approaches.


<details>
  <summary>Details</summary>
Motivation: Industrial catastrophes demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment.

Method: The method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN). It uses full sequences of historical action-observation pairs, effectively approximating latent states. A general-purpose simulation environment based on the Gaussian Plume Model (GPM) is used, incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources.

Result: Extensive simulations show that the algorithm significantly outperforms conventional approaches, with agents exploring only 1.29% of the environment to successfully locate pollution sources.

Conclusion: The MARL algorithm provides an effective solution for localizing multiple airborne pollution sources in complex, partially observable environments.

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [399] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/abs/2505.10387)
*Artem Agafonov,Konstantin Yakovlev*

Main category: cs.MA

TL;DR: 在多智能体路径寻找（MAPF）问题中，考虑智能体大小会使问题变得更加复杂。本文证明了带有大型智能体的MAPF问题是NP难问题，这意味着如果P!=NP，则不存在多项式时间算法可以解决该问题。


<details>
  <summary>Details</summary>
Motivation: 传统的MAPF问题假设智能体没有大小，并仅考虑两种冲突类型：在同一时间步占用同一顶点或使用同一边。然而，在许多实际应用中（如机器人技术），考虑智能体的大小对于确保MAPF解决方案的安全执行至关重要。因此，研究引入大型智能体后对问题复杂性的影响是有意义的。

Method: 作者通过将著名的3SAT问题（已知为NP完全问题）归约到带有大型智能体的MAPF问题来证明其NP难度。具体而言，他们为任意的3SAT公式构造一个专门的图，并展示出给定的3SAT公式是可满足的当且仅当相应的路径寻找实例有解。

Result: 成功证明了带有大型智能体的MAPF问题是NP难问题。这意味着如果P!=NP，则不存在能够解决该问题的完整多项式时间算法。

Conclusion: 带有大型智能体的MAPF问题是NP难问题，这表明我们需要寻求启发式方法或其他近似算法来解决这一问题，而不是期望存在一个通用的多项式时间算法。

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [400] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/abs/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

TL;DR: The paper discusses the development of a protocol for real-time content moderation in live video streams, enabling removal of objectionable content and resumption of playback when appropriate.


<details>
  <summary>Details</summary>
Motivation: There is an increased need for robust content moderation in live video streaming due to its popularity on social media platforms. Live streaming imposes restrictions on latency for both analysis and distribution.

Method: Extensions to the Media Over QUIC Transport protocol are presented, which enable real-time content moderation in one-to-many video live streams. The solution removes only the video segments that contain objectionable content and allows playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices.

Result: The system was implemented and evaluated in the context of light strobe removal for photosensitive viewers, resulting in an increased latency of only one group-of-pictures duration for streaming clients.

Conclusion: The proposed extensions to the Media Over QUIC Transport protocol effectively enable real-time content moderation in live video streams with minimal impact on latency.

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [401] [Virtual Dosimetrists: A Radiotherapy Training "Flight Simulator"](https://arxiv.org/abs/2505.09796)
*Skylar S. Gay,Tucker Netherton,Barbara Marquez,Raymond Mumme,Mary Gronberg,Brent Parker,Chelsea Pinnix,Sanjay Shete,Carlos Cardenas,Laurence Court*

Main category: physics.med-ph

TL;DR: This paper introduces 'Virtual Dosimetrist' models that generate suboptimal radiotherapy plans and allow trainees to improve them via natural language prompts, addressing limitations in the current clinic-based training paradigm.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective education in radiotherapy plan quality review which requires robust, regularly updated examples and flexibility to demonstrate multiple planning approaches and their consequences. The current clinic-based paradigm does not support these needs.

Method: The researchers developed 'Virtual Dosimetrist' models that can generate training examples of suboptimal treatment plans and enable trainees to improve the plan quality through simple natural language prompts.

Result: The dose generation and modification process is accurate, rapid, and requires only modest resources. This work successfully combines dose distribution prediction with natural language processing.

Conclusion: The 'Virtual Dosimetrist' models provide a robust pipeline for generating suboptimal training plans and allowing trainees to practice their critical plan review and improvement skills, addressing the challenges of the current clinic-based paradigm.

Abstract: Effective education in radiotherapy plan quality review requires a robust,
regularly updated set of examples and the flexibility to demonstrate multiple
possible planning approaches and their consequences. However, the current
clinic-based paradigm does not support these needs. To address this, we have
developed 'Virtual Dosimetrist' models that can both generate training examples
of suboptimal treatment plans and then allow trainees to improve the plan
quality through simple natural language prompts, as if communicating with a
dosimetrist. The dose generation and modification process is accurate, rapid,
and requires only modest resources. This work is the first to combine dose
distribution prediction with natural language processing; providing a robust
pipeline for both generating suboptimal training plans and allowing trainees to
practice their critical plan review and improvement skills that addresses the
challenges of the current clinic-based paradigm.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [402] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

TL;DR: TensorRL-QAS 是一种结合张量网络方法与强化学习的可扩展框架，用于设计量子电路。通过矩阵乘积态近似目标解来初始化架构搜索，从而缩小搜索空间并加速收敛到理想解。在多个量子化学问题上，该方法显著减少了 CNOT 数量和电路深度，并提高了训练效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法有潜力解决嘈杂中等规模量子硬件上的有意义问题，但设计既符合设备限制又能解决问题的量子电路具有挑战性。基于强化学习的量子架构搜索（QAS）方法虽然有希望，但面临严重的可扩展性问题。

Method: 引入了 TensorRL-QAS 框架，将张量网络方法与强化学习结合，以设计量子电路。使用矩阵乘积态近似目标解来初始化架构搜索，从而缩小搜索空间至物理上有意义的电路。

Result: 在最多 12 量子比特的多个量子化学问题上，TensorRL-QAS 将 CNOT 数量和电路深度减少多达 10 倍，同时保持或超越化学精度。它将函数评估减少多达 100 倍，加速训练回合多达 98%，并在 10 量子比特系统上实现高达 50% 的成功概率。在无噪声和有噪声场景中均表现出鲁棒性和多样性。

Conclusion: TensorRL-QAS 被确立为一个有前途的候选方案，用于在近期量子硬件上实现可扩展和高效的量子电路发现协议。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [403] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: The paper presents Quantum Parameter Adaptation (QPA), a hybrid quantum-classical framework for efficient typhoon forecasting model learning, which significantly reduces trainable parameters while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Typhoon trajectory forecasting is crucial but computationally intensive due to atmospheric dynamics complexity and deep learning resource demands.

Method: Quantum Parameter Adaptation (QPA) is introduced, leveraging quantum neural networks (QNNs) only during training as part of the Quantum-Train (QT) framework, integrated with an Attention-based Multi-ConvGRU model for parameter-efficient training.

Result: QPA reduces the number of trainable parameters significantly while preserving predictive performance.

Conclusion: This work marks the first application of quantum machine learning to large-scale typhoon trajectory prediction, providing a scalable and energy-efficient method for climate modeling.

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [404] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

TL;DR: This paper explores work extraction protocols for transferring maximum energy to a battery from unknown pure qubit states, using sequential access and adaptive strategies inspired by reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve energy harvesting from qubits by balancing the charging of the battery optimally with acquiring more information about the qubit state to enhance future rounds of energy extraction.

Method: The method involves leveraging exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies for optimal energy extraction from qubits.

Result: The result is an exponential improvement over current protocols based on full state tomography, with energy dissipation scaling only poly-logarithmically in $N$ copies of the qubit state.

Conclusion: Adaptive strategies developed using reinforcement learning principles provide a significant enhancement in energy extraction efficiency compared to traditional methods.

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


### [405] [Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation](https://arxiv.org/abs/2505.09653)
*Samuel Yen-Chi Chen,Chen-Yu Liu,Kuan-Cheng Chen,Wei-Jia Huang,Yen-Jui Chang,Wei-Hao Huang*

Main category: quant-ph

TL;DR: The paper proposes an automated solution using differentiable optimization to design quantum neural networks (QNNs) that can generate classical neural network parameters. This method jointly optimizes both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for effective quantum circuit architectures for quantum-enhanced neural programmers, which remains non-trivial and often requires expertise in quantum information science.

Method: The method uses differentiable optimization to jointly optimize both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.

Result: Simulation results show that the proposed method matches or outperforms manually designed QNN architectures on classification, time-series prediction, and reinforcement learning tasks.

Conclusion: This work offers a scalable and automated pathway for designing QNNs that can generate classical neural network parameters across diverse applications.

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have led to the emergence of quantum machine learning (QML), which integrates
the strengths of both fields. Among QML approaches, variational quantum
circuits (VQCs), also known as quantum neural networks (QNNs), have shown
promise both empirically and theoretically. However, their broader adoption is
hindered by reliance on quantum hardware during inference. Hardware
imperfections and limited access to quantum devices pose practical challenges.
To address this, the Quantum-Train (QT) framework leverages the exponential
scaling of quantum amplitudes to generate classical neural network parameters,
enabling inference without quantum hardware and achieving significant parameter
compression. Yet, designing effective quantum circuit architectures for such
quantum-enhanced neural programmers remains non-trivial and often requires
expertise in quantum information science. In this paper, we propose an
automated solution using differentiable optimization. Our method jointly
optimizes both conventional circuit parameters and architectural parameters in
an end-to-end manner via automatic differentiation. We evaluate the proposed
framework on classification, time-series prediction, and reinforcement learning
tasks. Simulation results show that our method matches or outperforms manually
designed QNN architectures. This work offers a scalable and automated pathway
for designing QNNs that can generate classical neural network parameters across
diverse applications.

</details>


### [406] [Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering](https://arxiv.org/abs/2505.10012)
*Tadashi Kadowaki*

Main category: quant-ph

TL;DR: This paper explores the integration of quantum computing and AI in scientific automation, introducing Quantum CAE for tasks like simulation and optimization, with case studies on combinatorial problems. It emphasizes the role of AI agents in quantum algorithm design and questions the future collaboration between humans, AI, and quantum resources.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the recent advances in AI and quantum computing that are reshaping research methodologies, necessitating a framework that can leverage these technologies for enhanced automation in engineering processes.

Method: The method involves using Quantum CAE as a framework which applies quantum algorithms for simulation, optimization, and machine learning within engineering design, supported by practical implementations through case studies focused on combinatorial optimization problems.

Result: The results include successful illustrations of Quantum CAE applications in solving combinatorial optimization problems and discussions on advancements towards higher automation levels, highlighting the importance of specialized AI agents in proficient quantum algorithm design.

Conclusion: The conclusion underscores the transformative potential of integrating quantum computing with AI, posing significant questions about future collaborative dynamics among human scientists, engineers, AI systems, and quantum computational resources.

Abstract: Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.

</details>


### [407] [Role of scrambling and noise in temporal information processing with quantum systems](https://arxiv.org/abs/2505.10080)
*Weijie Xiong,Zoë Holmes,Armando Angrisani,Yudai Suzuki,Thiparat Chotibut,Supanut Thanasilp*

Main category: quant-ph

TL;DR: Quantum scrambling systems are effective for temporal information processing. This paper explores their scalability and memory retention using high-order unitary designs, showing exponential concentration of readouts with reservoir size but without worsening over iterations in noiseless settings. However, scaling problem size deteriorates generalization unless an exponential shot overhead is affordable. Memory of early inputs decays exponentially in both reservoir size and iterations, also proven for noisy channels.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical performance of quantum scrambling systems in temporal tasks, specifically focusing on scalability and memory retention.

Method: A general quantum reservoir processing framework was used to examine models with quantum systems. High-order unitary designs were applied to model reservoirs in both noiseless and noisy settings. New proof techniques were introduced to bound concentration in temporal quantum learning models.

Result: In noiseless settings, measurement readouts concentrate exponentially with increasing reservoir size without worsening over iterations. Memory of early inputs decays exponentially in both reservoir size and iterations. In noisy settings, exponential memory decay with iterations was proven for local noisy channels.

Conclusion: Quantum scrambling systems show potential for temporal information processing but face challenges in scalability and memory retention. Theoretical analysis reveals that while small reservoir reusability is possible, scaling up requires significant resources.

Abstract: Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [408] [ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks](https://arxiv.org/abs/2505.10371)
*Kai Sun,Peibo Duan,Levin Kuhlmann,Beilun Wang,Bin Zhang*

Main category: cs.NE

TL;DR: A temporal Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model is proposed to solve the dilemma of gamma in Spiking Neural Networks (SNNs), which improves energy efficiency, stabilizes training and enhances accuracy.


<details>
  <summary>Details</summary>
Motivation: The non-differentiable spike function in SNNs is approximated by surrogate gradients with a narrow range of nonzero derivatives near the firing threshold. This leads to a dilemma: large gamma causes overactivation while small gamma results in vanishing gradients and weakened temporal dependencies.

Method: The ILIF neuron model incorporates interconnected inhibitory units for membrane potential and current, inspired by biological inhibitory mechanisms, to mitigate overactivation while preserving gradient propagation.

Result: Theoretical analysis shows that ILIF effectively overcomes the gamma dilemma. Experiments on multiple datasets demonstrate reduced firing rates, stabilized training, enhanced accuracy, and improved energy efficiency.

Conclusion: The ILIF neuron model successfully addresses the gamma dilemma in SNNs, leading to more energy-efficient, stable, and accurate networks.

Abstract: The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [409] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

TL;DR: 为了适应视觉语言模型（VLM）在医疗保健等高风险领域的开发、适配和应用，本文提出需要建立清晰和标准化的报告协议。文章建议重新构建传统的机器学习报告标准和评估指南以适应多阶段的VLM研究，并提出了一个分类框架及相应的报告标准，最后还提供了一个检查列表以确保VLM相关研究发表的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的研究具有多样性和复杂性，从新模型的开发到特定领域的微调，再到直接应用于诊断和预测任务，这些差异使得制定统一的报告标准变得困难。然而，在如医疗保健这样的高风险领域，缺乏明确和标准化的报告协议可能会导致严重的后果。因此，有必要为VLM研究制定专门的报告标准和评估指南。

Method: 作者首先讨论了传统机器学习报告标准的不足，并提出需要根据VLM研究的特点进行调整。接着，文章提出了一种针对VLM研究的分类框架，该框架涵盖了从模型开发到实际应用的不同阶段。然后，根据这一分类框架，详细描述了各个阶段的报告标准，包括性能评估、数据报告协议以及论文撰写建议。最后，提供了一个综合性的检查列表，帮助研究人员确保其研究符合所提出的报告标准。

Result: 通过提出分类框架和具体的报告标准，本文为VLM研究提供了一个全面的指导方案，有助于提高研究的透明度和可重复性。此外，所提供的检查列表可以作为工具，促进社区对这些标准的采纳和使用。

Conclusion: 视觉语言模型的研究需要专门设计的报告标准来适应其多样性和复杂性。本文提出的分类框架和报告标准为未来VLM研究提供了指导方向，同时，通过提供的检查列表，促进了这些标准在研究社区中的应用。这将有助于提升VLM研究的质量和一致性，尤其是在医疗保健等关键领域。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [410] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [411] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

TL;DR: The paper introduces FareShare, a computational tool designed to automate lost wage estimation for deactivated rideshare drivers. Through a partnership with Washington's largest rideshare labor union, the deployment of FareShare showed significant reductions in calculation time and errors, while also highlighting socio-technical challenges.


<details>
  <summary>Details</summary>
Motivation: Deactivation from gig work platforms can severely impact workers' financial stability due to arbitrary AI and algorithmic decisions. Current policies mandate appeals processes and compensation recovery, but lack effective tools to support these workflows.

Method: The authors partnered with the State of Washington's largest rideshare labor union over 6 months to design FareShare. The tool was then deployed in the field for 3 months, registering 178 account signups.

Result: FareShare reduced lost wage calculation time by over 95%, eliminated manual data entry errors, and enabled more efficient report generation for legal teams.

Conclusion: While FareShare demonstrated significant improvements in efficiency and accuracy, the deployment also revealed important socio-technical challenges related to trust, consent, and tool adoption in high-stakes labor contexts.

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [412] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

TL;DR: This paper investigates how socioeconomic factors influence industry clustering and workforce distribution in the transportation cybersecurity ecosystem across the US, presenting a BiTransGCN framework to model visitor flow patterns.


<details>
  <summary>Details</summary>
Motivation: To understand spatiotemporal dynamics of visitor flows and examine how socioeconomic factors shape industry clustering and workforce distribution within evolving sectors related to transportation cybersecurity.

Method: Develops a BiTransGCN framework which integrates an attention-based Transformer architecture with a Graph Convolutional Network backbone for modeling and predicting visitor flow patterns.

Result: Improves the ability to track, interpret, and anticipate changes in industry clustering and mobility trends, providing support for strategic planning in transportation network.

Conclusion: The study provides a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem.

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [413] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

TL;DR: An abstract about a new framework-agnostic method called FedIDA which aims to improve fairness in federated learning for healthcare AI models.


<details>
  <summary>Details</summary>
Motivation: There is a need to ensure fairness in high-stakes domains like healthcare when using AI. Current federated learning methods, while enabling privacy-preserving collaboration across institutions, are still vulnerable to algorithmic bias and subgroup imbalance especially with multiple intersecting sensitive attributes.

Method: Propose FedIDA, a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling to support multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm.

Result: Theoretical analysis shows fairness improvement bounds using Lipschitz continuity and concentration inequalities. Empirical results on benchmark and real-world clinical datasets confirm that FedIDA improves fairness while maintaining competitive predictive performance.

Conclusion: FedIDA demonstrates effectiveness for equitable and privacy-preserving modeling in healthcare.

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [414] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

TL;DR: Since 2022, RLHF technique has been used to fine-tune LLMs, making their outputs more human-like. This convergence of human and machine-written text raises ethical, sociotechnical, and pedagogical concerns. This paper conducts a rhetorical analysis of the procedures reshaped by RLHF-enhanced chatbots, shifting focus from content persuasiveness to underlying persuasive mechanisms, and opens new directions for AI ethics inquiry.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to highlight the severe ethical, sociotechnical, and pedagogical implications brought about by the increasing convergence of human and machine-written text due to the integration of RLHF in LLMs.

Method: The method involves conducting a rhetorical analysis of the central procedures and processes reshaped by RLHF-enhanced generative AI chatbots using Ian Bogost's concept of procedural rhetoric, shifting the site of investigation from content analysis to the underlying mechanisms of persuasion.

Result: The result is a theoretical investigation that uncovers how AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships.

Conclusion: This paper concludes that there is a need for further inquiry in AI ethics considering the implications of procedures rerouted through AI-driven technologies, which will be of interest to educators, researchers, scholars, and users of generative AI chatbots.

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [415] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

TL;DR: 尽管单个查询效率高，但大规模应用导致不成比例的资源消耗。本研究提供了一种标准化、基于实证的方法来衡量LLM部署的可持续性，为AI发展的环境责任和可持续性标准奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各行业的广泛应用，理解其在推理层面的环境足迹变得至关重要。然而，大多数现有研究存在局限性，如排除专有模型、忽视基础设施差异和开销或仅关注训练阶段，而推理对AI环境影响的占比日益增加。因此需要填补这一空白。

Method: 引入了一个新的基础设施感知基准测试框架，用于量化30个最先进的LLM在商业数据中心中的环境足迹。该框架结合了公共API性能数据、区域特定的环境乘数和硬件配置的统计推断，并使用交叉效率的数据包络分析（DEA）方法根据性能与环境成本的比率对模型进行排名。

Result: 结果显示o3和DeepSeek-R1是最耗能的模型，每处理一个长提示消耗超过33瓦时，是GPT-4.1 nano的70多倍；Claude-3.7 Sonnet在生态效率方面排名最高。例如，每天7亿次GPT-4o查询会导致相当于35,000户美国家庭用电量的电力消耗等显著年度环境影响。

Conclusion: 研究揭示了个体查询高效但全球规模应用导致资源过度消耗的矛盾现象，并提供了标准化的实证基准测试方法，以推动未来AI开发中的环境责任和可持续性标准。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


### [416] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: This paper comprehensively reviews the use of AI and GenAI in assessing damage from natural disasters, discussing its strengths, limitations, applications to multimodal data, ethical concerns, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Natural disasters pose significant risks to human lives and infrastructure, necessitating rapid and efficient damage assessment for effective disaster response. The potential of AI and GenAI to revolutionize this process by handling various data types and simulating realistic scenarios drives the motivation for this review.

Method: The authors conducted a comprehensive review of existing literature on the application of AI and GenAI techniques to damage assessment across different types of natural disasters. They analyzed the use of these technologies with multimodal data (text, image, video, audio), identified ethical and security issues, and explored potential misuse threats such as misinformation and adversarial attacks.

Result: The review highlights that AI and GenAI can significantly enhance the speed and accuracy of damage assessment in natural disasters through their ability to process diverse data sources and simulate scenarios. However, challenges remain regarding data privacy, security, and ethical considerations. Misuse of GenAI could lead to misinformation or adversarial attacks.

Conclusion: This work is presented as the first comprehensive survey of GenAI techniques in disaster assessment and response. Future research should focus on developing secure, reliable, and ethical GenAI systems to support disaster management efforts.

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


### [417] [Healthy Distrust in AI systems](https://arxiv.org/abs/2505.09747)
*Benjamin Paaßen,Suzana Alpsancar,Tobias Matzner,Ingrid Scharlau*

Main category: cs.CY

TL;DR: This paper introduces the concept of 'healthy distrust' in AI systems, emphasizing its necessity for building meaningful trust and respecting human autonomy. It investigates prior notions of trust and distrust across multiple disciplines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current AI system designs which focus solely on inspiring trust, without considering the social context that may create tension with personal interests.

Method: The method involves examining previous concepts of trust and distrust from various fields such as computer science, sociology, history, psychology, and philosophy to identify gaps and define 'healthy distrust'.

Result: The result is the conceptualization of 'healthy distrust' as an essential element for AI usage that respects human autonomy, suggesting that distrust can be justified and necessary.

Conclusion: The conclusion is that 'healthy distrust' should be recognized and incorporated into AI design and practice to ensure respect for human autonomy and build meaningful trust.

Abstract: Under the slogan of trustworthy AI, much of contemporary AI research is
focused on designing AI systems and usage practices that inspire human trust
and, thus, enhance adoption of AI systems. However, a person affected by an AI
system may not be convinced by AI system design alone -- neither should they,
if the AI system is embedded in a social context that gives good reason to
believe that it is used in tension with a person's interest. In such cases,
distrust in the system may be justified and necessary to build meaningful trust
in the first place. We propose the term "healthy distrust" to describe such a
justified, careful stance towards certain AI usage practices. We investigate
prior notions of trust and distrust in computer science, sociology, history,
psychology, and philosophy, outline a remaining gap that healthy distrust might
fill and conceptualize healthy distrust as a crucial part for AI usage that
respects human autonomy.

</details>


### [418] [Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?](https://arxiv.org/abs/2505.09868)
*Tin Trung Nguyen,Jiannan Xu,Phuong-Anh Nguyen-Le,Jonathan Lazar,Donald Braman,Hal Daumé III,Zubin Jelveh*

Main category: cs.CY

TL;DR: 尽管“个体公平”标准有其美国宪法基础，但它尚未在州或联邦法规中得到具体化。本文通过人体实验探讨了哪些人口统计特征与个体公平评估再犯风险评估（RRA）工具相关，并得出个体相似性函数应考虑年龄和性别，但不应考虑种族。


<details>
  <summary>Details</summary>
Motivation: 探讨个体公平标准在法律条款中的缺失，并试图明确哪些人口统计学特征对于个体公平评估再犯风险评估工具是相关的。

Method: 进行人体实验来评价不同人口统计特征在个体公平评估再犯风险评估工具中的相关性。

Result: 个体相似性函数应该考虑年龄和性别，但不应考虑种族。

Conclusion: 为了填补个体公平标准在法律条款中的空白，实验结果表明个体相似性函数应考虑年龄和性别，但不应考虑种族。

Abstract: Despite its U.S. constitutional foundation, the technical ``individual
fairness'' criterion has not been operationalized in state or federal
statutes/regulations. We conduct a human subjects experiment to address this
gap, evaluating which demographic features are relevant for individual fairness
evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude
that the individual similarity function should consider age and sex, but it
should ignore race.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [419] [Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems](https://arxiv.org/abs/2505.09734)
*Babak Esmaeili,Nariman Niknejad,Hamidreza Modares*

Main category: eess.SY

TL;DR: This paper proposes a risk-aware safe reinforcement learning control design for stochastic discrete-time linear systems, integrating safety and optimality through a novel interpolation technique.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods that rely on myopic interventions or high-fidelity models in ensuring safety for RL-controlled stochastic systems.

Method: Learning a risk-informed safe controller alongside an RL controller and combining them using a data-driven interpolation technique. Utilizing piecewise affine controllers to expand the invariant set and optimizing over a scalar decision variable to minimize safety violations.

Result: Achieves high-confidence safety without needing a high-fidelity model, avoids undesired equilibria, provides efficient solutions, reduces data requirements, and minimizes variance of safety violations.

Conclusion: The proposed approach successfully integrates safety and optimality in RL control for stochastic systems, validated through a simulation example.

Abstract: This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.

</details>


### [420] [A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024](https://arxiv.org/abs/2505.10367)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Songyuan Liu,Jinming Yu*

Main category: eess.SY

TL;DR: This paper presents team GEB's solution that ranked 3rd in trading, 4th in forecasting, and 1st among student teams in HEFTCom2024. They provide accurate probabilistic forecasts for a wind-solar hybrid system and achieve substantial trading revenue.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of obtaining accurate probabilistic energy forecasts and making effective decisions amid diverse uncertainties in future energy systems.

Method: Key components include: (1) a stacking-based approach combining sister forecasts from various NWPs to provide wind power forecasts; (2) an online solar post-processing model to address the distribution shift in the online test set caused by increased solar capacity; (3) a probabilistic aggregation method for accurate quantile forecasts of hybrid generation; (4) a stochastic trading strategy to maximize expected trading revenue considering uncertainties in electricity prices.

Result: Substantial trading revenue in the day-ahead electricity market was achieved.

Conclusion: The proposed methods were validated through detailed case studies, and code is available for reproduction and further research.

Abstract: Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [421] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

TL;DR: CellTypeAgent, a trustworthy large language model (LLM)-agent that integrates LLMs with verification from relevant databases for cell type annotation in single-cell RNA sequencing analysis, achieves higher accuracy than existing methods while mitigating hallucinations.


<details>
  <summary>Details</summary>
Motivation: Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis.

Method: Presented is CellTypeAgent, which integrates large language models (LLMs) with verification from relevant databases.

Result: Evaluated across nine real datasets involving 303 cell types from 36 tissues, CellTypeAgent achieves higher accuracy than existing methods while reducing hallucinations.

Conclusion: The combined approach of CellTypeAgent holds promise for more efficient and reliable cell type annotation.

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [422] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

TL;DR: 研究人员应用字节对编码（BPE）技术于九个端粒到端粒（T2T）灵长类动物基因组，包括三个完整人类基因组。他们发现仅有少量的token在所有基因组中共享，并且基于token重叠构建的系统发育树不能准确反映已知的灵长类动物关系。这归因于物种特异性高拷贝重复元素的影响。研究强调了BPE技术在压缩重复序列方面的有效性，但同时指出其作为比较基因组学通用工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管T2T基因组组装的出现为比较基因组学提供了新途径，但基因组序列的有效标记化策略仍需探索。

Method: 使用自定义工具dnaBPE，训练独立的BPE标记器对九个T2T灵长类动物基因组进行分析，每个标记器具有512,000个固定词汇量。

Result: 只有11,569个token在所有基因组组装中共享，而约991,854个token仅在一个基因组中独有。基于token重叠构建的系统发育树未能重现已知的灵长类动物关系。

Conclusion: BPE标记化技术能有效压缩重复序列，但受高拷贝重复元素影响，不适合作为比较基因组学的通用工具。需要采用混合策略和重复屏蔽方法来改进基因组标记化，开发大规模基因组语言模型时应考虑领域特定的适应性。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [423] [LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting](https://arxiv.org/abs/2505.10191)
*Qingyu Zheng,Qi Shao,Guijun Han,Wei Li,Hong Li,Xuan Wang*

Main category: physics.ao-ph

TL;DR: LanTu, a dynamics-enhanced deep learning regional eddy-resolving ocean forecasting system incorporating cross-scale interactions and multiscale physical constraints, outperforms existing operational numerical and AI-based systems in predicting temperature, salinity, sea level anomaly, and currents with a lead time of over 10 days.


<details>
  <summary>Details</summary>
Motivation: Eddy-resolving ocean forecasting is crucial for fisheries and navigational safety but presents scientific challenges and high computational costs. AI-based systems offer a balance between forecast performance and efficiency, yet still face challenges in mesoscale eddy forecasting.

Method: Developed LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. Incorporated cross-scale interactions and constructed multiscale physical constraints guided by eddy dynamics knowledge to improve mesoscale evolution forecasting skill.

Result: LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly, and current prediction with a lead time of more than 10 days.

Conclusion: Dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting.

Abstract: Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [424] [On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion](https://arxiv.org/abs/2505.09766)
*Roberto Ponciroli*

Main category: math.NA

TL;DR: This paper presents a methodology for reconstructing the spatial distribution of neutron flux in a nuclear reactor using real-time ex-core detector measurements and solving the Kirchhoff-Helmholtz equation as an inverse problem to approximate the Green's function.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable method for reconstructing the spatial distribution of neutron flux within a nuclear reactor, leveraging real-time boundary data from ex-core detectors.

Method: Utilize the Kirchhoff-Helmholtz equation to define the problem, derive the Green's function from the one-speed neutron diffusion model, establish symmetry properties, and demonstrate the procedure for sensor data interpretation and flux reconstruction.

Result: The existence and uniqueness of the Green's function inferred from sampled data are demonstrated, ensuring the reliability of the proposed neutron flux reconstruction method.

Conclusion: The data-driven approximation of the Green's function is well-posed, providing a reliable framework for neutron flux reconstruction in nuclear reactors.

Abstract: This work presents a methodology for reconstructing the spatial distribution
of the neutron flux in a nuclear reactor, leveraging real-time measurements
obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation
inherently defines the problem of estimating a scalar field within a domain
based on boundary data, making it a natural mathematical framework for this
task. The main challenge lies in deriving the Green's function specific to the
domain and the neutron diffusion process. While analytical solutions for
Green's functions exist for simplified geometries, their derivation of complex,
heterogeneous domains-such as a nuclear reactor-requires a numerical approach.
The objective of this work is to demonstrate the well-posedness of the
data-driven Green's function approximation by formulating and solving the K-H
equation as an inverse problem. After establishing the symmetry properties that
the Green's function must satisfy, the K-H equation is derived from the
one-speed neutron diffusion model. This is followed by a comprehensive
description of the procedure for interpreting sensor readings and implementing
the neutron flux reconstruction algorithm. Finally, the existence and
uniqueness of the Green's function inferred from the sampled data are
demonstrated, ensuring the reliability of the proposed method and its
predictions.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [425] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

TL;DR: An abstract about developing a weakly-supervised textual classification system called Ornithologist that measures the hawkishness and dovishness of central bank text, using taxonomy-guided reasoning to guide a large language model with human-authored decision trees.


<details>
  <summary>Details</summary>
Motivation: To create a more transparent, explainable, and accessible system for measuring the hawkishness and dovishness of central bank text that requires less supervision than traditional classification systems and can be applied to other problems or sources of text (e.g. news) without much modification.

Method: Developing Ornithologist, a weakly-supervised textual classification system that uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees.

Result: Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.

Conclusion: This system is not only more transparent and explainable but also reduces hallucination risk and can be easily applied to other problems or sources of text.

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [426] [A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds](https://arxiv.org/abs/2505.10201)
*Victor Lagerkvist,Mohamed Maizia,Johannes Schmidt*

Main category: cs.CC

TL;DR: The paper explores the complexity of non-monotonic reasoning, specifically abductive reasoning, by analyzing intractable abduction problems with respect to the number of variables in the knowledge base. It presents positive results for certain problem fragments and shows improvements over exhaustive search for a $\Sigma^P_2$-complete problem.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between monotonic and non-monotonic reasoning, especially focusing on abductive reasoning which lacks detailed complexity analysis beyond classic theory.

Method: Analyze the complexity of intractable abduction problems using the parameter n (number of variables in the knowledge base) and provide both positive results and lower bounds for various problem fragments.

Result: Achieved several positive results for different complexity classes including $\Sigma^P_2$, NP, and coNP-complete fragments. Demonstrated an improvement over exhaustive search for a $\Sigma^P_2$-complete problem. Provided lower bounds under the strong exponential-time hypothesis.

Conclusion: This work marks a significant step in understanding the complexity of non-monotonic reasoning, providing both advancements and limitations in solving abductive reasoning problems.

Abstract: The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [427] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

TL;DR: Open-source AI libraries are crucial for modern AI systems but pose significant risks. LibVulnWatch is a graph-based framework that performs deep evaluations of these libraries, generating reproducible scores across five critical domains and publishing them to a public leaderboard.


<details>
  <summary>Details</summary>
Motivation: Open-source AI libraries pose significant risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance.

Method: LibVulnWatch uses a graph-based agentic assessment framework built on LangGraph. It coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases.

Result: Applied to 20 widely used libraries, LibVulnWatch covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library, including critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability.

Conclusion: LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [428] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

TL;DR: This paper surveys the security and privacy issues in Internet of Agents (IoA) systems, which are composed of autonomous AI agents. It outlines the architecture of IoA, its unique vulnerabilities in aspects like identity authentication and cross-agent trust, reviews defense mechanisms, and points out open research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to provide a comprehensive examination of the security and privacy landscape in IoA systems as these systems become more prevalent and important for coordination among different AI agents.

Method: The method involves reviewing literature on IoA architecture and its vulnerabilities, analyzing existing and emerging defense mechanisms against identified threats, and identifying gaps and challenges in current research.

Result: The result is an overview of the state-of-the-art in securing IoA systems, including insights into the specific threats such as identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. The review also highlights persistent challenges in creating secure and private IoA ecosystems.

Conclusion: The conclusion emphasizes the need for further research in developing resilient and privacy-preserving IoA systems, suggesting several open research directions.

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [429] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

TL;DR: This paper proposes MixBridge, a novel diffusion Schrödinger bridge framework for arbitrary input distributions, and addresses the challenges of implanting multiple heterogeneous backdoor triggers using a Divide-and-Merge strategy.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor formulations are mainly designed for single-attack scenarios and limited to Gaussian noise input models. This paper aims to address these limitations by proposing a framework that can cater to complex and arbitrary input distributions.

Method: The proposed method is called MixBridge, a diffusion Schrödinger bridge (DSB) framework. It allows for the implantation of multiple heterogeneous backdoor triggers by directly training with poisoned image pairs. To handle the performance conflict across backdoor tasks, a Divide-and-Merge strategy is proposed where models are independently pre-trained and then integrated into a unified model. Additionally, a Weight Reallocation Scheme (WRS) is designed to enhance the stealthiness of MixBridge.

Result: Empirical studies across diverse generation tasks demonstrate the efficacy of MixBridge in handling arbitrary input distributions and successfully implanting multiple backdoor triggers while maintaining performance.

Conclusion: MixBridge provides a flexible tool to study backdoor behavior for bridge models and effectively addresses the challenge of training multiple backdoor triggers in a single DSB model through the Divide-and-Merge strategy.

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [430] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

TL;DR: 本研究提出了一种强大且可扩展的框架，用于检测高速公路VANET环境中的DDoS攻击，使用XGBoost和CatBoost实现了96%的F1分数，展示了其在保障紧急通信方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 车辆自组织网络（VANET）在智能交通系统中起着关键作用，特别是在为应急车辆提供实时通信方面。然而，分布式拒绝服务（DDoS）攻击会干扰安全关键的通信信道，严重影响其可靠性，因此需要一种强大的方法来检测这些攻击。

Method: 构建了一个合成数据集，使用NS-3网络模拟器与城市交通模拟（SUMO）结合，并通过OpenStreetMap提取了德国A81高速公路的实际移动轨迹。模拟了三种流量类别：DDoS、VoIP和基于TCP的视频流（VideoTCP）。数据预处理管道包括归一化、信噪比（SNR）特征工程、缺失值填补以及使用合成少数类过采样技术（SMOTE）进行类别平衡。使用SHapley Additive exPlanations (SHAP)评估特征重要性。基准测试了11个分类器，其中包括XGBoost、CatBoost、AdaBoost、GradientBoosting和人工神经网络（ANN）。

Result: XGBoost和CatBoost表现最佳，分别达到了96%的F1分数。

Conclusion: 该框架展示了其在VANET中实时部署的鲁棒性和潜力，以确保关键的应急通信。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [431] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: The paper reviews the advancements and challenges in Federated Large Language Models (FLLM), focusing on feasibility, robustness, security, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of the latest advancements in FLLM and identify key challenges and future research directions.

Method: Conducting an exhaustive survey of existing studies on FLLM feasibility, introducing methods to enhance robustness, analyzing privacy and security risks, reviewing defense mechanisms, and exploring promising future research areas.

Result: Analyzed the challenges from four critical perspectives (feasibility, robustness, security, and future directions) and highlighted the need for further research in enhancing system robustness and security.

Conclusion: FLLM faces significant challenges but holds great potential. Future research should focus on improving robustness, security, and addressing unique integration challenges.

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [432] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

TL;DR: This paper explores the vulnerabilities of AI-based automated checkout systems in unmanned stores through adversarial patch attacks, evaluates their effectiveness digitally and physically, proposes a new color histogram similarity loss function, and suggests defense strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expose and address the security vulnerabilities in AI-based automated checkout systems used in unmanned stores, which can be exploited by adversarial patch attacks leading to significant issues such as theft and inventory discrepancies.

Method: The method involves investigating three types of adversarial patch attacks (Hiding, Creating, and Altering), introducing a novel color histogram similarity loss function, proposing a new bounding-boxes-based metric for analysis, evaluating these attacks both digitally on object detection models and physically in a simulated unmanned store environment, and assessing the robustness of these attacks in black-box scenarios.

Result: The results show that adversarial patches can severely disrupt object detection models in unmanned stores, with high success rates even in physical environments and black-box scenarios. The proposed color histogram similarity loss function and bounding-boxes-based metric provide new ways to evaluate attack impacts.

Conclusion: The study concludes by emphasizing the need for robust defense mechanisms to protect unmanned stores from adversarial threats, highlighting current limitations in real-time detection systems, and suggesting proactive measures to improve model robustness.

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [433] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

TL;DR: The paper explores the trade-offs of model performance, privacy and explainability when using Differential Privacy to counteract model extraction attacks in MLaaS platforms that incorporate explainable AI.


<details>
  <summary>Details</summary>
Motivation: MLaaS platforms are becoming increasingly important for deploying predictive models. However, these platforms face security and privacy challenges such as model extraction attacks which can be facilitated by explainable AI's counterfactual explanations.

Method: Investigates two distinct Differential Privacy strategies - one implemented during classification model training and another at the explainer during counterfactual generation - to evaluate their impact on model performance, privacy, and explainability.

Result: Not explicitly stated in the abstract but expected to provide insights into how each DP strategy affects the balance among model performance, privacy, and explainability.

Conclusion: Differential Privacy shows promise in mitigating counterfactual-facilitated model extraction attacks while managing the trade-offs among model performance, privacy, and explainability.

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [434] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本研究提出了新的隐私保护对齐算法，结合DPO和RLHF技术，在适度的隐私预算下实现最先进的性能。DP-AdamW算法显著提高了对齐质量（最高达15%），并探讨了隐私、效能与计算需求之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的对齐过程虽重要，但涉及敏感用户数据，引发隐私问题。尽管已有工作将差分隐私（DP）与对齐技术结合，但其效果仍有限，需要改进。

Method: 提出新的隐私保护对齐算法，适用于直接偏好优化（DPO）和基于人类反馈的强化学习（RLHF）两种技术。通过系统实验评估这些算法在不同隐私预算下的表现，重点分析DP-AdamW算法的效果。

Result: 所提方法在适度隐私预算下达到最佳性能，其中DP-AdamW与DPO结合可使对齐质量提高多达15%。同时明确了隐私保障、对齐效果和计算需求之间的关系。

Conclusion: 新算法在隐私保护前提下有效提升了语言模型的对齐质量，并为优化隐私与效能间的权衡提供了实用指导。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [435] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 研究人员通过假设检验与辅助信息的视角，分析了一次性文本水印中的基本权衡，并提出了优化的水印设计策略，同时提供了理论和实际效果的对比。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的文本几乎与人类生成的内容无法区分，因此需要一种方法在不显著改变输出质量的情况下检测出这些文本是由LLM生成的。

Method: 通过假设检验和辅助信息的视角，研究者们构建了一个框架，用以分析文本水印中检测能力和生成文本质量之间的权衡。他们提出了一种关键的设计组件，即在提供给水印探测器的辅助信息和LLM词汇表的随机划分之间创建耦合。此外，还确定了在最坏情况下满足最小熵约束的最优耦合和随机化策略。

Result: 研究者给出了在提出的方案下检测率的闭式表达式，并以最大最小的方式量化了成本。数值结果表明，该方案在合成数据和LLM水印上均接近理论最优，并优于现有方案。

Conclusion: 本文通过深入分析和实验验证，提供了一种有效的文本水印设计方案，能够在保持文本生成质量的同时提高水印检测能力。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [436] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

TL;DR: The paper introduces TokenProber, a method for sensitivity-aware differential testing to evaluate the robustness of refusal mechanisms in T2I models by generating adversarial prompts. It distinguishes between dirty words and discrepant words to create prompts that can bypass safety filters while maintaining NSFW content generation. The evaluation shows its superior effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of subtly modifying prompts to preserve sensitive nature while bypassing refusal mechanisms in T2I models.

Method: TokenProber conducts fine-grained analysis of specific words within prompts, distinguishing between dirty words essential for NSFW content and discrepant words showing different sensitivity assessments. Through sensitivity-aware mutation, it generates adversarial prompts balancing NSFW content generation and evading detection.

Result: TokenProber demonstrates superior effectiveness in bypassing safety filters compared to existing methods, with a 54%+ increase on average when evaluated against 5 safety checkers on 3 popular T2I models using 324 NSFW prompts.

Conclusion: TokenProber is effective in uncovering robustness issues in existing refusal mechanisms of T2I models.

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [437] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

TL;DR: The paper introduces a self-supervised contrastive learning method using transformer encoders for intrusion detection, which performs better than traditional NetFlow methods in both intra-dataset and inter-dataset evaluations.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for innovative Intrusion Detection Systems (IDS) due to the increase in zero-day attacks. Traditional machine learning-based IDS face challenges in generalization when encountering unseen traffic patterns.

Method: A novel self-supervised contrastive learning approach based on transformer encoders is proposed. It uses packet-level data augmentation and a transformer-based architecture to automatically learn comprehensive packet sequence representations without relying on handcrafted statistical features.

Result: The transformer-based framework outperforms existing NetFlow self-supervised methods with up to 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. It also shows adaptability when fine-tuned across different datasets.

Conclusion: This approach provides a strong baseline for supervised intrusion detection with limited labeled data and demonstrates strong performance even when lacking benign data from the target domain.

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [438] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

TL;DR: The paper proposes a security policy management framework using reinforcement learning (RL) to adapt dynamically in cloud environments, significantly outperforming static policies.


<details>
  <summary>Details</summary>
Motivation: Static security policies have become inadequate due to the complex and dynamic nature of cloud environments, such as Amazon Web Services (AWS).

Method: The proposed method employs deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, leveraging cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies.

Result: Experimental results demonstrate that the adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and reducing incident detection and response times by 58%.

Conclusion: The findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [439] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

TL;DR: The paper presents two contributions, Prioritized Binary Rounding and the sigma-binary attack, to improve adversarial attacks in binary-constrained domains for Android malware detection. Experiments show that current defenses are highly vulnerable.


<details>
  <summary>Details</summary>
Motivation: Current machine learning-based Android malware detectors are vulnerable to evasion attacks, especially due to lack of comprehensive evaluation frameworks in binary-constrained domains.

Method: Introduced Prioritized Binary Rounding for converting continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Also introduced sigma-binary attack which achieves attack goals with minimal feature changes.

Result: The sigma-binary attack outperforms existing methods by achieving over 90% attack success rate using fewer than 10 feature modifications, reaching 100% with just 20. Even strong defenses like PAD-SMA have a 94.56% success rate under unrestricted perturbations.

Conclusion: The findings emphasize the need for precise adversarial methods like sigma-binary to expose vulnerabilities in existing defenses and support development of more robust malware detection systems.

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [440] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: In this paper, the authors propose SafeFL, a new algorithm that accurately identifies malicious clients in federated learning by generating a synthetic dataset from a series of global models. SafeFL shows better performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of current Byzantine-robust aggregation rules and FL detection techniques in identifying malicious clients within the federated learning framework, particularly against advanced threats.

Method: SafeFL involves the server collecting a series of global models to generate a synthetic dataset, which is used to distinguish between malicious and benign models based on their behavior.

Result: Extensive testing demonstrates that SafeFL outperforms existing methods in terms of efficiency and accuracy for detecting malicious clients.

Conclusion: SafeFL is an effective solution for accurately identifying malicious clients in federated learning.

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [441] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

TL;DR: This paper proposes a new method for identifying sybil addresses in blockchain ecosystems using subgraph feature extraction and lightGBM, which performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Sybil attacks are a significant security threat to blockchain ecosystems, especially in token airdrop events.

Method: The method constructs a two-layer deep transaction subgraph for each address, extracts key event operation features according to the lifecycle of sybil addresses, and also extracts amount and network structure features.

Result: Experiments on a dataset with 193,701 addresses show that this method outperforms existing approaches in precision, recall, F1 score, and AUC, with all metrics exceeding 0.9.

Conclusion: This study's methods and results can be applied to broader blockchain security areas like transaction manipulation identification and token liquidity risk assessment, promoting a more secure and fair blockchain ecosystem.

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


### [442] [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
*Yidan Wang,Yanan Cao,Yubing Ren,Fang Fang,Zheng Lin,Binxing Fang*

Main category: cs.CR

TL;DR: This paper explores the use of jailbreak attacks for extracting sensitive information from Large Language Models (LLMs) and introduces PIG, a novel framework targeting Personally Identifiable Information (PII). Experiments on multiple LLMs demonstrate PIG's superior performance compared to existing methods, highlighting significant privacy risks in LLMs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of jailbreak attacks in extracting sensitive information from LLMs and address the limitations of current methods for evaluating privacy leakage.

Method: The proposed framework, PIG, identifies PII entities and their types in privacy queries, builds a privacy context using in-context learning, and iteratively updates it with three gradient-based strategies to elicit target PII.

Result: PIG outperforms baseline methods when evaluated on four white-box and two black-box LLMs using two privacy-related datasets, achieving state-of-the-art results.

Conclusion: The findings reveal significant privacy risks in LLMs and stress the importance of enhancing safeguards.

Abstract: Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.

</details>


### [443] [Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data](https://arxiv.org/abs/2505.09974)
*Adel ElZemity,Budi Arief,Shujun Li*

Main category: cs.CR

TL;DR: The integration of LLMs in cyber security applications has opportunities and risks. Fine-tuning reduces safety resilience across tested LLMs. A safety alignment approach rewords instruction-response pairs to include safety precautions and ethical considerations, maintaining or improving model safety while preserving technical utility.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the safety risks in fine-tuned LLMs for cyber security applications and propose a method to maintain or improve model safety while preserving technical utility.

Method: Using the OWASP Top 10 for LLM Applications framework, seven open-source LLMs were assessed. A safety alignment approach was proposed and evaluated, which rewords instruction-response pairs to include explicit safety precautions and ethical considerations.

Result: Fine-tuning reduces safety resilience across all tested LLMs. The proposed safety alignment approach can maintain or even improve model safety while preserving technical utility.

Conclusion: This work provides a systematic evaluation of safety risks in LLMs, facilitating safer adoption of generative AI in sensitive domains and contributing to the development of secure, trustworthy, and ethically aligned LLMs.

Abstract: The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.

</details>


### [444] [AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons](https://arxiv.org/abs/2505.10273)
*Hexu Li,Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: Vehicle platooning via V2X communication improves fuel efficiency and road use but is susceptible to insider attacks. This paper introduces AttentionGuard, a transformer-based framework using self-attention to detect anomalies in mobility data, achieving up to 0.95 F1-score with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Vehicle platooning systems are vulnerable to falsification attacks by authenticated insiders, which can destabilize formations and cause collisions. There is a need for effective misbehavior detection mechanisms to ensure safety and stability.

Method: AttentionGuard leverages a multi-head transformer-encoder and the self-attention mechanism to process sequential kinematic information from vehicles. It differentiates between normal patterns and falsification attacks across various platooning scenarios including steady-state operation, join, and exit maneuvers.

Result: AttentionGuard achieves an F1-score of up to 0.95 in detecting attacks within diverse attack vectors and operational parameters. The system maintains robust performance during complex maneuvers and operates with minimal latency (100ms decision intervals).

Conclusion: AttentionGuard demonstrates superior detection capabilities for securing Cooperative Intelligent Transport Systems (C-ITS) against insider threats, making it a promising solution for real-time transportation safety applications.

Abstract: Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.

</details>


### [445] [Private Transformer Inference in MLaaS: A Survey](https://arxiv.org/abs/2505.10315)
*Yang Li,Xinyu Zhou,Yitong Wang,Liangxin Qian,Jun Zhao*

Main category: cs.CR

TL;DR: Transformer models, despite their success in AI applications, pose privacy risks when deployed in MLaaS due to centralized data processing. Private Transformer Inference (PTI) mitigates these risks through cryptographic techniques like secure multi-party computation and homomorphic encryption, allowing for private inference. This paper reviews PTI advancements, provides a taxonomy, and evaluates solutions focusing on balancing resource efficiency with privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the increasing privacy concerns associated with deploying transformer models in MLaaS environments, where sensitive user data is processed centrally.

Method: The paper reviews recent advancements in Private Transformer Inference (PTI), which uses cryptographic techniques such as secure multi-party computation and homomorphic encryption to enable private inference. It also introduces a structured taxonomy and evaluation framework to assess PTI solutions.

Result: The paper highlights state-of-the-art PTI solutions and challenges, providing insights into balancing resource efficiency with privacy.

Conclusion: Private Transformer Inference offers a promising approach to address privacy concerns in transformer model deployments, and the introduced taxonomy and evaluation framework will aid in advancing PTI solutions.

Abstract: Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.

</details>


### [446] [AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents](https://arxiv.org/abs/2505.10321)
*Julius Henke*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.

</details>


### [447] [Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts](https://arxiv.org/abs/2505.09843)
*Melissa Turcotte,François Labrèche,Serge-Olivier Paquette*

Main category: cs.CR

TL;DR: AACT系统通过学习分析师对网络安全警报的分类操作，自动化SOC工作流程，显著减少展示给分析师的警报数量，同时保持低误报率。


<details>
  <summary>Details</summary>
Motivation: 企业网络规模日益扩大，安全警报数量激增，导致SOC分析师面临严重的警报疲劳问题。此外，在管理SOC服务中，上下文切换和业务流程可见性不足进一步加剧了这一问题。

Method: 引入名为AACT的新系统，该系统通过分析分析师对网络安全警报的分类行为，实时预测分类决策，从而实现自动关闭良性警报并优先处理关键警报。

Result: 在真实SOC数据和公开数据集上进行训练和评估后，系统表现出高性能，能有效区分恶意和良性警报。在实际应用中，AACT系统在六个月内减少了61%展示给分析师的警报，同时误报率仅为1.36%。

Conclusion: AACT系统成功减少了SOC分析师的工作负担，提高了工作效率，并且具有高准确性和低误报率。

Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack
surface, increasing the volume of security alerts generated from security
controls. Security Operations Centre (SOC) analysts triage these alerts to
identify malicious activity, but they struggle with alert fatigue due to the
overwhelming number of benign alerts. Organisations are turning to managed SOC
providers, where the problem is amplified by context switching and limited
visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by
learning from analysts' triage actions on cybersecurity alerts. It accurately
predicts triage decisions in real time, allowing benign alerts to be closed
automatically and critical ones prioritised. This reduces the SOC queue
allowing analysts to focus on the most severe, relevant or ambiguous threats.
The system has been trained and evaluated on both real SOC data and an open
dataset, obtaining high performance in identifying malicious alerts from benign
alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC
environment, reducing alerts shown to analysts by 61% over six months, with a
low false negative rate of 1.36% over millions of alerts.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [448] [Deconstructing Subset Construction -- Reducing While Determinizing](https://arxiv.org/abs/2505.10319)
*John Nicol,Markus Frohme*

Main category: cs.FL

TL;DR: The paper presents a new method for NFA canonization using intermediate minimization steps and equivalence registries, which can be integrated into existing approaches, showing improvements in worst-case scenarios.


<details>
  <summary>Details</summary>
Motivation: To reduce the exploration space during NFA canonization and improve performance, especially in worst-case scenarios.

Method: Introducing intermediate minimization steps and utilizing equivalence registries to manage equivalent states, allowing for additional optimizations like convexity closures or simulation.

Result: Evaluation on real-world examples shows improvement in worst-case scenarios for NFA canonization.

Conclusion: The novel approach is effective and can be embedded in classic methods, with an open-source implementation available for experimentation.

Abstract: We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [449] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

TL;DR: 在肿瘤学中，利用少量标注样本进行上下文学习（ICL）是一种不需要重新训练模型即可适应新诊断任务的方法。本研究首次比较了多个视觉-语言模型在不同肿瘤分类任务上的表现，结果表明ICL具有接近特定任务行为的潜力，尤其适用于稀有癌症和资源有限的情境。


<details>
  <summary>Details</summary>
Motivation: 当前AI在肿瘤学中的应用受限于对大型标注数据集的依赖以及针对特定领域诊断任务需要重新训练模型的需求。为解决这些限制，研究探索了上下文学习作为无需重新训练模型的实用替代方案。

Method: 研究使用四个视觉-语言模型（Paligemma、CLIP、ALIGN和GPT-4o），通过仅使用少量标注样本来评估其在三个肿瘤学数据集（MHIST、PatchCamelyon和HAM10000）上的性能，而无需更新模型参数。

Result: 所有模型在少量样本提示下表现出显著提升，其中GPT-4o在二分类任务中达到F1分数0.81，在多分类任务中达到0.60。尽管这些结果低于完全微调系统的水平，但开源模型如Paligemma和CLIP也展示了竞争力。

Conclusion: 上下文学习在肿瘤学中展现出作为实际解决方案的潜力，特别是在稀有癌症和资源受限的情况下，可以利用少量例子来近似特定任务的行为。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [450] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

TL;DR: The paper presents a unified framework for multi-organ and multilingual ultrasound report generation, which integrates fragment-based multilingual training, curates a bilingual dataset, and leverages standardized US reports.


<details>
  <summary>Details</summary>
Motivation: Ultrasound report generation is challenging due to variability in images, operator dependence, and lack of consistent datasets. Current methods have limitations in generating accurate and standardized reports across different organs and languages.

Method: A unified framework is proposed that includes fragment-based multilingual training, alignment of modular text fragments with imaging data, and curation of a bilingual English-Chinese dataset. Fine-tuning with selective unfreezing of the vision transformer (ViT) is used to improve text-image alignment.

Result: Compared to the previous state-of-the-art KMVE method, the approach achieves relative gains of about 2% in BLEU scores, approximately 3% in ROUGE-L, and about 15% in CIDEr, while significantly reducing errors such as missing or incorrect content.

Conclusion: This work demonstrates strong potential for real-world clinical workflows by unifying multi-organ and multi-language report generation into a single, scalable framework.

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [451] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

TL;DR: Deep learning models for cervical cancer classification often lack reliable uncertainty estimation. Conformal prediction can address this issue, but current methods still have limitations in aligning with human expectations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are overconfident and cannot reliably reflect diagnostic uncertainty. The conventional evaluation of conformal prediction primarily focuses on whether the prediction set includes the true class, ignoring extraneous classes.

Method: Using expert annotation sets collected from multiple annotators to comprehensively validate conformal prediction sets, evaluating three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification.

Result: Conventional coverage-based evaluations overestimate performance, and current conformal prediction methods often produce prediction sets that do not align well with human labels. Additionally, conformal prediction methods show some capabilities in identifying ambiguous and out-of-distribution data.

Conclusion: Conformal prediction shows promise in addressing uncertainty issues in deep learning models for cervical cancer classification, but improvements are needed to better align prediction sets with human expectations.

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [452] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

TL;DR: The paper introduces a new mask pattern for the SparK model, named Mesh Mask-ed SparK model, and investigates the impact of mask patterns on pre-training performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of the SparK model by introducing a new mask pattern.

Method: The method involves proposing a new mask pattern for the SparK model, resulting in the Mesh Mask-ed SparK model.

Result: The effect of the mask pattern on the performance during pre-training is reported.

Conclusion: A new mask pattern for the SparK model has been proposed and its influence on performance has been demonstrated.

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [453] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/abs/2505.08843)
*Marco Corrias,Giada Franceschi,Michele Riva,Alberto Tampieri,Karin Föttinger,Ulrike Diebold,Thomas Pock,Cesare Franchini*

Main category: eess.IV

TL;DR: 实验获取的显微图像不可避免地受到噪声和其他不想要信号的影响，这会降低它们的质量并可能隐藏相关特征。本研究通过基于全变差（TV）的工作流程对显微图像进行分解和降噪，处理了包括原子力显微镜（AFM）、扫描隧道显微镜（STM）和扫描电子显微镜（SEM）在内的各种显微技术获得的图像。


<details>
  <summary>Details</summary>
Motivation: 实验获取的显微图像受噪声和其他不想要信号影响，需要现代降噪和恢复解决方案。

Method: 通过基于全变差（TV）的工作流程对显微图像进行分解和降噪，评估TV-$L^1$、Huber-ROF和TGV-$L^1$在不同研究案例中的性能。

Result: Huber-ROF是最灵活的方法，而TGV-$L^1$最适合降噪。该方法在显微镜中有更广泛的应用潜力。

Conclusion: Python代码公开可用，并可集成到实验工作流程中用于图像获取或降噪已获取的图像。

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


### [454] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/abs/2505.09193)
*Wei Jiang,Junru Li,Kai Zhang,Li Zhang*

Main category: eess.IV

TL;DR: The paper introduces BiECVC, a bidirectional video compression framework that incorporates local and non-local context modeling with adaptive gating, achieving state-of-the-art performance surpassing VTM 13.2 RA.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing bidirectional video compression (BVC) methods which primarily exploit temporal motion while neglecting non-local correlations across frames and lack adaptability to suppress harmful contexts from fast motion or occlusion.

Method: BiECVC reuses high-quality features from lower layers for local context enhancement, adopts a linear attention mechanism for efficient non-local dependency modeling, and introduces Bidirectional Context Gating to dynamically filter contextual information based on conditional coding results.

Result: BiECVC reduces bit-rate by 13.4% and 15.7% compared to VTM 13.2 under Random Access configuration with intra periods of 32 and 64 respectively, making it the first learned video codec to surpass VTM 13.2 RA across all standard test datasets.

Conclusion: BiECVC achieves state-of-the-art performance in bidirectional video compression by effectively modeling local and non-local contexts and adaptively gating contextual information.

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [455] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/abs/2505.09323)
*Pengli Zhu,Yingji Fu,Nanguang Chen,Anqi Qiu*

Main category: eess.IV

TL;DR: This study proposes Q-CATN, a novel network for MS-HARDI synthesis from flexible q-space sampling using structural MRI data. It uses collaborative attention and task-specific constraints to preserve anatomical fidelity in DWI, outperforming existing methods on the HCP dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a method for multi-shell, high-angular resolution DWI synthesis that can work with flexible q-space sampling while preserving anatomical fidelity, improving upon current methods that require fixed sampling schemes.

Method: Q-CATN employs a collaborative attention mechanism to extract complementary information from multiple modalities and adjusts its internal representations based on flexible q-space information. Task-specific constraints are introduced to preserve anatomical fidelity in DWI.

Result: Extensive experiments on the HCP dataset show that Q-CATN outperforms existing methods (1D-qDL, 2D-qDL, MESC-SD, QGAN) in estimating parameter maps and fiber tracts both quantitatively and qualitatively while preserving fine-grained details.

Conclusion: Q-CATN's ability to accommodate flexible q-space sampling makes it a promising toolkit for clinical and research applications.

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [456] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/abs/2505.09334)
*Sadman Sakib Alif,Nasim Anzum Promise,Fiaz Al Abid,Aniqua Nusrat Zereen*

Main category: eess.IV

TL;DR: Lung cancer is a major cause of cancer-related deaths. Deep learning models, while effective, are resource-intensive and lack transparency. Knowledge distillation and explainable AI (XAI) can address these issues. We propose DCSNet, a lightweight student model for lung cancer detection, using ResNet50 as the teacher model among eight evaluated CNNs.


<details>
  <summary>Details</summary>
Motivation: Deep learning models like CNNs have revolutionized medical image analysis but face challenges such as computational expense and lack of transparency which hinder their adoption in healthcare, especially in resource-constrained environments.

Method: The paper evaluates eight CNNs as potential teacher models and uses knowledge distillation to develop a lightweight student model called Distilled Custom Student Network (DCSNet). ResNet50 was selected as the teacher model. The method also incorporates XAI techniques to enhance the transparency of the model.

Result: DCSNet ensures high diagnostic performance even in resource-constrained settings and addresses transparency concerns through the use of XAI techniques.

Conclusion: This approach facilitates the broader adoption of AI-driven diagnostic tools in healthcare by making them more efficient and trustworthy.

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [457] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/abs/2505.09521)
*Dongyi He,Shiyang Li,Bin Jiang,He Yan*

Main category: eess.IV

TL;DR: Spec2VolCAMU-Net是一种轻量级的频谱图到体积生成器，通过多方向时频卷积注意力编码器和Vision-Mamba U-Net解码器，实现了从EEG生成类似fMRI的脑部活动数据，显著提高了效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率功能性磁共振成像（fMRI）对于绘制人类大脑活动至关重要，但成本高昂且操作复杂。如果能直接从广泛使用的头皮脑电图（EEG）生成可比的体积数据，先进的神经成像将变得更加普及。然而，现有的方法要么无法捕捉跨通道的时间频率线索，要么依赖于内存密集和不稳定的变压器/ GAN解码器。

Method: 提出了Spec2VolCAMU-Net，一个轻量级的频谱图到体积生成器。它包括一个多方向时频卷积注意力编码器，该编码器堆叠了时间、光谱和联合卷积与自注意力机制，并使用Vision-Mamba U-Net解码器，其线性时间状态空间块能够有效进行长距离空间建模。整个模型通过混合SSI-MSE损失函数进行端到端训练。

Result: 在三个公开基准测试中，Spec2VolCAMU-Net达到了最先进的保真度：NODDI上的SSIM为0.693，Oddball上为0.725，CN-EPFL上为0.788，分别比之前的最佳SSIM分数提高了14.5%，14.9%和16.9%。此外，在PSNR评分方面表现也具有竞争力，尤其是在CN-EPFL数据集上，比之前的最佳PSNR提高了4.6%。

Conclusion: Spec2VolCAMU-Net不仅在重建质量上取得了更好的平衡，而且由于其轻量级和高效的特点，非常适合用于临床和研究环境中的实时应用。

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [458] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

TL;DR: The paper presents a novel SVR method using implicit neural representations for fast and accurate MRI reconstruction, especially in cases of severe motion and image corruption. It shows improvements in quality and up to 50% reduction in reconstruction time compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing SVR methods struggle with image artifacts and severe subject motion or require slice pre-alignment for good performance.

Method: The proposed method performs motion correction, outlier handling, and super-resolution reconstruction based on implicit neural representations. The model can be initialized with task-specific priors through self-supervised meta-learning.

Result: The method showed improvements in reconstruction quality, particularly in the presence of severe motion, and up to 50% reduction in reconstruction time in experiments with over 480 reconstructions of simulated and clinical MRI brain data.

Conclusion: The novel SVR method using implicit neural representations is effective for fast and accurate MRI reconstruction in cases of severe motion and image corruption.

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [459] [ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions](https://arxiv.org/abs/2505.09831)
*Tushar Kataria,Beatrice Knudsen,Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: The paper introduces ImplicitStainer, a new method using local implicit functions for virtual staining to generate IHC stains from H&E images. It addresses the limitations of existing GAN and diffusion models by improving performance with limited data and shows superior results through extensive validation.


<details>
  <summary>Details</summary>
Motivation: H&E staining is the standard in pathology but lacks comprehensive diagnostic information. Current IHC staining methods are time-consuming and only available at specialized centers, causing delays in patient treatment. Virtual staining via deep learning offers an alternative solution.

Method: ImplicitStainer uses local implicit functions to enhance pixel-level predictions in image translation tasks, specifically focusing on generating high-quality IHC stains from H&E images even when data is limited.

Result: Validated on two datasets using multiple metrics, ImplicitStainer outperforms over fifteen state-of-the-art GAN and diffusion-based models, demonstrating robustness and effectiveness with limited data.

Conclusion: ImplicitStainer provides a promising advancement in virtual staining technology, offering improved performance with less data compared to existing methods. Full code and trained models will be made publicly available.

Abstract: Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.

</details>


### [460] [Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/abs/2505.09985)
*Pengfei Yu,Bin Huang,Minghui Zhang,Weiwen Wu,Shaoyu Wang,Qiegen Liu*

Main category: eess.IV

TL;DR: OSMM is a new model that divides CT projection data into subsets for independent learning, integrates OWDM for global information constraint, and uses an unsupervised learning framework to improve sparse-view CT reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current score-based diffusion models face challenges with large, redundant projection datasets in sparse-view CT reconstruction, leading to lower learning effectiveness, higher difficulty, and reconstructed images lacking fine details.

Method: The OSMM divides CT projection data into equal subsets and applies MSDM to learn from each subset independently. It also integrates OWDM with complete sinogram data as a global information constraint.

Result: Experimental results show that OSMM surpasses traditional diffusion models in image quality and noise resilience for sparse-view CT reconstruction.

Conclusion: OSMM offers a robust and versatile solution for advanced CT imaging in sparse-view scenarios.

Abstract: Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.

</details>


### [461] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/abs/2505.10405)
*Jianhao Huang,Qunsong Zeng,Kaibin Huang*

Main category: eess.IV

TL;DR: The paper proposes a hybrid generative semantic communication (Gen-SemCom) system with critical information embedding framework for 6G networks. It introduces the GVIF metric to evaluate visual quality and designs a channel-adaptive system.


<details>
  <summary>Details</summary>
Motivation: To overcome the issues of losing fine-grained visual details in purely prompt-driven generation and the lack of systematic metrics to evaluate Gen-SemCom systems.

Method: Developed a hybrid Gen-SemCom system using a critical information embedding (CIE) framework which transmits both text prompts and semantically critical features. Proposed the generative visual information fidelity (GVIF) metric to assess visual quality. Designed a channel-adaptive Gen-SemCom system based on maximizing the GVIF metric.

Result: Validated that the GVIF metric is sensitive to visual fidelity, correlating with PSNR and critical information volume. The optimized system outperforms benchmarking schemes with higher PSNR and lower FID scores.

Conclusion: The hybrid Gen-SemCom system with CIE framework and GVIF metric effectively addresses the limitations of current Gen-SemCom systems and shows superior performance.

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


### [462] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/abs/2505.10464)
*Jiaming Liang,Lihuan Dai,Xiaoqi Sheng,Xiangguang Chen,Chun Yao,Guihua Tao,Qibin Leng,Honming Cai,Xi Zhong*

Main category: eess.IV

TL;DR: In the field of gastric cancer lesion analysis, multimodal medical image segmentation encounters challenges due to lack of datasets and difficulty in aligning modalities. To solve these problems, the paper presents GCM 2025, a new large-scale dataset, and HWA-UNETR, a novel 3D segmentation framework. Experiments show that HWA-UNETR outperforms existing methods with a Dice score improvement of up to 1.68%.


<details>
  <summary>Details</summary>
Motivation: Multimodal medical image segmentation for gastric cancer lesion analysis is constrained by the scarcity of independent multimodal datasets and the challenge of amalgamating misaligned modalities, which leads to substantial resource expenditure and potential decline in accuracy.

Method: The paper introduces HWA-UNETR, a 3D segmentation framework incorporating an HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures. It also utilizes a tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies.

Result: Experiments on the GCM 2025 and BraTS 2021 datasets demonstrate that HWA-UNETR surpasses existing methods by up to 1.68% in the Dice score while maintaining robustness.

Conclusion: The introduction of the GCM 2025 dataset and the HWA-UNETR framework provides a valuable resource and effective solution for multimodal medical image segmentation in gastric cancer lesion analysis.

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [463] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/abs/2505.10492)
*Taylor L. Bobrow,Mayank Golhar,Suchapa Arayakarnkul,Anthony A. Song,Saowanee Ngamruengphong,Nicholas J. Durr*

Main category: eess.IV

TL;DR: The paper introduces Multi-contrast Laser Endoscopy (MLE) for better gastrointestinal imaging, showing improved contrast and color difference in detecting polyps compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the detection of diseases in the gastrointestinal tract by enhancing the contrast of visual abnormalities that are often subtle in white light endoscopy.

Method: Developed MLE platform with rapidly tunable spectral, coherent, and directional illumination. Demonstrated capabilities include multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.

Result: MLE achieved a three-fold improvement in contrast and five-fold improvement in color difference when imaging 31 polyps compared to white light and narrow band imaging.

Conclusion: MLE shows potential as an investigative tool to enhance gastrointestinal imaging by revealing multiple types of tissue contrast while integrating into the clinical environment.

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [464] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

TL;DR: The paper establishes a novel lower bound for the Neyman-Pearson boundary using $f$-divergence, improves Pinsker's inequality with KL divergence, obtains an upper bound in terms of the Chernoff $\alpha$-coefficient, and presents methods for constructing distribution pairs to realize given boundaries.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of the Neyman-Pearson region by deriving new bounds for its boundary and presenting methods to construct distribution pairs that can realize given boundaries.

Method: Establishing a novel lower bound for the Neyman-Pearson boundary using any $f$-divergence, improving Pinsker's inequality with KL divergence, obtaining a closed-form refined upper bound using the Chernoff $\alpha$-coefficient, and proposing methods for constructing pairs of distributions to approximately or exactly realize given Neyman-Pearson boundaries.

Result: A novel lower bound for the Neyman-Pearson boundary was established in terms of any $f$-divergence, which is best possible when generated by hockey-stick $f$-divergences. An improved Pinsker's inequality was achieved with KL divergence. A closed-form refined upper bound for the Neyman-Pearson boundary was obtained in terms of the Chernoff $\alpha$-coefficient. Methods for constructing pairs of distributions were presented.

Conclusion: The analysis provides new insights into the Neyman-Pearson region by offering tighter bounds on its boundary and demonstrating how to construct distribution pairs to achieve specific boundaries.

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>


### [465] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

TL;DR: The paper extends standard decision theory to include counterfactual losses for better assessing decision quality.


<details>
  <summary>Details</summary>
Motivation: Classical statistical decision theory evaluates treatment choices based solely on observed outcomes, ignoring counterfactual outcomes which limits its ability to assess the quality of decisions relative to feasible alternatives.

Method: Extend standard decision theory to incorporate counterfactual losses under the assumption of strong ignorability. Identify that a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes.

Result: Counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions when more than two treatment options are involved.

Conclusion: Incorporating counterfactual losses into decision theory allows for a more comprehensive evaluation of decision quality.

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [466] [Neural models for prediction of spatially patterned phase transitions: methods and challenges](https://arxiv.org/abs/2505.09718)
*Daniel Dylewsky,Sonia Kéfi,Madhur Anand,Chris T. Bauch*

Main category: physics.comp-ph

TL;DR: Dryland vegetation ecosystems are susceptible to critical transitions between alternative stable states when subjected to external forcing. Recent methodological developments in Early Warning Signal (EWS) detection have shown promise in identifying dynamical signatures of oncoming critical transitions, with particularly strong predictive capabilities being demonstrated by deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Dryland vegetation ecosystems are known to be susceptible to critical transitions between alternative stable states when subjected to external forcing.

Method: This paper explores the successes and shortcomings of neural EWS detection for spatially patterned phase transitions, and shows how these models can be used to gain insight into where and how EWS-relevant information is encoded in spatiotemporal dynamics.

Result: Results reveal that model performance often changes dramatically when training and test data sources are interchanged, which offers new insight into the criteria for model generalization.

Conclusion: The study reveals the potential and limitations of using neural networks for detecting early warning signals in dryland vegetation ecosystems.

Abstract: Dryland vegetation ecosystems are known to be susceptible to critical
transitions between alternative stable states when subjected to external
forcing. Such transitions are often discussed through the framework of
bifurcation theory, but the spatial patterning of vegetation, which is
characteristic of drylands, leads to dynamics that are much more complex and
diverse than local bifurcations. Recent methodological developments in Early
Warning Signal (EWS) detection have shown promise in identifying dynamical
signatures of oncoming critical transitions, with particularly strong
predictive capabilities being demonstrated by deep neural networks. However, a
machine learning model trained on synthetic examples is only useful if it can
effectively transfer to a test case of practical interest. These models'
capacity to generalize in this manner has been demonstrated for bifurcation
transitions, but it is not as well characterized for high-dimensional phase
transitions. This paper explores the successes and shortcomings of neural EWS
detection for spatially patterned phase transitions, and shows how these models
can be used to gain insight into where and how EWS-relevant information is
encoded in spatiotemporal dynamics. A few paradigmatic test systems are used to
illustrate how the capabilities of such models can be probed in a number of
ways, with particular attention to the performances of a number of proposed
statistical indicators for EWS and to the supplementary task of distinguishing
between abrupt and continuous transitions. Results reveal that model
performance often changes dramatically when training and test data sources are
interchanged, which offers new insight into the criteria for model
generalization.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [467] [Predictive Models for Chronic Heart Failure](https://arxiv.org/abs/2505.09619)
*Pietro Cassieri,Aiman Faiz,Anna Maria De Roberto,Claudio Pascarelli,Gianvito Mitrano,Gianluca Fimiani,Marina Garofano,Christiancarmine Esposito,Genoveffa Tortora,Alessia Bramanti,Giuseppe Scanniello*

Main category: stat.OT

TL;DR: 研究人员开发了一个基于机器学习的预测模型，用于识别心力衰竭风险患者。该模型结合了临床和超声心动图特征，表现良好，尤其是在高灵敏度方面。它有望成为医疗决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 慢性心力衰竭管理面临巨大挑战，需要持续监测、早期恶化检测和个性化治疗策略。因此，有必要开发一种能够有效识别心力衰竭风险患者的预测模型。

Method: 该模型采用集成学习方法，具体为一种修改后的堆叠技术。它使用两个专门模型分别利用临床和超声心动图特征，然后通过元模型将这两个模型的预测结果结合起来。

Result: 在真实数据集上的初步评估显示，该模型在心力衰竭风险患者分层方面表现出色，灵敏度高达95%，准确率为84%。与三个基线模型相比，该模型性能更优。

Conclusion: 基于机器学习的风险分层模型可以作为有价值的决策支持工具，不仅适用于PrediHealth项目，还对医疗专业人员有益，有助于早期干预和个性化患者管理。

Abstract: The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [468] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

TL;DR: The paper proposes a new extension of EL with weakened negation to represent negative knowledge while maintaining tractability.


<details>
  <summary>Details</summary>
Motivation: Biomedical ontologies contain numerous concept or role names involving negative knowledge such as lacks_part, absence_of. Current representation with labels rather than logical constructors do not allow a reasoner to interpret these as a kind of negation. Adding full negation makes the logic intractable.

Method: Introduce categorical semantics of all logical constructors of the DL SH including EL with disjunction, negation, universal restriction, role inclusion and transitive roles. Weaken semantics of disjunction and universal restriction by identifying independent categorical properties that cause intractability and dropping them.

Result: The resulting logic from weakening semantics is more expressive than EL with the bottom concept, transitive roles and role inclusion.

Conclusion: A new extension of EL with weakened negation is proposed which allows to represent negative knowledge while retaining tractability.

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


### [469] [Inconsistency Handling in DatalogMTL](https://arxiv.org/abs/2505.10394)
*Meghyn Bienvenu,Camille Bourgaux,Atefe Khodadaditaghanaki*

Main category: cs.LO

TL;DR: The paper explores inconsistency handling in DatalogMTL, defines conflict and repair notions, and analyzes data complexity for generating conflicts/repairs and query entailment.


<details>
  <summary>Details</summary>
Motivation: Inconsistency handling is crucial in DatalogMTL as facts associated with time intervals may contradict rules; thus, understanding how to restore consistency through various methods like removing facts or altering time intervals is important.

Method: The authors define relevant notions of conflicts (minimal explanations for inconsistency) and repairs (ways to restore consistency) specific to the DatalogMTL setting. They also study the properties of these notions and their inconsistency-tolerant semantics.

Result: The first contribution includes definitions and analysis of conflicts and repairs in DatalogMTL. The second contribution involves a detailed data complexity analysis for tasks such as generating a single conflict/repair and query entailment under repair-based semantics.

Conclusion: This work provides essential insights into inconsistency handling in DatalogMTL by defining key concepts and analyzing computational complexities, which will help advance the field.

Abstract: In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [470] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

TL;DR: 本研究比较了通用大型语言模型（GPT-4o）和推理优化模型（o1-preview）在解决德国物理奥林匹克竞赛问题上的表现，并与人类参赛者进行了对比。结果表明，两种LLM在解题能力上优于人类平均水平，其中o1-preview表现尤为突出。研究还探讨了如何在物理教育中设计评估方式以维持其完整性和促进学生对LLM的批判性使用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的普及，它们在教育中的应用引发了对学生学习过程和评估形式完整性的担忧。特别是在物理教育中，理解LLMs在特定问题解决方面的能力至关重要，这将有助于负责任地将LLMs整合到教学和评估中。

Method: 研究选取了德国物理奥林匹克竞赛的一系列明确定义的问题，比较了GPT-4o（使用不同的提示技术）和o1-preview模型与人类参赛者的解题表现。除了评估解题正确性外，还分析了LLM生成解决方案的特征优势和局限性。

Result: 结果显示，GPT-4o和o1-preview在解决奥林匹克类型物理问题上表现出高级别的解题能力，平均而言超越了人类参赛者。不同提示技术对GPT-4o的影响较小，而o1-preview几乎始终优于GPT-4o和人类基准。

Conclusion: 基于这些发现，研究讨论了物理教育中总结性和形成性评估的设计影响，包括如何保持评估的完整性以及支持学生批判性地与LLMs互动。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [471] [Inferring entropy production in many-body systems using nonequilibrium MaxEnt](https://arxiv.org/abs/2505.10444)
*Miguel Aguilera,Sosuke Ito,Artemy Kolchinsky*

Main category: cond-mat.stat-mech

TL;DR: A method for inferring entropy production in high-dimensional stochastic systems is proposed, which uses trajectory observables and doesn't require probability distributions or special assumptions. It has intuitive physical interpretation and can be used for hierarchical decomposition of EP.


<details>
  <summary>Details</summary>
Motivation: Standard techniques for estimating entropy production become intractable in high-dimensional stochastic systems due to computational and statistical limitations.

Method: Infer trajectory-level EP and lower bounds on average EP by exploiting a nonequilibrium analogue of the Maximum Entropy principle, along with convex duality, using samples of trajectory observables.

Result: Demonstrated numerical performance on a disordered nonequilibrium spin model with 1000 spins and a large neural spike-train dataset.

Conclusion: The proposed method provides an intuitive physical interpretation as a thermodynamic uncertainty relation and may be used to compute a hierarchical decomposition of EP.

Abstract: We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [472] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

TL;DR: This paper introduces RT-cache, a novel trajectory memory pipeline that accelerates robot inference via big-data retrieval and learning. It stores successful trajectories, retrieves relevant motions, and adapts to new environments with few samples. Experiments show it outperforms baselines without retrieval.


<details>
  <summary>Details</summary>
Motivation: Modern Vision-Language-Action (VLA) models can handle diverse robotic tasks but suffer from high per-step inference costs and latency.

Method: RT-cache includes a Memory Builder and Trajectory Retrieval system. It stores large-scale memory of successful robot trajectories and retrieves relevant multistep motion snippets for current scenes.

Result: Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks faster and more successfully than a baseline lacking retrieval.

Conclusion: RT-cache provides a practical, data-driven solution for real-time robotic manipulation by reducing inference overhead through experience-based retrieval.

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [473] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: This paper introduces a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) collaborate to accomplish missions specified in natural language, using a Large Language Model (LLM)-enabled planner.


<details>
  <summary>Details</summary>
Motivation: As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification.

Method: The system leverages a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. The system considers task-driven navigation in urban and rural areas and must infer mission-relevant semantics and actively acquire information via semantic mapping.

Result: The system was demonstrated on seven different natural-language specifications at up to kilometer-scale navigation in both ground and air-ground teaming experiments.

Conclusion: The authors have presented a system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly.

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [474] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

TL;DR: The paper introduces ChicGrasp, an end-to-end hardware-software co-design for automating poultry processing. It uses a pneumatic gripper and a conditional diffusion-policy controller trained on 50 teleoperation demonstrations to grasp and lift chicken carcasses with a 40.6% success rate.


<details>
  <summary>Details</summary>
Motivation: To automate the task of lifting slippery chicken carcasses onto a shackle conveyor in poultry processing lines, addressing challenges such as deformability, anatomical variance, and hygiene rules that make conventional methods unreliable.

Method: ChicGrasp employs a dual-jaw pneumatic gripper to clamp chicken legs and a conditional diffusion-policy controller trained from multi-view teleoperation demonstrations (RGB + proprioception) to plan 5 DoF end-effector motion including jaw commands.

Result: Achieves a 40.6% grasp-and-lift success rate and completes the pick-to-shackle cycle in 38 seconds, outperforming state-of-the-art implicit behaviour cloning (IBC) and LSTM-GMM baselines which fail entirely.

Conclusion: ChicGrasp demonstrates that imitation learning can bridge the gap between rigid hardware and variable bio-products, providing a reproducible benchmark and public dataset for agricultural engineering and robot learning research.

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [475] [Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery](https://arxiv.org/abs/2505.08932)
*Mohammad Wasil,Ahmad Drak,Brennan Penfold,Ludovico Scarton,Maximilian Johenneken,Alexander Asteroth,Sebastian Houben*

Main category: cs.RO

TL;DR: The paper adapts the Segment Anything Model (SAM) using parameter-efficient fine-tuning (PEFT) for forest floor object segmentation, achieving high mIoU. LoRA is proposed as a lightweight alternative for UAVs.


<details>
  <summary>Details</summary>
Motivation: Forest floor understanding is challenging due to variability and ambiguous annotations. SAM's strong generalization capabilities make it suitable for this task.

Method: Adapt SAM with PEFT by fine-tuning a subset of parameters and adjusting the mask decoder for automatic segmentation of forest floor objects.

Result: Adapter-based PEFT achieves the highest mIoU. LoRA offers a lightweight solution for UAVs.

Conclusion: SAM adapted with PEFT effectively segments forest floor objects, with LoRA being a viable option for resource-constrained UAV platforms.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.

</details>


### [476] [Multi-step manipulation task and motion planning guided by video demonstration](https://arxiv.org/abs/2505.08949)
*Kateryna Zorina,David Kovar,Mederic Fourmy,Florent Lamiraux,Nicolas Mansard,Justin Carpentier,Josef Sivic,Vladimir Petrik*

Main category: cs.RO

TL;DR: This paper leverages instructional video to solve complex multi-step task-and-motion planning tasks in robotics.


<details>
  <summary>Details</summary>
Motivation: To solve complex multi-step task-and-motion planning tasks in robotics by utilizing instructional video as guidance.

Method: Propose an extension of the RRT planner which simultaneously grows multiple trees around grasp and release states extracted from the guiding video, combining contact states and 3D object poses extracted from the video with a traditional planning algorithm.

Result: Demonstrate the effectiveness of the proposed planning algorithm on several robots through a new benchmark with three challenging tasks.

Conclusion: Developed a trajectory refinement approach formulated as an optimal control problem for a seamless transfer of the obtained plans to the real robot.

Abstract: This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).

</details>


### [477] [FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis](https://arxiv.org/abs/2505.09109)
*Yuxing Chen,Bowen Xiao,He Wang*

Main category: cs.RO

TL;DR: This paper addresses the challenge of generating high-quality data for robotic garment manipulation by presenting a synthetic garment dataset and proposing KG-DAgger, which improves model performance and boosts real-world success rate.


<details>
  <summary>Details</summary>
Motivation: The deformability of garments makes it difficult to generate a large amount of high-quality data for robotic garment manipulation tasks.

Method: The authors construct geometric garment templates based on keypoints and apply generative models to create realistic texture patterns. They use these keypoint annotations to generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. They also propose KG-DAgger, a keypoint-based strategy to generate demonstration data for recovering from failures.

Result: After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75% success rate in the real world, with KG-DAgger boosting the real-world success rate by 25%. Experiments validate the effectiveness of the proposed framework in both simulation and real-world settings.

Conclusion: The synthetic garment dataset and KG-DAgger significantly improve the robustness and success rate of robotic garment folding.

Abstract: Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.

</details>


### [478] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

TL;DR: This paper proposes a customized wearable robotic controller using reinforcement learning and a musculoskeletal model to compensate for specific muscle deficits in hand-object manipulation tasks. The controller is trained with video data of human grasping tasks and fine-tuned for object-specific interactions. When integrated into a virtual exoglove, it provides shared assistance for weakened hand muscles, achieving 90.5% of original manipulation proficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the need for more personalized rehabilitation solutions for patients with hand motor impairments, acknowledging the uniqueness of each patient's muscle loss.

Method: The method involves using reinforcement learning on a biologically accurate musculoskeletal model to create a customized wearable robotic controller. Video data of human grasping tasks is used to train a manipulation model which is then fine-tuned for specific object interaction tasks. Muscle forces in the model are weakened to simulate impairments and compensated by a virtual wearable robotics glove.

Result: The results demonstrate that integrating the virtual wearable robotic glove provides effective shared assistance to support hand manipulators with weakened muscle forces. The learned exoglove controller achieves an average of 90.5% of the original manipulation proficiency.

Conclusion: In conclusion, the proposed customized wearable robotic controller shows promise in addressing specific muscle deficits in hand-object manipulation tasks through the use of reinforcement learning and musculoskeletal models.

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [479] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

TL;DR: TransDiffuser is an encoder-decoder model for generative trajectory planning in autonomous driving, which uses scene information as multi-modal conditional input and introduces a decorrelation optimization mechanism to solve mode collapse dilemma. It achieves PDMS of 94.85 on NAVSIM benchmark without anchor-based prior trajectories.


<details>
  <summary>Details</summary>
Motivation: To transfer the capabilities of diffusion models to modern autonomous driving systems and generate high-quality diverse trajectories.

Method: Propose TransDiffuser, an encoder-decoder based generative trajectory planning model. The encoded scene information serves as the multi-modal conditional input of the denoising decoder and a multi-modal representation decorrelation optimization mechanism is introduced during the training process.

Result: Achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.

Conclusion: TransDiffuser successfully generates high-quality diverse trajectories in autonomous driving systems and outperforms previous methods.

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [480] [APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression](https://arxiv.org/abs/2505.09356)
*Srinivas Ravuri,Yuan Xu,Martin Ludwig Zehetner,Ketan Motlag,Sahin Albayrak*

Main category: cs.RO

TL;DR: APR-Transformer是一种新的模型架构，使用图像或LiDAR数据预测绝对姿态（3D位置和3D方向），在多个数据集上达到最先进的性能，并在GNSS拒绝环境下通过实车验证。


<details>
  <summary>Details</summary>
Motivation: 精确初始化对于定位算法的性能至关重要，特别是在机器人、自动驾驶和计算机视觉领域。不准确的初始姿态会导致定位精度差，尤其是在无法依赖GPS信号的环境中。利用深度神经网络进行姿态回归的最新进展显著提高了准确性和鲁棒性。

Method: 引入了APR-Transformer模型架构，该架构可以使用图像或LiDAR数据来预测绝对姿态（3D位置和3D方向）。该方法在几个基准数据集上进行了测试，并扩展到自定义的复杂数据集APR-BeIntelli。此外，在GNSS拒绝环境下，通过在自主测试车辆上实时部署模型来验证其可靠性。

Result: 在Radar Oxford Robot-Car、DeepLoc和APR-BeIntelli数据集上取得了最先进的性能，并在GNSS拒绝环境下的实车测试中展示了实际可行性和有效性。

Conclusion: APR-Transformer在多种数据集上表现出色，并在GNSS拒绝环境下通过实车验证，证明了其实际可行性和有效性。源代码已公开。

Abstract: Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.

</details>


### [481] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: The paper discusses deploying foundation model (FM)-enabled robots in large-scale and unstructured environments, presenting SPINE - an LLM-enabled autonomy framework. It showcases the first large-scale LLM-based robot planning in such settings and introduces a language-driven UAV planner using onboard language models.


<details>
  <summary>Details</summary>
Motivation: Foundation model (FM)-enabled robots have been primarily used in closed-world settings with full prior maps or complete workspace views. The motivation is to address the challenge of deploying these robots in real-world missions that require operation in large-scale and unstructured environments.

Method: The method involves using SPINE, an LLM-enabled autonomy framework, which is adaptable to different LLMs and can be distilled into smaller models for SWaP-limited platforms. This approach enables effective exploration, navigation through obstacles, handling unexpected sensor inputs, and operation within compute constraints.

Result: The result includes successful deployments of SPINE in field robotic settings, demonstrating the first large-scale LLM-enabled robot planning over several kilometers in unstructured environments. Additionally, a language-driven UAV planner using on-device language models was presented.

Conclusion: The conclusion proposes several promising directions for future research, indicating the potential advancements in FM-enabled robotics for complex, real-world applications.

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [482] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出Past-Token Prediction (PTP) 方法，通过预测过去动作标记来增强策略对未来和过去动作之间依赖关系的捕捉，有效解决了长序列任务中的信息保留问题。实验表明，该方法将长上下文扩散策略性能提升3倍，并加速训练10倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前学习长上下文策略的方法通常通过截断上下文长度来规避挑战，但这种方法会丢弃对后续决策至关重要的历史信息。因此需要一种新方法来显式地规范过去信息的保留。

Method: 作者引入了Past-Token Prediction (PTP) 作为辅助任务，让策略同时学习预测过去和未来动作标记。此外，还提出了一个多阶段训练策略：先用短上下文预训练视觉编码器，再用缓存的长上下文嵌入微调策略头。最后，扩展PTP为自验证机制以在推理时选择与过去动作一致的候选方案。

Result: 在四个真实世界和六个模拟任务上的实验表明，所提方法使长上下文扩散策略的性能提高了3倍，并且将策略训练速度提升了10倍以上。

Conclusion: Past-Token Prediction (PTP) 辅助任务显著改善了策略的时间建模能力，多阶段训练策略大幅降低了内存和计算开销，而自验证机制进一步增强了推理时的一致性。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [483] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

TL;DR: The paper presents Mini-Diffuser, a method for training multi-task vision-language robotic diffusion policies that significantly reduces training time and memory usage while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: To reduce the training time and memory usage for multi-task vision-language robotic diffusion policies by exploiting the dimensional differences between action diffusion and image diffusion techniques.

Method: Mini-Diffuser uses Level-2 minibatching, pairing multiple noised action samples with each vision-language condition. Architectural adaptations to the diffusion transformer prevent information leakage across samples while maintaining full conditioning access.

Result: In RLBench simulations, Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies, using only 5% of the training time and 7% of the memory. Real-world experiments validate its ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs.

Conclusion: Mini-Diffuser effectively reduces resource requirements for training multi-task vision-language robotic diffusion policies without significant loss in performance.

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [484] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: This paper addresses policy distillation under privileged information by introducing two methods to help the student policy learn effectively from a teacher with full-state access.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the problem of information asymmetry in policy distillation, where the student policy has only partial observations and cannot directly access the teacher's state space.

Method: The authors propose two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration.

Result: The proposed methods were validated in both simulated and real-world robotic tasks, showing significant improvements over standard teacher-student baselines in terms of training efficiency and final performance.

Conclusion: The introduced methods provide a more efficient way for the student policy to learn from the teacher policy despite information asymmetry.

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [485] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: DataMIL is a policy-driven data selection framework that optimizes data selection for task success in robotics, showing consistent gains in success rates and superior performance over multiple baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of generalist robot policies which underperform on individual, specialized tasks. The authors aim to improve specialized policies by combining task-specific data with subsets of large prior datasets via co-training, while avoiding naive data selection that may harm downstream performance.

Method: The method introduced is DataMIL, a policy-driven data selection framework based on the datamodels paradigm. It uses an end-to-end approach to select data points that will most improve performance, directly optimizing for task success rather than relying on human notions of quality. A novel surrogate loss function on task-specific data is used to avoid expensive rollouts in the environment during selection.

Result: The approach was validated on over 60 simulation and real-world manipulation tasks, demonstrating consistent gains in success rates and superior performance compared to multiple baselines. Notably, successful data selection was shown from the Open X-Embodiment datasets.

Conclusion: The results highlight the importance of end-to-end, performance-aware data selection for maximizing the potential of large prior datasets in robotics.

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


### [486] [ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation](https://arxiv.org/abs/2505.09698)
*Enyu Zhao,Vedant Raval,Hejia Zhang,Jiageng Mao,Zeyu Shangguan,Stefanos Nikolaidis,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: Vision-Language Models (VLMs) have transformed AI and robotics. While used in high-level planning, recent studies explore their lower-level reasoning ability for precise robot movements. A new benchmark, ManipBench, is proposed to evaluate VLMs' low-level robotic manipulation capabilities across different dimensions. Testing 33 representative VLMs from 10 model families on this benchmark reveals significant performance variation across tasks, correlating with real-world manipulation trends, but a substantial gap remains between these models and human-level understanding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a clear and common benchmark evaluating how well Vision-Language Models (VLMs) can aid low-level reasoning in robotics, particularly in understanding object-object interactions and deformable object manipulation.

Method: Propose a novel benchmark called ManipBench to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions. Extensively test 33 representative VLMs from 10 model families on this benchmark, including variants testing different model sizes.

Result: The evaluation shows significant variation in VLM performance across tasks, strong correlation between this performance and trends in real-world manipulation tasks, and a significant gap between these models and human-level understanding.

Conclusion: ManipBench provides a comprehensive evaluation framework for VLMs' low-level robotic manipulation reasoning capabilities, revealing performance variations, correlations with real-world tasks, and highlighting the need for further advancements to approach human-level understanding.

Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.

</details>


### [487] [FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation](https://arxiv.org/abs/2505.10075)
*Jun Guo,Xiaojian Ma,Yikai Wang,Min Yang,Huaping Liu,Qing Li*

Main category: cs.RO

TL;DR: The paper presents FlowDreamer, a model that uses 3D scene flow for predicting future frames in robot manipulation tasks, showing improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: To develop better visual world models for robot manipulation that can accurately predict future visual observations using past frames and robot actions.

Method: FlowDreamer predicts 3D scene flow using a U-Net and then uses a diffusion model to predict future frames, trained end-to-end despite its modular nature.

Result: FlowDreamer outperforms baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate across various robot manipulation domains.

Conclusion: FlowDreamer, with its explicit motion representation through 3D scene flow, provides enhanced performance in video prediction and visual planning tasks for robot manipulation.

Abstract: This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.

</details>


### [488] [Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering](https://arxiv.org/abs/2505.10073)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: A novel framework for Multi-Robot Task Allocation (MRTA) and collision avoidance is presented, using spatial clustering, K-means, and 2-Opt algorithm to reduce time by up to 93% and eliminate collisions in industrial environments.


<details>
  <summary>Details</summary>
Motivation: To create a combined solution for MRTA and collision avoidance in industrial environments with homogeneous measurement tasks, improving computational efficiency and ensuring operation free from collisions.

Method: The method uses spatial clustering to divide the workspace into operational zones for each robot, applying K-means clustering to divide task sites and the 2-Opt algorithm to schedule robot routes within corresponding clusters.

Result: The framework achieves up to 93% time reduction and a 7% improvement in solution quality compared to the best performing method, while completely eliminating collision points.

Conclusion: The findings indicate that spatial partitioning unifies task allocation and collision avoidance problems, making this approach highly significant for real-world applications requiring both computational efficiency and collision-free operations.

Abstract: In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.

</details>


### [489] [EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation](https://arxiv.org/abs/2505.10105)
*Zibin Dong,Fei Ni,Yifu Yuan,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: The paper introduces EmbodiedMAE, a multi-modal masked autoencoder that learns representations across RGB, depth, and point cloud modalities to improve robot manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information.

Method: Enhance the DROID dataset with high-quality depth maps and point clouds to construct DROID-3D, then develop EmbodiedMAE which is a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion.

Result: EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms.

Conclusion: EmbodiedMAE is established as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly effective in precise tabletop manipulation settings where spatial perception is critical.

Abstract: We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.

</details>


### [490] [Learning Rock Pushability on Rough Planetary Terrain](https://arxiv.org/abs/2505.09833)
*Tuba Girgin,Emre Girgin,Cagri Kilic*

Main category: cs.RO

TL;DR: 在非结构化环境中，通过移动障碍物而非避开它来提高多智能体重复使用的路径效率。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，传统的路径规划算法依赖于避障，这可能导致长期效率低下，并需要持续的路径规划系统支持。为了解决这个问题，提出了一种新的方法。

Method: 利用安装在移动机器人上的机械臂的操作能力，结合外部和本体感觉反馈来评估障碍物的可推性，从而实现障碍物的重新定位。初步视觉估计考虑了障碍物及其所在表面的特性，而推动可负担性评估模块则利用与障碍物交互时通过机械臂获得的力反馈作为引导信号。

Result: 该导航方法旨在通过减少车队在需要自主基础设施发展的环境（如月球或火星表面）中所花费的总体时间，提高多智能体长时间使用路径的效率。

Conclusion: 通过整合机械臂操作和反馈机制，提出的框架可以有效改善非结构化环境中多智能体路径的长期效率。

Abstract: In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.

</details>


### [491] [IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning](https://arxiv.org/abs/2505.10442)
*Dechen Gao,Hang Wang,Hanchu Zhou,Nejib Ammar,Shatadal Mishra,Ahmadreza Moradipari,Iman Soltani,Junshan Zhang*

Main category: cs.RO

TL;DR: IN-RIL是一种将模仿学习（IL）和强化学习（RL）交织进行的策略微调方法，通过周期性注入IL更新来提高稳定性及采样效率，并通过梯度分离机制防止优化冲突。实验表明，IN-RIL在多个机器人操作和运动任务中显著提高了采样效率并缓解了性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人学习方法通常采用IL预训练加RL微调的两步范式，但这种范式在RL微调阶段存在不稳定性和样本效率低的问题。因此需要一种新的方法，在整个微调过程中都能从IL的稳定性和专家数据指导中受益，同时保持RL的探索能力。

Method: 提出了一种名为IN-RIL的方法，该方法通过在多次RL更新后定期注入IL更新，使策略在整个微调过程中既能从IL的稳定性中受益，又能得到专家数据的指导以实现更有效的探索。此外，开发了梯度分离机制，以防止由于IL和RL的不同优化目标而产生的破坏性干扰。

Result: 广泛的实验结果表明，IN-RIL可以在长视域和短视域任务中、稀疏或密集奖励的任务中，显著提高采样效率并减轻在线微调期间的性能崩溃问题。例如，在Robomimic Transport任务上，成功率从12%提升到88%，提升了6.3倍。

Conclusion: IN-RIL作为一种通用插件，与各种最先进的RL算法兼容，能够显著改善RL微调过程中的稳定性和采样效率。

Abstract: Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.

</details>


### [492] [Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests](https://arxiv.org/abs/2505.10033)
*Luis F. W. Batista,Stéphanie Aravecchia,Seth Hutchinson,Cédric Pradalier*

Main category: cs.RO

TL;DR: This paper evaluates the resilience of a DRL-based agent for Autonomous Surface Vehicles (ASVs) under various perturbations, finding that it performs reliably despite significant disturbances.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient exploration of robustness in real-world conditions for DRL-based ASVs, especially under external disturbances.

Method: The agent is trained using domain randomization and its performance is evaluated in both simulation and real-world experiments under unexpected disturbances like asymmetric drag and off-center payload. Performance degradation is quantified and benchmarked against an MPC baseline.

Result: Results show that the DRL agent performs reliably even with significant disturbances.

Conclusion: The study provides insights into effective training strategies, real-world challenges, and practical considerations for deploying DRL-based ASV controllers, alongside releasing the implementation open-source.

Abstract: Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.

</details>


### [493] [Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation](https://arxiv.org/abs/2505.10522)
*Xinrui Wang,Yan Jin*

Main category: cs.RO

TL;DR: An abstract about a Knowledge Capture, Adaptation, and Composition (KCAC) framework to improve reinforcement learning in robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning has shown great potential but is limited by sample inefficiency and lack of interpretability. To address these challenges and enhance the agent's understanding and adaptability, there is a need for strategic knowledge utilization.

Method: The KCAC framework integrates knowledge transfer into RL via cross-task curriculum learning. It involves redesigning the benchmark reward function for flexibility, defining sub-tasks, and implementing a structured curriculum to facilitate efficient learning.

Result: KCAC achieves a 40% reduction in training time and improves task success rates by 10% compared to traditional RL methods. Key curriculum design parameters are identified for optimizing learning efficiency.

Conclusion: This work provides valuable insights into curriculum design in reinforcement learning and robotic learning, offering conceptual guidance for future curriculum-based RL frameworks.

Abstract: Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.

</details>


### [494] [Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning](https://arxiv.org/abs/2505.10547)
*Milan Ganai,Rohan Sinha,Christopher Agia,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures, outperforming slow reasoning models in safety classification accuracy and improving system safety and planning success.


<details>
  <summary>Details</summary>
Motivation: Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, but current methods lack the ability to plan generalizable, semantically safe motions due to high inference latency of Large Vision and Language Models.

Method: FORTRESS uses multi-modal reasoners at a low frequency in nominal operations to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time.

Result: FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.

Conclusion: By bridging open-world, multi-modal reasoning with dynamics-aware planning, FORTRESS eliminates the need for hard-coded fallbacks and human safety interventions.

Abstract: Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.

</details>


### [495] [AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics](https://arxiv.org/abs/2505.10398)
*Alexandre Banks,Randy Moore,Sayem Nazmuz Zaman,Alaa Eldin Abdelaal,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: The paper introduces AutoCam, an automatic auxiliary camera placement method for RAMIS using da Vinci Research Kit. It ensures robust camera tracking with high visibility and low pose error while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To enhance spatial awareness in RAMIS by incorporating an autonomous auxiliary camera that eliminates manual viewpoint control and addresses limitations of existing path planning methods which do not simultaneously account for camera orientation, workspace constraints, and robot joint limits.

Method: AutoCam uses a priority-based, workspace-constrained control algorithm combining heuristic geometric placement with nonlinear optimization. Implemented on the da Vinci Research Kit, it tracks a salient feature autonomously.

Result: Maintained 99.84% visibility of a salient feature with a pose error of 4.36 ± 2.11 degrees and 1.95 ± 5.66 mm. The controller was computationally efficient with a loop time of 6.8 ± 12.8 ms. Novices performed equally well using AutoCam's viewpoint as with the endoscope's during a training task.

Conclusion: An auxiliary camera can be autonomously controlled using the da Vinci patient-side manipulators to track a salient feature, paving the way for new multi-camera visualization methods in RAMIS.

Abstract: Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [496] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

TL;DR: An enhanced Retrieval-Augmented Generation (RAG) application is presented to assist data scientists in accessing precise and contextually relevant academic resources, with substantial improvements shown in Context Relevance.


<details>
  <summary>Details</summary>
Motivation: Efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation in data science.

Method: The AI-powered application integrates advanced techniques including GROBID for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method.

Result: A comprehensive evaluation using the RAGAS framework demonstrates substantial improvements in key metrics, particularly Context Relevance.

Conclusion: The findings highlight the potential of this enhanced RAG system to transform academic exploration within data science and advance the workflow of research and innovation.

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [497] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

TL;DR: FocusedRetriever是一个基于半结构化知识库（SKB）的模块化框架，用于多跳问答。它整合了多种组件，在STaRK基准测试的所有三个数据集上超越了现有最佳方法，平均首次命中率高出25.7%。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，机器学习模型和交互系统可以访问结构化知识（如知识图谱或表格）和非结构化内容（如自然语言文档），但大多数系统只能依赖其中一种。为了弥合这一差距，本文提出了利用半结构化知识库的方法，以链接非结构化内容与结构化数据，从而实现更有效的知识访问和使用。

Method: FocusedRetriever通过以下步骤进行多跳问答：1) 利用大型语言模型（LLM）从非结构化文本中提取关系事实和实体属性；2) 使用节点集连接过滤答案候选者；3) 采用向量相似性搜索检索和排名相关非结构化内容；4) 最后再次利用LLM的情境能力对前k个答案进行排名。

Result: 实验结果表明，FocusedRetriever在STaRK基准测试的所有三个数据集上表现优于现有方法，平均首次命中率比第二佳方法高出25.7%。此外，中间结果分析揭示了通过微调等方法进一步提升性能的机会。

Conclusion: FocusedRetriever展示了在多跳问答任务中的卓越性能，并为未来的研究提供了改进方向，例如通过微调大型语言模型来增强其性能。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [498] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

TL;DR: 尽管扩散模型在推荐系统中的应用被广泛报道为显著进步，但实验表明，这些模型在计算复杂度和碳排放较高的情况下，仍然逊色于简单的现有模型。这揭示了方法论问题的持续存在以及对科研严谨性和文化变革的需求。


<details>
  <summary>Details</summary>
Motivation: 调查现代去噪扩散概率模型在推荐系统中的最新进展，并检查早期研究中发现的方法论问题是否仍然存在于当前的研究中。

Method: 通过实验重现SIGIR 2023和2024年发布的四种基于扩散模型的推荐系统，并与调优后的基线模型进行比较，评估其性能。

Result: 发现扩散模型在传统top-n推荐任务上表现不佳，且其生成能力受到限制，同时确认了方法论问题依然存在。

Conclusion: 需要更高的科研严谨性以及研究和发表文化的变革来应对这些问题。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


### [499] [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
*Alejo Lopez-Avila,Jinhua Du*

Main category: cs.IR

TL;DR: Multimodal recommender systems (MRS) can be enhanced by large language models (LLMs), which offer semantic reasoning, in-context learning, and dynamic input handling. This survey reviews the integration of LLMs into MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models (LLMs) can enhance multimodal recommender systems (MRS) compared to pre-trained language models (PLMs), and address challenges related to scalability and model accessibility.

Method: The survey proposes a taxonomy for integration patterns of LLMs in MRS, identifies transferable techniques from related domains, provides an overview of evaluation metrics and datasets, and suggests future research directions.

Result: Provides insights into prompting strategies, fine-tuning methods, and data adaptation techniques when integrating LLMs into MRS, supporting future research advancements.

Conclusion: LLMs introduce significant opportunities for enhancing MRS performance but also present challenges that need addressing. The proposed taxonomy and identified techniques aim to guide future research.

Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.

</details>


### [500] [Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights](https://arxiv.org/abs/2505.10043)
*Yifan Wu,Lutao Yan,Yizhang Zhu,Yinan Mei,Jiannan Wang,Nan Tang,Yuyu Luo*

Main category: cs.IR

TL;DR: 为了提升文本到图表检索的性能，本文提出了一种生成层次语义信息的训练数据管道，并基于此训练了名为ChartFinder的模型。该模型在精确查询和模糊查询任务中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图表检索解决方案往往无法捕捉图表的语义内容和上下文信息，主要是由于缺乏全面的元数据（或语义洞见）。

Method: 提出了一个训练数据开发管道，自动合成图表的层次语义洞见，包括视觉模式、统计特性和实际应用，并基于此训练了一个名为ChartFinder的CLIP-based模型。

Result: 实验表明，ChartFinder在文本到图表检索任务中显著优于现有方法。对于精确查询，ChartFinder的NDCG@10达到66.9%，比最先进的模型高出11.58%。在模糊查询任务中，该方法也显示出一致的改进，几乎所有指标平均提高了5%。

Conclusion: 本文提出的ChartFinder模型通过利用丰富的语义洞见，在训练阶段学习到了更好的图表表示，从而显著提升了文本到图表检索的性能。

Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.

</details>


### [501] [Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M](https://arxiv.org/abs/2505.10212)
*Dario Di Palma,Felice Antonio Merra,Maurizio Sfilio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: Large Language Models (LLMs) may have memorized public recommendation datasets, which affects their generalizability and fairness. This study investigates the extent of memorization in LLMs using MovieLens-1M dataset.


<details>
  <summary>Details</summary>
Motivation: To verify whether Large Language Models (LLMs) have memorized public recommendation datasets as part of their training data, which is crucial for ensuring the generalizability of research findings and preventing bias amplification.

Method: Define dataset memorization as the ability to retrieve item attributes, user profiles, and user-item interactions by prompting LLMs. Analyze two model families (GPT and Llama) across multiple sizes on the MovieLens-1M dataset. Examine the impact of memorization on recommendation performance and whether it varies across model families and sizes.

Result: All examined models exhibit some degree of memorization of the MovieLens-1M dataset. Recommendation performance is related to the extent of memorization.

Conclusion: LLMs have memorized parts of the MovieLens-1M dataset, impacting recommendation performance. The code for this study has been made publicly available.

Abstract: Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [502] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

TL;DR: This paper proposes DPN-GAN, a novel GAN architecture for audio sequence generation that addresses the issues of resolution constraints and mode collapse through kernel-based periodic ReLU activation and deformable convolution operations. It demonstrates superior performance and robustness in both speech synthesis and music generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GANs for audio sequence generation rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences and lead to mode collapse during conditional generation.

Method: The proposed method is DPN-GAN, which incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation and utilizes deformable convolution operations for multi-resolution generation with adaptive receptive fields. The discriminator network is also enhanced using deformable convolution to better distinguish between real and generated samples.

Result: DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics across five different datasets covering speech synthesis and music generation tasks. It shows increased robustness in synthesized audio, especially on out-of-distribution and noisy data.

Conclusion: DPN-GAN delivers superior performance and robustness in audio sequence generation compared to existing GAN architectures.

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [503] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: Voice timbre is being studied in the VtaD 2025 challenge, where human impression of voice timbre is verbalized with sensory descriptors and compared between two voices.


<details>
  <summary>Details</summary>
Motivation: To explain voice timbre attribute in a comparative manner using sensory descriptors.

Method: The timbre is explained from the comparison between two voices in their intensity within a specific descriptor dimension.

Result: Not mentioned in the abstract.

Conclusion: VtaD 2025 challenge will have a special proposal at the NCMMSC2025 conference.

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [504] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [505] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/abs/2505.09616)
*Yuqi Li,Yuanzhong Zheng,Zhongtian Guo,Yaoxuan Wang,Jianjun Yin,Haojun Fei*

Main category: cs.SD

TL;DR: SpecWav-Attack is an adversarial model which uses Wav2Vec2 for feature extraction, spectrogram resizing and incremental training to detect speakers in anonymized speech. It performs better than conventional attacks, showing the weaknesses of anonymized speech systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective method to detect speakers in anonymized speech and reveal vulnerabilities in these systems.

Method: Using Wav2Vec2 for feature extraction, incorporating spectrogram resizing and incremental training.

Result: Outperforms conventional attacks on librispeech-dev and librispeech-test datasets.

Conclusion: Anonymized speech systems have significant vulnerabilities which need stronger defenses, as benchmarked against the ICASSP 2025 Attacker Challenge.

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [506] [Introducing voice timbre attribute detection](https://arxiv.org/abs/2505.09661)
*Jinghao He,Zhengyan Sheng,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: This paper introduces voice timbre attribute detection (vTAD) to explain voice timbre using sensory attributes. A framework built on speaker embeddings is proposed and tested on VCTK-RVA dataset. ECAPA-TDNN performs better in seen scenarios, while FACodec shows superior generalization in unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: To explain the timbre conveyed by speech signals through a set of sensory attributes describing its human perception.

Method: Propose a framework based on speaker embeddings extracted from speech utterances for voice timbre attribute detection (vTAD). Investigate using VCTK-RVA dataset with ECAPA-TDNN and FACodec speaker encoders.

Result: ECAPA-TDNN outperforms in seen scenarios where testing speakers are part of training set; FACodec is superior in unseen scenarios indicating better generalization capability.

Conclusion: VCTK-RVA dataset and open-source code are provided for future research on voice timbre attribute detection.

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [507] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/abs/2505.10101)
*Jongmin Jung,Dasaem Jeong*

Main category: cs.SD

TL;DR: LAV系统结合了EnCodec的神经音频压缩与StyleGAN2的生成能力，通过随机初始化线性映射将EnCodec嵌入转换为StyleGAN2的样式潜在空间，从而生成由预录音频驱动的视觉动态输出。此方法保留了语义丰富性，并展示了预训练音频压缩模型在艺术和计算应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖显式特征映射进行音视频转换，而本文旨在探索一种新方法，利用EnCodec的嵌入作为潜在表示，直接转换到StyleGAN2的样式潜在空间，以实现更细致和语义连贯的音视频转换。

Method: LAV系统使用EnCodec的嵌入作为潜在表示，并通过随机初始化的线性映射将其直接转换到StyleGAN2的样式潜在空间中，从而生成视觉动态输出。这种方法无需显式的特征映射，而是保留了转换过程中的语义丰富性。

Result: 该框架成功实现了由预录音频驱动的视觉动态输出，展现了语义连贯且细腻的音视频转换效果。

Conclusion: 研究证明了使用预训练音频压缩模型（如EnCodec）进行艺术和计算应用的潜力，为未来音视频生成技术的发展提供了新的思路。

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [508] [Detecting Musical Deepfakes](https://arxiv.org/abs/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: This study explores the detection of AI-generated songs using the FakeMusicCaps dataset and mel spectrograms with a CNN model, while also discussing ethical and societal implications of TTM platforms.


<details>
  <summary>Details</summary>
Motivation: The rise of Text-to-Music platforms has made music creation more accessible but also poses challenges to musicians and the music industry, necessitating research into detecting AI-generated music.

Method: Using the FakeMusicCaps dataset, tempo stretching and pitch shifting were applied to simulate adversarial conditions. Mel spectrograms were generated from the modified audio for training and evaluating a convolutional neural network.

Result: Technical results on detecting AI-generated songs are presented, showing the effectiveness of the approach under adversarial conditions.

Conclusion: Carefully designed detection systems are crucial for protecting artists and harnessing the positive potential of generative AI in music.

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [509] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/abs/2505.10511)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: The paper explores combining modal decomposition with neural ordinary differential equations to model distributed musical systems, demonstrating its ability to reproduce nonlinear dynamics using synthetic data of a nonlinear string.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling geometric nonlinearities in distributed musical systems, such as pitch glides and brightness changes in high-amplitude string vibrations.

Method: Combine modal decomposition with neural ordinary differential equations. Use an analytical solution for linear vibration modes and a neural network for nonlinear dynamics. Avoid the need for a parameter encoder in the network architecture.

Result: Successfully trained the model on synthetic data of a nonlinear transverse string to reproduce the system's nonlinear dynamics.

Conclusion: This approach provides a proof of concept for modeling nonlinear musical systems while keeping physical parameters accessible.

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [510] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

TL;DR: The paper argues that instead of comparing LLMs to human experts, research should focus on developing strategies for humans to work efficiently with LLMs in clinical settings to ensure patient safety amidst rapid LLM advancements.


<details>
  <summary>Details</summary>
Motivation: There is a growing concern about the potential harm LLMs could cause to patient care if not properly safeguarded. The current trend of comparing LLMs to human experts is seen as counterproductive.

Method: The authors advocate for shifting research focus from comparison to collaboration between humans and LLMs in clinical settings.

Result: Demonstrates the need for strategies that enable efficient, almost symbiotic work of humans with LLMs rather than simply comparing their capabilities.

Conclusion: Future research efforts must prioritize characterizing the safe use of LLMs in clinical settings and fostering human-LLM collaboration.

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [511] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

TL;DR: Recent advances in generative AI have transformed information access, but the digital divide persists in many developing regions. To address this, WaLLM - a custom AI chatbot on WhatsApp - was developed. It has features beyond answering queries and has been operational for over 6 months with significant user engagement. Analysis of user interactions shows that most queries seek factual information, especially on 'Health and well-being'. User activity is concentrated around the daily top question and those accessing the 'Leaderboard' feature interact more. The paper concludes with implications for customization, UI design, and trust calibration.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in AI access in developing regions due to the persistent digital divide by creating an accessible AI chatbot on a widely used platform.

Method: Development of WaLLM, a custom AI chatbot on WhatsApp, with features such as answering queries, daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Logs were analyzed systematically to understand user interactions.

Result: WaLLM amassed over 14.7K queries from approximately 100 users over 6 months. 55% of queries sought factual information, with 'Health and well-being' being the most popular topic (28%). Two-thirds of user activity occurred within 24 hours of the daily top question, and users who accessed the 'Leaderboard' interacted 3x as much.

Conclusion: The findings suggest implications for culture-based customization, user interface design, and appropriate calibration of users' trust in AI systems in developing regions.

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [512] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

TL;DR: This study analyzes reflections from 33 student teams using AI tools in an HCI design course, identifying new types of design judgments.


<details>
  <summary>Details</summary>
Motivation: To understand the kinds of judgments students make when engaging with AI tools as collaborators rather than just aids.

Method: Analyzed reflections from 33 student teams in an HCI design course who used AI tools.

Result: Identified established and emergent types of design judgment including agency-distribution and reliability judgment.

Conclusion: Generative AI adds complexity to design reasoning, prompting reflection on AI's outputs and when to rely on it; a conceptual lens is provided for understanding co-creative sensemaking with AI.

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [513] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

TL;DR: PreCare is a website with AI-driven assistants designed to guide users through Advance Care Planning (ACP). It significantly improved personal value exploration, knowledge and decisional confidence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap of online ACP which lacks personalized value exploration and immediate clarification of decision consequences.

Method: Conducted two formative studies and designed PreCare in collaboration with ACP professionals. PreCare has three AI-driven assistants to help users explore personal values, gain ACP knowledge, and support informed decision-making.

Result: A usability study showed that PreCare achieved an excellent System Usability Scale (SUS) rating. A comparative evaluation showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.

Conclusion: PreCare successfully bridges the gap between online ACP and clinical consultations.

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [514] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

TL;DR: This paper explores 'default images' in text-to-image generation using Midjourney, providing insights into their impact and suggesting ways to improve the technology.


<details>
  <summary>Details</summary>
Motivation: To understand why and how default images are generated in text-to-image models, especially when prompts contain unknown terms, to enhance TTI solutions and prompt engineering.

Method: A systematic approach was developed to create input prompts that trigger default images on Midjourney. Initial experiments, small-scale ablation studies, and a survey were conducted to investigate their characteristics and effects on user satisfaction.

Result: The study reveals insights into default images, including how they affect user satisfaction, and identifies challenges for improving TTI models.

Conclusion: Investigating default images is crucial for advancing text-to-image generation technology, offering a foundation for future research and improvements.

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [515] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

TL;DR: The study explores the impact of generative AI on engineering students' learning experience in China, highlighting opportunities and challenges.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI affects engineering students' learning experiences and to identify the opportunities and challenges it presents in higher education.

Method: Surveyed 148 students from diverse engineering disciplines across China regarding their use of generative AI, its impact on learning, and associated challenges.

Result: More than half of the participants reported positive effects on learning efficiency, initiative, and creativity, with nearly half also noting improved independent thinking. However, many felt academic performance was largely unchanged and expressed concerns about AI accuracy and reliability.

Conclusion: Generative AI offers significant benefits to engineering students but also poses challenges. Recommendations are provided for integrating AI effectively into engineering education.

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


### [516] [Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents](https://arxiv.org/abs/2505.09757)
*Botao Amber Hu,Yuhan Liu,Helena Rong*

Main category: cs.HC

TL;DR: The study explores the challenges and governance issues in Decentralized AI Agents (DeAgents) that combine AI with blockchain technologies, interviewing stakeholders to understand motivations, benefits, and dilemmas for guiding future design and discussions.


<details>
  <summary>Details</summary>
Motivation: To address the empirical research gap concerning the paradoxical tension between trustlessness and unreliable autonomy in DeAgents, which combines LLM-based AI agents with decentralization technologies.

Method: Through interviews with DeAgents stakeholders including experts, founders, and developers to examine their motivations, perceived benefits, and governance challenges.

Result: The findings will provide insights to guide the future design of DeAgents systems and protocols, as well as inform discussions on governance within sociotechnical AI systems.

Conclusion: This research aims to navigate the complex landscape of decentralized AI agents by offering guidance for system design and fostering discussions on governance mechanisms.

Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents)
combines Large Language Model (LLM)-based AI agents with decentralization
technologies such as blockchain smart contracts and trusted execution
environments (TEEs). These tamper-resistant trustless substrates allow agents
to achieve self-sovereignty through ownership of cryptowallet private keys and
control of digital assets and social media accounts. DeAgent eliminates
centralized control and reduces human intervention, addressing key trust
concerns inherent in centralized AI systems. However, given ongoing challenges
in LLM reliability such as hallucinations, this creates paradoxical tension
between trustlessness and unreliable autonomy. This study addresses this
empirical research gap through interviews with DeAgents stakeholders-experts,
founders, and developers-to examine their motivations, benefits, and governance
dilemmas. The findings will guide future DeAgents system and protocol design
and inform discussions about governance in sociotechnical AI systems in the
future agentic web.

</details>


### [517] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)
*Ruichen Yang,György M. Lévay,Christopher L. Hunt,Dániel Czeiner,Megan C. Hodgson,Damini Agarwal,Rahul R. Kaliki,Nitish V. Thakor*

Main category: cs.HC

TL;DR: The paper introduces the Reviewer, a 3D visual interface for EMG signal translation that enhances myoelectric prosthesis control through structured feedback and mutual adaptation. Study results show improved performance in completion rates, path efficiency, and throughput compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge users face in producing distinct EMG patterns for reliable classification as prosthesis movement complexity increases, and to improve upon existing training methods that rely on heuristic, trial-and-error adjustments.

Method: A 10-session study with 12 able-bodied participants comparing PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task.

Result: Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group.

Conclusion: The 3D visual feedback provided by the Reviewer significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments.

Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.

</details>


### [518] [SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition](https://arxiv.org/abs/2505.10312)
*Anh Tuan Ha,Hoang Khang Phan,Thai Minh Tien Ngo,Anh Phan Truong,Nhat Tan Le*

Main category: cs.HC

TL;DR: In Human Activity Recognition (HAR), generating high-quality data and handling data heterogeneity are challenges. This study uses deep learning methods (Attention Autoencoder and conditional GANs) to create a generation dataset, and shuffles data sequences to homogenize distribution. Experiments show this random sequence strategy improves classification performance with accuracy up to 0.70 ± 0.03 and macro F1 score of 0.64 ± 0.01. Disrupting temporal dependencies forces the model to focus on instant recognition, improving robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of obtaining high-quality and diverse data in HAR, as well as dealing with data heterogeneity.

Method: Used Attention Autoencoder and conditional Generative Adversarial Networks to generate a dataset, and applied a random sequence strategy to shuffle data for homogenizing distribution.

Result: Classification performance significantly improved with accuracy up to 0.70 ± 0.03 and macro F1 score of 0.64 ± 0.01.

Conclusion: This approach not only broadens the effective training dataset but also enhances HAR systems' robustness in complex real-world scenarios.

Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.

</details>


### [519] [AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages](https://arxiv.org/abs/2505.10300)
*Muzhe Wu,Yanzhi Zhao,Shuyi Han,Michael Xieyang Liu,Hong Shen*

Main category: cs.HC

TL;DR: To address the challenge of transferring technical design rationales for ethical evaluation, the authors developed AI LEGO, a web-based prototype that facilitates knowledge handoff and helps identify harmful design choices in early AI development stages. It uses interactive blocks and persona simulations, leading to more effective harm identification compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the persistent knowledge handoff challenge in cross-functional industry teams working on Responsible AI (RAI). This involves transferring high-level technical design rationales from technical experts to non-technical roles for ethical evaluation and harm identification early in the AI development lifecycle.

Method: The method involved literature review and a co-design study with 8 practitioners to understand the current challenges. Based on insights, they developed AI LEGO, a web-based prototype. Technical roles use interactive blocks to draft development plans, while non-technical roles engage through checklists and LLM-driven persona simulations to identify potential harms.

Result: In a study with 18 cross-functional practitioners, AI LEGO increased the volume and likelihood of harms identified compared to baseline worksheets. Participants found its modular structure and persona prompts made harm identification more accessible and fostered clearer RAI practices.

Conclusion: AI LEGO effectively supports cross-functional AI practitioners in facilitating knowledge handoff and identifying harmful design choices early in the development process, leading to more collaborative RAI practices.

Abstract: Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [520] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

TL;DR: 准确的血糖预测对1型糖尿病治疗至关重要。本研究探索了transformer模型在血糖预测中的潜力，使用DCLP3和OhioT1DM数据集进行训练、验证和测试。结果表明，采用patch-wise方法的Crossformer和PatchTST模型分别在短期（30分钟）和中长期（1小时、2小时、4小时）预测中表现最佳，展示了transformer架构通过捕捉多变量时间序列数据中的季节性模式以提高预测精度的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的架构在复杂多变量时间序列预测中展现了强大的注意力机制，但其在血糖预测领域的潜力尚未被充分挖掘。准确的血糖预测可以为1型糖尿病的个性化治疗提供支持，包括胰岛素和饮食调整等干预措施。

Method: 研究采用了公开的DCLP3数据集（n=112）按80%-10%-10%的比例划分训练、验证和测试集，并使用OhioT1DM数据集（n=12）作为外部测试集。训练了具有点式、块式、序列式和混合嵌入方式的网络模型，利用连续血糖监测（CGM）、胰岛素和餐食数据进行多步血糖预测。

Result: 对于短期（30分钟）血糖预测，Crossformer模型表现出色，均方根误差（RMSE）为15.6 mg/dL。而对于较长期预测（1小时、2小时、4小时），PatchTST模型表现最佳，RMSE分别为24.6 mg/dL、36.1 mg/dL和46.5 mg/dL。总体而言，使用分块标记化的模型在更大的输入尺寸下显示出更高的准确性，其中一周的历史数据提供了最佳结果。

Conclusion: 基于transformer的架构在血糖预测中展现出巨大潜力，能够通过捕捉多变量时间序列数据中的季节性模式来提高预测准确性，为1型糖尿病患者的个性化治疗提供了有力支持。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


### [521] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: The study compares LLM-based clustering methods with classical methods on pediatric sepsis data from a low-income country, finding that LLMs, especially Stella-En-400M-V5, outperform classical techniques in capturing richer context and identifying distinct patient subgroups.


<details>
  <summary>Details</summary>
Motivation: Clustering patient subgroups is crucial for personalized care and efficient resource use, but traditional methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding.

Method: Patient records were serialized into text with and without a clustering objective. Embeddings were generated using three LLMs: quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with LoRA, and Stella-En-400M-V5. K-means clustering was applied to these embeddings. Classical methods included K-Medoids clustering on UMAP and FAMD-reduced mixed data.

Result: Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles.

Conclusion: LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features, showing potential for contextual phenotyping and informed decision-making in resource-limited settings.

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


### [522] [Generative diffusion model surrogates for mechanistic agent-based biological models](https://arxiv.org/abs/2505.09630)
*Tien Comlekoglu,J. Quetzalcóatl Toledo-Marín,Douglas W. DeSimone,Shayn M. Peirce,Geoffrey Fox,James A. Glazier*

Main category: q-bio.QM

TL;DR: This paper explores the use of denoising diffusion probabilistic models to create a generative AI surrogate for the Cellular-Potts Model (CPM), which is used to study in vitro vasculogenesis. The surrogate model significantly reduces computational time and aids in model selection and verification.


<details>
  <summary>Details</summary>
Motivation: The Cellular-Potts Model, while powerful for modeling biological systems at single-cell resolution, becomes computationally expensive at large scales. There's a need for accelerated evaluation methods without losing accuracy or detail.

Method: Denoising diffusion probabilistic models are employed to train a generative AI surrogate of the CPM focused on in vitro vasculogenesis. An image classifier is used to understand parameter space characteristics aiding in surrogate model selection and verification.

Result: The surrogate model successfully generates configurations 20,000 timesteps ahead with about a 22x reduction in computational time compared to native code execution.

Conclusion: This work marks progress in applying DDPMs for creating digital twins of stochastic biological systems, offering significant computational advantages.

Abstract: Mechanistic, multicellular, agent-based models are commonly used to
investigate tissue, organ, and organism-scale biology at single-cell
resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework
for developing and interrogating these models. CPMs become computationally
expensive at large space- and time- scales making application and investigation
of developed models difficult. Surrogate models may allow for the accelerated
evaluation of CPMs of complex biological systems. However, the stochastic
nature of these models means each set of parameters may give rise to different
model configurations, complicating surrogate model development. In this work,
we leverage denoising diffusion probabilistic models to train a generative AI
surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We
describe the use of an image classifier to learn the characteristics that
define unique areas of a 2-dimensional parameter space. We then apply this
classifier to aid in surrogate model selection and verification. Our CPM model
surrogate generates model configurations 20,000 timesteps ahead of a reference
configuration and demonstrates approximately a 22x reduction in computational
time as compared to native code execution. Our work represents a step towards
the implementation of DDPMs to develop digital twins of stochastic biological
systems.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [523] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

TL;DR: This paper introduces ARCANE, the first framework for early ICME detection in streaming solar wind data under realistic operational constraints. It compares a machine learning-based method (ResUNet++) with a threshold-based baseline and finds that ResUNet++ significantly outperforms the baseline, especially in detecting high-impact events. Using real-time solar wind data leads to only minimal performance degradation compared to high-resolution science data.


<details>
  <summary>Details</summary>
Motivation: Interplanetary coronal mass ejections (ICMEs) are major drivers of space weather disturbances, posing risks to technological infrastructure and human activities. There is a need for robust real-time detection of ICMEs in solar wind in situ data for early warning systems.

Method: The authors developed ARCANE, which evaluates the strengths and limitations of detection models by comparing a machine learning-based method (ResUNet++) to a threshold-based baseline. They use real-time solar wind (RTSW) data instead of high-resolution science data.

Result: The ResUNet++ model significantly outperforms the baseline, particularly in detecting high-impact events, while retaining solid performance on lower-impact cases. The detection pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5% of the event's duration. As more data becomes available, the performance increases significantly.

Conclusion: ARCANE marks a substantial step forward in automated space weather monitoring and lays the groundwork for enhanced real-time forecasting capabilities.

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [524] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

TL;DR: Accurate wind power forecasting is crucial for grid stability and renewable energy integration. Conventional GP models with stationary kernels are inadequate for modeling the non-stationary nature of wind speed and power output. This paper proposes a non-stationary GP framework incorporating the GSM kernel to capture time-varying patterns and heteroscedastic behaviors in wind data. Evaluated on real-world SCADA data, the GSM-based model outperforms standard kernels especially in short-term forecasts.


<details>
  <summary>Details</summary>
Motivation: Wind power forecasting is essential for maintaining grid stability and integrating renewable energy sources. However, existing GP models with stationary kernels cannot adequately model the non-stationary characteristics of wind speed and power output.

Method: The paper introduces a non-stationary GP framework that uses the generalized spectral mixture (GSM) kernel. This allows the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data.

Result: The proposed GSM-based model was evaluated on real-world SCADA data across different forecasting horizons. It showed superior performance compared to standard radial basis function and spectral mixture kernels, particularly in short-term forecasts.

Conclusion: Modeling non-stationarity is necessary for accurate wind power forecasting. The proposed non-stationary GP model demonstrates practical value in operational settings.

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


### [525] [Pure Component Property Estimation Framework Using Explainable Machine Learning Methods](https://arxiv.org/abs/2505.09783)
*Jianfeng Jiao,Xi Gao,Jie Li*

Main category: stat.AP

TL;DR: An enhanced framework using explainable machine learning for predicting pure component physiochemical properties is proposed, reducing feature count without compromising accuracy and aligning with mechanistic interpretations.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of pure component physiochemical properties is crucial for process integration, multiscale modeling, and optimization.

Method: The framework uses molecular representation based on connectivity matrix for feature generation, random forest for feature ranking and pooling, and employs Shapley values for feature analysis. Models like Artificial Neural Network and Gaussian Process Regression are used for predictions.

Result: The root-mean-square error on the test set was reduced by up to 83.8% compared to GC based models. The number of features was reduced from 13316 to 100 without loss in model accuracy.

Conclusion: The proposed framework is feasible and provides a solid foundation for mixture component reconstruction and process integration modelling.

Abstract: Accurate prediction of pure component physiochemical properties is crucial
for process integration, multiscale modeling, and optimization. In this work,
an enhanced framework for pure component property prediction by using
explainable machine learning methods is proposed. In this framework, the
molecular representation method based on the connectivity matrix effectively
considers atomic bonding relationships to automatically generate features. The
supervised machine learning model random forest is applied for feature ranking
and pooling. The adjusted R2 is introduced to penalize the inclusion of
additional features, providing an assessment of the true contribution of
features. The prediction results for normal boiling point (Tb), liquid molar
volume, critical temperature (Tc) and critical pressure (Pc) obtained using
Artificial Neural Network and Gaussian Process Regression models confirm the
accuracy of the molecular representation method. Comparison with GC based
models shows that the root-mean-square error on the test set can be reduced by
up to 83.8%. To enhance the interpretability of the model, a feature analysis
method based on Shapley values is employed to determine the contribution of
each feature to the property predictions. The results indicate that using the
feature pooling method reduces the number of features from 13316 to 100 without
compromising model accuracy. The feature analysis results for Tb, Tc, and Pc
confirms that different molecular properties are influenced by different
structural features, aligning with mechanistic interpretations. In conclusion,
the proposed framework is demonstrated to be feasible and provides a solid
foundation for mixture component reconstruction and process integration
modelling.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [526] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

TL;DR: The paper introduces EDBench, a large-scale dataset of electron density (ED) data covering 3.3 million molecules to advance learning-based research in molecular machine learning force fields (MLFFs). It provides benchmark tasks for prediction, retrieval, and generation, demonstrating feasible and accurate ED calculation with reduced computational cost compared to DFT.


<details>
  <summary>Details</summary>
Motivation: Existing MLFFs focus on atoms, molecules, and simple quantum chemical properties but overlook the importance of electron density (ED), which is crucial for understanding molecular force fields according to the Hohenberg-Kohn theorem. The lack of large-scale ED data limits its application in MLFFs due to time-consuming DFT calculations.

Method: The authors introduce EDBench, a dataset built upon PCQM4Mv2 that includes accurate ED data for 3.3 million molecules. They design ED-centric benchmark tasks for prediction, retrieval, and generation to evaluate models' ability to understand electronic information.

Result: Evaluation shows that learning from EDBench is feasible and achieves high accuracy. Learning-based methods can calculate ED efficiently with comparable precision while significantly reducing computational cost relative to traditional DFT calculations.

Conclusion: EDBench provides a robust foundation for ED-driven drug discovery and materials science by offering freely available data and benchmarks.

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [527] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

TL;DR: RNA语言模型被分为三类，并与DNA和蛋白质语言模型一起在零样本RNA二级结构预测和功能分类中进行了比较。结果表明，擅长二级结构预测的模型在功能分类上表现较差，反之亦然，提示需要更平衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型在结构和功能推断中的有效性，RNA语言模型在过去几年才开始受到更多关注，但缺乏统一的标准进行比较。

Method: 将RNA语言模型分为三类：在多种RNA类型（特别是非编码RNA）上预训练的模型、特定用途RNA模型以及将RNA与DNA或蛋白质或两者统一起来的模型。然后使用13个RNA语言模型、3个DNA模型和1个蛋白质模型作为对照，在零样本RNA二级结构预测和功能分类任务中进行比较。

Result: 擅长二级结构预测的模型在功能分类任务上的表现往往较差，反之亦然。

Conclusion: 需要更平衡的无监督训练来提高RNA语言模型在不同任务上的综合性能。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [528] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/abs/2505.09651)
*Xixuan Hao,Yutian Jiang,Xingchen Zou,Jiabo Liu,Yifang Yin,Yuxuan Liang*

Main category: cs.DB

TL;DR: Location Intelligence (LI) evolves through two revolutions: deep learning and large language models (LLMs). This survey reviews geospatial representation learning across both eras, organizing advancements into data, methodology, and application perspectives. It highlights current progress, limitations, and future research directions in the LLM era.


<details>
  <summary>Details</summary>
Motivation: The field of Location Intelligence is being reshaped by advancements in deep learning and large language models, offering new capabilities for processing structured and unstructured geospatial data.

Method: The paper provides a comprehensive review of geospatial representation learning, categorizing it into a taxonomy based on data perspective, methodological perspective, and application perspective across two technological eras.

Result: The survey outlines current advancements and limitations in geospatial representation learning and suggests potential future research directions, particularly in the context of LLMs.

Conclusion: This work aims to provide a roadmap for further innovation in Location Intelligence, summarizing key developments and pointing towards promising areas for exploration.

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [529] [Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts](https://arxiv.org/abs/2505.09798)
*Bojan Ristov,Stefan Eftimov,Milena Trajanoska,Dimitar Trajanov*

Main category: cs.DB

TL;DR: The paper explores transforming traditional procurement data into a semantic knowledge graph using ontological modeling and automated data transformation techniques, enhancing data transparency, decision-making, and analysis in public procurement.


<details>
  <summary>Details</summary>
Motivation: Traditional procurement data is often stored in rigid, tabular formats which limits its analytical potential and hinders transparency.

Method: Transform structured procurement data into a semantic knowledge graph by leveraging ontological modeling and automated data transformation techniques. Integrate RDF and SPARQL-based querying and incorporate machine learning-driven predictive modeling.

Result: Improved data transparency, support for evidence-based decision-making, and enabled in-depth analysis of procurement activities in North Macedonia.

Conclusion: This work contributes to the field of public procurement intelligence by demonstrating methods to enhance accessibility, interpretability, and analytics of procurement records.

Abstract: Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [530] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

TL;DR: The paper introduces ELIS, a serving system for LLMs with an ISRTF scheduler that cuts average job completion time by up to 19.6%.


<details>
  <summary>Details</summary>
Motivation: Current LLM serving systems use first-come-first-served scheduling, causing the 'head-of-line blocking' problem.

Method: ELIS uses a trained response length predictor and an ISRTF scheduling strategy to efficiently manage inference tasks.

Result: ELIS reduces the average job completion time by up to 19.6% in experimental results.

Conclusion: ELIS is a cloud-native scheduler system on Kubernetes that overcomes limitations of current LLM serving systems.

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [531] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

TL;DR: DeepSeek-V3通过硬件感知模型协同设计解决了当前硬件架构中的关键限制，如内存容量、计算效率和互连带宽。其创新包括多头潜在注意力（MLA）、专家混合（MoE）架构、FP8混合精度训练和多平面网络拓扑。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展揭示了当前硬件架构的关键限制，需要一种新的方法来实现成本效益的训练和推理。

Method: 使用Multi-head Latent Attention提高内存效率，Mixture of Experts优化计算-通信权衡，FP8 mixed-precision training充分利用硬件能力，以及Multi-Plane Network Topology减少集群级别的网络开销。

Result: DeepSeek-V3在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型协同设计的有效性，能够实现大规模的成本效益训练和推理。

Conclusion: 硬件与模型的协同设计对于满足日益增长的人工智能工作负载需求至关重要，为下一代人工智能系统提供了实际的创新蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


### [532] [AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron](https://arxiv.org/abs/2505.09989)
*Tella Rajashekhar Reddy,Palak,Rohan Gandhi,Anjaly Parayil,Chaojie Zhang,Mike Shepperd,Liangcheng Yu,Jayashree Mohan,Srinivasan Iyengar,Shivkumar Kalyanaraman,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: This paper proposes to bring AI workload to modular compute clusters co-located in wind farms, and introduces Heron, a cross-site software router that routes AI inferencing workload according to power generation, improving the aggregate goodput of AI compute by up to 80%.


<details>
  <summary>Details</summary>
Motivation: AI power demand is growing unprecedentedly while abundant wind power awaits grid access. This drives the motivation to explore bringing AI workloads closer to renewable energy sources.

Method: Deploying AI workloads in modular compute clusters co-located in wind farms, using a deployment right-sizing strategy. Developing Heron, a cross-site software router, to leverage complementarity of power generation across wind farms by routing AI inferencing workload around power drops.

Result: Heron improves aggregate goodput of AI compute by up to 80% compared to the state-of-the-art when tested with 1-week production traces from Azure and real variable wind power traces.

Conclusion: Bringing AI workloads to wind farms through modular compute clusters and leveraging Heron for efficient workload routing can significantly improve AI compute goodput while utilizing cheap, green power.

Abstract: AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.

</details>


### [533] [KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems](https://arxiv.org/abs/2505.10183)
*Jieke Lin,Wanyu Wang,Longxiang Yin,Yinhe Han*

Main category: cs.DC

TL;DR: KAITIAN is a new distributed communication framework that improves resource utilization and scalability in heterogeneous AI systems.


<details>
  <summary>Details</summary>
Motivation: Embodied AI systems require diverse accelerators to meet real-time and energy-efficiency demands, but vendor-specific libraries create interoperability barriers hindering performance.

Method: KAITIAN provides a unified abstraction layer integrating vendor-optimized libraries for intra-group efficiency and general-purpose protocols for inter-group interoperability, with a load-adaptive scheduling mechanism.

Result: KAITIAN accelerates training time by up to 42% compared to baseline homogeneous systems with minimal communication overhead (2.8--4.3%) and maintained model accuracy.

Conclusion: KAITIAN enhances flexible and powerful heterogeneous computing for complex embodied AI applications.

Abstract: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [534] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

TL;DR: The paper proposes spherical attention algorithm and FlashSign kernel using neural circuit diagrams to enable systematic reasoning about deep learning architectures. It overcomes bottlenecks in standard attention mechanisms and achieves high performance.


<details>
  <summary>Details</summary>
Motivation: To enable artificial intelligence models to improve themselves by developing a principled and systematic method for reasoning about deep learning architectures.

Method: Using neural circuit diagrams based in category theory, the authors prove a general theorem related to deep learning algorithms, develop a novel attention algorithm (spherical attention) for gene regulatory networks, and produce an efficient kernel (FlashSign).

Result: The proposed spherical attention algorithm overcomes the special function unit bottleneck of standard attention while retaining essential streaming properties. The FlashSign kernel achieves 3.6x the performance of PyTorch and is comparable to state-of-the-art FlashAttention on an A100 GPU.

Conclusion: Neural circuit diagrams are suitable as a high-level framework for the automated development of efficient, novel artificial intelligence architectures.

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [535] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

TL;DR: The paper introduces SALM, a new framework that integrates language models into social network simulations. It features a hierarchical prompting architecture, an attention-based memory system, and formal bounds on personality stability, enabling stable long-term simulations with reduced token usage and efficient memory management.


<details>
  <summary>Details</summary>
Motivation: Current agent-based modeling methods for social systems rely heavily on rule-based behaviors, which limits their ability to capture complex dynamics. The motivation is to move beyond predefined rules by leveraging contextual understanding from language models of human social interaction.

Method: The method involves creating SALM (Social Agent LM Framework), which includes a hierarchical prompting architecture for stable simulation, an attention-based memory system for efficient caching, and formal bounds on personality stability.

Result: SALM achieves unprecedented temporal stability in multi-agent scenarios, reducing token usage by 73% and achieving 80% cache hit rates with sub-linear memory growth. It is validated against SNAP ego networks, demonstrating its capability to model long-term social phenomena accurately.

Conclusion: SALM represents a significant advancement in integrating language models into social network simulations, providing a robust framework for modeling long-term social dynamics with high behavioral fidelity.

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>


### [536] [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
*Sulong Zhou,Qunying Huang,Shaoheng Zhou,Yun Hang,Xinyue Ye,Aodong Mei,Kathryn Phung,Yuning Ye,Uma Govindswamy,Zehan Li*

Main category: cs.SI

TL;DR: This study analyzes Reddit discourse during the 2025 Los Angeles wildfires using topic modeling methods enhanced by LLMs and HITL refinement, identifying two main categories of topics (Situational Awareness and Crisis Narratives) and contributing an annotated social media dataset.


<details>
  <summary>Details</summary>
Motivation: To understand how affected populations perceive and respond during wildfire crises for timely and empathetic disaster response.

Method: Adopting topic modeling methods enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement to analyze Reddit posts and comments related to the Palisades and Eaton fires.

Result: Identified two main categories of latent topics: Situational Awareness (SA) and Crisis Narratives (CN). SA closely aligns with real-world fire progressions and peaks within the first 2-5 days. CN includes grief signals and mental health risks, with the highest volume occurring at night.

Conclusion: Contributed the first annotated social media dataset on the 2025 LA fires and introduced a scalable multi-layer framework for crisis discourse analysis, which can inform more empathetic and adaptive strategies for disaster response.

Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.

</details>


### [537] [Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion](https://arxiv.org/abs/2505.10197)
*Anjali de Silva,Gang Chen,Hui Ma,Seyed Mohammad Nekooei,Xingquan Zuo*

Main category: cs.SI

TL;DR: 社区检测技术对于实际应用至关重要，它通过利用社交网络中的拓扑和属性相似性来发现连贯的节点组（社区）。现有的图卷积网络（GCNs）在最大化模块度时通常收敛到次优解。此外，直接使用人工标注的社区进行训练可能会通过仅基于节点属性对不相连的节点进行分组而削弱拓扑连贯性。为了解决这些问题，我们提出了一个新的基于拓扑和属性相似性的社区检测（TAS-Com）方法。TAS-Com引入了一种新的损失函数，利用高效且可扩展的Leiden算法来检测具有全局最优模块度的社区结构。进一步利用Leiden算法优化人工标注的社区，以确保每个社区内的连通性，使TAS-Com能够检测到在模块度和符合人工标签之间具有良好权衡的社区结构。多个基准网络上的实验结果证实，TAS-Com可以显著优于几种最先进的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积网络（GCNs）在最大化模块度时通常收敛到次优解，并且直接使用人工标注的社区进行训练可能会通过仅基于节点属性对不相连的节点进行分组而削弱拓扑连贯性。

Method: 提出了一种新的基于拓扑和属性相似性的社区检测（TAS-Com）方法。该方法引入了一种新的损失函数，利用高效且可扩展的Leiden算法来检测具有全局最优模块度的社区结构，并进一步利用Leiden算法优化人工标注的社区，以确保每个社区内的连通性。

Result: 多个基准网络上的实验结果证实，TAS-Com可以显著优于几种最先进的算法。

Conclusion: TAS-Com方法能够有效解决现有GCNs收敛到次优解以及人工标注社区可能削弱拓扑连贯性的问题，在社区检测任务中表现出色。

Abstract: Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [538] [$XX^{t}$ Can Be Faster](https://arxiv.org/abs/2505.09814)
*Dmitry Rybin,Yushun Zhang,Zhi-Quan Luo*

Main category: cs.DS

TL;DR: The paper introduces RXTX, a novel algorithm that computes the product of a matrix by its transpose using fewer multiplications and additions than current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithm that can compute the product of a matrix by its transpose more efficiently in terms of reducing the number of multiplications and additions.

Method: By integrating Machine Learning-based search methods with Combinatorial Optimization techniques to discover the new algorithm RXTX.

Result: RXTX uses 5% less multiplications and additions than the State-of-the-Art, providing accelerations even for small sizes of matrix X.

Conclusion: RXTX is a more efficient algorithm for computing the product of a matrix by its transpose.

Abstract: We present a new algorithm RXTX that computes product of matrix by its
transpose $XX^{t}$. RXTX uses $5\%$ less multiplications and additions than
State-of-the-Art and achieves accelerations even for small sizes of matrix $X$.
The algorithm was discovered by combining Machine Learning-based search methods
with Combinatorial Optimization.

</details>


### [539] [On Unbiased Low-Rank Approximation with Minimum Distortion](https://arxiv.org/abs/2505.09647)
*Leighton Pate Barnes,Stephen Cameron,Benjamin Howard*

Main category: cs.DS

TL;DR: The paper presents an algorithm for sampling a low-rank random matrix that optimally approximates a fixed target matrix in terms of expected Frobenius norm error.


<details>
  <summary>Details</summary>
Motivation: To develop an optimal method for approximating a fixed target matrix with a low-rank random matrix, ensuring unbiasedness and minimizing the expected Frobenius norm error.

Method: The algorithm mirrors the solution to the efficient unbiased sparsification problem for vectors but is applied to the singular components of the matrix P. It ensures that Q is unbiased, has a rank less than or equal to r, and minimizes the expected Frobenius norm error.

Result: The algorithm achieves optimality by matching the error from an existing lower bound.

Conclusion: The presented algorithm provides an optimal way to sample a low-rank random matrix that best approximates a given target matrix.

Abstract: We describe an algorithm for sampling a low-rank random matrix $Q$ that best
approximates a fixed target matrix $P\in\mathbb{C}^{n\times m}$ in the
following sense: $Q$ is unbiased, i.e., $\mathbb{E}[Q] = P$;
$\mathsf{rank}(Q)\leq r$; and $Q$ minimizes the expected Frobenius norm error
$\mathbb{E}\|P-Q\|_F^2$. Our algorithm mirrors the solution to the efficient
unbiased sparsification problem for vectors, except applied to the singular
components of the matrix $P$. Optimality is proven by showing that our
algorithm matches the error from an existing lower bound.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [540] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

TL;DR: 提出了一种基于最小均方误差（MMSE）估计的对抗性评估框架，用于敏感特征推断，并通过理论下界和实证评估展示了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个有效的评估框架来衡量从其他相关特征的噪声观测中推断敏感特征的准确性，并理解有限样本和受限假设类对推断的影响。

Method: 提出了基于MMSE估计的对抗性评估框架，该框架包括理论下界的推导、闭式界表达以及在不同特征关系下的逼近误差分析。还引入了一个基于验证数据集的新下界。

Result: 推导出的理论下界能够准确反映敏感特征推断的性能限制，并且通过实证评估证明了框架的有效性与实用性。

Conclusion: 提出的框架为敏感特征推断提供了平衡理论保证与实际效率的有效工具，适用于线性预测模型及多种特征关系。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [541] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

TL;DR: This paper examines risk bounds for nonparametric distributional regression estimators, establishing upper bounds for CRPS and MSE under convex constraints, and deriving a general bound for non-convex constraints with neural network-based estimators. Experiments validate the theoretical contributions.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of the performance guarantees (risk bounds) for nonparametric distributional regression estimators, particularly focusing on CRPS and MSE metrics under different types of constraints.

Method: The authors develop general upper bounds for the CRPS and worst-case MSE in the context of convex-constrained distributional regression. They also explore isotonic and trend filtering distributional regression, and derive a general upper bound for non-convex constrained distributional regression, including neural network-based methods.

Result: Established upper bounds match convergence rates seen in mean estimation problems. Theoretical results were validated through comprehensive experiments on both simulated and real data, showing practical effectiveness.

Conclusion: The study successfully provides risk bounds for various forms of distributional regression, extending to non-convex settings such as neural networks, and demonstrates their practical relevance through empirical validation.

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [542] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

TL;DR: 研究了具有符号激活函数的前馈神经网络的在线学习，探讨了不同条件下的错误界限以及如何通过增加网络限制来减少对维度d的依赖。


<details>
  <summary>Details</summary>
Motivation: 研究具有符号激活函数的前馈神经网络的在线学习能力，特别是分析其在分类任务中的错误界限及影响因素。

Method: 1. 定义并分析了一个边距条件，该条件对于在线学习神经网络是充分且在某些情况下是必要的。
2. 证明了任何网络的最优错误界限大约为$(d,\gamma)$-完全可分离包装数$\mathtt{TS}(d,\gamma)$。
3. 构造了一个网络实例，展示任何学习算法都会犯$\mathtt{TS}(d,\gamma)$次错误。
4. 提出了两个额外的网络限制：多指标模型和扩展边距假设，以减少对维度d的依赖，并分别给出了相应的错误界限。

Result: 1. 在一般情况下，错误界限与维度d呈指数关系。
2. 在多指标模型中，错误界限与输入方向数量k有关，而与维度d无关。
3. 在扩展边距假设下，错误界限与网络深度L和标签数量Y有关，减少了对维度d的依赖。

Conclusion: 通过引入额外的自然限制（如多指标模型和扩展边距假设），可以有效减少错误界限对输入维度d的依赖，从而改善高维数据上的在线学习性能。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [543] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

TL;DR: Optimal Transport (OT) is used for domain adaptation in supervised settings with linear regression models, especially when domains differ by rotations. The paper proposes an algorithm combining K-means, OT, and SVD to estimate the rotation angle and adapt the model, demonstrating effectiveness in sparsely sampled target domains.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is crucial when source and target domains have differing statistical properties, such as rotations. Current methods lack a specific approach for adapting linear regression models under rotational shifts.

Method: The method involves using Optimal Transport theory to recover the underlying rotation between domains in $\mathbb{R}^2$ with p-norm cost ($p \ge 2$). An algorithm is proposed that combines K-means clustering, OT, and SVD to estimate the rotation angle and adapt the regression model.

Result: The proposed algorithm effectively estimates the rotation angle and adapts the regression model, particularly excelling in scenarios where the target domain is sparsely sampled. This leverages the abundant source data to improve generalization.

Conclusion: This work provides both theoretical understanding of OT-based model adaptation under geometric transformations and practical solutions for domain adaptation in supervised settings involving rotations.

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [544] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

TL;DR: This paper explores the classification of functional data with fairness constraints, proposing a unified framework and a post-processing algorithm called Fair-FLDA. It provides theoretical guarantees on fairness and excess risk controls, supported by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically study fair classification in functional data while controlling disparity levels below a pre-specified threshold, addressing challenges like absence of density ratios and intractability of posterior probabilities.

Method: A unified framework for fairness-aware functional classification is proposed. A post-processing algorithm named Fair-FLDA is designed for homoscedastic Gaussian processes using group-wise thresholding.

Result: Theoretical guarantees on fairness and excess risk controls are established under weak structural assumptions. The results also cover the excess risk control of standard FLDA as a special case.

Conclusion: The findings are supported by extensive numerical experiments on both synthetic and real datasets, demonstrating the practicality of the Fair-FLDA algorithm.

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [545] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

TL;DR: Offline reinforcement learning using pre-collected data faces challenges when dealing with heterogeneous data. This paper proposes an individualized offline policy optimization framework for heterogeneous MDPs, introducing a model with individual latent variables and the P4L algorithm to efficiently estimate individual Q-functions and guarantee a fast rate on average regret.


<details>
  <summary>Details</summary>
Motivation: Traditional methods in offline RL focus on learning optimal policies from homogeneous data, which may lead to suboptimal results for heterogeneous populations.

Method: The authors propose an individualized offline policy optimization framework for heterogeneous time-stationary MDPs that includes a heterogeneous model with individual latent variables and the Penalized Pessimistic Personalized Policy Learning (P4L) algorithm.

Result: Simulation studies and real data application show superior numerical performance of the proposed method compared to existing methods under weak partial coverage assumption on behavior policies.

Conclusion: The proposed individualized offline policy optimization framework effectively addresses the challenge of learning from heterogeneous data in offline RL.

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [546] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

TL;DR: The paper introduces Deep-SITAR, a supervised deep learning framework combining autoencoder architecture with the SITAR model to predict growth trajectories.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing methods for capturing the complexity and nonlinearity of human growth by integrating deep learning techniques with traditional SITAR models.

Method: Development of the Deep-SITAR model which uses an autoencoder architecture that includes a deep neural network and B-spline model. The encoder estimates random effects for individuals, while the decoder fits data using B-splines.

Result: Deep-SITAR allows for predicting random effects of new individuals without re-estimating the full model, providing a flexible and efficient approach.

Conclusion: Deep-SITAR is a powerful method for predicting growth trajectories, merging deep learning flexibility with the interpretability of mixed-effects models.

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [547] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

TL;DR: This paper introduces AWNN, an adaptively weighted nearest neighbor method for matrix completion. It balances bias and variance without needing cross-validation, providing theoretical guarantees and synthetic experiment support.


<details>
  <summary>Details</summary>
Motivation: Existing nearest neighbor methods lack a systematic approach to choosing radii and weights without using cross-validation.

Method: AWNN method which judiciously balances the bias variance trade off in weighted nearest-neighbor regression.

Result: Theoretical guarantees for AWNN are provided under minimal assumptions, supported by synthetic experiments.

Conclusion: AWNN is an effective method for matrix completion that addresses the challenge of choosing radii and weights systematically.

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


### [548] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha,Dhruv Vansraj Rathore,Soumadeep Saha,Utpal Garain,David Doermann*

Main category: stat.ML

TL;DR: The paper explores intrinsic causal contributions (ICC) in neural networks using a generative post-hoc framework, showing ICC provides more intuitive and reliable explanations than existing methods.


<details>
  <summary>Details</summary>
Motivation: Quantifying the causal influence of input features within neural networks has become a topic of increasing interest.

Method: Treat NNs as structural causal models (SCMs) and extend focus to include intrinsic causal contributions (ICC), proposing an identifiable generative post-hoc framework for quantifying ICC and drawing a relationship between ICC and Sobol' indices.

Result: Experiments on synthetic and real-world datasets demonstrate that ICC generates more intuitive and reliable explanations compared to existing global explanation techniques.

Conclusion: ICC offers a new perspective in understanding causal influences within neural networks.

Abstract: Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [549] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: The paper proposes a method for estimating differences in two multi-attribute Gaussian graphical models (GGMs) using a penalized D-trace loss function with non-convex penalties, providing theoretical analysis and numerical examples.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-attribute differential graph estimation rely on group lasso penalized loss functions, but this paper aims to improve upon that by introducing a penalized D-trace loss function with non-convex penalties.

Method: The method involves using a penalized D-trace loss function with non-convex penalties such as log-sum and SCAD. Two proximal gradient descent methods are used to optimize the objective function.

Result: Theoretical analysis is provided that establishes sufficient conditions for consistency in support recovery, convexity, and estimation in high-dimensional settings. The approaches are demonstrated through numerical examples based on synthetic and real data.

Conclusion: This approach offers an alternative to existing methods for multi-attribute differential graph estimation, potentially providing better performance due to the use of non-convex penalties.

Abstract: We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [550] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski,Michael Ivanitskiy,Nathan Lenssen,Douglas Nychka,Daniel McKenzie*

Main category: stat.ML

TL;DR: The paper explores the use of image-to-image (I2I) networks for parameter estimation in spatially autoregressive (SAR) models, offering faster and more accurate results compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: In many applications, acquiring a large ensemble of data instances is expensive or impractical. Statistical emulation using SAR models can generate synthetic fields, but parameter inference via MLE is computationally prohibitive for large, non-stationary fields.

Method: By recognizing that SAR parameters can be arranged on a regular grid, both inputs (spatial fields) and outputs (model parameters) are treated as images. This allows the application of image-to-image (I2I) networks for parameter estimation.

Result: I2I networks enable faster and more accurate parameter estimation for non-stationary SAR models with high complexity.

Conclusion: Image-to-image networks provide an efficient alternative for parameter estimation in complex SAR models, outperforming traditional methods in terms of speed and accuracy.

Abstract: In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [551] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka,Matias Quiroz,Vali Asimit,Samuel Muller*

Main category: stat.ML

TL;DR: The paper proposes a gradient-based approach for sparse portfolio selection that transforms the combinatorial problem into a continuous optimization task, providing a fast and scalable solution with negligible error compared to commercial solvers.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of current methods for sparse portfolio selection which rely on mixed-integer quadratic programming and become slow as the number of assets and constraints increase.

Method: A gradient-based approach is introduced, converting the sparse selection problem into a constrained continuous optimization task through Boolean relaxation. A tunable parameter is used to transition the objective function from convex to concave, enabling a stable starting point and progressing towards a sparse binary solution.

Result: The method matches commercial solvers in asset selection for most instances, and when it differs, the solution varies by only a few assets with negligible error in portfolio variance.

Conclusion: The proposed gradient-based method offers a fast, scalable alternative to commercial solvers for sparse portfolio selection with comparable accuracy.

Abstract: Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [552] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl,Leon Klein*

Main category: stat.ML

TL;DR: Boltzmann Generators利用Normalizing Flows和重要性加权从分子系统的平衡分布生成样本。Flow Matching加速了Continuous Normalizing Flows（CNFs），使其能够扩展到更复杂的分子系统，并最小化流积分轨迹的长度。本文研究了在已知目标能量的情况下，使用路径梯度微调通过Flow Matching初步训练的CNFs的好处。实验表明，这种混合方法在不增加模型复杂性和计算预算的情况下，将分子系统的采样效率提高了三倍。此外，通过测量微调期间流轨迹的长度，证明路径梯度在很大程度上保留了流的学习结构。


<details>
  <summary>Details</summary>
Motivation: 尽管Boltzmann Generators已经可以通过Normalizing Flows和重要性加权生成分子系统的平衡分布样本，但为了进一步提高采样效率，需要探索新的优化方法。Flow Matching虽然加速了CNFs并扩展到更复杂的分子系统，但在已知目标能量的情况下，如何进一步优化这些初步训练的CNFs仍需研究。

Method: 使用路径梯度对通过Flow Matching初步训练的Continuous Normalizing Flows（CNFs）进行微调。这种方法结合了Flow Matching和路径梯度的优势，在不需要额外采样的情况下，优化CNFs以提高采样效率。同时，通过测量微调过程中流轨迹的长度，评估路径梯度对流学习结构的保留程度。

Result: 实验结果表明，与仅使用Flow Matching相比，该混合方法将分子系统的采样效率提高了三倍。此外，路径梯度在微调过程中能够很大程度上保留流的学习结构，这说明该方法具有良好的稳定性和有效性。

Conclusion: 使用路径梯度微调由Flow Matching初步训练的CNFs是一种有效的混合方法，可以在不增加模型复杂性和计算预算的情况下显著提高分子系统的采样效率。这种方法为分子模拟和其他相关领域提供了潜在的应用价值。

Abstract: Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [553] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: stat.ML

TL;DR: 本文提出了一种新的单阶段Top-k学习框架，统一了预测和延迟决策，通过共享的基于得分的模型选择每个输入的k个最具成本效益的实体（标签或专家）。该方法在单一端到端目标下联合优化预测和延迟决策，并定义了一个与k无关的成本敏感损失函数及其凸替代函数。此外，还引入了一个自适应变体Top-k(x)，动态选择每个输入咨询的实体数量以平衡预测准确性和咨询成本。实验表明，该方法优于传统的Top-1延迟策略，而Top-k(x)通过根据输入复杂性调整分配实现了更好的准确性-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的单阶段学习到延迟（L2D）方法仅限于将决策延迟给单一专家，缺乏灵活性和泛化能力，无法有效处理多实体情况下的预测和延迟决策问题。因此，需要一种能够同时优化预测和延迟并适用于多个实体的新方法。

Method: 1. 提出了一种单阶段Top-k Learning-to-Defer框架，使用共享的基于得分的模型选择k个最具成本效益的实体（标签或专家）。2. 定义了一个成本敏感损失函数，并推导出一个不依赖于k的新型凸替代函数，从而实现跨不同Top-k场景的泛化而无需重新训练。3. 引入了自适应变体Top-k(x)，动态调整每个输入咨询的实体数量，以平衡预测准确性和咨询成本。

Result: 1. 实验结果表明，单阶段Top-k方法在CIFAR-10和SVHN数据集上严格优于Top-1延迟策略。2. Top-k(x)通过根据输入复杂性调整分配，在准确性-成本权衡方面表现出色。

Conclusion: 本文提出的单阶段Top-k Learning-to-Defer框架及其自适应变体Top-k(x)为多实体情况下的预测和延迟决策提供了一种灵活且高效的解决方案，显著提升了性能并在准确性与成本之间实现了更好的权衡。

Abstract: We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [554] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato,Harvinder Lehal,Simon Maskell,Lee Devlin,Malcolm Strens*

Main category: stat.ML

TL;DR: The paper investigates sampling algorithms using subset evaluations to reduce computational overhead in Bayesian inference with MCMC for irregular and expensive likelihood functions. An improved version of HINTS with adaptive proposals and a data-driven proxy shows the best performance in a fixed computational budget.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when dealing with irregular and costly likelihood functions.

Method: Adapt subset samplers without gradient information, introduce data-driven proxies instead of Taylor expansions, and define a novel computation-cost aware adaptive controller. Use hierarchical delayed acceptance for efficient exact sampling.

Result: Improved HINTS obtains the best sampling error within a fixed computational budget. Subset evaluations provide cost-effective exploration and data-driven proxies successfully pre-screen proposals.

Conclusion: Subset evaluations can offer cheap and tempered exploration while data-driven proxies can effectively pre-screen proposals leading to efficient exact sampling through hierarchical delayed acceptance.

Abstract: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [555] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin,Shixiao Liang,Christopher Tunnell*

Main category: stat.ML

TL;DR: FlowVAT is a new method for normalizing flow variational inference that tempers both base and target distributions simultaneously, overcoming mode-seeking behavior and posterior collapse in multi-modal and high-dimensional problems. It outperforms traditional methods in experiments with multi-modal distributions and moves toward fully-automatic black-box variational inference.


<details>
  <summary>Details</summary>
Motivation: Variational inference faces challenges with multi-modal and high-dimensional posteriors, leading to mode-seeking behavior and posterior collapse. Traditional annealing methods require temperature schedules and hyperparameter tuning, which hinders the goal of truly black-box variational inference.

Method: FlowVAT uses a conditional tempering approach where both the base and target distributions are tempered simultaneously while maintaining affine-invariance. The normalizing flow is conditioned on temperature, allowing it to generalize across a range of temperatures using overparameterized neural networks. This single flow represents the posterior across different temperatures.

Result: In experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT finds more modes and achieves better ELBO values compared to traditional and adaptive annealing methods, particularly excelling in higher dimensions where existing approaches fail.

Conclusion: FlowVAT advances the field towards fully-automatic black-box variational inference for complicated posteriors by requiring minimal hyperparameter tuning and no annealing schedule.

Abstract: Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [556] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

Main category: stat.ML

TL;DR: The paper introduces BaNk-UCB, a nonparametric algorithm combining k-NN regression with UCB principle for batched contextual bandits, achieving near-optimal regret and outperforming baselines in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of sequential decision-making in contexts like medicine and marketing where online feedback is limited, especially when actions are taken over a finite horizon divided into a small number of batches.

Method: Proposes BaNk-UCB, which uses adaptive k-nearest neighbor regression combined with the upper confidence bound principle. The method leverages local geometry for reward estimation and adaptively balances exploration and exploitation.

Result: Provides near-optimal regret guarantees under standard assumptions and demonstrates superior performance compared to binning-based methods through empirical evaluations on both synthetic and real-world datasets.

Conclusion: BaNk-UCB is a fully nonparametric algorithm that adapts to context dimensions, achieves minimax-optimal rates, and performs well across different datasets.

Abstract: We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [557] [Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks](https://arxiv.org/abs/2505.10134)
*Guangjin Pan,Kaixuan Huang,Hui Chen,Shunqing Zhang,Christian Häger,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出了一种基于基础模型的无线定位解决方案LWLM，通过自监督学习框架预训练并优化三个目标：空间频率屏蔽信道建模(SF-MCM)、域变换不变性(DTI)和位置不变对比学习(PICL)，在多个定位任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确且鲁棒的定位对于5G和6G应用至关重要，但现有的数据驱动方法需要大量标注数据且难以泛化到不同部署场景和无线配置中。

Method: 首先分析了不同的自监督学习(SSL)任务如何根据信息瓶颈(IB)理论获取通用和特定任务的语义特征，然后设计了LWLM的预训练方法，包括联合优化三个互补目标：SF-MCM、DTI和PICL，并为关键下游任务设计了轻量级解码器。

Result: 实验结果表明，LWLM在所有定位任务中一致超越基于模型和监督学习的基线方法，相较于无预训练的Transformer模型提升了26.0%-87.5%，并在标注有限和未见过的基站配置下表现出强大的泛化能力。

Conclusion: LWLM作为一种基础模型，具有解决无线定位问题的巨大潜力，能够显著提高定位精度并增强泛化能力。

Abstract: Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.

</details>
