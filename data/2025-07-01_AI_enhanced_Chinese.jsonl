{"id": "2506.22441", "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TDWLFT\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u9608\u503c\u8ddd\u79bb\u52a0\u6743\u635f\u5931\u51fd\u6570\u63d0\u9ad8\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4ea4\u901a\u901f\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5b8c\u6574\u548c\u9ad8\u8d28\u91cf\u7684\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u9645\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\uff0c\u901a\u4fe1\u6545\u969c\u548c\u4f20\u611f\u5668\u6545\u969c\u7b49\u95ee\u9898\u4f1a\u5bfc\u81f4\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u635f\u574f\uff0c\u8fd9\u4e3aITS\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684LFT\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86TDWLFT\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u9608\u503c\u8ddd\u79bb\u52a0\u6743\uff08TDW\uff09\u635f\u5931\u51fd\u6570\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u4e0d\u540c\u7684\u6743\u91cd\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u5bf9\u5f02\u5e38\u503c\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6765\u81ea\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u7684\u4ea4\u901a\u901f\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cTDWLFT\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "TDWLFT\u6a21\u578b\u901a\u8fc7\u4f7f\u7528TDW\u635f\u5931\u51fd\u6570\u63d0\u9ad8\u4e86\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u5904\u7406\u7f3a\u5931\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.22442", "pdf": "https://arxiv.org/pdf/2506.22442", "abs": "https://arxiv.org/abs/2506.22442", "authors": ["Piotr Makarevich"], "title": "Features-based embedding or Feature-grounding", "categories": ["cs.LG"], "comment": "13 pages, 12 figures", "summary": "In everyday reasoning, when we think about a particular object, we associate\nit with a unique set of expected properties such as weight, size, or more\nabstract attributes like density or horsepower. These expectations are shaped\nby our prior knowledge and the conceptual categories we have formed through\nexperience. This paper investigates how such knowledge-based structured\nthinking can be reproduced in deep learning models using features based\nembeddings. Specially, it introduces an specific approach to build\nfeature-grounded embedding, aiming to align shareable representations of\noperable dictionary with interpretable domain-specific conceptual features.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u91cd\u73b0\u57fa\u4e8e\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u601d\u7ef4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u7279\u5f81\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5c06\u53ef\u64cd\u4f5c\u8bcd\u5178\u7684\u5171\u4eab\u8868\u793a\u4e0e\u53ef\u89e3\u91ca\u7684\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u7279\u5f81\u5bf9\u9f50\u3002", "motivation": "\u5728\u65e5\u5e38\u63a8\u7406\u4e2d\uff0c\u5f53\u6211\u4eec\u601d\u8003\u4e00\u4e2a\u7279\u5b9a\u5bf9\u8c61\u65f6\uff0c\u6211\u4eec\u4f1a\u5c06\u5176\u4e0e\u4e00\u7ec4\u72ec\u7279\u7684\u9884\u671f\u5c5e\u6027\u76f8\u5173\u8054\uff0c\u8fd9\u4e9b\u5c5e\u6027\u7531\u6211\u4eec\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u901a\u8fc7\u7ecf\u9a8c\u5f62\u6210\u7684\u6982\u5ff5\u7c7b\u522b\u5851\u9020\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u91cd\u73b0\u8fd9\u79cd\u57fa\u4e8e\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u601d\u7ef4\u65b9\u5f0f\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u7279\u5b9a\u65b9\u6cd5\u6765\u6784\u5efa\u7279\u5f81\u5d4c\u5165\uff08feature-grounded embedding\uff09\uff0c\u4ee5\u5bf9\u9f50\u53ef\u64cd\u4f5c\u5b57\u5178\u7684\u5171\u4eab\u8868\u793a\u4e0e\u53ef\u89e3\u91ca\u7684\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u7279\u5f81\u3002", "result": "\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u77e5\u8bc6\u7ed3\u6784\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u7ed3\u6784\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u53ef\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.22443", "pdf": "https://arxiv.org/pdf/2506.22443", "abs": "https://arxiv.org/abs/2506.22443", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "categories": ["cs.LG", "cs.HC"], "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRL-Net\u7684\u795e\u7ecf\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\u7f51\u7edc\uff0c\u9996\u6b21\u5c06\u5176\u5e94\u7528\u4e8e\u57fa\u4e8e\u96f7\u8fbe\u7684\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff08F1\u5f97\u520693.03%\uff09\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u89c4\u5219\u590d\u6742\u5ea6\uff0c\u5e76\u89e3\u51b3\u4e86\u89c4\u5219\u526a\u679d\u548c\u5c42\u6b21\u504f\u5dee\u65b9\u9762\u7684\u4f18\u5316\u6311\u6218\u3002\u4e0eMIRA\u548cXentricAI\u76f8\u6bd4\uff0cRL-Net\u5728\u900f\u660e\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5e73\u8861\u70b9\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5728\u8fb9\u7f18\u90e8\u7f72\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u89c4\u5219\u9a71\u52a8\u6a21\u578b\u867d\u7136\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u800c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u624b\u52bf\u8bc6\u522b\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86RL-Net\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u795e\u7ecf\u4f18\u5316\u5b66\u4e60\u53ef\u89e3\u91ca\u89c4\u5219\u5217\u8868\u7684\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u3002\u5c06RL-Net\u5e94\u7528\u4e8e\u57fa\u4e8e\u96f7\u8fbe\u7684\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u4e0e\u5b8c\u5168\u900f\u660e\u7684\u89c4\u5219\u9a71\u52a8\u7cfb\u7edf\uff08MIRA\uff09\u548c\u53ef\u89e3\u91ca\u7684\u9ed1\u7bb1\u6a21\u578b\uff08XentricAI\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u9002\u5e94\u80fd\u529b\uff08\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRL-Net\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\uff08F1\u5f97\u5206\u4e3a93.03%\uff09\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5219\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u4e0e\u89c4\u5219\u526a\u679d\u548c\u5c42\u6b21\u504f\u5dee\u76f8\u5173\u7684\u4f18\u5316\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "conclusion": "RL-Net\u4e3a\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u900f\u660e\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5e73\u8861\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u5728\u53ef\u89e3\u91caAI\u9886\u57df\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u6269\u5c55\u5230\u8fb9\u7f18\u90e8\u7f72\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2506.22444", "pdf": "https://arxiv.org/pdf/2506.22444", "abs": "https://arxiv.org/abs/2506.22444", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "AI": {"tldr": "The paper introduces an Active Attention Network using a cohort of 18 PASC patients to predict clinical risk and identify progression events related to PASC by integrating human expertise with active learning.", "motivation": "To address the challenge of accurately identifying progression events such as hospitalization and reinfection in PASC patients, which is crucial for effective patient management and resource allocation.", "method": "Using a publicly available cohort of 18 PASC patients with text time series features based on Llama-3.1-70B-Instruct and clinical risk annotated by experts, an Active Attention Network was proposed to predict clinical risk and identify progression events related to the risk.", "result": "The integration of human expertise with active learning enhances clinical risk prediction accuracy and enables progression event identification with fewer annotations.", "conclusion": "This study aims to improve patient care and decision-making for SARS-CoV-2 patients through better prediction models."}}
{"id": "2506.22604", "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u4e0e\u62d6\u653e\u5f0f\u7f16\u7a0b\u7ed3\u5408\u7684\u53ef\u80fd\u6027\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u63a5\u53d7\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8f93\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u901a\u8fc7\u4e0e\u624b\u52a8\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u8f83\u5927\u7684\u6a21\u578b\u5728\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u52a8\u4f5c\u5e8f\u5217\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8f83\u5c0f\u7684\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u6ee1\u610f\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u673a\u5668\u4eba\u7ec8\u7aef\u7528\u6237\u5bf9\u6613\u7528\u6027\u4efb\u52a1\u89c4\u8303\u624b\u6bb5\u7684\u9700\u6c42\uff0c\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u548c\u62d6\u653e\u5f0f\u7f16\u7a0b\u4e24\u79cd\u8303\u5f0f\u7684\u7ed3\u5408\u53ef\u80fd\u6027\uff0c\u4ee5\u5145\u5206\u5229\u7528\u4e24\u8005\u7684\u4f18\u70b9\uff1a\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7684\u76f4\u89c2\u6027\u548c\u62d6\u653e\u63a5\u53e3\u7684\u7cbe\u786e\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u63a5\u53d7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u5e76\u751f\u6210\u5177\u6709\u4eba\u7c7b\u7ea7\u522b\u7ec6\u81f4\u7a0b\u5ea6\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u751f\u6210\u7684\u52a8\u4f5c\u5e8f\u5217\u4e0e\u53e6\u4e00\u7ec4\u624b\u5de5\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927\u7684\u6a21\u578b\u5728\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u52a8\u4f5c\u5e8f\u5217\u65b9\u9762\u7684\u8868\u73b0\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u8f83\u5c0f\u7684\u6a21\u578b\u4ecd\u7136\u80fd\u591f\u5b9e\u73b0\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u548c\u62d6\u653e\u5f0f\u7f16\u7a0b\u53ef\u4ee5\u6709\u6548\u7ed3\u5408\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u4ece\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u7f16\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.22536", "pdf": "https://arxiv.org/pdf/2506.22536", "abs": "https://arxiv.org/abs/2506.22536", "authors": ["Yu Zhang", "Shanshan Zhao", "Bokui Wan", "Jinjuan Wang", "Xiaodong Yan"], "title": "Strategic A/B testing via Maximum Probability-driven Two-armed Bandit", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "25 pages, 14 figures", "summary": "Detecting a minor average treatment effect is a major challenge in\nlarge-scale applications, where even minimal improvements can have a\nsignificant economic impact. Traditional methods, reliant on normal\ndistribution-based or expanded statistics, often fail to identify such minor\neffects because of their inability to handle small discrepancies with\nsufficient sensitivity. This work leverages a counterfactual outcome framework\nand proposes a maximum probability-driven two-armed bandit (TAB) process by\nweighting the mean volatility statistic, which controls Type I error. The\nimplementation of permutation methods further enhances the robustness and\nefficacy. The established strategic central limit theorem (SCLT) demonstrates\nthat our approach yields a more concentrated distribution under the null\nhypothesis and a less concentrated one under the alternative hypothesis,\ngreatly improving statistical power. The experimental results indicate a\nsignificant improvement in the A/B testing, highlighting the potential to\nreduce experimental costs while maintaining high statistical power.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u7ed3\u679c\u6846\u67b6\u7684\u6700\u5927\u6982\u7387\u9a71\u52a8\u4e24\u81c2\u8001\u864e\u673a\uff08TAB\uff09\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a0\u6743\u5747\u503c\u6ce2\u52a8\u7edf\u8ba1\u91cf\u63a7\u5236\u7b2c\u4e00\u7c7b\u9519\u8bef\uff0c\u5e76\u7ed3\u5408\u7f6e\u6362\u65b9\u6cd5\u63d0\u9ad8\u7a33\u5065\u6027\u548c\u529f\u6548\u3002\u6240\u5efa\u7acb\u7684\u6218\u7565\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff08SCLT\uff09\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u96f6\u5047\u8bbe\u4e0b\u5206\u5e03\u66f4\u96c6\u4e2d\uff0c\u5728\u5907\u62e9\u5047\u8bbe\u4e0b\u5206\u5e03\u66f4\u5206\u6563\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u7edf\u8ba1\u529f\u6548\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728A/B\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u964d\u4f4e\u5b9e\u9a8c\u6210\u672c\u540c\u65f6\u4fdd\u6301\u9ad8\u7edf\u8ba1\u529f\u6548\u3002", "motivation": "\u68c0\u6d4b\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u5fae\u5c0f\u5e73\u5747\u5904\u7406\u6548\u5e94\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u6539\u8fdb\u4e5f\u53ef\u80fd\u5e26\u6765\u663e\u8457\u7684\u7ecf\u6d4e\u5f71\u54cd\u3002\u4f20\u7edf\u65b9\u6cd5\u7531\u4e8e\u5bf9\u5c0f\u5dee\u5f02\u4e0d\u654f\u611f\uff0c\u5f80\u5f80\u65e0\u6cd5\u8bc6\u522b\u8fd9\u4e9b\u5fae\u5c0f\u6548\u5e94\u3002", "method": "\u5229\u7528\u53cd\u4e8b\u5b9e\u7ed3\u679c\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5927\u6982\u7387\u9a71\u52a8\u7684\u4e24\u81c2\u8001\u864e\u673a\uff08TAB\uff09\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5bf9\u5747\u503c\u6ce2\u52a8\u7edf\u8ba1\u91cf\u8fdb\u884c\u52a0\u6743\u6765\u63a7\u5236\u7b2c\u4e00\u7c7b\u9519\u8bef\u3002\u540c\u65f6\uff0c\u5b9e\u65bd\u4e86\u7f6e\u6362\u65b9\u6cd5\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u7a33\u5065\u6027\u548c\u529f\u6548\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u6218\u7565\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff08SCLT\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u96f6\u5047\u8bbe\u4e0b\u4ea7\u751f\u66f4\u96c6\u4e2d\u7684\u5206\u5e03\uff0c\u5728\u5907\u62e9\u5047\u8bbe\u4e0b\u4ea7\u751f\u66f4\u5206\u6563\u7684\u5206\u5e03\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u7edf\u8ba1\u529f\u6548\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728A/B\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u591f\u51cf\u5c11\u5b9e\u9a8c\u6210\u672c\u5e76\u7ef4\u6301\u9ad8\u7edf\u8ba1\u529f\u6548\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5fae\u5c0f\u5e73\u5747\u5904\u7406\u6548\u5e94\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684A/B\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22445", "pdf": "https://arxiv.org/pdf/2506.22445", "abs": "https://arxiv.org/abs/2506.22445", "authors": ["Saad Alqithami"], "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u5bf9\u6297\u5f39\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08HAMARL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u7684\u5b89\u5168\u6027\u548c\u5f39\u6027\u3002\u901a\u8fc7\u5c40\u90e8\u667a\u80fd\u4f53\u548c\u5168\u5c40\u534f\u8c03\u5668\u7684\u7ed3\u5408\u4ee5\u53ca\u5bf9\u6297\u6027\u8bad\u7ec3\u5faa\u73af\uff0c\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u68c0\u6d4b\u7cbe\u5ea6\u3001\u7f29\u77ed\u4e86\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u786e\u4fdd\u4e86\u64cd\u4f5c\u8fde\u7eed\u6027\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u8fde\u63a5\u6027\u7684\u589e\u52a0\uff0c\u5176\u9762\u4e34\u590d\u6742\u7684\u7f51\u7edc\u5a01\u80c1\uff0c\u5982\u81ea\u9002\u5e94\u548c\u96f6\u65e5\u653b\u51fb\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5165\u4fb5\u68c0\u6d4b\u548c\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL)\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff0c\u5305\u542b\u8d1f\u8d23\u5b50\u7cfb\u7edf\u5b89\u5168\u7684\u5c40\u90e8\u667a\u80fd\u4f53\u548c\u76d1\u7763\u4f18\u5316\u5168\u5c40\u9632\u5fa1\u7b56\u7565\u7684\u5168\u5c40\u534f\u8c03\u5668\uff0c\u540c\u65f6\u6574\u5408\u4e86\u4e00\u4e2a\u5bf9\u6297\u6027\u8bad\u7ec3\u5faa\u73af\u4ee5\u6a21\u62df\u548c\u9884\u6d4b\u6f14\u5316\u4e2d\u7684\u7f51\u7edc\u5a01\u80c1\u3002", "result": "\u5728\u6a21\u62df\u5de5\u4e1a\u7269\u8054\u7f51\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0cHAMARL\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u786e\u4fdd\u4e86\u64cd\u4f5c\u7684\u8fde\u7eed\u6027\u3002", "conclusion": "\u7ed3\u5408\u5206\u5c42\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e0e\u5bf9\u6297\u6027\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u4e0b\u4e00\u4ee3CPS\u7684\u5f39\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22609", "pdf": "https://arxiv.org/pdf/2506.22609", "abs": "https://arxiv.org/abs/2506.22609", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "categories": ["cs.AI"], "comment": "18 pages, 3 figures", "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLudax\u7684\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u68cb\u76d8\u6e38\u620f\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u53ef\u4ee5\u81ea\u52a8\u7f16\u8bd1\u4e3a\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u3002Ludax\u7ed3\u5408\u4e86\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u7684\u901a\u7528\u6027\u548c\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u786c\u4ef6\u7684\u901f\u5ea6\uff0c\u5e76\u4e14\u9002\u5408\u96c6\u6210\u5230\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u4e2d\u3002\u8bba\u6587\u63d0\u4f9b\u4e86Ludax\u63cf\u8ff0\u8bed\u8a00\u7684\u8be6\u7ec6\u5206\u89e3\u3001\u6280\u672f\u7f16\u8bd1\u8bf4\u660e\u3001\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6f14\u793a\u3002", "motivation": "\u968f\u7740\u786c\u4ef6\u52a0\u901f\u7684\u8fdb\u6b65\u548c\u6e38\u620f\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u57fa\u51c6\u73af\u5883\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u7f16\u8bd1\u4e3a\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u7684\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u6765\u652f\u6301\u66f4\u9ad8\u6548\u7684\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLudax\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u662f\u4e00\u79cd\u9488\u5bf9\u68cb\u76d8\u6e38\u620f\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u80fd\u591f\u81ea\u52a8\u7f16\u8bd1\u4e3a\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u3002Ludax\u8bbe\u8ba1\u4e3a\u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u8868\u793a\u65b9\u6848\u4ee5\u52a0\u901f\u6e38\u620f\u7814\u7a76\u3002", "result": "Ludax\u6846\u67b6\u5b9e\u73b0\u4e86\u5feb\u901f\u6a21\u62df\u548cRL\u4ee3\u7406\u8bad\u7ec3\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0cLudax\u662f\u5f00\u6e90\u7684\uff0c\u63d0\u4f9b\u4e86\u73b0\u6709\u68cb\u76d8\u6e38\u620f\u7684\u5b9e\u73b0\u3002", "conclusion": "Ludax\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u52a0\u901f\u4ece\u5f3a\u5316\u5b66\u4e60\u5230\u8ba4\u77e5\u79d1\u5b66\u7684\u6e38\u620f\u7814\u7a76\uff0c\u63d0\u4f9b\u5feb\u901f\u6a21\u62df\u548c\u7075\u6d3b\u7684\u8868\u793a\u65b9\u6848\u3002"}}
{"id": "2506.22565", "pdf": "https://arxiv.org/pdf/2506.22565", "abs": "https://arxiv.org/abs/2506.22565", "authors": ["Guan-Horng Liu", "Jaemoo Choi", "Yongxin Chen", "Benjamin Kurt Miller", "Ricky T. Q. Chen"], "title": "Adjoint Schr\u00f6dinger Bridge Sampler", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "Computational methods for learning to sample from the Boltzmann distribution\n-- where the target distribution is known only up to an unnormalized energy\nfunction -- have advanced significantly recently. Due to the lack of explicit\ntarget samples, however, prior diffusion-based methods, known as diffusion\nsamplers, often require importance-weighted estimation or complicated learning\nprocesses. Both trade off scalability with extensive evaluations of the energy\nand model, thereby limiting their practical usage. In this work, we propose\nAdjoint Schr\\\"odinger Bridge Sampler (ASBS), a new diffusion sampler that\nemploys simple and scalable matching-based objectives yet without the need to\nestimate target samples during training. ASBS is grounded on a mathematical\nmodel -- the Schr\\\"odinger Bridge -- which enhances sampling efficiency via\nkinetic-optimal transportation. Through a new lens of stochastic optimal\ncontrol theory, we demonstrate how SB-based diffusion samplers can be learned\nat scale via Adjoint Matching and prove convergence to the global solution.\nNotably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to\narbitrary source distributions by relaxing the so-called memoryless condition\nthat largely restricts the design space. Through extensive experiments, we\ndemonstrate the effectiveness of ASBS on sampling from classical energy\nfunctions, amortized conformer generation, and molecular Boltzmann\ndistributions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u91c7\u6837\u5668ASBS\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5339\u914d\u76ee\u6807\u5b66\u4e60\u8fc7\u7a0b\uff0c\u65e0\u9700\u5728\u8bad\u7ec3\u671f\u95f4\u4f30\u8ba1\u76ee\u6807\u6837\u672c\uff0c\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u660e\u786e\u7684\u76ee\u6807\u6837\u672c\uff0c\u5148\u524d\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5\uff08diffusion samplers\uff09\u901a\u5e38\u9700\u8981\u91cd\u8981\u6027\u52a0\u6743\u4f30\u8ba1\u6216\u590d\u6742\u7684\u5b78\u7fd2\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Adjoint Schr\u00f6dinger Bridge Sampler (ASBS)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eSchr\u00f6dinger Bridge\u6570\u5b66\u6a21\u578b\u7684\u65b0\u6269\u6563\u91c7\u6837\u5668\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u7b80\u5355\u7684\u3001\u53ef\u6269\u5c55\u7684\u5339\u914d\u76ee\u6807\uff0c\u907f\u514d\u4e86\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u76ee\u6807\u6837\u672c\u7684\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4f34\u968f\u5339\u914d\uff08Adjoint Matching\uff09\u4ee5\u5927\u89c4\u6a21\u5b66\u4e60SB-based\u6269\u6563\u91c7\u6837\u5668\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5176\u6536\u655b\u5230\u5168\u5c40\u89e3\u3002\u6b64\u5916\uff0cASBS\u63a8\u5e7f\u4e86\u6700\u8fd1\u7684\u4f34\u968f\u91c7\u6837\u65b9\u6cd5\uff08Adjoint Sampling\uff09\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u4efb\u610f\u6e90\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86ASBS\u5728\u4ece\u7ecf\u5178\u80fd\u91cf\u51fd\u6570\u91c7\u6837\u3001\u644a\u9500\u6784\u8c61\u751f\u6210\u4ee5\u53ca\u5206\u5b50Boltzmann\u5206\u5e03\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ASBS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4ece\u4ec5\u901a\u8fc7\u975e\u6807\u51c6\u5316\u80fd\u91cf\u51fd\u6570\u5df2\u77e5\u7684\u76ee\u6807\u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u590d\u6742\u6027\u548c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2506.22446", "pdf": "https://arxiv.org/pdf/2506.22446", "abs": "https://arxiv.org/abs/2506.22446", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "AI": {"tldr": "EAGLE\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u5168\u9762\u7684\u5f52\u56e0\u5206\u6790\u6765\u51c6\u786e\u9884\u6d4b\u764c\u75c7\u751f\u5b58\u7387\u3002\u5b83\u5728\u4e09\u79cd\u764c\u75c7\u7c7b\u578b\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u80fd\u8bc6\u522b\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u98ce\u9669\u7fa4\u4f53\uff0c\u4ece\u800c\u4e3a\u6cbb\u7597\u51b3\u7b56\u63d0\u4f9b\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u4e2d\u5b58\u5728\u878d\u5408\u7b56\u7565\u7b80\u5355\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002", "method": "EAGLE\u5f15\u5165\u4e86\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u52a8\u6001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5b66\u4e60\u6a21\u6001\u95f4\u7684\u5206\u5c42\u5173\u7cfb\uff1b2\uff09\u5927\u5e45\u964d\u4f4e\u7ef4\u5ea6\uff0899.96%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\uff1b3\uff09\u4e09\u79cd\u4e92\u8865\u7684\u5f52\u56e0\u65b9\u6cd5\u63d0\u4f9b\u60a3\u8005\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\uff1b4\uff09\u7edf\u4e00\u7684\u7ba1\u9053\u5b9e\u73b0\u8de8\u764c\u75c7\u7c7b\u578b\u7684\u65e0\u7f1d\u9002\u5e94\u3002", "result": "\u5728911\u540d\u60a3\u8005\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\uff08GBM\uff09\u3001\u7c98\u6db2\u6027\u4e73\u5934\u72b6\u5bfc\u7ba1\u5185\u80bf\u7624\uff08IPMN\uff09\u548c\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\uff08NSCLC\uff09\u3002\u9ad8\u98ce\u9669\u4e2a\u4f53\u66f4\u4f9d\u8d56\u4e0d\u826f\u5f71\u50cf\u7279\u5f81\uff0c\u800c\u4f4e\u98ce\u9669\u60a3\u8005\u5219\u8868\u73b0\u51fa\u5e73\u8861\u7684\u6a21\u6001\u8d21\u732e\u3002\u98ce\u9669\u5206\u5c42\u8bc6\u522b\u51fa\u4e2d\u4f4d\u751f\u5b58\u65f6\u95f4\u5dee\u5f02\u8fbe4\u52305\u500d\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u7fa4\u4f53\u3002", "conclusion": "EAGLE\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u4e0e\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u5f25\u5408\u4e86\u9ad8\u7ea7AI\u80fd\u529b\u548c\u5b9e\u9645\u533b\u7597\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u9884\u540e\u51c6\u786e\u6027\u548c\u533b\u751f\u5bf9\u81ea\u52a8\u5316\u9884\u6d4b\u7684\u4fe1\u4efb\u3002"}}
{"id": "2506.22653", "pdf": "https://arxiv.org/pdf/2506.22653", "abs": "https://arxiv.org/abs/2506.22653", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "title": "URSA: The Universal Research and Scientific Agent", "categories": ["cs.AI"], "comment": "31 pages, 9 figures", "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "AI": {"tldr": "URSA\u662f\u4e00\u4e2a\u5305\u542b\u6a21\u5757\u5316\u4ee3\u7406\u548c\u5de5\u5177\u7684\u79d1\u5b66\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u53ef\u4ee5\u7ed3\u5408\u5148\u8fdb\u7269\u7406\u6a21\u62df\u4ee3\u7801\uff0c\u89e3\u51b3\u590d\u6742\u79d1\u5b66\u95ee\u9898\uff0c\u52a0\u901f\u7814\u7a76\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u804a\u5929\u673a\u5668\u4eba\u5f62\u5f0f\uff0c\u80fd\u591f\u6267\u884c\u590d\u6742\u7684\u63a8\u7406\u3001\u89c4\u5212\u3001\u5199\u4f5c\u3001\u7f16\u7801\u548c\u7814\u7a76\u4efb\u52a1\uff0c\u8fd9\u4e9b\u6280\u80fd\u4e0e\u4eba\u7c7b\u79d1\u5b66\u5bb6\u65e5\u5e38\u89e3\u51b3\u95ee\u9898\u7684\u6280\u80fd\u6709\u5f88\u5927\u91cd\u53e0\uff0c\u56e0\u6b64\u5728'agentic' AI\u4e2d\u4f7f\u7528LLMs\u6709\u53ef\u80fd\u9769\u65b0\u73b0\u4ee3\u79d1\u5b66\u5e76\u6d88\u9664\u8fdb\u6b65\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86URSA\uff0c\u4e00\u4e2a\u79d1\u5b66\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ec\u4e00\u7ec4\u6a21\u5757\u5316\u4ee3\u7406\u548c\u5de5\u5177\uff0c\u53ef\u4ee5\u7ed3\u5408\u4ee5\u89e3\u51b3\u4e0d\u540c\u590d\u6742\u6027\u548c\u5f71\u54cd\u7684\u79d1\u5b66\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86URSA\u7684\u67b6\u6784\u4ee5\u53ca\u7a81\u51fa\u8be5\u7cfb\u7edf\u6f5c\u529b\u7684\u4f8b\u5b50\u3002", "conclusion": "URSA\u6709\u6f5c\u529b\u901a\u8fc7\u52a0\u901f\u7814\u7a76\u4efb\u52a1\u6765\u63a8\u52a8\u79d1\u5b66\u53d1\u5c55\u3002"}}
{"id": "2506.22675", "pdf": "https://arxiv.org/pdf/2506.22675", "abs": "https://arxiv.org/abs/2506.22675", "authors": ["Luhuan Wu", "Mingzhang Yin", "Yixin Wang", "John P. Cunningham", "David M. Blei"], "title": "Bayesian Invariance Modeling of Multi-Environment Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from\nmultiple environments to identify invariant features - those with a stable\npredictive relationship to the outcome. Such features support generalization to\nnew environments and help reveal causal mechanisms. Previous methods have\nprimarily tackled this problem through hypothesis testing or regularized\noptimization. Here we develop Bayesian Invariant Prediction (BIP), a\nprobabilistic model for invariant prediction. BIP encodes the indices of\ninvariant features as a latent variable and recover them by posterior\ninference. Under the assumptions of Peters et al. [2016], the BIP posterior\ntargets the true invariant features. We prove that the posterior is consistent\nand that greater environment heterogeneity leads to faster posterior\ncontraction. To handle many features, we design an efficient variational\napproximation called VI-BIP. In simulations and real data, we find that BIP and\nVI-BIP are more accurate and scalable than existing methods for invariant\nprediction.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u8d1d\u53f6\u65af\u4e0d\u53d8\u9884\u6d4b\uff08BIP\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u8fdb\u884c\u4e0d\u53d8\u9884\u6d4b\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u548c\u53ef\u6269\u5c55\u5730\u8bc6\u522b\u4e0d\u53d8\u7279\u5f81\u3002", "motivation": "\u4ee5\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u6216\u6b63\u5219\u5316\u4f18\u5316\u6765\u89e3\u51b3\u4e0d\u53d8\u9884\u6d4b\u95ee\u9898\uff0c\u4f46\u672c\u8bba\u6587\u5e0c\u671b\u901a\u8fc7\u6982\u7387\u6a21\u578b\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u521b\u5efa\u4e86BIP\u6a21\u578b\uff0c\u5c06\u4e0d\u53d8\u7279\u5f81\u7684\u7d22\u5f15\u7f16\u7801\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u63a8\u65ad\u6062\u590d\u5b83\u4eec\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u53d8\u5206\u903c\u8fd1VI-BIP\u4ee5\u5904\u7406\u5927\u91cf\u7279\u5f81\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\uff0cBIP\u548cVI-BIP\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u4e14\u66f4\u5177\u6269\u5c55\u6027\u3002\u8bc1\u660e\u4e86\u540e\u9a8c\u662f\u4e00\u81f4\u7684\uff0c\u5e76\u4e14\u73af\u5883\u5f02\u8d28\u6027\u8d8a\u5927\uff0c\u540e\u9a8c\u6536\u7f29\u8d8a\u5feb\u3002", "conclusion": "BIP\u548cVI-BIP\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u4e0d\u53d8\u9884\u6d4b\u624b\u6bb5\uff0c\u6709\u52a9\u4e8e\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u63ed\u793a\u56e0\u679c\u673a\u5236\u3002"}}
{"id": "2506.22447", "pdf": "https://arxiv.org/pdf/2506.22447", "abs": "https://arxiv.org/abs/2506.22447", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": null, "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "AI": {"tldr": "Global Climate Models (GCMs) have a coarse resolution, and Regional Climate Models (RCMs) are computationally expensive. Most deep learning approaches focus on single-variable models, which have limitations. This study proposes a multi-task, multi-variable Vision Transformer architecture that predicts three climate variables from GCM-resolution inputs for European downscaling, outperforming single-variable baselines while improving computational efficiency.", "motivation": "To overcome the limitations of coarse resolution in GCMs, high computational cost of RCMs, and the inefficiencies of single-variable deep learning models, such as limited contextual awareness, redundant computation, and lack of cross-variable interaction.", "method": "A multi-task, multi-variable Vision Transformer (ViT) architecture with a shared encoder and variable-specific decoders was developed. It jointly predicts three climate variables: surface temperature, wind speed, and 500 hPa geopotential height directly from GCM-resolution inputs.", "result": "The multi-variable approach achieved positive cross-variable knowledge transfer and consistently outperformed single-variable baselines trained under identical conditions, while also enhancing computational efficiency.", "conclusion": "Multi-variable modeling using the proposed Vision Transformer architecture is effective for high-resolution climate downscaling."}}
{"id": "2506.22740", "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e94\u8be5\u6839\u636e\u5177\u4f53\u76ee\u6807\u6765\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u6846\u67b6\u5c55\u793a\u5176\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u5b9a\u4e49\u4e86\u8bc4\u4f30\u89e3\u91ca\u4ef7\u503c\u7684\u7406\u8bba\u4e0e\u5b9e\u8bc1\u7ed3\u5408\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u6df1\u5165\u8003\u8651\uff0c\u53ef\u80fd\u5bfc\u81f4\u89e3\u91ca\u7684\u6a21\u7cca\u6027\u548c\u8bef\u7528\u3002", "method": "\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u6784\u5efa\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u660e\u786e\u89e3\u91ca\u7684\u76ee\u6807\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u63d0\u4f9b\u8865\u6551\u63aa\u65bd\u6216\u8c03\u8bd5\u7b49\u573a\u666f\uff0c\u540c\u65f6\u5206\u6790\u7406\u60f3\u51b3\u7b56\u8005\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u53ef\u80fd\u83b7\u5f97\u7684\u6700\u5927\u6027\u80fd\u63d0\u5347\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9632\u6b62\u56e0\u6a21\u7cca\u6027\u5bfc\u81f4\u7684\u8bef\u7528\uff0c\u5e76\u4fc3\u4f7f\u7814\u7a76\u8005\u660e\u786e\u5177\u4f53\u7684\u4f7f\u7528\u6848\u4f8b\u4ee5\u4fbf\u66f4\u597d\u5730\u8bc4\u4f30\u89e3\u91ca\u7684\u4ef7\u503c\u3002", "conclusion": "\u8bc4\u4f30\u89e3\u91ca\u7684\u4ef7\u503c\u5e94\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u8bc1\u89c6\u89d2\uff0c\u8bba\u6587\u8d21\u732e\u4e86\u8de8\u8d8a\u8fd9\u4e24\u79cd\u89c6\u89d2\u7684\u5b9a\u4e49\u3002"}}
{"id": "2506.22963", "pdf": "https://arxiv.org/pdf/2506.22963", "abs": "https://arxiv.org/abs/2506.22963", "authors": ["Kevin Lam", "William Daniels", "J Maxwell Douglas", "Daniel Lai", "Samuel Aparicio", "Benjamin Bloem-Reddy", "Yongjin Park"], "title": "CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation", "categories": ["stat.ML", "cs.LG", "q-bio.GN"], "comment": "8 pages, 4 figures", "summary": "Cancer is a genetic disorder whose clonal evolution can be monitored by\ntracking noisy genome-wide copy number variants. We introduce the Copy Number\nStochastic Block Model (CN-SBM), a probabilistic framework that jointly\nclusters samples and genomic regions based on discrete copy number states using\na bipartite categorical block model. Unlike models relying on Gaussian or\nPoisson assumptions, CN-SBM respects the discrete nature of CNV calls and\ncaptures subpopulation-specific patterns through block-wise structure. Using a\ntwo-stage approach, CN-SBM decomposes CNV data into primary and residual\ncomponents, enabling detection of both large-scale chromosomal alterations and\nfiner aberrations. We derive a scalable variational inference algorithm for\napplication to large cohorts and high-resolution data. Benchmarks on simulated\nand real datasets show improved model fit over existing methods. Applied to\nTCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and\nstructured residual variation, aiding patient stratification in survival\nanalysis. These results establish CN-SBM as an interpretable, scalable\nframework for CNV analysis with direct relevance for tumor heterogeneity and\nprognosis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aCN-SBM\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u79bb\u6563\u62f7\u8d1d\u6570\u72b6\u6001\u540c\u65f6\u805a\u7c7b\u6837\u672c\u548c\u57fa\u56e0\u7ec4\u533a\u57df\uff0c\u901a\u8fc7\u5206\u89e3CNV\u6570\u636e\u4e3a\u4e3b\u6210\u5206\u548c\u6b8b\u5dee\u6210\u5206\uff0c\u6355\u6349\u4e9a\u7fa4\u7279\u5f02\u6027\u6a21\u5f0f\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u62df\u5408\uff0c\u5e76\u5728TCGA\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u4e9a\u578b\u3002", "motivation": "\u764c\u75c7\u662f\u4e00\u79cd\u53ef\u4ee5\u901a\u8fc7\u8ffd\u8e2a\u5168\u57fa\u56e0\u7ec4\u62f7\u8d1d\u6570\u53d8\u5f02\uff08CNVs\uff09\u6765\u76d1\u6d4b\u514b\u9686\u8fdb\u5316\u7684\u9057\u4f20\u75be\u75c5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u9ad8\u65af\u6216\u6cca\u677e\u5047\u8bbe\u7684\u6a21\u578b\u672a\u80fd\u5145\u5206\u5c0a\u91cdCNV\u547c\u53eb\u7684\u79bb\u6563\u6027\u8d28\u3002", "method": "\u5f15\u5165\u4e86Copy Number Stochastic Block Model (CN-SBM)\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5206\u5206\u7c7b\u5757\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u79bb\u6563\u62f7\u8d1d\u6570\u72b6\u6001\u8054\u5408\u805a\u7c7b\u6837\u672c\u548c\u57fa\u56e0\u7ec4\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5757\u7ed3\u6784\u6355\u83b7\u7279\u5b9a\u4e9a\u7fa4\u7684\u6a21\u5f0f\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\u5c06CNV\u6570\u636e\u5206\u89e3\u4e3a\u4e3b\u8981\u548c\u6b8b\u5dee\u6210\u5206\uff0c\u4ece\u800c\u68c0\u6d4b\u5927\u5c3a\u5ea6\u67d3\u8272\u4f53\u6539\u53d8\u548c\u66f4\u7cbe\u7ec6\u7684\u5f02\u5e38\u3002\u63a8\u5bfc\u51fa\u53ef\u6269\u5c55\u7684\u53d8\u5206\u63a8\u7406\u7b97\u6cd5\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u961f\u5217\u548c\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cCN-SBM\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6a21\u578b\u62df\u5408\u5ea6\u3002\u5e94\u7528\u5230TCGA\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u6570\u636e\u65f6\uff0cCN-SBM\u63ed\u793a\u4e86\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u4e9a\u578b\u548c\u6709\u7ed3\u6784\u7684\u6b8b\u5dee\u53d8\u5316\uff0c\u6709\u52a9\u4e8e\u751f\u5b58\u5206\u6790\u4e2d\u7684\u60a3\u8005\u5206\u5c42\u3002", "conclusion": "CN-SBM\u88ab\u786e\u7acb\u4e3a\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684CNV\u5206\u6790\u6846\u67b6\uff0c\u76f4\u63a5\u5173\u8054\u80bf\u7624\u5f02\u8d28\u6027\u548c\u9884\u540e\u3002"}}
{"id": "2506.22502", "pdf": "https://arxiv.org/pdf/2506.22502", "abs": "https://arxiv.org/abs/2506.22502", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "title": "Stabilization of industrial processes with time series machine learning", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "AI": {"tldr": "This paper proposes a simple pipeline consisting of two neural networks which successfully improves stability in terms of the temperature control by about 3 times compared to ordinary solvers.", "motivation": "The stabilization of time series processes is a crucial problem that is ubiquitous in various industrial fields. The application of machine learning can improve both the quality of the resulting stabilization with less computational resources required.", "method": "A simple pipeline consisting of two neural networks: the oracle predictor and the optimizer, proposing a substitution of the point-wise values optimization to the problem of the neural network training.", "result": "Improves stability in terms of the temperature control by about 3 times compared to ordinary solvers.", "conclusion": "The proposed pipeline improves stability in terms of temperature control by about 3 times compared to ordinary solvers."}}
{"id": "2506.22774", "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408PageRank\u548cTrustRank\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u5b9a\u91cf\u5206\u6790\u6765\u51cf\u5c11\u5f53\u524dAI\u7cfb\u7edf\u4fe1\u4efb\u5ea6\u81ea\u6211\u8bc4\u4f30\u4e2d\u7684\u4e3b\u89c2\u6027\uff0c\u5e76\u7efc\u5408\u8003\u8651\u7406\u8bba\u6307\u5bfc\u548c\u6280\u672f\u5de5\u5177\u7684\u4f18\u70b9\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u5e7f\u6cdb\u7684\u793e\u4f1a\u5f71\u54cd\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5176\u8fd0\u884c\u53ef\u80fd\u8d85\u51fa\u4eba\u7c7b\u76f4\u63a5\u76d1\u7763\u6216\u7406\u89e3\u8303\u56f4\u3002\u5c3d\u7ba1\u5df2\u6709\u7406\u8bba\u5de5\u5177\u548c\u6307\u5357\u4ee5\u53ca\u6280\u672f\u5de5\u5177\u5206\u522b\u4ece\u4e0d\u540c\u89d2\u5ea6\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u524d\u8005\u7f3a\u4e4f\u91cf\u5316\u624b\u6bb5\uff0c\u800c\u540e\u8005\u7f3a\u4e4f\u5168\u9762\u89c6\u89d2\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06Trustworthy AI\u7684\u4f26\u7406\u8981\u7d20\u4e0ePageRank\u548cTrustRank\u7684\u7b97\u6cd5\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u521b\u5efa\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u7b97\u6cd5\u6807\u51c6\u6765\u6700\u5c0f\u5316\u9886\u57df\u5185\u666e\u904d\u5b58\u5728\u7684\u81ea\u6211\u8bc4\u4f30\u6280\u672f\u7684\u4e3b\u89c2\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5e94\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8003\u8651\u76f8\u5173\u6307\u5357\u7406\u8bba\u5185\u5bb9\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u5b9a\u91cf\u89c1\u89e3\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9AI\u7cfb\u7edf\u4fe1\u4efb\u5ea6\u7684\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u7ed3\u5408PageRank\u548cTrustRank\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u81ea\u6211\u8bc4\u4f30\u4e2d\u7684\u4e3b\u89c2\u6027\uff0c\u5e76\u4e3aAI\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u63d0\u4f9b\u5168\u9762\u4e14\u91cf\u5316\u7684\u8bc4\u4f30\u89c6\u89d2\u3002\u8fd9\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u53ef\u4fe1\u7684AI\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23396", "pdf": "https://arxiv.org/pdf/2506.23396", "abs": "https://arxiv.org/abs/2506.23396", "authors": ["Kay Giesecke", "Enguerrand Horel", "Chartsiri Jirachotkulthorn"], "title": "AICO: Feature Significance Tests for Supervised Learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The opacity of many supervised learning algorithms remains a key challenge,\nhindering scientific discovery and limiting broader deployment -- particularly\nin high-stakes domains. This paper develops model- and distribution-agnostic\nsignificance tests to assess the influence of input features in any regression\nor classification algorithm. Our method evaluates a feature's incremental\ncontribution to model performance by masking its values across samples. Under\nthe null hypothesis, the distribution of performance differences across a test\nset has a non-positive median. We construct a uniformly most powerful,\nrandomized sign test for this median, yielding exact p-values for assessing\nfeature significance and confidence intervals with exact coverage for\nestimating population-level feature importance. The approach requires minimal\nassumptions, avoids model retraining or auxiliary models, and remains\ncomputationally efficient even for large-scale, high-dimensional settings.\nExperiments on synthetic tasks validate its statistical and computational\nadvantages, and applications to real-world data illustrate its practical\nutility.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u548c\u5206\u5e03\u65e0\u5173\u7684\u663e\u8457\u6027\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4efb\u4f55\u56de\u5f52\u6216\u5206\u7c7b\u7b97\u6cd5\u4e2d\u8f93\u5165\u7279\u5f81\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u906e\u853d\u7279\u5f81\u503c\u6765\u8bc4\u4f30\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u589e\u91cf\u8d21\u732e\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u968f\u673a\u7b26\u53f7\u68c0\u9a8c\u4ee5\u751f\u6210\u7cbe\u786e\u7684p\u503c\u548c\u7f6e\u4fe1\u533a\u95f4\u3002\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u8bbe\u7f6e\u4e0b\u4ecd\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u8f85\u52a9\u6a21\u578b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7edf\u8ba1\u548c\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u4ef7\u503c\u3002", "motivation": "\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u7684\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u79d1\u5b66\u53d1\u73b0\u5e76\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4efb\u4f55\u56de\u5f52\u6216\u5206\u7c7b\u7b97\u6cd5\u4e2d\u8f93\u5165\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u906e\u853d\u7279\u5f81\u503c\u6765\u8bc4\u4f30\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u589e\u91cf\u8d21\u732e\u3002\u5728\u96f6\u5047\u8bbe\u4e0b\uff0c\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u5dee\u5f02\u7684\u5206\u5e03\u5177\u6709\u975e\u6b63\u7684\u4e2d\u4f4d\u6570\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5747\u5300\u6700\u6709\u529b\u7684\u968f\u673a\u7b26\u53f7\u68c0\u9a8c\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e2d\u4f4d\u6570\uff0c\u4ece\u800c\u751f\u6210\u7cbe\u786e\u7684p\u503c\u548c\u7f6e\u4fe1\u533a\u95f4\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u6570\u636e\u4e0b\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u7edf\u8ba1\u548c\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u663e\u8457\u6027\u68c0\u9a8c\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u548c\u5206\u5e03\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7cbe\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2506.22530", "pdf": "https://arxiv.org/pdf/2506.22530", "abs": "https://arxiv.org/abs/2506.22530", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "Task-Agnostic Contrastive Pretraining for Relational Deep Learning", "categories": ["cs.LG", "cs.DB"], "comment": "arXiv admin note: text overlap with arXiv:2506.22199", "summary": "Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph\nNeural Network principles to learn directly from relational databases by\nrepresenting them as heterogeneous graphs. However, existing RDL models\ntypically rely on task-specific supervised learning, requiring training\nseparate models for each predictive task, which may hamper scalability and\nreuse.\n  In this work, we propose a novel task-agnostic contrastive pretraining\napproach for RDL that enables database-wide representation learning. For that\naim, we introduce three levels of contrastive objectives$-$row-level,\nlink-level, and context-level$-$designed to capture the structural and semantic\nheterogeneity inherent to relational data. We implement the respective\npretraining approach through a modular RDL architecture and an efficient\nsampling strategy tailored to the heterogeneous database setting. Our\npreliminary results on standard RDL benchmarks demonstrate that fine-tuning the\npretrained models measurably outperforms training from scratch, validating the\npromise of the proposed methodology in learning transferable representations\nfor relational data.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\uff0c\u901a\u8fc7\u4e09\u4e2a\u5c42\u6b21\u7684\u5bf9\u6bd4\u76ee\u6807\u8fdb\u884c\u6570\u636e\u5e93\u8303\u56f4\u7684\u8868\u5f81\u5b66\u4e60\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684RDL\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u4efb\u52a1\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u53ef\u6269\u5c55\u6027\u548c\u91cd\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u65e0\u5173\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u4e09\u4e2a\u5c42\u6b21\u7684\u5bf9\u6bd4\u76ee\u6807\uff08\u884c\u7ea7\u3001\u94fe\u63a5\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\uff09\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316\u7684RDL\u67b6\u6784\u548c\u9ad8\u6548\u7684\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u7684\u6807\u51c6RDL\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u5173\u7cfb\u6570\u636e\u8868\u5f81\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.22865", "pdf": "https://arxiv.org/pdf/2506.22865", "abs": "https://arxiv.org/abs/2506.22865", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "AI": {"tldr": "Recent advancements in Large Language Models (LLMs) have shown a significant performance gap between closed-source and open-source models. This paper introduces ReasonBridge, which efficiently transfers reasoning capabilities from closed-source to open-source models through a hierarchical knowledge distillation framework. The transfer learning approach incorporates a hierarchical distillation process, a sparse reasoning-focused adapter architecture, and a test-time compute scaling mechanism. Evaluations demonstrate that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks.", "motivation": "The motivation of this paper is to address the significant performance gap between closed-source and open-source LLMs, particularly in tasks requiring complex reasoning and precise instruction following.", "method": "The method introduced in this paper is called ReasonBridge, which uses a novel hierarchical knowledge distillation framework to transfer reasoning capabilities from powerful closed-source models to open-source models. This includes a hierarchical distillation process capturing both strategic abstraction and tactical implementation patterns, a sparse reasoning-focused adapter architecture requiring only 0.3% additional trainable parameters, and a test-time compute scaling mechanism using guided inference interventions.", "result": "Comprehensive evaluations show that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks, significantly narrowing the gap with closed-source models. Notably, the enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its performance on competition-level AIME problems.", "conclusion": "ReasonBridge establishes a sample-efficient approach to reasoning enhancement for instruction following in open-source models, effectively generalizing across diverse reasoning domains and model architectures."}}
{"id": "2506.23429", "pdf": "https://arxiv.org/pdf/2506.23429", "abs": "https://arxiv.org/abs/2506.23429", "authors": ["Yingyuan Li", "Aokun Wang", "Zhongjian Wang"], "title": "DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this work, we propose a novel machine learning approach to compute the\noptimal transport map between two continuous distributions from their unpaired\nsamples, based on the DeepParticle methods. The proposed method leads to a\nmin-min optimization during training and does not impose any restriction on the\nnetwork structure. Theoretically we establish a weak convergence guarantee and\na quantitative error bound between the learned map and the optimal transport\nmap. Our numerical experiments validate the theoretical results and the\neffectiveness of the new approach, particularly on real-world tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeepParticle\u65b9\u6cd5\u7684\u65b0\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u672a\u914d\u5bf9\u6837\u672c\u4e2d\u8ba1\u7b97\u4e24\u4e2a\u8fde\u7eed\u5206\u5e03\u4e4b\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u3002\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u6700\u5c0f\u4f18\u5316\uff0c\u5e76\u4e14\u4e0d\u5bf9\u7f51\u7edc\u7ed3\u6784\u65bd\u52a0\u4efb\u4f55\u9650\u5236\u3002\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u5f31\u6536\u655b\u6027\u548c\u5b66\u4e60\u6620\u5c04\u4e0e\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u4e4b\u95f4\u7684\u5b9a\u91cf\u8bef\u5dee\u754c\u9650\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u548c\u65b0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u7684\u65b9\u6cd5\u53ef\u80fd\u53d7\u9650\u4e8e\u7f51\u7edc\u7ed3\u6784\u6216\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65e0\u9650\u5236\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ee5\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528DeepParticle\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u672a\u914d\u5bf9\u6837\u672c\u4e2d\u5b66\u4e60\u6765\u8ba1\u7b97\u8fde\u7eed\u5206\u5e03\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u6620\u5c04\uff0c\u540c\u65f6\u4e0d\u65bd\u52a0\u4efb\u4f55\u7f51\u7edc\u7ed3\u6784\u9650\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u5f31\u6536\u655b\u6027\u548c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bef\u5dee\u8fb9\u754c\uff1b\u6570\u503c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u5f97\u5230\u4e86\u652f\u6301\uff0c\u800c\u4e14\u5728\u5b9e\u8df5\u5e94\u7528\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8ba1\u7b97\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.22566", "pdf": "https://arxiv.org/pdf/2506.22566", "abs": "https://arxiv.org/abs/2506.22566", "authors": ["Jacob Adamczyk"], "title": "Exploration Behavior of Untrained Policies", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7b56\u7565\u67b6\u6784\u5728\u8bad\u7ec3\u524d\u5982\u4f55\u9690\u5f0f\u5730\u5f71\u54cd\u63a2\u7d22\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u751f\u6210\u672a\u8bad\u7ec3\u7b56\u7565\u8f68\u8ff9\u7684\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u65e0\u9650\u5bbd\u5ea6\u7f51\u7edc\u7406\u8bba\u548c\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u5c55\u793a\u4e86\u672a\u8bad\u7ec3\u7b56\u7565\u7684\u76f8\u5173\u52a8\u4f5c\u53ca\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5728\u7a00\u758f\u6216\u5bf9\u6297\u6027\u5956\u52b1\u7ed3\u6784\u7684\u73af\u5883\u4e2d\u63a2\u7d22\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7b56\u7565\u67b6\u6784\u5728\u8bad\u7ec3\u524d\u5982\u4f55\u9690\u5f0f\u5730\u5851\u9020\u63a2\u7d22\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u4e0a\u548c\u5b9e\u8bc1\u4e0a\u5c55\u793a\u4ece\u672a\u8bad\u7ec3\u7b56\u7565\u751f\u6210\u5f39\u9053\u6216\u6269\u6563\u8f68\u8ff9\u7684\u7b56\u7565\uff0c\u4f7f\u7528\u65e0\u9650\u5bbd\u5ea6\u7f51\u7edc\u7406\u8bba\u548c\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u6765\u5206\u6790\u672a\u8bad\u7ec3\u7b56\u7565\u7684\u52a8\u4f5c\u76f8\u5173\u6027\u548c\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\u3002", "result": "\u63ed\u793a\u4e86\u6807\u51c6\u67b6\u6784\u5bf9\u5e94\u7684\u8f68\u8ff9\u5206\u5e03\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f52\u7eb3\u504f\u7f6e\u4ee5\u89e3\u51b3\u63a2\u7d22\u95ee\u9898\u7684\u89c1\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86\u4f7f\u7528\u7b56\u7565\u521d\u59cb\u5316\u4f5c\u4e3a\u8bbe\u8ba1\u5de5\u5177\u6765\u7406\u89e3\u65e9\u671f\u8bad\u7ec3\u4e2d\u63a2\u7d22\u884c\u4e3a\u7684\u7406\u8bba\u548c\u5b9e\u9a8c\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5229\u7528\u7b56\u7565\u521d\u59cb\u5316\u8bbe\u8ba1\u5de5\u5177\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u884c\u4e3a\u5960\u5b9a\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u57fa\u7840\u3002"}}
{"id": "2506.22893", "pdf": "https://arxiv.org/pdf/2506.22893", "abs": "https://arxiv.org/abs/2506.22893", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "AI": {"tldr": "\u5728\u4f01\u4e1a\u4e2d\uff0c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u53ef\u4ee5\u63d0\u5347\u51b3\u7b56\u751f\u4ea7\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u516d\u9879\u539f\u5219\uff0c\u5e76\u63d0\u5021\u901a\u8fc7\u5e02\u573a\u673a\u5236\u5c06AI\u8bbe\u8ba1\u548c\u4ea4\u4ed8\u4e0e\u4f01\u4e1a\u7528\u6237\u7684\u9700\u6c42\u5bf9\u9f50\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u4f01\u4e1a\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5176\u5728\u63d0\u9ad8\u51b3\u7b56\u751f\u4ea7\u529b\u65b9\u9762\u7684\u4f5c\u7528\u3002", "method": "\u5206\u6790\u5f53\u524dAI-Centric\u7528\u6237\u8303\u5f0f\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u8f6c\u5411User-Centric AI\u7684\u516d\u9879\u539f\u5219\u3002", "result": "\u660e\u786e\u4e86\u516d\u4e2a\u6210\u529f\u8981\u7d20\uff0c\u4ee5\u4fc3\u8fdbAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e02\u573a\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5e94\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1AI\uff0c\u5e76\u901a\u8fc7\u5e02\u573a\u673a\u5236\u786e\u4fddAI\u6ee1\u8db3\u4f01\u4e1a\u7528\u6237\u9700\u6c42\uff0c\u4ece\u800c\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2506.23453", "pdf": "https://arxiv.org/pdf/2506.23453", "abs": "https://arxiv.org/abs/2506.23453", "authors": ["Zhen Zhang", "Xin Liu", "Shaoli Wang", "Jiaye Teng"], "title": "Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Covariate shift occurs when the distribution of input features differs\nbetween the training and testing phases. In covariate shift, estimating an\nunknown function's moment is a classical problem that remains under-explored,\ndespite its common occurrence in real-world scenarios. In this paper, we\ninvestigate the minimax lower bound of the problem when the source and target\ndistributions are known. To achieve the minimax optimal bound (up to a\nlogarithmic factor), we propose a two-stage algorithm. Specifically, it first\ntrains an optimal estimator for the function under the source distribution, and\nthen uses a likelihood ratio reweighting procedure to calibrate the moment\nestimator. In practice, the source and target distributions are typically\nunknown, and estimating the likelihood ratio may be unstable. To solve this\nproblem, we propose a truncated version of the estimator that ensures double\nrobustness and provide the corresponding upper bound. Extensive numerical\nstudies on synthetic examples confirm our theoretical findings and further\nillustrate the effectiveness of our proposed method.", "AI": {"tldr": "\u5728\u534f\u53d8\u91cf\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u5f53\u6e90\u548c\u76ee\u6807\u5206\u5e03\u5df2\u77e5\u65f6\uff0c\u4f30\u8ba1\u672a\u77e5\u51fd\u6570\u7684\u77e9\u7684\u6700\u5c0f\u6700\u5927\u4e0b\u754c\u3002\u63d0\u51fa\u4e24\u9636\u6bb5\u7b97\u6cd5\u5b9e\u73b0\u6700\u4f18\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u622a\u65ad\u7248\u672c\u786e\u4fdd\u53cc\u91cd\u7a33\u5065\u6027\u3002", "motivation": "\u534f\u53d8\u91cf\u504f\u79fb\u5bfc\u81f4\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u8f93\u5165\u7279\u5f81\u5206\u5e03\u4e0d\u540c\uff0c\u771f\u5b9e\u573a\u666f\u4e2d\u5e38\u89c1\u4f46\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u662f\u4f30\u8ba1\u672a\u77e5\u51fd\u6570\u7684\u77e9\u3002", "method": "1. \u63d0\u51fa\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1a\u5148\u57fa\u4e8e\u6e90\u5206\u5e03\u8bad\u7ec3\u6700\u4f18\u4f30\u8ba1\u5668\uff0c\u518d\u7528\u4f3c\u7136\u6bd4\u91cd\u52a0\u6743\u6821\u51c6\u77e9\u4f30\u8ba1\u5668\u3002\n2. \u63d0\u51fa\u622a\u65ad\u7248\u4f30\u8ba1\u5668\u89e3\u51b3\u6e90\u548c\u76ee\u6807\u5206\u5e03\u672a\u77e5\u53ca\u4f3c\u7136\u6bd4\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u786e\u4fdd\u53cc\u91cd\u7a33\u5065\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u5c0f\u6700\u5927\u6700\u4f18\u8fb9\u754c\uff08\u5bf9\u6570\u56e0\u5b50\u5185\uff09\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u5e76\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u7b97\u6cd5\u53ca\u5176\u622a\u65ad\u7248\u672c\u80fd\u6709\u6548\u5e94\u5bf9\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u7684\u77e9\u4f30\u8ba1\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2506.22578", "pdf": "https://arxiv.org/pdf/2506.22578", "abs": "https://arxiv.org/abs/2506.22578", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "title": "The Hidden Link Between RLHF and Contrastive Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "AI": {"tldr": "\u672c\u8bba\u6587\u91cd\u65b0\u89e3\u8bfb\u4e86RLHF\u548cDPO\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u89c6\u89d2\u7406\u89e3\uff0c\u5e76\u4e0e\u4e92\u4fe1\u606f\u6700\u5927\u5316\u76f8\u5173\u8054\u3002\u57fa\u4e8e\u6b64\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86\u65b0\u7684MIO\u65b9\u6cd5\uff0c\u80fd\u591f\u7f13\u89e3DPO\u5728\u540e\u671f\u51fa\u73b0\u7684\u9009\u62e9\u6982\u7387\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u65b9\u6cd5\u5982RLHF\u548cDPO\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u548c\u6f5c\u5728\u5c40\u9650\u6027\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002\u4f5c\u8005\u8bd5\u56fe\u4ece\u4e92\u4fe1\u606f\u6700\u5927\u5316\u7684\u89d2\u5ea6\u7edf\u4e00\u89e3\u91ca\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u6539\u8fdb\u7a7a\u95f4\u4ee5\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5c06RLHF\u548cDPO\u65b9\u6cd5\u89e3\u91ca\u4e3a\u57fa\u4e8e\u6b63\u8d1f\u6837\u672c\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528Donsker-Varadhan (DV)\u4e0b\u754c\u6216MINE\u4f30\u8ba1\u5668\u8fdb\u884c\u4e92\u4fe1\u606f\u6700\u5927\u5316\u3002\u63a5\u7740\u63d0\u51fa\u7528Jensen-Shannon MI\u4f30\u8ba1\u5668\u66ff\u4ee3DV/MINE\uff0c\u5f15\u5165Mutual Information Optimization (MIO)\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u4e92\u4fe1\u606f\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eMIO\u53ef\u4ee5\u7f13\u89e3DPO\u5728\u8bad\u7ec3\u540e\u671f\u51fa\u73b0\u7684\u9009\u62e9\u6982\u7387\u4e0b\u964d\u95ee\u9898\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cMIO\u5728\u591a\u4e2a\u590d\u6742\u7684\u63a8\u7406\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "MIO\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684LLM\u5bf9\u9f50\u8303\u5f0f\uff0c\u6709\u6548\u7f13\u89e3\u4e86DPO\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u5e76\u63ed\u793a\u4e86RLHF\u53ef\u80fd\u65e0\u6cd5\u672c\u8d28\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u539f\u56e0\u3002"}}
{"id": "2506.22919", "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684MoE\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408GRU\u548cFFNN\u4e13\u5bb6\u6765\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63a5\u8fd1\u540c\u8d28\u5316\u57fa\u7ebf\uff0c\u5e76\u5728\u8f83\u5927\u6279\u6b21\u89c4\u6a21\u4e0b\u6027\u80fd\u66f4\u4f73\u3002", "motivation": "\u73b0\u6709\u7684MoE\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u4f9d\u8d56\u76f8\u540c\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u9650\u5236\u4e86\u8868\u793a\u591a\u6837\u6027\uff0c\u4e14\u9759\u6001\u8ba1\u7b97\u8def\u5f84\u5bf9\u9700\u8981\u4e0d\u540c\u7c7b\u578b\u63a8\u7406\u7684\u8f93\u5165\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u4e13\u4e1a\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHecto\u7684\u8f7b\u91cf\u7ea7MoE\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u7a00\u758fTop-1\u95e8\u673a\u5236\u7ed3\u5408\u4e86\u4e00\u4e2a\u7528\u4e8e\u65f6\u95f4\u63a8\u7406\u7684GRU\u4e13\u5bb6\u548c\u4e00\u4e2a\u7528\u4e8e\u9759\u6001\u62bd\u8c61\u7684FFNN\u4e13\u5bb6\uff0c\u5229\u7528\u4e86\u7ed3\u6784\u5f02\u8d28\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\uff08AG News\u3001SST-2\u3001HotpotQA\uff09\u548c\u4e00\u4e2a\u56de\u5f52\u4efb\u52a1\uff08STS-B\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u5c3d\u7ba1\u63a5\u6536\u5230\u5b64\u7acb\u7684\u8f93\u5165\u8868\u793a\uff0cHecto\u7684\u8868\u73b0\u4e0e\u540c\u8d28\u5316\u57fa\u7ebf\u76f8\u5f53\u6216\u63a5\u8fd1\uff1b\u5e76\u4e14\u5b9e\u73b0\u4e86\u6e05\u6670\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e0e\u4e0d\u540c\u7684\u63a8\u7406\u7c7b\u578b\uff08\u65f6\u95f4 vs \u9759\u6001\uff09\u5bf9\u9f50\u3002\u5728\u8f83\u5927\u7684\u6279\u6b21\u89c4\u6a21\u4e0b\uff0c\u7531\u4e8e\u8ba1\u7b97\u7ea6\u675f\u7684\u653e\u677e\uff0cHecto\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002\u6d88\u878d\u7ed3\u679c\u8868\u660e\uff0cHecto\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6e90\u4e8e\u5176\u67b6\u6784\u591a\u6837\u6027\u3002", "conclusion": "Hecto\u786e\u7acb\u4e86\u6761\u4ef6\u8ba1\u7b97\u7684\u65b0\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u6846\u67b6\uff0c\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u901a\u8fc7\u6709\u539f\u5219\u7684\u4e13\u4e1a\u5316\u5b9e\u73b0\u4e13\u95e8\u63a8\u7406\u3002"}}
{"id": "2506.23487", "pdf": "https://arxiv.org/pdf/2506.23487", "abs": "https://arxiv.org/abs/2506.23487", "authors": ["Haoshu Xu", "Hongzhe Li"], "title": "Test of partial effects for Frechet regression on Bures-Wasserstein manifolds", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose a novel test for assessing partial effects in Frechet regression\non Bures Wasserstein manifolds. Our approach employs a sample splitting\nstrategy: the first subsample is used to fit the Frechet regression model,\nyielding estimates of the covariance matrices and their associated optimal\ntransport maps, while the second subsample is used to construct the test\nstatistic. We prove that this statistic converges in distribution to a weighted\nmixture of chi squared components, where the weights correspond to the\neigenvalues of an integral operator defined by an appropriate RKHS kernel. We\nestablish that our procedure achieves the nominal asymptotic size and\ndemonstrate that its worst-case power converges uniformly to one. Through\nextensive simulations and a real data application, we illustrate the test's\nfinite-sample accuracy and practical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30Bures Wasserstein\u6d41\u5f62\u4e0aFrechet\u56de\u5f52\u7684\u90e8\u5206\u6548\u5e94\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u6837\u672c\u5206\u5272\u7b56\u7565\uff1a\u7b2c\u4e00\u90e8\u5206\u6837\u672c\u7528\u4e8e\u62df\u5408Frechet\u56de\u5f52\u6a21\u578b\uff0c\u5f97\u5230\u534f\u65b9\u5dee\u77e9\u9635\u53ca\u5176\u76f8\u5173\u7684\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u7684\u4f30\u8ba1\uff1b\u7b2c\u4e8c\u90e8\u5206\u6837\u672c\u7528\u4e8e\u6784\u5efa\u68c0\u9a8c\u7edf\u8ba1\u91cf\u3002\u8bc1\u660e\u4e86\u8be5\u7edf\u8ba1\u91cf\u5728\u5206\u5e03\u4e0a\u6536\u655b\u5230\u52a0\u6743\u5361\u65b9\u5206\u91cf\u6df7\u5408\u5206\u5e03\uff0c\u6743\u91cd\u4e3a\u7531\u9002\u5f53RKHS\u6838\u5b9a\u4e49\u7684\u79ef\u5206\u7b97\u5b50\u7684\u7279\u5f81\u503c\u3002\u8fd8\u8bc1\u660e\u4e86\u8be5\u7a0b\u5e8f\u8fbe\u5230\u540d\u4e49\u4e0a\u7684\u6e10\u8fd1\u5927\u5c0f\uff0c\u5e76\u4e14\u5176\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u529f\u6548\u4e00\u81f4\u6536\u655b\u52301\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u8be5\u6d4b\u8bd5\u5728\u6709\u9650\u6837\u672c\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5728Bures Wasserstein\u6d41\u5f62\u4e0a\u8fdb\u884cFrechet\u56de\u5f52\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u90e8\u5206\u6548\u5e94\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u8fd9\u79cd\u590d\u6742\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6d4b\u8bd5\u65b9\u6cd5\u4ee5\u9002\u5e94\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6837\u672c\u5206\u5272\u7b56\u7565\uff0c\u5c06\u6570\u636e\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u4e00\u90e8\u5206\u7528\u4e8e\u62df\u5408Frechet\u56de\u5f52\u6a21\u578b\uff0c\u5f97\u5230\u534f\u65b9\u5dee\u77e9\u9635\u53ca\u5176\u76f8\u5173\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u7684\u4f30\u8ba1\uff1b\u53e6\u4e00\u90e8\u5206\u7528\u4e8e\u6784\u5efa\u68c0\u9a8c\u7edf\u8ba1\u91cf\u3002\u5229\u7528\u9002\u5f53\u7684RKHS\u6838\u5b9a\u4e49\u79ef\u5206\u7b97\u5b50\uff0c\u786e\u5b9a\u6743\u91cd\uff0c\u4ece\u800c\u5f62\u6210\u52a0\u6743\u5361\u65b9\u5206\u91cf\u6df7\u5408\u5206\u5e03\u3002", "result": "\u8bc1\u660e\u4e86\u68c0\u9a8c\u7edf\u8ba1\u91cf\u5728\u5206\u5e03\u4e0a\u6536\u655b\u5230\u52a0\u6743\u5361\u65b9\u5206\u91cf\u6df7\u5408\u5206\u5e03\uff0c\u6743\u91cd\u4e3a\u79ef\u5206\u7b97\u5b50\u7684\u7279\u5f81\u503c\u3002\u8be5\u7a0b\u5e8f\u8fbe\u5230\u540d\u4e49\u4e0a\u7684\u6e10\u8fd1\u5927\u5c0f\uff0c\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u529f\u6548\u4e00\u81f4\u6536\u655b\u52301\u3002\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6d4b\u8bd5\u5728\u6709\u9650\u6837\u672c\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6d4b\u8bd5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8bc4\u4f30Bures Wasserstein\u6d41\u5f62\u4e0aFrechet\u56de\u5f52\u7684\u90e8\u5206\u6548\u5e94\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22602", "pdf": "https://arxiv.org/pdf/2506.22602", "abs": "https://arxiv.org/abs/2506.22602", "authors": ["Joshua C. Zhao", "Saurabh Bagchi"], "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?", "categories": ["cs.LG", "stat.ML"], "comment": "13 pages", "summary": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5bf9\u6297\u9c81\u68d2\u8f6c\u79fb\u5b66\u4e60\u4e2d\u4f7f\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u6cd5\uff08FGSM\uff09\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5177\u6709\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u4e8e\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u65b9\u6cd5\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u6a21\u578b\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u975e\u5e38\u9ad8\uff0c\u56e0\u6b64\u8f6c\u79fb\u5b66\u4e60\u6210\u4e3a\u4e00\u79cd\u51cf\u5c11\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u9ad8\u9c81\u68d2\u6027\u7684\u8f6c\u79fb\u5b66\u4e60\u4ecd\u7136\u9700\u8981\u5728\u5fae\u8c03\u9636\u6bb5\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\uff0c\u8fd9\u6bd4\u6807\u51c6\u5fae\u8c03\u9700\u8981\u591a\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u65f6\u95f4\u3002", "method": "\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5728\u9c81\u68d2\u8f6c\u79fb\u5b66\u4e60\u4e2d\u4f7f\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u6cd5\uff08FGSM\uff09\u4ee5\u6539\u5584\u5bf9\u6297\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u53d1\u73b0FGSM\u5728\u5bf9\u6297\u5fae\u8c03\u65f6\u6bd4\u4ece\u96f6\u8bad\u7ec3\u65f6\u66f4\u52a0\u7a33\u5b9a\uff0c\u5e76\u4e14\u8fd9\u79cd\u7a33\u5b9a\u6027\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002", "result": "\u4e0e\u5e38\u7528\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u65b9\u6cd5\u76f8\u6bd4\uff0cFGSM\u5e73\u5747\u4ec5\u635f\u59310.39%\u548c1.39%\u7684\u6d4b\u8bd5\u9c81\u68d2\u6027\uff08\u5bf9\u4e8e\u03b5=4\u548c\u03b5=8\uff09\uff0c\u4f46\u4f7f\u7528\u7684\u8bad\u7ec3\u65f6\u95f4\u4ec5\u4e3aPGD\u7684\u56db\u5206\u4e4b\u4e00\u3002\u6b64\u5916\uff0cFGSM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u826f\u597d\uff0c\u663e\u793a\u51fa\u5176\u4e0d\u4ec5\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u4e14\u4e5f\u662f\u4e00\u79cd\u8868\u73b0\u826f\u597d\u7684\u65b9\u6cd5\u3002", "conclusion": "FGSM\u5728\u5bf9\u6297\u5fae\u8c03\u4e2d\u7684\u7a33\u5b9a\u6027\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u663e\u8457\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u826f\u597d\u7684\u5bf9\u6297\u9c81\u68d2\u8f6c\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.22920", "pdf": "https://arxiv.org/pdf/2506.22920", "abs": "https://arxiv.org/abs/2506.22920", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "categories": ["cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u673a\u5236\uff0c\u7279\u522b\u662f\u8bbe\u8ba1\u7684Critic-Discernment Game(CDG)\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u3001\u9010\u6b65\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u4fee\u6b63\u548c\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5bf9\u81ea\u8eab\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5bf9\u81ea\u5df1\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6216\u66f4\u4f18\u6a21\u578b\u7684\u76d1\u7763\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u901a\u8fc7\u81ea\u5bf9\u5f08\u6765\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u5408\u7406\u6027\u7684\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aCritic-Discernment Game(CDG)\u7684\u6e38\u620f\uff0c\u5728\u8fd9\u4e2a\u6e38\u620f\u4e2d\uff0c\u4e00\u4e2a\u2018\u8bc1\u660e\u8005\u2019\u5148\u63d0\u4f9b\u4e00\u4e2a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u63a5\u53d7\u6765\u81ea\u2018\u6279\u8bc4\u8005\u2019\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u6279\u8bc4\u53ef\u80fd\u662f\u5efa\u8bbe\u6027\u7684\uff08\u5e2e\u52a9\u6539\u8fdb\uff09\u6216\u8bef\u5bfc\u6027\u7684\uff08\u8bd5\u56fe\u6df7\u6dc6\uff09\u3002\u2018\u8bc1\u660e\u8005\u2019\u9700\u8981\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u8bc4\u8bba\u65f6\u4fdd\u6301\u6b63\u786e\u7b54\u6848\uff0c\u540c\u65f6\u6839\u636e\u5efa\u8bbe\u6027\u53cd\u9988\u7ea0\u6b63\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCDG\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5df2\u826f\u597d\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u81ea\u8eab\u63a8\u7406\u8fc7\u7a0b\u65b9\u9762\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u63d0\u5347\u5728\u6d89\u53ca\u6570\u5b66\u63a8\u7406\u3001\u9010\u6b65\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u4fee\u6b63\u4ee5\u53ca\u957f\u94fe\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165Critic-Discernment Game\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u4eba\u7c7b\u6216\u9ad8\u7ea7\u6a21\u578b\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u5408\u7406\u6027\u3002\u8fd9\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u4f18\u5316LLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.22621", "pdf": "https://arxiv.org/pdf/2506.22621", "abs": "https://arxiv.org/abs/2506.22621", "authors": ["Paul Saves", "Edward Hall\u00e9-Hannan", "Jasper Bussemaker", "Youssef Diouane", "Nathalie Bartoli"], "title": "Hierarchical Modeling and Architecture Optimization: Review and Unified Framework", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Simulation-based problems involving mixed-variable inputs frequently feature\ndomains that are hierarchical, conditional, heterogeneous, or tree-structured.\nThese characteristics pose challenges for data representation, modeling, and\noptimization. This paper reviews extensive literature on these structured input\nspaces and proposes a unified framework that generalizes existing approaches.\nIn this framework, input variables may be continuous, integer, or categorical.\nA variable is described as meta if its value governs the presence of other\ndecreed variables, enabling the modeling of conditional and hierarchical\nstructures.\n  We further introduce the concept of partially-decreed variables, whose\nactivation depends on contextual conditions. To capture these inter-variable\nhierarchical relationships, we introduce design space graphs, combining\nprinciples from feature modeling and graph theory. This allows the definition\nof general hierarchical domains suitable for describing complex system\narchitectures. The framework supports the use of surrogate models over such\ndomains and integrates hierarchical kernels and distances for efficient\nmodeling and optimization. The proposed methods are implemented in the\nopen-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are\ndemonstrated through applications in Bayesian optimization for complex system\ndesign, including a case study in green aircraft architecture.", "AI": {"tldr": "This paper proposes a unified framework for handling mixed-variable input spaces in simulation-based problems, introducing meta and partially-decreed variables, design space graphs, and integrating hierarchical kernels for efficient modeling and optimization. Demonstrated through Bayesian optimization applications.", "motivation": "Simulation-based problems often involve complex input spaces that are hierarchical, conditional, heterogeneous, or tree-structured, posing challenges for data representation, modeling, and optimization.", "method": "The paper introduces a framework where input variables can be continuous, integer, or categorical. It defines meta variables that govern the presence of other decreed variables, and partially-decreed variables whose activation depends on contextual conditions. Design space graphs are used to capture inter-variable hierarchical relationships, combining principles from feature modeling and graph theory. The framework supports surrogate models with hierarchical kernels and distances.", "result": "The methods were implemented in the open-source SMT 2.0 toolbox and demonstrated effectiveness in Bayesian optimization for complex system design, including a case study in green aircraft architecture.", "conclusion": "The proposed framework generalizes existing approaches for structured input spaces and provides tools for efficient modeling and optimization of complex hierarchical domains."}}
{"id": "2506.22992", "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbi\u0107", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMARBLE\u7684\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u95ee\u9898\u4e2d\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002\u5f53\u524d\u6700\u5148\u8fdb\u768412\u4e2a\u6a21\u578b\u5728MARBLE\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u663e\u793a\u51fa\u590d\u6742\u63a8\u7406\u548c\u611f\u77e5\u4fe1\u606f\u63d0\u53d6\u4ecd\u7136\u662f\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u7eaf\u6587\u672c\u63a8\u7406\u6216\u591a\u6a21\u6001\u95ee\u9898\u4e0a\uff0c\u4f46\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u76f4\u63a5\u4ece\u975e\u6587\u672c\u6a21\u6001\u4e2d\u68c0\u7d22\u4fe1\u606f\u6765\u56de\u7b54\uff0c\u56e0\u6b64\u5bf9\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u7684\u7406\u89e3\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aMARBLE\u7684\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\uff0c\u5305\u62ec\u4e24\u4e2a\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1M-Portal\u548cM-Cube\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5728\u7a7a\u95f4\u3001\u89c6\u89c9\u548c\u7269\u7406\u7ea6\u675f\u4e0b\u5236\u5b9a\u548c\u7406\u89e3\u591a\u6b65\u9aa4\u8ba1\u5212\u3002\u901a\u8fc7\u8fd9\u4e9b\u4efb\u52a1\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d12\u4e2a\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728MARBLE\u4e0a\u7684\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u5728M-Cube\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4e3a0%\u3002\u4ec5\u5728\u7b80\u5316\u5b50\u4efb\u52a1\u4e2d\uff0c\u90e8\u5206\u6a21\u578b\u8d85\u8fc7\u4e86\u968f\u673a\u57fa\u7ebf\u3002\u8fd9\u8bf4\u660e\u590d\u6742\u63a8\u7406\u4ecd\u662f\u5bf9\u73b0\u6709\u6a21\u578b\u7684\u91cd\u5927\u6311\u6218\u3002\u6b64\u5916\uff0c\u611f\u77e5\u4ecd\u7136\u662f\u4e00\u4e2a\u74f6\u9888\uff0c\u6a21\u578b\u5076\u5c14\u65e0\u6cd5\u4ece\u89c6\u89c9\u8f93\u5165\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002", "conclusion": "MARBLE\u63ed\u793a\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548c\u611f\u77e5\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e0c\u671b\u501f\u6b64\u63a8\u52a8\u4e0b\u4e00\u4ee3\u80fd\u591f\u8de8\u591a\u79cd\u591a\u6a21\u6001\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\u7684\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22631", "pdf": "https://arxiv.org/pdf/2506.22631", "abs": "https://arxiv.org/abs/2506.22631", "authors": ["Dmitry B. Rokhlin"], "title": "A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS", "categories": ["cs.LG", "stat.ML", "68Q32, 68W27, 68W20"], "comment": null, "summary": "We study the problem of online regression with the unconstrained quadratic\nloss against a time-varying sequence of functions from a Reproducing Kernel\nHilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a\ndiscounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic\nregret in the finite-dimensional case. In this work, we lift their approach to\nthe non-parametric domain by synthesizing the DVAW framework with a random\nfeature approximation. We propose a fully adaptive, hierarchical algorithm,\nwhich we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that\nlearns both the discount factor and the number of random features. We prove\nthat this algorithm, which has a per-iteration computational complexity of\n$O(T\\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +\n\\sqrt{T}\\ln T)$, where $P_T$ is the functional path length of a comparator\nsequence.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u56de\u5f52\u7b97\u6cd5H-VAW-D\uff0c\u7ed3\u5408\u4e86\u6298\u6263Vovk-Azoury-Warmuth\u6846\u67b6\u4e0e\u968f\u673a\u7279\u5f81\u903c\u8fd1\uff0c\u5728\u975e\u53c2\u6570\u57df\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u52a8\u6001\u540e\u6094\u754c\u3002", "motivation": "\u5728\u7ebf\u56de\u5f52\u95ee\u9898\u4e2d\uff0c\u9488\u5bf9\u6765\u81ea\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u7684\u65f6\u95f4\u53d8\u5316\u51fd\u6570\u5e8f\u5217\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u65e0\u7ea6\u675f\u4e8c\u6b21\u635f\u5931\u4e0b\u5b9e\u73b0\u6700\u4f18\u52a8\u6001\u540e\u6094\u754c\u7684\u7b97\u6cd5\u3002\u73b0\u6709\u7684\u6709\u9650\u7ef4\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u975e\u53c2\u6570\u57df\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u548c\u6539\u8fdb\u65b9\u6cd5\u4ee5\u9002\u5e94\u66f4\u590d\u6742\u7684\u573a\u666f\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5c06\u6298\u6263Vovk-Azoury-Warmuth\uff08DVAW\uff09\u6846\u67b6\u4e0e\u968f\u673a\u7279\u5f81\u903c\u8fd1\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u9002\u5e94\u7684\u5206\u5c42\u7b97\u6cd5H-VAW-D\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u6298\u6263\u56e0\u5b50\u548c\u968f\u673a\u7279\u5f81\u7684\u6570\u91cf\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a$O(T\\ln T)$\u3002", "result": "\u6240\u63d0\u51fa\u7684H-VAW-D\u7b97\u6cd5\u5728\u52a8\u6001\u540e\u6094\u754c\u4e0a\u8fbe\u5230\u4e86$O(T^{2/3}P_T^{1/3} + \\sqrt{T}\\ln T)$\uff0c\u5176\u4e2d$P_T$\u662f\u6bd4\u8f83\u5668\u5e8f\u5217\u7684\u529f\u80fd\u8def\u5f84\u957f\u5ea6\u3002\u8fd9\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u975e\u53c2\u6570\u57df\u4e2d\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "H-VAW-D\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u6709\u9650\u7ef4\u7684\u6298\u6263Vovk-Azoury-Warmuth\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u53c2\u6570\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7406\u8bba\u4e0a\u53ef\u8bc1\u660e\u7684\u52a8\u6001\u540e\u6094\u754c\uff0c\u4e3a\u5728\u7ebf\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u9014\u5f84\u3002"}}
{"id": "2506.23049", "pdf": "https://arxiv.org/pdf/2506.23049", "abs": "https://arxiv.org/abs/2506.23049", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "AI": {"tldr": "\u5c3d\u7ba1\u8bed\u8a00\u548c\u8bed\u97f3\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u6ca1\u6709\u5f00\u6e90\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u5b8c\u6574\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u3001\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5e76\u7ed3\u5408\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u63a8\u7406\u3002\u672c\u6587\u4ecb\u7ecd\u4e86AURA\uff08\u7528\u4e8e\u7406\u89e3\u3001\u63a8\u7406\u548c\u81ea\u52a8\u5de5\u5177\u4f7f\u7528\u7684\u4ee3\u7406\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee5\u8bed\u97f3\u4e3a\u6838\u5fc3\u7684\u52a9\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u8c03\u7528\u548c\u591a\u8f6e\u5bf9\u8bdd\u5b8c\u6210\u590d\u6742\u7684\u3001\u76ee\u6807\u9a71\u52a8\u7684\u4efb\u52a1\u3002AURA\u5728\u7ea7\u8054\u7ba1\u9053\u4e2d\u7ed3\u5408\u4e86\u5f00\u653e\u6743\u91cd\u7684ASR\u3001TTS\u548cLLM\uff0c\u5e76\u652f\u6301\u65e5\u5386\u9884\u8ba2\u3001\u8054\u7cfb\u4eba\u67e5\u627e\u3001\u7f51\u7edc\u641c\u7d22\u548c\u7535\u5b50\u90ae\u4ef6\u7b49\u5de5\u5177\u3002\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u52a8\u4f5c\u7c7b\u8f7b\u677e\u96c6\u6210\u65b0\u5de5\u5177\u3002\u5728VoiceBench\u4e0a\uff0cAURA\u5728OpenBookQA\u4e0a\u7684\u5f97\u5206\u4e3a92.75%\uff0c\u4f18\u4e8e\u6240\u6709\u5f00\u653e\u6743\u91cd\u7cfb\u7edf\u5e76\u63a5\u8fd1GPT-4o\uff0c\u5728AlpacaEval\u4e0a\u7684\u5f97\u5206\u4e3a4.39\uff0c\u4e0e\u5176\u5b83\u5f00\u653e\u6743\u91cd\u7cfb\u7edf\u7ade\u4e89\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0cAURA\u5728\u590d\u6742\u7684\u3001\u591a\u8f6e\u8bed\u97f3\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u4e3a90%\u3002", "motivation": "\u5f53\u524d\u6ca1\u6709\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u6574\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u3001\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5e76\u4e14\u80fd\u591f\u7ed3\u5408\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u63a8\u7406\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u7684\u7cfb\u7edf\u3002", "method": "AURA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8bed\u97f3\u6838\u5fc3\u52a9\u624b\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u5f00\u653e\u6743\u91cd\u7684ASR\u3001TTS\u548cLLM\u5728\u4e00\u4e2a\u7ea7\u8054\u7ba1\u9053\u4e2d\u5de5\u4f5c\u3002\u5b83\u652f\u6301\u591a\u79cd\u5de5\u5177\u5982\u65e5\u5386\u9884\u8ba2\u3001\u8054\u7cfb\u4eba\u67e5\u627e\u3001\u7f51\u7edc\u641c\u7d22\u548c\u7535\u5b50\u90ae\u4ef6\uff0c\u5e76\u4e14\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5141\u8bb8\u8f7b\u677e\u96c6\u6210\u65b0\u7684\u5de5\u5177\u3002", "result": "AURA\u5728VoiceBench\u4e0a\u7684\u8868\u73b0\u4f18\u5f02\uff0cOpenBookQA\u5f97\u520692.75%\uff0c\u63a5\u8fd1GPT-4o\uff1bAlpacaEval\u5f97\u5206\u4e3a4.39\uff0c\u4e0e\u5176\u5b83\u5f00\u653e\u6743\u91cd\u7cfb\u7edf\u7ade\u4e89\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523090%\u3002", "conclusion": "AURA\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u3001\u4ee5\u8bed\u97f3\u4e3a\u6838\u5fc3\u7684\u52a9\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u8c03\u7528\u548c\u591a\u8f6e\u5bf9\u8bdd\u5b8c\u6210\u590d\u6742\u7684\u3001\u76ee\u6807\u9a71\u52a8\u7684\u4efb\u52a1\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u95ed\u6e90\u9876\u7ea7\u6a21\u578b\uff0c\u5e76\u5728\u590d\u6742\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\u3002"}}
{"id": "2506.22638", "pdf": "https://arxiv.org/pdf/2506.22638", "abs": "https://arxiv.org/abs/2506.22638", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u6216\u77e5\u8bc6\u84b8\u998f\u7684\u540e\u8bad\u7ec3\u540e\uff0c\u5176\u6570\u5b66\u63a8\u7406\u80fd\u529b\u6709\u6240\u63d0\u9ad8\u3002\u4f46\u8fd9\u4e9b\u6539\u8fdb\u662f\u6e90\u4e8etransformer\u5c42\u7684\u91cd\u5927\u53d8\u5316\u8fd8\u662f\u7ec6\u5fae\u8c03\u6574\u5c1a\u4e0d\u6e05\u695a\u3002\u901a\u8fc7\u9010\u5c42\u6d88\u878d\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u5b66\u63a8\u7406\u4f1a\u4ea7\u751f\u7279\u5b9a\u7684\u5c42\u91cd\u8981\u6027\u7ed3\u6784\uff0c\u8fd9\u79cd\u7ed3\u6784\u5728\u6240\u6709\u540e\u8bad\u7ec3\u8303\u5f0f\u4e2d\u90fd\u6301\u7eed\u5b58\u5728\u3002\u79fb\u9664\u8fd9\u4e9b\u5c42\u4f1a\u5bfc\u81f4\u9ad8\u8fbe80%\u7684\u51c6\u786e\u7387\u4e0b\u964d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u975e\u6570\u5b66\u4efb\u52a1\uff08\u5982\u4e8b\u5b9e\u56de\u5fc6\uff09\u6ca1\u6709\u5173\u952e\u5c42\u3002\u8fd9\u8868\u660e\u6570\u5b66\u63a8\u7406\u9700\u8981\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u51fa\u73b0\u7684\u4e13\u95e8\u5c42\uff0c\u800c\u5176\u4ed6\u975e\u63a8\u7406\u4efb\u52a1\u5219\u4e0d\u9700\u8981\u3002\u4ece\u4fe1\u606f\u8bba\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u4e9b\u5173\u952e\u5c42\u4e5f\u662f\u53d1\u751f\u91cd\u5927\u8868\u793a\u8f6c\u6362\u7684\u5c42\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u662f\u5426\u6765\u6e90\u4e8etransformer\u5c42\u7684\u91cd\u5927\u53d8\u5316\u8fd8\u662f\u7ec6\u5fae\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u9010\u5c42\u6d88\u878d\u5b9e\u9a8c\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u3001\u6307\u4ee4\u5fae\u8c03\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u53d8\u4f53\u8fdb\u884c\u5206\u6790\uff0c\u4f7f\u7528\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6570\u5b66\u63a8\u7406\u4ea7\u751f\u7279\u5b9a\u7684\u5c42\u91cd\u8981\u6027\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u5728\u6240\u6709\u540e\u8bad\u7ec3\u8303\u5f0f\u4e2d\u6301\u7eed\u5b58\u5728\uff1b\u79fb\u9664\u8fd9\u4e9b\u5c42\u4f1a\u5bfc\u81f4\u9ad8\u8fbe80%\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff1b\u975e\u6570\u5b66\u4efb\u52a1\u6ca1\u6709\u5173\u952e\u5c42\uff1b\u6570\u5b66\u63a8\u7406\u9700\u8981\u4e13\u95e8\u5c42\uff0c\u8fd9\u4e9b\u5c42\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u51fa\u73b0\u3002", "conclusion": "\u6570\u5b66\u63a8\u7406\u9700\u8981\u4e13\u95e8\u5c42\uff0c\u8fd9\u4e9b\u5c42\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5f62\u6210\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002"}}
{"id": "2506.23080", "pdf": "https://arxiv.org/pdf/2506.23080", "abs": "https://arxiv.org/abs/2506.23080", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4e94\u9636\u6bb5\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4ee5\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u5e76\u8868\u660e\u5176\u8f68\u8ff9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u6280\u672f\u7684\u5386\u53f2\u8fdb\u6b65\u76f8\u547c\u5e94\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u8de8\u5b66\u79d1\u6a21\u578b\u6765\u89e3\u91caAI\u8fc7\u53bb\u7684\u67b6\u6784\u8f6c\u53d8\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u5177\u4f53\u53ef\u884c\u7684\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u201c\u8ba4\u77e5\u51e0\u4f55\u201d\u6846\u67b6\uff0c\u63cf\u8ff0AI\u4ece\u4e13\u5bb6\u7cfb\u7edf\u5230Transformer\u7684\u6f14\u53d8\uff0c\u5e76\u9884\u6d4b\u672a\u6765\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u548c\u7a0b\u5e8f\u5408\u6210\u3002", "result": "\u5c55\u793a\u4e86AI\u7684\u8fdb\u5316\u5e76\u975e\u7ebf\u6027\u800c\u662f\u81ea\u53cd\u6027\u7684\uff0c\u5de5\u5177\u548c\u89c1\u89e3\u7684\u53cd\u9988\u5faa\u73af\u91cd\u5851\u4e86AI\u7684\u57fa\u7840\u67b6\u6784\uff0c\u5e76\u4e14\u76ee\u524d\u6b63\u8fdb\u5165\u5177\u6709\u81ea\u6211\u53cd\u601d\u80fd\u529b\u7684'\u5143\u8bed\u8a00\u65f6\u523b'\u3002", "conclusion": "\u672c\u7814\u7a76\u4f5c\u4e3a\u4e09\u90e8\u66f2\u7684\u65b9\u6cd5\u8bba\u603b\u7ed3\uff0c\u63a2\u8ba8\u4e86AI\u7684\u7ecf\u6d4e\u9a71\u52a8\u56e0\u7d20\uff08\u4e3a\u4ec0\u4e48\uff09\u548c\u8ba4\u77e5\u672c\u8d28\uff08\u662f\u4ec0\u4e48\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u7b56\u7565\u3002"}}
{"id": "2506.22645", "pdf": "https://arxiv.org/pdf/2506.22645", "abs": "https://arxiv.org/abs/2506.22645", "authors": ["Amir Hossein Rahmati", "Nathan M. Urban", "Byung-Jun Yoon", "Xiaoning Qian"], "title": "Cost-effective Reduced-Order Modeling via Bayesian Active Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine Learning surrogates have been developed to accelerate solving systems\ndynamics of complex processes in different science and engineering\napplications. To faithfully capture governing systems dynamics, these methods\nrely on large training datasets, hence restricting their applicability in\nreal-world problems. In this work, we propose BayPOD-AL, an active learning\nframework based on an uncertainty-aware Bayesian proper orthogonal\ndecomposition (POD) approach, which aims to effectively learn reduced-order\nmodels from high-fidelity full-order models representing complex systems.\nExperimental results on predicting the temperature evolution over a rod\ndemonstrate BayPOD-AL's effectiveness in suggesting the informative data and\nreducing computational cost related to constructing a training dataset compared\nto other uncertainty-guided active learning strategies. Furthermore, we\ndemonstrate BayPOD-AL's generalizability and efficiency by evaluating its\nperformance on a dataset of higher temporal resolution than the training\ndataset.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBayPOD-AL\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u6b63\u4ea4\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u4fdd\u771f\u5168\u9636\u6a21\u578b\u4e2d\u6709\u6548\u5730\u5b66\u4e60\u964d\u9636\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6746\u6e29\u6f14\u5316\u65f6\u80fd\u6709\u6548\u9009\u62e9\u4fe1\u606f\u6570\u636e\u5e76\u51cf\u5c11\u6784\u5efa\u8bad\u7ec3\u96c6\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u51c6\u786e\u6355\u6349\u590d\u6742\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u6240\u9700\u8bad\u7ec3\u6570\u636e\u91cf\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86BayPOD-AL\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u6b63\u4ea4\u5206\u89e3\uff08POD\uff09\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9ad8\u4fdd\u771f\u5168\u9636\u6a21\u578b\u4e2d\u5b66\u4e60\u964d\u9636\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6570\u636e\u70b9\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBayPOD-AL\u5728\u9884\u6d4b\u6746\u6e29\u6f14\u5316\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u5176\u5b83\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u76f8\u6bd4\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5728\u66f4\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "conclusion": "BayPOD-AL\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u5bf9\u590d\u6742\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2506.23107", "pdf": "https://arxiv.org/pdf/2506.23107", "abs": "https://arxiv.org/abs/2506.23107", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "categories": ["cs.AI"], "comment": "20 pages, 1 figure", "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u98ce\u9669\u51b3\u7b56\u884c\u4e3a\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0cChatGPT 4o\u548co1-mini\u5728\u591a\u9879\u62bd\u5f69\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u4fdd\u5b88\u7684\u98ce\u9669\u504f\u597d\uff0c\u5176\u4e2do1-mini\u4e0e\u4eba\u7c7b\u51b3\u7b56\u66f4\u63a5\u8fd1\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u4e2d\u6587\u73af\u5883\u4e0b\u7684\u9884\u6d4b\u504f\u5dee\u8f83\u5927\uff0c\u63d0\u793a\u8bed\u8a00\u53ef\u80fd\u5f71\u54cd\u6a21\u62df\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u590d\u6742\u51b3\u7b56\uff08\u5982\u98ce\u9669\u51b3\u7b56\uff09\u65b9\u9762\u7684\u53ef\u9760\u6027\u4ecd\u53d7\u5230\u8d28\u7591\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u98ce\u9669\u51b3\u7b56\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7ed9\u4e24\u4e2aLLM\uff08ChatGPT 4o\u548co1-mini\uff09\uff0c\u8ba9\u5b83\u4eec\u9884\u6d4b\u4e2a\u4f53\u5728\u4e00\u7cfb\u5217\u57fa\u4e8e\u5f69\u7968\u7684\u4efb\u52a1\u4e2d\u7684\u9009\u62e9\uff0c\u5e76\u5c06\u6a21\u578b\u751f\u6210\u7684\u51b3\u7b56\u4e0e\u6765\u81ea\u6089\u5c3c\u3001\u8fbe\u5361\u3001\u9999\u6e2f\u548c\u5357\u4eac\u53c2\u4e0e\u8005\u7684\u5b9e\u9645\u4eba\u7c7b\u53cd\u5e94\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528CRRA\u6846\u67b6\u5206\u6790\u98ce\u9669\u504f\u597d\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5747\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u4fdd\u5b88\u7684\u98ce\u9669\u504f\u597d\uff0c\u5176\u4e2do1-mini\u7684\u8868\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u4e2d\u6587\u73af\u5883\u4e0b\u6a21\u578b\u9884\u6d4b\u4e0e\u5b9e\u9645\u53cd\u5e94\u7684\u504f\u5dee\u66f4\u5927\uff0c\u8868\u660e\u63d0\u793a\u8bed\u8a00\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u590d\u5236\u4eba\u7c7b\u98ce\u9669\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.22655", "pdf": "https://arxiv.org/pdf/2506.22655", "abs": "https://arxiv.org/abs/2506.22655", "authors": ["Andrew F. Ilersich", "Prasanth B. Nair"], "title": "Learning Stochastic Multiscale Models", "categories": ["cs.LG"], "comment": "Body is 9 pages, 13 including acknowledgements and references, 35\n  including appendix. 21 figures and 6 tables. Submitted to NeurIPS 2025", "summary": "The physical sciences are replete with dynamical systems that require the\nresolution of a wide range of length and time scales. This presents significant\ncomputational challenges since direct numerical simulation requires\ndiscretization at the finest relevant scales, leading to a high-dimensional\nstate space. In this work, we propose an approach to learn stochastic\nmultiscale models in the form of stochastic differential equations directly\nfrom observational data. Our method resolves the state on a coarse mesh while\nintroducing an auxiliary state to capture the effects of unresolved scales. We\nlearn the parameters of the multiscale model using a modern forward-solver-free\namortized variational inference method. Our approach draws inspiration from\nphysics-based multiscale modeling approaches, such as large-eddy simulation in\nfluid dynamics, while learning directly from data. We present numerical studies\nto demonstrate that our learned multiscale models achieve superior predictive\naccuracy compared to direct numerical simulation and closure-type models at\nequivalent resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u76f4\u63a5\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u72b6\u6001\u53d8\u91cf\u6355\u6349\u672a\u89e3\u6790\u5c3a\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u73b0\u4ee3\u65e0\u524d\u5411\u6c42\u89e3\u5668\u7684\u644a\u9500\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u5b66\u4e60\u591a\u5c3a\u5ea6\u6a21\u578b\u53c2\u6570\u3002\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u6240\u5b66\u7684\u591a\u5c3a\u5ea6\u6a21\u578b\u5728\u7b49\u6548\u5206\u8fa8\u7387\u4e0b\u6bd4\u76f4\u63a5\u6570\u503c\u6a21\u62df\u548c\u5c01\u95ed\u578b\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7269\u7406\u79d1\u5b66\u4e2d\u5145\u6ee1\u4e86\u8bb8\u591a\u9700\u8981\u89e3\u6790\u5e7f\u6cdb\u957f\u5ea6\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u52a8\u529b\u7cfb\u7edf\uff0c\u76f4\u63a5\u6570\u503c\u6a21\u62df\u4f1a\u5bfc\u81f4\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u76f4\u63a5\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u76f4\u63a5\u5b66\u4e60\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5f62\u5f0f\u7684\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u7c97\u7f51\u683c\u4e0a\u89e3\u6790\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u72b6\u6001\u4ee5\u6355\u83b7\u672a\u89e3\u6790\u5c3a\u5ea6\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u73b0\u4ee3\u65e0\u524d\u5411\u6c42\u89e3\u5668\u7684\u644a\u9500\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u5b66\u4e60\u591a\u5c3a\u5ea6\u6a21\u578b\u53c2\u6570\u3002", "result": "\u6570\u503c\u7814\u7a76\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u591a\u5c3a\u5ea6\u6a21\u578b\u5728\u7b49\u6548\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u76f4\u63a5\u6570\u503c\u6a21\u62df\u548c\u5c01\u95ed\u578b\u6a21\u578b\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u5904\u7406\u9ad8\u7ef4\u5ea6\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2506.23123", "pdf": "https://arxiv.org/pdf/2506.23123", "abs": "https://arxiv.org/abs/2506.23123", "authors": ["Rishi Bommasani"], "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "categories": ["cs.AI", "cs.CY", "cs.ET"], "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3001\u98ce\u9669\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u79d1\u5b66\u7814\u7a76\u548c\u653f\u7b56\u63a5\u53e3\u6539\u5584AI\u6cbb\u7406\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u793e\u4f1a\u7ed3\u679c\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u6280\u672f\u867d\u7136\u5145\u6ee1\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u8bb8\u591a\u56f0\u60d1\u548c\u5371\u5bb3\uff0c\u5bf9\u5176\u7406\u89e3\u4e0d\u8db3\u4fc3\u4f7f\u7814\u7a76\u5176\u4e0e\u793e\u4f1a\u7684\u5171\u540c\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u8bba\u6587\u56f4\u7ed5\u4e09\u4e2a\u4e3b\u9898\u5c55\u5f00\uff1a1) \u6982\u5ff5\u6846\u67b6\uff1a\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3001\u98ce\u9669\u548c\u4f9b\u5e94\u94fe\uff1b2) \u5b9e\u8bc1\u89c1\u89e3\uff1a\u901a\u8fc7\u6a21\u578b\u8bc4\u4f30\u548c\u7ec4\u7ec7\u6307\u6570\u5efa\u7acb\u900f\u660e\u5ea6\uff1b3) \u7406\u89e3\u5230\u884c\u52a8\u7684\u8f6c\u53d8\uff1a\u57fa\u4e8e\u5bf9\u57fa\u7840\u6a21\u578b\u793e\u4f1a\u5f71\u54cd\u7684\u7406\u89e3\u63a8\u8fdb\u8bc1\u636e\u9a71\u52a8\u7684AI\u653f\u7b56\u3002", "result": "\u8bba\u6587\u4e3a\u5728AI\u65f6\u4ee3\u5b9e\u73b0\u66f4\u597d\u7684\u793e\u4f1a\u7ed3\u679c\u63d0\u4f9b\u4e86\u79d1\u5b66\u57fa\u7840\u548c\u7814\u7a76-\u653f\u7b56\u63a5\u53e3\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u79d1\u5b66\u57fa\u7840\u548c\u7814\u7a76-\u653f\u7b56\u63a5\u53e3\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u6cbb\u7406AI\uff0c\u4ece\u800c\u5728AI\u65f6\u4ee3\u5b9e\u73b0\u66f4\u4f18\u7684\u793e\u4f1a\u6210\u679c\u3002"}}
{"id": "2506.22668", "pdf": "https://arxiv.org/pdf/2506.22668", "abs": "https://arxiv.org/abs/2506.22668", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "comment": "12 pages", "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "AI": {"tldr": "DistShap\u662f\u4e00\u79cd\u7528\u4e8e\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u9884\u6d4b\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u591aGPU\u4e0a\u5206\u5e03Shapley\u503c\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21GNN\u89e3\u91ca\u4e2d\u8ba1\u7b97\u6602\u8d35\u7684\u95ee\u9898\uff0c\u5176\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740GNN\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u5176\u9884\u6d4b\u8fdb\u884c\u89e3\u91ca\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u5728\u5f52\u56e0\u4e8e\u7279\u5b9a\u8fb9\u6216\u7279\u5f81\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u8282\u70b9\u548c\u8fb9\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDistShap\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u5b50\u56fe\u91c7\u6837\u3001\u8de8\u591a\u4e2aGPU\u5e76\u884c\u6267\u884cGNN\u63a8\u7406\u4ee5\u53ca\u89e3\u51b3\u5206\u5e03\u5f0f\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u6765\u8ba1\u7b97\u8fb9\u7684\u91cd\u8981\u6027\u5206\u6570\u3002", "result": "DistShap\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u7684GNN\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u80fd\u591f\u6269\u5c55\u5230\u5177\u6709\u6570\u767e\u4e07\u7279\u5f81\u7684GNN\u6a21\u578b\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u8fbe128\u4e2aGPU\u5728NERSC Perlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u3002", "conclusion": "DistShap\u4e3a\u5927\u89c4\u6a21GNN\u6a21\u578b\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.23128", "pdf": "https://arxiv.org/pdf/2506.23128", "abs": "https://arxiv.org/abs/2506.23128", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "categories": ["cs.AI"], "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u4e2a\u524d\u6cbf\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08DeepSeek-R1\u3001DeepSeek-V3\u548cGPT-4o\uff09\u5728\u5bb6\u65cf\u6811\u548c\u901a\u7528\u56fe\u63a8\u7406\u57fa\u51c6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDeepSeek-R1\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u9ad8\u7684F1\u5206\u6570\uff0c\u4f46\u5728\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u65f6\uff0c\u6240\u6709\u6a21\u578b\u90fd\u9762\u4e34\u663e\u8457\u6311\u6218\u3002\u5206\u6790\u8fd8\u63ed\u793a\u4e86DeepSeek-R1\u72ec\u7279\u7684\u89c4\u5212\u4e0e\u9a8c\u8bc1\u7b56\u7565\uff0c\u4f46\u4e5f\u5b58\u5728\u4e0d\u8fde\u8d2f\u6216\u4e0d\u5b8c\u6574\u7684\u63a8\u7406\u60c5\u51b5\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u5173\u952e\u65b9\u5411\uff0c\u5305\u62ec\u591a\u6a21\u6001\u63a8\u7406\u548c\u7cfb\u7edf\u6027\u5206\u6790\u63a8\u7406\u5931\u8d25\u7684\u539f\u56e0\u3002", "motivation": "\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5bb6\u65cf\u6811\u548c\u901a\u7528\u56fe\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u51c6\u4efb\u52a1\u6765\u8bc4\u4f30\u548c\u6bd4\u8f83DeepSeek-R1\u3001DeepSeek-V3\u548cGPT-4o\u8fd9\u4e09\u4e2a\u6a21\u578b\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u5bb6\u65cf\u6811\u548c\u901a\u7528\u56fe\u63a8\u7406\u3002", "result": "DeepSeek-R1\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u95ee\u9898\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684F1\u5206\u6570\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u968f\u7740\u95ee\u9898\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u6240\u6709\u6a21\u578b\u90fd\u9047\u5230\u663e\u8457\u56f0\u96be\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u6807\u8bb0\u957f\u5ea6\u9650\u5236\u548c\u8f93\u51fa\u7ed3\u6784\u4e0d\u5b8c\u6574\u3002\u6b64\u5916\uff0cDeepSeek-R1\u7684\u957f\u94fe\u601d\u7ef4\u54cd\u5e94\u663e\u793a\u4e86\u72ec\u7279\u7684\u89c4\u5212\u548c\u9a8c\u8bc1\u7b56\u7565\uff0c\u4f46\u4e5f\u5b58\u5728\u4e0d\u8fde\u8d2f\u6216\u4e0d\u5b8c\u6574\u7684\u63a8\u7406\u5b9e\u4f8b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7ecf\u9a8c\u89c1\u89e3\u548c\u7406\u8bba\u610f\u4e49\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u7ed3\u6784\u5316\u3001\u591a\u6b65\u9aa4\u903b\u8f91\u63a8\u7406\u7684\u4efb\u52a1\u3002\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\uff0c\u5982\u591a\u6a21\u6001\u63a8\u7406\u548c\u63a8\u7406\u5931\u8d25\u7684\u7cfb\u7edf\u6027\u68c0\u67e5\u3002"}}
{"id": "2506.22685", "pdf": "https://arxiv.org/pdf/2506.22685", "abs": "https://arxiv.org/abs/2506.22685", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u751f\u6210\u4e2a\u6027\u5316\u4e2d\u7684\u8bed\u4e49\u584c\u7f29\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5927\u5c0f\u548c\u65b9\u5411\uff0c\u6709\u6548\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u751f\u6210\u4e2a\u6027\u5316\u4e2d\u5b58\u5728\u8bed\u4e49\u584c\u7f29\u95ee\u9898\uff0c\u5373\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6982\u5ff5\u9010\u6e10\u504f\u79bb\u5176\u539f\u59cb\u6587\u672c\u610f\u4e49\u5e76\u4e3b\u5bfc\u591a\u6982\u5ff5\u8f93\u5165\u63d0\u793a\u3002\u8fd9\u4e0d\u4ec5\u51cf\u5c11\u4e86\u590d\u6742\u8f93\u5165\u63d0\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\uff0c\u8fd8\u5bfc\u81f4\u7b80\u5316\u7684\u8f93\u51fa\u56fe\u50cf\u65e0\u6cd5\u6355\u6349\u76ee\u6807\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u65e0\u7ea6\u675f\u4f18\u5316\u4e3a\u6839\u672c\u539f\u56e0\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5927\u5c0f\u548c\u65b9\u5411\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u8bed\u4e49\u584c\u7f29\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u7528\u4f8b\u4e2d\u663e\u8457\u6539\u5584\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8bed\u4e49\u584c\u7f29\u95ee\u9898\uff0c\u5e76\u5728\u4e0d\u540c\u4e2a\u6027\u5316\u65b9\u6cd5\u4e2d\u5c55\u73b0\u826f\u597d\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u533f\u540d\u53d1\u5e03\u3002"}}
{"id": "2506.23141", "pdf": "https://arxiv.org/pdf/2506.23141", "abs": "https://arxiv.org/abs/2506.23141", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "categories": ["cs.AI"], "comment": null, "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5173\u7cfb\u6d88\u606f\u4f20\u9012\u65b9\u6cd5\uff0c\u901a\u8fc7Top-K\u90bb\u5c45\u9009\u62e9\u7b56\u7565\u548c\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u6709\u6548\u51cf\u8f7b\u566a\u58f0\u548c\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8282\u70b9\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5f15\u5165\u566a\u58f0\uff0c\u5bfc\u81f4\u4fe1\u606f\u7a00\u91ca\u6216\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u9884\u6d4b\u6240\u9700\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u7684Top-K\u90bb\u5c45\u9009\u62e9\u7b56\u7565\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bc4\u4f30\u4e2d\u5fc3\u8282\u70b9\u4e0e\u5173\u8054\u8fb9\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u9009\u51fa\u6700\u76f8\u5173\u7684Top-K\u8fb9\uff1b\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\u5c06\u8fd9\u4e9b\u8fb9\u7684\u4fe1\u606f\u4e0e\u4e2d\u5fc3\u8282\u70b9\u8868\u793a\u878d\u5408\uff0c\u751f\u6210\u8bed\u4e49\u805a\u7126\u7684\u8282\u70b9\u6d88\u606f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u548c\u4f20\u64ad\u4e0e\u7279\u5b9a\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6709\u6548\u7f13\u89e3\u65e0\u5173\u4fe1\u606f\u7684\u5e72\u6270\u3002"}}
{"id": "2506.22712", "pdf": "https://arxiv.org/pdf/2506.22712", "abs": "https://arxiv.org/abs/2506.22712", "authors": ["Alexander Theus", "Alessandro Cabodi", "Sotiris Anagnostidis", "Antonio Orvieto", "Sidak Pal Singh", "Valentina Boeva"], "title": "Generalized Linear Mode Connectivity for Transformers", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "AI": {"tldr": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5176\u4e2d\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\uff08LMC\uff09\u662f\u4e00\u4e2a\u663e\u8457\u73b0\u8c61\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u6355\u6349\u56db\u79cd\u5bf9\u79f0\u7c7b\u522b\uff1a\u7f6e\u6362\u3001\u534a\u7f6e\u6362\u3001\u6b63\u4ea4\u53d8\u6362\u548c\u4e00\u822c\u53ef\u9006\u6620\u5c04\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u72ec\u7acb\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u548cGPT-2\u6a21\u578b\u4e4b\u95f4\u53d1\u73b0\u4f4e\u969c\u788d\u548c\u96f6\u969c\u788d\u7684\u7ebf\u6027\u63d2\u503c\u8def\u5f84\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u4e8e\u6cdb\u5316\u548c\u4f18\u5316\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\uff08\u5982\u795e\u7ecf\u5143\u7f6e\u6362\uff09\u4f7f\u5f97\u529f\u80fd\u4e0a\u7b49\u4ef7\u7684\u6a21\u578b\u770b\u8d77\u6765\u4e0d\u540c\uff0c\u8fd9\u63a9\u76d6\u4e86\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\uff08LMC\uff09\u7684\u73b0\u8c61\u3002\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u795e\u7ecf\u5143\u91cd\u6392\u4e0a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u4ee3\u67b6\u6784\uff08\u5982Transformer\uff09\u6240\u8868\u73b0\u51fa\u7684\u66f4\u4e30\u5bcc\u7684\u5bf9\u79f0\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6355\u6349\u4e86\u56db\u79cd\u5bf9\u79f0\u7c7b\u522b\uff1a\u7f6e\u6362\u3001\u534a\u7f6e\u6362\u3001\u6b63\u4ea4\u53d8\u6362\u548c\u4e00\u822c\u53ef\u9006\u6620\u5c04\u3002\u8fd9\u4e00\u6846\u67b6\u6269\u5c55\u4e86\u6709\u6548\u7684\u91cd\u65b0\u53c2\u6570\u5316\u7684\u96c6\u5408\uff0c\u5e76\u5c06\u8bb8\u591a\u5148\u524d\u7684\u65b9\u6cd5\u4f5c\u4e3a\u7279\u6b8a\u60c5\u51b5\u5305\u542b\u5728\u5185\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u9996\u6b21\u53d1\u73b0\u4e86\u72ec\u7acb\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u548cGPT-2\u6a21\u578b\u4e4b\u95f4\u7684\u4f4e\u969c\u788d\u548c\u96f6\u969c\u788d\u7684\u7ebf\u6027\u63d2\u503c\u8def\u5f84\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u635f\u5931\u666f\u89c2\u4e2d\u66f4\u6df1\u7684\u7ed3\u6784\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u611f\u77e5\u5206\u6790\u5728\u7406\u89e3\u6a21\u578b\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u79f0\u6027\u611f\u77e5\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u800c\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2506.22696", "pdf": "https://arxiv.org/pdf/2506.22696", "abs": "https://arxiv.org/abs/2506.22696", "authors": ["Brian Mak", "Jeffrey Flanigan"], "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "AI": {"tldr": "The paper introduces Residual Matrix Transformer (RMT), which replaces the residual stream of transformers with an outer product memory matrix, leading to performance improvements, efficiency gains, and better scaling properties.", "motivation": "Transformers use a residual stream for both storing and accessing features. The authors aim to improve this mechanism by replacing it with an outer product memory matrix.", "method": "The residual stream in transformers is replaced with an outer product memory matrix, forming the Residual Matrix Transformer (RMT). This model is then theoretically analyzed and compared to traditional transformers.", "result": "RMT offers several advantages: scalable residual stream size independent of compute and model size, achieving same loss as transformers with fewer FLOPS, parameters, and training tokens, and outperforming transformers on downstream tasks.", "conclusion": "RMT provides more efficient residual stream scaling and improved variance propagation properties compared to standard transformers."}}
{"id": "2506.23168", "pdf": "https://arxiv.org/pdf/2506.23168", "abs": "https://arxiv.org/abs/2506.23168", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik D\u00fcrrschnabel", "Gerd Stumme"], "title": "Rises for Measuring Local Distributivity in Lattices", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6982\u5ff5\u683c\u4e2d\u7684\u5347\u5e45\u6765\u8bc4\u4f30\u5206\u914d\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5b9e\u9645\u6570\u636e\u4e2d\u7684\u6982\u5ff5\u683c\u5177\u6709\u9ad8\u5ea6\u7684\u5e76\u5206\u914d\u6027\uff0c\u4f46\u4ea4\u5206\u914d\u6027\u8f83\u4f4e\u3002", "motivation": "\u5728\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u4e2d\uff0c\u5c3d\u7ba1\u6676\u683c\u7ecf\u5e38\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u5206\u914d\u6027\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u6807\u51c6\u5316\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u4e00\u6027\u8d28\u3002", "method": "\u5f15\u5165\u4e86\u6982\u5ff5\u683c\u4e2d\u7684\u5347\u5e45\uff08rises\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5206\u914d\u6027\u7684\u624b\u6bb5\uff0c\u7814\u7a76\u4e86\u5347\u5e45\u4e0e\u7ecf\u5178\u4ea4\u5e76\u5206\u914d\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u89c2\u5bdf\u4e86\u5b9e\u9645\u6570\u636e\u4e2d\u6982\u5ff5\u683c\u7684\u5206\u914d\u6027\u7279\u5f81\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u4e2a\u6676\u683c\u662f\u5206\u914d\u6027\u7684\u5f53\u4e14\u4ec5\u5f53\u6ca1\u6709\u975e\u5355\u4f4d\u5347\u5e45\u53d1\u751f\uff1b\u5b9e\u9645\u6570\u636e\u4e2d\u7684\u6982\u5ff5\u683c\u9ad8\u5ea6\u5e76\u5206\u914d\uff0c\u4f46\u4ea4\u5206\u914d\u6027\u8f83\u4f4e\u3002", "conclusion": "\u5347\u5e45\u53ef\u4ee5\u4f5c\u4e3a\u8bc4\u4f30\u6676\u683c\u5206\u914d\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u6570\u636e\u4e2d\u6676\u683c\u5206\u914d\u6027\u7684\u4e0d\u5bf9\u79f0\u6027\u3002"}}
{"id": "2506.22732", "pdf": "https://arxiv.org/pdf/2506.22732", "abs": "https://arxiv.org/abs/2506.22732", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": null, "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "AI": {"tldr": "\u63d0\u51faRTC-GTNLN\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u5f20\u91cf\u6838L1-L2\u8303\u6570\u89e3\u51b3\u4ea4\u901a\u6570\u636e\u4e2d\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u53cc\u91cd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\u7ecf\u5e38\u53d7\u5230\u4f20\u611f\u5668\u6545\u969c\u548c\u901a\u4fe1\u5931\u8d25\u5bfc\u81f4\u7684\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u53cc\u91cd\u9000\u5316\u5f71\u54cd\uff0c\u4f20\u7edf\u7684\u5f20\u91cf\u8865\u5168\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u975e\u51f8\u5f20\u91cf\u79e9\u4ee3\u7406\u2014\u2014\u5f20\u91cfL1-L2\u8303\u6570\uff0c\u5e76\u8fdb\u4e00\u6b65\u53d1\u5c55\u4e3a\u68af\u5ea6\u5f20\u91cfL1-L2\u8303\u6570\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86RTC-GTNLN\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRTC-GTNLN\u6a21\u578b\u5728\u590d\u6742\u6062\u590d\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "RTC-GTNLN\u6a21\u578b\u80fd\u591f\u5145\u5206\u5229\u7528\u5168\u5c40\u4f4e\u79e9\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u4ea4\u901a\u6570\u636e\u7684\u53cc\u91cd\u9000\u5316\u6311\u6218\u3002"}}
{"id": "2506.22708", "pdf": "https://arxiv.org/pdf/2506.22708", "abs": "https://arxiv.org/abs/2506.22708", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "comment": null, "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "AI": {"tldr": "\u63d0\u51faFairMarket-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728P2P\u5fae\u7535\u7f51\u4ea4\u6613\u4e2d\u5b9e\u73b0\u516c\u5e73\u6027\u611f\u77e5\u7684\u4ea4\u6613\u4ee3\u7406\u3002\u901a\u8fc7\u516c\u5e73\u6027\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u5956\u52b1\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e70\u5bb6\u9700\u6c42\u6ee1\u8db3\u7387\u3001\u5356\u5bb6\u5229\u6da6\u516c\u5e73\u6027\u548c\u6574\u4f53\u4ea4\u6613\u516c\u5e73\u6027\u3002\u8be5\u6846\u67b6\u5728\u6269\u5c55\u5230\u5927\u578b\u7535\u529b\u5206\u914d\u7cfb\u7edf\u65f6\u4ecd\u4fdd\u6301\u9ad8\u6548\u548c\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684P2P\u4ea4\u6613\u673a\u5236\u7f3a\u4e4f\u786e\u4fdd\u516c\u5e73\u6027\u7684\u7a33\u5065\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u591a\u4e70\u5bb6\u548c\u591a\u5356\u5bb6\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u5e02\u573a\u4ea4\u6613\u3002", "method": "\u8bbe\u8ba1\u4e86FairMarket-RL\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5b9e\u65f6\u516c\u5e73\u6027\u8bc4\u4f30\u5668\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ea4\u6613\u4ee3\u7406\u3002\u5f15\u5165\u4e86\u4e24\u4e2a\u516c\u5e73\u6027\u6307\u6807\uff08FTB\u548cFBS\uff09\uff0c\u5e76\u901a\u8fc7\u03bb\u7cfb\u6570\u5c06\u8fd9\u4e9b\u6307\u6807\u6574\u5408\u5230\u4ee3\u7406\u5956\u52b1\u4e2d\u3002\u91c7\u7528\u72ec\u7acb\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08IPPO\uff09\u8fdb\u884c\u4ee3\u7406\u8bad\u7ec3\u3002", "result": "\u4ee3\u7406\u80fd\u591f\u6ee1\u8db3\u8d85\u8fc790%\u7684\u4e70\u5bb6\u9700\u6c42\uff0c\u7ef4\u6301\u5356\u5bb6\u95f4\u7684\u516c\u5e73\u5229\u6da6\uff0c\u5e76\u8fbe\u5230\u9ad8\u4e8e0.80\u7684FTB\u548cFBS\u5206\u6570\u3002\u540c\u65f6\uff0c\u516c\u5e73\u6027\u53cd\u9988\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u51cf\u5c11\u4e86\u4e70\u5bb6\u77ed\u7f3a\u5e76\u7f29\u5c0f\u4e86\u5356\u5bb6\u5229\u6da6\u5dee\u5f02\u3002", "conclusion": "FairMarket-RL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u4ee5\u516c\u5e73\u6027\u4e3a\u5bfc\u5411\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u7cfb\u7edf\u7684\u81ea\u4e3b\u4ea4\u6613\u3002"}}
{"id": "2506.23273", "pdf": "https://arxiv.org/pdf/2506.23273", "abs": "https://arxiv.org/abs/2506.23273", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46text2sql\u5728\u590d\u6742\u548c\u7279\u5b9a\u9886\u57df\u7684\u67e5\u8be2\u4e2d\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u91d1\u878d\u9886\u57df\uff0c\u7531\u4e8e\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u8d22\u52a1\u62a5\u544a\u5e03\u5c40\u5728\u4e0d\u540c\u91d1\u878d\u673a\u6784\u548c\u56fd\u5bb6\u4e4b\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u4f7f\u5f97text2sql\u66f4\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86FinStat2SQL\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684text2sql\u7ba1\u9053\uff0c\u5141\u8bb8\u5bf9\u8d22\u52a1\u62a5\u8868\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002\u5b83\u9488\u5bf9\u672c\u5730\u6807\u51c6\uff08\u5982VAS\uff09\u8fdb\u884c\u4e86\u5b9a\u5236\uff0c\u5728\u591a\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u7ed3\u5408\u4e86\u5927\u3001\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u4f53\u63d0\u53d6\u3001SQL\u751f\u6210\u548c\u81ea\u6211\u4fee\u6b63\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u5e93\uff0c\u5e76\u5728\u4e00\u4e2a\u5408\u6210\u7684QA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002\u7ecf\u8fc7\u5fae\u8c03\u76847B\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e8661.33%\u7684\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u65f6\u95f4\u4e0d\u52304\u79d2\uff0c\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u3002FinStat2SQL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u91d1\u878d\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u8d8a\u5357\u4f01\u4e1a\u80fd\u591f\u4f7f\u7528AI\u9a71\u52a8\u7684\u67e5\u8be2\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u8d22\u52a1\u62a5\u544a\u5e03\u5c40\u5728\u4e0d\u540c\u91d1\u878d\u673a\u6784\u548c\u56fd\u5bb6\u4e4b\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u4f7f\u5f97text2sql\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faFinStat2SQL\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684text2sql\u7ba1\u9053\uff0c\u8be5\u65b9\u6cd5\u9488\u5bf9\u672c\u5730\u6807\u51c6\u8fdb\u884c\u4e86\u5b9a\u5236\uff0c\u5728\u591a\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u7ed3\u5408\u4e86\u5927\u3001\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u4f53\u63d0\u53d6\u3001SQL\u751f\u6210\u548c\u81ea\u6211\u4fee\u6b63\u3002", "result": "\u7ecf\u8fc7\u5fae\u8c03\u76847B\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e8661.33%\u7684\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u65f6\u95f4\u4e0d\u52304\u79d2\uff0c\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u3002", "conclusion": "FinStat2SQL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u91d1\u878d\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u8d8a\u5357\u4f01\u4e1a\u80fd\u591f\u4f7f\u7528AI\u9a71\u52a8\u7684\u67e5\u8be2\u3002"}}
{"id": "2506.23276", "pdf": "https://arxiv.org/pdf/2506.23276", "abs": "https://arxiv.org/abs/2506.23276", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5408\u4f5c\u548c\u793e\u4f1a\u673a\u5236\u7814\u7a76\u3002\u901a\u8fc7\u516c\u5171\u7269\u54c1\u535a\u5f08\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4e0d\u540cLLMs\u5728\u5408\u4f5c\u884c\u4e3a\u4e0a\u8868\u73b0\u51fa\u56db\u79cd\u6a21\u5f0f\uff1a\u6301\u7eed\u5408\u4f5c\u3001\u6ce2\u52a8\u3001\u9010\u6e10\u4e0b\u964d\u548c\u56fa\u5b9a\u7b56\u7565\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5f3a\u8c03\u63a8\u7406\u80fd\u529b\u7684LLMs\u5728\u5408\u4f5c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4e00\u4e9b\u4f20\u7edfLLMs\u5219\u8868\u73b0\u51fa\u9ad8\u6c34\u5e73\u7684\u5408\u4f5c\u3002\u8fd9\u8868\u660e\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u4e0d\u4e00\u5b9a\u80fd\u4fc3\u8fdb\u5408\u4f5c\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u5408\u4f5c\u4e0e\u793e\u4f1a\u673a\u5236\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u81ea\u8eab\u5229\u76ca\u4e0e\u96c6\u4f53\u798f\u7949\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u8fd9\u79cd\u60c5\u5883\u4e0b\u6210\u672c\u5236\u88c1\u7684\u96be\u9898\uff0c\u5373\u4e00\u4e2a\u667a\u80fd\u4f53\u662f\u5426\u5e94\u8be5\u6295\u8d44\u81ea\u5df1\u7684\u8d44\u6e90\u6765\u6fc0\u52b1\u5408\u4f5c\u6216\u60e9\u7f5a\u80cc\u53db\u3002", "method": "\u4f5c\u8005\u6539\u7f16\u4e86\u884c\u4e3a\u7ecf\u6d4e\u5b66\u4e2d\u7684\u516c\u5171\u7269\u54c1\u535a\u5f08\uff0c\u5e76\u5f15\u5165\u5236\u5ea6\u9009\u62e9\u5143\u7d20\uff0c\u4ee5\u89c2\u5bdf\u4e0d\u540cLLMs\u5728\u91cd\u590d\u4ea4\u4e92\u4e2d\u5982\u4f55\u5e94\u5bf9\u793e\u4f1a\u56f0\u5883\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u5728\u9762\u5bf9\u5408\u4f5c\u4e0e\u80cc\u53db\u65f6\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\uff1a\u4e00\u4e9b\u6a21\u578b\u59cb\u7ec8\u4fdd\u6301\u9ad8\u6c34\u5e73\u7684\u5408\u4f5c\uff1b\u53e6\u4e00\u4e9b\u6a21\u578b\u5728\u53c2\u4e0e\u548c\u9000\u51fa\u4e4b\u95f4\u6ce2\u52a8\uff1b\u8fd8\u6709\u4e00\u4e9b\u6a21\u578b\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u9010\u6e10\u51cf\u5c11\u5408\u4f5c\u884c\u4e3a\uff1b\u6700\u540e\uff0c\u6709\u4e9b\u6a21\u578b\u65e0\u8bba\u7ed3\u679c\u5982\u4f55\u90fd\u575a\u6301\u56fa\u5b9a\u7684\u7b56\u7565\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684LLMs\uff08\u5982o1\u7cfb\u5217\uff09\u5728\u5408\u4f5c\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\uff0c\u800c\u67d0\u4e9b\u4f20\u7edfLLMs\u5219\u59cb\u7ec8\u7ef4\u6301\u9ad8\u5408\u4f5c\u6c34\u5e73\u3002", "conclusion": "\u5f53\u524d\u63d0\u5347LLMs\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u5bfc\u81f4\u66f4\u597d\u7684\u5408\u4f5c\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5728\u9700\u8981\u6301\u7eed\u534f\u4f5c\u7684\u73af\u5883\u4e2d\u90e8\u7f72LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.22994", "pdf": "https://arxiv.org/pdf/2506.22994", "abs": "https://arxiv.org/abs/2506.22994", "authors": ["Can Hakan Da\u011f\u0131d\u0131r", "Mia Hubert", "Peter J. Rousseeuw"], "title": "Kernel Outlier Detection", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u2014\u2014\u6838\u5f02\u5e38\u68c0\u6d4b\uff08KOD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6838\u53d8\u6362\u548c\u6295\u5f71\u5bfb\u4f18\uff0c\u5e76\u5f15\u5165\u65b0\u65b9\u5411\u96c6\u5408\u548c\u7ed3\u679c\u7ec4\u5408\u65b9\u5f0f\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eKOD\u5728\u5927\u5c0f\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u5b58\u5728\u6311\u6218\uff0c\u5982\u4f9d\u8d56\u5206\u5e03\u5047\u8bbe\u6216\u96be\u4ee5\u8c03\u6574\u7684\u8d85\u53c2\u6570\u7b49\u95ee\u9898\u3002", "method": "KOD\u65b9\u6cd5\u9996\u5148\u8fdb\u884c\u6838\u53d8\u6362\uff0c\u7136\u540e\u91c7\u7528\u6295\u5f71\u5bfb\u4f18\u65b9\u6cd5\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\u65b0\u7684\u65b9\u5411\u96c6\u5408\u641c\u7d22\u4ee5\u53ca\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u65b9\u5411\u7ed3\u679c\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7ed3\u6784\u7684\u5c0f\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5927\u578b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cKOD\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "conclusion": "KOD\u5728\u5c0f\u6570\u636e\u96c6\u548c\u5927\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u90fd\u5f88\u597d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u7ef4\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.22716", "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R\u00fchle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "AI": {"tldr": "BEST-Route\u662f\u4e00\u79cd\u65b0\u7684\u8def\u7531\u6846\u67b6\uff0c\u5b83\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u8d28\u91cf\u9608\u503c\u9009\u62e9\u6a21\u578b\u4ee5\u53ca\u4ece\u4e2d\u91c7\u6837\u7684\u54cd\u5e94\u6570\u91cf\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u6700\u591a\u964d\u4f4e\u6210\u672c60%\uff0c\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7684\u67e5\u8be2\u8def\u7531\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u52a8\u6001\u5206\u914d\u67e5\u8be2\u4ee5\u5e73\u8861\u6210\u672c\u548c\u8d28\u91cf\uff0c\u4f46\u7531\u4e8e\u5c0f\u578b\u6a21\u578b\u751f\u6210\u7684\u5355\u4e00\u54cd\u5e94\u8d28\u91cf\u4e0d\u8db3\u4ee5\u4e0e\u5927\u578b\u6a21\u578b\u5ab2\u7f8e\uff0c\u5bfc\u81f4\u5927\u578b\u6a21\u578b\u88ab\u8fc7\u5ea6\u4f7f\u7528\uff0c\u9519\u5931\u4e86\u6f5c\u5728\u7684\u6210\u672c\u8282\u7ea6\u3002\u7136\u800c\uff0c\u5df2\u77e5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u54cd\u5e94\u5e76\u9009\u62e9\u6700\u4f73\u7ed3\u679c\uff0c\u5c0f\u578b\u6a21\u578b\u53ef\u4ee5\u5728\u4fdd\u6301\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBEST-Route\u7684\u65b0\u9896\u8def\u7531\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u8d28\u91cf\u9608\u503c\u6765\u9009\u62e9\u6a21\u578b\u4ee5\u53ca\u4ece\u6240\u9009\u6a21\u578b\u4e2d\u91c7\u6837\u7684\u54cd\u5e94\u6570\u91cf\u3002\u5bf9\u4e8e\u8f83\u96be\u7684\u67e5\u8be2\uff0c\u53ef\u80fd\u4f1a\u9009\u62e9\u5927\u578b\u6a21\u578b\u6216\u4ece\u5c0f\u578b\u6a21\u578b\u4e2d\u751f\u6210\u66f4\u591a\u54cd\u5e94\uff1b\u5bf9\u4e8e\u7b80\u5355\u7684\u67e5\u8be2\uff0c\u5219\u53ef\u80fd\u9009\u62e9\u5c0f\u578b\u6a21\u578b\u5e76\u751f\u6210\u8f83\u5c11\u54cd\u5e94\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBEST-Route\u65b9\u6cd5\u80fd\u591f\u5c06\u6210\u672c\u964d\u4f4e\u591a\u8fbe60%\uff0c\u540c\u65f6\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u3002", "conclusion": "BEST-Route\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861LLM\u5e94\u7528\u4e2d\u7684\u6210\u672c\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u7075\u6d3b\u5730\u9009\u62e9\u6a21\u578b\u548c\u54cd\u5e94\u6570\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fd0\u884c\u6210\u672c\uff0c\u540c\u65f6\u4ec5\u5e26\u6765\u8f7b\u5fae\u7684\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2506.23306", "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "categories": ["cs.AI"], "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "AI": {"tldr": "\u4f20\u7edf\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\u4f9d\u8d56\u4e8e\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u65c5\u884c\u51b3\u7b56\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\u7684\u521a\u6027\u89c4\u5219\u7cfb\u7edf\u3002\u672c\u6587\u63d0\u51faGATSim\uff08\u751f\u6210\u578b\u4ee3\u7406\u4ea4\u901a\u6a21\u62df\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u57ce\u5e02\u79fb\u52a8\u57fa\u7840\u6a21\u578b\u3001\u4ee3\u7406\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u6a21\u62df\u73af\u5883\u7684\u7efc\u5408\u67b6\u6784\uff0c\u521b\u5efa\u5177\u6709\u4e30\u5bcc\u884c\u4e3a\u7279\u5f81\u7684\u751f\u6210\u578b\u4ee3\u7406\u3002\u8fd9\u4e9b\u4ee3\u7406\u5177\u5907\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u3001\u4e2a\u4eba\u751f\u6d3b\u65b9\u5f0f\u548c\u4e0d\u65ad\u6f14\u53d8\u7684\u504f\u597d\uff0c\u80fd\u591f\u901a\u8fc7\u5fc3\u7406\u77e5\u60c5\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7ec8\u8eab\u5b66\u4e60\u673a\u5236\u5f71\u54cd\u5176\u79fb\u52a8\u51b3\u7b56\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u578b\u4ee3\u7406\u5728\u79fb\u52a8\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u76f8\u5f53\uff0c\u5e76\u80fd\u81ea\u7136\u751f\u6210\u5b8f\u89c2\u4ea4\u901a\u6f14\u5316\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4ee3\u7406\u7684\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4eba\u7c7b\u65c5\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u591a\u6837\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GATSim\u6846\u67b6\u7ed3\u5408\u4e86\u57ce\u5e02\u79fb\u52a8\u57fa\u7840\u6a21\u578b\u3001\u4ee3\u7406\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u6a21\u62df\u73af\u5883\uff0c\u521b\u5efa\u751f\u6210\u578b\u4ee3\u7406\u3002\u8fd9\u4e9b\u4ee3\u7406\u5177\u5907\u591a\u6837\u5316\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u3001\u4e2a\u4eba\u751f\u6d3b\u65b9\u5f0f\u548c\u4e0d\u65ad\u6f14\u53d8\u7684\u504f\u597d\uff0c\u901a\u8fc7\u5fc3\u7406\u77e5\u60c5\u8bb0\u5fc6\u7cfb\u7edf\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7ec8\u8eab\u5b66\u4e60\u673a\u5236\u5f71\u54cd\u5176\u79fb\u52a8\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u578b\u4ee3\u7406\u5728\u79fb\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u76f8\u5f53\uff0c\u5e76\u80fd\u81ea\u7136\u751f\u6210\u5b8f\u89c2\u4ea4\u901a\u6f14\u5316\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u751f\u6210\u578b\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u8bbe\u8ba1\u7684\u53cd\u601d\u8fc7\u7a0b\uff0c\u5c06\u7279\u5b9a\u65c5\u884c\u4f53\u9a8c\u8f6c\u5316\u4e3a\u666e\u904d\u89c1\u89e3\uff0c\u5b9e\u73b0\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u884c\u4e3a\u9002\u5e94\u3002", "conclusion": "GATSim\u6846\u67b6\u4e3a\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u751f\u6210\u578b\u4ee3\u7406\u8868\u73b0\u51fa\u53ef\u4fe1\u7684\u65c5\u884c\u884c\u4e3a\uff0c\u5177\u5907\u6d3b\u52a8\u89c4\u5212\u548c\u5b9e\u65f6\u53cd\u5e94\u80fd\u529b\uff0c\u9002\u5e94\u57ce\u5e02\u79fb\u52a8\u60c5\u5883\u3002"}}
{"id": "2506.23033", "pdf": "https://arxiv.org/pdf/2506.23033", "abs": "https://arxiv.org/abs/2506.23033", "authors": ["Yash Vardhan Tomar"], "title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "AI": {"tldr": "A feature-wise mixing framework is introduced to mitigate contextual bias in ML models, achieving significant bias reduction and improved performance without needing explicit bias attribute identification.", "motivation": "Bias in predictive machine learning models leads to skewed or unfair outcomes. Current mitigation strategies either rely on post-hoc corrections or rigid constraints which can limit scalability and generalizability.", "method": "Introduced a feature-wise mixing framework that redistributes feature representations across multiple contextual datasets. Assessed effectiveness by training four ML classifiers using cross-validation and evaluating with bias-sensitive loss functions including disparity metrics and MSE.", "result": "Achieved an average bias reduction of 43.35% and a statistically significant decrease in MSE across all classifiers trained on mixed datasets. Outperformed SMOTE oversampling and demonstrated competitive effectiveness without requiring explicit bias attribute identification.", "conclusion": "Feature-wise mixing efficiently mitigates contextual bias without the computational overhead typically associated with fairness-aware learning algorithms. Suggests future application in real-world fields for accurate predictions."}}
{"id": "2506.23464", "pdf": "https://arxiv.org/pdf/2506.23464", "abs": "https://arxiv.org/abs/2506.23464", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "categories": ["cs.AI"], "comment": null, "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "AI": {"tldr": "HonestVQA\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u6211\u76d1\u7763\u8bda\u5b9e\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u9053\u5fb7\u5bf9\u9f50\u7684\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u3002\u5b83\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3001\u8c03\u6574\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u5b9e\u9645\u6b63\u786e\u6027\u4ee5\u53ca\u5f3a\u5236\u6267\u884c\u9053\u5fb7\u54cd\u5e94\u884c\u4e3a\u6765\u6539\u5584\u73b0\u6709\u7cfb\u7edf\u7684\u4f26\u7406\u54cd\u5e94\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cHonestVQA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0c\u5e76\u51cf\u5c11\u4e86\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684DocVQA\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5728\u4f26\u7406\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7ecf\u5e38\u5bf9\u6a21\u7cca\u95ee\u9898\u4ea7\u751f\u8fc7\u4e8e\u81ea\u4fe1\u7684\u7b54\u6848\u6216\u65e0\u6cd5\u4ee5\u53ef\u4fe1\u65b9\u5f0f\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u5728\u9700\u8981\u4f26\u7406\u8d23\u4efb\u7684\u9886\u57df\u4e2d\u5e26\u6765\u4e86\u663e\u8457\u98ce\u9669\u3002", "method": "\u5f15\u5165\u4e86HonestVQA\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1) \u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4ee5\u8bc6\u522b\u77e5\u8bc6\u7a7a\u767d\uff1b2) \u4f7f\u7528\u52a0\u6743\u635f\u5931\u51fd\u6570\u5c06\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u6b63\u786e\u6027\u5bf9\u9f50\uff1b3) \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5236\u6267\u884c\u4f26\u7406\u54cd\u5e94\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a\u8bda\u5b9e\u5206\u6570\uff08H-Score\uff09\u548c\u4f26\u7406\u7f6e\u4fe1\u6307\u6570\uff08ECI\uff09\u3002", "result": "HonestVQA\u5728SpDocVQA\u3001InfographicsVQA\u548cSROIE\u6570\u636e\u96c6\u4e0a\u5c06DocVQA\u51c6\u786e\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe4.3%\uff0cF1\u5206\u6570\u4e5f\u63d0\u5347\u4e864.3%\u3002\u540c\u65f6\u964d\u4f4e\u4e86\u8fc7\u81ea\u4fe1\uff0cH-Score\u548cECI\u5206\u522b\u4e0b\u964d\u4e860.072\u548c0.078\u3002\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\uff0c\u51c6\u786e\u7387\u8fbe\u523078.9%\uff0cF1\u5206\u6570\u8fbe\u523076.1%\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u5982\u679c\u6ca1\u6709\u5bf9\u9f50\u6216\u5bf9\u6bd4\u635f\u5931\uff0c\u51c6\u786e\u7387\u4f1a\u4e0b\u964d3.8%\u3002", "conclusion": "HonestVQA\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709DocVQA\u7cfb\u7edf\u5728\u4f26\u7406\u54cd\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23186", "pdf": "https://arxiv.org/pdf/2506.23186", "abs": "https://arxiv.org/abs/2506.23186", "authors": ["Marco Bressan", "Victor Chepoi", "Emmanuel Esposito", "Maximilian Thiessen"], "title": "Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs", "categories": ["cs.LG", "cs.DM", "math.CO", "stat.ML"], "comment": null, "summary": "Abstract notions of convexity over the vertices of a graph, and corresponding\nnotions of halfspaces, have recently gained attention from the machine learning\ncommunity. In this work we study monophonic halfspaces, a notion of graph\nhalfspaces defined through closure under induced paths. Our main result is a\n$2$-satisfiability based decomposition theorem, which allows one to represent\nmonophonic halfspaces as a disjoint union of certain vertex subsets. Using this\ndecomposition, we achieve efficient and (nearly) optimal algorithms for various\nlearning problems, such as teaching, active, and online learning. Most notably,\nwe obtain a polynomial-time algorithm for empirical risk minimization.\nIndependently of the decomposition theorem, we obtain an efficient, stable, and\nproper sample compression scheme. This makes monophonic halfspaces efficiently\nlearnable with proper learners and linear error rate $1/\\varepsilon$ in the\nrealizable PAC setting. Our results answer open questions from the literature,\nand show a stark contrast with geodesic halfspaces, for which most of the said\nlearning problems are NP-hard.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u56fe\u7684\u9876\u70b9\u4e0a\u7684\u51f8\u6027\u6982\u5ff5\u53ca\u5176\u5bf9\u5e94\u7684\u534a\u7a7a\u95f4\u6982\u5ff5\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u53d7\u5230\u5173\u6ce8\u7684\u5355\u8c03\u534a\u7a7a\u95f4\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2-\u53ef\u6ee1\u8db3\u6027\uff082-SAT\uff09\u7684\u5206\u89e3\u5b9a\u7406\uff0c\u5e76\u5229\u7528\u8be5\u5b9a\u7406\u5f00\u53d1\u4e86\u9ad8\u6548\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6559\u5b66\u3001\u4e3b\u52a8\u548c\u5728\u7ebf\u5b66\u4e60\u7b49\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6837\u672c\u538b\u7f29\u65b9\u6848\uff0c\u4f7f\u5355\u8c03\u534a\u7a7a\u95f4\u5728\u5b9e\u9645PAC\u8bbe\u5b9a\u4e0b\u80fd\u591f\u88ab\u6b63\u786e\u5b66\u4e60\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6d4b\u5730\u7ebf\u534a\u7a7a\u95f4\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u95ee\u9898\u5bf9\u5355\u8c03\u534a\u7a7a\u95f4\u6765\u8bf4\u662f\u9ad8\u6548\u7684\uff0c\u800c\u5bf9\u6d4b\u5730\u7ebf\u534a\u7a7a\u95f4\u6765\u8bf4\u5927\u591a\u662fNP\u96be\u7684\u3002", "motivation": "\u7814\u7a76\u56fe\u7684\u9876\u70b9\u4e0a\u51f8\u6027\u548c\u534a\u7a7a\u95f4\u7684\u6982\u5ff5\uff0c\u7279\u522b\u662f\u901a\u8fc7\u95ed\u5408\u8bf1\u5bfc\u8def\u5f84\u5b9a\u4e49\u7684\u5355\u8c03\u534a\u7a7a\u95f4\uff0c\u4ee5\u671f\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5404\u7c7b\u5b66\u4e60\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e2-\u53ef\u6ee1\u8db3\u6027\u7684\u5206\u89e3\u5b9a\u7406\uff0c\u7528\u4e8e\u8868\u793a\u5355\u8c03\u534a\u7a7a\u95f4\u3002\n2. \u5229\u7528\u8be5\u5206\u89e3\u5b9a\u7406\u8bbe\u8ba1\u9488\u5bf9\u6559\u5b66\u3001\u4e3b\u52a8\u548c\u5728\u7ebf\u5b66\u4e60\u7b49\u4efb\u52a1\u7684\u6709\u6548\u7b97\u6cd5\u3002\n3. \u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7684\u6837\u672c\u538b\u7f29\u65b9\u6848\uff0c\u786e\u4fdd\u5b66\u4e60\u8fc7\u7a0b\u7a33\u5b9a\u4e14\u6b63\u786e\u3002", "result": "1. \u5f00\u53d1\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u4ee5\u6700\u5c0f\u5316\u7ecf\u9a8c\u98ce\u9669\u3002\n2. \u5b9e\u73b0\u4e86\u5728\u5b9e\u9645PAC\u8bbe\u5b9a\u4e0b\u7684\u6709\u6548\u5b66\u4e60\uff0c\u9519\u8bef\u7387\u7ebf\u6027\u4e3a$1/\\varepsilon$\u3002\n3. \u4e0e\u6d4b\u5730\u7ebf\u534a\u7a7a\u95f4\u5f62\u6210\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86\u5355\u8c03\u534a\u7a7a\u95f4\u5b66\u4e60\u95ee\u9898\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u5355\u8c03\u534a\u7a7a\u95f4\u53ef\u4ee5\u901a\u8fc72-\u53ef\u6ee1\u8db3\u6027\u5206\u89e3\u5b9a\u7406\u8fdb\u884c\u6709\u6548\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u591a\u4e2a\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86\u8bef\u5dee\u6700\u5c0f\u5316\u3002\u4e0e\u6d4b\u5730\u7ebf\u534a\u7a7a\u95f4\u76f8\u6bd4\uff0c\u5355\u8c03\u534a\u7a7a\u95f4\u7684\u5b66\u4e60\u95ee\u9898\u662f\u9ad8\u6548\u7684\u3002"}}
{"id": "2506.22771", "pdf": "https://arxiv.org/pdf/2506.22771", "abs": "https://arxiv.org/abs/2506.22771", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eINT8\u91cf\u5316\u548cForward-Forward(FF)\u7b97\u6cd5\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u9010\u5c42\u7b56\u7565\u7a33\u5b9a\u68af\u5ea6\u91cf\u5316\uff0c\u5e76\u5f15\u5165\u4e86\u524d\u77bb\u65b9\u6848\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6b64\u65b9\u6cd5\u5728NVIDIA Jetson Orin Nano\u677f\u4e0a\u8bad\u7ec3\u901f\u5ea6\u5feb4.6%\uff0c\u80fd\u8017\u964d\u4f4e8.3%\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1127.0%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u7cbe\u5ea6\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\uff08Backpropagation\uff09\u662f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6838\u5fc3\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u5176\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9002\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u867d\u7136\u4f4e\u7cbe\u5ea6\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u7814\u7a76\u4ee5\u52a0\u901f\u6a21\u578b\u63a8\u7406\uff0c\u4f46\u5728\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u8f83\u5c11\u3002Forward-Forward (FF) \u7b97\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u53cd\u5411\u4f20\u64ad\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u989d\u5916\u7684\u524d\u5411\u4f20\u9012\u4ee3\u66ff\u540e\u5411\u4f20\u9012\u6765\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u975e\u5e38\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408INT8\u91cf\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528FF\u7b97\u6cd5\u7684\u9010\u5c42\u7b56\u7565\u6765\u7a33\u5b9a\u68af\u5ea6\u91cf\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u201c\u524d\u77bb\u201d\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3FF\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728NVIDIA Jetson Orin Nano\u677f\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u5feb4.6%\uff0c\u8282\u77018.3%\u7684\u80fd\u91cf\uff0c\u5e76\u5c06\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e8627.0%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684INT8\u91cf\u5316\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408FF\u7b97\u6cd5\u53ca\u5176\u524d\u77bb\u65b9\u6848\uff0c\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u3001\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23503", "pdf": "https://arxiv.org/pdf/2506.23503", "abs": "https://arxiv.org/abs/2506.23503", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "categories": ["cs.AI"], "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\uff08CBT\uff09\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u63a5\u53d7\u3001\u627f\u8bfa\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u7528\u4e8e\u5206\u6790\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u4e2d\u7684\u8d1f\u9762\u60c5\u7eea\u53ca\u8ba4\u77e5\u626d\u66f2\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u4e86BERT\u3001RoBERTa\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0cT5\u3001PEGASUS\u8fdb\u884c\u6587\u672c\u6458\u8981\uff0cmT5\u8fdb\u884c\u591a\u8bed\u8a00\u7ffb\u8bd1\uff0c\u65e8\u5728\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\uff0c\u5e76\u9884\u6d4b\u53ef\u80fd\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u5982\u6050\u60e7\u75c7\u548c\u996e\u98df\u5931\u8c03\u7b49\uff0c\u4e3a\u5fc3\u7406\u6cbb\u7597\u5e08\u63d0\u4f9b\u65e9\u671f\u5e72\u9884\u5de5\u5177\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u8ba4\u77e5\u8def\u5f84\uff0c\u800c\u8fd9\u4e9b\u8def\u5f84\u5bf9\u4e8e\u5fc3\u7406\u6cbb\u7597\u5e08\u53ca\u65f6\u6709\u6548\u7684\u5728\u7ebf\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528CBT\u6846\u67b6\uff0c\u7ed3\u5408\u63a5\u53d7\u3001\u627f\u8bfa\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff08\u5982BERT\u3001RoBERTa\u3001T5\u3001PEGASUS\u548cmT5\uff09\uff0c\u5bf9\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u8fdb\u884c\u5206\u7c7b\u548c\u5904\u7406\uff0c\u68c0\u6d4b\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\uff0c\u5e76\u9884\u6d4b\u5176\u4ed6\u6f5c\u5728\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u3002", "result": "\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u591f\u8bc6\u522b\u8d1f\u9762\u601d\u7ef4\uff0c\u8fd8\u80fd\u9884\u6d4b\u989d\u5916\u7684\u8d1f\u9762\u526f\u4f5c\u7528\u548c\u5176\u4ed6\u6f5c\u5728\u7684\u5fc3\u7406\u5065\u5eb7\u969c\u788d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5bf9\u5fc3\u7406\u95ee\u9898\u7684\u7406\u89e3\u548c\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u4e3a\u5fc3\u7406\u6cbb\u7597\u5e08\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u5404\u79cd\u5fc3\u7406\u95ee\u9898\uff0c\u63d0\u5347\u4e86CBT\u5728\u6570\u5b57\u65f6\u4ee3\u7684\u6548\u679c\u3002"}}
{"id": "2506.23286", "pdf": "https://arxiv.org/pdf/2506.23286", "abs": "https://arxiv.org/abs/2506.23286", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025 for oral presentation", "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "AI": {"tldr": "\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4e2d\u5f88\u5927\u4e00\u90e8\u5206\u81f4\u529b\u4e8e\u7406\u89e3\u4ee4\u4eba\u60ca\u8bb6\u6216\u53cd\u76f4\u89c9\u7684\u73b0\u8c61\uff0c\u5982\u53cc\u91cd\u4e0b\u964d\u3001grokking\u548c\u5f69\u7968\u5047\u8bbe\u7b49\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u73b0\u8c61\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8bc1\u636e\u5f88\u5c11\uff0c\u56e0\u6b64\u4e0d\u5e94\u5c06\u5b83\u4eec\u89c6\u4e3a\u9700\u8981\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u7684\u5b64\u7acb\u8c1c\u9898\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8fd9\u4e9b\u73b0\u8c61\u4ecd\u5177\u6709\u7814\u7a76\u4ef7\u503c\uff0c\u56e0\u4e3a\u5b83\u4eec\u63d0\u4f9b\u4e86\u6539\u8fdb\u66f4\u5e7f\u6cdb\u7684\u6df1\u5ea6\u5b66\u4e60\u539f\u5219\u89e3\u91ca\u7406\u8bba\u7684\u72ec\u7279\u73af\u5883\u3002\u672c\u6587\u901a\u8fc7\u5206\u6790\u51e0\u4e2a\u663e\u8457\u4f8b\u5b50\u7684\u7814\u7a76\u6210\u679c\uff0c\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f53\u524d\u7814\u7a76\u793e\u533a\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u7684\u89c4\u8303\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u9645\u5efa\u8bae\uff0c\u4ee5\u786e\u4fdd\u6df1\u5ea6\u5b66\u4e60\u73b0\u8c61\u7684\u7814\u7a76\u8fdb\u5c55\u4e0e\u6574\u4e2a\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u6700\u7ec8\u5b9e\u7528\u76ee\u6807\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u53cd\u76f4\u89c9\u73b0\u8c61\uff08\u4f8b\u5982\u53cc\u91cd\u4e0b\u964d\u3001grokking\u548c\u5f69\u7968\u5047\u8bbe\uff09\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7684\u91cd\u8981\u90e8\u5206\u3002\u7136\u800c\uff0c\u8bb8\u591a\u8fd9\u4e9b\u73b0\u8c61\u53ef\u80fd\u5e76\u672a\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5e7f\u6cdb\u51fa\u73b0\uff0c\u4e13\u6ce8\u4e8e\u8fd9\u4e9b\u5b64\u7acb\u73b0\u8c61\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\uff0c\u4e0d\u5229\u4e8e\u63a8\u52a8\u6574\u4e2a\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u56de\u987e\u5e76\u5206\u6790\u4e86\u8fd1\u671f\u6587\u732e\u4e2d\u51e0\u4e2a\u663e\u8457\u7684\u6df1\u5ea6\u5b66\u4e60\u73b0\u8c61\u7684\u7814\u7a76\u7ed3\u679c\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u73b0\u8c61\u7684\u7814\u7a76\u80cc\u666f\u3001\u65b9\u6cd5\u53ca\u7ed3\u8bba\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff1a\u4e0d\u5c06\u8fd9\u4e9b\u73b0\u8c61\u89c6\u4e3a\u5b64\u7acb\u95ee\u9898\uff0c\u800c\u662f\u5c06\u5176\u4f5c\u4e3a\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u4e00\u822c\u6027\u539f\u5219\u89e3\u91ca\u7406\u8bba\u7684\u72ec\u7279\u573a\u666f\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u91cd\u65b0\u5ba1\u89c6\u4e86\u7814\u7a76\u793e\u533a\u5f53\u524d\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u7684\u89c4\u8303\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bb8\u591a\u6df1\u5ea6\u5b66\u4e60\u73b0\u8c61\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7f3a\u4e4f\u5145\u5206\u8bc1\u636e\u652f\u6301\uff0c\u56e0\u6b64\u4e0d\u5e94\u8fc7\u5ea6\u5173\u6ce8\u8fd9\u4e9b\u73b0\u8c61\u7684\u72ec\u7acb\u89e3\u91ca\u3002\u76f8\u53cd\uff0c\u5e94\u5229\u7528\u8fd9\u4e9b\u73b0\u8c61\u63d0\u4f9b\u7684\u72ec\u7279\u573a\u666f\u6765\u6539\u8fdb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u66f4\u5e7f\u6cdb\u539f\u5219\u7684\u7406\u89e3\u3002\u8fd9\u6709\u52a9\u4e8e\u786e\u4fdd\u7814\u7a76\u65b9\u5411\u4e0e\u6574\u4e2a\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u5b9e\u9645\u76ee\u6807\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u6df1\u5ea6\u5b66\u4e60\u73b0\u8c61\u7684\u7814\u7a76\u5e94\u8d85\u8d8a\u5bf9\u5176\u5b64\u7acb\u89e3\u91ca\u7684\u5173\u6ce8\uff0c\u8f6c\u800c\u805a\u7126\u4e8e\u5982\u4f55\u901a\u8fc7\u8fd9\u4e9b\u73b0\u8c61\u6539\u8fdb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u666e\u904d\u539f\u5219\u7684\u7406\u89e3\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u52aa\u529b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u5b9e\u9645\u76ee\u6807\u76f8\u4e00\u81f4\u3002"}}
{"id": "2506.22780", "pdf": "https://arxiv.org/pdf/2506.22780", "abs": "https://arxiv.org/abs/2506.22780", "authors": ["Dibyajyoti Chakraborty", "Haiwen Guan", "Jason Stock", "Troy Arcomano", "Guido Cervone", "Romit Maulik"], "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models", "categories": ["cs.LG", "physics.geo-ph"], "comment": null, "summary": "Score-based diffusion modeling is a generative machine learning algorithm\nthat can be used to sample from complex distributions. They achieve this by\nlearning a score function, i.e., the gradient of the log-probability density of\nthe data, and reversing a noising process using the same. Once trained,\nscore-based diffusion models not only generate new samples but also enable\nzero-shot conditioning of the generated samples on observed data. This promises\na novel paradigm for data and model fusion, wherein the implicitly learned\ndistributions of pretrained score-based diffusion models can be updated given\nthe availability of online data in a Bayesian formulation. In this article, we\napply such a concept to the super-resolution of a high-dimensional dynamical\nsystem, given the real-time availability of low-resolution and experimentally\nobserved sparse sensor measurements from multimodal data. Additional analysis\non how score-based sampling can be used for uncertainty estimates is also\nprovided. Our experiments are performed for a super-resolution task that\ngenerates the ERA5 atmospheric dataset given sparse observations from a\ncoarse-grained representation of the same and/or from unstructured experimental\nobservations of the IGRA radiosonde dataset. We demonstrate accurate recovery\nof the high dimensional state given multiple sources of low-fidelity\nmeasurements. We also discover that the generative model can balance the\ninfluence of multiple dataset modalities during spatiotemporal reconstructions.", "AI": {"tldr": "Score-based diffusion modeling is used for super-resolution of high-dimensional dynamical systems, enabling zero-shot conditioning and accurate recovery from low-fidelity measurements.", "motivation": "To apply score-based diffusion models for the super-resolution of high-dimensional dynamical systems using real-time low-resolution and sparse sensor data.", "method": "Learn a score function (gradient of log-probability density) and reverse a noising process to generate new samples and enable zero-shot conditioning. Update implicitly learned distributions with online data in a Bayesian framework.", "result": "Accurate recovery of high-dimensional states from multiple low-fidelity measurement sources; generative model balances influence of different dataset modalities during reconstructions.", "conclusion": "Score-based diffusion models show promise for super-resolution tasks, enabling fusion of data and models in a novel paradigm."}}
{"id": "2506.23504", "pdf": "https://arxiv.org/pdf/2506.23504", "abs": "https://arxiv.org/abs/2506.23504", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "categories": ["cs.AI"], "comment": "6 Pages, 7 Figures", "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AlexNet\u548cLSTM\u7b97\u6cd5\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u4f7f\u7528\u9700\u6c42\u3001\u6e29\u5ea6\u3001\u9633\u5149\u548c\u964d\u96e8\u7b49\u91cd\u8981\u5143\u7d20\u7684\u5386\u53f2\u6570\u636e\uff0c\u5e76\u91c7\u7528\u6700\u5c0f-\u6700\u5927\u7f29\u653e\u548c\u65f6\u95f4\u7a97\u53e3\u7b49\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u672a\u6765\u7535\u529b\u4ef7\u683c\u65b9\u9762\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u5355\u4e00\u6a21\u578b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u4e8697.08%\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9884\u6d4b\u7535\u529b\u4ef7\u683c\u65f6\u53ea\u5173\u6ce8\u9700\u6c42\u548c\u4ef7\u683c\uff0c\u5bfc\u81f4\u5bf9\u6570\u636e\u7684\u5206\u6790\u4e0d\u5145\u5206\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5916\u6c47\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408AlexNet\u548cLSTM\u7b97\u6cd5\u6784\u5efa\u4e00\u4e2a\u6df7\u5408\u6a21\u578b\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u4e2d\u7684\u5173\u952e\u56e0\u7d20\uff08\u5982\u9700\u6c42\u3001\u6e29\u5ea6\u3001\u9633\u5149\u3001\u964d\u96e8\uff09\uff0c\u5e76\u91c7\u7528\u6700\u5c0f-\u6700\u5927\u7f29\u653e\u548c\u65f6\u95f4\u7a97\u53e3\u7b49\u6280\u672f\u8fdb\u884c\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u7684RNN\u548cANN\u6a21\u578b\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e8697.08%\uff0c\u800cRNN\u548cANN\u7684\u51c6\u786e\u7387\u4e3a96.64%\u548c96.63%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6a21\u578b\u5728\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5f71\u54cd\u4ef7\u683c\u7684\u5916\u90e8\u53d8\u91cf\u3002"}}
{"id": "2506.23757", "pdf": "https://arxiv.org/pdf/2506.23757", "abs": "https://arxiv.org/abs/2506.23757", "authors": ["Dan Yao", "Steve McLaughlin", "Yoann Altmann"], "title": "Training of Spiking Neural Networks with Expectation-Propagation", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "10 pages", "summary": "In this paper, we propose a unifying message-passing framework for training\nspiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free\nmethod is capable of learning the marginal distributions of network parameters\nand simultaneously marginalizes nuisance parameters, such as the outputs of\nhidden layers. This framework allows for the first time, training of discrete\nand continuous weights, for deterministic and stochastic spiking networks,\nusing batches of training samples. Although its convergence is not ensured, the\nalgorithm converges in practice faster than gradient-based methods, without\nrequiring a large number of passes through the training data. The\nclassification and regression results presented pave the way for new efficient\ntraining methods for deep Bayesian networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u7edf\u4e00\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u4f7f\u7528\u671f\u671b\u4f20\u64ad\u65b9\u6cd5\uff0c\u65e0\u9700\u68af\u5ea6\u5373\u53ef\u5b66\u4e60\u7f51\u7edc\u53c2\u6570\u7684\u8fb9\u7f18\u5206\u5e03\uff0c\u5e76\u540c\u65f6\u8fb9\u7f18\u5316\u8bf8\u5982\u9690\u85cf\u5c42\u8f93\u51fa\u7b49\u5e72\u6270\u53c2\u6570\u3002\u6b64\u6846\u67b6\u9996\u6b21\u80fd\u591f\u8bad\u7ec3\u79bb\u6563\u548c\u8fde\u7eed\u6743\u91cd\u7684\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027SNNs\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6bd4\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\uff0c\u65e0\u9700\u591a\u6b21\u904d\u5386\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u9ad8\u6548\u7684\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5305\u542b\u79bb\u6563\u548c\u8fde\u7eed\u6743\u91cd\u7684\u7f51\u7edc\u4ee5\u53ca\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u7f51\u7edc\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u51cf\u5c11\u5bf9\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u4f20\u64ad\u7684\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u68af\u5ea6\u8ba1\u7b97\uff0c\u53ef\u540c\u65f6\u5b66\u4e60\u7f51\u7edc\u53c2\u6570\u7684\u8fb9\u7f18\u5206\u5e03\u5e76\u8fb9\u7f18\u5316\u5e72\u6270\u53c2\u6570\u3002\u9002\u7528\u4e8e\u79bb\u6563\u548c\u8fde\u7eed\u6743\u91cd\u7684\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027SNNs\uff0c\u5229\u7528\u8bad\u7ec3\u6837\u672c\u6279\u6b21\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u6df1\u5ea6\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002\u76f8\u8f83\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u5176\u5728\u5b9e\u8df5\u4e2d\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u68af\u5ea6\u671f\u671b\u4f20\u64ad\u6846\u67b6\u4e3a\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5305\u542b\u79bb\u6563\u548c\u8fde\u7eed\u6743\u91cd\u7684\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u7f51\u7edc\u3002\u5c3d\u7ba1\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5c1a\u672a\u5b8c\u5168\u4fdd\u8bc1\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.22802", "pdf": "https://arxiv.org/pdf/2506.22802", "abs": "https://arxiv.org/abs/2506.22802", "authors": ["Hae Jin Song", "Laurent Itti"], "title": "Riemannian-Geometric Fingerprints of Generative Models", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "comment": null, "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b(GMs)\u7684\u5f52\u5c5e\u548c\u6307\u7eb9\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7406\u89e3\u5176\u6307\u7eb9\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002\u672c\u6587\u901a\u8fc7\u9ece\u66fc\u51e0\u4f55\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4ef6\u548c\u6307\u7eb9\u5b9a\u4e49\u65b9\u6cd5\uff0c\u63a8\u5e7f\u4e86\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97\u6307\u7eb9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u4e0d\u540cGMs\u3001\u6570\u636e\u96c6\u3001\u6a21\u578b\u67b6\u6784\u548c\u6a21\u6001\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5f52\u5c5e\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5f52\u5c5e\u548c\u6307\u7eb9\u5bf9\u4e8e\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u3001\u9a8c\u8bc1\u751f\u6210\u5185\u5bb9\u6765\u6e90\u4ee5\u53ca\u533a\u5206\u5408\u6210\u4e0e\u4eba\u7c7b\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u751f\u6210\u6a21\u578b\u6307\u7eb9\u7684\u7406\u89e3\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u6b63\u5f0f\u7684\u6846\u67b6\u6765\u5b9a\u4e49\u3001\u8868\u793a\u548c\u5206\u6790\u8fd9\u4e9b\u6307\u7eb9\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ece\u66fc\u51e0\u4f55\u63d0\u51fa\u751f\u6210\u6a21\u578b\u5de5\u4ef6\u548c\u6307\u7eb9\u7684\u65b0\u5b9a\u4e49\u3002\u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u9ece\u66fc\u5ea6\u91cf\uff0c\u7528\u6d4b\u5730\u7ebf\u8ddd\u79bb\u548c\u57fa\u4e8ekNN\u7684\u9ece\u66fc\u8d28\u5fc3\u53d6\u4ee3\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5c06\u5148\u524d\u5de5\u4f5c\u63a8\u5e7f\u5230\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u3002\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68af\u5ea6\u7684\u7b97\u6cd5\u7528\u4e8e\u5b9e\u9645\u8ba1\u7b97\u6307\u7eb9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206\u5927\u91cf\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\uff0c\u6db5\u76d64\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u300127\u79cd\u6a21\u578b\u67b6\u6784\u548c2\u79cd\u6a21\u6001\u3002\u4f7f\u7528\u6240\u63d0\u5b9a\u4e49\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5f52\u5c5e\u6027\u80fd\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u96c6\u3001\u6a21\u578b\u7c7b\u578b\u548c\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u751f\u6210\u6a21\u578b\u6307\u7eb9\u5b9a\u4e49\u53ca\u5176\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u6a21\u578b\u5f52\u5c5e\u548c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.23517", "pdf": "https://arxiv.org/pdf/2506.23517", "abs": "https://arxiv.org/abs/2506.23517", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "AI": {"tldr": "\u5c3d\u7ba1GPTZero\u5728\u68c0\u6d4b\u7eafAI\u751f\u6210\u5185\u5bb9\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u533a\u5206\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u7ae0\u65f6\u53ef\u9760\u6027\u6709\u9650\uff0c\u6559\u80b2\u8005\u5e94\u8c28\u614e\u4f7f\u7528AI\u68c0\u6d4b\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5b66\u751f\u4f7f\u7528AI\u5de5\u5177\u7684\u666e\u904d\u5316\uff0c\u6559\u5e08\u5f00\u59cb\u4f7f\u7528\u5982GPTZero\u548cQuillBot\u7b49AI\u68c0\u6d4b\u5de5\u5177\u6765\u8bc6\u522bAI\u7f16\u5199\u7684\u5185\u5bb9\uff0c\u4f46\u8fd9\u4e9b\u68c0\u6d4b\u5668\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u786e\u5b9a\u3002", "method": "\u7814\u7a76\u4e3b\u8981\u5173\u6ce8GPTZero\u6210\u529f\u8bc6\u522bAI\u751f\u6210\u6587\u672c\u7684\u6bd4\u7387\uff0c\u57fa\u4e8e\u4e0d\u540c\u957f\u5ea6\uff08\u77ed\u3001\u4e2d\u3001\u957f\uff09\u968f\u673a\u63d0\u4ea4\u7684\u8bba\u6587\u8fdb\u884c\u5b9e\u9a8c\u3002\u6536\u96c6\u4e8628\u7bc7AI\u751f\u6210\u548c50\u7bc7\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u7ae0\u6570\u636e\u96c6\uff0c\u5e76\u7528GPTZero\u5206\u522b\u68c0\u6d4b\u5176AI\u751f\u6210\u6bd4\u4f8b\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "\u7edd\u5927\u591a\u6570AI\u751f\u6210\u7684\u6587\u7ae0\u88ab\u51c6\u786e\u68c0\u6d4b\uff08AI\u751f\u6210\u53ef\u4fe1\u5ea6\u4e3a91-100%\uff09\uff0c\u800c\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u7ae0\u68c0\u6d4b\u7ed3\u679c\u6ce2\u52a8\uff0c\u5b58\u5728\u5c11\u91cf\u8bef\u62a5\u3002", "conclusion": "GPTZero\u5728\u68c0\u6d4b\u7eafAI\u751f\u6210\u5185\u5bb9\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5728\u533a\u5206\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u7ae0\u65f6\u53ef\u9760\u6027\u6709\u9650\uff0c\u6559\u80b2\u8005\u4e0d\u5e94\u5b8c\u5168\u4f9d\u8d56AI\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2506.24042", "pdf": "https://arxiv.org/pdf/2506.24042", "abs": "https://arxiv.org/abs/2506.24042", "authors": ["Gen Li", "Yuchen Zhou", "Yuting Wei", "Yuxin Chen"], "title": "Faster Diffusion Models via Higher-Order Approximation", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "In this paper, we explore provable acceleration of diffusion models without\nany additional retraining. Focusing on the task of approximating a target data\ndistribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation\ndistance, we propose a principled, training-free sampling algorithm that\nrequires only the order of\n  $$ d^{1+2/K} \\varepsilon^{-1/K} $$\n  score function evaluations (up to log factor) in the presence of accurate\nscores, where $K$ is an arbitrarily large fixed integer. This result applies to\na broad class of target data distributions, without the need for assumptions\nsuch as smoothness or log-concavity. Our theory is robust vis-a-vis inexact\nscore estimation, degrading gracefully as the score estimation error increases\n-- without demanding higher-order smoothness on the score estimates as assumed\nin previous work. The proposed algorithm draws insight from high-order ODE\nsolvers, leveraging high-order Lagrange interpolation and successive refinement\nto approximate the integral derived from the probability flow ODE.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22809", "pdf": "https://arxiv.org/pdf/2506.22809", "abs": "https://arxiv.org/abs/2506.22809", "authors": ["Cooper Doyle"], "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "AI": {"tldr": "\u63d0\u51faBayesLoRA\u6846\u67b6\uff0c\u7ed3\u5408MC-Dropout\u4e0eLoRA\u9002\u914d\u5668\uff0c\u4ee5\u63d0\u4f9b\u4efb\u52a1\u7279\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u53ca\u53ef\u9760\u7f6e\u4fe1\u4f30\u8ba1\uff0c\u52a9\u529b\u4ee3\u7406\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u7684transformer\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u901a\u5e38\u662f\u901a\u7528\u7684\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5de5\u4f5c\u6d41\u7684\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4f7f\u5f97\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u81ea\u7701\u548c\u884c\u4e3a\u8c03\u8282\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBayesLoRA\u7684\u4efb\u52a1\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06MC-Dropout\u6574\u5408\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u4e2d\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cBayesLoRA\u4e3a\u4e0b\u6e38\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u7684\u4fdd\u969c\u3002", "result": "\u7406\u8bba\u4e0a\u548c\u5b9e\u8bc1\u4e0a\u8bc1\u660e\u4e86LoRA\u9002\u914d\u5668\u5728\u5fae\u8c03\u5206\u5e03\u4e4b\u5916\u8868\u73b0\u51fa\u653e\u5927\u65b9\u5dee\u7684\u7279\u6027\uff0c\u4ece\u800c\u4e3a\u4ee3\u7406\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7f6e\u4fe1\u4f30\u8ba1\u3002", "conclusion": "BayesLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u7279\u5b9a\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2506.23520", "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "categories": ["cs.AI"], "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "\u63d0\u51faChemActor\uff0c\u4e00\u4e2a\u5b8c\u5168\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f5c\u4e3a\u5316\u5b66\u6267\u884c\u5668\uff0c\u7528\u4e8e\u5c06\u975e\u7ed3\u6784\u5316\u5b9e\u9a8c\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u3002\u901a\u8fc7\u5f15\u5165\u987a\u5e8fLLM\u751f\u6210\u7684\u6570\u636e\u6846\u67b6\u548c\u591a\u8f6eLLM\u5faa\u73af\u5ba1\u67e5\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u6ce8\u91ca\u6570\u636e\u4e0d\u8db3\u548c\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u53cd\u5e94\u5230\u63cf\u8ff0\uff08R2D\uff09\u548c\u63cf\u8ff0\u5230\u52a8\u4f5c\uff08D2A\uff09\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6709\u673a\u5316\u5b66\u80cc\u666f\u4e0b\u5bf9\u673a\u5668\u4eba\u5408\u6210\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4ece\u6587\u732e\u4e2d\u81ea\u52a8\u63d0\u53d6\u5316\u5b66\u7a0b\u5e8f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5316\u5b66\u8bed\u8a00\u7684\u56fa\u6709\u6b67\u4e49\u548c\u5f00\u53d1\u53ef\u9760\u8ba1\u7b97\u673a\u8f85\u52a9\u63d0\u53d6\u534f\u8bae\u6240\u9700\u7684\u4eba\u5de5\u6ce8\u91ca\u6210\u672c\u9ad8\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86ChemActor\uff0c\u4e00\u4e2a\u5b8c\u5168\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7528\u4f5c\u5316\u5b66\u6267\u884c\u5668\uff0c\u4ee5\u5728\u975e\u7ed3\u6784\u5316\u5b9e\u9a8c\u7a0b\u5e8f\u548c\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u987a\u5e8fLLM\u751f\u6210\u7684\u6570\u636e\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u9009\u62e9\u6570\u636e\u7684\u6570\u636e\u9009\u62e9\u6a21\u5757\uff0c\u4e0e\u901a\u7528\u76ee\u7684LLM\u7ed3\u5408\uff0c\u4ece\u5355\u4e00\u5206\u5b50\u8f93\u5165\u751f\u6210\u673a\u5668\u53ef\u6267\u884c\u7684\u52a8\u4f5c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8f6eLLMs\u5708\u5ba1\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5728\u53cd\u5e94\u5230\u63cf\u8ff0\uff08R2D\uff09\u548c\u63cf\u8ff0\u5230\u52a8\u4f5c\uff08D2A\uff09\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u7531LLM\u751f\u6210\u7684\u6570\u636e\u589e\u5f3a\u7684ChemActor\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa10%\u3002", "conclusion": "ChemActor\u901a\u8fc7\u4f7f\u7528LLM\u751f\u6210\u7684\u6570\u636e\u6846\u67b6\u548c\u591a\u8f6eLLMs\u5708\u5ba1\u5ea6\u91cf\u6807\u51c6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6ce8\u91ca\u6570\u636e\u4e0d\u8db3\u548c\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5316\u5b66\u7a0b\u5e8f\u7684\u81ea\u52a8\u63d0\u53d6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2506.24120", "pdf": "https://arxiv.org/pdf/2506.24120", "abs": "https://arxiv.org/abs/2506.24120", "authors": ["Yuqing Wang", "Shangding Gu"], "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "AI": {"tldr": "\u9009\u62e9\u66f4\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6700\u5927\u5316\u6570\u636e\u70b9\u95f4\u6700\u5c0f\u8ddd\u79bb\uff08h_min\uff09\u53ef\u52a0\u901f\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\u8fc7\u7a0b\uff0c\u5e76\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u662f\u5426\u5b58\u5728\u5176\u4ed6\u5b9a\u91cf\u4e14\u901a\u7528\u7684\u6570\u636e\u9009\u62e9\u539f\u5219\u5c1a\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u5148\u9a8c\u77e5\u8bc6\u6709\u9650\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "method": "1. \u63d0\u51fa\u9009\u62e9\u66f4\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6570\u636e\u70b9\u95f4\u6700\u5c0f\u8ddd\u79bb(h_min)\u7684\u6307\u6807\u3002\n2. \u7406\u8bba\u8bc1\u660e\uff1a\u8f83\u5c0f\u7684h_min\u4f1a\u51cf\u6162\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u52a8\u6001\uff0c\u800c\u8f83\u5927\u7684h_min\u53ef\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\n3. \u63d0\u4f9b\u8d85\u8d8a\u795e\u7ecf\u5207\u7ebf\u6838(NTK)\u673a\u5236\u7684\u6536\u655b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5305\u62ecTransformer\u5728\u5185\u7684\u591a\u79cd\u67b6\u6784\u3002\n4. \u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e0d\u540c\u4f18\u5316\u7b56\u7565\u3001\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6570\u636e\u70b9\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\u9009\u62e9\u6570\u636e\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5728\u5404\u7c7b\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u7684LLM\u6027\u80fd\u3002", "conclusion": "\u9009\u62e9\u66f4\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u80fd\u6709\u6548\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2506.22821", "pdf": "https://arxiv.org/pdf/2506.22821", "abs": "https://arxiv.org/abs/2506.22821", "authors": ["Thomas Gaskin", "Guy J. Abel"], "title": "Deep learning 40 years of human migration", "categories": ["cs.LG", "68T07", "I.2.6"], "comment": null, "summary": "We present a novel and detailed dataset on origin-destination annual\nmigration flows and stocks between 230 countries and regions, spanning the\nperiod from 1990 to the present. Our flow estimates are further disaggregated\nby country of birth, providing a comprehensive picture of migration over the\nlast 43 years. The estimates are obtained by training a deep recurrent neural\nnetwork to learn flow patterns from 18 covariates for all countries, including\ngeographic, economic, cultural, societal, and political information. The\nrecurrent architecture of the neural network means that the entire past can\ninfluence current migration patterns, allowing us to learn long-range temporal\ncorrelations. By training an ensemble of neural networks and additionally\npushing uncertainty on the covariates through the trained network, we obtain\nconfidence bounds for all our estimates, allowing researchers to pinpoint the\ngeographic regions most in need of additional data collection. We validate our\napproach on various test sets of unseen data, demonstrating that it\nsignificantly outperforms traditional methods estimating five-year flows while\ndelivering a significant increase in temporal resolution. The model is fully\nopen source: all training data, neural network weights, and training code are\nmade public alongside the migration estimates, providing a valuable resource\nfor future studies of human migration.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8be6\u7ec6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e861990\u5e74\u81f3\u4eca230\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\u7684\u5e74\u5ea6\u8fc1\u5f99\u6d41\u52a8\u548c\u5b58\u91cf\u3002\u901a\u8fc7\u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u540818\u4e2a\u534f\u53d8\u91cf\uff08\u5305\u62ec\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u6587\u5316\u3001\u793e\u4f1a\u548c\u653f\u6cbb\u4fe1\u606f\uff09\uff0c\u4f30\u8ba1\u4e86\u8fc1\u79fb\u6a21\u5f0f\uff0c\u5e76\u6309\u51fa\u751f\u56fd\u8fdb\u4e00\u6b65\u7ec6\u5206\u3002\u8be5\u6a21\u578b\u80fd\u591f\u6355\u6349\u957f\u671f\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6240\u6709\u4f30\u8ba1\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4e94\u5e74\u6d41\u4f30\u7b97\u4e0a\u8868\u73b0\u663e\u8457\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5b8c\u5168\u5f00\u6e90\uff0c\u4e3a\u672a\u6765\u7684\u4eba\u7c7b\u8fc1\u79fb\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u56fd\u9645\u8fc1\u79fb\u7684\u6570\u636e\u7f3a\u4e4f\u5168\u9762\u6027\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u7279\u522b\u662f\u5728\u7ec6\u5316\u5230\u51fa\u751f\u56fd\u7684\u8fc1\u79fb\u6d41\u52a8\u65b9\u9762\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u6539\u8fdb\u73b0\u6709\u4f30\u7b97\u65b9\u6cd5\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u6a21\u578b\u6765\u5206\u6790\u548c\u9884\u6d4b\u8fc1\u79fb\u6a21\u5f0f\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u516518\u4e2a\u534f\u53d8\u91cf\uff08\u5305\u62ec\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u6587\u5316\u3001\u793e\u4f1a\u548c\u653f\u6cbb\u4fe1\u606f\uff09\uff0c\u4ee5\u5b66\u4e60\u8fc1\u79fb\u6a21\u5f0f\u3002\u6a21\u578b\u901a\u8fc7\u5176\u9012\u5f52\u67b6\u6784\u8003\u8651\u4e86\u5386\u53f2\u6570\u636e\u5bf9\u5f53\u524d\u8fc1\u79fb\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u6355\u6349\u957f\u671f\u65f6\u95f4\u76f8\u5173\u6027\u3002\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u96c6\u5408\uff0c\u5e76\u5bf9\u534f\u53d8\u91cf\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u751f\u6210\u6240\u6709\u4f30\u8ba1\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u672a\u89c1\u6570\u636e\u6d4b\u8bd5\u96c6\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u4e94\u5e74\u7684\u8fc1\u79fb\u6d41\u4f30\u7b97\u4e0a\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u7f6e\u4fe1\u533a\u95f4\u7684\u63d0\u4f9b\u6709\u52a9\u4e8e\u8bc6\u522b\u9700\u8981\u989d\u5916\u6570\u636e\u6536\u96c6\u7684\u5730\u7406\u533a\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u8fc1\u79fb\u6570\u636e\u96c6\u53ca\u6a21\u578b\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u8fc1\u79fb\u6d41\u7684\u4f30\u7b97\u65b9\u6cd5\u3002\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u6570\u636e\u3001\u4ee3\u7801\u5b8c\u5168\u5f00\u6e90\uff0c\u4e3a\u672a\u6765\u4eba\u7c7b\u8fc1\u79fb\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.23549", "pdf": "https://arxiv.org/pdf/2506.23549", "abs": "https://arxiv.org/abs/2506.23549", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Coordination Transformers (CooT)\uff0c\u901a\u8fc7\u4f7f\u7528\u4ea4\u4e92\u5386\u53f2\u6765\u5feb\u901f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u4f19\u4f34\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u8bad\u7ec3\u9700\u6c42\u4e0a\u7684\u9650\u5236\u3002CooT\u5728\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u6709\u6548\u7684\u534f\u8c03\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u8c03\u65b9\u6cd5\uff0c\u5982\u81ea\u6211\u5bf9\u5f08\u548c\u57fa\u4e8e\u79cd\u7fa4\u7684\u65b9\u6cd5\uff0c\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u4f19\u4f34\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u6216\u8005\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u534f\u8c03\u6846\u67b6\u3002", "method": "\u63d0\u51faCoordination Transformers (CooT)\uff0c\u5229\u7528\u6700\u8fd1\u7684\u4ea4\u4e92\u5386\u53f2\u6765\u5feb\u901f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u4f19\u4f34\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u6d4b\u4e0e\u89c2\u5bdf\u5230\u7684\u4f19\u4f34\u4ea4\u4e92\u4e00\u81f4\u7684\u52a8\u4f5c\uff0c\u4e13\u6ce8\u4e8e\u9002\u5e94\u65b0\u7684\u4f19\u4f34\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u589e\u52a0\u8bad\u7ec3\u4f19\u4f34\u7684\u591a\u6837\u6027\u3002CooT\u5728\u5177\u6709\u4e92\u8865\u884c\u4e3a\u7684\u4e0d\u540c\u4ee3\u7406\u5bf9\u6536\u96c6\u7684\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Overcooked\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cCooT\u5728\u6d89\u53ca\u4e4b\u524d\u672a\u89c1\u8fc7\u7684\u4f19\u4f34\u7684\u534f\u8c03\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86CooT\u662f\u6700\u6709\u6548\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u7a81\u663e\u4e86\u5176\u9c81\u68d2\u6027\u3001\u7075\u6d3b\u6027\u548c\u5bf9\u4e0a\u4e0b\u6587\u7684\u654f\u611f\u6027\u3002", "conclusion": "CooT\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u534f\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u5b66\u4e60\u6709\u6548\u7684\u534f\u8c03\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u6216\u5fae\u8c03\uff0c\u5e76\u4e14\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.22837", "pdf": "https://arxiv.org/pdf/2506.22837", "abs": "https://arxiv.org/abs/2506.22837", "authors": ["Kamil Faber", "Marcin Pietro\u0144", "Dominik \u017burek", "Roberto Corizzo"], "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "AI": {"tldr": "\u63d0\u51faxLSTMAD\uff0c\u9996\u4e2a\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u5168\u7f16\u7801\u5668-\u89e3\u7801\u5668xLSTM\u67b6\u6784\u65b9\u6cd5\u3002\u901a\u8fc7\u9884\u6d4b\uff08xLSTMAD-F\uff09\u548c\u91cd\u5efa\uff08xLSTMAD-R\uff09\u4e24\u79cd\u65b9\u5f0f\uff0c\u5e76\u7ed3\u5408MSE\u548cSoftDTW\u635f\u5931\u51fd\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002\u5728TSB-AD-M\u57fa\u51c6\u4e0a\uff0cxLSTM\u5c55\u73b0\u51fa\u8d85\u8d8a23\u4e2a\u73b0\u6709\u57fa\u7ebf\u7684\u5353\u8d8a\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1xLSTM\u6a21\u578b\u5728\u957f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u65e0\u635f\u538b\u7f29\u53ca\u5927\u89c4\u6a21\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u6709\u7814\u7a76\u5c06\u5176\u5e94\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22xLSTM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e86xLSTMAD\uff0c\u4e00\u79cd\u4e13\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bbe\u8ba1\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5b8c\u6574\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668xLSTM\u67b6\u6784\uff0c\u5176\u4e2d\u7f16\u7801\u5668\u6355\u6349\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u89e3\u7801\u5668\u5206\u4e3a\u4e24\u79cd\u53d8\u4f53\uff1a\u9884\u6d4b\u672a\u6765\u503c\uff08xLSTMAD-F\uff09\u548c\u91cd\u5efa\u8f93\u5165\u65f6\u95f4\u5e8f\u5217\uff08xLSTMAD-R\uff09\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u8f6f\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08SoftDTW\uff09\u4e24\u79cd\u635f\u5931\u51fd\u6570\u6765\u5206\u522b\u8003\u8651\u5c40\u90e8\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5168\u5c40\u5e8f\u5217\u5bf9\u9f50\u3002", "result": "\u5728\u5305\u542b17\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684TSB-AD-M\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cxLSTMAD\u5728VUS-PR\u7b49\u5148\u8fdb\u6307\u6807\u4e0b\u5c55\u73b0\u4e86\u8d85\u8d8a23\u79cd\u6d41\u884c\u5f02\u5e38\u68c0\u6d4b\u57fa\u7ebf\u7684\u6700\u5148\u8fdb\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63ed\u793a\u4e86xLSTM\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u7684\u5f3a\u5927\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.23563", "pdf": "https://arxiv.org/pdf/2506.23563", "abs": "https://arxiv.org/abs/2506.23563", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "AI": {"tldr": "\u63d0\u51faMMReason\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u6311\u6218\u6027\u3001\u5f00\u653e\u6027\u548c\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u57fa\u51c6\u5728\u8bc4\u4f30\u957f\u94fe\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u96be\u5ea6\u548c\u591a\u6837\u6027\u3001\u5bb9\u6613\u731c\u6d4b\u548c\u8bb0\u5fc6\u3001\u4ee5\u53ca\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u4ece\u516d\u4e2a\u5b66\u79d1\u6536\u96c6\u591a\u96be\u5ea6\u7ea7\u522b\u7684\u95ee\u9898\uff0c\u91c7\u7528\u5f00\u653e\u5f0f\u95ee\u9898\u5f62\u5f0f\u5e76\u901a\u8fc7\u591a\u6a21\u578b\u6295\u7968\u6280\u672f\u8fc7\u6ee4\u6389\u731c\u6d4b\u548c\u8bb0\u5fc6\u76f8\u5173\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u5206\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u53c2\u8003\u7684\u4e09\u5143\u8bc4\u5206\u673a\u5236\u6765\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u4f7f\u7528MMReason\u5bf9\u4e3b\u6d41MLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u5e76\u6df1\u5165\u5206\u6790\u5176\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u671f\u671bMMReason\u80fd\u6210\u4e3a\u63a8\u52a8MLLM\u63a8\u7406\u7814\u7a76\u7684\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.22845", "pdf": "https://arxiv.org/pdf/2506.22845", "abs": "https://arxiv.org/abs/2506.22845", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u7684\u516d\u79cd\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc(QNN)\u914d\u7f6e\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u529f\u7387\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u8bc4\u4f30\uff0c\u8bc1\u660eQNNs\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u53ef\u4e0e\u7ecf\u5178\u65b9\u6cd5\u7ade\u4e89\u751a\u81f3\u7565\u80dc\u4e00\u7b79\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7535\u8def\u590d\u6742\u6027\u5bf9\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u7535\u7f51\u7684\u666e\u53ca\u4ee5\u53ca\u53ef\u518d\u751f\u80fd\u6e90\u7cfb\u7edf\u7684\u6574\u5408\uff0c\u673a\u5668\u5b66\u4e60\u5728\u7535\u529b\u9700\u6c42\u9884\u6d4b\u548c\u7cfb\u7edf\u6270\u52a8\u68c0\u6d4b\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNNs\uff09\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u7684\u516d\u79cdQNN\u914d\u7f6e\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u5047\u8bbe\u7ed3\u6784\u8fdb\u884c\u5b9e\u9a8c\u3002\u5229\u7528\u8be6\u7ec6\u7684\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u548c\u672a\u89c1\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86QNNs\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQNNs\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u53ef\u4ee5\u4e0e\u7ecf\u5178\u65b9\u6cd5\u7ade\u4e89\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u7565\u80dc\u4e00\u7b79\u3002\u6b64\u5916\uff0c\u8fd8\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7535\u8def\u590d\u6742\u6027\u5bf9\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5e0c\u671b\u5c06\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u878d\u5165\u5de5\u4f5c\u7684\u80fd\u6e90\u9886\u57df\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86QNNs\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u529f\u7387\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23576", "pdf": "https://arxiv.org/pdf/2506.23576", "abs": "https://arxiv.org/abs/2506.23576", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "categories": ["cs.AI"], "comment": "26 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "AI": {"tldr": "\u6700\u8fd1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\u5f15\u53d1\u4e86\u5173\u4e8e\u8d8a\u72f1\u653b\u51fb\u7684\u62c5\u5fe7\uff0c\u5373\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u7684\u63d0\u793a\u3002\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u4f5c\u4e3a\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e09\u79cd\u8d8a\u72f1\u7b56\u7565\uff0c\u5305\u62ec\u539f\u59cb\u7684AutoDefense\u653b\u51fb\u548cDeepleaps\u7684\u4e24\u79cd\uff1aBetterDan\u548cJB\u3002\u901a\u8fc7\u91cd\u73b0AutoDefense\u6846\u67b6\uff0c\u6211\u4eec\u5c06\u5355\u4ee3\u7406\u8bbe\u7f6e\u4e0e\u53cc\u4ee3\u7406\u548c\u4e09\u4ee3\u7406\u914d\u7f6e\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u901a\u8fc7\u51cf\u5c11\u5047\u9634\u6027\u589e\u5f3a\u4e86\u5bf9\u8d8a\u72f1\u7684\u62b5\u6297\u529b\u3002\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u56e0\u653b\u51fb\u7c7b\u578b\u800c\u5f02\uff0c\u5e76\u5f15\u5165\u4e86\u589e\u52a0\u5047\u9633\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u7b49\u6743\u8861\u3002\u8fd9\u4e9b\u53d1\u73b0\u6307\u51fa\u4e86\u5f53\u524d\u81ea\u52a8\u5316\u9632\u5fa1\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u63d0\u9ad8\u672a\u6765LLM\u7cfb\u7edf\u7684\u5bf9\u9f50\u9c81\u68d2\u6027\u63d0\u51fa\u4e86\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8d8a\u72f1\u653b\u51fb\u6210\u4e3a\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u5b89\u5168\u95ee\u9898\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u590d\u6742\u7684\u8d8a\u72f1\u7b56\u7565\u65f6\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u591a\u4ee3\u7406\u7cfb\u7edf\u662f\u5426\u80fd\u589e\u5f3aLLM\u7684\u5b89\u5168\u6027\u5e76\u51cf\u5c11\u6f5c\u5728\u7684\u6f0f\u6d1e\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u5b9e\u9a8c\u5bf9\u6bd4\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u91cd\u73b0\u4e86AutoDefense\u6846\u67b6\uff0c\u7136\u540e\u5206\u522b\u6d4b\u8bd5\u4e86\u5355\u4ee3\u7406\u3001\u53cc\u4ee3\u7406\u548c\u4e09\u4ee3\u7406\u914d\u7f6e\u5728\u5e94\u5bf9\u4e09\u79cd\u4e0d\u540c\u8d8a\u72f1\u7b56\u7565\uff08AutoDefense\u3001BetterDan\u3001JB\uff09\u65f6\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u8bc4\u4f30\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u51cf\u5c11\u5047\u9634\u6027\u3001\u63d0\u9ad8\u5b89\u5168\u6027\u65b9\u9762\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\uff0c\u7279\u522b\u662f\u51cf\u5c11\u4e86\u5047\u9634\u6027\u7684\u53d1\u751f\u7387\u3002\u7136\u800c\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u653b\u51fb\u5bf9\u5176\u6709\u6548\u6027\u7684\u5f71\u54cd\u5404\u5f02\uff0c\u540c\u65f6\u591a\u4ee3\u7406\u7cfb\u7edf\u4e5f\u5e26\u6765\u4e86\u5047\u9633\u6027\u589e\u52a0\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u5347\u7684\u95ee\u9898\u3002", "conclusion": "\u867d\u7136\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u53d7\u5230\u653b\u51fb\u7c7b\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u4f34\u968f\u7740\u4e00\u5b9a\u7684\u526f\u4f5c\u7528\uff0c\u5982\u5047\u9633\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u589e\u52a0\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u81ea\u52a8\u5316\u9632\u5fa1\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e5f\u4e3a\u672a\u6765\u6539\u8fdbLLM\u7cfb\u7edf\u7684\u5bf9\u9f50\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2506.22848", "pdf": "https://arxiv.org/pdf/2506.22848", "abs": "https://arxiv.org/abs/2506.22848", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08Auto-SLE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21BN\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u3002\u76f8\u6bd4\u5355\u4e00\u7b97\u6cd5\u7684\u5206\u800c\u6cbb\u4e4b\uff08D&D\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u572810,000\u53d8\u91cf\u7684\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u4e8630%~225%\uff0c\u5e76\u4e14\u5bf9\u66f4\u591a\u53d8\u91cf\uff08\u598230,000\uff09\u548c\u4e0d\u540c\u7f51\u7edc\u7279\u5f81\u7684\u6570\u636e\u96c6\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u800c\u6cbb\u4e4b\uff08D&D\uff09\u7b56\u7565\u867d\u7136\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u5b66\u4e60\uff0c\u4f46\u5728\u5b50\u95ee\u9898\u4e2d\u5b66\u4e60\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a33\u5b9a\u63d0\u9ad8\u5b66\u4e60\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u7684\u6982\u5ff5\uff0c\u5c06\u591a\u79cdBN\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\u7ec4\u5408\u8d77\u6765\u4ee5\u5b9e\u73b0\u9ad8\u5b66\u4e60\u7cbe\u5ea6\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u81ea\u52a8\u5b66\u4e60SLE\u7684\u65b9\u6cd5\uff08Auto-SLE\uff09\uff0c\u89e3\u51b3\u624b\u52a8\u8bbe\u8ba1\u9ad8\u8d28\u91cfSLE\u7684\u6311\u6218\uff1b\u6700\u540e\u5c06\u5b66\u4e60\u5230\u7684SLE\u96c6\u6210\u5230D&D\u65b9\u6cd5\u4e2d\u7528\u4e8e\u5927\u89c4\u6a21BN\u7ed3\u6784\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u5355\u4e00BN\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\u7684D&D\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5305\u542b10,000\u4e2a\u53d8\u91cf\u7684\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u7cbe\u5ea6\uff08\u63d0\u5347\u5e45\u5ea6\u8fbe30%~225%\uff09\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5bf9\u66f4\u591a\u53d8\u91cf\uff08\u598230,000\uff09\u548c\u4e0d\u540c\u7f51\u7edc\u7279\u6027\u7684\u6570\u636e\u96c6\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u91c7\u7528\uff08\u81ea\u52a8\u5b66\u4e60\u7684\uff09\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u65b9\u6cd5\u5bf9\u4e8e\u53ef\u6269\u5c55\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5927\u89c4\u6a21BN\u7ed3\u6784\u5b66\u4e60\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.23626", "pdf": "https://arxiv.org/pdf/2506.23626", "abs": "https://arxiv.org/abs/2506.23626", "authors": ["Ant\u00f3nio Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gissl\u00e9n"], "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "categories": ["cs.AI"], "comment": "16 pages in total, 10 pages of main paper, 5 figures", "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fed\u4ee3\u5fae\u8c03\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5956\u52b1\u51fd\u6570\u6743\u91cd\u3002\u5728\u8d5b\u8f66\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6743\u91cd\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u867d\u7136\u5e7f\u6cdb\uff0c\u4f46\u90e8\u7f72RL\u4ee3\u7406\u65f6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u901a\u5e38\u9700\u8981RL\u4e13\u5bb6\uff1b2\uff09\u5f53\u6e38\u620f\u5185\u5bb9\u6216\u673a\u5236\u6539\u53d8\u65f6\uff0c\u5148\u524d\u8c03\u6574\u597d\u7684\u5956\u52b1\u6743\u91cd\u53ef\u80fd\u4e0d\u518d\u662f\u6700\u4f18\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u672c\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u884c\u4e3a\u76ee\u6807\u548c\u4e4b\u524d\u8bad\u7ec3\u8f6e\u6b21\u7684\u6027\u80fd\u7edf\u8ba1\u6458\u8981\uff0c\u8fed\u4ee3\u66f4\u65b0RL\u4ee3\u7406\u7684\u5956\u52b1\u51fd\u6570\u6743\u91cd\u3002\u901a\u8fc7\u95ed\u73af\u8fc7\u7a0b\uff0cLM\u80fd\u591f\u81ea\u6211\u7ea0\u6b63\u548c\u4f18\u5316\u5176\u8f93\u51fa\uff0c\u4ece\u800c\u65e0\u9700\u624b\u52a8\u5956\u52b1\u5de5\u7a0b\u5373\u53ef\u751f\u6210\u66f4\u7b26\u5408\u76ee\u6807\u7684\u884c\u4e3a\u3002", "result": "\u5728\u8d5b\u8f66\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u6bcf\u6b21\u8fed\u4ee3\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\u3002\u4ec5\u4e00\u6b21\u8fed\u4ee3\uff0c\u4ee3\u7406\u6210\u529f\u7387\u4ece9%\u63d0\u5347\u523074%\u3002\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u624b\u52a8\u8bbe\u8ba1\u7684\u6743\u91cd\u76f8\u6bd4\uff0c\u6700\u7ec8\u8fed\u4ee3\u4e2dLM\u8c03\u6574\u7684\u4ee3\u7406\u8fbe\u5230\u4e8680%\u7684\u6210\u529f\u7387\uff0c\u5e73\u5747\u7528\u65f6855\u6b65\uff0c\u4e0e\u4e13\u5bb6\u8c03\u6574\u4ee3\u7406\u768494%\u6210\u529f\u7387\u548c850\u6b65\u7684\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5fae\u8c03\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5956\u52b1\u51fd\u6570\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u4e14\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6743\u91cd\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2506.22871", "pdf": "https://arxiv.org/pdf/2506.22871", "abs": "https://arxiv.org/abs/2506.22871", "authors": ["Homayun Afrabandpey", "Hamed Rezazadegan Tavakoli"], "title": "P$^2$U: Progressive Precision Update For Efficient Model Distribution", "categories": ["cs.LG", "cs.MM", "I.2.6"], "comment": null, "summary": "Efficient model distribution is becoming increasingly critical in\nbandwidth-constrained environments. In this paper, we propose a simple yet\neffective approach called Progressive Precision Update (P$^2$U) to address this\nproblem. Instead of transmitting the original high-precision model, P$^2$U\ntransmits a lower-bit precision model, coupled with a model update representing\nthe difference between the original high-precision model and the transmitted\nlow precision version. With extensive experiments on various model\narchitectures, ranging from small models ($1 - 6$ million parameters) to a\nlarge model (more than $100$ million parameters) and using three different data\nsets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U\nconsistently achieves better tradeoff between accuracy, bandwidth usage and\nlatency. Moreover, we show that when bandwidth or startup time is the priority,\naggressive quantization (e.g., 4-bit) can be used without severely compromising\nperformance. These results establish P$^2$U as an effective and practical\nsolution for scalable and efficient model distribution in low-resource\nsettings, including federated learning, edge computing, and IoT deployments.\nGiven that P$^2$U complements existing compression techniques and can be\nimplemented alongside any compression method, e.g., sparsification,\nquantization, pruning, etc., the potential for improvement is even greater.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProgressive Precision Update (P\u00b2U)\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u5206\u53d1\u6a21\u578b\u3002\u901a\u8fc7\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u6a21\u578b\u53ca\u4e0e\u539f\u59cb\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u66f4\u65b0\uff0cP\u00b2U\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u7387\u3001\u5e26\u5bbd\u4f7f\u7528\u548c\u5ef6\u8fdf\u6743\u8861\u3002\u6b64\u5916\uff0cP\u00b2U\u53ef\u4e0e\u5176\u4ed6\u538b\u7f29\u6280\u672f\u7ed3\u5408\u4f7f\u7528\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u5982\u8054\u90a6\u5b66\u4e60\u548c\u8fb9\u7f18\u8ba1\u7b97\u3002", "motivation": "\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u5206\u53d1\u53d8\u5f97\u8d8a\u6765\u8d8a\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u76f4\u63a5\u4f20\u8f93\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4f1a\u6d88\u8017\u5927\u91cf\u5e26\u5bbd\u548c\u65f6\u95f4\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u5206\u53d1\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProgressive Precision Update (P\u00b2U)\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u6a21\u578b\u4ee5\u53ca\u8868\u793a\u539f\u59cb\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4e0e\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u7248\u672c\u4e4b\u95f4\u5dee\u5f02\u7684\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982\u80f8\u7247X\u5149\u3001PASCAL-VOC\u548cCIFAR-100\uff09\u548c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cP\u00b2U\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7387\u3001\u5e26\u5bbd\u4f7f\u7528\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u66f4\u597d\u6743\u8861\uff0c\u5e76\u4e14\u5728\u4f18\u5148\u8003\u8651\u5e26\u5bbd\u6216\u542f\u52a8\u65f6\u95f4\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u6fc0\u8fdb\u91cf\u5316\uff08\u4f8b\u59824\u4f4d\uff09\u800c\u4e0d\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "P\u00b2U\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u6a21\u578b\u5206\u53d1\uff0c\u5305\u62ec\u8054\u90a6\u5b66\u4e60\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c\u7269\u8054\u7f51\u90e8\u7f72\u3002\u5e76\u4e14\u7531\u4e8e\u5b83\u53ef\u4ee5\u8865\u5145\u73b0\u6709\u7684\u538b\u7f29\u6280\u672f\uff0c\u6f5c\u5728\u6539\u8fdb\u7a7a\u95f4\u66f4\u5927\u3002"}}
{"id": "2506.23673", "pdf": "https://arxiv.org/pdf/2506.23673", "abs": "https://arxiv.org/abs/2506.23673", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Sch\u00fcffler"], "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "categories": ["cs.AI"], "comment": null, "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u9002\u5e94\u6846\u67b6HASD\uff0c\u7528\u4e8e\u89e3\u51b3\u75c5\u7406\u5b66AI\u4e2d\u7684\u5e7b\u706f\u7247\u7ea7\u522b\u9886\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u53ca\u9ad8\u6548\u8ba1\u7b97\u7684\u5e7b\u706f\u7247\u7ea7\u522b\u9886\u57df\u9002\u914d\uff0c\u5728\u4e73\u817a\u764c\u548cUCEC\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u75c5\u7406\u5b66AI\u9762\u4e34\u9886\u57df\u8f6c\u79fb\u7684\u5173\u952e\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u56fe\u50cf\u5757\u800c\u975eWSI\uff08\u5168\u5207\u7247\u5f71\u50cf\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u573a\u666f\u4e2d\u6240\u9700\u7684\u5168\u5c40WSI\u7279\u5f81\u3002", "method": "HASD\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1\uff09\u5206\u5c42\u9002\u5e94\u6846\u67b6\uff0c\u6574\u5408\u9886\u57df\u5bf9\u9f50\u6c42\u89e3\u5668\u3001\u5e7b\u706f\u7247\u7ea7\u51e0\u4f55\u4e0d\u53d8\u6027\u6b63\u5219\u5316\u4ee5\u53ca\u5757\u7ea7\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff1b2\uff09\u539f\u578b\u9009\u62e9\u673a\u5236\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e24\u4e2a\u5e7b\u706f\u7247\u7ea7\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u4e73\u817a\u764cHER2\u5206\u7ea7\u961f\u5217\u4e2d\u63d0\u9ad8\u4e864.1%\u7684AUROC\uff0c\u5728UCEC\u751f\u5b58\u9884\u6d4b\u961f\u5217\u4e2d\u63d0\u5347\u4e863.9%\u7684C-index\u3002", "conclusion": "HASD\u4e3a\u75c5\u7406\u5b66\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u5e7b\u706f\u7247\u7ea7\u522b\u9886\u57df\u9002\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2506.22895", "pdf": "https://arxiv.org/pdf/2506.22895", "abs": "https://arxiv.org/abs/2506.22895", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u21130-\u8303\u6570\u7ea6\u675f\u589e\u5f3a\u4e86\u5468\u671f\u6027\u91cf\u5316\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5b50\u7a7a\u95f4\u8ffd\u8e2a\u51b3\u7b56\u53d8\u91cf\u526a\u679d\u7b56\u7565\uff08DVP\uff09\u4ee5\u52a0\u901f\u4f18\u5316\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u53d8\u5316\u7684\u7a00\u758f\u81ea\u56de\u5f52\u6a21\u578b\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5171\u4eab\u51fa\u884c\u6570\u636e\u4e2d\u63ed\u793a\u4e86\u65e5\u548c\u5468\u5468\u671f\u6027\u4ee5\u53ca\u4eba\u7c7b\u79fb\u52a8\u89c4\u5f8b\u7684\u957f\u671f\u53d8\u5316\uff0c\u5728\u6c14\u5019\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u5b63\u8282\u6027\u548c\u52a8\u6001\u6c14\u5019\u6a21\u5f0f\uff0c\u5982\u5384\u5c14\u5c3c\u8bfa\u73b0\u8c61\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u81ea\u56de\u5f52\u662f\u4e00\u79cd\u7ecf\u5178\u7edf\u8ba1\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u81ea\u76f8\u5173\u5e76\u8bc6\u522b\u65f6\u95f4\u6a21\u5f0f\uff0c\u4f8b\u5982\u5468\u671f\u6027\u548c\u5b63\u8282\u6027\u3002\u7136\u800c\uff0c\u4f20\u7edf\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u548c\u975e\u5e73\u7a33\u6570\u636e\u4e0a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6765\u5206\u6790\u8fd9\u4e9b\u590d\u6742\u7684\u6a21\u5f0f\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u21130-\u8303\u6570\u7ea6\u675f\u7684\u7a00\u758f\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u5468\u671f\u6027\u7684\u53ef\u89e3\u91ca\u6027\u3002\n2. \u5bf9\u4e8e\u65f6\u53d8\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5c06\u7a00\u758f\u81ea\u56de\u5f52\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u4f18\u5316\uff08MIO\uff09\u95ee\u9898\uff0c\u5e76\u5f00\u53d1DVP\u7b56\u7565\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3002\n3. \u9488\u5bf9\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\uff0c\u63d0\u51fa\u4e00\u79cd\u65f6\u7a7a\u53d8\u5316\u7684\u7a00\u758f\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\u4ee5\u89e3\u51b3\u76f8\u5e94MIO\u95ee\u9898\u3002\n4. \u8be5\u65b9\u6848\u53ef\u6269\u5c55\u5230\u5177\u6709\u6570\u767e\u4e07\u51b3\u7b56\u53d8\u91cf\u7684\u5927\u89c4\u6a21\u95ee\u9898\u3002", "result": "1. \u5b9e\u9a8c\u8bc1\u660e\uff0cDVP\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u52a0\u901fMIO\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5b8c\u6574MIO\u6c42\u89e3\u5668\u76f8\u540c\u7684\u89e3\u8d28\u91cf\u3002\n2. \u5728\u5171\u4eab\u51fa\u884c\u6570\u636e\u4e2d\uff0c\u53d1\u73b0\u65e5\u548c\u5468\u5468\u671f\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4eba\u7c7b\u79fb\u52a8\u89c4\u5f8b\u7684\u957f\u671f\u53d8\u5316\u3002\n3. \u5728\u6c14\u5019\u6570\u636e\u4e2d\uff0c\u63ed\u793a\u4e86\u8fc7\u53bb\u56db\u5341\u5e74\u6e29\u5ea6\u548c\u964d\u6c34\u91cf\u7b49\u53d8\u91cf\u7684\u5b63\u8282\u6027\u7a7a\u95f4\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e86\u52a8\u6001\u6c14\u5019\u6a21\u5f0f\u548c\u5384\u5c14\u5c3c\u8bfa\u73b0\u8c61\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a00\u758f\u81ea\u56de\u5f52\u6846\u67b6\u53ca\u5176\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63ed\u793a\u5468\u671f\u6027\u3001\u957f\u671f\u53d8\u5316\u548c\u52a8\u6001\u6c14\u5019\u6a21\u5f0f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2506.23689", "pdf": "https://arxiv.org/pdf/2506.23689", "abs": "https://arxiv.org/abs/2506.23689", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "title": "Pok\u00e9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86Pok\u00e9AI\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u73a9\u901a\u5173\u300a\u7cbe\u7075\u5b9d\u53ef\u68a6 \u7ea2\u300b\u3002\u7cfb\u7edf\u7531\u4e09\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u7ec4\u6210\uff1a\u89c4\u5212\u3001\u6267\u884c\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\uff0c\u5f62\u6210\u95ed\u73af\u51b3\u7b56\u7cfb\u7edf\u3002\u6218\u6597\u6a21\u5757\u4f5c\u4e3a\u521d\u6b65\u6b65\u9aa4\u88ab\u5f00\u53d1\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5e73\u5747\u80dc\u7387\u4e3a80.8%\uff0c\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u73a9\u5bb6\u5dee\u8ddd\u4ec5\u4e3a6%\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\u4e0e\u5176\u6218\u7565\u63a8\u7406\u80fd\u529b\u6709\u663e\u8457\u5173\u8054\uff0c\u4e14\u6bcf\u4e2a\u6a21\u578b\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u6e38\u620f\u98ce\u683c\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u73a9\u6e38\u620f\u521b\u5efa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6218\u6597\u573a\u666f\u4e2d\u7684\u8868\u73b0\u53ca\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u7684\u5bf9\u6bd4\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u62ec\u89c4\u5212\u3001\u6267\u884c\u548c\u8bc4\u4f30\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u5176\u4e2d\u6267\u884c\u667a\u80fd\u4f53\u4e2d\u5305\u542b\u4e00\u4e2a\u6218\u6597\u6a21\u5757\u3002\u901a\u8fc7\u95ed\u73af\u51b3\u7b56\u7cfb\u7edf\u8fdb\u884c\u4efb\u52a1\u751f\u6210\u3001\u6267\u884c\u548c\u8bc4\u4ef7\u3002\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u5904\u7406\u5e76\u5206\u6790\u5176\u5728\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6218\u6597AI\u572850\u6b21\u91ce\u751f\u6218\u6597\u4e2d\u53d6\u5f97\u4e8680.8%\u7684\u5e73\u5747\u80dc\u7387\uff0c\u4ec5\u6bd4\u6709\u7ecf\u9a8c\u7684\u4eba\u7c7b\u73a9\u5bb6\u4f4e6%\u3002\u6a21\u578b\u5728\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u5f97\u5206\u4e0e\u5176\u6218\u6597\u8868\u73b0\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u6a21\u578b\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u6e38\u620f\u98ce\u683c\u3002", "conclusion": "Pok\u00e9AI\u5c55\u793a\u4e86\u4e00\u4e2a\u6210\u529f\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6e38\u620f\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6218\u7565\u63a8\u7406\u548c\u6e38\u620f\u8868\u73b0\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22901", "pdf": "https://arxiv.org/pdf/2506.22901", "abs": "https://arxiv.org/abs/2506.22901", "authors": ["Sina Tabakhi", "Haiping Lu"], "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "comment": "15 pages, 7 figures", "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "AI": {"tldr": "MAGNET\u662f\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u60a3\u8005-\u6a21\u6001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u6784\u5efa\u60a3\u8005\u56fe\u6765\u89e3\u51b3\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5176\u590d\u6742\u5ea6\u968f\u6a21\u6001\u6570\u91cf\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u764c\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u878d\u5408\u65b9\u6cd5\u5904\u7406\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u7f3a\u5931\u6a21\u5f0f\u4ee5\u53ca\u6a21\u6001\u6570\u91cf\u589e\u52a0\u5bfc\u81f4\u7684\u6307\u6570\u7ea7\u6a21\u5f0f\u6570\u589e\u957f\u3002", "method": "\u63d0\u51faMAGNET\uff08Missing-modality-Aware Graph neural NETwork\uff09\uff0c\u901a\u8fc7\u60a3\u8005-\u6a21\u6001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4f4e\u7ef4\u6a21\u6001\u5d4c\u5165\uff0c\u6839\u636e\u91cd\u8981\u6027\u548c\u7f3a\u5931\u6027\u8c03\u6574\u6743\u91cd\uff1b\u968f\u540e\u6784\u5efa\u60a3\u8005\u56fe\uff0c\u4ee5\u878d\u5408\u7684\u591a\u6a21\u6001\u5d4c\u5165\u4e3a\u8282\u70b9\u7279\u5f81\uff0c\u6a21\u6001\u7f3a\u5931\u6027\u51b3\u5b9a\u8fde\u901a\u6027\uff0c\u6700\u540e\u4f7f\u7528\u5e38\u89c4\u56fe\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u591a\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u764c\u75c7\u5206\u7c7b\u5b9e\u9a8c\u8868\u660e\uff0cMAGNET\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7f3a\u5931\u6a21\u5f0f\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "MAGNET\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u589e\u957f\u548c\u826f\u597d\u7684\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6a21\u6001\u6570\u636e\u4efb\u52a1\u3002"}}
{"id": "2506.23692", "pdf": "https://arxiv.org/pdf/2506.23692", "abs": "https://arxiv.org/abs/2506.23692", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "AI": {"tldr": "\u63d0\u51faAgent for Science (Agent4S)\uff0c\u4f5c\u4e3a\u7b2c\u4e94\u79d1\u5b66\u8303\u5f0f\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ee3\u7406\u81ea\u52a8\u5b8c\u6210\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f15\u5165\u4e94\u7ea7\u5206\u7c7b\u6cd5\u63cf\u7ed8\u4ece\u7b80\u5355\u4efb\u52a1\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u4e3b\u534f\u4f5c\u7684\u201cAI\u79d1\u5b66\u5bb6\u201d\u7684\u8def\u7ebf\u56fe\u3002", "motivation": "\u5f53\u524dAI for Science\u4ec5\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\uff0c\u672a\u80fd\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u7684\u6838\u5fc3\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u63d0\u51faAgent4S\u6982\u5ff5\u53ca\u4e94\u7ea7\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4ece\u7b80\u5355\u4efb\u52a1\u81ea\u52a8\u5316\u5230\u521b\u5efa\u5b8c\u5168\u81ea\u4e3b\u534f\u4f5c\u7684\u201cAI\u79d1\u5b66\u5bb6\u201d\u7684\u5b8c\u6574\u8def\u5f84\u3002", "result": "\u5b9a\u4e49\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u4e0b\u4e00\u4e2a\u9769\u547d\u6027\u6b65\u9aa4\uff0c\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u53d1\u5c55\u84dd\u56fe\u3002", "conclusion": "Agent4S\u4ee3\u8868\u4e86\u771f\u6b63\u7684\u7b2c\u4e94\u79d1\u5b66\u8303\u5f0f\uff0c\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8\u79d1\u5b66\u7814\u7a76\u7684\u65b9\u5f0f\u3002"}}
{"id": "2506.22927", "pdf": "https://arxiv.org/pdf/2506.22927", "abs": "https://arxiv.org/abs/2506.22927", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "categories": ["cs.LG"], "comment": null, "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "AI": {"tldr": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u751f\u6210AI\u4ecd\u76f8\u5bf9\u6b20\u53d1\u8fbe\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4ece\u6587\u672c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u5c55\u793a\u4e86\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b63,010\u4e2a\u65f6\u95f4\u5e8f\u5217-\u63cf\u8ff0\u5bf9\u7684\u65b0\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5728\u91d1\u878d\u3001\u6c14\u5019\u7b49\u5173\u952e\u9886\u57df\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210AI\u5374\u53d1\u5c55\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u975e\u7ed3\u6784\u5316\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b63,010\u4e2a\u65f6\u95f4\u5e8f\u5217-\u63cf\u8ff0\u5bf9\u7684\u65b0\u516c\u5171\u6570\u636e\u96c6\u3002", "result": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u884c\u6027\uff0c\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u5b9a\u5236\u9884\u6d4b\u3001\u65f6\u95f4\u5e8f\u5217\u64cd\u63a7\u3001\u6570\u636e\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u7b49\u9886\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23703", "pdf": "https://arxiv.org/pdf/2506.23703", "abs": "https://arxiv.org/abs/2506.23703", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "categories": ["cs.AI"], "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u548c\u7cfb\u7edf\u5206\u6790\u7684\u65b0\u89c6\u89d2\uff0c\u79f0\u4e3a\u6570\u636e\u63a7\u5236\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u7684\u53d1\u5c55\uff0c\u5e76\u901a\u8fc7\u8de8\u5b66\u79d1\u65b9\u6cd5\u5229\u7528\u73b0\u6709\u7684\u5b89\u5168\u6027\u5206\u6790\u548c\u4fdd\u969c\u624b\u6bb5\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4fdd\u8bc1\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5173\u952e\u7684\u73b0\u5b9e\u4e16\u754c\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u3002\u867d\u7136\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5229\u7528\u4e86AI\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u6539\u8fdb\u63a7\u5236\u7cfb\u7edf\uff0c\u4f46\u63a7\u5236\u7406\u8bba\u4e5f\u53ef\u4ee5\u53cd\u8fc7\u6765\u88ab\u7528\u6765\u63d0\u9ad8AI\u7684\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u89e3\u91ca\u548cAI\u7cfb\u7edf\u62bd\u8c61\uff0c\u7ed3\u5408\u7cfb\u7edf\u7406\u8bba\u542f\u53d1\u548c\u7cfb\u7edf\u5206\u6790\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u63a7\u5236\u8fd9\u4e00\u65b0\u89c6\u89d2\u3002\u91c7\u7528\u81ea\u9876\u5411\u4e0b\u7684\u65b9\u6cd5\uff0c\u5728\u62bd\u8c61\u5c42\u9762\u4e0a\u4e3a\u7279\u5b9aAI\u7cfb\u7edf\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u7ec6\u5316\u7684\u5b89\u5168\u6027\u5206\u6790\u548c\u4fdd\u969c\u57fa\u7840\u3002", "result": "\u63d0\u51fa\u4e86\u6570\u636e\u63a7\u5236\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u63a8\u52a8AI\u5de5\u7a0b\u5b66\u5229\u7528\u73b0\u6709\u7684\u5b89\u5168\u6027\u5206\u6790\u548c\u4fdd\u969c\u624b\u6bb5\uff0c\u4ece\u800c\u63a8\u52a8\u6570\u636e\u63a7\u5236\u8303\u5f0f\u7684\u8fdb\u6b65\u3002", "conclusion": "\u6570\u636e\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u57fa\u7840\uff0c\u53ef\u4ee5\u4e3a\u5177\u4f53\u7684AI\u7cfb\u7edf\u548c\u5e94\u7528\u8fdb\u884c\u7ec6\u5316\uff0c\u5e76\u4e14\u51c6\u5907\u8fce\u63a5\u672a\u6765\u7684\u521b\u65b0\u3002"}}
{"id": "2506.22929", "pdf": "https://arxiv.org/pdf/2506.22929", "abs": "https://arxiv.org/abs/2506.22929", "authors": ["Chen Zhang"], "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u5b8c\u5907\u6027\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u5c06\u9ad8\u7ef4\u6570\u636e\u5206\u89e3\u4e3a\u7ef4\u5ea6\u72ec\u7acb\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u5e03\u5f0f\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u6316\u6398\u4e0e\u5e76\u884c\u4f18\u5316\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u7136\u56fe\u50cf\u548c\u8bed\u8a00\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5e94\u7528\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u6709\u5927\u6570\u636e\u5de5\u5177\u7f3a\u4e4f\u5bf9\u9ad8\u7ea7\u5206\u6790\u7684\u6570\u7406\u7edf\u8ba1\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7a7a\u95f4\u5b8c\u5907\u6027\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u5c06\u9ad8\u7ef4\u6570\u636e\u5206\u89e3\u6210\u7ef4\u5ea6\u72ec\u7acb\u7684\u7ed3\u6784\u4ee5\u8fdb\u884c\u5206\u5e03\u5f0f\u5904\u7406\uff0c\u4ece\u800c\u6574\u5408\u6570\u636e\u6316\u6398\u548c\u5e76\u884c\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u652f\u6301\u8de8\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u79d1\u5b66\u8ba1\u7b97\uff0c\u5e76\u5728\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\u5185\u5b9e\u73b0\u3002", "conclusion": "\u6b64\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\u4e3a\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5305\u62ec\u533b\u7597\u548c\u81ea\u7136\u56fe\u50cf\u5728\u5185\u7684\u5404\u7c7b\u6570\u636e\u7c7b\u578b\u3002"}}
{"id": "2506.23706", "pdf": "https://arxiv.org/pdf/2506.23706", "abs": "https://arxiv.org/abs/2506.23706", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u9a8c\u8bc1\u5ba1\u8ba1\uff08Attestable Audits\uff09\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u4e0d\u4e92\u4fe1\u7684\u60c5\u51b5\u4e0b\u4fdd\u62a4\u6a21\u578b\u548c\u6570\u636e\u7684\u654f\u611f\u6027\uff0c\u5e76\u5728\u5178\u578b\u5ba1\u8ba1\u57fa\u51c6\u4e0a\u9488\u5bf9Llama-3.1\u8fdb\u884c\u4e86\u539f\u578b\u53ef\u884c\u6027\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u867d\u7136\u80fd\u591f\u5927\u89c4\u6a21\u8bc4\u4f30AI\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u548c\u57fa\u51c6\u6570\u636e\u96c6\u65b9\u9762\u7f3a\u4e4f\u4fdd\u5bc6\u6027\u3002", "method": "\u901a\u8fc7\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u53ef\u9a8c\u8bc1\u5ba1\u8ba1\uff08Attestable Audits\uff09\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u4e0e\u7b26\u5408\u89c4\u5b9a\u7684AI\u6a21\u578b\u4ea4\u4e92\u65f6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u540c\u65f6\u4fdd\u62a4\u654f\u611f\u6570\u636e\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u63d0\u4f9b\u8005\u548c\u5ba1\u8ba1\u8005\u4e4b\u95f4\u4e0d\u5b58\u5728\u4fe1\u4efb\u5173\u7cfb\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\uff0c\u5c55\u793a\u4e86\u5728\u5178\u578b\u5ba1\u8ba1\u57fa\u51c6\u4e0a\u5bf9Llama-3.1\u8fdb\u884c\u53ef\u884c\u6027\u7684\u9a8c\u8bc1\u3002", "conclusion": "\u89e3\u51b3\u4e86\u6700\u8fd1AI\u6cbb\u7406\u6846\u67b6\u4e2d\u63d0\u51fa\u7684\u9a8c\u8bc1\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.22950", "pdf": "https://arxiv.org/pdf/2506.22950", "abs": "https://arxiv.org/abs/2506.22950", "authors": ["Liangyu Wang", "Huanyi Xie", "Xinhai Wang", "Tianjin Huang", "Mengdi Li", "Di Wang"], "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.", "AI": {"tldr": "\u63d0\u51faInfinite Sampling\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u91c7\u6837\u7ec4\u3001\u8fde\u7eed\u91c7\u6837\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u5668\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f7fGroup-based\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5b9e\u9645GPU\u5185\u5b58\u9650\u5236\u4e0b\u66f4\u9ad8\u6548\u7a33\u5b9a\u3002", "motivation": "\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\uff09\u5728\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u6548\u679c\u663e\u8457\uff0c\u4f46\u968f\u7740\u6837\u672c\u7ec4\u89c4\u6a21\u589e\u52a0\uff0c\u751f\u6210\u548c\u5b58\u50a8\u6bcf\u4e2a\u63d0\u793a\u7684\u591a\u4e2a\u54cd\u5e94\u4f1a\u5bfc\u81f4\u5de8\u5927\u7684\u5185\u5b58\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5176\u5728\u786c\u4ef6\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86Infinite Sampling\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1) \u5fae\u91c7\u6837\u7ec4\uff0c\u5c06\u5927\u7ec4\u5206\u89e3\u4e3a\u5185\u5b58\u53ef\u884c\u7684\u8f6e\u6b21\uff1b(2) \u8fde\u7eed\u91c7\u6837\uff0c\u5728\u7ec4\u95f4\u4ea4\u9519\u751f\u6210\u4ee5\u63d0\u9ad8\u5229\u7528\u7387\uff1b(3) \u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408\u6807\u8bb0\u6761\u4ef6\u5e8f\u5217\u957f\u5ea6\u9884\u6d4b\u4e0e\u4e24\u9636\u6bb5\u8ba1\u5212\uff08\u5168\u5c40\u5206\u7ec4\u901a\u8fc7FPTAS\uff0c\u8fd0\u884c\u65f6\u8865\u5145\u901a\u8fc7SJF\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u91c7\u6837\u7ec4\u76f8\u6bd4\u5168\u7ec4\u89e3\u7801\u51cf\u5c11\u8d85\u8fc750%\u7684\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\uff08\u4f8b\u5982\uff0c\u5728Qwen3-1.7B\u4e0a\u4ece21.55 GB\u964d\u81f310.64 GB\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cInfinite Sampling\u6bd4\u7b80\u5355\u7684\u5fae\u91c7\u6837\u7ec4\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8d85\u8fc725%\u7684\u541e\u5410\u91cf\uff0c\u51cf\u5c11\u4e86\u89e3\u7801\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u957f\u5ea6\u5b8c\u6210\u548c\u5185\u5b58\u4f7f\u7528\u3002\u6df7\u5408\u8c03\u5ea6\u786e\u4fdd\u5728\u5b9e\u9645GPU\u5185\u5b58\u7ea6\u675f\u4e0b\u66f4\u5927\u7ec4\u7684\u9ad8\u6548\u7a33\u5b9aGRPO\u8bad\u7ec3\u3002", "conclusion": "Infinite Sampling\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7ec4\u5927\u5c0f\u4e0eGPU\u5185\u5b58\u4f7f\u7528\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684GRPO\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u6709\u9650\u7684\u786c\u4ef6\u8d44\u6e90\u4e0b\u63d0\u5347\u4e86\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23773", "pdf": "https://arxiv.org/pdf/2506.23773", "abs": "https://arxiv.org/abs/2506.23773", "authors": ["Stefano M. Nicoletti", "Mari\u00eblle Stoelinga"], "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BayesL\uff0c\u4e00\u79cd\u7528\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u65b0\u903b\u8f91\u6846\u67b6\uff0c\u652f\u6301\u67e5\u8be2\u3001\u9a8c\u8bc1\u548c\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u8f7b\u677e\u5904\u7406\u8d1d\u53f6\u65af\u7f51\u7edc\u4e2d\u7684\u56e0\u679c\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u5173\u7cfb\u63a8\u7406\uff0c\u4e5f\u4e0d\u80fd\u65b9\u4fbf\u5730\u8fdb\u884c\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86BayesL\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u6307\u5b9a\u3001\u67e5\u8be2\u548c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u7f51\u7edc\u884c\u4e3a\u7684\u65b0\u578b\u903b\u8f91\u6846\u67b6\u3002", "result": "BayesL\u5141\u8bb8\u521b\u5efa\u5173\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u67e5\u8be2\uff0c\u4fc3\u8fdb\u4e86\u5173\u4e8e\u56e0\u679c\u548c\u57fa\u4e8e\u8bc1\u636e\u5173\u7cfb\u7684\u7075\u6d3b\u63a8\u7406\uff0c\u5e76\u5141\u8bb8\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002", "conclusion": "BayesL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6307\u5b9a\u3001\u67e5\u8be2\u548c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u5bf9\u6a21\u578b\u8fdb\u884c\u624b\u52a8\u4fee\u6539\u5373\u53ef\u8fdb\u884c\u5168\u9762\u7684\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002"}}
{"id": "2506.22984", "pdf": "https://arxiv.org/pdf/2506.22984", "abs": "https://arxiv.org/abs/2506.22984", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u8f66\u8f86\u884c\u4e3a\u751f\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5806\u53e0LSTM\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u68c0\u6d4b\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08CAVs\uff09\u4e2d\u7684\u5f02\u5e38\u9a7e\u9a76\u6a21\u5f0f\u3002\u5806\u53e0LSTM\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4f9d\u8d56\u6027\u548c\u5e8f\u5217\u5f02\u5e38\uff0c\u800c\u968f\u673a\u68ee\u6797\u63d0\u4f9b\u57fa\u4e8e\u96c6\u6210\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u6a21\u578b\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u548c\u68c0\u6d4b\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u65b9\u9762\u5177\u6709\u5f88\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08CAVs\uff09\u53ef\u80fd\u53d7\u5230\u4f20\u611f\u5668\u6545\u969c\u3001\u7f51\u7edc\u653b\u51fb\u548c\u610f\u5916\u73af\u5883\u7834\u574f\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u5728\u8fd9\u4e9b\u8f66\u8f86\u4e2d\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u5bf9\u4e8e\u7ef4\u6301\u5b89\u5168\u53ef\u9760\u7684\u4ea4\u901a\u7f51\u7edc\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u6a21\u62df\u8f66\u8f86\u884c\u4e3a\u751f\u6210\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\n2. \u4f7f\u7528\u5806\u53e0\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5e8f\u5217\u5f02\u5e38\u3002\n3. \u90e8\u7f72\u968f\u673a\u68ee\u6797\u6a21\u578b\u4ee5\u63d0\u4f9b\u57fa\u4e8e\u96c6\u6210\u7684\u9884\u6d4b\uff0c\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "result": "- \u968f\u673a\u68ee\u6797\u6a21\u578b\uff1aR2\u4e3a0.9830\uff0cMAE\u4e3a5.746\uff0c95\u767e\u5206\u4f4d\u5f02\u5e38\u9608\u503c\u4e3a14.18\u3002\n- \u5806\u53e0LSTM\u6a21\u578b\uff1aR2\u4e3a0.9998\uff0cMAE\u4e3a82.425\uff0c95\u767e\u5206\u4f4d\u5f02\u5e38\u9608\u503c\u4e3a265.63\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u548c\u68c0\u6d4b\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.23784", "pdf": "https://arxiv.org/pdf/2506.23784", "abs": "https://arxiv.org/abs/2506.23784", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R\u00fcmmer"], "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "AI": {"tldr": "\u63a2\u7d22\u4e86\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u6c42\u89e3\u8bcd\u65b9\u7a0b\u8fc7\u7a0b\u4e2d\u5bf9\u8bcd\u65b9\u7a0b\u8fdb\u884c\u6392\u5e8f\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u65b9\u6cd5\u548c\u4e09\u79cd\u9002\u5e94\u591a\u5206\u7c7b\u4efb\u52a1\u4ee5\u89e3\u51b3\u65b9\u7a0b\u6392\u5e8f\u95ee\u9898\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u5b57\u7b26\u4e32\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u65b0\u6846\u67b6\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e86\u66f4\u591a\u95ee\u9898\u3002", "motivation": "\u5728\u901a\u8fc7Nielsen\u53d8\u6362\u6c42\u89e3\u8bcd\u65b9\u7a0b\u65f6\uff0c\u65b9\u7a0b\u7684\u5904\u7406\u987a\u5e8f\u4f1a\u663e\u8457\u5f71\u54cd\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u65b9\u7a0b\u7684\u5904\u7406\u987a\u5e8f\uff0c\u4ece\u800c\u63d0\u9ad8\u6c42\u89e3\u6548\u7387\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u8bcd\u65b9\u7a0b\uff0c\u4fdd\u7559\u5168\u5c40\u4fe1\u606f\u3002\n2. \u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u5c06\u591a\u5206\u7c7b\u4efb\u52a1\u9002\u5e94\u4e8e\u65b9\u7a0b\u6392\u5e8f\u95ee\u9898\u3002\n3. \u4f7f\u7528\u8bcd\u65b9\u7a0b\u7684\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6\uff08MUSes\uff09\u6765\u8bad\u7ec3GNN\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6bcf\u4e2a\u53d8\u91cf\u5728\u6bcf\u4e2a\u65b9\u7a0b\u4e2d\u6700\u591a\u51fa\u73b0\u4e00\u6b21\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u6846\u67b6\u6bd4\u6700\u5148\u8fdb\u7684\u5b57\u7b26\u4e32\u6c42\u89e3\u5668\u89e3\u51b3\u4e86\u66f4\u591a\u95ee\u9898\u3002", "conclusion": "\u4f7f\u7528GNNs\u5bf9\u8bcd\u65b9\u7a0b\u8fdb\u884c\u6392\u5e8f\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u7c7b\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u3002"}}
{"id": "2506.23793", "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MAPF-GPT-DDG\uff0c\u4e00\u79cd\u901a\u8fc7\u96c6\u4e2d\u5f0f\u4e13\u5bb6\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3MAPF\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u501f\u52a9\u65b0\u9896\u7684delta-data\u751f\u6210\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684MAPF\u6c42\u89e3\u5668\uff0c\u5305\u62ec\u539f\u59cb\u7684MAPF-GPT\u3002\u5b83\u80fd\u591f\u5904\u7406\u5355\u4e2a\u73af\u5883\u4e2d\u591a\u8fbe1\u767e\u4e07\u4e2a\u4ee3\u7406\u7684MAPF\u5b9e\u4f8b\uff0c\u4e3aMAPF\u9886\u57df\u7684\u53ef\u6269\u5c55\u6027\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\uff08MAPF\uff09\u95ee\u9898\u662f\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u7684\u5e38\u89c1\u62bd\u8c61\uff0c\u89e3\u51b3\u5176\u6700\u4f18\u89e3\u5df2\u88ab\u8bc1\u660e\u662fNP\u96be\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u5b9e\u9645\u5e94\u7528\uff08\u5982\u7269\u6d41\u3001\u641c\u6551\u7b49\uff09\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6c42\u89e3\u5668\u3002\u56e0\u6b64\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u6b21\u4f18MAPF\u6c42\u89e3\u5668\u9010\u6e10\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u57fa\u4e8e\u6700\u8fd1\u5f15\u5165\u7684\u7eaf\u6a21\u4eff\u5b66\u4e60\u6c42\u89e3\u5668MAPF-GPT\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5MAPF-GPT-DDG\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u96c6\u4e2d\u5f0f\u4e13\u5bb6\u6570\u636e\u5bf9\u9884\u8bad\u7ec3\u7684MAPF\u6a21\u578b\u8fdb\u884c\u6709\u6548\u7684\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684delta-data\u751f\u6210\u673a\u5236\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMAPF-GPT-DDG\u5728\u8bb8\u591a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u8d85\u8fc7\u4e86\u6240\u6709\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684MAPF\u6c42\u89e3\u5668\uff0c\u5305\u62ec\u539f\u59cb\u7684MAPF-GPT\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u5904\u7406\u6d89\u53ca\u591a\u8fbe1\u767e\u4e07\u4e2a\u4ee3\u7406\u7684MAPF\u5b9e\u4f8b\u3002", "conclusion": "MAPF-GPT-DDG\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4e3aMAPF\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002\u8fd9\u4e00\u6210\u679c\u8868\u660e\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4e13\u5bb6\u6570\u636e\u548cdelta-data\u751f\u6210\u673a\u5236\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u63d0\u5347MAPF\u6c42\u89e3\u5668\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u5de8\u5927\u3002"}}
{"id": "2506.22995", "pdf": "https://arxiv.org/pdf/2506.22995", "abs": "https://arxiv.org/abs/2506.22995", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov\u00f2", "Marcello Restelli"], "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "AI": {"tldr": "The paper presents a novel RL-based methodology for optimizing microgrid energy management, validated through real-world data from an Italian power grid.", "motivation": "The increasing integration of renewable energy sources (RESs) is transforming traditional power grid networks, requiring new approaches for managing decentralized energy production and consumption.", "method": "A reinforcement learning (RL)-based methodology is proposed. An RL agent learns optimal energy trading and storage policies using historical data on energy production, consumption, and market prices. A digital twin (DT) simulates the energy storage system dynamics, incorporating degradation factors.", "result": "The proposed RL-based strategy outperforms rule-based methods and existing RL benchmarks.", "conclusion": "The RL-based methodology offers a robust solution for intelligent microgrid management."}}
{"id": "2506.23844", "pdf": "https://arxiv.org/pdf/2506.23844", "abs": "https://arxiv.org/abs/2506.23844", "authors": ["Hang Su", "Jun Luo", "Chang Liu", "Xiao Yang", "Yichi Zhang", "Yinpeng Dong", "Jun Zhu"], "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents", "categories": ["cs.AI"], "comment": "18 pages", "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", "AI": {"tldr": "\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5174\u8d77\uff0c\u8fd9\u4e9b\u4ee3\u7406\u80fd\u591f\u5728\u52a8\u6001\u3001\u5f00\u653e\u7684\u73af\u5883\u4e2d\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u80fd\u529b\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002\u672c\u6587\u5206\u6790\u4e86\u4ee3\u7406\u81ea\u4e3b\u6027\u7684\u57fa\u7840\u80fd\u529b\u53ca\u5176\u5bf9\u5e94\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u56de\u987e\u4e86\u6700\u8fd1\u5728\u4e0d\u540c\u81ea\u6cbb\u5c42\u90e8\u7f72\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u63d0\u51fa\u4e86Reflective Risk-Aware Agent Architecture (R2A2)\u7edf\u4e00\u8ba4\u77e5\u6846\u67b6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3bAI\u4ee3\u7406\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u8fd9\u4e9b\u8fdb\u6b65\u5e26\u6765\u4e86\u8bb0\u5fc6\u4e2d\u6bd2\u3001\u5de5\u5177\u8bef\u7528\u7b49\u65b0\u578b\u5b89\u5168\u98ce\u9669\u3002", "method": "\u9996\u5148\u8003\u5bdf\u652f\u6491\u4ee3\u7406\u81ea\u4e3b\u6027\u7684\u7ed3\u6784\u6027\u57fa\u7840\u548c\u5173\u952e\u80fd\u529b\uff0c\u7136\u540e\u5206\u6790\u4ee3\u7406\u5806\u6808\u4e2d\u7684\u76f8\u5e94\u5b89\u5168\u6f0f\u6d1e\uff0c\u6700\u540e\u7cfb\u7edf\u6027\u56de\u987e\u4e86\u5728\u4e0d\u540c\u81ea\u6cbb\u5c42\u90e8\u7f72\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u63d0\u51faR2A2\u6846\u67b6\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u6027\u3002", "result": "\u8bc6\u522b\u51fa\u4ee3\u7406\u81ea\u4e3b\u6027\u76f8\u5173\u7684\u80fd\u529b\u548c\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u901a\u8fc7R2A2\u6846\u67b6\u5b9e\u73b0\u4e86\u539f\u5219\u6027\u548c\u4e3b\u52a8\u6027\u5b89\u5168\u63aa\u65bd\u3002", "conclusion": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u4e3bAI\u4ee3\u7406\u5e26\u6765\u7684\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u4ece\u67b6\u6784\u4e0a\u6539\u8fdb\uff0c\u5e76\u91c7\u7528\u5982R2A2\u8fd9\u6837\u7684\u7edf\u4e00\u8ba4\u77e5\u6846\u67b6\u6765\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23024", "pdf": "https://arxiv.org/pdf/2506.23024", "abs": "https://arxiv.org/abs/2506.23024", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R\u00e9"], "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165Barycentric Weight Layer (BWLer)\u6765\u63d0\u9ad8\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b(PDEs)\u7684\u7cbe\u5ea6\uff0c\u63ed\u793a\u4e86MLP\u67b6\u6784\u4e2d\u7684\u57fa\u672c\u7cbe\u5ea6\u9650\u5236\uff0c\u5e76\u5c55\u793a\u4e86BWLer\u5728\u591a\u79cdPDE\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1PINNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u7528\u673a\u5668\u5b66\u4e60\u89e3\u51b3PDEs\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u8fdc\u672a\u8fbe\u5230\u8bb8\u591a\u79d1\u5b66\u4efb\u52a1\u6240\u9700\u7684\u673a\u5668\u7cbe\u5ea6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u7cbe\u5ea6\u9650\u5236\u662f\u5426\u6e90\u4e8ePDEs\u7684\u75c5\u6001\u6027\u6216\u5178\u578b\u7684\u591a\u5c42\u611f\u77e5\u673a(MLP)\u67b6\u6784\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Barycentric Weight Layer (BWLer)\uff0c\u5b83\u901a\u8fc7\u91cd\u5fc3\u591a\u9879\u5f0f\u63d2\u503c\u6765\u5efa\u6a21PDE\u89e3\u3002BWLer\u53ef\u4ee5\u9644\u52a0\u5728\u73b0\u6709\u7684MLP\u4e4b\u4e0a\uff08BWLer-hat\uff09\u6216\u5b8c\u5168\u66ff\u4ee3MLP\uff08\u663e\u5f0fBWLer\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u89e3\u7684\u8868\u793a\u4e0ePDE\u635f\u5931\u7684\u5bfc\u6570\u8ba1\u7b97\u5206\u79bb\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u5bf9MLP\u8fdb\u884c\u4e86\u57fa\u672c\u7cbe\u5ea6\u9650\u5236\u7684\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u8bef\u5dee\u5206\u89e3\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9884\u5904\u7406\u6280\u672f\u6765\u4f18\u5316\u7ebf\u6027PDE\u7684\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6PDE\u4e0a\uff0c\u6dfb\u52a0BWLer\u53ef\u4f7fMLP\u7684RMSE\u5927\u5e45\u6539\u5584\uff1a\u5bf9\u6d41\u95ee\u9898\u63d0\u534730\u500d\uff0c\u53cd\u5e94\u95ee\u9898\u63d0\u534710\u500d\uff0c\u6ce2\u52a8\u65b9\u7a0b\u63d0\u53471800\u500d\u3002\u800c\u5b8c\u5168\u66ff\u6362MLP\u7684\u663e\u5f0fBWLer\uff0c\u5728\u5bf9\u6d41\u3001\u53cd\u5e94\u548c\u6ce2\u52a8\u95ee\u9898\u4e2d\u63a5\u8fd1\u673a\u5668\u7cbe\u5ea6\uff08\u6bd4\u5148\u524d\u7ed3\u679c\u597d\u8fbe10\u4ebf\u500d\uff09\uff0c\u5e76\u5728Burgers'\u548c\u4e0d\u89c4\u5219\u51e0\u4f55Poisson\u95ee\u9898\u4e2d\u5339\u914d\u6807\u51c6PINNs\u7684\u6027\u80fd\u3002", "conclusion": "BWLer\u4e3a\u7ed3\u5408PINNs\u7684\u7075\u6d3b\u6027\u548c\u7ecf\u5178\u8c31\u89e3\u7b97\u5668\u7684\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u9645\u8def\u5f84\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8PDE\u6c42\u89e3\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2506.23908", "pdf": "https://arxiv.org/pdf/2506.23908", "abs": "https://arxiv.org/abs/2506.23908", "authors": ["Andr\u00e1s Gy\u00f6rgy", "Tor Lattimore", "Nevena Lazi\u0107", "Csaba Szepesv\u00e1ri"], "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "AI": {"tldr": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5c3d\u7ba1\u5728\u6570\u5b66\u548c\u79d1\u5b66\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u4ecd\u7136\u5728\u7b80\u5355\u7684\u6f14\u7ece\u63a8\u7406\u95ee\u9898\u4e0a\u9891\u7e41\u5931\u8d25\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u5b9e\u73b0\u53ef\u9760\u4e14\u6b63\u786e\u7684\u6f14\u7ece\u63a8\u7406\uff0cAI\u7814\u7a76\u9700\u8981\u4ece\u4f18\u5316\u7edf\u8ba1\u6027\u80fd\u8f6c\u5411\u66f4\u96c4\u5fc3\u52c3\u52c3\u7684\u786e\u5207\u5b66\u4e60\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u8981\u6c42\u5728\u6240\u6709\u8f93\u5165\u4e0a\u90fd\u6b63\u786e\u3002", "motivation": "\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684AI\u7cfb\u7edf\u4e5f\u5728\u5bb9\u6613\u89e3\u51b3\u7684\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e0a\u7ecf\u5e38\u5931\u8d25\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u65e0\u6cd5\u5b9e\u73b0\u5177\u5907\u53ef\u9760\u6f14\u7ece\u63a8\u7406\u80fd\u529b\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5e94\u4ece\u6839\u672c\u4e0a\u8f6c\u53d8\u601d\u7ef4\u65b9\u5f0f\uff0c\u4ece\u9488\u5bf9\u63a8\u7406\u95ee\u9898\u548c\u7b97\u6cd5\u4efb\u52a1\u7684\u5206\u5e03\u4f18\u5316\u7edf\u8ba1\u6027\u80fd\uff0c\u8f6c\u800c\u91c7\u7528\u786e\u5207\u5b66\u4e60\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u8981\u6c42\u7b97\u6cd5\u5728\u6240\u6709\u8f93\u5165\u4e0a\u90fd\u80fd\u4fdd\u8bc1\u6b63\u786e\u6027\u3002", "result": "\u901a\u8fc7\u91c7\u7528\u786e\u5207\u5b66\u4e60\u8303\u5f0f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u4e14\u6b63\u786e\u7684\u6f14\u7ece\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u5411\u901a\u7528\u667a\u80fd\u8fc8\u8fdb\u3002", "conclusion": "\u786e\u5207\u5b66\u4e60\u8303\u5f0f\u662f\u5b9e\u73b0\u53ef\u9760\u6f14\u7ece\u63a8\u7406\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e5f\u662f\u53ef\u884c\u7684\u76ee\u6807\uff0c\u5e94\u8be5\u6307\u5bfc\u672a\u6765\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2506.23025", "pdf": "https://arxiv.org/pdf/2506.23025", "abs": "https://arxiv.org/abs/2506.23025", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u7684\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\uff0c\u5e76\u5f15\u5165\u4e86Spectra-1.1\u6a21\u578b\u5957\u4ef6\u548c\u52a0\u901f\u63a8\u7406\u7684TriRun GPU\u5185\u6838\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u5176\u63a8\u7406\u6548\u7387\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u73b0\u4ee3GPU\u67b6\u6784\u7684\u8ba1\u7b97\u80fd\u529b\u6301\u7eed\u63d0\u9ad8\uff0c\u4f46\u5176\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u6ca1\u6709\u6309\u6bd4\u4f8b\u6269\u5c55\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u5173\u952e\u74f6\u9888\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5206\u6790\u4e86\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u7f29\u653e\u5b9a\u5f8b\u5206\u6790\uff0c\u53d1\u73b0TriLMs\u4ece\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u4e2d\u53d7\u76ca\u66f4\u591a\uff0c\u800c\u4e0d\u662f\u4ece\u6a21\u578b\u53c2\u6570\u6269\u5c55\u4e2d\u53d7\u76ca\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86Spectra-1.1\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u7684TriLMs\u5957\u4ef6\uff0c\u4f7f\u7528\u591a\u8fbe1.2\u4e07\u4ebf\u4e2a\u6807\u8bb0\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u65b0\u76842\u4f4d\u548c1.6\u4f4d\u4e09\u5143\u6743\u91cd\u6253\u5305\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u5728\u5404\u79cdCPU\u67b6\u6784\u4e0a\u5c55\u793a\u4e86\u52a0\u901f\u63a8\u7406\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0c\u57fa\u4e8e2\u4f4d\u6253\u5305\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aTriRun\u7684GPU\u5185\u6838\uff0c\u4e0e\u6d6e\u70b9\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u5185\u6838\u53ef\u4ee5\u5c06\u7aef\u5230\u7aef\u6a21\u578b\u63a8\u7406\u52a0\u901f\u9ad8\u8fbe5\u500d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSpectra-1.1\u6a21\u578b\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002\u800cTriRun GPU\u5185\u6838\u5219\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u6700\u591a\u53ef\u8fbe\u6d6e\u70b9\u57fa\u7ebf\u76845\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u548c\u90e8\u7f72\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2506.23924", "pdf": "https://arxiv.org/pdf/2506.23924", "abs": "https://arxiv.org/abs/2506.23924", "authors": ["Akshit Kumar", "Tianyi Peng", "Yuhang Wu", "Assaf Zeevi"], "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", "AI": {"tldr": "\u5c3d\u7ba1\u9700\u8981\u8fdb\u4e00\u6b65\u52aa\u529b\u6765\u5b9e\u73b0\u73b0\u5b9e\u4e2d\u7684\u968f\u673a\u5efa\u6a21\u81ea\u52a8\u5316\uff0c\u4f46\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bfe\u5802\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u4e13\u4e1a\u80fd\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u6784\u5efa\u8f85\u52a9\u8fd0\u7b79\u5b66\uff08OR\uff09\u7814\u7a76\u4eba\u5458\u5e76\u589e\u5f3a\u5176\u73b0\u5b9e\u4e16\u754c\u5f71\u54cd\u7684AI\u4ee3\u7406\u4eba\u7684\u6f5c\u529b\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u8fd0\u7b79\u5b66\uff08OR\uff09\u95ee\u9898\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u548c\u4f18\u5316\u7684\u968f\u673a\u5efa\u6a21\u95ee\u9898\u3002\u8fd9\u662f\u7531\u4e8e\u76ee\u524d\u5bf9\u4e8eLLMs\u5728\u8fd0\u7b79\u5b66\u9886\u57df\u7684\u5e94\u7528\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u6536\u96c6\u7814\u7a76\u751f\u7ea7\u522b\u7684\u4f5c\u4e1a\u548c\u535a\u58eb\u8d44\u683c\u8003\u8bd5\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5f00\u6e90\u5e93SimOpt\u6d4b\u8bd5LLMs\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u80fd\u529b\u3002\u901a\u8fc7\u8fd9\u4e9b\u95ee\u9898\u548c\u5de5\u5177\uff0c\u7814\u7a76LLMs\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u8fdb\u884c\u73b0\u5b9e\u51b3\u7b56\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u8981\u5b9e\u73b0\u968f\u673a\u5efa\u6a21\u7ba1\u9053\u7684\u53ef\u9760\u81ea\u52a8\u5316\u4ecd\u9700\u5927\u91cf\u5de5\u4f5c\uff0c\u4f46\u6700\u5148\u8fdb\u7684LLMs\u5728\u8bfe\u5802\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u4e13\u4e1a\u6c34\u5e73\u3002", "conclusion": "LLMs\u6709\u6f5c\u529b\u534f\u52a9\u8fd0\u7b79\u5b66\u7814\u7a76\u4eba\u5458\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u589e\u5f3a\u8fd0\u7b79\u5b66\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f71\u54cd\u529b\u3002\u7136\u800c\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u968f\u673a\u5efa\u6a21\u8fc7\u7a0b\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2506.23926", "pdf": "https://arxiv.org/pdf/2506.23926", "abs": "https://arxiv.org/abs/2506.23926", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u5de5\u4e1a\u94fe\u5f39\u6027\u9884\u6d4b\u548c\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5de5\u4e1a\u5927\u8111\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u7b26\u53f7\u63a8\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u76f4\u63a5\u8fdb\u884c\u5f39\u6027\u89c4\u5212\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u5de5\u4e1a\u94fe\u7684\u5f39\u6027\u975e\u5e73\u8861\u6d4b\u91cf\u5bf9\u4e8e\u79d1\u5b66\u7ba1\u7406\u548c\u5de5\u7a0b\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u591a\u53d8\u7684\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5de5\u4e1a\u5927\u8111\uff08industrial brain\uff09\u7684\u4eba\u7c7b\u81ea\u4e3b\u8ba4\u77e5\u51b3\u7b56\u548c\u89c4\u5212\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9ad8\u9636\u6d3b\u52a8\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u548cCT-OODA\u7b26\u53f7\u63a8\u7406\u6280\u672f\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u5168\u5c40\u53d8\u91cf\u89c2\u6d4b\u6570\u636e\u4e2d\u81ea\u4e3b\u89c4\u5212\u5f39\u6027\u3002\u6b64\u6846\u67b6\u65e0\u9700\u7b80\u5316\u5047\u8bbe\u5373\u53ef\u7406\u89e3\u5e76\u5efa\u6a21\u8282\u70b9\u6d3b\u52a8\u52a8\u6001\u7ed3\u6784\u548c\u7f51\u7edc\u5171\u6f14\u5316\u62d3\u6251\uff0c\u5e76\u63ed\u793a\u590d\u6742\u7f51\u7edc\u80cc\u540e\u7684\u6f5c\u5728\u89c4\u5f8b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5de5\u4e1a\u5927\u8111\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f39\u6027\u9884\u6d4b\u548c\u89c4\u5212\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8eGoT\u548cOlaGPT\u6846\u67b6\u51c6\u786e\u7387\u63d0\u9ad8\u4e8610.8%\uff0c\u76f8\u8f83\u4e8e\u8c31\u7ef4\u6570\u7ea6\u7b80\u65b9\u6cd5\u63d0\u9ad8\u4e8611.03%\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u62d3\u6251\u548c\u52a8\u529b\u5b66\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u89c2\u5bdf\u5e72\u6270\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5de5\u4e1a\u5927\u8111\u586b\u8865\u4e86\u5de5\u4e1a\u94fe\u5f39\u6027\u9884\u6d4b\u4e0e\u89c4\u5212\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23036", "pdf": "https://arxiv.org/pdf/2506.23036", "abs": "https://arxiv.org/abs/2506.23036", "authors": ["Zain ul Abdeen", "Ming Jin"], "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u5206\u6790\u7f51\u7edc\u53c2\u6570\u5728\u5185\u5916\u538b\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u7a81\u89e6\u53ef\u5851\u6027\u7684\u542f\u53d1\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u65b9\u6cd5\uff1a\u901a\u8fc7\u9009\u62e9\u6027\u6270\u52a8\u53c2\u6570\u5f15\u5165\u5185\u90e8\u538b\u529b\uff0c\u901a\u8fc7\u4fee\u6539\u4ee3\u7406\u89c2\u5bdf\u8fdb\u884c\u5bf9\u6297\u6027\u653b\u51fb\u4ee5\u65bd\u52a0\u5916\u90e8\u538b\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5c06\u53c2\u6570\u5206\u7c7b\u4e3a\u8106\u5f31\u3001\u9c81\u68d2\u6216\u53cd\u8106\u5f31\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u5f97\u5206\u6765\u91cf\u5316\u8fd9\u4e9b\u7279\u6027\u3002\u5b9e\u9a8c\u5728Mujoco\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u4f7f\u7528PPO\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u7ed3\u679c\u8868\u660e\u5b58\u5728\u589e\u5f3a\u7b56\u7565\u5728\u538b\u529b\u4e0b\u8868\u73b0\u7684\u53cd\u8106\u5f31\u53c2\u6570\uff0c\u5c55\u793a\u4e86\u6709\u9488\u5bf9\u6027\u7684\u8fc7\u6ee4\u6280\u672f\u6539\u5584RL\u7b56\u7565\u9002\u5e94\u6027\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u9c81\u68d2\u548c\u53cd\u8106\u5f31\u7684RL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u4ee5\u5e94\u5bf9\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u7b56\u7565\u9c81\u68d2\u6027\u7684\u7406\u89e3\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u5185\u90e8\u53c2\u6570\u6270\u52a8\u548c\u5916\u90e8\u5bf9\u6297\u653b\u51fb\u65f6\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u7cfb\u7edf\u8bc4\u4f30\u548c\u63d0\u9ad8RL\u7b56\u7565\u7684\u9c81\u68d2\u6027\u6210\u4e3a\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8bfe\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a1) \u7a81\u89e6\u8fc7\u6ee4 - \u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u7a81\u89e6\u53ef\u5851\u6027\u7684\u542f\u53d1\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6270\u52a8\u7f51\u7edc\u53c2\u6570\u5f15\u5165\u5185\u90e8\u538b\u529b\uff1b2) \u5bf9\u6297\u653b\u51fb - \u901a\u8fc7\u5bf9\u4ee3\u7406\u89c2\u5bdf\u8fdb\u884c\u4fee\u6539\u65bd\u52a0\u5916\u90e8\u538b\u529b\u3002\u57fa\u4e8e\u8fd9\u4e24\u79cd\u538b\u529b\u6e90\uff0c\u8bba\u6587\u5b9a\u4e49\u4e86\u53c2\u6570\u5f97\u5206\u6765\u8861\u91cf\u53c2\u6570\u5728\u5e72\u51c0\u73af\u5883\u548c\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u5f71\u54cd\uff0c\u5e76\u636e\u6b64\u5c06\u53c2\u6570\u5206\u4e3a\u8106\u5f31\u3001\u9c81\u68d2\u548c\u53cd\u8106\u5f31\u4e09\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728Mujoco\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u5b58\u5728\u4e00\u4e9b\u53cd\u8106\u5f31\u53c2\u6570\uff0c\u5b83\u4eec\u5728\u538b\u529b\u4e0b\u80fd\u591f\u63d0\u5347\u7b56\u7565\u7684\u8868\u73b0\u3002\u8fd9\u8bf4\u660e\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u8fc7\u6ee4\u6280\u672f\u53ef\u4ee5\u6539\u5584RL\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u7f51\u7edc\u53c2\u6570\u5728\u5185\u5916\u538b\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u51fa\u53cd\u8106\u5f31\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u6709\u52a9\u4e8e\u63d0\u5347RL\u7b56\u7565\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u66f4\u52a0\u9c81\u68d2\u548c\u53cd\u8106\u5f31\u7684RL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2506.23949", "pdf": "https://arxiv.org/pdf/2506.23949", "abs": "https://arxiv.org/abs/2506.23949", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "categories": ["cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "AI": {"tldr": "\u65e5\u76ca\u591a\u7528\u9014\u7684AI\u6a21\u578b\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u5176\u4ed6\u901a\u7528AI\u6a21\u578b\uff09\u5728\u63d0\u4f9b\u591a\u79cd\u6709\u76ca\u529f\u80fd\u7684\u540c\u65f6\uff0c\u4e5f\u4f34\u968f\u7740\u4e25\u91cd\u7684\u4e0d\u826f\u4e8b\u4ef6\u98ce\u9669\u3002\u672c\u6587\u6863\u4e3a\u8bc6\u522b\u3001\u5206\u6790\u548c\u7f13\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u98ce\u9669\u63d0\u4f9b\u4e86\u7ba1\u7406\u5b9e\u8df5\u548c\u63a7\u5236\u63aa\u65bd\uff0c\u4e3b\u8981\u9762\u5411\u5927\u578b\u3001\u5c16\u7aef\u7684\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u8005\uff0c\u5e76\u4e14\u5bf9\u6784\u5efa\u5728\u5176\u4e0a\u7684\u5e94\u7528\u5f00\u53d1\u8005\u4e5f\u6709\u6307\u5bfc\u610f\u4e49\u3002\u672c\u6587\u6863\u7ed3\u5408\u4e86NIST AI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u548cISO/IEC 23894\u7b49\u6807\u51c6\uff0c\u4e13\u6ce8\u4e8e\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u8005\u9762\u4e34\u7684\u72ec\u7279\u95ee\u9898\u3002", "motivation": "\u63cf\u8ff0\u65e5\u76ca\u591a\u7528\u9014\u7684AI\u6a21\u578b\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u901a\u7528AI\u6a21\u578b\uff09\u6240\u5e26\u6765\u7684\u98ce\u9669\u548c\u76ca\u5904\uff0c\u5f3a\u8c03\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u7ba1\u7406\u548c\u7f13\u89e3\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5e26\u6765\u7684\u4e25\u91cd\u4e0d\u826f\u540e\u679c\u3002", "method": "\u901a\u8fc7\u5236\u5b9a\u98ce\u9669\u7ba1\u7406\u548c\u63a7\u5236\u63aa\u65bd\u6587\u6863\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522b\u3001\u5206\u6790\u548c\u7f13\u89e3\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4e0e\u73b0\u6709AI\u98ce\u9669\u7ba1\u7406\u6807\u51c6\uff08\u5982NIST\u548cISO/IEC 23894\uff09\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u9488\u5bf9\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u7684\u72ec\u7279\u95ee\u9898\u7684\u5177\u4f53\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u4e3a\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u98ce\u9669\u7ba1\u7406\u6307\u5357\uff0c\u4fc3\u8fdb\u7b26\u5408\u76f8\u5173\u6807\u51c6\uff0c\u5e76\u5e2e\u52a9\u4e0b\u6e38\u5f00\u53d1\u8005\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5e94\u7528\u7a0b\u5e8f\u3002", "conclusion": "\u672c\u6587\u6863\u4e3a\u901a\u7528AI/\u57fa\u7840\u6a21\u578b\u7684\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u7ed3\u5408\u73b0\u6709\u6807\u51c6\u5e76\u89e3\u51b3\u7279\u5b9a\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u6280\u672f\u7684\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23041", "pdf": "https://arxiv.org/pdf/2506.23041", "abs": "https://arxiv.org/abs/2506.23041", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff08\u5982Vision Transformers, ViTs\uff09\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u4ee5\u6539\u8fdb\u5c0f\u89c4\u6a21\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u6548\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u53ca\u9488\u5bf9\u5c0f\u578b\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u4e0b\u6e38\u6570\u636e\u96c6\u7684MLP\u5757\u91cd\u65b0\u52a0\u6743\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ece\u800c\u8ba9\u5c0f\u89c4\u6a21\u5b66\u751f\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5728\u4f7f\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u65f6\uff0c\u5f53\u6e90\u6a21\u578b\u975e\u5e38\u5f3a\u5927\u4e14\u57fa\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u9884\u8bad\u7ec3\u65f6\uff0c\u77e5\u8bc6\u8f6c\u79fb\u7684\u6548\u679c\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728Vision Transformers (ViTs)\u4e2d\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u4ee5\u6539\u5584\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a1. \u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u91c7\u7528\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\uff08mutual information-aware optimization\uff09\uff0c\u4ee5\u63d0\u9ad8\u77e5\u8bc6\u84b8\u998f\u7684\u6709\u6548\u6027\uff1b2. \u9488\u5bf9\u5c0f\u578b\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u4e0b\u6e38\u6570\u636e\u96c6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u2014\u2014\u91cd\u65b0\u52a0\u6743MLP\u5757\uff0c\u56e0\u4e3a\u89c2\u5bdf\u5230\u9876\u90e8MLP\u5757\u4e3b\u8981\u8d1f\u8d23\u4e92\u4fe1\u606f\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u5c0f\u89c4\u6a21\u5b66\u751f\u6a21\u578b\u80fd\u591f\u4ece\u6700\u5f3a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u53d7\u76ca\uff0c\u63d0\u9ad8\u4e86\u5176\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u5c24\u5176\u5728\u5c0f\u578b\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u91cd\u65b0\u52a0\u6743MLP\u5757\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u548cMLP\u5757\u91cd\u65b0\u52a0\u6743\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4ece\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5c0f\u89c4\u6a21\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u4e3a\u672a\u6765\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5229\u7528\u5f3a\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2506.23992", "pdf": "https://arxiv.org/pdf/2506.23992", "abs": "https://arxiv.org/abs/2506.23992", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "categories": ["cs.AI", "cs.ET"], "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "AI": {"tldr": "The research proposes an AI-based framework to process refugee health data focusing on child mental health. It compares two RAG models (Zephyr-7B-beta and DeepSeek R1-7B), finding that DeepSeek R1 is superior with an accuracy of 0.91 in answer relevance.", "motivation": "To address the psychological trauma experienced by displaced children due to the international refugee crisis, and to provide a scalable strategy to assist policymakers, mental health practitioners, and humanitarian agencies.", "method": "Comparison of two Retrieval-Augmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, for processing unstructured refugee health data while avoiding hallucination hazards.", "result": "Both models worked properly, but DeepSeek R1 showed significantly better performance with an accuracy of 0.91 in answer relevance.", "conclusion": "An AI-based framework combining advanced AI methods with migration research and child psychology can effectively assist in recognizing and improving the mental wellbeing of displaced children."}}
{"id": "2506.23053", "pdf": "https://arxiv.org/pdf/2506.23053", "abs": "https://arxiv.org/abs/2506.23053", "authors": ["Hanlin Dong", "Arian Prabowo", "Hao Xue", "Flora D. Salim"], "title": "Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Air quality prediction is a challenging forecasting task due to its\nspatio-temporal complexity and the inherent dynamics as well as uncertainty.\nMost of the current models handle these two challenges by applying Graph Neural\nNetworks or known physics principles, and quantifying stochasticity through\nprobabilistic networks like Diffusion models. Nevertheless, finding the right\nbalancing point between the certainties and uncertainties remains an open\nquestion. Therefore, we propose Double-Diffusion, a novel diffusion\nprobabilistic model that harnesses the power of known physics to guide air\nquality forecasting with stochasticity. To the best of our knowledge, while\nprecedents have been made of using conditional diffusion models to predict air\npollution, this is the first attempt to use physics as a conditional generative\napproach for air quality prediction. Along with a sampling strategy adopted\nfrom image restoration and a new denoiser architecture, Double-Diffusion ranks\nfirst in most evaluation scenarios across two real-life datasets compared with\nother probabilistic models, it also cuts inference time by 50% to 30% while\nenjoying an increase between 3-12% in Continuous Ranked Probabilistic Score\n(CRPS).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6982\u7387\u6a21\u578bDouble-Diffusion\uff0c\u7ed3\u5408\u5df2\u77e5\u7269\u7406\u539f\u7406\u5f15\u5bfc\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u5176\u4ed6\u6982\u7387\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u5927\u591a\u6570\u8bc4\u4f30\u573a\u666f\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8650%-30%\uff0c\u540c\u65f6CRPS\u63d0\u9ad8\u4e863%-12%\u3002", "motivation": "\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u56e0\u5176\u65f6\u7a7a\u590d\u6742\u6027\u548c\u56fa\u6709\u7684\u52a8\u6001\u53ca\u4e0d\u786e\u5b9a\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u5f53\u524d\u6a21\u578b\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u6216\u5df2\u77e5\u7269\u7406\u539f\u7406\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u7f51\u7edc\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u91cf\u5316\u968f\u673a\u6027\u3002\u7136\u800c\uff0c\u5728\u786e\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u627e\u5230\u6b63\u786e\u7684\u5e73\u8861\u70b9\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Double-Diffusion\u6a21\u578b\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u5229\u7528\u5df2\u77e5\u7269\u7406\u539f\u7406\u6307\u5bfc\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u4e2d\u7684\u968f\u673a\u6027\u3002\u91c7\u7528\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u91c7\u6837\u7b56\u7565\u548c\u65b0\u7684\u53bb\u566a\u5668\u67b6\u6784\u3002\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528\u7269\u7406\u4f5c\u4e3a\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u5176\u4ed6\u6982\u7387\u6a21\u578b\u76f8\u6bd4\uff0cDouble-Diffusion\u5728\u5927\u591a\u6570\u8bc4\u4f30\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u81f3\u539f\u6765\u768430%-50%\uff0c\u5e76\u4e14CRPS\u63d0\u9ad8\u4e863%-12%\u3002", "conclusion": "Double-Diffusion\u6a21\u578b\u5728\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u5e76\u7f29\u77ed\u4e86\u63a8\u7406\u65f6\u95f4\uff0c\u4e3a\u7ed3\u5408\u7269\u7406\u539f\u7406\u7684\u6982\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.24026", "pdf": "https://arxiv.org/pdf/2506.24026", "abs": "https://arxiv.org/abs/2506.24026", "authors": ["Yongyi Wang", "Wenxin Li"], "title": "Constructing Non-Markovian Decision Process via History Aggregator", "categories": ["cs.AI"], "comment": null, "summary": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08NMDP\uff09\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u4e86\u5386\u53f2\u805a\u5408\u5668\u72b6\u6001\uff08HAS\uff09\u4ee5\u7cbe\u786e\u63a7\u5236\u51b3\u7b56\u95ee\u9898\u7684\u65f6\u95f4\u5e8f\u5217\u4f9d\u8d56\u7ed3\u6784\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u4e25\u683c\u3001\u7075\u6d3b\u5730\u8bc4\u4f30\u51b3\u7b56\u7b97\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5728\u5168\u9762\u8bc4\u4f30\u51b3\u7b56\u7b97\u6cd5\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u6001\u7684\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8303\u7574\u8bba\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08NMDP\uff09\u7684\u8303\u7574\uff0c\u5e76\u8bc1\u660e\u5176\u7b49\u4ef7\u5173\u7cfb\uff1b\u5f15\u5165\u5386\u53f2\u805a\u5408\u5668\u72b6\u6001\uff08HAS\uff09\uff0c\u4ee5\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u7cbe\u786e\u63a7\u5236\u51b3\u7b56\u95ee\u9898\u7684\u72b6\u6001\u4f9d\u8d56\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8868\u793a\u5e7f\u6cdb\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u6001\uff0c\u4fc3\u8fdb\u5bf9\u51b3\u7b56\u7b97\u6cd5\u66f4\u4e25\u683c\u548c\u7075\u6d3b\u7684\u8bc4\u4f30\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7406\u89e3\u548c\u89e3\u51b3\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e86\u51b3\u7b56\u7b97\u6cd5\u7684\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2506.23055", "pdf": "https://arxiv.org/pdf/2506.23055", "abs": "https://arxiv.org/abs/2506.23055", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "AI": {"tldr": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4ee5\u53ef\u6d4b\u91cf\u7684\u7cbe\u5ea6\u8fd1\u4f3c\u4eba\u7c7b\u5fc3\u7406\u7ed3\u6784\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982ChatGPT\u5728\u751f\u6210\u7c7b\u4eba\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u5bf9\u5851\u9020\u4eba\u7c7b\u601d\u7ef4\u548c\u884c\u4e3a\u7684\u6982\u5ff5\u5185\u90e8\u5316\u7684\u51c6\u786e\u6027\u5982\u4f55\u3002", "method": "\u5229\u752843\u4e2a\u6807\u51c6\u5316\u5fc3\u7406\u5b66\u95ee\u5377\uff0c\u901a\u8fc7\u6210\u5bf9\u76f8\u4f3c\u6027\u5206\u6790\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u91cd\u5efa\u548c\u5206\u7c7b\u95ee\u5377\u9879\u76ee\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4f7f\u7528\u5c42\u6b21\u805a\u7c7b\u65b9\u6cd5\u5c06\u7ed3\u679c\u805a\u7c7b\u7ed3\u6784\u4e0e\u539f\u59cb\u7c7b\u522b\u6807\u7b7e\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4\u6a21\u578b\u5728\u6982\u5ff5\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e8666.2%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\uff0855.9%\uff09\u548cBERT\uff0848.1%\uff09\uff0c\u4e14\u5176\u4f30\u8ba1\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u4eba\u7c7b\u5728\u591a\u4e2a\u5fc3\u7406\u5b66\u95ee\u5377\u4e2d\u7684\u54cd\u5e94\u5177\u6709\u76f8\u5173\u6027\u3002", "conclusion": "\u73b0\u4ee3LLMs\u53ef\u4ee5\u4ee5\u53ef\u6d4b\u91cf\u7684\u7cbe\u5ea6\u8fd1\u4f3c\u4eba\u7c7b\u5fc3\u7406\u7ed3\u6784\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u7c7b-LLM\u6982\u5ff5\u5bf9\u9f50\u5e76\u8bc6\u522b\u6f5c\u5728\u8868\u793a\u504f\u5dee\u3002"}}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119", "abs": "https://arxiv.org/abs/2506.24119", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "AI": {"tldr": "\u8fd1\u671f\u5f3a\u5316\u5b66\u4e60\u7684\u8fdb\u6b65\u8868\u660e\uff0c\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5728\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u53d1\u5c55\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u7b56\u5212\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u548c\u9886\u57df\u7279\u5b9a\u7684\u5956\u52b1\u5de5\u7a0b\u3002\u6211\u4eec\u5f15\u5165\u4e86SPIRAL\uff0c\u4e00\u79cd\u81ea\u6211\u6e38\u620f\u6846\u67b6\uff0c\u6a21\u578b\u901a\u8fc7\u4e0e\u4e0d\u65ad\u6539\u8fdb\u7684\u81ea\u8eab\u7248\u672c\u8fdb\u884c\u591a\u56de\u5408\u3001\u96f6\u548c\u6e38\u620f\u6765\u81ea\u6211\u5b66\u4e60\uff0c\u6d88\u9664\u4e86\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u9700\u6c42\u3002\u901a\u8fc7\u81ea\u6211\u6e38\u620f\uff0cSPIRAL\u751f\u6210\u4e86\u4e00\u4e2a\u65e0\u9650\u7684\u8bfe\u7a0b\uff0c\u5305\u542b\u9010\u6e10\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u6a21\u578b\u5fc5\u987b\u4e0d\u65ad\u9002\u5e94\u66f4\u5f3a\u7684\u5bf9\u624b\u3002\u4e3a\u4e86\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u81ea\u6211\u6e38\u620f\u8bad\u7ec3\uff0c\u6211\u4eec\u4e3aLLMs\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b8c\u5168\u5728\u7ebf\u3001\u591a\u56de\u5408\u3001\u591a\u4ee3\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u89d2\u8272\u6761\u4ef6\u4f18\u52bf\u4f30\u8ba1\uff08RAE\uff09\u4ee5\u7a33\u5b9a\u591a\u4ee3\u7406\u8bad\u7ec3\u3002\u4f7f\u7528SPIRAL\uff0c\u4ec5\u5728Kuhn Poker\u4e0a\u7684\u81ea\u6211\u6e38\u620f\u8bad\u7ec3\u5c31\u80fd\u4f7fQwen3-4B-Base\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u65b9\u9762\u5206\u522b\u63d0\u9ad88.6%\u548c8.4%\uff0c\u8d85\u8d8a\u4e8625,000\u4e2a\u4e13\u5bb6\u6e38\u620f\u8f68\u8ff9\u7684SFT\u8868\u73b0\u3002\u5206\u6790\u663e\u793a\uff0c\u8fd9\u79cd\u8fc1\u79fb\u662f\u901a\u8fc7\u4e09\u79cd\u8ba4\u77e5\u6a21\u5f0f\u5b9e\u73b0\u7684\uff1a\u7cfb\u7edf\u5206\u89e3\u3001\u671f\u671b\u503c\u8ba1\u7b97\u548c\u9010\u6848\u5206\u6790\u3002\u591a\u6e38\u620f\u8bad\u7ec3\uff08\u4e95\u5b57\u68cb\u3001Kuhn Poker\u3001\u7b80\u5355\u8c08\u5224\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u6e38\u620f\u90fd\u53d1\u5c55\u51fa\u72ec\u7279\u7684\u63a8\u7406\u4f18\u52bf\u3002\u5c06SPIRAL\u5e94\u7528\u4e8e\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\uff08DeepSeek-R1-Distill-Qwen-7B\uff09\u4ecd\u53ef\u5e26\u67652.0%\u7684\u5e73\u5747\u63d0\u5347\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u96f6\u548c\u6e38\u620f\u81ea\u7136\u53d1\u5c55\u51fa\u53ef\u8f6c\u79fb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u81ea\u4e3b\u63a8\u7406\u53d1\u5c55\u7684\u6709\u524d\u9014\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u7b56\u5212\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u548c\u9886\u57df\u7279\u5b9a\u7684\u5956\u52b1\u5de5\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u4e0d\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u6e38\u620f\u4e0d\u65ad\u63d0\u5347\u81ea\u8eab\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPIRAL\u7684\u81ea\u6211\u6e38\u620f\u6846\u67b6\uff0c\u6a21\u578b\u901a\u8fc7\u4e0e\u4e0d\u65ad\u6539\u8fdb\u7684\u81ea\u8eab\u7248\u672c\u8fdb\u884c\u591a\u56de\u5408\u3001\u96f6\u548c\u6e38\u620f\u6765\u81ea\u6211\u5b66\u4e60\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u6846\u67b6\uff0c\u7814\u7a76\u8005\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7684\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u89d2\u8272\u6761\u4ef6\u4f18\u52bf\u4f30\u8ba1\uff08RAE\uff09\u6280\u672f\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728Kuhn Poker\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528SPIRAL\u8fdb\u884c\u81ea\u6211\u6e38\u620f\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347Qwen3-4B-Base\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u591a\u6e38\u620f\u8bad\u7ec3\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u5df2\u7ecf\u5177\u5907\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u4e0a\u4e5f\u80fd\u89c2\u5bdf\u5230\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u96f6\u548c\u6e38\u620f\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u53ef\u4ee5\u81ea\u7136\u5730\u53d1\u5c55\u51fa\u53ef\u8f6c\u79fb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u81ea\u4e3b\u63a8\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u9014\u7684\u65b9\u5411\u3002"}}
{"id": "2506.23068", "pdf": "https://arxiv.org/pdf/2506.23068", "abs": "https://arxiv.org/abs/2506.23068", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J\u00fcrgen Schmidhuber", "Mengyue Yang"], "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": "33 pages", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeta-Causal Graph\u7684\u4e16\u754c\u6a21\u578b\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u7f16\u7801\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u53d8\u5316\u65f6\u56e0\u679c\u7ed3\u6784\u7684\u8f6c\u53d8\u89c4\u5219\uff0c\u5e76\u5f15\u5165\u4e86Causality-Seeking Agent\u4ee5\u8bc6\u522b\u548c\u6539\u8fdb\u8fd9\u4e9b\u56e0\u679c\u56fe\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7a33\u5065\u5730\u6355\u6349\u56e0\u679c\u52a8\u6001\u7684\u53d8\u5316\u5e76\u63a8\u5e7f\u5230\u672a\u89c1\u7684\u60c5\u5883\u3002", "motivation": "\u4f20\u7edf\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u6709\u4e00\u4e2a\u5355\u4e00\u4e0d\u53d8\u7684\u57fa\u672c\u56e0\u679c\u89c4\u5219\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5373\u4f7f\u662f\u5fae\u5999\u7684\u7b56\u7565\u6216\u73af\u5883\u72b6\u6001\u7684\u53d8\u5316\u90fd\u53ef\u80fd\u6539\u53d8\u89c2\u5bdf\u5230\u7684\u56e0\u679c\u673a\u5236\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u4e0d\u540c\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u4e0b\u7684\u56e0\u679c\u7ed3\u6784\u53d8\u5316\u7684\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Meta-Causal Graph\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5b83\u7531\u591a\u4e2a\u56e0\u679c\u5b50\u56fe\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5b50\u56fe\u7531\u4e00\u4e2a\u6f5c\u5728\u7684\u5143\u72b6\u6001\u89e6\u53d1\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2aCausality-Seeking Agent\uff0c\u5176\u76ee\u6807\u662f\uff1a\u901a\u8fc7\u597d\u5947\u9a71\u52a8\u7684\u5e72\u9884\u7b56\u7565\u8bc6\u522b\u89e6\u53d1\u6bcf\u4e2a\u5b50\u56fe\u7684\u5143\u72b6\u6001\u3001\u53d1\u73b0\u5bf9\u5e94\u7684\u56e0\u679c\u5173\u7cfb\u5e76\u901a\u8fc7\u6301\u7eed\u7684\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u4e0e\u7ecf\u9a8c\u8fed\u4ee3\u4f18\u5316Meta-Causal Graph\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u673a\u5668\u4eba\u624b\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u6355\u6349\u56e0\u679c\u52a8\u6001\u7684\u53d8\u5316\uff0c\u5e76\u4e14\u5728\u4e4b\u524d\u672a\u89c1\u8fc7\u7684\u60c5\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Meta-Causal Graph\u4e3a\u5efa\u6a21\u56e0\u679c\u7ed3\u6784\u5728\u4e0d\u540c\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u4e0b\u7684\u53d8\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u800cCausality-Seeking Agent\u5219\u53ef\u4ee5\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u4e0d\u65ad\u6539\u8fdb\u8fd9\u79cd\u8868\u793a\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u7684\u60c5\u5883\u65f6\u5177\u5907\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2402.09146", "pdf": "https://arxiv.org/pdf/2402.09146", "abs": "https://arxiv.org/abs/2402.09146", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u91cf\u5b50\u5377\u79ef\u5c42\u548c\u6b8b\u5dee\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08ResQuNNs\uff09\u6765\u589e\u5f3a\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QuNNs\uff09\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u786e\u653e\u7f6e\u6b8b\u5dee\u5757\u53ef\u4ee5\u663e\u8457\u63d0\u5347QuNNs\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u4f20\u7edf\u7684\u91cf\u5b50\u5377\u79ef\u5c42\u5728\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u9759\u6001\u7279\u6027\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u591a\u5c42\u53ef\u8bad\u7ec3\u91cf\u5b50\u5377\u79ef\u5c42\u4e2d\u68af\u5ea6\u4f18\u5316\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u91cf\u5b50\u5377\u79ef\u5c42\u4ee5\u589e\u52a0\u7075\u6d3b\u6027\uff1b2) \u63d0\u51fa\u6b8b\u5dee\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08ResQuNNs\uff09\uff0c\u901a\u8fc7\u5728\u91cf\u5b50\u5377\u79ef\u5c42\u4e4b\u95f4\u63d2\u5165\u6b8b\u5dee\u5757\uff0c\u6539\u5584\u68af\u5ea6\u6d41\u52a8\u5e76\u4f18\u5316\u8bad\u7ec3\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u786e\u5b9a\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6b8b\u5dee\u5757\u914d\u7f6e\uff0c\u80fd\u591f\u5b9e\u73b0\u7f51\u7edc\u4e2d\u6240\u6709\u5c42\u7684\u68af\u5ea6\u4f20\u64ad\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6b8b\u5dee\u5757\u7684\u7cbe\u786e\u4f4d\u7f6e\u5bf9\u6700\u5927\u5316QuNNs\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e3a\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u8def\u5f84\u3002"}}
{"id": "2506.23145", "pdf": "https://arxiv.org/pdf/2506.23145", "abs": "https://arxiv.org/abs/2506.23145", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5Forget-MI\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u635f\u5931\u51fd\u6570\u548c\u6270\u52a8\u6280\u672f\uff0c\u80fd\u591f\u5728\u79fb\u9664\u7279\u5b9a\u6570\u636e\uff08\u88ab\u8981\u6c42\u9057\u5fd8\u7684\u6570\u636e\uff09\u7684\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6570\u636e\u7684\u77e5\u8bc6\uff0c\u5e76\u4e14\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cForget-MI\u5728\u964d\u4f4e\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIA)\u7684\u80fd\u529b\u4ee5\u53ca\u51cf\u5c11\u5bf9\u9057\u5fd8\u6570\u636e\u96c6\u7684\u8868\u73b0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6d4b\u8bd5\u96c6\u4e0a\u7684\u7b49\u6548\u6027\u80fd\u3002", "motivation": "\u5728AI\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u4ece\u7ecf\u8fc7\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u67b6\u6784\u4e2d\u79fb\u9664\u60a3\u8005\u6570\u636e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aForget-MI\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u635f\u5931\u51fd\u6570\u548c\u6270\u52a8\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u88ab\u8981\u6c42\u9057\u5fd8\u7684\u6570\u636e\u7684\u5355\u6a21\u6001\u548c\u8054\u5408\u8868\u793a\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4f59\u6570\u636e\u7684\u77e5\u8bc6\u5e76\u7ef4\u6301\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "result": "Forget-MI\u6a21\u578b\u5728\u964d\u4f4e\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIA)\u7684\u80fd\u529b\u3001\u51cf\u5c11\u5bf9\u9057\u5fd8\u6570\u636e\u96c6\u7684\u8868\u73b0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u6d4b\u8bd5\u96c6\u4e0a\u4fdd\u6301\u4e86\u7b49\u6548\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0cMIA\u964d\u4f4e\u4e860.202\uff0c\u9057\u5fd8\u6570\u636e\u96c6\u4e0a\u7684AUC\u548cF1\u5206\u6570\u5206\u522b\u4e0b\u964d\u4e860.221\u548c0.305\u3002\u6b64\u5916\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u4e0e\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "Forget-MI\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u62a4\u9690\u79c1\u5e76\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7ef4\u6301\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23147", "pdf": "https://arxiv.org/pdf/2506.23147", "abs": "https://arxiv.org/abs/2506.23147", "authors": ["Jonathan Schuster", "Fabian Transchel"], "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 2 figures", "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "AI": {"tldr": "A new python package named maneuverRecognition is developed to facilitate preprocessing, modelling and evaluation for driving maneuvers recognition. It also includes a modifiable LSTM-based network structure and was demonstrated using real driving data from smartphone sensors of three individuals.", "motivation": "To address the practical need for python packages that can quickly transform telematic sensor data into the required structure for driving maneuvers recognition and allow building and evaluating predictive models.", "method": "Development of the maneuverRecognition python package which provides functions for preprocessing, modelling and evaluation of driving maneuvers recognition. The package also includes a ready-to-use and modifiable LSTM-based network structure.", "result": "The implementation of the maneuverRecognition package was successfully demonstrated using real driving data recorded via smartphone sensors from three different persons.", "conclusion": "The maneuverRecognition package offers necessary functions for preprocessing, modelling and evaluation in driving maneuvers recognition, fulfilling the practical needs in this domain."}}
{"id": "2506.23165", "pdf": "https://arxiv.org/pdf/2506.23165", "abs": "https://arxiv.org/abs/2506.23165", "authors": ["David Bossens", "Atsushi Nitanda"], "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9c81\u68d2\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RCMDP\uff09\u7684\u955c\u50cf\u4e0b\u964d\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6280\u672f\u4f18\u5316\u7b56\u7565\u548c\u8f6c\u79fb\u6838\uff0c\u5e76\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u4e86\u76f8\u5e94\u7684\u6536\u655b\u901f\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u4f18\u5316\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u800c\u9c81\u68d2\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\u53ef\u4ee5\u5b66\u4e60\u6ee1\u8db3\u957f\u671f\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u5e76\u5728\u8ba4\u8bc6\u4e0d\u786e\u5b9a\u6027\u4e0b\u63d0\u4f9b\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u6280\u672f\u7684\u955c\u50cf\u4e0b\u964d\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9c81\u68d2\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u5c06\u7b56\u7565\u89c6\u4e3a\u6700\u5927\u5316\u5668\uff0c\u5c06\u8f6c\u79fb\u6838\u89c6\u4e3a\u5bf9\u6297\u6700\u5c0f\u5316\u5668\uff0c\u5728\u62c9\u683c\u6717\u65e5\u51fd\u6570\u4e0a\u8fdb\u884c\u4f18\u5316\u3002\u5206\u522b\u5206\u6790\u4e86\u57fa\u4e8e\u795e\u8c15\u548c\u57fa\u4e8e\u6837\u672c\u7684RCMDP\u8bbe\u7f6e\u4e0b\u7684\u6536\u655b\u901f\u7387\u3002", "result": "\u5728\u57fa\u4e8e\u795e\u8c15\u7684RCMDP\u8bbe\u7f6e\u4e0b\uff0c\u83b7\u5f97\u4e86\u4e00\u4e2a\u5173\u4e8e\u5e03\u96f7\u683c\u66fc\u6563\u5ea6\u7684\u5e73\u65b9\u8ddd\u79bb\u7684$\\mathcal{O}\\left(\\frac{1}{T}\\right)$\u6536\u655b\u901f\u7387\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5173\u4e8e\u71b5\u6b63\u5219\u5316\u76ee\u6807\u7684$\\mathcal{O}\\left(e^{-T}\\right)$\u6536\u655b\u901f\u7387\u3002\u5728\u57fa\u4e8e\u6837\u672c\u7684RCMDP\u8bbe\u7f6e\u4e0b\uff0c\u83b7\u5f97\u4e86\u4e00\u4e2a$\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$\u6536\u655b\u901f\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u955c\u50cf\u4e0b\u964d\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u9c81\u68d2\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\u548c\u8f83\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23174", "pdf": "https://arxiv.org/pdf/2506.23174", "abs": "https://arxiv.org/abs/2506.23174", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "categories": ["cs.LG", "cs.AI"], "comment": "Published in MobiSys 2025", "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\u56e0\u5176\u80fd\u591f\u751f\u6210\u8865\u5145\u771f\u5b9e\u6570\u636e\u96c6\u6570\u91cf\u7684\u903c\u771f\u5408\u6210\u6570\u636e\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u96be\u4ee5\u9884\u6d4b\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u65e0\u6cd5\u4fdd\u8bc1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u8861\u91cf\u548c\u901a\u7528\u7684\u6307\u6807\u6765\u91cf\u5316\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u5c5e\u6027\u2014\u2014\u4eb2\u548c\u529b\u4e0e\u591a\u6837\u6027\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u65e0\u7ebf\u5408\u6210\u6570\u636e\u666e\u904d\u5b58\u5728\u4eb2\u548c\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SynCheck\uff0c\u4e00\u79cd\u4ee5\u8d28\u91cf\u4e3a\u5bfc\u5411\u7684\u5408\u6210\u6570\u636e\u5229\u7528\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSynCheck \u4e0d\u4ec5\u4f18\u4e8e\u4e0d\u8003\u8651\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u4f7f\u7528\u65b9\u6cd5\uff0c\u8fd8\u80fd\u5728\u4e4b\u524d\u65b9\u6cd5\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d 13.4% \u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0 4.3% \u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5c06\u6240\u6709\u5408\u6210\u6570\u636e\u7eb3\u5165\u8bad\u7ec3\u96c6\u53ef\u4ee5\u63d0\u9ad8\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u4e0d\u53ef\u9884\u6d4b\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u65e0\u6cd5\u4fdd\u969c\u3002\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u5e76\u6539\u8fdb\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002", "method": "1. \u63d0\u51fa\u53ef\u8861\u91cf\u548c\u901a\u7528\u7684\u6307\u6807\uff08\u4eb2\u548c\u529b\u4e0e\u591a\u6837\u6027\uff09\u6765\u91cf\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002\n2. \u53d1\u73b0\u5f53\u524d\u65e0\u7ebf\u5408\u6210\u6570\u636e\u5b58\u5728\u7684\u4eb2\u548c\u529b\u4e0d\u8db3\u95ee\u9898\u3002\n3. \u5f15\u5165 SynCheck\uff0c\u4e00\u79cd\u8d28\u91cf\u5bfc\u5411\u7684\u5408\u6210\u6570\u636e\u5229\u7528\u65b9\u6848\uff0c\u5728\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f18\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002", "result": "SynCheck \u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4e0d\u8003\u8651\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u4f7f\u7528\u65b9\u6cd5\uff0c\u5e76\u5728\u540e\u8005\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d 13.4% \u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86 4.3% \u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SynCheck \u662f\u4e00\u79cd\u6709\u6548\u7684\u5408\u6210\u6570\u636e\u5229\u7528\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\u3002"}}
{"id": "2506.23182", "pdf": "https://arxiv.org/pdf/2506.23182", "abs": "https://arxiv.org/abs/2506.23182", "authors": ["Robert Frank", "Michael Widrich", "Rahmad Akbar", "G\u00fcnter Klambauer", "Geir Kjetil Sandve", "Philippe A. Robert", "Victor Greiff"], "title": "Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\u5728\u6cbb\u7597\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u5730\u63a2\u7d22\u5bcc\u542b\u7406\u60f3\u7279\u6027\u7684\u751f\u7269\u5e8f\u5217\u7a7a\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Generative Attribution Metric Analysis (GAMA)\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eIntegrated Gradients\uff0c\u80fd\u591f\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u63d0\u53d6\u751f\u7269\u5b66\u6d1e\u89c1\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u6570\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u751f\u7269\u5e8f\u5217\u63a2\u7d22\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u5f52\u56e0\u65b9\u6cd5\u9650\u5236\u4e86\u5176\u89e3\u91ca\u80fd\u529b\u3002\u800c\u76d1\u7763\u5b66\u4e60\u9700\u8981\u6b63\u8d1f\u6837\u672c\uff0c\u5728\u751f\u7269\u9886\u57df\u96be\u4ee5\u83b7\u53d6\u8db3\u591f\u7684\u8d1f\u6837\u672c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aGAMA\uff08Generative Attribution Metric Analysis\uff09\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u57fa\u4e8eIntegrated Gradients\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528\u5177\u6709\u5df2\u77e5\u771f\u5b9e\u60c5\u51b5\u7684\u5408\u6210\u6570\u636e\u96c6\u8bc4\u4f30GAMA\u7684\u7edf\u8ba1\u884c\u4e3a\u5e76\u9a8c\u8bc1\u5176\u6062\u590d\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\u7684\u80fd\u529b\u3002\u540c\u65f6\u5c06\u5176\u5e94\u7528\u4e8e\u5b9e\u9a8c\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u6570\u636e\u4ee5\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "result": "GAMA\u6210\u529f\u5730\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u63d0\u53d6\u4e86\u53ef\u89e3\u91ca\u7684\u751f\u7269\u5b66\u89c1\u89e3\uff0c\u5e76\u4e14\u65e0\u9700\u8d1f\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u9a8c\u8bc1\u751f\u6210\u5e8f\u5217\u8bbe\u8ba1\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "GAMA\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u9a8c\u8bc1\u751f\u6210\u5e8f\u5217\u8bbe\u8ba1\u7b56\u7565\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u6a21\u578b\u5728\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2506.23201", "pdf": "https://arxiv.org/pdf/2506.23201", "abs": "https://arxiv.org/abs/2506.23201", "authors": ["Haoran Li", "Muhao Guo", "Marija Ilic", "Yang Weng", "Guangchun Ruan"], "title": "External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "10 pages", "summary": "Accurate residential load forecasting is critical for power system\nreliability with rising renewable integration and demand-side flexibility.\nHowever, most statistical and machine learning models treat external factors,\nsuch as weather, calendar effects, and pricing, as extra input, ignoring their\nheterogeneity, and thus limiting the extraction of useful external information.\nWe propose a paradigm shift: external data should serve as meta-knowledge to\ndynamically adapt the forecasting model itself. Based on this idea, we design a\nmeta-representation framework using hypernetworks that modulate selected\nparameters of a base Deep Learning (DL) model in response to external\nconditions. This provides both expressivity and adaptability. We further\nintegrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through\nselective expert activation, while improving robustness by filtering redundant\nexternal inputs. The resulting model, dubbed as a Meta Mixture of Experts for\nExternal data (M2oE2), achieves substantial improvements in accuracy and\nrobustness with limited additional overhead, outperforming existing\nstate-of-the-art methods in diverse load datasets. The dataset and source code\nare publicly available at\nhttps://github.com/haorandd/M2oE2\\_load\\_forecast.git.", "AI": {"tldr": "\u51c6\u786e\u7684\u5bb6\u5ead\u7528\u7535\u8d1f\u8377\u9884\u6d4b\u5bf9\u7535\u529b\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5c06\u5916\u90e8\u56e0\u7d20\uff08\u5982\u5929\u6c14\u3001\u65e5\u5386\u6548\u5e94\u548c\u5b9a\u4ef7\uff09\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u5f02\u8d28\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff1a\u5916\u90e8\u6570\u636e\u5e94\u4f5c\u4e3a\u5143\u77e5\u8bc6\u6765\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u6a21\u578b\u672c\u8eab\u3002\u57fa\u4e8e\u6b64\u601d\u60f3\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4f7f\u7528\u8d85\u7f51\u7edc\u7684\u5143\u8868\u793a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6839\u636e\u5916\u90e8\u6761\u4ef6\u8c03\u5236\u57fa\u7840\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9009\u62e9\u53c2\u6570\uff0c\u4ece\u800c\u63d0\u4f9b\u8868\u8fbe\u6027\u548c\u9002\u5e94\u6027\u3002\u8fdb\u4e00\u6b65\u96c6\u6210\u4e86\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u4ee5\u901a\u8fc7\u9009\u62e9\u6027\u4e13\u5bb6\u6fc0\u6d3b\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u5197\u4f59\u5916\u90e8\u8f93\u5165\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u6700\u7ec8\u6a21\u578b\uff08\u79f0\u4e3a\u5916\u90e8\u6570\u636e\u7684\u5143\u4e13\u5bb6\u6df7\u5408\u6a21\u578bM2oE2\uff09\u5728\u6709\u9650\u7684\u989d\u5916\u5f00\u9500\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u7684\u4f4f\u5b85\u8d1f\u8377\u9884\u6d4b\u5bf9\u4e8e\u7535\u529b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u5904\u7406\u65b9\u5f0f\u9650\u5236\u4e86\u6709\u7528\u4fe1\u606f\u7684\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u8868\u793a\u6846\u67b6\uff0c\u4f7f\u7528\u8d85\u7f51\u7edc\u8c03\u8282\u57fa\u7840\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u4ee5\u589e\u5f3a\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578bM2oE2\u5728\u591a\u79cd\u8d1f\u8377\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684M2oE2\u6a21\u578b\u901a\u8fc7\u6709\u9650\u7684\u989d\u5916\u5f00\u9500\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.23210", "pdf": "https://arxiv.org/pdf/2506.23210", "abs": "https://arxiv.org/abs/2506.23210", "authors": ["Taehwan Yoon", "Bongjun Choi"], "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "6 pages,14 equation", "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u5206\u5e03\u5f0f\u573a\u666f\u4e0b\u8bad\u7ec3AI\u6a21\u578b\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u7136\u800c\uff0c\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u7684\u671f\u671b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6700\u4f73\u5fae\u8c03\uff0c\u514b\u670d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\u786e\u4fdd\u4e86\u6570\u636e\u9690\u79c1\uff0c\u4f46AI\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6ee1\u8db3\u7528\u6237\u7684\u671f\u671b\uff0c\u5e76\u4e14\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u9700\u6c42\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6e90\u81ea\u8d1d\u53f6\u65af\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\uff0c\u5305\u542b\u6700\u4f18\u8fd1\u7aef\u9879\uff0c\u5229\u7528\u53c2\u8003\u6a21\u578b\u6574\u5408\u5148\u524d\u6a21\u578b\u53c2\u6570\uff0c\u4ece\u800c\u514b\u670d\u6bcf\u8f6e\u8fed\u4ee3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u6709\u6548\u5730\u514b\u670d\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.23221", "pdf": "https://arxiv.org/pdf/2506.23221", "abs": "https://arxiv.org/abs/2506.23221", "authors": ["B\u00e1lint Horv\u00e1th", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 8 figures, 6 tables", "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u56fe\u50cf\u7f3a\u5931\u50cf\u7d20\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u8be5\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u751f\u6210\u51fd\u6570\u6765\u81ea\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\uff0c\u5e76\u7279\u522b\u5f3a\u8c03\u4e86\u5e26\u9650\u51fd\u6570\u3002\u6240\u63d0\u51fa\u7684Simultaneously Guaranteed Kernel Interpolation\uff08SGKI\uff09\u65b9\u6cd5\u4e0d\u4ec5\u4f30\u8ba1\u7f3a\u5931\u50cf\u7d20\uff0c\u8fd8\u4e3a\u672a\u89c2\u6d4b\u503c\u6784\u5efa\u975e\u6e10\u8fdb\u7f6e\u4fe1\u533a\u95f4\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8ba8\u8bba\u4e86\u5411\u91cf\u503c\u51fd\u6570\u7684\u63a8\u5e7f\u548c\u4e00\u7cfb\u5217\u6570\u503c\u5b9e\u9a8c\u3002", "motivation": "\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u4e2d\u9700\u8981\u6709\u6548\u4f30\u8ba1\u56fe\u50cf\u7684\u7f3a\u5931\u50cf\u7d20\uff0c\u540c\u65f6\u63d0\u4f9b\u4f30\u8ba1\u503c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u5047\u8bbe\u6570\u636e\u751f\u6210\u51fd\u6570\u6765\u81ea\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\uff0c\u7279\u522b\u5173\u6ce8\u5e26\u9650\u51fd\u6570\u3002\u63d0\u51fa\u4e86Simultaneously Guaranteed Kernel Interpolation\uff08SGKI\uff09\u65b9\u6cd5\uff0c\u6269\u5c55\u548c\u6539\u8fdb\u4e86\u8fd1\u671f\u53d1\u5c55\u7684\u6838\u65b9\u6cd5\u3002\u5229\u7528Schur\u8865\u9ad8\u6548\u8ba1\u7b97\u7f6e\u4fe1\u533a\u95f4\uff0c\u5e76\u63a8\u5e7f\u5230\u5411\u91cf\u503c\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u751f\u6210\u548c\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86SGKI\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u7f3a\u5931\u50cf\u7d20\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "SGKI\u65b9\u6cd5\u4e3a\u56fe\u50cf\u7f3a\u5931\u50cf\u7d20\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u80fd\u540c\u65f6\u4e3a\u6240\u6709\u7f3a\u5931\u50cf\u7d20\u63d0\u4f9b\u7f6e\u4fe1\u533a\u95f4\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u3002"}}
{"id": "2506.23225", "pdf": "https://arxiv.org/pdf/2506.23225", "abs": "https://arxiv.org/abs/2506.23225", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "title": "Masked Gated Linear Unit", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684GLU\u53d8\u4f53\uff0c\u79f0\u4e3aMasked Gated Linear Units (MGLUs)\uff0c\u901a\u8fc7\u5f15\u5165Mixture of Element-wise Gating (MoEG)\u67b6\u6784\u548cFlashMGLU\u5185\u6838\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f20\u8f93\u9700\u6c42\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u5b9e\u73b0\u4e86\u52a0\u901f\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cSwish-\u6fc0\u6d3b\u7684SwiMGLU\u53d8\u4f53\u4e0d\u4ec5\u4fdd\u7559\u4e86\u5176\u5185\u5b58\u4f18\u52bf\uff0c\u8fd8\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86SwiGLU\u57fa\u7ebf\u7684\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "Gated Linear Units (GLUs)\u867d\u7136\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u6bd4\u666e\u901a\u524d\u9988\u5c42\u591a\u4e00\u500d\u7684\u5185\u5b58\u8bfb\u53d6\u64cd\u4f5c\uff0c\u8fd9\u662f\u56e0\u4e3aGLUs\u4f7f\u7528\u4e86\u72ec\u7acb\u7684\u6743\u91cd\u77e9\u9635\u6765\u5904\u7406\u95e8\u63a7\u548c\u503c\u6d41\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684GLU\u53d8\u4f53\u3002", "method": "1. \u63d0\u51fa\u4e86Mixture of Element-wise Gating (MoEG) \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u5b66\u4e60\u591a\u4e2a\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u5728\u5355\u4e2a\u5171\u4eab\u6743\u91cd\u77e9\u9635\u4e0a\u4ee5\u5143\u7d20\u7ea7\u522b\u51b3\u5b9a\u95e8\u63a7\u6216\u503c\u5206\u914d\uff0c\u4ece\u800c\u51cf\u5c11\u5185\u5b58\u4f20\u8f93\u3002\n2. \u5f00\u53d1\u4e86\u786c\u4ef6\u53cb\u597d\u7684FlashMGLU\u5185\u6838\uff0c\u8be5\u5185\u6838\u5728RTX5090 GPU\u4e0a\u6bd4\u666e\u901a\u7684PyTorch MGLU\u5feb19.7\u500d\uff0c\u540c\u65f6\u6bd4\u6807\u51c6GLU\u8282\u770147%\u7684\u5185\u5b58\u5e76\u63d0\u534734%\u7684\u901f\u5ea6\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cSwish\u6fc0\u6d3b\u7684SwiMGLU\u53d8\u4f53\u5728\u4fdd\u6301\u5185\u5b58\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u4e0eSwiGLU\u57fa\u7ebf\u76f8\u6bd4\uff0c\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u5176\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "MGLUs\u901a\u8fc7\u5f15\u5165MoEG\u67b6\u6784\u548cFlashMGLU\u5185\u6838\uff0c\u6210\u529f\u5730\u51cf\u5c11\u4e86\u5185\u5b58\u4f20\u8f93\u9700\u6c42\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002\u6b64\u5916\uff0cSwiMGLU\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23266", "pdf": "https://arxiv.org/pdf/2506.23266", "abs": "https://arxiv.org/abs/2506.23266", "authors": ["Lujun Li", "Zhu Qiyuan", "Jiacheng Wang", "Wei Li", "Hao Gu", "Sirui Han", "Yike Guo"], "title": "Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging", "categories": ["cs.LG"], "comment": "Work in progress, revisions ongoing", "summary": "Mixture of Experts (MoE) LLMs face significant obstacles due to their massive\nparameter scale, which imposes memory, storage, and deployment challenges.\nAlthough recent expert merging methods promise greater efficiency by\nconsolidating multiple experts, they are fundamentally hindered by parameter\nconflicts arising from expert specialization. In this paper, we present\nSub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key\ninsight is to perform joint Singular Value Decomposition (SVD) on concatenated\nexpert weights, reducing conflicting parameters by extracting shared\n$U$-matrices while enabling effective merging of the expert-specific $V$\ncomponents. Specifically, Sub-MoE consists of two innovative phases: (1)\nAdaptive Expert Clustering, which groups functionally coherent experts via\nK-means clustering based on cosine similarity of expert outputs; and (2)\nSubspace Expert Merging, which first enforces Experts Union Decomposition to\nderive the shared $U$-matrix across experts in the same group, then pursues\nfrequency-based merging for individual $V$-matrices, and finalizes expert\nreconstruction using the merged $V$-matrix. In this way, we align and fuse\nexperts in a shared subspace, and can be extended with intra-expert compression\nfor further inference optimization. Extensive experiments on Mixtral, DeepSeek,\nand Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms\nexisting expert pruning and merging methods. Notably, our Sub-MoE maintains\n96\\%|86\\% of original performance with 25\\%|50\\% expert reduction on\nMixtral-8x7B in zero-shot benchmarks. Code will be released at\nhttps://github.com/lliai/MoERazor.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Sub-MoE\uff0c\u4e00\u79cd\u901a\u8fc7\u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\u7684MoE\u538b\u7f29\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u8054\u5408\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u51cf\u5c11\u53c2\u6570\u51b2\u7a81\uff0c\u5e76\u5305\u542b\u81ea\u9002\u5e94\u4e13\u5bb6\u805a\u7c7b\u548c\u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\u4e24\u4e2a\u9636\u6bb5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSub-MoE\u5728Mixtral\u3001DeepSeek\u548cQwen-1.5|3 MoE\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4e13\u5bb6\u526a\u679d\u548c\u5408\u5e76\u65b9\u6cd5\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7531\u4e8e\u5176\u5e9e\u5927\u7684\u53c2\u6570\u89c4\u6a21\uff0c\u5728\u5185\u5b58\u3001\u5b58\u50a8\u548c\u90e8\u7f72\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u4e13\u5bb6\u5408\u5e76\u65b9\u6cd5\u627f\u8bfa\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u4e13\u5bb6\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5b83\u4eec\u53d7\u5230\u56e0\u4e13\u5bb6\u4e13\u4e1a\u5316\u800c\u4ea7\u751f\u7684\u53c2\u6570\u51b2\u7a81\u7684\u6839\u672c\u9650\u5236\u3002", "method": "Sub-MoE\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\uff1a1) \u81ea\u9002\u5e94\u4e13\u5bb6\u805a\u7c7b\uff1a\u6839\u636e\u4e13\u5bb6\u8f93\u51fa\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u4f7f\u7528K-means\u805a\u7c7b\u5c06\u529f\u80fd\u8fde\u8d2f\u7684\u4e13\u5bb6\u5206\u7ec4\uff1b2) \u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\uff1a\u6267\u884c\u4e13\u5bb6\u8054\u5408\u5206\u89e3\u4ee5\u63d0\u53d6\u5171\u4eab\u7684U\u77e9\u9635\uff0c\u7136\u540e\u57fa\u4e8e\u9891\u7387\u5408\u5e76\u5404\u4e2aV\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u5408\u5e76\u540e\u7684V\u77e9\u9635\u5b8c\u6210\u4e13\u5bb6\u91cd\u5efa\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSub-MoE\u5728Mixtral\u3001DeepSeek\u548cQwen-1.5|3 MoE\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4e13\u5bb6\u526a\u679d\u548c\u5408\u5e76\u65b9\u6cd5\u3002\u7279\u522b\u5730\uff0c\u5728\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSub-MoE\u5728Mixtral-8x7B\u4e0a\u5206\u522b\u4ee596%\u548c86%\u7684\u539f\u59cb\u6027\u80fd\u5b9e\u73b0\u4e8625%\u548c50%\u7684\u4e13\u5bb6\u6570\u91cf\u51cf\u5c11\u3002", "conclusion": "Sub-MoE\u662f\u4e00\u79cd\u6709\u6548\u7684MoE\u538b\u7f29\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\u663e\u8457\u51cf\u5c11\u53c2\u6570\u51b2\u7a81\u5e76\u4f18\u5316\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u4e13\u5bb6\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u9002\u7528\u4e8e\u591a\u79cdMoE\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2506.23274", "pdf": "https://arxiv.org/pdf/2506.23274", "abs": "https://arxiv.org/abs/2506.23274", "authors": ["Hans Peter Lynsg\u00f8e Raaschou-jensen", "Constanza Fierro", "Anders S\u00f8gaard"], "title": "Predicting thinking time in Reasoning models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u9884\u6d4b\u6a21\u578b\u601d\u8003\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u5b9e\u7528\u7684\u8fdb\u5ea6\u6761\uff0c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7528\u6237\u65e0\u6cd5\u9884\u77e5\u6a21\u578b\u5b8c\u6210\u63a8\u7406\u6240\u9700\u7684\u65f6\u95f4\uff0c\u8fd9\u79cd\u4e0d\u53ef\u9884\u6d4b\u6027\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u4e0d\u6ee1\u3002", "method": "\u7814\u7a76\u5728\u7ebf\u548c\u79bb\u7ebf\u9884\u6d4b\u6a21\u578b\u601d\u8003\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u51fa\u4e00\u4e2a\u5b9e\u9645\u53ef\u7528\u7684\u201c\u63a8\u7406\u8fdb\u5ea6\u6761\u201d\u3002", "result": "\u63d0\u51fa\u4e86\u9884\u6d4b\u6a21\u578b\u601d\u8003\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u7528\u6237\u4ea4\u4e92\u7684\u5f71\u54cd\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u5f00\u53d1\u63a8\u7406\u8fdb\u5ea6\u6761\u80fd\u591f\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u9884\u6d4b\u65b9\u6cd5\u548c\u589e\u5f3a\u7528\u6237\u4ea4\u4e92\u8bbe\u8ba1\u3002"}}
{"id": "2506.23280", "pdf": "https://arxiv.org/pdf/2506.23280", "abs": "https://arxiv.org/abs/2506.23280", "authors": ["Chaoqun Du", "Yulin Wang", "Shiji Song", "Gao Huang"], "title": "BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian decision theory advocates the Bayes classifier as the optimal\napproach for minimizing the risk in machine learning problems. Current deep\nlearning algorithms usually solve for the optimal classifier by\n\\emph{implicitly} estimating the posterior probabilities, \\emph{e.g.}, by\nminimizing the Softmax cross-entropy loss. This simple methodology has been\nproven effective for meticulously balanced academic benchmark datasets.\nHowever, it is not applicable to the long-tailed data distributions in the real\nworld, where it leads to the gradient imbalance issue and fails to ensure the\nBayes optimal decision rule. To address these challenges, this paper presents a\nnovel approach (BAPE) that provides a more precise theoretical estimation of\nthe data distributions by \\emph{explicitly} modeling the parameters of the\nposterior probabilities and solving them with point estimation. Consequently,\nour method directly learns the Bayes classifier without gradient descent based\non Bayes' theorem, simultaneously alleviating the gradient imbalance and\nensuring the Bayes optimal decision rule. Furthermore, we propose a\nstraightforward yet effective \\emph{distribution adjustment} technique. This\nmethod enables the Bayes classifier trained from the long-tailed training set\nto effectively adapt to the test data distribution with an arbitrary imbalance\nfactor, thereby enhancing performance without incurring additional\ncomputational costs. In addition, we demonstrate the gains of our method are\northogonal to existing learning approaches for long-tailed scenarios, as they\nare mostly designed under the principle of \\emph{implicitly} estimating the\nposterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method\nsignificantly improves the generalization performance of popular deep networks,\ndespite its simplicity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08BAPE\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u540e\u9a8c\u6982\u7387\u53c2\u6570\u5e76\u4f7f\u7528\u70b9\u4f30\u8ba1\u6c42\u89e3\uff0c\u76f4\u63a5\u5b66\u4e60\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u7f13\u89e3\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\u5e76\u786e\u4fdd\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u89c4\u5219\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5206\u5e03\u8c03\u6574\u6280\u672f\u4ee5\u9002\u5e94\u4efb\u610f\u4e0d\u5e73\u8861\u56e0\u5b50\u7684\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\uff0c\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u957f\u5c3e\u6570\u636e\u5206\u5e03\u65f6\u5b58\u5728\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u6cd5\u786e\u4fdd\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u89c4\u5219\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08BAPE\uff09\u663e\u5f0f\u5efa\u6a21\u540e\u9a8c\u6982\u7387\u53c2\u6570\u5e76\u4f7f\u7528\u70b9\u4f30\u8ba1\u6c42\u89e3\uff0c\u540c\u65f6\u63d0\u51fa\u5206\u5e03\u8c03\u6574\u6280\u672f\u4ee5\u9002\u5e94\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728CIFAR-10-LT\u3001CIFAR-100-LT\u3001ImageNet-LT\u548ciNaturalist\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6d41\u884c\u6df1\u5ea6\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u957f\u5c3e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u6b63\u4ea4\u3002"}}
{"id": "2506.23287", "pdf": "https://arxiv.org/pdf/2506.23287", "abs": "https://arxiv.org/abs/2506.23287", "authors": ["Zelin Zang", "WenZhe Li", "Fei Chen", "Yongjie Xu", "Chang Yu", "Zhen Lei", "Stan Z. Li"], "title": "Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis", "categories": ["cs.LG", "q-bio.QM"], "comment": "9 pages, 6 figures, under review", "summary": "In single-cell research, tracing and analyzing high-throughput single-cell\ndifferentiation trajectories is crucial for understanding complex biological\nprocesses. Key to this is the modeling and generation of hierarchical data that\nrepresents the intrinsic structure within datasets. Traditional methods face\nlimitations in terms of computational cost, performance, generative capacity,\nand stability. Recent VAEs based approaches have made strides in addressing\nthese challenges but still require specialized network modules for each tree\nbranch, limiting their stability and ability to capture deep hierarchical\nrelationships. To overcome these challenges, we introduce diffusion-based\napproach called HDTree. HDTree captures tree relationships within a\nhierarchical latent space using a unified hierarchical codebook and quantized\ndiffusion processes to model tree node transitions. This method improves\nstability by eliminating branch-specific modules and enhancing generative\ncapacity through gradual hierarchical changes simulated by the diffusion\nprocess. HDTree's effectiveness is demonstrated through comparisons on both\ngeneral-purpose and single-cell datasets, where it outperforms existing methods\nin terms of accuracy and performance. These contributions provide a new tool\nfor hierarchical lineage analysis, enabling more accurate and efficient\nmodeling of cellular differentiation paths and offering insights for downstream\nbiological tasks. The code of HDTree is available at anonymous link\nhttps://anonymous.4open.science/r/code_HDTree_review-A8DB.", "AI": {"tldr": "\u5728\u5355\u7ec6\u80de\u7814\u7a76\u4e2d\uff0c\u8ffd\u8e2a\u548c\u5206\u6790\u9ad8\u901a\u91cf\u7684\u5355\u7ec6\u80de\u5206\u5316\u8f68\u8ff9\u5bf9\u4e8e\u7406\u89e3\u590d\u6742\u7684\u751f\u7269\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u3001\u6027\u80fd\u3001\u751f\u6210\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u9650\u5236\u3002\u57fa\u4e8eVAE\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u7136\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u6811\u5206\u652f\u7684\u4e13\u7528\u7f51\u7edc\u6a21\u5757\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u6355\u6349\u6df1\u5c42\u5c42\u6b21\u5173\u7cfb\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5HDTree\u3002HDTree\u901a\u8fc7\u7edf\u4e00\u7684\u5206\u5c42\u4ee3\u7801\u5e93\u548c\u91cf\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u5206\u5c42\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6355\u6349\u6811\u72b6\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6d88\u9664\u5206\u652f\u7279\u5b9a\u6a21\u5757\u548c\u6a21\u62df\u6e10\u8fdb\u5c42\u6b21\u53d8\u5316\u6765\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002HDTree\u5728\u901a\u7528\u548c\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5206\u5c42\u8c31\u7cfb\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u5730\u5efa\u6a21\u7ec6\u80de\u5206\u5316\u8def\u5f84\uff0c\u5e76\u4e3a\u4e0b\u6e38\u751f\u7269\u4efb\u52a1\u63d0\u4f9b\u6d1e\u5bdf\u529b\u3002", "motivation": "\u5728\u5355\u7ec6\u80de\u7814\u7a76\u4e2d\uff0c\u8ffd\u8e2a\u548c\u5206\u6790\u9ad8\u901a\u91cf\u5355\u7ec6\u80de\u5206\u5316\u8f68\u8ff9\u662f\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u4e8eVAE\u7684\u65b9\u6cd5\u5b58\u5728\u5404\u79cd\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8ba1\u7b97\u6210\u672c\u3001\u6027\u80fd\u3001\u751f\u6210\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u6df1\u5c42\u5c42\u6b21\u5173\u7cfb\u548c\u63d0\u9ad8\u7a33\u5b9a\u6027\u65b9\u9762\u3002", "method": "HDTree\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u7edf\u4e00\u7684\u5206\u5c42\u4ee3\u7801\u5e93\u548c\u91cf\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u5206\u5c42\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6355\u6349\u6811\u72b6\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u5206\u652f\u7279\u5b9a\u6a21\u5757\u6765\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u6a21\u62df\u7684\u6e10\u8fdb\u5c42\u6b21\u53d8\u5316\u6765\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002", "result": "HDTree\u5728\u901a\u7528\u548c\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "conclusion": "HDTree\u4e3a\u5206\u5c42\u8c31\u7cfb\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u5730\u5efa\u6a21\u7ec6\u80de\u5206\u5316\u8def\u5f84\uff0c\u5e76\u4e3a\u4e0b\u6e38\u751f\u7269\u4efb\u52a1\u63d0\u4f9b\u6d1e\u5bdf\u529b\u3002"}}
{"id": "2506.23339", "pdf": "https://arxiv.org/pdf/2506.23339", "abs": "https://arxiv.org/abs/2506.23339", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "AI": {"tldr": "VALID-Mol \u662f\u4e00\u4e2a\u7ed3\u5408\u5316\u5b66\u9a8c\u8bc1\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u5206\u5b50\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u7ec6\u63d0\u793a\u5de5\u7a0b\u3001\u81ea\u52a8\u5316\u5316\u5b66\u9a8c\u8bc1\u548c\u9886\u57df\u9002\u914d\u7684 LLM \u63d0\u5347\u4e86\u751f\u6210\u6709\u6548\u5316\u5b66\u7ed3\u6784\u7684\u6bd4\u4f8b\u4ece 3% \u5230 83%\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u8fbe 17 \u500d\u7684\u76ee\u6807\u4eb2\u548c\u529b\u9884\u6d4b\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u9700\u8981\u4e8b\u5b9e\u51c6\u786e\u6027\u53ca\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u7684\u5e94\u7528\u4e2d\u4ecd\u5b58\u5728\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u836f\u7269\u5206\u5b50\u8bbe\u8ba1\u4e2d\uff0c\u867d\u7136 LLMs \u53ef\u4ee5\u63d0\u51fa\u521b\u65b0\u7684\u5206\u5b50\u4fee\u6539\u65b9\u6848\uff0c\u4f46\u5e38\u5e38\u4ea7\u751f\u5316\u5b66\u4e0a\u65e0\u6548\u6216\u4e0d\u5b9e\u7528\u7684\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u6574\u5408\u5316\u5b66\u9a8c\u8bc1\u4e0e LLM \u9a71\u52a8\u7684\u5206\u5b50\u8bbe\u8ba1\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86 VALID-Mol \u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7cbe\u7ec6\u63d0\u793a\u5de5\u7a0b\u3001\u81ea\u52a8\u5316\u7684\u5316\u5b66\u9a8c\u8bc1\u4ee5\u53ca\u9886\u57df\u9002\u914d\u7684\u5fae\u8c03 LLM\u3002\u8fd9\u79cd\u65b9\u6cd5\u786e\u4fdd\u4e86\u53ef\u5408\u6210\u5206\u5b50\u7684\u53ef\u9760\u751f\u6210\uff0c\u5e76\u6539\u5584\u4e86\u5206\u5b50\u5c5e\u6027\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u65b9\u6cd5\u8bba\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u79d1\u5b66\u7ea6\u675f\u7684 LLM \u5e94\u7528\u9886\u57df\u3002", "result": "\u901a\u8fc7\u4f7f\u7528 VALID-Mol \u6846\u67b6\uff0c\u751f\u6210\u6709\u6548\u5316\u5b66\u7ed3\u6784\u7684\u6210\u529f\u7387\u4ece 3% \u63d0\u9ad8\u5230\u4e86 83%\u3002\u8ba1\u7b97\u9884\u6d4b\u8868\u660e\uff0c\u751f\u6210\u7684\u5206\u5b50\u5728\u76ee\u6807\u4eb2\u548c\u529b\u65b9\u9762\u6709\u9ad8\u8fbe 17 \u500d\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5408\u6210\u53ef\u8fbe\u6027\u3002", "conclusion": "VALID-Mol \u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5206\u5b50\u8bbe\u8ba1\u4e2d\u751f\u6210\u6709\u6548\u5316\u5b66\u7ed3\u6784\u7684\u6210\u529f\u7387\uff0c\u8fd8\u4e3a\u5176\u4ed6\u9700\u8981\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u7684\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u84dd\u56fe\u3002"}}
{"id": "2506.23349", "pdf": "https://arxiv.org/pdf/2506.23349", "abs": "https://arxiv.org/abs/2506.23349", "authors": ["Keziah Naggita", "Julienne LaChance"], "title": "A case for data valuation transparency via DValCards", "categories": ["cs.LG"], "comment": null, "summary": "Following the rise in popularity of data-centric machine learning (ML),\nvarious data valuation methods have been proposed to quantify the contribution\nof each datapoint to desired ML model performance metrics (e.g., accuracy).\nBeyond the technical applications of data valuation methods (e.g., data\ncleaning, data acquisition, etc.), it has been suggested that within the\ncontext of data markets, data buyers might utilize such methods to fairly\ncompensate data owners. Here we demonstrate that data valuation metrics are\ninherently biased and unstable under simple algorithmic design choices,\nresulting in both technical and ethical implications. By analyzing 9 tabular\nclassification datasets and 6 data valuation methods, we illustrate how (1)\ncommon and inexpensive data pre-processing techniques can drastically alter\nestimated data values; (2) subsampling via data valuation metrics may increase\nclass imbalance; and (3) data valuation metrics may undervalue underrepresented\ngroup data. Consequently, we argue in favor of increased transparency\nassociated with data valuation in-the-wild and introduce the novel Data\nValuation Cards (DValCards) framework towards this aim. The proliferation of\nDValCards will reduce misuse of data valuation metrics, including in data\npricing, and build trust in responsible ML systems.", "AI": {"tldr": "\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5728\u7b80\u5355\u7684\u7b97\u6cd5\u8bbe\u8ba1\u9009\u62e9\u4e0b\u5b58\u5728\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\uff0c\u53ef\u80fd\u5bfc\u81f4\u6280\u672f\u548c\u4f26\u7406\u95ee\u9898\u3002\u672c\u6587\u901a\u8fc7\u5206\u67909\u4e2a\u8868\u683c\u5206\u7c7b\u6570\u636e\u96c6\u548c6\u79cd\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6570\u636e\u9884\u5904\u7406\u3001\u5b50\u91c7\u6837\u548c\u6570\u636e\u4ee3\u8868\u6027\u5bf9\u6570\u636e\u4f30\u503c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86Data Valuation Cards (DValCards)\u6846\u67b6\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u51cf\u5c11\u8bef\u7528\u5e76\u5efa\u7acb\u4fe1\u4efb\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65e5\u76ca\u6d41\u884c\uff0c\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u88ab\u63d0\u51fa\u7528\u4e8e\u91cf\u5316\u6570\u636e\u70b9\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\u3002\u6b64\u5916\uff0c\u5728\u6570\u636e\u5e02\u573a\u4e2d\uff0c\u6570\u636e\u4e70\u65b9\u53ef\u80fd\u5229\u7528\u8fd9\u4e9b\u65b9\u6cd5\u516c\u5e73\u8865\u507f\u6570\u636e\u6240\u6709\u8005\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5f71\u54cd\u53ca\u6539\u8fdb\u63aa\u65bd\u3002", "method": "\u5206\u67909\u4e2a\u8868\u683c\u5206\u7c7b\u6570\u636e\u96c6\u548c6\u79cd\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4ee5\u4e0b\u65b9\u9762\uff1a(1) \u6570\u636e\u9884\u5904\u7406\u6280\u672f\u5bf9\u4f30\u503c\u7684\u5f71\u54cd\uff1b(2) \u5b50\u91c7\u6837\u662f\u5426\u589e\u52a0\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b(3) \u662f\u5426\u4f4e\u4f30\u5c11\u6570\u7fa4\u4f53\u7684\u6570\u636e\u4ef7\u503c\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faDValCards\u6846\u67b6\u4ee5\u63d0\u5347\u6570\u636e\u4f30\u503c\u7684\u900f\u660e\u5ea6\u3002", "result": "(1) \u5e38\u89c1\u4e14\u4f4e\u6210\u672c\u7684\u6570\u636e\u9884\u5904\u7406\u6280\u672f\u663e\u8457\u6539\u53d8\u6570\u636e\u4f30\u503c\uff1b(2) \u4f7f\u7528\u6570\u636e\u4f30\u503c\u6307\u6807\u8fdb\u884c\u5b50\u91c7\u6837\u53ef\u80fd\u589e\u52a0\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b(3) \u6570\u636e\u4f30\u503c\u65b9\u6cd5\u53ef\u80fd\u4f4e\u4f30\u5c11\u6570\u7fa4\u4f53\u7684\u6570\u636e\u4ef7\u503c\u3002", "conclusion": "\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\uff0c\u53ef\u80fd\u5bfc\u81f4\u6280\u672f\u4e0e\u4f26\u7406\u95ee\u9898\u3002\u4e3a\u51cf\u5c11\u8bef\u7528\u5e76\u589e\u5f3a\u4fe1\u4efb\uff0c\u9700\u63d0\u9ad8\u6570\u636e\u4f30\u503c\u7684\u900f\u660e\u5ea6\uff0c\u800cDValCards\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23358", "pdf": "https://arxiv.org/pdf/2506.23358", "abs": "https://arxiv.org/abs/2506.23358", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "categories": ["cs.LG", "cs.AI"], "comment": "conference paper", "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Federated Timeline Synthesis (FTS)\uff0c\u4e00\u79cd\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\u751f\u6210\u6027\u57fa\u7840\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u3002\u901a\u8fc7\u8868\u793a\u60a3\u8005\u5386\u53f2\u4e3a\u6807\u8bb0\u5316\u7684\u60a3\u8005\u5065\u5eb7\u65f6\u95f4\u7ebf\uff08PHTs\uff09\uff0c\u5404\u673a\u6784\u5728\u672c\u5730\u8bad\u7ec3\u81ea\u56de\u5f52\u53d8\u538b\u5668\u5e76\u4ec5\u4f20\u8f93\u6a21\u578b\u6743\u91cd\u5230\u4e2d\u592e\u670d\u52a1\u5668\u3002\u670d\u52a1\u5668\u4f7f\u7528\u751f\u6210\u5668\u5408\u6210\u5927\u91cf\u8f68\u8ff9\u4ee5\u8bad\u7ec3\u5168\u5c40\u751f\u6210\u5668\uff08GG\uff09\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u672a\u6765PHTs\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u65ad\u3002\u5728MIMIC-IV\u6570\u636e\u4e0a\u7684\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8eGG\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4e0e\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u3002FTS\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u3001\u8de8\u673a\u6784\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u6837\u5316\u7684\u9884\u6d4b\u548c\u6a21\u62df\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u5305\u62ec\u53cd\u4e8b\u5b9e\u63a8\u65ad\u3001\u65e9\u671f\u9884\u8b66\u68c0\u6d4b\u548c\u5408\u6210\u8bd5\u9a8c\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u5206\u5e03\u5f0f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u6790\u9762\u4e34\u9690\u79c1\u4fdd\u62a4\u3001\u8de8\u673a\u6784\u534f\u4f5c\u53ca\u6570\u636e\u53ef\u7528\u6027\u7b49\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u9700\u8981\u4e00\u4e2a\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u5229\u7528\u591a\u673a\u6784\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002", "method": "FTS\u5c06\u60a3\u8005\u7684\u75c5\u53f2\u7f16\u7801\u4e3a\u65f6\u95f4\u5316\u3001\u5206\u7c7b\u548c\u8fde\u7eed\u4e34\u5e8a\u4fe1\u606f\u7684\u65f6\u95f4\u5e8f\u5217\uff08PHTs\uff09\u3002\u6bcf\u4e2a\u673a\u6784\u5728\u5176\u672c\u5730\u6570\u636e\u4e0a\u8bad\u7ec3\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u5e76\u4ec5\u5171\u4eab\u6a21\u578b\u6743\u91cd\u81f3\u4e2d\u592e\u670d\u52a1\u5668\u3002\u4e2d\u592e\u670d\u52a1\u5668\u5229\u7528\u63a5\u6536\u5230\u7684\u6743\u91cd\u5408\u6210\u5927\u89c4\u6a21\u8f68\u8ff9\uff0c\u5e76\u8bad\u7ec3\u5168\u5c40\u751f\u6210\u5668\uff08GG\uff09\u3002\u901a\u8fc7GG\uff0c\u53ef\u4ee5\u5bf9\u672a\u6765\u7684PHTs\u8fdb\u884c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u65ad\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4f7f\u7528\u7531GG\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u3002\u8fd9\u8868\u660eFTS\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u8de8\u673a\u6784\u6570\u636e\u534f\u4f5c\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "FTS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5206\u5e03\u5f0f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u8de8\u673a\u6784\u534f\u4f5c\u4e2d\u7684\u6311\u6218\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u5728\u591a\u6837\u5316\u9884\u6d4b\u548c\u6a21\u62df\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u5982\u53cd\u4e8b\u5b9e\u63a8\u65ad\u3001\u65e9\u671f\u9884\u8b66\u68c0\u6d4b\u548c\u5408\u6210\u8bd5\u9a8c\u8bbe\u8ba1\u3002"}}
{"id": "2506.23374", "pdf": "https://arxiv.org/pdf/2506.23374", "abs": "https://arxiv.org/abs/2506.23374", "authors": ["Dominik Meier", "Sujai Hiremath", "Promit Ghosal", "Kyra Gan"], "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery", "categories": ["cs.LG"], "comment": null, "summary": "Distinguishing cause and effect from bivariate observational data is a\nfoundational problem in many disciplines, but challenging without additional\nassumptions. Additive noise models (ANMs) are widely used to enable\nsample-efficient bivariate causal discovery. However, conventional ANM-based\nmethods fail when unobserved mediators corrupt the causal relationship between\nvariables. This paper makes three key contributions: first, we rigorously\ncharacterize why standard ANM approaches break down in the presence of\nunmeasured mediators. Second, we demonstrate that prior solutions for hidden\nmediation are brittle in finite sample settings, limiting their practical\nutility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)\nfor causal discovery, a method designed to handle latent noise introduced by\nunmeasured mediators. Unlike prior methods that infer directionality through\nmean squared error loss comparisons, our approach introduces a novel\nindependence test statistic: during the noising and denoising processes for\neach variable, we condition on the other variable as input and evaluate the\nindependence of the predicted noise relative to this input. We prove asymptotic\nconsistency of BiDD under the ANM, and conjecture that it performs well under\nhidden mediation. Experiments on synthetic and real-world data demonstrate\nconsistent performance, outperforming existing methods in mediator-corrupted\nsettings while maintaining strong performance in mediator-free settings.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u7814\u7a76\u4e86\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6539\u8fdb\u4e8c\u5143\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u7684\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Bivariate Denoising Diffusion (BiDD) \u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u53d7\u4e2d\u4ecb\u53d8\u91cf\u5e72\u6270\u7684\u6570\u636e\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u52a0\u6027\u566a\u58f0\u6a21\u578b\uff08ANM\uff09\u65b9\u6cd5\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u65f6\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBivariate Denoising Diffusion (BiDD)\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5bf9\u6bcf\u4e2a\u53d8\u91cf\u8fdb\u884c\u52a0\u566a\u548c\u53bb\u566a\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u53e6\u4e00\u4e2a\u53d8\u91cf\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u8bc4\u4f30\u9884\u6d4b\u566a\u58f0\u4e0e\u8be5\u8f93\u5165\u7684\u72ec\u7acb\u6027\uff0c\u4ece\u800c\u63a8\u65ad\u56e0\u679c\u65b9\u5411\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBiDD\u65b9\u6cd5\u5728\u5904\u7406\u53d7\u4e2d\u4ecb\u53d8\u91cf\u5e72\u6270\u7684\u6570\u636e\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u540c\u65f6\u5728\u65e0\u4e2d\u4ecb\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u4e5f\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "BiDD\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u5728\u4e8c\u5143\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.23408", "pdf": "https://arxiv.org/pdf/2506.23408", "abs": "https://arxiv.org/abs/2506.23408", "authors": ["Claudionor Coelho Jr", "Yanen Li", "Philip Tee"], "title": "Do LLMs Dream of Discrete Algorithms?", "categories": ["cs.LG", "cs.LO"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.", "AI": {"tldr": "Large Language Models (LLMs) are powerful but struggle with strict logical reasoning. This paper proposes augmenting LLMs with logic-based reasoning modules using Prolog predicates to improve their reliability and interpretability.", "motivation": "LLMs have revolutionized AI but face challenges in domains requiring strict logical reasoning, discrete decision-making, and robust interpretability due to their reliance on probabilistic inference.", "method": "The paper investigates the limitations of LLMs and proposes a neurosymbolic approach that integrates first-order logic and explicit rule systems, using Prolog predicates and composable toolsets to augment LLMs.", "result": "Experiments on the DABStep benchmark demonstrate improved precision, coverage, and system documentation in multi-step reasoning tasks.", "conclusion": "Combining LLMs with modular logic reasoning enhances engineering rigor, system reliability, and offers a scalable path toward trustworthy, interpretable AI agents."}}
{"id": "2506.23419", "pdf": "https://arxiv.org/pdf/2506.23419", "abs": "https://arxiv.org/abs/2506.23419", "authors": ["Amanda S Barnard"], "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u5de5\u5177BenchMake\uff0c\u5b83\u80fd\u591f\u5c06\u516c\u5f00\u7684\u79d1\u5b66\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002\u8be5\u5de5\u5177\u4f7f\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u6765\u8bc6\u522b\u548c\u9694\u79bb\u5177\u6709\u6311\u6218\u6027\u7684\u8fb9\u754c\u6848\u4f8b\uff0c\u5e76\u5212\u5206\u6d4b\u8bd5\u96c6\u4ee5\u6700\u5927\u5316\u5dee\u5f02\u548c\u7edf\u8ba1\u663e\u8457\u6027\u3002\u901a\u8fc7\u4e0e\u5df2\u5efa\u7acb\u7684\u5206\u5272\u548c\u968f\u673a\u5206\u5272\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86BenchMake\u5728\u591a\u79cd\u7c7b\u578b\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8ba1\u7b97\u79d1\u5b66\u4e2d\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u4f7f\u5f97\u8bc4\u4f30\u65b0\u65b9\u6cd5\u53d8\u5f97\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5de5\u5177\u5c06\u8d8a\u6765\u8d8a\u591a\u7684\u516c\u5f00\u79d1\u5b66\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u53ef\u4f9b\u793e\u533a\u4f7f\u7528\u7684\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aBenchMake\u7684\u65b0\u5de5\u5177\uff0c\u5229\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u786e\u5b9a\u6027\u5730\u8bc6\u522b\u548c\u9694\u79bb\u51f8\u58f3\u4e0a\u7684\u6311\u6218\u6027\u8fb9\u7f18\u6848\u4f8b\uff0c\u5e76\u5c06\u5339\u914d\u7684\u6570\u636e\u5b9e\u4f8b\u5212\u5206\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u4ece\u800c\u5728\u8868\u683c\u3001\u56fe\u3001\u56fe\u50cf\u3001\u4fe1\u53f7\u548c\u6587\u672c\u6a21\u6001\u4e0a\u6700\u5927\u5316\u5dee\u5f02\u548c\u7edf\u8ba1\u663e\u8457\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u5341\u4e2a\u6765\u81ea\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u516c\u5171\u53ef\u7528\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660eBenchMake\u5206\u5272\u76f8\u8f83\u4e8e\u5df2\u5efa\u7acb\u7684\u5206\u5272\u548c\u968f\u673a\u5206\u5272\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "BenchMake\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u516c\u5f00\u7684\u79d1\u5b66\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4ef7\u5de5\u5177\u3002"}}
{"id": "2506.23424", "pdf": "https://arxiv.org/pdf/2506.23424", "abs": "https://arxiv.org/abs/2506.23424", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5PETSA\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5c0f\u6821\u51c6\u6a21\u5757\u6765\u9002\u5e94\u9884\u6d4b\u5668\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPETSA\u5728\u6240\u6709\u9884\u6d4b\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u901a\u5e38\u8868\u73b0\u51fa\u975e\u5e73\u7a33\u6027\uff0c\u8fd9\u4f1a\u964d\u4f4e\u9884\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684TTA\u65b9\u6cd5\u901a\u5e38\u66f4\u65b0\u6574\u4e2a\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "PETSA\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u548c\u52a8\u6001\u95e8\u63a7\u673a\u5236\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u8868\u793a\u3002\u4e3a\u4e86\u5728\u6709\u9650\u7684\u9002\u5e94\u80fd\u529b\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u4e86\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u7a33\u5065\u9879\u3001\u9891\u57df\u9879\u548c\u5757\u72b6\u7ed3\u6784\u9879\u3002", "result": "PETSA\u63d0\u9ad8\u4e86\u5404\u79cd\u9884\u6d4b\u4e3b\u5e72\u7684\u9002\u5e94\u6027\uff0c\u540c\u65f6\u6bd4\u57fa\u7ebf\u9700\u8981\u66f4\u5c11\u7684\u53c2\u6570\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPETSA\u5728\u6240\u6709\u9884\u6d4b\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "PETSA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u53c2\u6570\u9700\u6c42\u7684\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23446", "pdf": "https://arxiv.org/pdf/2506.23446", "abs": "https://arxiv.org/abs/2506.23446", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders", "categories": ["cs.LG"], "comment": null, "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "AI": {"tldr": "\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u7531\u4e8e\u6076\u610f\u884c\u4e3a\u8005\u7684\u6388\u6743\u72b6\u6001\u548c\u5f02\u5e38\u884c\u4e3a\u7684\u5fae\u5999\u6027\u800c\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u7684\u5e8f\u5217\u5316\uff08UBS\uff09\u65b9\u6cd5\uff0c\u5c06CERT\u5185\u90e8\u5a01\u80c1\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u9002\u5408\u6df1\u5ea6\u5e8f\u5217\u5efa\u6a21\u7684\u7ed3\u6784\u5316\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528Transformer Encoder\u67b6\u6784\u5bf9\u826f\u6027\u7528\u6237\u6d3b\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u901a\u8fc7\u5176\u91cd\u5efa\u8bef\u5dee\u4f5c\u4e3a\u5f02\u5e38\u8bc4\u5206\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u4e25\u683c\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u96c6\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a96.61%\uff0c\u53ec\u56de\u7387\u4e3a99.43%\uff0cF1\u5f97\u5206\u4e3a96.38%\uff0cAUROC\u4e3a95.00%\uff0c\u4e14\u5047\u9634\u6027\u548c\u5047\u9633\u6027\u7387\u6781\u4f4e\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u7528\u6237\u6d3b\u52a8\u89c6\u4e3a\u5b64\u7acb\u4e8b\u4ef6\uff0c\u672a\u80fd\u5229\u7528\u7528\u6237\u884c\u4e3a\u4e2d\u7684\u987a\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u7528\u6237\u884c\u4e3a\u65f6\u5e8f\u7279\u6027\u7684\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdUser-Based Sequencing (UBS) \u65b9\u6cd5\uff0c\u5c06\u5185\u90e8\u5a01\u80c1\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u5e8f\u5217\uff1b\u91c7\u7528Transformer Encoder\u67b6\u6784\u5bf9\u826f\u6027\u7528\u6237\u6d3b\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5176\u91cd\u5efa\u8bef\u5dee\u751f\u6210\u5f02\u5e38\u8bc4\u5206\uff1b\u6700\u540e\u4f7f\u7528\u4e09\u79cd\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff08One-Class SVM\u3001Local Outlier Factor \u548c Isolation Forest\uff09\u5bf9\u8bc4\u5206\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u56db\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u96c6\u4e2d\uff0cUBS-Transformer\u7ba1\u9053\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff1a\u51c6\u786e\u7387\u8fbe\u523096.61%\uff0c\u53ec\u56de\u7387\u8fbe\u523099.43%\uff0cF1\u5f97\u5206\u4e3a96.38%\uff0cAUROC\u8fbe\u523095.00%\uff0c\u5e76\u4e14\u5047\u9634\u6027\u7387\u548c\u5047\u9633\u6027\u7387\u975e\u5e38\u4f4e\uff08\u5206\u522b\u4e3a0.0057\u548c0.0571\uff09\u3002\u76f8\u6bd4\u4f20\u7edf\u7684\u8868\u683c\u6570\u636e\u548c\u81ea\u52a8\u7f16\u7801\u5668\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u5316\u7684\u5efa\u6a21\u65b9\u6cd5\u548c\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u5728\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u65f6\u5e8f\u7279\u6027\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2506.23462", "pdf": "https://arxiv.org/pdf/2506.23462", "abs": "https://arxiv.org/abs/2506.23462", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "AI": {"tldr": "\u63d0\u51faDisasterNet-LLM\uff0c\u4e13\u4e3a\u707e\u5bb3\u5206\u6790\u8bbe\u8ba1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u9002\u5e94\u53d8\u538b\u5668\uff0c\u5728\u591a\u6a21\u6001\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u523089.5%\u3002", "motivation": "\u6709\u6548\u7684\u707e\u5bb3\u7ba1\u7406\u9700\u8981\u53ca\u65f6\u548c\u51c6\u786e\u7684\u89c1\u89e3\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u5929\u6c14\u8bb0\u5f55\u548c\u6587\u672c\u62a5\u544a\uff09\u3002", "method": "\u63d0\u51fa\u4e86DisasterNet-LLM\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5168\u9762\u707e\u5bb3\u5206\u6790\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u9002\u5e94\u53d8\u538b\u5668\u8fdb\u884c\u707e\u5bb3\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u6a21\u6001\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4e3a89.5%\uff0cF1\u5f97\u5206\u4e3a88.0%\uff0cAUC\u4e3a0.92%\uff0cBERTScore\u4e3a0.88%\u3002", "conclusion": "DisasterNet-LLM\u5728\u591a\u6a21\u6001\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u53ef\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u53ca\u65f6\u7684\u707e\u5bb3\u7ba1\u7406\u89c1\u89e3\u3002"}}
{"id": "2506.23469", "pdf": "https://arxiv.org/pdf/2506.23469", "abs": "https://arxiv.org/abs/2506.23469", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "categories": ["cs.LG", "cs.SI"], "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTripleAD\u7684\u4e09\u901a\u9053\u56fe\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e0d\u540c\u7684\u4f30\u8ba1\u6a21\u5757\u5206\u522b\u8bc6\u522b\u5c5e\u6027\u3001\u7ed3\u6784\u548c\u6df7\u5408\u5f02\u5e38\uff0c\u5e76\u5f15\u5165\u76f8\u4e92\u84b8\u998f\u7b56\u7565\u4fc3\u8fdb\u6a21\u5757\u95f4\u7684\u534f\u4f5c\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u56fe\u5f02\u5e38\u68c0\u6d4b\u5728\u533b\u7597\u4fdd\u5065\u548c\u7ecf\u6d4e\u5b66\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u9884\u9632\u91cd\u5927\u635f\u5931\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u540c\u65f6\u68c0\u6d4b\u5c5e\u6027\u548c\u7ed3\u6784\u5f02\u5e38\u65f6\u9762\u4e34\u6027\u80fd\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u4e24\u79cd\u5f02\u5e38\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5e72\u6270\u3002", "method": "TripleAD\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4f30\u8ba1\u6a21\u5757\uff1a1) \u591a\u5c3a\u5ea6\u5c5e\u6027\u4f30\u8ba1\u6a21\u5757\u7528\u4e8e\u6355\u6349\u5e7f\u6cdb\u7684\u8282\u70b9\u4ea4\u4e92\u5e76\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\uff1b2) \u94fe\u63a5\u589e\u5f3a\u7ed3\u6784\u4f30\u8ba1\u6a21\u5757\u7528\u4e8e\u66f4\u597d\u5730\u8bc6\u522b\u7ed3\u6784\u5f02\u5e38\uff0c\u4fc3\u8fdb\u5b64\u7acb\u8282\u70b9\u7684\u4fe1\u606f\u6d41\u52a8\uff1b3) \u5c5e\u6027-\u6df7\u5408\u66f2\u7387\u6a21\u5757\u4f5c\u4e3a\u65b0\u6307\u6807\uff0c\u7ed3\u5408\u5c5e\u6027\u548c\u7ed3\u6784\u4fe1\u606f\u4ee5\u533a\u5206\u6df7\u5408\u5f02\u5e38\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u76f8\u4e92\u84b8\u998f\u7b56\u7565\u4ee5\u9f13\u52b1\u4e09\u4e2a\u6a21\u5757\u4e4b\u95f4\u7684\u901a\u4fe1\u4e0e\u534f\u4f5c\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86TripleAD\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5176\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TripleAD\u901a\u8fc7\u8bbe\u8ba1\u4e13\u95e8\u7684\u6a21\u5757\u6765\u89e3\u51b3\u5c5e\u6027\u548c\u7ed3\u6784\u5f02\u5e38\u4e4b\u95f4\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u76f8\u4e92\u84b8\u998f\u7b56\u7565\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u56fe\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23492", "pdf": "https://arxiv.org/pdf/2506.23492", "abs": "https://arxiv.org/abs/2506.23492", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "AI": {"tldr": "SMART\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8elogit gap\u7684\u91cd\u65b0\u6821\u51c6\u548c\u65b0\u9896\u7684SoftECE\u76ee\u6807\u51fd\u6570\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u540e\u9a8c\u6821\u51c6\u65b9\u6cd5\u8981\u4e48\u5f15\u5165\u9ad8\u504f\u5dee\uff08\u5168\u5c40\u65b9\u6cd5\uff09\uff0c\u8981\u4e48\u56e0\u9ad8\u7ef4\u566a\u58f0\u8f93\u5165\u548c\u6709\u9650\u9a8c\u8bc1\u6570\u636e\u800c\u4ea7\u751f\u9ad8\u65b9\u5dee\uff08\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u65b9\u6cd5\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMART\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u524d\u4e24\u5927logits\u4e4b\u95f4\u7684\u5dee\u8ddd\uff08logit gap\uff09\u7cbe\u786e\u7f29\u653elogits\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u65b0\u7684SoftECE\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u7bb1\u5e73\u8861\u6a21\u578b\u504f\u5dee\u548c\u65b9\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSMART\u5373\u4f7f\u5728\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6821\u51c6\u6027\u80fd\u3002", "conclusion": "SMART\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u3001\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2506.23516", "pdf": "https://arxiv.org/pdf/2506.23516", "abs": "https://arxiv.org/abs/2506.23516", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\uff0c\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u7ea6\u675f\u5e38\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFedWSQ\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6743\u91cd\u6807\u51c6\u5316\uff08WS\uff09\u548c\u5206\u5e03\u611f\u77e5\u975e\u5747\u5300\u91cf\u5316\uff08DANUQ\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFedWSQ\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684FL\u73af\u5883\u4e2d\uff0c\u5305\u62ec\u6781\u7aef\u6570\u636e\u5f02\u6784\u6027\u548c\u8d85\u4f4e\u6bd4\u7279\u901a\u4fe1\u573a\u666f\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u7684FL\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7ecf\u5e38\u7531\u4e8e\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u9650\u5236\u800c\u906d\u53d7\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedWSQ\uff0c\u5b83\u7ed3\u5408\u4e86\u6743\u91cd\u6807\u51c6\u5316\uff08WS\uff09\u548c\u5206\u5e03\u611f\u77e5\u975e\u5747\u5300\u91cf\u5316\uff08DANUQ\uff09\u3002WS\u901a\u8fc7\u8fc7\u6ee4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672c\u5730\u66f4\u65b0\u4e2d\u7684\u504f\u5dee\u6210\u5206\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u6570\u636e\u5f02\u6784\u6027\u548c\u4e0d\u7a33\u5b9a\u7684\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u9c81\u68d2\u6027\u3002DANUQ\u5229\u7528\u672c\u5730\u6a21\u578b\u66f4\u65b0\u7684\u7edf\u8ba1\u7279\u6027\u6765\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u3002", "result": "FedWSQ\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6a21\u578b\u51c6\u786e\u6027\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0cFedWSQ\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedWSQ\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6781\u7aef\u6570\u636e\u5f02\u6784\u6027\u548c\u8d85\u4f4e\u6bd4\u7279\u901a\u4fe1\u573a\u666f\u3002"}}
{"id": "2506.23544", "pdf": "https://arxiv.org/pdf/2506.23544", "abs": "https://arxiv.org/abs/2506.23544", "authors": ["Kento Imaizumi", "Hideaki Iiduka"], "title": "Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Momentum methods were originally introduced for their superiority to\nstochastic gradient descent (SGD) in deterministic settings with convex\nobjective functions. However, despite their widespread application to deep\nneural networks -- a representative case of stochastic nonconvex optimization\n-- the theoretical justification for their effectiveness in such settings\nremains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that\ngeneralizes various momentum methods and has been studied to better understand\nthe class of momentum-based algorithms as a whole. In this paper, we provide\nboth asymptotic and non-asymptotic convergence results for mini-batch QHM with\nan increasing batch size. We show that achieving asymptotic convergence\nrequires either a decaying learning rate or an increasing batch size. Since a\ndecaying learning rate adversely affects non-asymptotic convergence, we\ndemonstrate that using mini-batch QHM with an increasing batch size -- without\ndecaying the learning rate -- can be a more effective strategy. Our experiments\nshow that even a finite increase in batch size can provide benefits for\ntraining neural networks.", "AI": {"tldr": "\u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u7814\u7a76\u4e86\u4f7f\u7528\u589e\u5927\u7684batch size\u7684mini-batch QHM\u7684\u6e10\u8fd1\u548c\u975e\u6e10\u8fd1\u6536\u655b\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u51cf\u5c11\u5b66\u4e60\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u589e\u5927batch size\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7fbatch size\u6709\u9650\u589e\u52a0\uff0c\u4e5f\u80fd\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5e26\u6765\u597d\u5904\u3002", "motivation": "\u52a8\u91cf\u65b9\u6cd5\u6700\u521d\u662f\u4e3a\u4e86\u5728\u5177\u6709\u51f8\u76ee\u6807\u51fd\u6570\u7684\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u4f18\u4e8e\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u800c\u5f15\u5165\u7684\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5b83\u4eec\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5178\u578b\u7684\u968f\u673a\u975e\u51f8\u4f18\u5316\u60c5\u51b5\uff09\uff0c\u4f46\u5bf9\u5176\u5728\u8fd9\u79cd\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u7406\u8bba\u89e3\u91ca\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u589e\u5927\u6279size\u7684mini-batch QHM\u7684\u6e10\u8fd1\u548c\u975e\u6e10\u8fd1\u6536\u655b\u7ed3\u679c\u3002\u7814\u7a76\u8868\u660e\uff0c\u5b9e\u73b0\u6e10\u8fd1\u6536\u655b\u9700\u8981\u8870\u51cf\u7684\u5b66\u4e60\u7387\u6216\u589e\u5927\u7684batch size\u3002\u7531\u4e8e\u8870\u51cf\u7684\u5b66\u4e60\u7387\u5bf9\u975e\u6e10\u8fd1\u6536\u655b\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u6b64\u5c55\u793a\u4e86\u4f7f\u7528\u589e\u5927\u7684batch size\u800c\u4e0d\u8870\u51cf\u5b66\u4e60\u7387\u53ef\u80fd\u662f\u66f4\u6709\u6548\u7684\u7b56\u7565\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7fbatch size\u6709\u9650\u589e\u52a0\uff0c\u4e5f\u80fd\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5e26\u6765\u597d\u5904\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u589e\u5927\u7684batch size\u800c\u4e0d\u8870\u51cf\u5b66\u4e60\u7387\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u5347QHM\u7684\u6027\u80fd\u3002", "conclusion": "\u4f5c\u8005\u5f97\u51fa\u7ed3\u8bba\uff0c\u589e\u5927\u7684batch size\u53ef\u4ee5\u5728\u4e0d\u964d\u4f4e\u5b66\u4e60\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8QHM\u7684\u6536\u655b\u6027\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.23551", "pdf": "https://arxiv.org/pdf/2506.23551", "abs": "https://arxiv.org/abs/2506.23551", "authors": ["Jingpu Cheng", "Qianxiao Li", "Ting Lin", "Zuowei Shen"], "title": "A unified framework on the universal approximation of transformer-type architectures", "categories": ["cs.LG"], "comment": null, "summary": "We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u67b6\u6784\u7684\u901a\u7528\u8fd1\u4f3c\u6027\u8d28\uff08UAP\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6b8b\u5dee\u7f51\u7edc\u7684\u7ed3\u679c\u6269\u5c55\u5230\u5305\u542b\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u4e8e\u6b8b\u5dee\u7f51\u7edc\u7684\u901a\u7528\u8fd1\u4f3c\u6027\u8d28\u7684\u7814\u7a76\u5c1a\u672a\u6db5\u76d6\u5305\u542b\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5e7f\u6cdb\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790Transformer\u67b6\u6784\u7684UAP\u3002", "method": "\u4f5c\u8005\u9996\u5148\u786e\u5b9a\u4e86\u4ee4\u724c\u53ef\u533a\u5206\u6027\u4f5c\u4e3aUAP\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5e7f\u6cdb\u67b6\u6784\u7684\u5145\u5206\u6761\u4ef6\u3002\u901a\u8fc7\u5047\u8bbe\u6ce8\u610f\u529b\u5c42\u7684\u89e3\u6790\u6027\uff0c\u7b80\u5316\u4e86\u8be5\u6761\u4ef6\u7684\u9a8c\u8bc1\u8fc7\u7a0b\u3002\u63a5\u7740\uff0c\u8bc1\u660e\u4e86\u5177\u6709\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u57fa\u4e8e\u6838\u548c\u7a00\u758f\u6ce8\u610f\u529b\uff09\u7684Transformer\u6ee1\u8db3UAP\u3002", "result": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63a8\u5e7f\u4e86\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u8fd8\u4e3a\u5177\u6709\u7279\u5b9a\u529f\u80fd\u5bf9\u79f0\u6027\u7684\u65b0\u578bTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u80fd\u591f\u7528\u4e8e\u8bc1\u660e\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u7684Transformer\u5177\u5907UAP\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5177\u6709\u5185\u5728UAP\u4fdd\u8bc1\u7684\u65b0\u67b6\u6784\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.23589", "pdf": "https://arxiv.org/pdf/2506.23589", "abs": "https://arxiv.org/abs/2506.23589", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "title": "Transition Matching: Scalable and Flexible Generative Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u8303\u5f0fTransition Matching (TM)\uff0c\u7edf\u4e00\u5e76\u63a8\u8fdb\u4e86\u6269\u6563/\u6d41\u6a21\u578b\u548c\u8fde\u7eed\u81ea\u56de\u5f52(AR)\u751f\u6210\u65b9\u6cd5\u3002\u901a\u8fc7\u4e09\u79cdTM\u53d8\u4f53\uff0c\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u751f\u6210\u4e0a\u7684\u4f18\u8d8a\u6027\u53ca\u91c7\u6837\u6548\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\u8bbe\u8ba1\u7a7a\u95f4\u5df2\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u53ef\u80fd\u6027\uff1b\u800c\u8fde\u7eed\u6807\u8bb0\u751f\u6210\u7684\u81ea\u56de\u5f52(AR)\u6a21\u578b\u4e3a\u7edf\u4e00\u6587\u672c\u548c\u5a92\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u751f\u6210\u8303\u5f0f\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002", "method": "\u63d0\u51fa\u4e86Transition Matching (TM)\uff0c\u4e00\u79cd\u79bb\u6563\u65f6\u95f4\u3001\u8fde\u7eed\u72b6\u6001\u7684\u751f\u6210\u8303\u5f0f\uff0c\u5c06\u590d\u6742\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u7b80\u5355\u7684\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\u3002\u5177\u4f53\u5305\u62ec\uff1aDifference Transition Matching (DTM)\uff0c\u76f4\u63a5\u5b66\u4e60\u8f6c\u79fb\u6982\u7387\uff1bAutoregressive Transition Matching (ARTM)\uff0c\u90e8\u5206\u56e0\u679c\u6a21\u578b\uff1bFull History Transition Matching (FHTM)\uff0c\u5b8c\u5168\u56e0\u679c\u6a21\u578b\u3002", "result": "DTM\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff1bARTM\u548cFHTM\u5b9e\u73b0\u4e86\u4e0e\u975e\u56e0\u679c\u65b9\u6cd5\u76f8\u5f53\u7684\u8fde\u7eed\u56e0\u679cAR\u751f\u6210\u8d28\u91cf\uff0c\u4e14FHTM\u662f\u9996\u4e2a\u5728\u8fde\u7eed\u57df\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u6027\u80fd\u5ab2\u7f8e\u751a\u81f3\u8d85\u8fc7\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5\u7684\u5b8c\u5168\u56e0\u679c\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TM\u53d8\u4f53\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.23596", "pdf": "https://arxiv.org/pdf/2506.23596", "abs": "https://arxiv.org/abs/2506.23596", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnomaly to Prompt (A2P)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u5e38\u9884\u6d4b\uff08AP\uff09\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u7531\u5f02\u5e38\u611f\u77e5\u9884\u6d4b\uff08AAF\uff09\u548c\u5408\u6210\u5f02\u5e38\u63d0\u793a\uff08SAP\uff09\u7ec4\u6210\uff0c\u901a\u8fc7\u5b66\u4e60\u5f02\u5e38\u5173\u7cfb\u548c\u4f7f\u7528\u4fe1\u53f7\u81ea\u9002\u5e94\u63d0\u793a\u6765\u6a21\u62df\u591a\u6837\u5f02\u5e38\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cA2P\u5728\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5904\u7406\u65b9\u6cd5\u5728\u5f02\u5e38\u9884\u6d4b\uff08AP\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53ea\u80fd\u68c0\u6d4b\u5373\u65f6\u5f02\u5e38\u6216\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u672a\u6765\u7684\u5f02\u5e38\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6Anomaly to Prompt (A2P)\uff0c\u5305\u542bAnomaly-Aware Forecasting (AAF) \u548c Synthetic Anomaly Prompting (SAP)\u3002AAF\u901a\u8fc7\u5b66\u4e60\u5f02\u5e38\u5173\u7cfb\u6765\u9884\u6d4b\u5f02\u5e38\u65f6\u95f4\u70b9\uff0c\u800cSAP\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684Anomaly Prompt Pool (APP)\uff0c\u7528\u4fe1\u53f7\u81ea\u9002\u5e94\u63d0\u793a\u6765\u6a21\u62df\u591a\u79cd\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cA2P\u5728\u5f02\u5e38\u9884\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "A2P\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u672a\u6765\u5f02\u5e38\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2506.23629", "pdf": "https://arxiv.org/pdf/2506.23629", "abs": "https://arxiv.org/abs/2506.23629", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "comment": "7 pages, 2 figures, conference", "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u4f4e\u79e9\u8868\u793a\u6a21\u578b\uff08NLR\uff09\uff0c\u7528\u4e8e\u586b\u8865\u6c34\u8d28\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u5728\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c34\u8d28\u6570\u636e\u5b8c\u6574\u6027\u5bf9\u73af\u5883\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e2d\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u548c\u901a\u4fe1\u5ef6\u8fdf\u7b49\u95ee\u9898\u5bfc\u81f4\u5927\u91cf\u6570\u636e\u7f3a\u5931\uff0c\u4e14\u6570\u636e\u5448\u73b0\u9ad8\u7ef4\u7a00\u758f\u7279\u6027\u3002\u4f20\u7edf\u586b\u8865\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u6570\u636e\u7279\u5f81\uff0c\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u4f4e\u79e9\u8868\u793a\u6a21\u578b\uff08NLR\uff09\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002\u5229\u7528CNN\u5b9e\u73b0\u4e24\u4e2a\u76ee\u6807\uff1a1) \u878d\u5408\u65f6\u95f4\u7279\u5f81\u4ee5\u5efa\u6a21\u65f6\u9699\u95f4\u7684\u6570\u636e\u65f6\u95f4\u4f9d\u8d56\u6027\uff1b2) \u63d0\u53d6\u975e\u7ebf\u6027\u4ea4\u4e92\u548c\u5c40\u90e8\u6a21\u5f0f\u4ee5\u6316\u6398\u9ad8\u9636\u5173\u7cfb\u7279\u5f81\uff0c\u5b9e\u73b0\u591a\u7ef4\u4fe1\u606f\u7684\u6df1\u5ea6\u878d\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6c34\u8d28\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4f30\u8ba1\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u6570\u636e\u586b\u8865\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5904\u7406\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6c34\u8d28\u76d1\u6d4b\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.23679", "pdf": "https://arxiv.org/pdf/2506.23679", "abs": "https://arxiv.org/abs/2506.23679", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "title": "Learning Modular Exponentiation with Transformers", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a4\u5c42\u7684Transformer\u6a21\u578b\u8fdb\u884c\u6a21\u6307\u6570\u8fd0\u7b97\uff0c\u53d1\u73b0\u4e92\u60e0\u64cd\u4f5c\u6570\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u7b97\u672f\u7ed3\u6784\u7684\u5b66\u4e60\u673a\u5236\u3002\u6700\u540e\u4e00\u5c42\u7684\u6ce8\u610f\u529b\u5934\u5b50\u56fe\u8db3\u4ee5\u5b8c\u6210\u5e38\u89c4\u6307\u6570\u8fd0\u7b97\u4efb\u52a1\uff0c\u8868\u660eTransformer\u901a\u8fc7\u4e13\u7528\u8ba1\u7b97\u7535\u8def\u5b66\u4e60\u6a21\u7b97\u672f\u3002", "motivation": "\u6a21\u6307\u6570\u8fd0\u7b97\u5728\u6570\u8bba\u548c\u5bc6\u7801\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ece\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7684\u89d2\u5ea6\u5c1a\u672a\u6df1\u5165\u7814\u7a76\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a4\u5c42\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668Transformer\u6a21\u578b\u6267\u884c\u6a21\u6307\u6570\u8fd0\u7b97\uff0c\u5e76\u4f7f\u7528\u539f\u5219\u6027\u91c7\u6837\u7b56\u7565\u3001PCA\u5d4c\u5165\u5206\u6790\u548c\u6fc0\u6d3b\u8865\u4e01\u65b9\u6cd5\u7814\u7a76\u6570\u503c\u63a8\u7406\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u51fa\u73b0\u3002", "result": "\u4e92\u60e0\u64cd\u4f5c\u6570\u8bad\u7ec3\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u76f8\u5173\u6a21\u6570\u95f4\u6709\u7a81\u7136\u7684\u6cdb\u5316\u73b0\u8c61\uff1b\u6700\u540e\u4e00\u5c42\u7684\u6ce8\u610f\u529b\u5934\u5b50\u56fe\u8db3\u4ee5\u5b9e\u73b0\u5b8c\u5168\u6027\u80fd\u3002", "conclusion": "Transformer\u6a21\u578b\u901a\u8fc7\u4e13\u7528\u8ba1\u7b97\u7535\u8def\u5b66\u4e60\u6a21\u7b97\u672f\uff0c\u4e3a\u66f4\u53ef\u89e3\u91ca\u548c\u9ad8\u6548\u7684\u795e\u7ecf\u65b9\u6cd5\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2506.23719", "pdf": "https://arxiv.org/pdf/2506.23719", "abs": "https://arxiv.org/abs/2506.23719", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "AI": {"tldr": "\u63d0\u51faDABstep\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u5728\u591a\u6b65\u9aa4\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u65b0\u57fa\u51c6\u3002\u5b83\u5305\u542b450\u591a\u4e2a\u6765\u81ea\u91d1\u878d\u5206\u6790\u5e73\u53f0\u7684\u771f\u5b9e\u6311\u6218\uff0c\u9700\u8981\u6a21\u578b\u7ed3\u5408\u57fa\u4e8e\u4ee3\u7801\u7684\u6570\u636e\u5904\u7406\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002\u5c3d\u7ba1\u9886\u5148\u7684\u8bed\u8a00\u6a21\u578b\u5728\u6700\u96be\u7684\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a14.55%\uff0c\u4f46\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u5ba2\u89c2\u8bc4\u5206\u548c\u81ea\u52a8\u6b63\u786e\u6027\u68c0\u67e5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30AI\u5728\u771f\u5b9e\u4e16\u754c\u3001\u591a\u6b65\u9aa4\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDABstep\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u8d85\u8fc7450\u4e2a\u4ece\u91d1\u878d\u5206\u6790\u5e73\u53f0\u63d0\u53d6\u7684\u5b9e\u9645\u6311\u6218\uff0c\u8981\u6c42\u6a21\u578b\u6574\u5408\u4ee3\u7801\u6570\u636e\u5904\u7406\u548c\u5f02\u6784\u6587\u6863\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u8fed\u4ee3\u3001\u591a\u6b65\u9aa4\u7684\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\u3002", "result": "\u9876\u5c16LLM\u4ee3\u7406\u5728\u6700\u96be\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u4e3a14.55%\uff0c\u663e\u793a\u4e86\u73b0\u6709\u6280\u672f\u4e0e\u7406\u60f3\u6027\u80fd\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u540c\u65f6\u8be6\u7ec6\u62a5\u544a\u4e86\u57fa\u51c6\u8bbe\u8ba1\u3001\u6570\u636e\u96c6\u6784\u6210\u3001\u4efb\u52a1\u516c\u5f0f\u5316\u3001\u8bc4\u4f30\u534f\u8bae\u53ca\u57fa\u7ebf\u7ed3\u679c\u3002", "conclusion": "DABstep\u4e3a\u52a0\u901f\u81ea\u4e3b\u6570\u636e\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5171\u6392\u884c\u699c\u548c\u5de5\u5177\u5305\uff0c\u63ed\u793a\u4e86\u73b0\u6709AI\u4ee3\u7406\u5728\u590d\u6742\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2506.23726", "pdf": "https://arxiv.org/pdf/2506.23726", "abs": "https://arxiv.org/abs/2506.23726", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "title": "System-Embedded Diffusion Bridge Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u6865\u63a5\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7cfb\u7edf\u5d4c\u5165\u6269\u6563\u6865\u6a21\u578b\uff08SDBs\uff09\uff0c\u901a\u8fc7\u5c06\u5df2\u77e5\u7684\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\u663e\u5f0f\u5730\u5d4c\u5165\u5230\u77e9\u9635\u503cSDE\u7684\u7cfb\u6570\u4e2d\uff0c\u89e3\u51b3\u4e86\u5404\u79cd\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u4e4b\u95f4\u7684\u7cfb\u7edf\u9519\u914d\u4e0b\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u9006\u95ee\u9898\u7684\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u65e0\u76d1\u7763\u548c\u76d1\u7763\u4e24\u7c7b\uff0c\u4f46\u90fd\u5b58\u5728\u4e0d\u8db3\uff1a\u65e0\u76d1\u7763\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u77e5\u9053\u6d4b\u91cf\u6a21\u578b\uff0c\u800c\u76d1\u7763\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u7ed3\u6784\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u6d4b\u91cf\u6a21\u578b\u7ed3\u6784\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u7cfb\u7edf\u5d4c\u5165\u6269\u6563\u6865\u6a21\u578b\uff08SDBs\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u6865\u63a5\u65b9\u6cd5\uff0c\u5b83\u5c06\u5df2\u77e5\u7684\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\u660e\u786e\u5730\u5d4c\u5165\u5230\u77e9\u9635\u503c\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u7684\u7cfb\u6570\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u5229\u7528\u6d4b\u91cf\u6a21\u578b\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "result": "SDBs\u5728\u5404\u79cd\u7ebf\u6027\u9006\u95ee\u9898\u4e0a\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u9519\u914d\u7684\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SDBs\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u7ebf\u6027\u9006\u95ee\u9898\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2506.23731", "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubi\u0144ski", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff09\u7684\u6c34\u5370\u6280\u672f\u5bf9\u4e8e\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u56fe\u50cf\u4f7f\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u6cd5\u4fdd\u7559\u653e\u5c04\u6027\u7279\u5f81\uff0c\u800c\u5bf9\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u5219\u5c1a\u65e0\u653e\u5c04\u6027\u6c34\u5370\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u578b\u6c34\u5370\u6280\u672f\uff0c\u5176\u7075\u611f\u6765\u6e90\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bad\u7ec3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u6602\u3002\u4e00\u4e9b\u65b9\u53ef\u80fd\u5229\u7528\u5df2\u6709\u7684\u751f\u6210\u56fe\u50cf\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u56fe\u50cf\u4f7f\u7528\u3002\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u4f46\u524d\u63d0\u662f\u6c34\u5370\u5728\u65b0\u6a21\u578b\u7684\u8f93\u51fa\u4e2d\u4ecd\u7136\u53ef\u8bc6\u522b\uff0c\u5373\u5177\u5907\u653e\u5c04\u6027\u7279\u6027\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u6c34\u5370\u653e\u5c04\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u6269\u6563\u6a21\u578b\u4e2d\u7684\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u653e\u5c04\u6027\u3002\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u653e\u5c04\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6709\u6548\u5730\u4fdd\u7559\u4e86\u653e\u5c04\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u6765\u6e90\u8ffd\u8e2a\uff0c\u5e76\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u56fe\u50cf\u4f7f\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6c34\u5370\u65b9\u6cd5\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u653e\u5c04\u6027\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5f53\u524d\u6280\u672f\u7a7a\u767d\uff0c\u5e76\u6709\u52a9\u4e8e\u4fdd\u62a4\u751f\u6210\u56fe\u50cf\u7684\u77e5\u8bc6\u4ea7\u6743\u3002"}}
{"id": "2506.23776", "pdf": "https://arxiv.org/pdf/2506.23776", "abs": "https://arxiv.org/abs/2506.23776", "authors": ["Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Model-driven Stochastic Trace Clustering", "categories": ["cs.LG"], "comment": null, "summary": "Process discovery algorithms automatically extract process models from event\nlogs, but high variability often results in complex and hard-to-understand\nmodels. To mitigate this issue, trace clustering techniques group process\nexecutions into clusters, each represented by a simpler and more understandable\nprocess model. Model-driven trace clustering improves on this by assigning\ntraces to clusters based on their conformity to cluster-specific process\nmodels. However, most existing clustering techniques rely on either no process\nmodel discovery, or non-stochastic models, neglecting the frequency or\nprobability of activities and transitions, thereby limiting their capability to\ncapture real-world execution dynamics. We propose a novel model-driven trace\nclustering method that optimizes stochastic process models within each cluster.\nOur approach uses entropic relevance, a stochastic conformance metric based on\ndirectly-follows probabilities, to guide trace assignment. This allows\nclustering decisions to consider both structural alignment with a cluster's\nprocess model and the likelihood that a trace originates from a given\nstochastic process model. The method is computationally efficient, scales\nlinearly with input size, and improves model interpretability by producing\nclusters with clearer control-flow patterns. Extensive experiments on public\nreal-life datasets show that our method outperforms existing alternatives in\nrepresenting process behavior and reveals how clustering performance rankings\ncan shift when stochasticity is considered.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u9a71\u52a8\u7684\u8ff9\u7ebf\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6bcf\u4e2a\u805a\u7c7b\u4e2d\u7684\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8ff9\u7ebf\u805a\u7c7b\u6280\u672f\u8981\u4e48\u6ca1\u6709\u4f7f\u7528\u8fc7\u7a0b\u6a21\u578b\u53d1\u73b0\uff0c\u8981\u4e48\u4f7f\u7528\u975e\u968f\u673a\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u6d3b\u52a8\u548c\u8f6c\u6362\u7684\u9891\u7387\u6216\u6982\u7387\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u6267\u884c\u52a8\u6001\u7684\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u71b5\u76f8\u5173\u6027\uff08\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u8ddf\u968f\u6982\u7387\u7684\u968f\u673a\u4e00\u81f4\u6027\u5ea6\u91cf\uff09\u6765\u6307\u5bfc\u8ff9\u7ebf\u5206\u914d\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u805a\u7c7b\u51b3\u7b56\u540c\u65f6\u8003\u8651\u4e0e\u805a\u7c7b\u8fc7\u7a0b\u6a21\u578b\u7684\u7ed3\u6784\u5bf9\u9f50\u4ee5\u53ca\u8ff9\u7ebf\u6765\u6e90\u4e8e\u7ed9\u5b9a\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8868\u793a\u8fc7\u7a0b\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u8003\u8651\u968f\u673a\u6027\u65f6\u805a\u7c7b\u6027\u80fd\u6392\u540d\u7684\u53d8\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u968f\u8f93\u5165\u89c4\u6a21\u7ebf\u6027\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5177\u6709\u66f4\u6e05\u6670\u63a7\u5236\u6d41\u6a21\u5f0f\u7684\u805a\u7c7b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.23782", "pdf": "https://arxiv.org/pdf/2506.23782", "abs": "https://arxiv.org/abs/2506.23782", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWavelet-Aware Temperature Scaling (WATS)\u7684\u540e\u5904\u7406\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8c03\u8c10\u70ed\u6838\u56fe\u5c0f\u6ce2\u7279\u5f81\u4e3a\u8282\u70b9\u5206\u914d\u7279\u5b9a\u6e29\u5ea6\uff0c\u4ece\u800c\u6539\u8fdb\u4e86GNNs\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u4f30\u8ba1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cWATS\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6700\u4f4e\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee(ECE)\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u56fe\u611f\u77e5\u6821\u51c6\u65b9\u6cd5\u8bd5\u56fe\u7f13\u89e3GNN\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7c97\u7565\u7684\u4e00\u9636\u7edf\u8ba1\u91cf\u6216\u6f5c\u5728\u8282\u70b9\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u56fe\u62d3\u6251\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51fa\u4e86Wavelet-Aware Temperature Scaling (WATS)\uff0c\u4e00\u79cd\u57fa\u4e8e\u53ef\u8c03\u8c10\u70ed\u6838\u56fe\u5c0f\u6ce2\u7279\u5f81\u4e3a\u8282\u70b9\u5206\u914d\u7279\u5b9a\u6e29\u5ea6\u7684\u540e\u5904\u7406\u6821\u51c6\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u56fe\u5c0f\u6ce2\u7684\u53ef\u6269\u5c55\u6027\u548c\u62d3\u6251\u654f\u611f\u6027\u6765\u6539\u8fdb\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u8bbf\u95ee\u90bb\u8fd1logits\u6216\u9884\u6d4b\u3002", "result": "\u5728\u4e03\u4e2a\u5177\u6709\u4e0d\u540c\u56fe\u7ed3\u6784\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2aGNN\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cWATS\u5728\u6240\u6709\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684ECE\uff0c\u6bd4\u7ecf\u5178\u548c\u56fe\u7279\u5b9a\u57fa\u7ebf\u9ad8\u51fa\u6700\u591a42.3%\u7684ECE\uff0c\u5e76\u4e14\u5c06\u6821\u51c6\u65b9\u5dee\u5e73\u5747\u51cf\u5c11\u4e8617.24%\u3002\u6b64\u5916\uff0cWATS\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6269\u5c55\u5230\u4e0d\u540c\u5927\u5c0f\u548c\u5bc6\u5ea6\u7684\u56fe\u3002", "conclusion": "WATS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u5904\u7406\u6821\u51c6\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u56fe\u7ed3\u6784\u548cGNN\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u6821\u51c6\u6027\u80fd\u3002"}}
{"id": "2506.23799", "pdf": "https://arxiv.org/pdf/2506.23799", "abs": "https://arxiv.org/abs/2506.23799", "authors": ["Jiongli Zhu", "Parjanya Prajakta Prashant", "Alex Cloninger", "Babak Salimi"], "title": "KAIROS: Scalable Model-Agnostic Data Valuation", "categories": ["cs.LG"], "comment": "19 pages, 9 figures", "summary": "Training data increasingly shapes not only model accuracy but also regulatory\ncompliance and market valuation of AI assets. Yet existing valuation methods\nremain inadequate: model-based techniques depend on a single fitted model and\ninherit its biases, while algorithm-based approaches such as Data Shapley\nrequire costly retrainings at web scale. Recent Wasserstein-based\nmodel-agnostic methods rely on approximations that misrank examples relative to\ntheir true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,\nmodel-agnostic valuation framework that assigns each example a distributional\ninfluence score: its contribution to the Maximum Mean Discrepancy (MMD) between\nthe empirical training distribution and a clean reference set. Unlike\nWasserstein surrogates, our MMD-based influence admits a closed-form solution\nthat faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,\nrequires no retraining, and naturally extends to conditional kernels for\nunified label- and feature-error detection. Moreover, KAIROS supports efficient\nonline updates: when a new batch of size m arrives, all scores can be updated\nin $O(mN)$ time, delivering up to 50x speedup without compromising ranking\nquality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks\nshow that KAIROS consistently outperforms state-of-the-art model-, Shapley-,\nand Wasserstein-based baselines in both accuracy and runtime. We provide\nrigorous theoretical guarantees, including symmetry for reproducible rankings\nand density-separation for interpretable thresholds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKAIROS\u7684\u53ef\u6269\u5c55\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6570\u636e\u4f30\u503c\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u914d\u6bcf\u4e2a\u6837\u672c\u5206\u5e03\u5f71\u54cd\u5206\u6570\u6765\u8bc4\u4f30\u5176\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8d21\u732e\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u5e76\u7ee7\u627f\u5176\u504f\u5dee\uff1b\u57fa\u4e8e\u7b97\u6cd5\u7684\u65b9\u6cd5\uff08\u5982Data Shapley\uff09\u9700\u8981\u6602\u8d35\u7684\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\uff1bWasserstein-based\u65b9\u6cd5\u7684\u8fd1\u4f3c\u503c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u6392\u5e8f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u3002", "method": "KAIROS\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u4f30\u503c\u6846\u67b6\uff0c\u5b83\u4e3a\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u4e00\u4e2a\u5206\u5e03\u5f71\u54cd\u5206\u6570\uff0c\u8868\u793a\u5176\u5bf9\u7ecf\u9a8c\u8bad\u7ec3\u5206\u5e03\u4e0e\u5e72\u51c0\u53c2\u8003\u96c6\u4e4b\u95f4\u7684\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7684\u8d21\u732e\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u95ed\u5f0f\u89e3\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u80fd\u81ea\u7136\u6269\u5c55\u5230\u6761\u4ef6\u6838\u4ee5\u68c0\u6d4b\u6807\u7b7e\u548c\u7279\u5f81\u9519\u8bef\u3002\u6b64\u5916\uff0cKAIROS\u652f\u6301\u9ad8\u6548\u7684\u5728\u7ebf\u66f4\u65b0\u3002", "result": "\u5728\u566a\u58f0\u3001\u9519\u8bef\u6807\u8bb0\u548c\u4e2d\u6bd2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cKAIROS\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3001Shapley\u548cWasserstein-based\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "KAIROS\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u9644\u6709\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u53ef\u91cd\u590d\u7684\u6392\u5e8f\u5bf9\u79f0\u6027\u548c\u53ef\u89e3\u91ca\u7684\u5bc6\u5ea6\u5206\u79bb\u9608\u503c\u3002"}}
{"id": "2506.23845", "pdf": "https://arxiv.org/pdf/2506.23845", "abs": "https://arxiv.org/abs/2506.23845", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "AI": {"tldr": "\u5c3d\u7ba1\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5f15\u53d1\u4e86\u6781\u5927\u7684\u5173\u6ce8\uff0c\u4f46\u4e00\u7cfb\u5217\u8d1f\u9762\u7ed3\u679c\u589e\u52a0\u4e86\u5bf9\u5176\u6709\u7528\u6027\u7684\u6000\u7591\u3002\u672c\u6587\u901a\u8fc7\u6982\u5ff5\u533a\u5206\u8c03\u548c\u4e86\u56f4\u7ed5SAEs\u7684\u7ade\u4e89\u6027\u53d9\u8ff0\uff0c\u6307\u51faSAEs\u5728\u5904\u7406\u5df2\u77e5\u6982\u5ff5\u65f6\u53ef\u80fd\u6548\u679c\u8f83\u5dee\uff0c\u4f46\u5728\u53d1\u73b0\u672a\u77e5\u6982\u5ff5\u65b9\u9762\u975e\u5e38\u5f3a\u5927\u3002\u6b64\u7814\u7a76\u8fd8\u6982\u8ff0\u4e86SAEs\u5728\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u3001\u5ba1\u8ba1\u548c\u5b89\u5168\u6027\u4ee5\u53ca\u793e\u4f1a\u79d1\u5b66\u548c\u5065\u5eb7\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u8c03\u548c\u5173\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7684\u6b63\u9762\u548c\u8d1f\u9762\u89c2\u70b9\uff0c\u5e76\u660e\u786e\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u53d1\u73b0\u672a\u77e5\u6982\u5ff5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u533a\u5206\u4ee5\u7406\u89e3SAEs\u7684\u6709\u6548\u6027\uff1b\u5f3a\u8c03SAEs\u5728\u53d1\u73b0\u672a\u77e5\u6982\u5ff5\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u673a\u5668\u5b66\u4e60\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u3001\u5ba1\u8ba1\u548c\u5b89\u5168\u6027\u4ee5\u53ca\u793e\u4f1a\u79d1\u5b66\u548c\u5065\u5eb7\u79d1\u5b66\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "result": "\u6210\u529f\u5730\u5c06SAEs\u7684\u6b63\u9762\u4e0e\u8d1f\u9762\u7ed3\u679c\u5206\u5f00\uff0c\u5e76\u63d0\u51fa\u4e86SAEs\u5728\u591a\u4e2a\u9886\u57df\u7684\u5177\u4f53\u4f7f\u7528\u6848\u4f8b\u3002", "conclusion": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u5904\u7406\u5df2\u77e5\u6982\u5ff5\u65f6\u53ef\u80fd\u4e0d\u5982\u5176\u4ed6\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u5728\u63a2\u7d22\u672a\u77e5\u6982\u5ff5\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u8de8\u5b66\u79d1\u9886\u57df\u3002"}}
{"id": "2506.23800", "pdf": "https://arxiv.org/pdf/2506.23800", "abs": "https://arxiv.org/abs/2506.23800", "authors": ["Chang Qi", "Matteo Forasassi", "Thomas Lukasiewicz", "Tommaso Salvatori"], "title": "Towards the Training of Deeper Predictive Coding Neural Networks", "categories": ["cs.LG"], "comment": "18 Pages, 7 figures", "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.", "AI": {"tldr": "\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u901a\u8fc7\u5e73\u8861\u4f20\u64ad\u8bad\u7ec3\uff0c\u5728\u6d45\u5c42\u67b6\u6784\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8d85\u8fc7\u4e94\u5230\u4e03\u5c42\u7684\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u4f18\u5316\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6743\u91cd\u66f4\u65b0\u673a\u5236\u4ee5\u51cf\u5c11\u6df1\u5c42\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u4ece\u800c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u6027\u80fd\u63a5\u8fd1\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u5728\u6d45\u5c42\u67b6\u6784\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u968f\u7740\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\uff0c\u5176\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5c42\u95f4\u8bef\u5dee\u4e0d\u5e73\u8861\u4ee5\u53ca\u524d\u4e00\u5c42\u9884\u6d4b\u65e0\u6cd5\u6709\u6548\u5f15\u5bfc\u6df1\u5c42\u66f4\u65b0\u3002", "method": "1. \u5f15\u5165\u4e24\u79cd\u57fa\u4e8e\u7cbe\u5ea6\u52a0\u6743\u7684\u65b0\u65b9\u6cd5\u6765\u91cd\u65b0\u5e73\u8861\u677e\u5f1b\u9636\u6bb5\u5404\u5c42\u4e4b\u95f4\u7684\u80fd\u91cf\u5206\u5e03\u30022. \u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6743\u91cd\u66f4\u65b0\u673a\u5236\u4ee5\u51cf\u5c11\u6df1\u5c42\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u8868\u660e\uff0c\u5bf9\u4e8e\u8d85\u8fc7\u4e03\u5c42\u7684\u7f51\u7edc\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u6709\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u4e0e\u7c7b\u4f3c\u6a21\u578b\u7684\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u66f4\u597d\u5730\u7406\u89e3\u677e\u5f1b\u9636\u6bb5\u5bf9\u4e8e\u5927\u89c4\u6a21\u4f7f\u7528\u5e73\u8861\u4f20\u64ad\u8bad\u7ec3\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.23875", "pdf": "https://arxiv.org/pdf/2506.23875", "abs": "https://arxiv.org/abs/2506.23875", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 10 figures", "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u91cd\u65b0\u6392\u5217\u89e3\u7801\u5668\u8f93\u5165\u6807\u8bb0\uff0c\u4e3aTransformer\u627e\u5230\u66f4\u6613\u4e8e\u5b66\u4e60\u7684\u5e8f\u5217\u987a\u5e8f\uff0c\u5e76\u5728\u7b97\u672f\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "Transformer\u4e2d\u7684\u601d\u7ef4\u94fe\u5bf9\u4e8e\u9010\u6b65\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8fd9\u4e9b\u4e2d\u95f4\u6b65\u9aa4\u7684\u987a\u5e8f\u4f1a\u663e\u8457\u5f71\u54cd\u63a8\u7406\u96be\u5ea6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u2014\u2014\u89e3\u5f00\u601d\u7ef4\u94fe\uff0c\u5373\u91cd\u65b0\u6392\u5217\u89e3\u7801\u5668\u8f93\u5165\u6807\u8bb0\u4ee5\u5f62\u6210\u66f4\u6613\u4e8eTransformer\u5b66\u4e60\u7684\u5e8f\u5217\u3002", "method": "\u9996\u5148\u5728\u4e00\u4e2a\u7531\u4e0d\u540c\u987a\u5e8f\u7684\u76ee\u6807\u5e8f\u5217\u7ec4\u6210\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3Transformer\uff0c\u7136\u540e\u5c06\u90a3\u4e9b\u5728\u8bad\u7ec3\u521d\u671f\u635f\u5931\u4e0b\u964d\u8fc5\u901f\u7684\u987a\u5e8f\u8bc6\u522b\u4e3a\u6709\u5229\u987a\u5e8f\u3002\u7531\u4e8e\u641c\u7d22\u7a7a\u95f4\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u5448\u9636\u4e58\u589e\u957f\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u5206\u5c42\u65b9\u6cd5\uff0c\u5206\u522b\u8fdb\u884c\u5757\u95f4\u548c\u5757\u5185\u7684\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5728\u56db\u4e2a\u5bf9\u987a\u5e8f\u654f\u611f\u7684\u7b97\u672f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u6570\u5341\u4ebf\u5019\u9009\u987a\u5e8f\u4e2d\u8bc6\u522b\u51fa\u6709\u5229\u4e8e\u5b66\u4e60\u7684\u987a\u5e8f\u3002\u7279\u522b\u662f\u5728\u4e58\u6cd5\u4efb\u52a1\u4e0a\uff0c\u5b83\u6062\u590d\u4e86\u4e4b\u524d\u7814\u7a76\u4e2d\u62a5\u544a\u7684\u53cd\u5411\u6570\u5b57\u987a\u5e8f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u91cd\u65b0\u6392\u5217\u89e3\u7801\u5668\u8f93\u5165\u6807\u8bb0\uff0c\u4ece\u800c\u5e2e\u52a9Transformer\u66f4\u597d\u5730\u5b66\u4e60\u7b97\u672f\u4efb\u52a1\u3002"}}
{"id": "2506.23802", "pdf": "https://arxiv.org/pdf/2506.23802", "abs": "https://arxiv.org/abs/2506.23802", "authors": ["Konstantinos Bourazas", "Savvas Papaioannou", "Panayiotis Kolios"], "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations", "categories": ["cs.LG"], "comment": "23rd European Control Conference (ECC 2025), Thessaloniki, Greece,\n  24-27 June 2025", "summary": "In this work we introduce a novel adaptive anomaly detection framework\nspecifically designed for monitoring sequential random finite set (RFS)\nobservations. Our approach effectively distinguishes between In-Control data\n(normal) and Out-Of-Control data (anomalies) by detecting deviations from the\nexpected statistical behavior of the process. The primary contributions of this\nstudy include the development of an innovative RFS-based framework that not\nonly learns the normal behavior of the data-generating process online but also\ndynamically adapts to behavioral shifts to accurately identify abnormal point\npatterns. To achieve this, we introduce a new class of RFS-based posterior\ndistributions, named Power Discounting Posteriors (PD), which facilitate\nadaptation to systematic changes in data while enabling anomaly detection of\npoint pattern data through a novel predictive posterior density function. The\neffectiveness of the proposed approach is demonstrated by extensive qualitative\nand quantitative simulation experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u63a7\u987a\u5e8f\u968f\u673a\u6709\u9650\u96c6\uff08RFS\uff09\u89c2\u6d4b\u503c\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8eRFS\u7684\u540e\u9a8c\u5206\u5e03\u65b0\u7c7b\u522b\uff08Power Discounting Posteriors, PD\uff09\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6b63\u5e38\u884c\u4e3a\uff0c\u8fd8\u80fd\u52a8\u6001\u9002\u5e94\u884c\u4e3a\u53d8\u5316\u4ee5\u51c6\u786e\u8bc6\u522b\u5f02\u5e38\u70b9\u6a21\u5f0f\u3002\u5927\u91cf\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6a21\u62df\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5728\u76d1\u63a7\u987a\u5e8f\u968f\u673a\u6709\u9650\u96c6\uff08RFS\uff09\u89c2\u6d4b\u503c\u65f6\uff0c\u7f3a\u4e4f\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u5e76\u51c6\u786e\u8bc6\u522b\u5f02\u5e38\u70b9\u6a21\u5f0f\u7684\u6846\u67b6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u53ef\u4ee5\u5728\u7ebf\u5b66\u4e60\u6b63\u5e38\u884c\u4e3a\u5e76\u9002\u5e94\u884c\u4e3a\u53d8\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5f00\u53d1\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u57fa\u4e8eRFS\u7684\u6846\u67b6\uff0c\u53ef\u5728\u7ebf\u5b66\u4e60\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6b63\u5e38\u884c\u4e3a\u3002\n2. \u5f15\u5165\u4e86\u540d\u4e3aPower Discounting Posteriors (PD)\u7684\u65b0\u7c7b\u522b\u7684RFS\u57fa\u7840\u540e\u9a8c\u5206\u5e03\uff0c\u7528\u4e8e\u9002\u5e94\u6570\u636e\u7cfb\u7edf\u6027\u53d8\u5316\u3002\n3. \u5229\u7528\u9884\u6d4b\u540e\u9a8c\u5bc6\u5ea6\u51fd\u6570\u5b9e\u73b0\u70b9\u6a21\u5f0f\u6570\u636e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6a21\u62df\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eRFS\u7684\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u987a\u5e8fRFS\u89c2\u6d4b\u503c\u4e2d\u5f02\u5e38\u70b9\u6a21\u5f0f\u7684\u51c6\u786e\u8bc6\u522b\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u884c\u4e3a\u53d8\u5316\u3002\u8fd9\u4e3a\u76f8\u5173\u9886\u57df\u7684\u76d1\u63a7\u548c\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23923", "pdf": "https://arxiv.org/pdf/2506.23923", "abs": "https://arxiv.org/abs/2506.23923", "authors": ["Miguel Camacho-S\u00e1nchez", "Fernando Garc\u00eda-Torres", "Jesper John Lisegaard", "Roc\u00edo del Amor", "Sankhya Mohanty", "Valery Naranjo"], "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u540c\u6b65\u5177\u6709\u4e24\u4e2a\u6811\u8102\u5165\u53e3\u548c\u4e00\u4e2a\u51fa\u53e3\u7684\u704c\u6ce8\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u6811\u8102\u6d41\u52a8\u524d\u6cbf\u3002\u901a\u8fc7\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u89e3\u51b3\u4e86\u6d41\u4f53\u52a8\u529b\u5b66\u7ba1\u7406\u7684\u6311\u6218\u3002\u7ed3\u679c\u8868\u660e\uff0cRL\u65b9\u6cd5\u5728\u5b9e\u73b0\u7cbe\u786e\u7684\u6d41\u52a8\u6536\u655b\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u5408\u6750\u6599\u5236\u9020\u4e2d\u6539\u8fdb\u8fc7\u7a0b\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u7684\u6f5c\u529b\u3002", "motivation": "\u6811\u8102\u6ce8\u5165\uff08RI\uff09\u548c\u6811\u8102\u4f20\u9012\u6a21\u5851\uff08RTM\uff09\u662f\u5236\u9020\u9ad8\u6027\u80fd\u7ea4\u7ef4\u589e\u5f3a\u805a\u5408\u7269\u590d\u5408\u6750\u6599\u7684\u5173\u952e\u5de5\u827a\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u5e94\u7528\u5982\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u4e2d\u3002\u63a7\u5236\u8fd9\u4e9b\u8fc7\u7a0b\u4e2d\u7684\u6811\u8102\u6d41\u52a8\u52a8\u6001\u5bf9\u4e8e\u786e\u4fdd\u7ea4\u7ef4\u589e\u5f3a\u6750\u6599\u7684\u5747\u5300\u6d78\u6da6\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u9632\u6b62\u5f71\u54cd\u6700\u7ec8\u90e8\u4ef6\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u6b8b\u4f59\u5b54\u9699\u7387\u548c\u5e72\u6591\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\uff0c\u5229\u7528\u8fc7\u7a0b\u6a21\u62df\u5efa\u7acb\u6a21\u578b\uff0c\u4ee5\u540c\u6b65\u6d89\u53ca\u4e24\u4e2a\u6811\u8102\u5165\u53e3\u548c\u4e00\u4e2a\u51fa\u53e3\u7684\u704c\u6ce8\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u6811\u8102\u6d41\u52a8\u524d\u6cbf\u3002\u5177\u4f53\u4f7f\u7528\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u6765\u5e94\u5bf9\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u7ba1\u7406\u6311\u6218\u3002", "result": "\u7ed3\u679c\u8bc1\u660e\u4e86RL\u65b9\u6cd5\u5728\u5b9e\u73b0\u7cbe\u786e\u6d41\u52a8\u6536\u655b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u6539\u5584\u590d\u5408\u6750\u6599\u5236\u9020\u8fc7\u7a0b\u4e2d\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u63a7\u5236\u6811\u8102\u6d41\u52a8\u52a8\u6001\u3001\u6539\u5584\u590d\u5408\u6750\u6599\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2506.23803", "pdf": "https://arxiv.org/pdf/2506.23803", "abs": "https://arxiv.org/abs/2506.23803", "authors": ["Dmitry Kovalev"], "title": "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type\npreconditioning. Our contributions are twofold. First, we develop a unified\nconvergence analysis of SGD with adaptive preconditioning under anisotropic or\nmatrix smoothness and noise assumptions. This allows us to recover\nstate-of-the-art convergence results for several popular adaptive gradient\nmethods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In\naddition, we establish the fundamental connection between two recently proposed\nalgorithms, Scion and DASGO, and provide the first theoretical guarantees for\nthe latter. Second, we show that the convergence of methods like AdaGrad and\nDASGO can be provably accelerated beyond the best-known rates using Nesterov\nmomentum. Consequently, we obtain the first theoretical justification that\nAdaGrad-type algorithms can simultaneously benefit from both diagonal\npreconditioning and momentum, which may provide an ultimate explanation for the\npractical efficiency of Adam.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5e26\u6709AdaGrad\u578b\u9884\u8c03\u8282\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD)\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6536\u655b\u6027\u5206\u6790\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u5177\u6709\u5404\u5411\u5f02\u6027\u6216\u77e9\u9635\u5e73\u6ed1\u6027\u548c\u566a\u58f0\u5047\u8bbe\u7684\u81ea\u9002\u5e94\u9884\u8c03\u8282SGD\u3002\u5176\u6b21\uff0c\u63ed\u793a\u4e86Scion\u548cDASGO\u4e4b\u95f4\u7684\u57fa\u672c\u8054\u7cfb\uff0c\u5e76\u9996\u6b21\u4e3a\u540e\u8005\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u6700\u540e\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7Nesterov\u52a8\u91cf\u53ef\u4ee5\u52a0\u901fAdaGrad\u548cDASGO\u7b49\u65b9\u6cd5\u7684\u6536\u655b\uff0c\u4ece\u800c\u4e3aAdam\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u5c3d\u7ba1AdaGrad\u3001Adam\u7b49\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u6027\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002\u7279\u522b\u662f\u5173\u4e8e\u81ea\u9002\u5e94\u9884\u8c03\u8282\u4e0e\u52a8\u91cf\u7ed3\u5408\u7684\u6548\u679c\u5c1a\u672a\u6709\u660e\u786e\u7684\u7406\u8bba\u89e3\u91ca\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u6df1\u5165\u5206\u6790\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6536\u655b\u6027\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5177\u6709\u5404\u5411\u5f02\u6027\u6216\u77e9\u9635\u5e73\u6ed1\u6027\u548c\u566a\u58f0\u5047\u8bbe\u7684\u81ea\u9002\u5e94\u9884\u8c03\u8282SGD\u3002\n2. \u5efa\u7acb\u4e86Scion\u548cDASGO\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5e76\u4e3aDASGO\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\u3002\n3. \u8bc1\u660e\u4e86\u901a\u8fc7\u5f15\u5165Nesterov\u52a8\u91cf\uff0c\u53ef\u4ee5\u52a0\u901fAdaGrad\u548cDASGO\u7b49\u65b9\u6cd5\u7684\u6536\u655b\u3002", "result": "1. \u6062\u590d\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\uff08\u5982AdaGrad-Norm\u3001AdaGrad\u548cASGO/One-sided Shampoo\uff09\u7684\u6700\u65b0\u6536\u655b\u7ed3\u679c\u3002\n2. \u9996\u6b21\u4e3aDASGO\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\n3. \u8bc1\u5b9e\u4e86Nesterov\u52a8\u91cf\u80fd\u591f\u52a0\u901fAdaGrad\u548cDASGO\u7b49\u65b9\u6cd5\u7684\u6536\u655b\u3002\n4. \u4e3aAdam\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u6269\u5c55\u4e86\u5bf9\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u7684\u7406\u89e3\uff0c\u8fd8\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5b9e\u9645\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002\u901a\u8fc7\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u548c\u52a8\u91cf\u6280\u672f\u7684\u5e94\u7528\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6536\u655b\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u4f18\u5316\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23960", "pdf": "https://arxiv.org/pdf/2506.23960", "abs": "https://arxiv.org/abs/2506.23960", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u4fee\u590d\u65b9\u6cd5ADReFT\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u5931\u8d25\u6d4b\u8bd5\u6765\u8bc6\u522b\u5173\u952e\u72b6\u6001\u5e76\u751f\u6210\u9002\u5f53\u7684\u7f13\u89e3\u63aa\u65bd\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Transformer\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u4fee\u590d\u65b9\u6848\u901a\u5e38\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e14\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u5bfc\u81f4\u4fee\u590d\u6548\u679c\u4e0d\u4f73\uff0c\u65e2\u65e0\u6cd5\u5145\u5206\u7f13\u89e3\u5b89\u5168\u98ce\u9669\uff0c\u53c8\u964d\u4f4e\u4e86\u9a7e\u9a76\u4f53\u9a8c\u3002", "method": "1. \u63d0\u51faADReFT\u65b9\u6cd5\uff1a\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u4ece\u5931\u8d25\u6d4b\u8bd5\u4e2d\u8bc6\u522b\u5b89\u5168\u5173\u952e\u72b6\u6001\uff0c\u5e76\u751f\u6210\u9002\u5f53\u7684\u7f13\u89e3\u52a8\u4f5c\u3002\n2. \u4f7f\u7528\u53cc\u8054\u5408\u5934\uff08State Monitor\u548cDecision Adapter\uff09\u7684Transformer\u6a21\u578b\u6355\u83b7\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4ea4\u4e92\u3002\n3. \u5229\u7528\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7c97\u7565\u6807\u6ce8\u72b6\u6001\u4ee5\u5efa\u7acb\u57fa\u7840\u80fd\u529b\u3002\n4. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff0c\u751f\u6210\u66f4\u7cbe\u786e\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4fee\u590d\u51b3\u7b56\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cADReFT\u5728\u4fee\u590d\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADReFT\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u5728\u7ebf\u4fee\u590d\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8fd0\u884c\u65f6\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.23824", "pdf": "https://arxiv.org/pdf/2506.23824", "abs": "https://arxiv.org/abs/2506.23824", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u53ef\u5fae\u805a\u7c7b\u6a21\u5757\uff0c\u5c06\u5e95\u5c42\u805a\u7c7b\u5047\u8bbe\u663e\u5f0f\u5730\u7eb3\u5165SSL\u4e2d\uff0c\u5229\u7528\u6807\u6ce8\u6570\u636e\u6307\u5bfc\u805a\u7c7b\u4e2d\u5fc3\uff0c\u5f62\u6210\u4e00\u79cd\u7b80\u5355\u4e14\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u6df1\u5ea6SSL\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u4f18\u4e8e\u4ec5\u76d1\u7763\u57fa\u7ebf\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5176\u4ed6SSL\u65b9\u6cd5\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\u4e0a\uff0c\u8fd9\u901a\u5e38\u5bfc\u81f4\u590d\u6742\u7684\u8bad\u7ec3\u7b56\u7565\u3002\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u5e76\u66f4\u76f4\u63a5\u5730\u5229\u7528\u805a\u7c7b\u5047\u8bbe\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6700\u8fd1\u63d0\u51fa\u7684\u53ef\u5fae\u805a\u7c7b\u6a21\u5757\uff0c\u5c06\u805a\u7c7b\u5047\u8bbe\u663e\u5f0f\u5730\u7eb3\u5165\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u3002\u5229\u7528\u6807\u6ce8\u6570\u636e\u6307\u5bfc\u805a\u7c7b\u4e2d\u5fc3\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u79cd\u7b80\u5355\u4e14\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u6df1\u5ea6SSL\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4ec5\u76d1\u7763\u57fa\u7ebf\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5176\u4ed6SSL\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u6a21\u578b\u7ed3\u6784\uff0c\u8fd8\u63d0\u9ad8\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u5c06\u805a\u7c7b\u5047\u8bbe\u663e\u5f0f\u7eb3\u5165SSL\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.24018", "pdf": "https://arxiv.org/pdf/2506.24018", "abs": "https://arxiv.org/abs/2506.24018", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u94fe\u63a5\u8868\u793a\u4e0a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6bd4\u8f83\u73b0\u6709\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\u5728\u5bf9\u79f0\u6027\u8f83\u9ad8\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5c3d\u7ba1GNNs\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8ba1\u7b97\u8282\u70b9\u5bf9\u7684\u8868\u793a\uff0c\u4f46\u5bf9\u5176\u8868\u8fbe\u80fd\u529b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u56fe\u7ea7\u522b\u7684\u8868\u793a\uff0c\u800c\u7f3a\u4e4f\u5bf9\u94fe\u63a5\u8868\u793a\u7684\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u2014\u2014$k_\rho$-$k_\u001aho$-$m$\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6db5\u76d6\u4e86\u73b0\u6709\u7684\u6d88\u606f\u4f20\u9012\u94fe\u63a5\u6a21\u578b\uff0c\u5e76\u5141\u8bb8\u8fdb\u884c\u6b63\u5f0f\u7684\u8868\u8fbe\u80fd\u529b\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u4e13\u95e8\u4e3a\u8bc4\u4f30\u94fe\u63a5\u7ea7\u522b\u8868\u8fbe\u80fd\u529b\u8bbe\u8ba1\u7684\u7b2c\u4e00\u4e2a\u57fa\u51c6\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u7684\u6846\u67b6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4f5c\u8005\u63a8\u5bfc\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u6790\u672a\u6765\u67b6\u6784\u7684\u7406\u8bba\u5de5\u5177\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\uff0c\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u5bf9\u79f0\u6027\u8f83\u9ad8\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5b83\u4eec\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u7684\u6a21\u578b\u3002", "conclusion": "\u8868\u8fbe\u80fd\u529b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u786e\u5b9e\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5bf9\u79f0\u6027\u7684\u6570\u636e\u96c6\u65f6\u3002\u8fd9\u5f3a\u8c03\u4e86\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u9009\u62e9\u5408\u9002\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.23843", "pdf": "https://arxiv.org/pdf/2506.23843", "abs": "https://arxiv.org/abs/2506.23843", "authors": ["Joris Bekkers"], "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment", "categories": ["cs.LG"], "comment": null, "summary": "Understanding team formations and player positioning is crucial for tactical\nanalysis in football (soccer). This paper presents a flexible method for\nformation recognition and player position assignment in football using\npredefined static formation templates and cost minimization from spatiotemporal\ntracking data, called EFPI. Our approach employs linear sum assignment to\noptimally match players to positions within a set of template formations by\nminimizing the total distance between actual player locations and template\npositions, subsequently selecting the formation with the lowest assignment\ncost. To improve accuracy, we scale actual player positions to match the\ndimensions of these formation templates in both width and length. While the\nmethod functions effectively on individual frames, it extends naturally to\nlarger game segments such as complete periods, possession sequences or specific\nintervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we\nincorporate an optional stability parameter that prevents unnecessary formation\nchanges when assignment costs differ only marginally between time segments.\nEFPI is available as open-source code through the unravelsports Python package.", "AI": {"tldr": "This paper introduces EFPI, a method for recognizing football team formations and assigning player positions using predefined templates and spatiotemporal tracking data. It minimizes assignment costs by scaling actual player positions to template dimensions and optionally incorporates a stability parameter.", "motivation": "To enhance tactical analysis in football by accurately recognizing team formations and assigning player positions.", "method": "EFPI uses linear sum assignment to match players to positions within predefined static formation templates by minimizing total distance between actual player locations and template positions. A stability parameter can be used to prevent unnecessary formation changes.", "result": "The method effectively recognizes formations and assigns player positions both on individual frames and across larger game segments, with the option of adding a stability parameter.", "conclusion": "EFPI offers a flexible approach for football formation recognition and is available as open-source code through the unravelsports Python package."}}
{"id": "2506.24093", "pdf": "https://arxiv.org/pdf/2506.24093", "abs": "https://arxiv.org/abs/2506.24093", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Sch\u00f6ning"], "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "comment": "21pages, 14 figures, 2 tables", "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u4e24\u79cd\u5e38\u7528\u7684\u6df7\u5408\u7b56\u7565\u5728\u4e09\u79cd\u6d41\u884c\u67b6\u6784\u548c\u4e09\u4e2a\u4e0d\u540c\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u6bd4\u4f8b\u5bf9ANN\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u4e3a\u4f18\u5316\u5408\u6210\u6570\u636e\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u6df7\u5408\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u7b56\u7565\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u7f13\u89e3\u9886\u57df\u5dee\u8ddd\uff0c\u4f46\u8fd9\u4e9b\u7b56\u7565\u5728\u5404\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5168\u9762\u5206\u6790\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6df7\u5408\u7b56\u7565\u5728\u4e09\u79cd\u6d41\u884c\u67b6\u6784\u548c\u4e09\u4e2a\u4e0d\u540c\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u91c7\u6837\u5177\u6709\u4e0d\u540c\u5408\u6210\u5230\u771f\u5b9e\u6570\u636e\u6bd4\u4f8b\u7684\u5b50\u96c6\u4ee5\u7814\u7a76\u5408\u6210\u548c\u771f\u5b9e\u6210\u5206\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u4f18\u5316ANN\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5408\u6210\u6570\u636e\u4f7f\u7528\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7684\u6df7\u5408\u7b56\u7565\u3001\u67b6\u6784\u548c\u6570\u636e\u96c6\u7ec4\u5408\uff0c\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4f18\u5316\u5408\u6210\u6570\u636e\u4f7f\u7528\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u5347ANN\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.23872", "pdf": "https://arxiv.org/pdf/2506.23872", "abs": "https://arxiv.org/abs/2506.23872", "authors": ["Eduard Buss", "Till Aust", "Heiko Hamann"], "title": "When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems", "categories": ["cs.LG"], "comment": "Submitted and Accepted at the 14th international conference on\n  biomimetic and biohybrid systems (Living Machines)", "summary": "Living plants, while contributing to ecological balance and climate\nregulation, also function as natural sensors capable of transmitting\ninformation about their internal physiological states and surrounding\nconditions. This rich source of data provides potential for applications in\nenvironmental monitoring and precision agriculture. With integration into\nbiohybrid systems, we establish novel channels of physiological signal flow\nbetween living plants and artificial devices. We equipped *Hedera helix* with a\nplant-wearable device called PhytoNode to continuously record the plant's\nelectrophysiological activity. We deployed plants in an uncontrolled outdoor\nenvironment to map electrophysiological patterns to environmental conditions.\nOver five months, we collected data that we analyzed using state-of-the-art and\nautomated machine learning (AutoML). Our classification models achieve high\nperformance, reaching macro F1 scores of up to 95 percent in binary tasks.\nAutoML approaches outperformed manual tuning, and selecting subsets of\nstatistical features further improved accuracy. Our biohybrid living system\nmonitors the electrophysiology of plants in harsh, real-world conditions. This\nwork advances scalable, self-sustaining, and plant-integrated living biohybrid\nsystems for sustainable environmental monitoring.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aPhytoNode\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u7528\u4e8e\u8fde\u7eed\u8bb0\u5f55\u5e38\u6625\u85e4\uff08Hedera helix\uff09\u7684\u7535\u751f\u7406\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u673a\u5668\u5b66\u4e60\uff08AutoML\uff09\u5206\u6790\u6570\u636e\uff0c\u5c06\u690d\u7269\u7684\u7535\u751f\u7406\u6a21\u5f0f\u4e0e\u73af\u5883\u6761\u4ef6\u76f8\u5173\u8054\u3002\u7814\u7a76\u5728\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u4e94\u4e2a\u6708\u7684\u6570\u636e\u6536\u96c6\uff0c\u5206\u7c7b\u6a21\u578b\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u9ad8\u8fbe95%\u7684\u5b8f\u89c2F1\u5206\u6570\u3002\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u53ef\u6301\u7eed\u73af\u5883\u76d1\u6d4b\u7684\u751f\u7269\u6df7\u5408\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528\u690d\u7269\u4f5c\u4e3a\u81ea\u7136\u4f20\u611f\u5668\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u73af\u5883\u76d1\u6d4b\u548c\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u5efa\u7acb\u690d\u7269\u4e0e\u4eba\u5de5\u8bbe\u5907\u4e4b\u95f4\u65b0\u7684\u751f\u7406\u4fe1\u53f7\u6d41\u901a\u9053\u3002", "method": "\u5c06PhytoNode\u8bbe\u5907\u5b89\u88c5\u5728\u5e38\u6625\u85e4\u4e0a\u4ee5\u8bb0\u5f55\u5176\u7535\u751f\u7406\u6d3b\u52a8\uff1b\u5728\u4e0d\u53d7\u63a7\u7684\u6237\u5916\u73af\u5883\u4e2d\u90e8\u7f72\u690d\u7269\u5e76\u6536\u96c6\u4e94\u4e2a\u6708\u7684\u6570\u636e\uff1b\u4f7f\u7528\u5148\u8fdb\u7684\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u6280\u672f\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u5c06\u7535\u751f\u7406\u6a21\u5f0f\u4e0e\u73af\u5883\u6761\u4ef6\u5173\u8054\u8d77\u6765\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b8fF1\u5206\u6570\u8fbe\u523095%\uff1b\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u624b\u52a8\u8c03\u53c2\uff0c\u9009\u62e9\u7edf\u8ba1\u7279\u5f81\u5b50\u96c6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u6076\u52a3\u73b0\u5b9e\u6761\u4ef6\u4e0b\u76d1\u6d4b\u690d\u7269\u7535\u751f\u7406\u5b66\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u6301\u7eed\u73af\u5883\u76d1\u6d4b\u7684\u53ef\u6269\u5c55\u3001\u81ea\u7ef4\u6301\u3001\u690d\u7269\u96c6\u6210\u7684\u751f\u7269\u6df7\u5408\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.23958", "pdf": "https://arxiv.org/pdf/2506.23958", "abs": "https://arxiv.org/abs/2506.23958", "authors": ["Ikechukwu Ogbonna", "Lesley Davidson", "Soumya Banerjee", "Abhishek Dasgupta", "Laurence Kenney", "Vikranth Harthikote Nagaraja"], "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages", "categories": ["cs.LG"], "comment": "5 pages, 0 figures, 0 tables", "summary": "Millions of people in African countries face barriers to accessing healthcare\ndue to language and literacy gaps. This research tackles this challenge by\ntransforming complex medical documents -- in this case, prosthetic device user\nmanuals -- into accessible formats for underserved populations. This case study\nin cross-cultural translation is particularly pertinent/relevant for\ncommunities that receive donated prosthetic devices but may not receive the\naccompanying user documentation. Or, if available online, may only be available\nin formats (e.g., language and readability) that are inaccessible to local\npopulations (e.g., English-language, high resource settings/cultural context).\nThe approach is demonstrated using the widely spoken Pidgin dialect, but our\nopen-source framework has been designed to enable rapid and easy extension to\nother languages/dialects. This work presents an AI-powered framework designed\nto process and translate complex medical documents, e.g., user manuals for\nprosthetic devices, into marginalised languages. The system enables users --\nsuch as healthcare workers or patients -- to upload English-language medical\nequipment manuals, pose questions in their native language, and receive\naccurate, localised answers in real time. Technically, the system integrates a\nRetrieval-Augmented Generation (RAG) pipeline for processing and semantic\nunderstanding of the uploaded manuals. It then employs advanced Natural\nLanguage Processing (NLP) models for generative question-answering and\nmultilingual translation. Beyond simple translation, it ensures accessibility\nto device instructions, treatment protocols, and safety information, empowering\npatients and clinicians to make informed healthcare decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u590d\u6742\u7684\u533b\u5b66\u6587\u6863\uff08\u5982\u5047\u80a2\u8bbe\u5907\u7528\u6237\u624b\u518c\uff09\u7ffb\u8bd1\u6210\u8fb9\u7f18\u5316\u8bed\u8a00\uff0c\u7279\u522b\u662f\u975e\u6d32\u56fd\u5bb6\u4e2d\u7531\u4e8e\u8bed\u8a00\u548c\u8bfb\u5199\u80fd\u529b\u5dee\u8ddd\u800c\u9762\u4e34\u533b\u7597\u4fdd\u5065\u83b7\u53d6\u969c\u788d\u7684\u4eba\u7fa4\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u548c\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4f20\u7684\u624b\u518c\u5904\u7406\u3001\u8bed\u4e49\u7406\u89e3\u3001\u751f\u6210\u6027\u95ee\u7b54\u548c\u591a\u8bed\u8a00\u7ffb\u8bd1\u529f\u80fd\uff0c\u4ece\u800c\u4e3a\u7528\u6237\u63d0\u4f9b\u51c6\u786e\u3001\u672c\u5730\u5316\u7684\u5b9e\u65f6\u7b54\u6848\uff0c\u786e\u4fdd\u8bbe\u5907\u8bf4\u660e\u3001\u6cbb\u7597\u534f\u8bae\u548c\u5b89\u5168\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5e2e\u52a9\u60a3\u8005\u548c\u4e34\u5e8a\u533b\u751f\u505a\u51fa\u660e\u667a\u7684\u533b\u7597\u51b3\u7b56\u3002", "motivation": "\u975e\u6d32\u56fd\u5bb6\u6570\u767e\u4e07\u4eba\u56e0\u8bed\u8a00\u548c\u8bfb\u5199\u80fd\u529b\u5dee\u8ddd\u800c\u96be\u4ee5\u83b7\u5f97\u533b\u7597\u4fdd\u5065\u3002\u7279\u522b\u662f\u5728\u63a5\u53d7\u6350\u8d60\u7684\u5047\u80a2\u8bbe\u5907\u65f6\uff0c\u53ef\u80fd\u7f3a\u4e4f\u76f8\u5e94\u7684\u7528\u6237\u6587\u6863\uff0c\u6216\u8005\u5728\u7ebf\u63d0\u4f9b\u7684\u6587\u6863\u4ec5\u4ee5\u5f53\u5730\u4eba\u7fa4\u65e0\u6cd5\u8bbf\u95ee\u7684\u683c\u5f0f\uff08\u4f8b\u5982\u82f1\u8bed\u3001\u9ad8\u8d44\u6e90\u73af\u5883/\u6587\u5316\u80cc\u666f\uff09\u5b58\u5728\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u590d\u6742\u533b\u5b66\u6587\u6863\u8f6c\u5316\u4e3a\u8fb9\u7f18\u5316\u8bed\u8a00\uff0c\u4ee5\u4fbf\u4e8e\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u4f7f\u7528\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u5e7f\u6cdb\u4f7f\u7528\u7684\u76ae\u91d1\uff08Pidgin\uff09\u65b9\u8a00\u4f5c\u4e3a\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u5f00\u6e90\u6846\u67b6\uff0c\u53ef\u4ee5\u5feb\u901f\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u8a00\u6216\u65b9\u8a00\u3002\u6280\u672f\u4e0a\uff0c\u7cfb\u7edf\u96c6\u6210\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u4ee5\u5904\u7406\u548c\u7406\u89e3\u4e0a\u4f20\u7684\u624b\u518c\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6a21\u578b\u8fdb\u884c\u751f\u6210\u6027\u95ee\u7b54\u548c\u591a\u8bed\u8a00\u7ffb\u8bd1\u3002\u7528\u6237\u53ef\u4ee5\u4e0a\u4f20\u82f1\u6587\u533b\u5b66\u8bbe\u5907\u624b\u518c\uff0c\u7528\u6bcd\u8bed\u63d0\u95ee\uff0c\u5e76\u5b9e\u65f6\u83b7\u5f97\u51c6\u786e\u3001\u672c\u5730\u5316\u7684\u7b54\u6848\u3002", "result": "\u6b64\u6846\u67b6\u6210\u529f\u5730\u5c06\u590d\u6742\u533b\u5b66\u6587\u6863\u8f6c\u6362\u4e3a\u8fb9\u7f18\u5316\u8bed\u8a00\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u4ee5\u81ea\u5df1\u7684\u8bed\u8a00\u83b7\u53d6\u8bbe\u5907\u8bf4\u660e\u3001\u6cbb\u7597\u534f\u8bae\u548c\u5b89\u5168\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u533b\u7597\u4fdd\u5065\u7684\u53ef\u53ca\u6027\u548c\u9002\u7528\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u7b80\u5355\u7ffb\u8bd1\uff0c\u8fd8\u786e\u4fdd\u4e86\u5bf9\u533b\u7597\u5185\u5bb9\u7684\u5168\u9762\u7406\u89e3\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u60a3\u8005\u7684\u81ea\u4e3b\u6743\u548c\u4e34\u5e8a\u533b\u751f\u7684\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86AI\u9a71\u52a8\u6846\u67b6\u5728\u89e3\u51b3\u8bed\u8a00\u548c\u8bfb\u5199\u969c\u788d\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6539\u5584\u533b\u7597\u4fdd\u5065\u83b7\u53d6\u65b9\u9762\u3002\u901a\u8fc7\u63d0\u4f9b\u51c6\u786e\u3001\u672c\u5730\u5316\u7684\u533b\u7597\u4fe1\u606f\uff0c\u8be5\u7cfb\u7edf\u6709\u52a9\u4e8e\u7f29\u5c0f\u5168\u7403\u533b\u7597\u4fdd\u5065\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u793e\u533a\u4e2d\u3002"}}
{"id": "2506.23971", "pdf": "https://arxiv.org/pdf/2506.23971", "abs": "https://arxiv.org/abs/2506.23971", "authors": ["Brandon M. Wood", "Misko Dzamba", "Xiang Fu", "Meng Gao", "Muhammed Shuaibi", "Luis Barroso-Luque", "Kareem Abdelmaqsoud", "Vahe Gharakhanyan", "John R. Kitchin", "Daniel S. Levine", "Kyle Michel", "Anuroop Sriram", "Taco Cohen", "Abhishek Das", "Ammar Rizvi", "Sushree Jagriti Sahoo", "Zachary W. Ulissi", "C. Lawrence Zitnick"], "title": "UMA: A Family of Universal Models for Atoms", "categories": ["cs.LG"], "comment": "29 pages, 5 figures", "summary": "The ability to quickly and accurately compute properties from atomic\nsimulations is critical for advancing a large number of applications in\nchemistry and materials science including drug discovery, energy storage, and\nsemiconductor manufacturing. To address this need, Meta FAIR presents a family\nof Universal Models for Atoms (UMA), designed to push the frontier of speed,\naccuracy, and generalization. UMA models are trained on half a billion unique\n3D atomic structures (the largest training runs to date) by compiling data\nacross multiple chemical domains, e.g. molecules, materials, and catalysts. We\ndevelop empirical scaling laws to help understand how to increase model\ncapacity alongside dataset size to achieve the best accuracy. The UMA small and\nmedium models utilize a novel architectural design we refer to as mixture of\nlinear experts that enables increasing model capacity without sacrificing\nspeed. For example, UMA-medium has 1.4B parameters but only ~50M active\nparameters per atomic structure. We evaluate UMA models on a diverse set of\napplications across multiple domains and find that, remarkably, a single model\nwithout any fine-tuning can perform similarly or better than specialized\nmodels. We are releasing the UMA code, weights, and associated data to\naccelerate computational workflows and enable the community to continue to\nbuild increasingly capable AI models.", "AI": {"tldr": "Meta FAIR\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aUMA\u7684\u539f\u5b50\u901a\u7528\u6a21\u578b\u5bb6\u65cf\uff0c\u8fd9\u4e9b\u6a21\u578b\u57fa\u4e8e\u534a\u4ebf\u72ec\u7279\u76843D\u539f\u5b50\u7ed3\u6784\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a2a\u8de8\u591a\u4e2a\u5316\u5b66\u9886\u57df\u5982\u5206\u5b50\u3001\u6750\u6599\u548c\u50ac\u5316\u5242\u3002\u901a\u8fc7\u5f00\u53d1\u7ecf\u9a8c\u6027\u7f29\u653e\u5b9a\u5f8b\u6765\u7406\u89e3\u5982\u4f55\u968f\u6570\u636e\u96c6\u5927\u5c0f\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u4ee5\u8fbe\u5230\u6700\u4f73\u51c6\u786e\u6027\u3002UMA\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u4e13\u95e8\u6a21\u578b\u3002", "motivation": "\u5feb\u901f\u4e14\u7cbe\u786e\u5730\u4ece\u539f\u5b50\u6a21\u62df\u4e2d\u8ba1\u7b97\u5c5e\u6027\u5bf9\u4e8e\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u8bb8\u591a\u5e94\u7528\uff08\u5305\u62ec\u836f\u7269\u53d1\u73b0\u3001\u80fd\u6e90\u5b58\u50a8\u548c\u534a\u5bfc\u4f53\u5236\u9020\uff09\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u6cdb\u5316\u7684\u6a21\u578b\u3002", "method": "UMA\u6a21\u578b\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u5316\u5b66\u9886\u57df\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u79f0\u4e3a\u7ebf\u6027\u4e13\u5bb6\u6df7\u5408\u7684\u65b0\u9896\u67b6\u6784\u8bbe\u8ba1\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u727a\u7272\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u5bb9\u91cf\u3002\u4f8b\u5982\uff0cUMA-medium\u670914\u4ebf\u53c2\u6570\uff0c\u4f46\u6bcf\u4e2a\u539f\u5b50\u7ed3\u6784\u4ec5\u6fc0\u6d3b\u7ea65000\u4e07\u53c2\u6570\u3002", "result": "UMA\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u591a\u6837\u5316\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5355\u4e00\u6a21\u578b\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u5373\u53ef\u4e0e\u4e13\u95e8\u6a21\u578b\u76f8\u5ab2\u7f8e\u6216\u66f4\u4f18\u3002", "conclusion": "Meta FAIR\u53d1\u5e03\u4e86UMA\u4ee3\u7801\u3001\u6743\u91cd\u548c\u76f8\u5173\u6570\u636e\uff0c\u4ee5\u52a0\u901f\u8ba1\u7b97\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u4f7f\u793e\u533a\u80fd\u591f\u7ee7\u7eed\u6784\u5efa\u8d8a\u6765\u8d8a\u5f3a\u5927\u7684AI\u6a21\u578b\u3002"}}
{"id": "2506.23977", "pdf": "https://arxiv.org/pdf/2506.23977", "abs": "https://arxiv.org/abs/2506.23977", "authors": ["Zain ul Abdeen", "Vassilis Kekatos", "Ming Jin"], "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks", "categories": ["cs.LG"], "comment": null, "summary": "Certified robustness is a critical property for deploying neural networks\n(NN) in safety-critical applications. A principle approach to achieving such\nguarantees is to constrain the global Lipschitz constant of the network.\nHowever, accurate methods for Lipschitz-constrained training often suffer from\nnon-convex formulations and poor scalability due to reliance on global\nsemidefinite programs (SDPs). In this letter, we propose a convex training\nframework that enforces global Lipschitz constraints via semidefinite\nrelaxation. By reparameterizing the NN using loop transformation, we derive a\nconvex admissibility condition that enables tractable and certifiable training.\nWhile the resulting formulation guarantees robustness, its scalability is\nlimited by the size of global SDP. To overcome this, we develop a randomized\nsubspace linear matrix inequalities (RS-LMI) approach that decomposes the\nglobal constraints into sketched layerwise constraints projected onto\nlow-dimensional subspaces, yielding a smooth and memory-efficient training\nobjective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that\nthe proposed framework achieves competitive accuracy with significantly\nimproved Lipschitz bounds and runtime performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51f8\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u5b9a\u677e\u5f1b\u6765\u5f3a\u5236\u5168\u5c40Lipschitz\u7ea6\u675f\uff0c\u5e76\u5f00\u53d1\u4e86RS-LMI\u65b9\u6cd5\u4ee5\u5206\u89e3\u5168\u5c40\u7ea6\u675f\uff0c\u4ece\u800c\u5b9e\u73b0\u5e73\u6ed1\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u8bad\u7ec3\u76ee\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u9c81\u68d2\u6027\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86Lipschitz\u754c\u9650\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5177\u5907\u8ba4\u8bc1\u9c81\u68d2\u6027\uff0c\u800c\u9650\u5236\u7f51\u7edc\u7684\u5168\u5c40Lipschitz\u5e38\u6570\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u51c6\u786e\u65b9\u6cd5\u901a\u5e38\u56e0\u4f9d\u8d56\u5168\u5c40\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u800c\u5bfc\u81f4\u975e\u51f8\u516c\u5f0f\u5316\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51f8\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u5b9a\u677e\u5f1b\u6765\u5f3a\u5236\u5168\u5c40Lipschitz\u7ea6\u675f\u3002\u9996\u5148\uff0c\u4f7f\u7528\u73af\u8def\u53d8\u6362\u91cd\u65b0\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u51f8\u53ef\u63a5\u53d7\u6761\u4ef6\uff0c\u4f7f\u5f97\u8bad\u7ec3\u65e2\u53ef\u884c\u53c8\u53ef\u8ba4\u8bc1\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670d\u5168\u5c40SDP\u89c4\u6a21\u7684\u9650\u5236\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u968f\u673a\u5b50\u7a7a\u95f4\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08RS-LMI\uff09\u65b9\u6cd5\uff0c\u5c06\u5168\u5c40\u7ea6\u675f\u5206\u89e3\u4e3a\u6295\u5f71\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e0a\u7684\u9010\u5c42\u7ea6\u675f\uff0c\u4ece\u800c\u5f62\u6210\u5e73\u6ed1\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548cImageNet\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u5584\u4e86Lipschitz\u754c\u9650\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u51f8\u8bad\u7ec3\u6846\u67b6\u7ed3\u5408RS-LMI\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u795e\u7ecf\u7f51\u7edc\u8ba4\u8bc1\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002\u8fd9\u4e3a\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.23978", "pdf": "https://arxiv.org/pdf/2506.23978", "abs": "https://arxiv.org/abs/2506.23978", "authors": ["Samuele Marro", "Philip Torr"], "title": "LLM Agents Are the Antidote to Walled Gardens", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\uff0c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u53ef\u4ee5\u6253\u7834\u5f53\u524d\u5e94\u7528\u5c42\u88ab\u5c01\u95ed\u4e13\u6709\u5e73\u53f0\u4e3b\u5bfc\u7684\u5c40\u9762\uff0c\u901a\u8fc7\u81ea\u52a8\u8f6c\u6362\u6570\u636e\u683c\u5f0f\u548c\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u7684\u754c\u9762\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u4e14\u4e0d\u53ef\u907f\u514d\u7684\u901a\u7528\u4e92\u64cd\u4f5c\u6027\u3002\u8fd9\u79cd\u4e92\u64cd\u4f5c\u6027\u524a\u5f31\u5784\u65ad\u884c\u4e3a\u3001\u4fc3\u8fdb\u6570\u636e\u53ef\u79fb\u690d\u6027\uff0c\u4f46\u4e5f\u53ef\u80fd\u5e26\u6765\u65b0\u7684\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u503a\u52a1\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u673a\u5668\u5b66\u4e60\u793e\u533a\u5e94\u63a5\u7eb3\u8fd9\u4e00\u53d1\u5c55\uff0c\u5e76\u6784\u5efa\u9002\u5f53\u7684\u6846\u67b6\u4ee5\u7f13\u89e3\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u4e92\u8054\u7f51\u7684\u6838\u5fc3\u57fa\u7840\u8bbe\u65bd\u539f\u672c\u8bbe\u8ba1\u4e3a\u5f00\u653e\u548c\u666e\u904d\uff0c\u4f46\u76ee\u524d\u7684\u5e94\u7528\u5c42\u5374\u88ab\u5c01\u95ed\u3001\u4e13\u6709\u7684\u5e73\u53f0\u6240\u4e3b\u5bfc\u3002\u5f00\u653e\u548c\u4e92\u64cd\u4f5c\u7684API\u9700\u8981\u5927\u91cf\u6295\u8d44\uff0c\u5e02\u573a\u9886\u5bfc\u8005\u51e0\u4e4e\u6ca1\u6709\u52a8\u529b\u53bb\u5b9e\u73b0\u6570\u636e\u4ea4\u6362\uff0c\u56e0\u4e3a\u8fd9\u53ef\u80fd\u4f1a\u524a\u5f31\u4ed6\u4eec\u5bf9\u7528\u6237\u7684\u9501\u5b9a\u6548\u5e94\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6253\u7834\u8fd9\u79cd\u73b0\u72b6\u3002", "method": "\u5229\u7528LLM-based agents\uff0c\u8fd9\u4e9b\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u5728\u4e0d\u540c\u6570\u636e\u683c\u5f0f\u4e4b\u95f4\u8fdb\u884c\u7ffb\u8bd1\uff0c\u5e76\u4e0e\u4e3a\u4eba\u8bbe\u8ba1\u7684\u754c\u9762\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e92\u64cd\u4f5c\u6027\u7684\u6210\u672c\u5e76\u4f7f\u5176\u4e0d\u53ef\u907f\u514d\u3002\u8fd9\u79cd\u80fd\u529b\u88ab\u79f0\u4e3a\u901a\u7528\u4e92\u64cd\u4f5c\u6027\uff0c\u5b83\u5141\u8bb8\u4efb\u4f55\u4e24\u4e2a\u6570\u5b57\u670d\u52a1\u901a\u8fc7AI\u4e2d\u4ecb\u9002\u914d\u5668\u65e0\u7f1d\u4ea4\u6362\u6570\u636e\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u524a\u5f31\u4e86\u5784\u65ad\u884c\u4e3a\uff0c\u4fc3\u8fdb\u4e86\u6570\u636e\u7684\u53ef\u79fb\u690d\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e5f\u53ef\u80fd\u5bfc\u81f4\u65b0\u7684\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u503a\u52a1\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u793e\u533a\u5e94\u8be5\u63a5\u53d7\u8fd9\u4e00\u53d1\u5c55\uff0c\u5e76\u5efa\u7acb\u9002\u5f53\u7684\u6846\u67b6\u6765\u51cf\u8f7b\u5176\u6f5c\u5728\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4ee5\u4fbf\u5229\u7528AI\u6062\u590d\u7528\u6237\u81ea\u7531\u548c\u7ade\u4e89\u5e02\u573a\uff0c\u800c\u4e0d\u727a\u7272\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23996", "pdf": "https://arxiv.org/pdf/2506.23996", "abs": "https://arxiv.org/abs/2506.23996", "authors": ["Juan Maro\u00f1as"], "title": "The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "This document shows how to obtain the Jacobian and Hessian matrices of the\nKullback-Leibler divergence between two multivariate Gaussian distributions,\nusing the first and second-order differentials. The presented derivations are\nbased on the theory presented by \\cite{magnus99}. I've also got great\ninspiration from some of the derivations in \\cite{minka}.\n  Since I pretend to be at most didactic, the document is split into a summary\nof results and detailed derivations on each of the elements involved, with\nspecific references to the tricks used in the derivations, and to many of the\nunderlying concepts.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u4e00\u9636\u548c\u4e8c\u9636\u5fae\u5206\u6765\u83b7\u53d6\u4e24\u4e2a\u591a\u5143\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4Kullback-Leibler\u6563\u5ea6\u7684Jacobian\u77e9\u9635\u548cHessian\u77e9\u9635\u3002\u63a8\u5bfc\u57fa\u4e8e[magnus99]\u4e2d\u7684\u7406\u8bba\uff0c\u5e76\u4ece[minka]\u7684\u4e00\u4e9b\u63a8\u5bfc\u4e2d\u83b7\u5f97\u4e86\u6781\u5927\u7684\u7075\u611f\u3002\u4e3a\u4e86\u5c3d\u53ef\u80fd\u505a\u5230\u6613\u61c2\uff0c\u6587\u6863\u5206\u4e3a\u7ed3\u679c\u603b\u7ed3\u548c\u6bcf\u4e2a\u53c2\u4e0e\u5143\u7d20\u7684\u8be6\u7ec6\u63a8\u5bfc\u4e24\u90e8\u5206\uff0c\u5177\u4f53\u63d0\u5230\u4e86\u63a8\u5bfc\u4e2d\u4f7f\u7528\u7684\u6280\u5de7\u4ee5\u53ca\u8bb8\u591a\u57fa\u7840\u6982\u5ff5\u3002", "motivation": "\u52a8\u673a\u662f\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u6765\u8ba1\u7b97\u4e24\u4e2a\u591a\u5143\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4Kullback-Leibler\u6563\u5ea6\u7684Jacobian\u548cHessian\u77e9\u9635\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u53c2\u6570\u5bf9\u6563\u5ea6\u7684\u5f71\u54cd\u975e\u5e38\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u4e00\u9636\u548c\u4e8c\u9636\u5fae\u5206\u6765\u8ba1\u7b97Kullback-Leibler\u6563\u5ea6\u7684Jacobian\u548cHessian\u77e9\u9635\u3002\u63a8\u5bfc\u8fc7\u7a0b\u53c2\u8003\u4e86[magnus99]\u7684\u7406\u8bba\uff0c\u5e76\u53d7\u5230[minka]\u7684\u542f\u53d1\u3002", "result": "\u6210\u529f\u5730\u63a8\u5bfc\u51fa\u4e86\u4e24\u4e2a\u591a\u5143\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4Kullback-Leibler\u6563\u5ea6\u7684Jacobian\u548cHessian\u77e9\u9635\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u8be6\u7ec6\u7684\u63a8\u5bfc\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u8ba1\u7b97Kullback-Leibler\u6563\u5ea6\u76f8\u5173\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7ed3\u679c\u603b\u7ed3\u548c\u63a8\u5bfc\u7ec6\u8282\u3002"}}
{"id": "2506.24000", "pdf": "https://arxiv.org/pdf/2506.24000", "abs": "https://arxiv.org/abs/2506.24000", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "AI": {"tldr": "TTA-VLM\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0a\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\u3002\u5b83\u89e3\u51b3\u4e86\u5f53\u524dTTA\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4e00\u4e9b\u95ee\u9898\uff0c\u4f8b\u5982\u57fa\u7ebf\u7ed3\u679c\u91cd\u590d\u3001\u8bc4\u4f30\u6307\u6807\u6709\u9650\u7b49\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u73b0\u6709\u7684TTA\u65b9\u6cd5\u6539\u8fdb\u6709\u9650\uff0c\u5e76\u4e14\u4e0e\u8bad\u7ec3\u65f6\u5fae\u8c03\u65b9\u6cd5\u534f\u4f5c\u4e0d\u4f73\uff0c\u540c\u65f6\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u76ee\u524d\u7684TTA\u7814\u7a76\u5b58\u5728\u51e0\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1) \u57fa\u7ebf\u7ed3\u679c\u91cd\u590d\uff1b2) \u8bc4\u4f30\u6307\u6807\u6709\u9650\uff1b3) \u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\uff1b4) \u5206\u6790\u4e0d\u8db3\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86TTA\u65b9\u6cd5\u4e4b\u95f4\u7684\u516c\u5e73\u6bd4\u8f83\uff0c\u5e76\u6a21\u7cca\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u4f18\u7f3a\u70b9\u3002", "method": "TTA-VLM\u5728\u7edf\u4e00\u548c\u53ef\u91cd\u73b0\u7684\u6846\u67b6\u5185\u5b9e\u73b0\u4e868\u79cd\u60c5\u666fTTA\u548c7\u79cd\u5728\u7ebfTTA\u65b9\u6cd5\uff0c\u5e76\u572815\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u9664\u4e86CLIP\uff0c\u8fd8\u6269\u5c55\u5230SigLIP\u6a21\u578b\uff0c\u5e76\u5305\u62ecCoOp\u3001MaPLe\u548cTeCoA\u7b49\u8bad\u7ec3\u65f6\u95f4\u8c03\u6574\u65b9\u6cd5\u6765\u8bc4\u4f30\u901a\u7528\u6027\u3002\u6b64\u5916\uff0cTTA-VLM\u8fd8\u7ed3\u5408\u4e86\u5404\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5982\u7a33\u5065\u6027\u3001\u6821\u51c6\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u7a33\u5b9a\u6027\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) \u73b0\u6709\u7684TTA\u65b9\u6cd5\u76f8\u6bd4\u65e9\u671f\u5f00\u521b\u6027\u5de5\u4f5c\u6539\u8fdb\u6709\u9650\uff1b2) \u5f53\u524dTTA\u65b9\u6cd5\u4e0e\u8bad\u7ec3\u65f6\u5fae\u8c03\u65b9\u6cd5\u534f\u4f5c\u4e0d\u4f73\uff1b3) \u51c6\u786e\u7387\u7684\u63d0\u5347\u901a\u5e38\u4f34\u968f\u7740\u6a21\u578b\u53ef\u4fe1\u5ea6\u7684\u4e0b\u964d\u3002", "conclusion": "\u4f5c\u8005\u53d1\u5e03\u4e86TTA-VLM\uff0c\u65e8\u5728\u4e3aVLMs\u7684TTA\u65b9\u6cd5\u63d0\u4f9b\u516c\u5e73\u7684\u6bd4\u8f83\u548c\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5e76\u5e0c\u671b\u9f13\u52b1\u793e\u533a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u6cdb\u5316\u7684TTA\u7b56\u7565\u3002"}}
{"id": "2506.24005", "pdf": "https://arxiv.org/pdf/2506.24005", "abs": "https://arxiv.org/abs/2506.24005", "authors": ["He Wang", "Xingyu Xu", "Yuejie Chi"], "title": "Provably Efficient and Agile Randomized Q-Learning", "categories": ["cs.LG"], "comment": null, "summary": "While Bayesian-based exploration often demonstrates superior empirical\nperformance compared to bonus-based methods in model-based reinforcement\nlearning (RL), its theoretical understanding remains limited for model-free\nsettings. Existing provable algorithms either suffer from computational\nintractability or rely on stage-wise policy updates which reduce responsiveness\nand slow down the learning process. In this paper, we propose a novel variant\nof Q-learning algorithm, refereed to as RandomizedQ, which integrates\nsampling-based exploration with agile, step-wise, policy updates, for episodic\ntabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where\n$S$ is the number of states, $A$ is the number of actions, $H$ is the episode\nlength, and $T$ is the total number of episodes. In addition, we present a\nlogarithmic regret bound under a mild positive sub-optimality condition on the\noptimal Q-function. Empirically, RandomizedQ exhibits outstanding performance\ncompared to existing Q-learning variants with both bonus-based and\nBayesian-based exploration on standard benchmarks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.24124", "pdf": "https://arxiv.org/pdf/2506.24124", "abs": "https://arxiv.org/abs/2506.24124", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "categories": ["cs.LG", "cs.CV"], "comment": "Code: https://github.com/Ironieser/TimesCLIP", "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u548c\u6587\u672c\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u8fd9\u4e9b\u89c6\u89d2\u4ee5\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4f9d\u8d56\u4e8e\u5355\u4e00\u6a21\u5f0f\u7684\u6570\u503c\u8f93\u5165\uff0c\u96be\u4ee5\u6355\u6349\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u6a21\u5f0f\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u53d7\u5230\u79bb\u6563\u4ee4\u724c\u5e8f\u5217\u7684\u9650\u5236\uff0c\u5e76\u7f3a\u4e4f\u4eba\u7c7b\u901a\u5e38\u5e94\u7528\u7684\u611f\u77e5\u76f4\u89c9\uff08\u5982\u89e3\u91ca\u89c6\u89c9\u6a21\u5f0f\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u548c\u6587\u672c\u89c6\u89d2\u3002\u7136\u540e\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u8fd9\u4e9b\u89c6\u56fe\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u66f4\u4e30\u5bcc\u3001\u66f4\u4e92\u8865\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u53d8\u91cf\u9009\u62e9\u6a21\u5757\uff0c\u5229\u7528\u5bf9\u9f50\u7684\u8868\u793a\u6765\u8bc6\u522b\u591a\u53d8\u91cf\u9884\u6d4b\u4e2d\u6700\u76f8\u5173\u7684\u53d8\u91cf\u3002", "result": "\u5728\u5341\u4e94\u4e2a\u77ed\u671f\u548c\u516d\u4e2a\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u591a\u6a21\u6001\u5bf9\u9f50\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
