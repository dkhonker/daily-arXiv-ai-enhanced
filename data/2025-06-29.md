<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 77]
- [cs.AI](#cs.AI) [总数: 13]
- [stat.ML](#stat.ML) [总数: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain, Muhammad Sohail, Nauman Ali Khan, Naima Iltaf, Ihtesham ul Islam*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于数据集大小的自适应联邦学习（SAFL）框架，揭示了数据集规模对联邦学习效果的影响，并在多个数据模态上进行了验证，证明了其高效性和实用性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法主要关注模型异构性和聚合技术，而忽视了数据集大小特征对联邦训练动态的根本影响。

**方法:** 引入了Size-Based Adaptive Federated Learning (SAFL)，一种新的渐进式训练框架，根据跨异构多模态数据的数据集大小特征系统地组织联邦学习。通过在13个不同数据集、7种模态上进行全面实验评估，揭示了联邦学习在不同数据规模和模态下的表现特点。

**结果:** 发现联邦学习的有效性最佳数据集规模范围为1000-1500样本；结构化数据的表现显著优于非结构化数据；超过2000样本的大数据集会导致性能系统性下降。SAFL在所有数据集上的平均准确率为87.68%，结构化数据模态可达99%以上的准确率，同时具有优越的通信效率。

**结论:** 本研究填补了对数据特征如何驱动联邦学习策略理解上的关键空白，提供了理论见解和实际指导，适用于神经网络和学习系统的实际联邦学习部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Progressive+Size-Adaptive+Federated+Learning%3A+A+Comprehensive+Framework+for+Heterogeneous+Multi-Modal+Data+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20685&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [2] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio, Tommaso Mazza, Pierangelo Veltri, Pietro Hiram Guzzi*

**主要类别:** cs.LG

**AI概要:** 随着大规模组学数据的日益增多，需要稳健的分析框架来处理复杂的基因表达数据集并提供可解释的结果。人工智能的最新进展使得识别区分疾病状态和健康对照的异常分子模式成为可能。结合模型可解释性的改进，这些工具现在支持识别可能驱动疾病表型的基因。然而，当前的基因异常检测方法通常局限于单一数据集，并缺乏易用的图形界面。本文介绍了一种通用且可解释的生物网络异常检测框架E-ABIN。E-ABIN将经典机器学习与基于图的深度学习技术结合在一个统一、用户友好的平台上，能够从基因表达或甲基化衍生网络中检测和解释异常。通过整合支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，E-ABIN确保了高预测准确性的同时保持了可解释性。通过膀胱癌和乳糜泻的案例研究，证明了E-ABIN的有效性，成功揭示了生物学相关的异常并提供了对疾病机制的见解。


<details>
  <summary>更多</summary>
  
**动机:** 当前基因异常检测方法通常局限于单一数据集，并缺乏易用的图形界面。因此，需要一个更通用、更易用的框架来处理复杂基因表达数据集并提供可解释的结果。

**方法:** E-ABIN是一个通用且可解释的生物网络异常检测框架，它结合了经典机器学习（如支持向量机、随机森林）和基于图的深度学习技术（如图自编码器和图对抗属性网络），并在一个用户友好的平台上实现。该框架可以从基因表达或甲基化衍生网络中检测和解释异常。

**结果:** 通过膀胱癌和乳糜泻的案例研究，E-ABIN有效地揭示了生物学相关的异常，并提供了对疾病机制的深入见解。

**结论:** E-ABIN作为一个通用、可解释的框架，能够在保证高预测准确率的同时维持良好的可解释性，适用于多种疾病的基因异常检测和解读。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是E-ABIN%3A+an+Explainable+module+for+Anomaly+detection+in+BIological+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20693，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20693&send_immediately=true&force_search=false)

**原文摘要:** The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [3] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

**主要类别:** cs.LG

**AI概要:** 论文提出了基于上下文-内容不确定性原理（CCUP）的分层计算框架，通过结构与具体性的对齐来最小化不确定性，并详细描述了四个操作原则层级及其实现方法。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解大脑和机器如何通过递归的结构-具体性对齐来最小化不确定性，提出了一种新的理论基础——上下文-内容不确定性原理（CCUP）。

**方法:** 该方法首先将推理形式化为方向熵最小化问题，并建立了一个偏向于内容优先结构化的变分梯度。接着，定义了四个层次的操作原则：核心推理约束、资源分配原则、时间引导学习动态以及空间分层组合。

**结果:** 展示了正式等价定理、原则依赖关系格以及计算模拟，证明了与CCUP一致的推理在效率上的提升。

**结论:** 本研究提供了一个统一的理论基础，用于解释大脑和机器如何通过路径依赖的内容驱动模拟来对齐结构和具体性，从而解决熵梯度问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Context-Content+Uncertainty+Principle，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20699&send_immediately=true&force_search=false)

**原文摘要:** The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [4] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain, Kusha Sareen, Mohammad Pedramfar, Siamak Ravanbakhsh*

**主要类别:** cs.LG

**AI概要:** 通过重用过去计算，将推理时对准问题转化为搜索问题，提出树状方法Diffusion Tree Sampling（DTS）和其贪婪变体Diffusion Tree Search（DTS$^\star$），分别在MNIST、CIFAR-10、文本到图像生成和语言完成任务中表现出显著的计算效率提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的引导方法在高噪声水平下容易出现偏差，并且未有效利用过去的计算结果来提高样本质量，导致计算资源使用效率低下。为了解决这些问题，受到蒙特卡洛树搜索成功的启发，作者将推理时对准问题视为一个搜索问题。

**方法:** 引入基于树的方法，通过从扩散链中回传终端奖励并迭代改进价值估计，从奖励对齐的目标密度中采样。提出两种方法：Diffusion Tree Sampling (DTS) 和 Diffusion Tree Search (DTS$^\star$)，前者通过无限次展开渐进精确地采样目标分布，后者进行全局搜索以找到高奖励样本。

**结果:** 在MNIST和CIFAR-10分类条件生成任务中，DTS与最佳基线的FID匹配，但计算量减少至最多$10\times$；在文本到图像生成和语言完成任务中，DTS$^\star$能以最多$5\times$更少的计算量找到与最佳样本匹配的结果。

**结论:** 通过重用先前生成的信息，该算法能够在额外计算资源投入时持续产生更好的样本，提供了一个可扩展的扩散模型推理时对准方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Tree+Sampling%3A+Scalable+inference-time+alignment+of+diffusion+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20701&send_immediately=true&force_search=false)

**原文摘要:** Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [5] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung, Rasa Hosseinzadeh, Gabriel Loaiza-Ganem*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了扩散模型（DMs）在估计局部内在维度（LID）方面的理论基础，并证明了FLIPD方法的正确性，同时扩展了其适用范围到均匀卷积的情况。


<details>
  <summary>更多</summary>
  
**动机:** 尽管FLIPD在LID估计中表现优异，但其理论基础仅在不切实际的仿射子流形假设下得到验证，因此需要更现实的理论支持。

**方法:** 通过数学推导和分析，作者在更一般的假设下正式证明了FLIPD方法的正确性，并探讨了将高斯卷积替换为均匀卷积时类似结果的适用性。

**结果:** 成功证明了FLIPD方法在更现实假设下的有效性，并展示了均匀卷积情况下的相似结果及其意义。

**结论:** 本研究弥补了FLIPD方法理论基础的不足，增强了其在实际应用中的可靠性，并为未来研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Convolutions%2C+Intrinsic+Dimension%2C+and+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20705&send_immediately=true&force_search=false)

**原文摘要:** The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [6] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao, Tianyi Li, Yurii Kvasiuk, Sai Chaitanya Tadepalli, Maja Rudolph, Daniel J. H. Chung, Frederic Sala, Moritz Münchmeyer*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）在复杂推理方面表现出色，测试时的扩展技术能以相对较低的成本提升其性能。本文研究了数学推理基准上的经验是否适用于高等理论物理领域，通过在TPBench数据集上评估多种常见的测试时间扩展方法，并开发了一个新的符号弱验证框架来改进平行扩展结果。实验表明该方法在TPBench上显著优于现有方法，并且在AIME上也表现良好，证明了逐步符号验证解决复杂科学问题的能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管许多测试时间扩展方法已在数学推理基准（如AIME）上得到了发展和评估，但尚未明确这些方法是否可以推广到高等理论物理领域。因此，需要探索并优化适用于物理学问题的扩展技术。

**方法:** 在TPBench数据集上评估一系列常见的测试时间扩展方法，并引入一个新颖的符号弱验证框架，以利用物理问题的结构特性来改善平行扩展效果。此外，还将该方法应用到AIME数据集进行对比验证。

**结果:** 新提出的符号弱验证框架在TPBench数据集上显著超越了现有的测试时间扩展方法，同时在AIME数据集上也展现了有效性，证实了其在解决高级数学问题中的能力。

**结论:** 逐步符号验证是一种强大的工具，可有效应对复杂的科学问题，尤其是在高等理论物理领域。本文的研究表明，这种方法能够显著提高LLMs在科学任务中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-time+Scaling+Techniques+in+Theoretical+Physics+--+A+Comparison+of+Methods+on+the+TPBench+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20729&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [7] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van, Prateek Verma, Chen Zhao, Xintao Wu*

**主要类别:** cs.LG

**AI概要:** 基础模型（FMs）正在推动材料科学领域的人工智能系统变革，本文综述了FMs、代理系统、数据集和计算工具，并提出了任务驱动的分类法，涵盖六个应用领域。同时讨论了单模态和多模态FMs的最新进展，指出了当前局限性并展望了未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 传统机器学习模型通常范围狭窄且需要特定任务工程，而FMs提供跨域泛化和新兴能力，特别适合涉及多种数据类型和规模的材料科学研究挑战。

**方法:** 通过引入任务驱动分类法，涵盖数据提取、原子模拟、属性预测、材料设计、工艺规划和多尺度建模等六大应用领域。回顾标准化数据集、开源工具和自主实验平台。

**结果:** 早期成功表明FMs在材料科学中的潜力，但仍存在可解释性、数据不平衡、安全性及多模态融合等挑战。

**结论:** 未来研究应关注可扩展预训练、持续学习、数据治理和可信度等方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+AI+for+Materials+Science%3A+Foundation+Models%2C+LLM+Agents%2C+Datasets%2C+and+Tools，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20743，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20743&send_immediately=true&force_search=false)

**原文摘要:** Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [8] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief, David Reber, Sean Richardson, Ari Holtzman*

**主要类别:** cs.LG

**AI概要:** 通过动态权重嫁接方法，研究微调语言模型在处理实体和生成预测时如何提取和回忆关系信息，并分析了这些信息通路的必要性和充分性。


<details>
  <summary>更多</summary>
  
**动机:** 了解微调过程中学习到的关系信息是如何存储和使用的，现有方法（如激活修补）不适合此分析，因为它们可能会删除信息。

**方法:** 提出动态权重嫁接方法，在微调和预训练语言模型之间进行比较，以展示微调模型在处理实体时提取关系信息并在生成预测时回忆该信息的方式。

**结果:** 微调模型在处理实体时提取关系信息，并在生成预测时回忆该信息；某些情况下需要两条路径共同作用，而其他情况下单个“丰富”或“回忆”路径就足够了。

**结论:** 微调模型通过任务特定的注意力机制和输出层中的关系提取步骤实现信息回忆，这些路径在不同层中表现出冗余，并涉及不同的模型组件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiple+Streams+of+Relation+Extraction%3A+Enriching+and+Recalling+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20746&send_immediately=true&force_search=false)

**原文摘要:** When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [9] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek, Guy Zamir, Yudong Chen*

**主要类别:** cs.LG

**AI概要:** 本文研究了平均奖励马尔可夫决策过程中的离线强化学习问题，提出了基于目标策略的性能保证，并引入了新的算法和复杂性度量。


<details>
  <summary>更多</summary>
  
**动机:** 离线强化学习在平均奖励MDPs中面临分布偏移和非均匀覆盖等挑战，且理论研究较少。已有工作依赖于单策略数据覆盖假设，但其复杂性度量对所有策略是统一的。

**方法:** 提出了一种基于悲观折扣值迭代的算法，并引入了新的分位数裁剪技术。该方法仅依赖目标策略，利用偏差跨度和新的策略击中半径作为复杂性度量。同时处理了一般弱通信MDPs。

**结果:** 实现了首个完全基于单策略的样本复杂度界限，并展示了硬实例证明学习需要超出目标策略平稳分布的覆盖假设。还开发了几乎与主要结果相匹配的下界。

**结论:** 新提出的算法和复杂性度量为平均奖励离线强化学习提供了更精细的性能保证，并揭示了单策略复杂性度量与先前研究的区别。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Single-Policy+Sample+Complexity+and+Transient+Coverage+for+Average-Reward+Offline+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20904&send_immediately=true&force_search=false)

**原文摘要:** We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [10] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su, Mujin Kwun, Stephanie Gil, Sham Kakade, Nikhil Anand*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Characterization+and+Mitigation+of+Training+Instabilities+in+Microscaling+Formats，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20752&send_immediately=true&force_search=false)

**原文摘要:** Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [11] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan, Peng Wang, Jing Yang, Cong Shen*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的浅层Transformer框架CHOOSE，用于无线符号检测，通过自回归潜在推理步骤增强推理能力，使浅层模型性能可与深层模型媲美，同时保持存储和计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于ICL的Transformer模型虽然能解决无线通信问题，但需要深层架构，导致存储和计算成本高。

**方法:** 设计了CHOOSE框架，在隐藏空间中引入自回归潜在推理步骤，增强了浅层Transformer（1-2层）的推理能力，无需增加模型深度。

**结果:** 实验表明，该方法性能优于传统浅层Transformer，且与深层Transformer相当，同时保持了存储和计算效率。

**结论:** CHOOSE为在资源受限设备上实现基于Transformer的无线接收算法提供了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chain-of-Thought+Enhanced+Shallow+Transformers+for+Wireless+Symbol+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21093&send_immediately=true&force_search=false)

**原文摘要:** Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [12] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong, Huchen Yang, Jin-Long Wu*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种潜在分数生成式AI框架，用于学习计算力学非线性动力系统中的随机、非局部闭合模型和本构定律。通过联合训练卷积自动编码器与条件扩散模型，该方法在降低采样维度的同时保留了关键物理特性，实现了显著的计算加速并保持了预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 建模复杂多尺度动力系统是计算力学中的关键挑战，特别是在没有明确尺度分离的情况下，数值解析所有尺度过于昂贵（如工程湍流）。传统闭合模型方法受限于确定性和局部假设，而基于扩散的随机模型虽然有潜力，但其高昂的计算成本限制了实际应用。

**方法:** 作者提出了一种潜在分数生成式AI框架，结合卷积自动编码器与条件扩散模型，在潜在空间中进行联合训练。此方法降低了采样过程的维度，同时保留了重要的物理特性。

**结果:** 数值结果表明，联合训练方法能够发现一个合适的潜在空间，保证小的重建误差，并确保扩散模型在潜在空间中的良好性能。将提出的随机建模框架集成到数值模拟中，可实现显著的计算加速，同时保持与标准扩散模型相当的预测准确性。

**结论:** 所提出的潜在条件扩散模型框架为复杂多尺度系统的闭合建模提供了一种有效的方法，显著降低了计算成本，同时保持了高预测精度，为实际工程应用提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+and+Non-local+Closure+Modeling+for+Nonlinear+Dynamical+Systems+via+Latent+Score-based+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20771&send_immediately=true&force_search=false)

**原文摘要:** We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [13] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler, Florian Lemmerich*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于线性的神经网络压缩新方法，可将模型大小无损压缩至原来的1/4，并且与现有的重要性剪枝方法兼容。


<details>
  <summary>更多</summary>
  
**动机:** 当前的神经网络压缩方法主要通过衡量参数的重要性与冗余度来减少不必要的参数。为了进一步优化现有的高效解决方案，本文提出了基于线性的压缩方法。

**方法:** 基于ReLU类激活函数的特性，对于几乎总是被激活的神经元表现出线性行为，允许合并后续层。通过引入相关的理论基础并进行实验评估来验证该方法的有效性。

**结果:** 该方法在多数测试模型中实现了无损压缩至原模型大小的1/4。并且在已经经过重要性剪枝的模型上应用此方法时，显示出不同类型压缩之间的干扰很小，表明技术可以成功结合。

**结论:** 本文为一种新的压缩方法奠定了基础，这种方法能够实现更小、更高效的神经网络模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Linearity-based+neural+network+compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21146&send_immediately=true&force_search=false)

**原文摘要:** In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [14] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq, Dan Braun, Lee Sharkey*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的神经网络参数分解方法，称为随机参数分解（SPD），该方法相较于现有的归因参数分解（APD）方法更具可扩展性和鲁棒性，并解决了其他问题如参数收缩。通过将因果中介分析与网络分解方法结合，SPD为机械解释性研究提供了新可能性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的线性参数分解框架中的主要方法——归因参数分解（APD）存在计算成本高和对超参数敏感的问题，限制了其在更大、更复杂模型上的应用。因此，需要一种更高效且稳定的参数分解方法。

**方法:** 论文引入了随机参数分解（SPD）方法，该方法比APD更具可扩展性和鲁棒性，适用于稍大和更复杂的模型。SPD通过避免参数收缩并更好地识别玩具模型中的真实机制来改进现有方法。此外，SPD连接了因果中介分析和网络分解方法。

**结果:** SPD成功地分解了比APD能够处理的更大、更复杂的模型，同时避免了参数收缩问题，并能更好地识别简单模型中的真实机制。这些结果表明SPD在机械解释性研究中具有潜在价值。

**结论:** 随机参数分解（SPD）是一种改进的参数分解方法，可以克服现有方法的局限性，促进线性参数分解技术在更大规模模型中的应用，为机械解释性研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Parameter+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20790&send_immediately=true&force_search=false)

**原文摘要:** A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [15] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews, Sam Witteveen*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于LLM的自动化方法，用于优化GPU内核性能，通过迭代生成和测试优化假设，减少对深度架构知识的需求，特别适用于资源受限或快速变化的硬件环境。


<details>
  <summary>更多</summary>
  
**动机:** 优化GPU内核以实现高性能是一项复杂的任务，通常需要深入的架构知识、广泛的分析和反复试验。在针对较新或文档较少的GPU架构时，这一挑战更加突出，因为传统的开发辅助工具稀缺。

**方法:** 该方法采用多阶段进化过程：（a）战略性选择有前途的先前代码版本作为新迭代的基础；（b）根据现有代码和从通用GPU文献中获取的知识生成优化实验的假设；（c）通过代码修改自主实施这些实验，并使用观察到的时间数据作为性能反馈提交给外部评估系统。

**结果:** 由于在论文提交日期时，正在进行的性能竞赛的定量结果被禁止发布，因此作者详细介绍了架构设计、操作流程和定性见解，展示了LLM驱动代理在民主化和加速GPU内核优化方面的潜力。

**结论:** 基于LLM的“GPU内核科学家”方法可以补偿特定领域人类专业知识的不足，尤其在资源受限或快速演变的硬件环境中具有显著潜力，为高性能计算领域的自动优化提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GPU+Kernel+Scientist%3A+An+LLM-Driven+Framework+for+Iterative+Kernel+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20807&send_immediately=true&force_search=false)

**原文摘要:** Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [16] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal, Jakoba Petri-Koenig, Thomas B. Preußer, Michaela Blott, Shreejith Shanker*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于FINN框架的通用方法，将LSTM模型部署到FPGA上，通过使用ONNX的Scan操作符和自定义转换来支持混合量化及功能验证。在中价股票预测任务中，生成的量化ConvLSTM加速器在性能、资源消耗和推理精度之间取得了平衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的FPGA工具主要针对前馈网络，而LSTM加速通常需要完全定制实现，难以在资源受限环境下实现实时部署。

**方法:** 利用开源可扩展的FINN框架，通过ONNX的Scan操作符对LSTM计算进行建模，支持混合量化和功能验证；引入FINN编译器中的自定义转换，将量化后的ONNX计算图映射到硬件块。

**结果:** 生成的量化ConvLSTM加速器在XCZU7EV设备上实现了性能（延迟）与资源消耗之间的平衡，同时保持或提高了推理精度。

**结论:** 所提出的流程具有通用性，为在FPGA上设计资源高效的RNN加速器铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FINN-GL%3A+Generalized+Mixed-Precision+Extensions+for+FPGA-Accelerated+LSTMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20810&send_immediately=true&force_search=false)

**原文摘要:** Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [17] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko, Jędrzej Ruciński, Dawid Płudowski, Antoni Zajko, Patryzja Żak, Mateusz Zacharecki, Anna Kozak, Katarzyna Woźnica*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Hellsemble的新颖集成框架，用于二元分类。该框架通过迭代处理错误分类实例，形成专门的基础学习者委员会，并使用路由模型分配实例。这种方法提高了分类准确性，同时保持了计算效率和可解释性。实验表明，Hellsemble优于传统集成方法。


<details>
  <summary>更多</summary>
  
**动机:** 集成学习已被证明能有效提高预测性能，但传统方法如bagging、boosting和动态集成选择（DES）存在高计算成本和对异构数据分布适应性有限的问题。

**方法:** Hellsemble是一个新的且可解释的二元分类集成框架，它利用数据集复杂性进行训练和推理。通过将错误分类的实例从简单的模型逐步传递给后续模型，Hellsemble将数据集逐渐划分为困难等级圈，形成一个由专门的基础学习者组成的委员会。每个模型都在越来越具有挑战性的子集上进行训练，同时一个独立的路由模型学习根据推断出的难度将新实例分配给最适合的基础模型。

**结果:** Hellsemble实现了强大的分类准确性，同时保持了计算效率和可解释性。

**结论:** Hellsemble在OpenML-CC18和Tabzilla基准测试中的实验结果表明，它通常优于经典的集成方法。这一发现表明，接受实例级别的难度为构建高效和稳健的集成系统提供了一个有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Divide%2C+Specialize%2C+and+Route%3A+A+New+Approach+to+Efficient+Ensemble+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20814&send_immediately=true&force_search=false)

**原文摘要:** Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [18] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu, Yasin Yilmaz*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新颖的、通用且高效的检测对抗样本的方法，通过训练轻量级回归模型预测深层特征并利用预测误差检测对抗样本，具有高有效性、实时处理效率以及与各种DNN架构和领域的兼容性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对抗攻击检测方法要么对最新攻击技术无效，要么计算效率低下，无法满足实时处理需求。因此，需要一种更高效和普适的检测方法来应对对抗样本问题。

**方法:** 提出了一种基于分析不同DNN层受攻击影响程度的方法，通过训练一个轻量级回归模型，从早期层特征预测深层特征，并使用预测误差检测对抗样本。

**结果:** 理论分析和大量实验证明，该方法在检测对抗样本方面具有高度有效性，计算效率适合实时处理，兼容任何DNN架构，并可应用于图像、视频和音频等多个领域。

**结论:** 所提出的检测方法解决了现有方法的不足，提供了一种普适、高效的对抗样本检测方案，为实际应用中的防御技术提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+and+Efficient+Detection+of+Adversarial+Data+through+Nonuniform+Impact+on+Network+Layers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20816&send_immediately=true&force_search=false)

**原文摘要:** Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [19] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang, Chul-Ho Lee*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了分布式图神经网络（GNN）在链接预测中的性能退化问题，并提出了一种名为SpLPG的方法，通过图稀疏化技术有效减少了通信开销，同时保持了链接预测的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 分布式GNN框架和系统虽然增强了GNN的可扩展性并加速了模型训练，但大多针对节点分类进行了优化。其在链接预测上的性能尚未得到充分探索。

**方法:** 研究发现性能退化的主要原因不仅来自于图划分导致的信息丢失，还与模型训练过程中负样本的抽取方式有关。虽然与每个工作节点共享完整的图信息可以解决该问题并保留链接预测的准确性，但这会导致较高的通信成本。为此，研究提出了SpLPG方法，利用图稀疏化技术以较低的通信成本缓解性能退化问题。

**结果:** 实验结果表明，SpLPG在多个公开的真实世界数据集上表现出有效性，能够将通信开销减少多达约80%，同时大部分保留了链接预测的准确性。

**结论:** SpLPG是一种有效的解决方案，可以在降低通信成本的同时缓解分布式GNN在链接预测任务中的性能退化问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+Distributed+Training+of+Graph+Neural+Networks+for+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20818&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [20] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的受限深度强化学习（CDRL）方法，用于优化雷达通信系统中跟踪与通信之间的时间分配，在满足时间预算约束的同时提升目标通信质量。实验结果表明，该方法在动态环境中能有效提高通信质量。


<details>
  <summary>更多</summary>
  
**动机:** 随着集成传感和通信系统的发展，如何在有限时间内平衡多目标跟踪和数据传输成为重要问题。需要一种智能方法来动态调整时间分配以适应环境变化并满足任务需求。

**方法:** 引入了一种新型的受限深度强化学习（CDRL）框架，用于解决时间分配问题。通过学习在跟踪和通信之间的资源分配策略，该方法能够在时间预算内最大化通信质量，同时确保跟踪性能。

**结果:** 数值实验结果表明，所提出的CDRL框架能够有效提升通信质量，尤其在高度动态环境中表现优异，同时满足时间约束条件。

**结论:** 所提出的CDRL方法为集成传感和通信系统提供了一种有效的解决方案，可以在时间受限的情况下优化跟踪与通信之间的资源分配，从而提升整体系统性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning-Based+Resource+Management+in+Integrated+Sensing+and+Communication+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20849&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [21] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu, Subodh Kalia, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** The paper explores time allocation in multi-function cognitive radar systems using deep reinforcement learning to balance scanning for new targets and tracking existing ones.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of optimally allocating time between detecting new targets and tracking known ones in dynamic environments.

**方法:** Formulate the problem as a multi-objective optimization task, employ deep reinforcement learning algorithms (DDPG and SAC) to find Pareto-optimal solutions, and use NSGA-II to estimate an upper bound on the Pareto front.

**结果:** Both DDPG and SAC algorithms effectively adapt to various scenarios, with SAC showing better stability and sample efficiency. NSGA-II provides an upper bound estimation for the Pareto front.

**结论:** Deep reinforcement learning, particularly SAC, can improve the efficiency and adaptability of cognitive radar systems in managing competing objectives.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Objective+Reinforcement+Learning+for+Cognitive+Radar+Resource+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20853&send_immediately=true&force_search=false)

**原文摘要:** The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [22] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang, Baochun Li*

**主要类别:** cs.LG

**AI概要:** 在LoRA微调中，模型的记忆化风险显著降低且任务表现依然强劲，这与全量微调和其他策略有不同趋势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管预训练中的记忆化已被广泛研究，但微调（特别是LoRA微调）中的记忆化影响尚未被深入探索，因此需要重新审视不同微调策略下的记忆化现象及其影响因素。

**方法:** 通过使用一种基于相似性的记忆化度量方法，研究了模型规模和数据重复等因素对LoRA微调、全量微调及预训练记忆化的影响，并比较了这些方法之间的差异。

**结果:** 发现模型规模和数据重复等因素在LoRA微调中的影响趋势与预训练和全量微调不同；LoRA微调相较于全量微调能显著降低记忆化风险，同时保持良好的任务性能。

**结论:** LoRA微调是一种参数高效的微调方法，能够有效减少模型记忆化带来的数据提取攻击风险，同时不影响模型的任务表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leaner+Training%2C+Lower+Leakage%3A+Revisiting+Memorization+in+LLM+Fine-Tuning+with+LoRA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20856&send_immediately=true&force_search=false)

**原文摘要:** Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [23] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Omniwise的端到端自监督微调管道，首次将大型语言模型（LLMs）应用于GPU内核性能预测任务。该方法无需执行代码或使用性能分析工具，能够直接从内核代码预测关键性能指标，并在AMD MI250和MI300X架构上实现了高精度预测。此外，还开发了在线推理服务器和Visual Studio Code插件以方便开发者使用。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，深度神经网络的发展推动了人工智能的进步，使其在处理复杂数据方面具备前所未有的能力。然而，在性能分析领域，仍然缺乏一种高效、轻量且无需执行代码即可准确预测性能的方法。因此，本研究旨在探索如何利用大型语言模型解决这一问题。

**方法:** 研究提出了Omniwise，这是一种端到端、自监督的微调管道，可以将大型语言模型应用于GPU内核性能预测任务。该方法通过学习内核代码的特征，直接预测包括内存带宽、缓存命中率、GFLOPs和算术强度等关键性能指标。并且，它与具体的模型无关，即使使用仅有3B参数的小型模型也能实现高性能预测。

**结果:** 实验结果表明，Omniwise能够在AMD MI250和MI300X架构上对GPU内核性能进行准确预测，超过90%的预测值相对误差小于10%。这证明了该方法的有效性和准确性。

**结论:** Omniwise是一种创新的解决方案，将大型语言模型引入到性能预测领域，填补了现有技术的空白。它的轻量化设计和高精度预测能力为开发者提供了一种全新的性能分析工具。此外，通过集成在线推理服务器和Visual Studio Code插件，进一步增强了用户体验和实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omniwise%3A+Predicting+GPU+Kernels+Performance+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20886&send_immediately=true&force_search=false)

**原文摘要:** In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [24] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang, Ali Ebrahimpour-Boroojeny, Hari Sundaram*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为RWFT的输出重加权遗忘方法，可以有效从训练好的分类器中删除整个类别而无需完全重新训练。相比现有方法，在新提出的基于总变差距离（TV）度量上性能提升111.45%，在先前使用的度量标准上提升2.79%。


<details>
  <summary>更多</summary>
  
**动机:** 从训练模型中遗忘特定类别对于执行用户删除权和减少有害或有偏见的预测至关重要。然而，完整重新训练成本高昂，且现有的遗忘方法在预测未学习类别样本时无法复制重新训练模型的行为。此外，这些方法容易受到成员推断攻击（MIA-NN）的影响，从而泄露未学习类别。

**方法:** 作者提出了一种简单的概率质量再分配方法，用于预测遗忘类别的样本，该方法对MIA-NN具有鲁棒性。同时引入了一个新的基于总变差（TV）距离的度量来量化残余泄漏，以防止未来方法对新攻击的脆弱性。

**结果:** 通过与机器遗忘领域的最新基线进行广泛实验比较，该方法在先前工作中使用的评估指标以及本文提出的新指标上均达到了与完整重新训练相同的结果。相较于现有最佳方法，在先前使用的度量标准上提高了2.79%，在新提出的基于TV的度量上提高了111.45%。

**结论:** RWFT是一种轻量级技术，能够有效地从训练好的分类器中删除整个类别而无需完全重新训练。它在预测未学习类别样本时表现出色，并且对成员推断攻击具有鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Necessity+of+Output+Distribution+Reweighting+for+Effective+Class+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20893&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [25] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi, Yanning Shen*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的多模型在线合规矩预测算法，通过从有效模型子集中选择模型来减少计算复杂度和预测集大小，并证明了其有效性和优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模型在线合规矩预测方法在处理分布偏移时面临两个主要挑战：1) 包含大量模型的候选集会增加计算复杂度；2) 包含性能差的无关模型可能对预测性能产生负面影响并导致不必要的大预测集。

**方法:** 提出一种新算法，通过收集来自二部图的反馈，在每个时间步识别出一个有效模型子集，并从中选择模型构建预测集。此外，利用预测集大小作为反馈（与模型损失结合），以进一步提高效率。

**结果:** 理论证明表明该算法可以确保有效的覆盖概率并实现次线性后悔。实验证明，所提出的方法能够构建更小的预测集，并优于现有的多模型在线合规矩预测方法。

**结论:** 所提出的算法成功解决了多模型在线合规矩预测中的计算复杂度和预测集大小问题，同时保持了所需的覆盖保证，展现了更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Structured+Feedback+Multimodel+Ensemble+Online+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20898&send_immediately=true&force_search=false)

**原文摘要:** Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [26] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于多代理框架的大型语言模型（LLM）方法，用于化学过程优化。通过自主生成操作约束和迭代多代理优化，该框架在无需预定义操作边界的情况下实现了与传统方法相当的性能，并显著提高了计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统优化方法在操作约束不明确或不可用时变得不实用，工程师需要依靠主观经验来估计可行参数范围。为解决这一瓶颈问题，研究者提出了一种新的多代理框架。

**方法:** 使用基于AutoGen的多代理框架，结合OpenAI的o3模型，包含专门用于约束生成、参数验证、模拟执行和优化指导的代理。通过两个阶段：自主约束生成和迭代多代理优化，消除对预定义操作边界的依赖。

**结果:** 在氢脱烷基化过程中，该框架在成本、产量和产量-成本比指标上表现出与传统优化方法相当的性能，同时具有更高的计算效率，收敛速度提高了31倍。此外，该方法展示了对复杂工艺的理解能力，能够正确识别效用权衡并应用领域知识启发式规则。

**结论:** 该多代理框架在操作约束不明确或不可用的优化场景中展现出巨大潜力，尤其适用于新兴工艺和改造应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-guided+Chemical+Process+Optimization+with+a+Multi-Agent+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20921，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20921&send_immediately=true&force_search=false)

**原文摘要:** Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [27] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh, Pierre Le Bodic, Geoffrey I. Webb, Mario Boley*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Representation+Learning+for+Additive+Rule+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20927&send_immediately=true&force_search=false)

**原文摘要:** Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [28] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** 在雷达资源管理中，将深度学习与LIME结合的DL-LIME方法，在保真度和任务性能上都优于传统LIME，并揭示了决策中的重要因素。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络的“黑箱”特性限制了其可解释性，而现有的LIME方法在采样过程中忽略了特征之间的相关性。因此，需要一种改进的方法来提高决策的可解释性并优化性能。

**方法:** 提出了一种修改版的LIME方法，即DL-LIME，通过将深度学习整合到采样过程中，克服了传统LIME忽略特征相关性的缺陷。该方法被应用于雷达资源管理的深度强化学习中。

**结果:** 数值结果表明，DL-LIME在保真度和任务性能两方面均优于传统LIME，并能提供雷达资源管理决策中关键因素的见解。

**结论:** DL-LIME提高了决策过程的可解释性和性能，适用于雷达资源管理等领域的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+AI+for+Radar+Resource+Management%3A+Modified+LIME+in+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20916&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [29] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen, Xiantao Cai, Jia Wu, Wenbin Hu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的抗体设计框架AbMEGD，通过多尺度等变图扩散方法，解决了现有计算方法在捕捉几何特征和泛化新抗原接口方面的不足，显著提高了氨基酸恢复率、改进百分比，并减少了关键CDR-H3区域的均方根偏差。


<details>
  <summary>更多</summary>
  
**动机:** 抗体设计对于治疗和诊断的发展至关重要，尤其是针对具有多样化结合界面的复杂抗原。当前的计算方法在捕捉几何特征的同时保持对称性以及泛化新型抗原界面方面存在局限性。

**方法:** 提出了AbMEGD，一个端到端框架，集成了多尺度等变图扩散方法，用于抗体序列和结构的协同设计。该方法结合原子级几何特征与残基级嵌入，确保了几何精度、计算效率和对复杂抗原的强大泛化能力。

**结果:** 实验表明，与领先模型DiffAb相比，AbMEGD在SAbDab数据库上的表现更优：氨基酸恢复率提高10.13%，改进百分比上升3.32%，关键CDR-H3区域的均方根偏差减少0.062Å。

**结论:** AbMEGD在保持结构完整性的同时提升了功能，为序列-结构协同设计和亲和力优化设定了新的基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Antibody+Design+and+Optimization+with+Multi-scale+Equivariant+Graph+Diffusion+Models+for+Accurate+Complex+Antigen+Binding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20957&send_immediately=true&force_search=false)

**原文摘要:** Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [30] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的图基分层强化学习框架——严格子目标执行（SSE），该框架通过结构化约束高层决策来强制执行单步子目标可达性，同时采用解耦探索策略和失败感知路径优化方法，在长时域任务中表现出更高的效率和成功率。


<details>
  <summary>更多</summary>
  
**动机:** 长时域目标条件任务对于强化学习提出了基本挑战，尤其是在目标遥远且奖励稀疏的情况下。现有的分层和基于图的方法虽然提供部分解决方案，但常因子目标不可行和规划效率低下而受限制。

**方法:** SSE是一种图基分层RL框架，通过结构化约束高层决策来确保单步子目标的可达性。为了增强探索能力，SSE采用了解耦探索策略，系统地遍历目标空间中未充分探索的区域。此外，还引入了失败感知路径优化方法，根据低层成功概率动态调整边成本，从而提高子目标的可靠性。

**结果:** 实验结果表明，SSE在多个长时域基准测试中，相较于现有的目标条件RL和分层RL方法，在效率和成功率方面均有显著提升。

**结论:** SSE框架通过解决子目标可行性和提高规划效率的问题，在长时域任务中表现出了优越性能，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strict+Subgoal+Execution%3A+Reliable+Long-Horizon+Planning+in+Hierarchical+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21039&send_immediately=true&force_search=false)

**原文摘要:** Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [31] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang, Ming Zhou, Shaopeng Zhai, Ying Sun, Hui Xiong*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于后悔值感知的方法，通过将技能发现框架化为技能生成和策略学习的最小最大博弈，在时间表示学习的基础上扩展了发现的技能空间，提升了效率和多样性。实验表明，该方法在高维环境中比现有方法提高了15%的零样本性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的无监督技能发现方法虽然在探索方面表现良好，但在效率上仍有局限性，特别是在高维情况下。

**方法:** 将技能发现视为技能生成与策略学习之间的最小最大博弈，并在此基础上提出了基于后悔值感知的方法，以指导可学习的技能生成器进行技能发现，同时避免退化。

**结果:** 在不同复杂性和维度大小的环境中进行实验，结果表明该方法在效率和多样性方面均优于基线方法，并且在高维环境中实现了15%的零样本改进。

**结论:** 所提出的方法在技能发现中表现出更高的效率和多样性，尤其是在高维环境中的零样本改进显著。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Skill+Discovery+via+Regret-Aware+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21044&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [32] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei, Mehrdad Saberi, Abhilasha Ravichander, Soheil Feizi*

**主要类别:** cs.LG

**AI概要:** 大型语言模型可能受到包含在训练数据中的不良数据点的影响，如私人数据、版权材料、事实不准确的数据或降低模型性能的数据。完全重新训练以消除这些数据点的影响是计算上不可行的。因此，出现了一些算法来消除特定数据点的影响，同时保留模型的其他部分。本文提出了一种新的算法MSA，通过利用模型检查点来估计和撤销数据点的影响。实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法，表明MSA可能是实现更灵活、能够数据擦除的大型语言模型的有效方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的训练数据中可能存在不良数据点，包括私人数据、版权材料、事实不准确的数据等，这些数据会影响模型的性能。由于重新训练模型以排除这些数据点在计算上是不可行的，因此需要一种有效的算法来消除这些数据点的影响，同时保留模型的其他部分。

**方法:** 本文提出了一种名为MSA的新算法，该算法通过利用模型检查点（即捕获预训练不同阶段模型状态的工件）来估计和撤销数据点的影响。

**结果:** 实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法。

**结论:** MSA可能是一种有效的方法，可以实现更灵活的大型语言模型，这些模型具备数据擦除的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+State+Arithmetic+for+Machine+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20941&send_immediately=true&force_search=false)

**原文摘要:** Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [33] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann, Luca Corbucci, Mattia Cerrato, Anna Monreale*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的工具和数据集集合，用于评估和改进联邦学习中的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习虽然可以保护用户隐私，但其系统中的不公平问题仍是一个重要挑战。现有的公平性解决方案大多仅关注单一敏感属性，忽视了不同客户端多样且可能冲突的公平性需求。

**方法:** 贡献包括：1) 开发FeDa4Fair库生成适用于评估公平联邦学习方法的数据集；2) 发布四个带有偏差异质性的数据集及相应的基准测试；3) 提供可用于评估这些数据集公平结果的现成函数。

**结果:** 提供了支持公平性研究的工具、数据集和基准测试，有助于推动更强大和可重复的联邦学习公平性研究。

**结论:** 通过提供的资源和方法，可以更好地评估和改进联邦学习环境下的公平性，为未来的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FeDa4Fair%3A+Client-Level+Federated+Datasets+for+Fairness+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21095&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [34] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的概念基础模型H-CMR，通过学习的概念有向无环图和神经注意力机制，提供对概念和任务预测的可解释性，并在实验中展现出与最先进方法相当的性能，同时允许通过概念和模型干预进行强烈的人机交互。


<details>
  <summary>更多</summary>
  
**动机:** 当前的概念基础模型(CBMs)虽然能够对最终任务预测提供可解释性，但其概念预测通常是通过黑箱神经网络实现的，缺乏透明度。因此需要一种能对概念和任务预测都提供可解释性的模型。

**方法:** 提出了分层概念记忆推理器(H-CMR)，该模型使用学习到的有向无环图来建模概念间的关系，边代表定义概念的逻辑规则。在推理过程中，H-CMR采用神经注意机制选择一部分规则，并以分层方式应用这些规则来预测所有概念和最终任务。

**结果:** 实验证明，H-CMR在匹配最先进性能的同时，允许通过概念和模型干预进行强烈的人机交互。前者可以在推理时显著提高准确性，后者在训练时如果有背景知识可用可以提高数据效率。

**结论:** H-CMR不仅实现了与现有最先进技术相媲美的性能，还提供了更强的可解释性和人机交互能力，有助于提高推理准确性和训练数据效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Hierarchical+Concept+Reasoning+through+Attention-Guided+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21102&send_immediately=true&force_search=false)

**原文摘要:** Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [35] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang, Zhen Zhang, Rupak Vignesh Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang*

**主要类别:** cs.LG

**AI概要:** Fine-tuning vision language models (VLMs) using backpropagation is effective but not suitable for edge devices due to memory constraints. This paper proposes SharpZO, a hybrid BP-free optimization method that enhances zeroth-order VLM fine-tuning via sharpness-aware warm-up training and two-stage optimization, improving accuracy and convergence speed.


<details>
  <summary>更多</summary>
  
**动机:** Fine-tuning VLMs has been successful in various tasks but requires model gradients through backpropagation, which is unsuitable for memory-constrained edge devices.

**方法:** SharpZO consists of a two-stage optimization process: a sharpness-aware ES stage for global exploration and smoothing of the loss landscape to construct a strong initialization, followed by a fine-grained local search via sparse ZO optimization.

**结果:** Extensive experiments on CLIP models show that SharpZO significantly improves accuracy and convergence speed, achieving up to 7% average gain over state-of-the-art forward-only methods.

**结论:** SharpZO is an effective BP-free optimization approach that enhances the performance of zeroth-order VLM fine-tuning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SharpZO%3A+Hybrid+Sharpness-Aware+Vision+Language+Model+Prompt+Tuning+via+Forward-Only+Passes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20990&send_immediately=true&force_search=false)

**原文摘要:** Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [36] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** The paper introduces an antifragile RL framework for UAV navigation that uses discounted Thompson sampling to enhance adaptability against adversarial attacks.


<details>
  <summary>更多</summary>
  
**动机:** Existing robust RL methods have limited generalization to out-of-distribution shifts in adversarial environments, particularly due to fixed perturbation handling.

**方法:** An antifragile RL framework is proposed, which incorporates a switching mechanism based on discounted Thompson sampling (DTS). This selects among multiple robust policies modeled as a multi-armed bandit problem, adapting to evolving adversarial strategies.

**结果:** Numerical simulations show superior performance in complex navigation environments with dynamic 3D obstacles and strong adversarial attacks, achieving shorter path lengths and higher conflict-free navigation rates compared to conventional methods.

**结论:** The antifragile RL framework effectively enhances adaptability and resilience against unseen adversarial attacks in UAV navigation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Policy+Switching+for+Antifragile+Reinforcement+Learning+for+UAV+Deconfliction+in+Adversarial+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21127&send_immediately=true&force_search=false)

**原文摘要:** The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [37] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton, Valeriy Klyukin, Maksim Artemev, Denis Derkach, Nikita Orlov, Humphrey Shi*

**主要类别:** cs.LG

**AI概要:** 显式密度学习者由于其对概率分布的更好建模能力，正成为生成模型中越来越受欢迎的技术。尽管它们的训练难度较大且采样质量较低，但与生成对抗网络相比，它们具有执行密度估计和精确潜在变量推断的优势。本文提出了新的知识蒸馏技术，以提高小型学生归一化流的采样质量和密度估计。通过研究组成归一化流中知识蒸馏的能力，我们发现可以通过蒸馏使学生模型显著缩小，同时获得实质性的性能提升。更小的模型会带来更高的吞吐量。


<details>
  <summary>更多</summary>
  
**动机:** 显式密度模型（如归一化流）在生成模型中有优势，但存在训练困难和采样质量低的问题。因此，作者希望通过知识蒸馏技术来改善小型学生归一化流的性能，从而实现更高效的模型。

**方法:** 提出新的知识蒸馏技术应用于组成归一化流，利用其独特的性质进行非传统的知识转移，包括在中间层之间传输知识。通过这种方法，训练出显著更小的学生模型，并取得性能上的实质性提升。

**结果:** 通过知识蒸馏，成功地使学生模型变得更小，同时提高了采样质量和密度估计的性能。更小的模型还带来了更高的吞吐量，因为吞吐量与网络中的双射器数量成反比。

**结论:** 知识蒸馏在组成归一化流中表现出色，能够有效减少模型大小并提升性能，为高效生成模型的设计提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilling+Normalizing+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21003&send_immediately=true&force_search=false)

**原文摘要:** Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [38] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda, Adolfo Perrusquia, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种抗脆弱强化学习框架，通过模拟攻击者逐步增强观测空间扰动，使RL代理能够适应更广泛的OOD观察，并在UAV冲突解除场景中展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习策略容易受到OOD对抗攻击的影响，导致价值估计显著下降，决策不安全或次优。为了解决这种脆弱性，需要一种新的方法来提高RL代理的抗脆弱能力。

**方法:** 提出抗脆弱RL框架，引入模拟攻击者逐步增加观测空间扰动强度，使代理适应更广的OOD观察并预测未见过的攻击；通过理论定义脆弱性和抗脆弱性，推导遗忘稳定的适应条件；使用Wasserstein距离最小化实现迭代专家指导的批评者对齐。

**结果:** 在UAV冲突解除场景中，与标准和鲁棒RL基线相比，抗脆弱策略在PGD和GPS欺骗攻击下表现更优，累积奖励高出15%，冲突事件减少30%以上。

**结论:** 抗脆弱强化学习在理论上和实践中都是可行的，可以用于安全和有弹性的决策制定，特别是在威胁情景不断演变的环境中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Curriculum-Guided+Antifragile+Reinforcement+Learning+for+Secure+UAV+Deconfliction+under+Observation-Space+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21129&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [39] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang, Mangal Prakash, Hehuan Ma, Jianyuan Deng, Yuzhi Guo, Amina Mollaysa, Tommaso Mansi, Rui Liao, Junzhou Huang*

**主要类别:** cs.LG

**AI概要:** TRIDENT是一种新的框架，它结合了分子的SMILES、文本描述和分类功能注释来学习丰富的分子表示。通过全局体积对齐目标和局部对齐目标，以及动量平衡机制，TRIDENT在11个下游任务中达到了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多模态学习在学习分子表示方面是一个强大的范式，但以前的工作大多忽略了分子的文本和分类信息。本研究旨在利用这些信息来提高分子性质预测的效果。

**方法:** TRIDENT整合了分子的SMILES、文本描述和分类功能注释，创建了一个全面的分子-文本对数据集。使用基于体积的对齐目标进行三模态特征的全局对齐，并引入新的局部对齐目标以捕捉分子子结构与子文本描述之间的详细关系。此外，采用基于动量的机制动态平衡全局和局部对齐。

**结果:** TRIDENT在11个下游任务中表现出色，达到最先进的水平，证明了结合SMILES、文本和分类功能注释对于分子性质预测的价值。

**结论:** TRIDENT提供了一种有效的策略来利用文本和分类信息学习分子表示，其方法显著提高了多种分子性质预测任务的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRIDENT%3A+Tri-Modal+Molecular+Representation+Learning+with+Taxonomic+Annotations+and+Local+Correspondence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21028&send_immediately=true&force_search=false)

**原文摘要:** Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [40] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang, Hongbin Wang, Tianwang Jia, Xingyi He, Siyang Li, Dongrui Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的双分支卷积Transformer网络DBConformer，用于EEG解码。该模型在电机想象和癫痫检测数据集上表现出色，参数量少且特征具有生理可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于CNN的EEG解码方法难以捕捉长时间依赖性和全局通道间关系，而现有的CNN-Transformer混合模型虽然部分解决了这一问题，但其串行设计导致局部和全局特征的整合不佳，并且常常忽略显式的通道建模。

**方法:** 提出了DBConformer，一个双分支卷积Transformer网络。它通过时间Conformer捕捉长时间依赖性，通过空间Conformer提取通道间交互，同时使用轻量级通道注意力模块进一步优化空间表示。

**结果:** 在五个电机想象数据集和两个癫痫检测数据集上的广泛实验表明，DBConformer以比高容量EEG Conformer基线模型少八倍以上的参数，持续优于10个竞争基线模型。可视化结果确认DBConformer提取的特征具有生理可解释性，并与电机想象中的感觉运动先验一致。

**结论:** DBConformer在EEG解码中表现出优越的性能和可解释性，使其成为鲁棒和可解释的EEG解码的可靠选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DBConformer%3A+Dual-Branch+Convolutional+Transformer+for+EEG+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21140&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [41] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, Dong Gong*

**主要类别:** cs.LG

**AI概要:** MoRA是一种新的混合专家方法，通过自激活和稀疏秩激活解决持续学习中的灾难性遗忘和任务干扰问题。它将每个秩-r更新分解为r个秩-1组件，实现细粒度的专家利用，同时减轻干扰和冗余。实验表明，MoRA在CLIP和大语言模型上有效提升了持续学习性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LoRA-based MoE方法通过分配和冻结任务特定适配器来缓解遗忘，但存在干扰、冗余和路由模糊等问题。这些问题限制了跨任务有用组件的选择性重用，导致新专家知识的重复或矛盾，以及任务特征的混淆。

**方法:** 提出了一种名为MoRA的方法，其核心思想是将每个秩-r更新分解为r个秩-1组件，并将每个组件视为独立专家。这种方法允许更细粒度地利用秩-1专家，从而减轻干扰和冗余。此外，MoRA还提出了让每个秩-1专家通过中间激活推断自身相关性的机制，结合秩剪枝和激活预算，以适应性选择每个输入的稀疏秩混合。

**结果:** 在CLIP和大型语言模型上的持续学习任务中验证了MoRA的有效性。结果表明，MoRA显著增强了预训练模型的持续学习能力，同时改善了泛化性能并减轻了遗忘。

**结论:** MoRA通过引入细粒度的秩-1专家混合和稀疏激活机制，成功解决了现有方法中的干扰、冗余和路由模糊问题，为持续学习领域提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Little+By+Little%3A+Continual+Learning+via+Self-Activated+Sparse+Mixture-of-Rank+Adaptive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21035&send_immediately=true&force_search=false)

**原文摘要:** Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [42] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng, Meng Zhang, Ming Tang*

**主要类别:** cs.LG

**AI概要:** 本文分析了联邦学习在概念漂移下的性能，并提出了一种缓解性能下降的算法。通过信息论方法建模和评估，实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有联邦学习研究多基于静态数据集，但现实世界数据通常是流式且分布会变化（概念漂移），这会导致模型性能下降。

**方法:** 1. 使用信息论分析联邦学习在概念漂移下的性能。
2. 建模概念漂移为马尔可夫链，引入“Stationary Generalization Error”来评估模型对未来未见数据的捕捉能力。
3. 提出一种结合KL散度和互信息正则化的算法，增强长期性能。
4. 研究三种漂移模式（周期性、渐进性和随机性）对联邦学习性能的影响，并探索性能-成本权衡。

**结果:** 实验结果表明，三种漂移模式显著影响联邦学习性能，所提方法在这三种模式下均优于现有方法。

**结论:** 本文提出的算法能够有效缓解概念漂移对联邦学习性能的影响，同时理论分析和实验证明了方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Information-Theoretic+Analysis+for+Federated+Learning+under+Concept+Drift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21036&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [43] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为DiLoCoX的低通信需求的大规模去中心化集群训练框架，适用于参数超百亿的基础模型（如大语言模型）。通过结合管道并行性、双优化策略、一步延迟通信与本地训练重叠以及自适应梯度压缩方案，显著提升了模型参数规模和预训练速度。实验表明，DiLoCoX可以在1Gbps网络上对107B参数模型进行预训练，并相较于传统的AllReduce方法实现了357倍的分布式训练加速，同时保持了模型收敛性的稳定。这是首个成功应用于超百亿参数模型的去中心化训练框架。


<details>
  <summary>更多</summary>
  
**动机:** 当前大规模基础模型（特别是大语言模型）的分布式训练依赖于高通信需求的集中式集群，限制了在慢速网络或去中心化集群上的应用。因此，研究者试图开发一种能够在低带宽网络环境下运行的去中心化训练框架，以应对超过100亿参数的模型训练需求。

**方法:** 论文提出了DiLoCoX框架，整合了以下关键技术：1) 管道并行性（Pipeline Parallelism）；2) 双优化策略（Dual Optimizer Policy）；3) 一步延迟通信与本地训练重叠（One-Step-Delay Overlap of Communication and Local Training）；4) 自适应梯度压缩方案（Adaptive Gradient Compression Scheme）。这些技术共同作用，减少了通信开销，提高了训练效率。

**结果:** 理论分析证明了一步延迟通信与本地训练重叠以及自适应梯度压缩方案的有效性。实验证明，DiLoCoX能够在1Gbps网络上成功预训练107B参数的基础模型，相较于传统AllReduce方法实现了357倍的训练加速，且模型收敛性能几乎没有下降。

**结论:** DiLoCoX是首个成功应用于超百亿参数模型的去中心化训练框架，其低通信需求和高效训练能力为大规模模型在慢速网络环境下的训练提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiLoCoX%3A+A+Low-Communication+Large-Scale+Training+Framework+for+Decentralized+Cluster，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21263，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21263&send_immediately=true&force_search=false)

**原文摘要:** The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [44] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang, Peijia Li, Furao Shen, Jian Zhao*

**主要类别:** cs.LG

**AI概要:** Modern deep learning models depend on large datasets but incur high costs. This paper introduces epsilon-sample cover and RL-Selector, a reinforcement learning-based data selection method that reduces redundancy and training costs while maintaining or improving performance.


<details>
  <summary>更多</summary>
  
**动机:** Current deep learning architectures require large-scale datasets for training, which are computationally and storage-intensive. Additionally, real-world datasets often contain redundancies, necessitating more efficient data usage paradigms.

**方法:** The authors introduce the concept of epsilon-sample cover to quantify sample redundancy based on inter-sample relationships. They reformulate data selection as a reinforcement learning (RL) problem and propose RL-Selector, where an RL agent optimizes the selection policy using epsilon-sample cover as a reward signal derived from the evolving dataset distribution.

**结果:** Experiments across various benchmark datasets and architectures show that the proposed method outperforms existing state-of-the-art techniques. Models trained with the selected datasets exhibit better generalization performance and improved training efficiency.

**结论:** RL-Selector provides a novel approach to data selection by leveraging reinforcement learning and epsilon-sample cover, leading to reduced training costs and enhanced model performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL-Selector%3A+Reinforcement+Learning-Guided+Data+Selection+via+Redundancy+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21037&send_immediately=true&force_search=false)

**原文摘要:** Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [45] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman, Jing Bi*

**主要类别:** cs.LG

**AI概要:** rQdia在基于像素的深度强化学习中通过增广图像来正则化Q值分布，提升算法性能。


<details>
  <summary>更多</summary>
  
**动机:** 在基于像素的深度强化学习中，需要一种方法来正则化Q值分布以提高样本效率和长期训练效果。

**方法:** 使用简单的辅助损失函数（MSE），使Q值分布在增广图像上趋于一致，从而提升DrQ、SAC等算法的性能。

**结果:** rQdia在MuJoCo Continuous Control Suite和Atari Arcade环境中分别提高了9/12、10/12以及18/26任务的表现，特别是在样本效率和长期训练方面。

**结论:** rQdia成功推动了无模型连续控制从像素超越状态编码基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是rQdia%3A+Regularizing+Q-Value+Distributions+With+Image+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21367&send_immediately=true&force_search=false)

**原文摘要:** rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [46] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou, Tom Jacobs, Advait Gadhikar, Rebekka Burkholz*

**主要类别:** cs.LG

**AI概要:** 微调大型预训练神经网络资源密集。通过分析梯度与权重关系，发现大梯度常与小权重相关。基于此提出NANOADAM方法，动态更新小权重参数，提供无梯度计算、保留大权重、允许更大学习率等优势，实验表明其在NLP和视觉任务中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 微调大型预训练模型资源消耗大，限制训练参数子集是一种常见方法。作者观察到微调时大梯度常与小权重相关，比从头训练时更明显。

**方法:** 提出NANOADAM方法，在微调过程中仅动态更新小权重参数。该方法无需梯度计算即可确定参数子集，保留大权重以减少灾难性遗忘，并允许使用更大的学习率。

**结果:** 在NLP和视觉任务的实验中，NANOADAM方法表现出更好的泛化性能。

**结论:** NANOADAM方法通过动态更新小权重参数，提供多种实际优势，包括无需梯度计算、保留关键特征以及改善泛化性能等。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pay+Attention+to+Small+Weights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21374&send_immediately=true&force_search=false)

**原文摘要:** Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [47] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng, Ming Tang*

**主要类别:** cs.LG

**AI概要:** 在联邦学习中，提出FedDAA框架以应对多源概念漂移，并保留有用的历史知识，实验表明其在多个数据集上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的数据分布会随时间变化，产生概念漂移，而现有方法主要关注真实漂移，无法有效应对虚拟或标签漂移，这可能导致灾难性遗忘。

**方法:** 提出了FedDAA框架，包含三个模块：聚类数量确定模块、真实漂移检测模块和概念漂移适应模块，以区分不同漂移源并采取相应的策略进行适应。

**结果:** 理论分析提供了收敛性保证，实验结果表明FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上分别比现有方法提高了7.84%到8.52%的准确率。

**结论:** FedDAA能够有效应对多源概念漂移，同时保留历史知识，在多个数据集上表现出显著的性能提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedDAA%3A+Dynamic+Client+Clustering+for+Concept+Drift+Adaptation+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21054&send_immediately=true&force_search=false)

**原文摘要:** In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [48] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng, Bochuan Zhou, Yuping Song*

**主要类别:** cs.LG

**AI概要:** 提出了一种增强的时间感知图注意力网络（ATGAT），通过三个模块提升加密货币交易欺诈检测性能，包括高级时间嵌入模块、时间感知三重注意力机制和加权BCE损失。实验表明，该方法在Elliptic++数据集上AUC达到0.9130，优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 加密货币交易欺诈检测面临日益复杂的交易模式和严重的类别不平衡问题，传统方法依赖手动特征工程，难以捕捉交易网络中的时间和结构依赖关系。

**方法:** 设计了增强的时间感知图注意力网络（ATGAT），包含以下模块：(1) 高级时间嵌入模块，融合多尺度时间差异特征与周期性位置编码；(2) 时间感知三重注意力机制，联合优化结构、时间和全局上下文注意力；(3) 使用加权BCE损失解决类别不平衡问题。

**结果:** 在Elliptic++数据集上的实验表明，ATGAT的AUC为0.9130，相较于最佳传统方法XGBoost提高了9.2%，相较于GCN提高了12.0%，相较于标准GAT提高了10.0%。

**结论:** 本研究验证了时间感知和三重注意力机制对图神经网络的增强效果，并为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时间图异常检测任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal-Aware+Graph+Attention+Network+for+Cryptocurrency+Transaction+Fraud+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21382，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21382&send_immediately=true&force_search=false)

**原文摘要:** Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [49] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种利用知识图谱生成高质量指令数据的新方法，显著提升大型语言模型的工具使用能力和整体性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型需要通过工具使用来增强问题解决能力与扩展应用范围，但现有生成指令数据的方法质量不足，限制了工具的有效使用。

**方法:** 通过知识图谱提取查询路径并转换为用户查询，将实体关系转化为可操作工具，并解析每个查询路径以生成详细解决方案步骤，从而创建高质量指令数据。

**结果:** 实验表明，仅用少量合成数据进行微调即可显著提升大型语言模型的工具使用效率和整体能力。

**结论:** 使用知识图谱生成高质量指令数据是提高大型语言模型工具使用能力的有效方法，具有广阔的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+LLM+Tool+Use+with+High-quality+Instruction+Data+from+Knowledge+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21071&send_immediately=true&force_search=false)

**原文摘要:** Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [50] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha*

**主要类别:** cs.LG

**AI概要:** 尽管大语言模型（LLMs）广泛使用，但它们存在产生错误信息和校准不良的问题。因此，在高风险领域中对这些模型的不确定性进行量化尤为重要。先前的研究通过在微调模型的低秩适应（LoRA）参数上进行推理，使贝叶斯深度学习方法更具可行性。然而，这些方法难以扩展到更大的LLM，因为与LoRA相比需要额外的参数。本文提出了可扩展贝叶斯低秩适应方法ScalaBL，通过随机变分子空间推理在r维子空间中进行贝叶斯推理，将LoRA参数重新用作投影矩阵，从而将子空间样本映射到LLM的完整权重空间。该方法仅需约1000个额外参数即可实现与最先进的方法竞争的性能，并且能够扩展到迄今为止最大的贝叶斯LLM，其基本参数数量是之前工作的四倍。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型（LLMs）虽然被广泛应用，但它们会产生不正确的信息并且校准较差。这使得对这些模型的不确定性量化变得尤为重要，特别是在自主性和医疗等高风险领域。

**方法:** 我们提出了ScalaBL方法，它在r维子空间中执行贝叶斯推理，其中LoRA秩为r。通过将LoRA参数重新用作投影矩阵，我们可以将此子空间中的样本映射到LLM的完整权重空间。这种方法允许我们使用随机变分推理来学习我们方法的所有参数。

**结果:** 尽管子空间维度较低，但我们能够以仅约1000个额外参数实现与最先进方法竞争的性能。此外，它使我们能够扩展到迄今为止最大的贝叶斯LLM，其基本参数数量是之前工作的四倍。

**结论:** ScalaBL提供了一种有效的解决方案，用于对大型语言模型进行不确定性量化，同时只需要少量额外参数即可扩展到更大规模的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Bayesian+Low-Rank+Adaptation+of+Large+Language+Models+via+Stochastic+Variational+Subspace+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21408&send_immediately=true&force_search=false)

**原文摘要:** Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [51] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship, Luis Miralles-Pechuan, Stephen O'Sullivan*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于优化低存储需求的扩展稳定性龙格-库塔（ESRK）方法。通过系统性参数减少，在保持四阶精度的同时显著提高了计算效率。在基准问题上的验证表明，该方法相较于传统ESRK优化过程可将IPOPT运行时间减少25%，同时保持数值稳定性和准确性。这为高保真模拟中的资源效率改进以及低存储Runge-Kutta方法在实际应用中的扩展奠定了基础，同时也开启了使用深度强化学习和AutoML进行启发式搜索的新方向。


<details>
  <summary>更多</summary>
  
**动机:** 在科学与工程的大规模计算问题中，如天气预报、空气动力学分析和复杂生物建模，扩展稳定性龙格-库塔（ESRK）方法起着关键作用。然而，对于高阶、低存储方案，要在精确度、稳定性和计算效率之间取得平衡仍然具有挑战性。

**方法:** 研究采用了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于自动化启发式发现，以优化低存储需求的ESRK方法。具体而言，利用GA驱动的变异进行搜索空间探索，并采用受RL启发的状态转换机制动态优化启发式选择。这种方法实现了系统性的参数减少，同时保留了四阶精度。

**结果:** 在包括1D和2D Brusselator系统及稳态Navier-Stokes方程等基准问题上的测试表明，表现最佳的启发式方法相比传统的ESRK优化流程，可以将IPOPT运行时间减少25%，并且保持数值稳定性和精度。

**结论:** 研究结果展示了自适应启发式发现改善高保真模拟资源效率的潜力，拓宽了低存储Runge-Kutta方法在实际计算流体力学、物理模拟及其他高要求领域中的应用范围。这项工作为数值方法的启发式优化建立了一个新的范例，为进一步利用深度强化学习和基于AutoML的启发式搜索开辟了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimising+4th-Order+Runge-Kutta+Methods%3A+A+Dynamic+Heuristic+Approach+for+Efficiency+and+Low+Storage，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21465&send_immediately=true&force_search=false)

**原文摘要:** Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [52] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale, Nicola Dall'Ora, Sebastiano Gaiardelli, Enrico Fraccaroli, Nicola Mazzocca, Franco Fummi*

**主要类别:** cs.LG

**AI概要:** A novel unsupervised fault diagnosis methodology for CPSs is proposed, integrating anomaly detection, process mining, and stochastic simulation. It is validated on the RoAD dataset.


<details>
  <summary>更多</summary>
  
**动机:** Manual modeling of faulty behaviors in CPSs requires extensive domain expertise and often results in complex, error-prone models that are difficult to interpret.

**方法:** The method involves detecting collective anomalies in multivariate time series, transforming them into event logs, applying process mining to discover interpretable process models, and using stochastic simulation for root cause analysis.

**结果:** Experimental results on the RoAD dataset show the effectiveness of the methodology in modeling, simulating, and classifying faulty behaviors in CPSs.

**结论:** This approach enables the creation of comprehensive fault dictionaries, supporting predictive maintenance and digital twin development for industrial environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Process+mining-driven+modeling+and+simulation+to+enhance+fault+diagnosis+in+cyber-physical+systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21502&send_immediately=true&force_search=false)

**原文摘要:** Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [53] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou, Constantin Brif, Ismini Lourentzou*

**主要类别:** cs.LG

**AI概要:** 多变量时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于变量间复杂的依赖关系、时间动态特性和稀疏的异常标签而具有挑战性。本文提出了mTSBench，这是目前最大的MTS-AD基准测试，涵盖了19个数据集和12个不同应用领域的344个带标签的时间序列。mTSBench评估了24种异常检测方法，包括基于大型语言模型（LLM）的多变量时间序列检测器，并在标准化条件下系统地基准化无监督模型选择技术。结果证实，没有任何单一检测器在所有数据集上表现优异，强调了模型选择的重要性。然而，即使是最先进的选择方法也远非最佳，揭示了关键差距。mTSBench提供了一个统一的评估套件，以实现严格、可重复的比较，并推动自适应异常检测和稳健模型选择的未来发展。


<details>
  <summary>更多</summary>
  
**动机:** 当前多变量时间序列异常检测面临诸多挑战，例如复杂的数据特性、稀疏标注以及缺乏全面的基准评测工具。这促使研究者开发一个大规模、跨领域的基准测试平台来评估现有的检测方法和模型选择技术。

**方法:** 构建了mTSBench，一个涵盖19个数据集、12个应用领域和344个带标签时间序列的基准测试平台。通过该平台，对24种异常检测方法进行了评估，其中包括基于大型语言模型的检测器，并在标准化条件下系统地基准化无监督模型选择技术。

**结果:** 评估结果显示，没有一种检测器能够在所有数据集上表现出色，表明模型选择对于提高检测性能至关重要。此外，最先进的模型选择方法仍存在显著改进空间。

**结论:** mTSBench为多变量时间序列异常检测提供了统一的评估框架，有助于推动未来更自适应的异常检测和更稳健的模型选择技术的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mTSBench%3A+Benchmarking+Multivariate+Time+Series+Anomaly+Detection+and+Model+Selection+at+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21550，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21550&send_immediately=true&force_search=false)

**原文摘要:** Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [54] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson, Laurence Aitchison*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的架构，通过动态跳过中间层来提高Transformer效率，尽管对于简单令牌旨在减少计算需求并可能促进多级表示层次结构的出现，但在所研究的规模下，与密集基线相比没有显示出更好的验证交叉熵和估计FLOPs之间的权衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法通常针对单独的模块或独立地跳过层，而解释性研究表明Transformer的中间层表现出更大的冗余，早期层将信息聚合到标记位置。基于这些见解，提出了一个新架构。

**方法:** 采用一种学习的门控机制根据输入决定是否跳过对称范围的中心块，并使用门控注意机制防止后续标记关注被跳过的标记位置。通过'sandwich'或'perilayernorm'方案控制残差范数，并用自适应正则化损失控制门控稀疏性。

**结果:** 该方法在所研究的规模下，与具有较少层数的密集基线相比，未能改善验证交叉熵和估计FLOPs之间的权衡。

**结论:** 尽管该方法没有显示出预期的改进，但代码已发布在https://github.com/tim-lawson/skip-middle。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Skip+the+Middle+Layers+of+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21103，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21103&send_immediately=true&force_search=false)

**原文摘要:** Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [55] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi, Jun Xia, Yufei Huang, Jingbo Zhou, Siyuan Li, Yunfan Liu, Chang Yu, Stan Z. Li*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于双重扩散隐式桥（DDIB）的框架，用于解决非配对单细胞扰动数据的问题，并结合基因调控网络（GRN）信息和掩码机制来增强模型生成质量。此外，还引入了新的评估指标以反映单细胞响应中的内在异质性。


<details>
  <summary>更多</summary>
  
**动机:** 单细胞测序是一种破坏性过程，无法在同一细胞上进行扰动前后的比较，导致收集到的数据本质上是非配对的。现有方法要么通过随机采样强行配对数据，要么忽略了未扰动和扰动细胞之间的内在关系。因此，需要一种新方法来有效处理这种非配对数据问题。

**方法:** 提出了基于双重扩散隐式桥（DDIB）的框架，学习不同数据分布之间的映射；将该框架解释为数据增强的一种形式；整合基因调控网络（GRN）信息以传播扰动信号；引入掩码机制预测沉默基因，提升生成数据的质量；针对单细胞表达的双峰分布特性，提出更合适的评估指标。

**结果:** 所提出的框架能够有效解决非配对单细胞扰动数据的问题，增强了模型对扰动的理解能力，同时提高了生成数据的质量，并且新引入的评估指标更好地反映了单细胞响应中的内在异质性。

**结论:** 该研究提出了一种新框架Unlasting，结合双重条件扩散模型、基因调控网络和掩码机制，解决了非配对单细胞扰动数据分析的挑战，提升了生成数据的质量并改进了评估方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlasting%3A+Unpaired+Single-Cell+Multi-Perturbation+Estimation+by+Dual+Conditional+Diffusion+Implicit+Bridges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21107&send_immediately=true&force_search=false)

**原文摘要:** Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [56] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng, Yadan Luo, Liangyu Huo, Yaowei Wang, Xin Li, Zheng Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的Norm-Aware Linear Attention机制，通过分解查询和键矩阵的范数和方向，实现范数一致性控制和动态熵减少，同时采用保范映射确保非负性约束。实验表明，NaLaFormer在视觉和语言任务上提升了性能和效率。


<details>
  <summary>更多</summary>
  
**动机:** 线性注意力机制虽然降低了计算复杂度，但存在范数忽略导致的熵差距以及内积交互丢失的问题。为了解决这些问题，需要一种新机制来恢复范数引导的动态尖锐性和修正核扰动的范数分布。

**方法:** 1. 将查询和键矩阵分解为范数和方向两个部分，分别实现范数感知的尖锐性控制和范数一致性。
2. 提出一种与查询范数相关的核函数，动态控制熵减少的程度。
3. 使用保范映射将角度矩阵的所有元素投影为正值，同时利用余弦相似性抑制反向维度。

**结果:** 提出的NaLaFormer在视觉和语言任务中表现优异，相较于现有方法，性能和效率提升高达4.2%。

**结论:** Norm-Aware Linear Attention机制有效地解决了线性注意力中的范数忽略和内积交互丢失问题，提升了模型的表达能力和计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NaLaFormer%3A+Norm-Aware+Linear+Attention+for+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21137&send_immediately=true&force_search=false)

**原文摘要:** Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [57] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于条件生成对抗网络（cGAN）的框架，用于制作能够规避入侵检测系统（IDS）机制的隐秘对抗攻击，并通过条件变分自编码器（CVAE）来检测这些攻击。


<details>
  <summary>更多</summary>
  
**动机:** 随着无人机在民用空域中的集成日益增加，需要更强大和智能的入侵检测系统（IDS）。传统方法难以识别新型威胁，且常将未知攻击视为分布外（OOD）样本，但这种方式对隐秘性攻击效果有限。

**方法:** 1. 设计了一个多类别的IDS分类器，训练数据包括良性无人机遥测数据及已知网络攻击。
2. 利用cGAN扰动已知攻击以生成被误分类为良性的对抗样本，同时保持与OOD分布的统计相似性。
3. 通过迭代优化提高隐秘性和成功率。
4. 使用CVAE结合负对数似然区分对抗输入与真实OOD样本。

**结果:** 实验结果表明，基于CVAE的遗憾分数显著优于传统的基于马氏距离的检测器，能有效识别隐秘对抗威胁。

**结论:** 研究强调了高级概率建模的重要性，以增强IDS应对适应性和基于生成模型的网络入侵的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Adversarial+Evasion+and+Out-of-Distribution+Detection+for+UAV+Cyber-Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21142&send_immediately=true&force_search=false)

**原文摘要:** The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [58] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang, Kuangpu Guo, Zhihe Lu, Yunbo Wang, Jian Liang*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，通过全局和局部提示以及交叉融合模块，使模型能够生成与每个客户端独特数据分布对齐的个性化表示，在九个数据集上的广泛实验表明，pFedDC在各种异构情况下始终优于最先进的方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦提示学习方法仅依赖于文本提示，并且忽略了联合标签域分布偏移的问题。

**方法:** 提出个性化的FL框架pFedDC，基于双提示学习和交叉融合。具体来说，每个客户端在视觉和语言模态中都维护全局和局部提示：全局提示捕获联邦共享的共同知识，而局部提示编码特定于客户端的语义和领域特征。同时设计了一个交叉融合模块，自适应地整合不同层次的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。

**结果:** 在具有各种异构性的九个数据集上进行的广泛实验表明，pFedDC始终优于最先进的方法。

**结论:** pFedDC框架通过结合全局和局部提示及交叉融合模块，有效应对了联邦学习中的数据、计算和通信异质性挑战，显著提高了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Federated+Learning+via+Dual-Prompt+Optimization+and+Cross+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21144，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21144&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [59] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson, Ola Engkvist, Jon Paul Janet, Christian Tyrchan, Morteza Haghir Chehreghani*

**主要类别:** cs.LG

**AI概要:** 在强化学习中，评估实例的优劣往往成本高昂且耗时。本文提出使用行列式点过程（DPP）选择多样化的mini-batch，以提高探索效率和解决方案的多样性。实验表明，该方法在新药设计中的分子生成任务上显著提高了解的多样性，同时保持了解的质量。


<details>
  <summary>更多</summary>
  
**动机:** 在许多实际应用中，如人类反馈或物理模拟，评估实例的好坏通常需要较高的成本和时间投入，尤其是在强化学习中，需要通过与环境的新交互来提供奖励信号进行学习。因此，充分的探索至关重要，而从多样化的mini-batch中学习可以显著影响效果并缓解模式崩溃问题。

**方法:** 本文引入了强化学习中的多样化mini-batch选择方法，并提出了使用行列式点过程（DPP）来完成这一任务。作者在药物发现这一实际问题中研究了该框架，通过实验探讨了所提出的框架如何改善从头药物设计中化学探索的有效性。

**结果:** 通过在三个已建立的分子生成模型上进行综合评估，实验结果表明，所提出的多样化mini-batch选择框架可以在保持解质量的同时，显著提高解的多样性。

**结论:** 本文提出的基于DPP的多样化mini-batch选择框架能够有效提升强化学习在药物发现任务中的探索效率，潜在地加速未满足医疗需求的药物研发进程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diverse+Mini-Batch+Selection+in+Reinforcement+Learning+for+Efficient+Chemical+Exploration+in+de+novo+Drug+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21158&send_immediately=true&force_search=false)

**原文摘要:** In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [60] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah, Axel Abels, Ann Nowé, Tom Lenaerts*

**主要类别:** cs.LG

**AI概要:** 在永久投票系统中，缺席投票者对公平性和代表性有显著影响。本文研究了将人工代表（Artificial Delegates）整合到永久投票系统中的效果。结果表明，人工代表能有效缓解缺席投票带来的问题，并提高系统的稳健性。


<details>
  <summary>更多</summary>
  
**动机:** 永久投票规则通常依赖于完全参与和完整的批准信息，但现实中部分投票是常态，这可能导致公平性和代表性的问题。因此，需要一种方法来解决缺席投票对系统的影响。

**方法:** 研究将人工代表（通过学习偏好训练的代理模型）引入永久投票系统，以代表缺席投票者进行决策。评估不同投票方法下缺席投票对公平性和代表性的影响，并分析人工代表在多大程度上能够弥补因缺席而导致的不公平。

**结果:** 研究发现，缺席投票显著影响公平性，而人工代表可以可靠地减轻这些影响，并在多种场景下增强系统的稳健性。

**结论:** 人工代表是一种有效的解决方案，可以在永久投票系统中缓解缺席投票带来的问题，从而提升整体的公平性和代表性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Delegates+Resolve+Fairness+Issues+in+Perpetual+Voting+with+Partial+Turnout，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21186&send_immediately=true&force_search=false)

**原文摘要:** Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [61] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种高效的微调方法，通过使用熵来识别复杂数据并仅对这些数据进行推理，从而减少所需数据量62%，同时达到与蒸馏法相当的性能，并优于标准的监督微调（SFT）方法。


<details>
  <summary>更多</summary>
  
**动机:** 通用大语言模型（LLMs）通常通过监督微调（SFT）来提升特定领域的性能，但此方法需要大量昂贵的调用和更多的数据。因此，研究更高效、成本更低的微调方法成为必要。

**方法:** 提出了一种新的微调蓝图，利用熵来识别复杂数据并对这些数据进行推理。具体步骤包括：将训练数据根据单标记答案熵分为不同复杂度类别；对大约30亿参数的小型开源模型进行微调；结合SFT和知识蒸馏方法进行实验验证。

**结果:** 所提出的管道在平均准确率上显著优于标准SFT方法（0.55 vs 0.43），并且在使用62%更少的数据的情况下，达到了与蒸馏法相当的性能（平均准确率均为0.55）。

**结论:** 新方法不仅提高了微调效率，还减少了所需的数据量，为未来的研究提供了有价值的参考。代码和数据已公开以促进进一步探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Complexity-aware+fine-tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21220&send_immediately=true&force_search=false)

**原文摘要:** General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [62] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad, Aya Mrabah, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的方法，利用零样本学习和大语言模型来预测组件过时的风险，并通过两个真实数据集验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 电子组件的过时给依赖它们的行业带来了重大挑战，包括成本增加以及系统安全性和可用性的中断。准确预测过时风险是必要的，但由于缺乏可靠的数据而受到阻碍。

**方法:** 本论文提出了一种使用零样本学习（ZSL）与大语言模型（LLMs）相结合的方法，通过利用表格数据集中的领域特定知识来解决数据限制问题，从而预测过时风险。

**结果:** 该方法在两个真实世界的数据集上应用，展示了有效的风险预测能力。对四种大语言模型的比较评估表明，为特定预测任务选择正确的模型非常重要。

**结论:** 零样本学习与大语言模型相结合的方法可以有效预测组件过时风险，且选择合适的语言模型对于任务的成功至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Learning+for+Obsolescence+Risk+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21240&send_immediately=true&force_search=false)

**原文摘要:** Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [63] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière, Frédéric Cazals*

**主要类别:** cs.LG

**AI概要:** This paper introduces improved seeding methods for k-means and k-GMM using a lookahead principle and multipass strategy, resulting in better performance with modest computational overhead.


<details>
  <summary>更多</summary>
  
**动机:** Existing seeding techniques for k-means and k-GMM have limitations, such as suboptimal performance relative to final evaluation metrics. The authors aim to improve upon these techniques through better conditioning of seed selection on the final algorithm metric and reducing the impact of randomization.

**方法:** The paper revisits randomized seeding techniques for k-means clustering and k-GMM by formalizing three key ingredients: seed sampling metric, number of candidate seeds, and seed selection metric. It proposes novel initialization methods that use a lookahead principle (conditioning seed selection on the final algorithm metric) and a multipass strategy to reduce randomization effects.

**结果:** Experiments demonstrate consistent improvements over classical methods in terms of final metrics (SSE for k-means, log-likelihood for k-GMM). The new methods outperform recent strategies like multi-swap, which was previously the best-performing method beyond greedy k-means++. Insights are also provided into subtle properties of k-means, such as variance reduction in iterative seeding and sensitivity of final SSE to pool size for greedy methods.

**结论:** The novel seeding methods for k-means clustering and k-GMM, which exploit a lookahead principle and a multipass strategy, outperform classical contenders with consistent improvement in final metrics (SSE for k-means and log-likelihood for k-GMM) at modest overhead. These methods are strong candidates to become standard techniques.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+seeding+strategies+for+k-means+and+k-GMM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21291&send_immediately=true&force_search=false)

**原文摘要:** We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [64] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

**主要类别:** cs.LG

**AI概要:** Mixture-of-Experts (MoE)架构是提升大规模语言模型效率的关键策略，但当前系统存在严重的负载不平衡问题。本文提出了一种新的路由框架Latent Prototype Routing (LPR)，通过聚类视角优化专家路由，有效促进了专家利用的平衡性，同时不损害下游性能。实验表明，LPR显著降低了专家负载的Gini系数，并提高了最小最大专家负载比率，接近完美的负载均衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的MoE系统在训练和推理过程中仅激活一小部分专家，导致模型容量和计算资源的严重未充分利用，因此需要一种方法来改善专家的负载均衡问题。

**方法:** 从聚类的角度重新审视专家路由，并提出了潜在原型路由（LPR）这一新框架。该框架推广了现有方法，能够在不损害下游性能的情况下促进专家的平衡使用。

**结果:** 通过多个开源MoE模型的广泛实验，证明LPR可以将专家负载的Gini系数从0.70降低到0.035，并将最小最大专家负载比率从1e-6提高到0.70，实现了近乎完美的负载均衡。

**结论:** Latent Prototype Routing (LPR)是一种有效的路由框架，能够解决MoE系统中的负载不平衡问题，提升资源利用效率，同时保持模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Prototype+Routing%3A+Achieving+Near-Perfect+Load+Balancing+in+Mixture-of-Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21328&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [65] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的图时间卷积网络（AGTCNet），用于改进脑机接口中的运动想象 EEG 分类。该模型显著优于现有方法，同时在模型大小和推理速度上更高效，实现了更高的分类准确率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管脑机接口技术具有巨大潜力，但开发跨个体和跨会话不变的 BCI 系统仍面临挑战，特别是捕捉多通道 EEG 信号中复杂的时空依赖关系方面存在不足。

**方法:** AGTCNet 利用 EEG 电极的地形配置作为归纳偏置，并结合图卷积注意力网络 (GCAT)，联合学习表达性强的时空 EEG 表示。

**结果:** AGTCNet 在多个数据集上表现出色：在 BCI Competition IV Dataset 2a 上实现 66.82% 的无监督分类准确率，微调后达到 82.88%；在 EEG Motor Movement/Imagery Dataset 上分别实现 64.14% 和 85.22% 的 4 类和 2 类无监督分类准确率，微调后提升至 72.13% 和 90.54%。此外，模型大小减少 49.87%，推理速度提高 64.65%。

**结论:** AGTCNet 是一种紧凑且高效的模型，能够显著提高运动想象 EEG 分类的性能，为实际 BCI 部署提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AGTCNet%3A+A+Graph-Temporal+Approach+for+Principled+Motor+Imagery+EEG+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21338，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21338&send_immediately=true&force_search=false)

**原文摘要:** Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [66] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia*

**主要类别:** cs.LG

**AI概要:** 提出DynamicBench评估大型语言模型处理实时信息的能力，并引入动态信息合成报告系统，实验结果表明在无文档和有文档辅助场景下分别超越GPT4o 7.0%和5.8%。


<details>
  <summary>更多</summary>
  
**动机:** 传统的大规模语言模型基准测试依赖于静态评估方法，无法满足现代应用中实时信息处理的动态需求。

**方法:** 设计了DynamicBench基准测试，采用双路径检索管道（结合网络搜索与本地报告数据库），需要领域特定知识，评估模型在提供或不提供外部文档情况下的能力，并引入先进的报告生成系统。

**结果:** 实验结果证明该方法的有效性，在无文档和有文档辅助场景下分别超过GPT4o 7.0%和5.8%。

**结论:** DynamicBench能够有效评估大规模语言模型处理实时信息的能力，代码和数据将公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynamicBench%3A+Evaluating+Real-Time+Report+Generation+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21343，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21343&send_immediately=true&force_search=false)

**原文摘要:** Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [67] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh, Mehmet Dik, Nguyen Viet Anh*

**主要类别:** cs.LG

**AI概要:** 持久拉普拉斯算子在数据形状和结构随尺度变化时进行追踪，其特征值是过滤过程中几何和拓扑特征的简洁描述符。本文通过证明一个均匀的Lipschitz界，填补了单个单纯形添加时特征值变化未知的空白，提供了谱拓扑数据分析的首个特征值级别的鲁棒性保证。


<details>
  <summary>更多</summary>
  
**动机:** 尽管之前的工作已经建立了这些算子的整体代数稳定性，但当添加一个单纯形（如顶点、边或三角形）时，单个特征值的确切变化仍然未知。由于下游工具直接依赖于这些特征值，因此了解这种变化至关重要。

**方法:** 作者证明了一个均匀的Lipschitz界：在插入一个单纯形后，每个上持续拉普拉斯特征值最多可以变化该单纯形边界欧几里得范数的两倍，且这一变化与过滤尺度和复形大小无关。

**结果:** 这一结果提供了谱拓扑数据分析的首个特征值级别的鲁棒性保证，确保了谱特征在局部更新下保持稳定，并在动态数据设置中实现了可靠的误差控制。

**结论:** 本文的研究成果为理解持久拉普拉斯特征值的变化提供了理论基础，同时确保了谱特征在动态数据环境中的稳定性，从而支持了相关下游工具的应用和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lipschitz+Bounds+for+Persistent+Laplacian+Eigenvalues+under+One-Simplex+Insertions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21352&send_immediately=true&force_search=false)

**原文摘要:** Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [68] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff, Maya Varma, Ossian Rabow, Subathra Adithan, Julie Kim, Ken Chang, Hannah Lee, Nidhi Rohatgi, Christian Bluethgen, Mohamed S. Muneer, Jean-Benoit Delbrouck, Michael Moor*

**主要类别:** cs.LG

**AI概要:** 多模态上下文学习（ICL）在医学等领域的潜力巨大，但尚未得到充分研究。本文介绍了SMMILE，首个针对医学任务的多模态ICL基准测试，包含111个问题和其增强版本SMMILE++。对15个多模态大语言模型的评估表明，这些模型在医学任务中的多模态ICL能力有限，且存在对无关示例的敏感性和最近偏差等问题。


<details>
  <summary>更多</summary>
  
**动机:** 医学领域中，临床医生经常需要从少量案例中适应多样化的专业任务，而多模态大语言模型在医学视觉问答方面的进展促使研究其在多模态上下文学习中的表现。

**方法:** 构建了SMMILE基准测试及其增强版本SMMILE++，并由医学专家策划了包含多模态查询和示例的问题。通过评估15个多模态大语言模型在这些问题上的表现，分析其多模态ICL能力及存在的问题。

**结果:** 大多数模型表现出中等到较差的多模态ICL能力，在开放性评估中，ICL仅带来8%和9.4%的平均改进。发现模型对无关示例敏感，并存在最近偏差。

**结论:** 当前多模态大语言模型在从上下文中学习多模态医学任务时存在关键限制和偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SMMILE%3A+An+Expert-Driven+Benchmark+for+Multimodal+Medical+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21355&send_immediately=true&force_search=false)

**原文摘要:** Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [69] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon, Georgios Makris, Sotirios Xydis, Kiamal Pekmestzi, Dimitrios Soudris*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在低功耗DNN计算中，利用硬件近似技术结合DNN任务的细粒度误差弹性来提高能效的方法。通过使用先进的ROUP近似乘法器，并根据层、滤波器和内核级别的方法进行系统探索，提出了一种能在ResNet-8模型上实现高达54%的能效提升（以不超过4%的精度损失为代价）的解决方案，同时相比最先进的DNN近似方法提供了2倍的能效且精度更高。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度神经网络(DNN)架构的迅速发展，它们已成为提供高精度机器学习任务的实际方法。然而，针对低功耗DNN计算的需求，需要探索如何利用DNN工作负载的细粒度误差弹性和硬件近似技术来进一步提高能源效率。

**方法:** 作者利用先进的ROUP近似乘法器，系统地研究了其在神经网络中的细粒度分布策略，包括层级别、滤波器级别和内核级别的方法，并评估了这些策略对精度和能耗的影响。实验基于ResNet-8模型和CIFAR-10数据集进行。

**结果:** 与基线量化模型相比，所提出的方案能够带来高达54%的能效提升，但会有最多4%的精度损失；同时，与当前最先进的DNN近似方法相比，该方案可以提供2倍的能效提升，并且保持更好的精度。

**结论:** 通过结合DNN任务的细粒度误差弹性和硬件近似技术，可以在一定程度上牺牲精度的情况下显著提高DNN计算的能效，是一种有效的低功耗DNN计算方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAx-DNN%3A+Multi-Level+Arithmetic+Approximation+for+Energy-Efficient+DNN+Hardware+Accelerators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21371&send_immediately=true&force_search=false)

**原文摘要:** Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [70] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken, Lennart Purucker, Frank Hutter*

**主要类别:** cs.LG

**AI概要:** 通过提前终止上下文学习过程，该研究提出了一种加速表格数据推理的方法，在小任务上可加速1.3倍，在大任务上可加速2.2倍，同时预测性能几乎没有下降。


<details>
  <summary>更多</summary>
  
**动机:** 尽管表格基础模型在无需下游微调的情况下展现了强大的泛化能力，但其推理成本仍然较高，特别是对于较大的数据集。因此，需要一种方法来降低这些模型的推理成本。

**方法:** 研究者提出在上下文学习过程中动态评估是否在每个Transformer编码器层之后停止学习，并在停止后使用预先训练好的逐层解码器对嵌入进行解码。这种方法被称为提前终止上下文学习。

**结果:** 实验表明，在34个小分类任务中，提前终止上下文学习可以将推理速度提升至1.3倍，且预测性能几乎无下降；在五个较大分类任务中，速度提升至2.2倍。

**结论:** 提前退出策略被证明是一种有效且实用的方法，可以提高表格上下文学习的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early+Stopping+Tabular+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21387&send_immediately=true&force_search=false)

**原文摘要:** Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [71] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris, Isaac Lyngaas, John Lagregren, Mohamed Wahib, Larry York, Prasanna Balaprakash, Dan Lu, Feiyi Wang, Xiao Wang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为D-CHAG的方法，用于处理多通道图像数据集，能够显著提高计算效率并减少内存使用。


<details>
  <summary>更多</summary>
  
**动机:** 视觉科学基础模型在推动科学发现和创新方面具有巨大潜力，但当前方法在处理大量图像数据时面临计算密集型的挑战，尚未得到充分解决。

**方法:** 引入了分布式跨通道分层聚合（D-CHAG）方法，该方法适用于具有大量通道的图像模态数据集，并且与任何模型并行策略和视觉Transformer架构兼容。

**结果:** 在超光谱成像和天气预报任务中评估了D-CHAG方法，结合张量并行性和模型分片后，在Frontier超级计算机上多达1,024个AMD GPU的情况下，内存使用减少了75%，持续吞吐量增加了一倍以上。

**结论:** D-CHAG方法为大规模多通道图像数据集提供了一种高效的解决方案，可以显著提升计算效率和降低资源消耗。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distributed+Cross-Channel+Hierarchical+Aggregation+for+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21411&send_immediately=true&force_search=false)

**原文摘要:** Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [72] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala, Cody Fleming*

**主要类别:** cs.LG

**AI概要:** SSCP是一种生成策略，结合了生成模型的表达能力和单模态策略的训练和推理效率，通过增强的流匹配目标实现了一步动作生成，在离线、离线到在线以及在线RL环境中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的生成模型如扩散模型和流匹配模型虽然能捕捉丰富的多模态动作分布，但迭代采样带来了高昂的推理成本和训练不稳定性。

**方法:** 提出了一种名为Single-Step Completion Policy (SSCP)的生成策略，使用增强的流匹配目标从中间流样本中直接预测完成向量，从而实现准确的一次性动作生成，并将其扩展到目标条件强化学习中，使扁平策略能够利用子目标结构而无需显式的分层推理。

**结果:** SSCP在标准离线强化学习和行为克隆基准测试中取得了强大的结果，展示了其在深度强化学习和顺序决策中的多功能性、表达能力和效率。

**结论:** SSCP提供了一种新的框架，有效提升了强化学习的速度和适应能力，特别是在不同类型的强化学习设置中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flow-Based+Single-Step+Completion+for+Efficient+and+Expressive+Policy+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21427&send_immediately=true&force_search=false)

**原文摘要:** Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [73] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon, Thomas Jack Samuels, Stephan Hau, Lennart Högman*

**主要类别:** cs.LG

**AI概要:** 研究发现结合语音和面部信息的多模态机器学习方法在检测欺骗行为方面表现更佳，尤其是在双人互动中使用后期融合策略时。


<details>
  <summary>更多</summary>
  
**动机:** 现有的欺骗检测研究通常关注单一模态数据或仅涉及单个参与者的数据，而本研究旨在探索双人互动中多模态数据（包括语音和面部信息）的整合如何提升欺骗检测的准确性。

**方法:** 研究采用了瑞典母语者在一个真实或谎言情境下的新收集数据集，分析了早期和后期融合方法在音频和视频数据（如动作单元和注视信息）上的效果，并比较了所有可能的模态和参与者组合。

**结果:** 结果表明，结合语音和面部信息比单一模态方法表现更好；同时包含两个参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是在后期融合策略应用于双模态和双参与者时实现的。

**结论:** 这是首个针对斯堪的纳维亚人群体进行的研究，其结果支持心理学理论中关于面部和声音表情在初期互动中的差异性控制观点，为未来在心理治疗等领域的双人互动研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deception+Detection+in+Dyadic+Exchanges+Using+Multimodal+Machine+Learning%3A+A+Study+on+a+Swedish+Cohort，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21429，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21429&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [74] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider, Simon Heilig, Asja Fischer, Timm Faulwasser*

**主要类别:** cs.LG

**AI概要:** 提出了一种针对ResNets的训练方法，该方法反映了一个适用于标准架构和通用损失函数的最优控制问题。通过惩罚隐藏状态的中间输出，对应于最优控制中的阶段成本项，将两者联系起来。对于标准的ResNets，通过后续的跳跃连接和输出层传播状态以获得中间输出。实验表明，这种训练动态会使不必要的更深残差层的权重趋于消失，这表明了一种基于理论支持的层剪枝策略的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习模型中，ResNets等深度网络的训练面临挑战，特别是在深层结构的有效性与效率方面。同时，最优控制理论提供了一种优化视角，可能对深度网络的训练有所启发。因此，研究者希望结合最优控制理论来改进ResNets的训练方法，并探索更高效的网络结构。

**方法:** 研究者提出了一种新的ResNets训练公式，该公式反映了一个最优控制问题。通过惩罚隐藏状态的中间输出（对应于最优控制中的阶段成本项），将最优控制理论与深度学习结合起来。对于标准的ResNets，通过后续的跳跃连接和输出层传播状态以获得中间输出。这种方法使得不必要的更深残差层的权重趋于消失。

**结果:** 实验结果表明，新提出的训练方法能够使不必要的更深残差层的权重趋于消失，从而验证了该方法的有效性。这一现象为一种基于理论支持的层剪枝策略提供了潜在的可能性。

**结论:** 本研究提出了一种基于最优控制理论的ResNets训练方法，通过惩罚隐藏状态的中间输出，成功地使不必要的深层结构权重趋于消失。这一发现为未来的层剪枝策略提供了理论基础，并可能进一步提升深度网络的训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+an+Optimal+Control+Perspective+of+ResNet+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21453&send_immediately=true&force_search=false)

**原文摘要:** We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [75] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud, Md Gulzar Hussain, Sumaiya Kabir, Hasnain Ahmad, Mahmudus Sobhan*

**主要类别:** cs.LG

**AI概要:** This paper presents an efficient electronic solution for evaluating subjective answer scripts, focusing on keyword identification, grammatical and spelling error checking. Tested with 100 students' scripts, it achieves a precision score of 0.91.


<details>
  <summary>更多</summary>
  
**动机:** To develop an efficient and automated method for evaluating subjective answer scripts in educational assessment, reducing human effort and increasing accuracy.

**方法:** The system identifies keywords from the answer scripts and compares them with keywords parsed from both open and closed domains. It also checks for grammatical and spelling errors in the answer scripts.

**结果:** The proposed system was tested with answer scripts from 100 students and achieved a precision score of 0.91.

**结论:** The integrated system provides an effective way to electronically evaluate written answer scripts, improving efficiency and accuracy in educational assessments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Keyword-Based+Technique+to+Evaluate+Broad+Question+Answer+Script，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21461&send_immediately=true&force_search=false)

**原文摘要:** Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [76] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad, Vedanti Kshirsagar, Urvi Ranjan, Mahek Nakhua*

**主要类别:** cs.LG

**AI概要:** 开发了基于人口统计因素的ML分类模型，预测乳腺癌和宫颈癌易感性，并设计系统提供最近的医院建议、整合健康卡功能、开展癌症防治宣传，使用决策树和支持向量分类算法分别用于宫颈癌和乳腺癌易感性预测。


<details>
  <summary>更多</summary>
  
**动机:** 2020年Telangana地区宫颈癌、乳腺癌和口腔癌筛查率极低，人们对癌症早期检测重要性及筛查实践的认知不足，导致发病率和死亡率较高。

**方法:** 通过机器学习分类模型（决策树分类和支持向量分类算法）预测个体患乳腺癌或宫颈癌的可能性，并根据用户位置提供最近的医院或癌症治疗中心建议，同时整合健康卡功能以维护个人医疗记录并进行癌症防治宣传活动。

**结果:** 成功开发出预测乳腺癌和宫颈癌易感性的ML分类模型，并设计了一套综合系统来提升癌症意识、降低癌症死亡率。

**结论:** 此解决方案有助于提高癌症认知水平，减少癌症死亡率，推动Telangana地区癌症防治工作的进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Devising+a+solution+to+the+problems+of+Cancer+awareness+in+Telangana，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21500，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21500&send_immediately=true&force_search=false)

**原文摘要:** According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [77] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li, Chenrui Fan, Tianyi Zhou*

**主要类别:** cs.LG

**AI概要:** 研究了7B参数的大语言模型(OLMoE)在单次预训练过程中的Grokking现象，发现大规模基础模型的预训练中仍然存在Grokking，并揭示了其内部机制。提出了两个新指标来量化通路距离和单一通路复杂度，能够预测下游任务的泛化改进。理论分析表明，更结构化的通路可以降低模型复杂度并提升泛化界。


<details>
  <summary>更多</summary>
  
**动机:** 之前关于Grokking的研究主要集中在小型模型和特定任务上，而本研究首次探讨了大规模语言模型预训练过程中是否存在Grokking现象，并试图揭示其背后的机制。

**方法:** 计算训练损失并评估多个基准任务上的泛化能力，包括数学推理、代码生成和常识/领域知识检索任务。通过研究训练样本路径（即各层专家选择）的变化，从随机实例特定到更具结构化和可共享，揭示Grokking过程中的泛化出现机制。提出两种新指标：通路距离和单一通路复杂度，用于量化这一过程。

**结果:** 验证了大规模模型预训练中仍存在Grokking现象，不同数据可能异步进入Grokking阶段。发现训练样本路径复杂度降低，表明从记忆到泛化的转换。提出的两个新指标能有效预测下游任务的泛化性能改进。理论上证明结构化路径降低模型复杂度并提高泛化界。

**结论:** 本研究首次在大规模语言模型预训练中验证了Grokking现象，揭示了记忆到泛化的转换机制，并提供了新的量化工具。这些工具无需微调即可监控泛化性能，具有实际应用价值。同时理论分析表明结构化路径有助于降低模型复杂度并改善泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+to+find+Grokking+in+LLM+Pretraining%3F+Monitor+Memorization-to-Generalization+without+Test，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21551&send_immediately=true&force_search=false)

**原文摘要:** Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio, Tegan Maharaj, Luke Ong, Stuart Russell, Dawn Song, Max Tegmark, Lan Xue, Ya-Qin Zhang, Stephen Casper, Wan Sie Lee, Sören Mindermann, Vanessa Wilfred, Vidhisha Balachandran, Fazl Barez, Michael Belinsky, Imane Bello, Malo Bourgon, Mark Brakel, Siméon Campos, Duncan Cass-Beggs, Jiahao Chen, Rumman Chowdhury, Kuan Chua Seah, Jeff Clune, Juntao Dai, Agnes Delaborde, Nouha Dziri, Francisco Eiras, Joshua Engels, Jinyu Fan, Adam Gleave, Noah Goodman, Fynn Heide, Dan Hendrycks, Cyrus Hodes, Bryan Low Kian Hsiang, Minlie Huang, Sami Jawhar, Wang Jingyu, Adam Tauman Kalai, Meindert Kamphuis, Mohan Kankanhalli, Subhash Kantamneni, Mathias Bonde Kirk, Thomas Kwa, Jeffrey Ladish, Kwok-Yan Lam, Wan Lee Sie, Taewhi Lee, Xiaojian Li, Jiajun Liu, Chaochao Lu, Yifan Mai, Richard Mallah, Julian Michael, Nick Moës, Simon Möller, Kihyuk Nam, Kwan Yee Ng, Mark Nitzberg, Besmira Nushi, Seán O hÉigeartaigh, Alejandro Ortega, Pierre Peigné, James Petrie, Benjamin Prud'Homme, Reihaneh Rabbany, Nayat Sanchez-Pi, Sarah Schwettmann, Buck Shlegeris, Saad Siddiqui, Aradhana Sinha, Martín Soto, Cheston Tan, Dong Ting, Robert Trager, Brian Tse, Anthony Tung K. H., Vanessa Wilfred, John Willes, Denise Wong, Wei Xu, Rongwu Xu, Yi Zeng, HongJiang Zhang, Djordje Žikelić*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了构建可信AI生态系统的重要性，并通过将AI安全研究领域分为三类（开发、评估和控制）来支持这一目标。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI能力的迅速提高，如何确保AI的安全性成为重要议题。为了建立人们的信心并避免抵制，构建一个可信的AI生态系统是必不可少的。

**方法:** 通过组织国际会议（2025新加坡AI安全会议），汇聚全球AI科学家共同确定和综合AI安全的研究优先事项。采用纵深防御模型，将AI安全研究领域分为三个类型：开发、评估和控制。

**结果:** 明确了AI安全研究的三个主要领域，为创建、评估和监控可信AI系统提供了指导方向。

**结论:** 构建可信的AI生态系统对于推动创新和获得公众信任至关重要，而分类研究领域有助于更有针对性地解决AI安全性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Singapore+Consensus+on+Global+AI+Safety+Research+Priorities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20702&send_immediately=true&force_search=false)

**原文摘要:** Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [79] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang*

**主要类别:** cs.AI

**AI概要:** 基于大语言模型（LLM）的代理日益增多，促使了跨代理合作在诸如调度、谈判和资源分配等任务中的广泛应用。然而，在这些系统中，隐私至关重要，因为代理通常需要访问专有工具和领域特定数据库，而这些都需要严格保密。本文研究了基于LLM的代理是否具备对情境隐私的理解，并探讨了在非对抗性的多轮对话中，当被指示时，这些系统是否能够保护用户隐私。我们提出了一个名为MAGPIE的新基准测试，包含15个领域的158个高风险现实场景。实验表明，包括GPT-4o和Claude-2.7-Sonnet在内的当前最先进的模型缺乏对情境隐私的稳健理解，在多轮对话中，即使在明确的隐私指令下，仍会泄露私人信息，且无法完成任务的比例高达71%。这表明现有模型未能同时实现情境隐私保护和协作任务解决。


<details>
  <summary>更多</summary>
  
**动机:** 随着基于LLM的代理数量增加，跨代理合作变得普遍，但隐私问题随之而来，特别是在涉及专有工具和敏感数据的情况下。因此，有必要评估这些代理是否能理解和遵守情境隐私，以确保用户数据的安全。

**方法:** 提出了一种新的基准测试MAGPIE，包含15个领域的158个高风险现实场景。通过该基准测试评估当前最先进的LLM模型（如GPT-4o和Claude-2.7-Sonnet）：(a) 对情境隐私数据的理解；(b) 在不违反用户隐私的情况下进行协作的能力。

**结果:** 实验证明，当前模型对情境隐私的理解不足，分别在25.2%和43.6%的情况下错误地将私人数据识别为可共享数据。在多轮对话中，即使在明确的隐私指令下，模型仍分别在59.9%和50.5%的情况下泄露私人信息。此外，多代理系统在71%的场景中无法完成任务。

**结论:** 当前最先进的LLM模型尚未达到既保护情境隐私又有效进行协作任务的要求。这强调了在未来开发更注重隐私保护和任务协作能力的模型的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAGPIE%3A+A+dataset+for+Multi-AGent+contextual+PrIvacy+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20737&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [80] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang, Haijun Zhai, Chaitanya Belwal, Vineeth Thayanithi, Philip Baumann, Yogesh K Roy*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的动态上下文感知的提示词推荐系统，通过结合上下文查询分析、检索增强的知识定位、分层技能组织和自适应技能排名来生成相关且可行的提示词建议。实验表明该方法在实际数据集上具有高实用性和相关性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）驱动的应用对用户提示词的质量高度敏感，而构建高质量的提示词尤其是针对特定领域的应用时颇具挑战性。

**方法:** 该系统利用行为遥测技术和两阶段分层推理过程，动态选择和排名相关技能，并使用增强了几例学习的预定义和自适应模板合成提示词。方法包括上下文查询分析、检索增强的知识定位、分层技能组织和自适应技能排名。

**结果:** 实验证明，该方法在实际数据集上表现出高实用性和相关性，得到了自动化和专家评估的验证。

**结论:** 提出的动态上下文感知提示词推荐系统能够有效提升特定领域AI应用中提示词的质量和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Context-Aware+Prompt+Recommendation+for+Domain-Specific+AI+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20815，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20815&send_immediately=true&force_search=false)

**原文摘要:** LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [81] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun, Denghui Zhang, ChengXiang Zhai, Heng Ji*

**主要类别:** cs.AI

**AI概要:** 提出了一种概念验证框架，评估语言模型生成的建议在社会系统中的长期影响，并引入了100个间接伤害场景的数据集，测试模型对非明显不良结果的预测能力。该方法在新数据集上提高了20%以上，在现有安全基准上平均胜率超过70%。


<details>
  <summary>更多</summary>
  
**动机:** 随着基于语言模型的代理在高风险社会决策（如公共政策和医疗）中影响力的增长，确保其积极影响需要理解其建议的深远影响。

**方法:** 提出了一个概念验证框架，用于评估模型生成的建议如何在宏观尺度上随时间传播于社会系统。同时引入了一个包含100个间接伤害场景的数据集，以测试模型预测看似无害用户提示可能引发的不良后果的能力。

**结果:** 该方法在新的间接伤害场景数据集上实现了超过20%的改进，在现有安全基准（AdvBench, SafeRLHF, WildGuardMix）上平均胜率超过70%。

**结论:** 该研究为开发更安全的语言模型代理提供了一个有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Reactive+Safety%3A+Risk-Aware+LLM+Alignment+via+Long-Horizon+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20949&send_immediately=true&force_search=false)

**原文摘要:** Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [82] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了大语言模型（LLMs）在因果推理方面的能力，指出当前的LLMs仅能进行浅层（level-1）因果推理，而无法实现类似人类的深层（level-2）因果推理。通过分析Transformer架构的自回归机制和引入新的因果问答基准CausalProbe-2024，作者证明LLMs主要依赖于参数中嵌入的因果知识。为提升至level-2推理，作者提出G^2-Reasoner方法，结合通用知识和目标导向提示，显著增强了LLMs在新颖和反事实情境中的因果推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在理解和遵循因果关系方面表现出一定的能力，但其是否能够进行类似于人类的真实因果推理仍不明确。当前证据表明，LLMs仅限于浅层因果推理（level-1），缺乏深层因果推理（level-2）的能力。这促使研究者深入探究LLMs的因果推理机制，并探索提升其因果推理能力的方法。

**方法:** 作者首先深入分析了基于Transformer的LLMs的自回归机制，揭示其非因果本质。接着，构建了一个名为CausalProbe-2024的新因果问答基准，用于评估LLMs在新鲜和未见过的数据上的表现。最后，提出了G^2-Reasoner方法，该方法将通用知识和目标导向提示融入到LLMs的因果推理过程中。

**结果:** 实验结果表明，与先前的基准相比，LLMs在CausalProbe-2024上的表现显著下降，验证了其主要进行level-1因果推理的事实。而使用G^2-Reasoner方法后，LLMs在新鲜和反事实情境中的因果推理能力得到了显著提升。

**结论:** 本研究揭示了当前LLMs在因果推理方面的局限性，并提出了一种有效的方法（G^2-Reasoner）来增强其因果推理能力，为未来LLMs向真实因果推理迈进提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unveiling+Causal+Reasoning+in+Large+Language+Models%3A+Reality+or+Mirage%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21215&send_immediately=true&force_search=false)

**原文摘要:** Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [83] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng QIu*

**主要类别:** cs.AI

**AI概要:** 大型视觉-语言模型(LVLMs)在具身规划任务中展现出潜力，但在涉及不熟悉环境和多步骤目标的复杂场景中表现不佳。当前方法依赖于与环境无关的模仿学习，将指令与环境上下文分离，导致模型难以处理与上下文相关的指令，并在长时间交互中依赖补充线索而非视觉推理。为解决此问题，本文提出了一种名为World-Aware Planning Narrative Enhancement (WAP) 的框架，通过四个认知能力（视觉外观建模、空间推理、功能抽象和句法接地）增强LVLMs对环境的全面理解，并仅使用原始视觉观察数据通过课程学习开发和评估模型。在EB-ALFRED基准上的评估显示了显著改进，Qwen2.5-VL在任务成功率上绝对提升了60.7%，特别是在常识推理(+60.0)和长时规划(+70.0)方面。值得注意的是，我们增强的开源模型大幅超越了如GPT-4o和Claude-3.5-Sonnet等专有系统。


<details>
  <summary>更多</summary>
  
**动机:** LVLMs在具身规划任务中的应用受限于其在复杂场景中表现不佳的问题，特别是面对不熟悉的环境和多步骤目标时。此外，当前的方法依赖于与环境无关的模仿学习，这使得模型难以处理与上下文相关的指令，并在长时间交互中依赖补充线索而非视觉推理。因此，需要一种新的框架来增强LVLMs对环境的理解能力。

**方法:** 提出了World-Aware Planning Narrative Enhancement (WAP) 框架，该框架通过四个认知能力（视觉外观建模、空间推理、功能抽象和句法接地）增强LVLMs对环境的全面理解。同时，模型的开发和评估仅使用原始视觉观察数据，并通过课程学习进行优化。

**结果:** 在EB-ALFRED基准上的评估显示了显著改进，Qwen2.5-VL在任务成功率上绝对提升了60.7%，特别是在常识推理(+60.0)和长时规划(+70.0)方面。

**结论:** WAP框架能够有效提升LVLMs在具身规划任务中的表现，特别是在复杂场景中。增强的开源模型大幅超越了现有的专有系统，展示了其在实际应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是World-aware+Planning+Narratives+Enhance+Large+Vision-Language+Model+Planner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21230&send_immediately=true&force_search=false)

**原文摘要:** Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [84] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann, Mario Nadj, Christian Janiesch*

**主要类别:** cs.AI

**AI概要:** 本研究开发了一个名为IXAII的交互式可解释智能系统，结合了四种可解释AI方法，并为五类用户群体提供了定制化视图。通过专家和普通用户的访谈评估，结果表明IXAII有助于提高透明度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的可解释AI方法大多是静态的，忽视了用户视角，限制了其对目标受众的有效性。

**方法:** 开发了一个名为IXAII的交互式可解释智能系统，结合了LIME、SHAP、Anchors和DiCE四种可解释AI方法，并为五类用户群体提供定制化视图。用户可以控制解释的内容和格式。通过与专家和普通用户的访谈进行评估。

**结果:** 评估结果表明，IXAII提供的多种解释和可视化选项被用户认为有助于提高透明度。

**结论:** 通过弥合可解释AI方法、交互性和实际应用之间的差距，本研究提供了一种关于AI解释实践和人机交互的新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IXAII%3A+An+Interactive+Explainable+Artificial+Intelligence+Interface+for+Decision+Support+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21310&send_immediately=true&force_search=false)

**原文摘要:** Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [85] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

**主要类别:** cs.AI

**AI概要:** 这篇论文强调了当前人工智能系统在科学发现中的局限性，并提出了通过弥合抽象差距、推理差距和现实差距来推动AI驱动的科学发展。它定义了一种新的主动推理AI系统架构，该架构结合了因果自监督基础模型、贝叶斯约束的符号或神经符号规划器、持久知识图以及与高保真模拟器和自动化实验室的闭环交互。尽管模拟和实验反馈存在固有的模糊性和不确定性，但论文认为人类判断是不可或缺的永久架构组件。


<details>
  <summary>更多</summary>
  
**动机:** 作者指出当前的人工智能系统在科学发现方面受到操作架构、脆弱的推理机制和脱离实验现实的限制。为了突破这些限制，需要解决三个基本差距：抽象差距、推理差距和现实差距。这成为开发更先进的AI驱动科学发现系统的动机。

**方法:** 论文提出了一种新型的主动推理AI系统架构，包括：(i)基于因果自监督基础模型的研究记忆；(ii)带有贝叶斯约束的符号或神经符号规划器；(iii)持续增长的知识图，用于生成概念节点、建立因果关系和验证连接；(iv)与高保真模拟器和自动化实验室的闭环互动，以改进内部表示。

**结果:** 这种新架构能够使AI系统进行反事实推理，并通过外部验证将假设扎根于现实。此外，尽管模拟和实验反馈存在模糊性和不确定性，但人类判断被确定为不可或缺的永久架构组件。

**结论:** 论文得出结论，未来的AI驱动科学发现需要依赖一种新型架构，其中内部模型支持反事实推理，而外部验证确保假设基于现实。同时，人类判断应当被视为一个永久的架构组成部分，而非临时工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Inference+AI+Systems+for+Scientific+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21329&send_immediately=true&force_search=false)

**原文摘要:** The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [86] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang, Pu Chen, Yin Zhang*

**主要类别:** cs.AI

**AI概要:** 提出TableMoE，一种用于多模态表格数据的强大结构推理的神经符号混合专家架构，通过创新的神经符号路由机制和大规模预训练数据集，显著超越现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 多模态表格理解在真实场景中面临结构复杂、符号密集和视觉退化（如模糊、倾斜、水印、不完整结构或字体、多跨度或分层嵌套布局）等挑战，现有的多模态大语言模型在这种条件下性能受限且泛化能力差。

**方法:** 提出了一种名为TableMoE的神经符号混合专家架构，其特点包括：1) 创新的神经符号路由机制，用于预测潜在语义标记角色并动态路由表元素到专门的专家模块；2) 使用由符号推理图指导的置信度感知门控策略；3) 引入了大规模的TableMoE-Align数据集进行对齐驱动的预训练；4) 构建四个具有挑战性的WildStruct基准测试数据集以评估模型性能。

**结果:** 实验结果表明，TableMoE显著超越现有最先进的模型。广泛的消融研究验证了每个核心组件的有效性，强调了神经符号路由和结构化专家对齐的关键作用。定性分析进一步展示了TableMoE的可解释性和增强的鲁棒性。

**结论:** TableMoE通过整合神经符号推理有效提升了多模态表格理解的能力，在真实场景中的表现更加稳健且具备更好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TableMoE%3A+Neuro-Symbolic+Routing+for+Structured+Expert+Reasoning+in+Multimodal+Table+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21393，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21393&send_immediately=true&force_search=false)

**原文摘要:** Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [87] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, Li Fei-Fei*

**主要类别:** cs.AI

**AI概要:** 通过构建认知地图并进行推理的方法，显著提高了视觉语言模型对不可见空间的理解能力，准确率从37.8%提升至70.7%。


<details>
  <summary>更多</summary>
  
**动机:** 评估现有视觉语言模型是否能够像人类一样从有限视角想象完整场景，并发现其在此任务上的性能接近随机。

**方法:** 创建MindCube基准测试，包含21,154个问题和3,268张图像，系统性地评估VLMs在表示位置、方向和动态方面的能力，并探索三种方法来帮助VLMs近似空间心理模型，最终采用协同的“map-then-reason”方法。

**结果:** 通过“map-then-reason”方法，准确率从37.8%提升到60.8%，再结合强化学习进一步提高到70.7%。

**结论:** 构建和利用内部结构化空间表示并结合灵活的推理过程，可以显著改善对不可见空间的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatial+Mental+Modeling+from+Limited+Views，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21458&send_immediately=true&force_search=false)

**原文摘要:** Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [88] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević, Ravi Hammond, Tobias Gessler, Anisoara Calinescu, Jonathan Cook, Matteo Gallici, Andrei Lupu, Jakob Nicolaus Foerster*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的挑战和方法，用于克服人类与AI协作中的评价难题。通过开发大规模数据集上的代理模型，这些模型可以模仿人类行为，从而提供一种低成本、可重复的评估方式。


<details>
  <summary>更多</summary>
  
**动机:** 实现AI与人类之间的无缝协作对于实际应用至关重要，但目前仍面临巨大挑战。Hanabi游戏因其特性（如不完美信息、受限沟通等）成为测试人类-AI协作的理想平台，然而其应用受限于人类评估的困难。

**方法:** 作者提出了Ad-Hoc Human-AI Coordination Challenge (AH2AC2)，并开发了基于大规模人类数据集的'human proxy agents'，以作为稳健、廉价且可重现的人类评估伙伴。同时，为了促进数据高效方法的发展，开源了一个包含3,079局游戏的数据集，有意限制了可用的人类游戏数据量。

**结果:** 提供了两个玩家和三个玩家Hanabi场景的基线结果，并通过受控评估系统托管代理模型以确保公平评估。

**结论:** 该研究为人类-AI协作的研究提供了一种新方法和资源，通过使用代理模型来减少对真实人类评估的依赖，推动更高效的算法开发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ad-Hoc+Human-AI+Coordination+Challenge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21490，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21490&send_immediately=true&force_search=false)

**原文摘要:** Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [89] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为Mind2Web 2的新基准，包含130个需要实时网络浏览和广泛信息整合的长周期任务。为应对时间变化和复杂答案的评估挑战，提出了Agent-as-a-Judge框架，基于树形结构评分设计构建特定任务的评判代理，自动评估答案正确性和来源归属。对九个前沿代理搜索系统和人类表现进行了全面评估，并通过详细错误分析为未来发展提供见解。表现最佳的OpenAI Deep Research系统已能达到人类表现的50-70%，同时用时减半。


<details>
  <summary>更多</summary>
  
**动机:** 当前代理搜索系统的复杂性和开放性已经超越了现有的评估基准和方法，特别是对于长时间周期和动态答案的评估能力不足，因此需要新的基准和评估方法来推动这一领域的发展。

**方法:** 构建了一个名为Mind2Web 2的基准，包含130个长周期、高质任务；提出了一种新型的Agent-as-a-Judge框架，利用树形结构评分设计创建特定任务的评判代理，以自动化方式评估答案正确性和信息来源归属。

**结果:** 通过对九个前沿代理搜索系统和人类表现的综合评估发现，表现最佳的OpenAI Deep Research系统在效率上优于人类（用时减半），并且达到了50-70%的人类性能水平。

**结论:** Mind2Web 2为开发和评估下一代代理搜索系统提供了严格的基准，有助于推动该领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind2Web+2%3A+Evaluating+Agentic+Search+with+Agent-as-a-Judge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21506&send_immediately=true&force_search=false)

**原文摘要:** Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [90] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PsyLite+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21536&send_immediately=true&force_search=false)

**原文摘要:** With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [91] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič, Martin Vuk, Aljaž Zalar*

**主要类别:** stat.ML

**AI概要:** 本论文通过解析方法解决了与多元准连接函数相关的质量分布极值问题，完整回答了之前未解决的问题，并纠正了一个最近的猜想。


<details>
  <summary>更多</summary>
  
**动机:** 尽管准连接函数在依赖性建模社区中的关注度有所提高，但其缺乏统计解释。作者之前的线性规划方法已部分解决问题，但需要进一步提供完整解答。

**方法:** 采用解析方法对准连接函数的质量分布极值问题进行研究。

**结果:** 完全解决了“Hitchhiker's Guide”中提出的原始问题，并推翻了有关该问题的一个近期猜想。

**结论:** 解析方法能够为多元准连接函数相关问题提供完整解决方案，并澄清了先前的误解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+final+solution+of+the+Hitchhiker%27s+problem+%235，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20672&send_immediately=true&force_search=false)

**原文摘要:** A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [92] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang, Dan Qiao, Yu-Xiang Wang, Rahul Parhi*

**主要类别:** stat.ML

**AI概要:** 本文研究了在两层过度参数化的ReLU网络中，平坦性/低（损失）曲率的隐式偏差及其对泛化的影响。作者针对多变量输入提出了新的理论结果，并证明了平坦性虽然能带来泛化能力，但其收敛速率会随着输入维度的增长呈指数级恶化。这表明平坦解与低范数解之间存在显著差异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的工作要么需要插值，要么仅关注单变量输入，而本文旨在探索多变量输入下的隐式偏差问题，特别是平坦性对泛化的影响。

**方法:** 通过分析两种自然设置：(1) 平坦解的泛化差距；(2) 稳定最小值在非参数函数估计中的均方误差，作者利用极小极大下界构造和边界局部化ReLU神经元的新颖填充论证来揭示平坦解在高维情况下的性能问题。

**结果:** 证明了平坦性确实可以带来泛化能力，但其收敛速率会随着输入维度的增长呈指数级恶化。此外，平坦解在高维情况下由于'神经破碎'现象导致性能较差。这些结论得到了广泛的数值模拟的支持。

**结论:** 平坦最小值在高维情况下可能无法泛化，这是由于其收敛速率随维度增长呈指数级恶化，以及神经元激活稀疏且权重较大的特性所导致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Minima+of+ReLU+Neural+Networks+Suffer+from+the+Curse+of+Dimensionality%3A+The+Neural+Shattering+Phenomenon，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20779&send_immediately=true&force_search=false)

**原文摘要:** We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [93] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng, Lulu Kang, Yiwei Wang, Chun Liu*

**主要类别:** stat.ML

**AI概要:** 本文介绍了一种用于流形高斯过程回归的主动学习框架，结合流形学习与战略性数据选择，以提高高维空间中的回归精度。


<details>
  <summary>更多</summary>
  
**动机:** 在高维空间中进行准确回归是一个具有挑战性的问题，而现有的方法可能无法很好地处理复杂、不连续的函数。通过结合流形学习和主动学习，可以更有效地探索数据的内在结构并提高回归性能。

**方法:** 该方法联合优化了一个用于降维的神经网络和潜在空间中的高斯过程回归器，由一个主动学习标准监督，该标准最小化全局预测误差。

**结果:** 在合成数据上的实验表明，该方法比随机顺序学习表现更好，能够高效处理复杂的不连续函数，同时保持计算的可行性。

**结论:** 主动学习框架结合流形学习与策略性数据选择，能有效提升高维空间回归准确性，并为科学与工程应用提供了实用价值。未来工作将关注可扩展性和不确定性感知的流形学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning+for+Manifold+Gaussian+Process+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20928&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [94] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn, Frederick Eberhardt, Leonard J. Schulman*

**主要类别:** stat.ML

**AI概要:** 在放松关于有向无环图（DAG）的常见假设后，马尔可夫等价类的大小可能呈指数级增长，这表明从观察数据中学习因果关系更具挑战性。


<details>
  <summary>更多</summary>
  
**动机:** 探讨当放松关于有向无环图（DAG）的常见假设时，马尔可夫等价类的大小会发生怎样的变化，以评估对因果图的学习限制。

**方法:** 通过证明在三种不同设置下的马尔可夫等价类期望大小的下界：稀疏随机有向无环图、均匀随机有向无环混合图和均匀随机有向循环图。

**结果:** 发现当放松任一假设时，马尔可夫等价类的期望大小会呈指数级增长。

**结论:** 表明在更广泛的图模型中，仅依靠观测数据来推断因果结构面临更大的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lower+Bounds+on+the+Size+of+Markov+Equivalence+Classes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20933&send_immediately=true&force_search=false)

**原文摘要:** Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [95] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang, Hayden Hampton*

**主要类别:** stat.ML

**AI概要:** 本研究通过引入STFT-VNNGP混合架构解决了GDELT等数据源的固有稀疏性、突发性和过度离散化问题，从而提高了地缘政治冲突预测的可靠性。该模型结合了TFT和VNNGP的优势，在2023年的ATD竞赛中表现出色，并在中东和美国的案例研究中展示了优于单独TFT的能力。


<details>
  <summary>更多</summary>
  
**动机:** 地缘政治冲突预测对于国家安全至关重要，但现有深度学习模型（如TFT）在处理GDELT等数据时，由于数据的稀疏性、突发性和过度离散化，难以生成可靠的长期预测。

**方法:** 提出了STFT-VNNGP混合架构，采用两阶段过程：第一阶段使用TFT捕捉复杂的时间动态并生成多分位数预测；第二阶段利用VNNGP对这些分位数进行空间时间平滑和不确定性量化。

**结果:** 在中东和美国的地缘政治冲突预测案例研究中，STFT-VNNGP显著优于单独的TFT模型，特别是在预测突发事件的时间和规模方面表现更为出色。

**结论:** STFT-VNNGP提供了一个强大的框架，可以从具有挑战性的事件数据中生成更可靠和可操作的情报。所有代码和工作流程均已公开，以确保结果的可重复性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forecasting+Geopolitical+Events+with+a+Sparse+Temporal+Fusion+Transformer+and+Gaussian+Process+Hybrid%3A+A+Case+Study+in+Middle+Eastern+and+U.S.+Conflict+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20935&send_immediately=true&force_search=false)

**原文摘要:** Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [96] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种新的方法，用于近似多个强化学习（RL）代理在有限状态马尔可夫博弈中交互的学习动态。通过同时降低学习率和增加更新频率来重新调整学习过程，将代理的参数视为受快速混合游戏状态影响的缓慢演变变量。在温和的假设下（状态过程的遍历性和更新的连续性），证明了这种重新缩放的过程收敛于一个常微分方程（ODE）。该ODE为代理的学习动态提供了一个可行的、确定性的近似。框架的实现可在https://github.com/yannKerzreho/MarkovGameApproximation找到。


<details>
  <summary>更多</summary>
  
**动机:** 在多代理强化学习环境中，理解代理之间交互的学习动态是一个具有挑战性的问题。现有的方法可能无法捕捉到复杂交互中的细微差别或难以分析大规模系统的行为。因此，需要一种新的方法来简化和近似这些学习动态，以便更好地理解和优化多代理系统的性能。

**方法:** 作者提出了一种重新调整学习过程的方法，通过同时降低学习率和增加更新频率，使得代理的参数可以被视为受快速混合游戏状态影响的缓慢演变变量。基于这一重新缩放的过程，在状态过程的遍历性和更新的连续性等温和假设下，证明了其收敛于一个常微分方程（ODE）。

**结果:** 该方法成功地将多代理强化学习的学习动态近似为一个常微分方程（ODE），从而提供了一个可行的、确定性的近似模型。这使得分析和预测多代理系统的行为变得更加简单和直观。

**结论:** 本文提出了一种新的方法，能够有效地近似多个强化学习代理在有限状态马尔可夫博弈中的学习动态。通过重新缩放学习过程并证明其收敛于一个ODE，为研究多代理强化学习系统提供了一个强有力的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Homogenization+of+Multi-agent+Learning+Dynamics+in+Finite-state+Markov+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21079&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [97] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica, Kurt Hornik*

**主要类别:** stat.ML

**AI概要:** 本研究提出了一种新的变分自编码器（VAE）架构，采用球形柯西（spCauchy）潜在分布。与传统的高斯潜在空间或广泛使用的冯·米塞斯-费舍尔（vMF）分布不同，spCauchy 提供了更自然的超球面表示形式，能够更好地捕捉方向数据，同时保持灵活性。其重尾特性可防止过度正则化，确保潜在空间的有效利用，并提供更具表现力的表示形式。此外，spCauchy 避免了 vMF 中固有的数值不稳定性，并通过莫比乌斯变换实现了完全可微且高效的重参数化技巧，从而实现稳定且可扩展的训练。KL 散度可以通过快速收敛的幂级数进行计算，消除了与超几何函数比率评估相关的下溢或上溢问题。这些特性使 spCauchy 成为 VAE 的一个有吸引力的替代方案，在高维生成建模中提供了理论优势和实际效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前大多数变分自编码器（VAE）使用高斯潜在空间或冯·米塞斯-费舍尔（vMF）分布来建模潜在变量。然而，这些方法在处理方向性数据时存在不足，可能引发数值不稳定性和过度正则化问题。因此，需要一种新的潜在分布来解决这些问题，从而提高 VAE 的表达能力和训练效率。

**方法:** 研究人员提出了一种新的 VAE 架构，采用球形柯西（spCauchy）潜在分布。该方法通过莫比乌斯变换实现完全可微且高效的重参数化技巧，以确保稳定且可扩展的训练过程。同时，通过快速收敛的幂级数计算 KL 散度，解决了传统方法中涉及的数值不稳定性问题。

**结果:** 实验表明，球形柯西潜在分布可以有效避免过度正则化问题，提高潜在空间的利用率，同时提供更具表现力的数据表示形式。此外，该方法在高维生成建模任务中表现出更高的理论优势和实际效率，且训练过程更加稳定。

**结论:** 球形柯西（spCauchy）潜在分布为变分自编码器提供了一种有吸引力的替代方案，其在高维生成建模任务中具有理论优势和实际效率。它不仅解决了数值不稳定性问题，还提高了潜在空间的利用率和数据表示能力，使得 VAE 的训练过程更加稳定和高效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperspherical+Variational+Autoencoders+Using+Efficient+Spherical+Cauchy+Distribution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21278&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [98] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

**主要类别:** stat.ML

**AI概要:** 本论文提出了一种名为wild refitting的高效重新拟合程序，用于计算基于最小二乘法最小化的惩罚非参数估计的实例均方预测误差的高概率上界。该方法通过三个步骤完成：计算合适的残差、对残差进行对称化和缩放、以及定义并解决一个以当前估计为中心的修改后的预测问题。在相对温和的条件下，作者证明了wild refitting可以提供预测误差的上界，并提供了关于如何设计这种程序的理论指导。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算实例均方预测误差的方法可能需要大量数据或者复杂的假设，而本文旨在开发一种只需要单一数据集且对预测方法具有黑箱访问权的高效方法，来计算惩罚非参数估计的高概率上界。

**方法:** 提出wild refitting方法，包含三个主要步骤：1) 计算适当的残差；2) 使用Rademacher residual symmetrization技术对残差进行对称化和缩放；3) 用调整后的残差定义并解决一个新的预测问题。此方法不需要额外的数据集或复杂假设。

**结果:** 在相对温和的条件下，wild refitting能够提供预测误差的高概率上界。实验结果表明该方法适用于多种问题，如非刚性结构从运动恢复、使用深度神经网络先验的图像修复以及核方法的随机素描等。

**结论:** Wild refitting是一种有效的重新拟合程序，它能够在较弱的假设下为预测误差提供高概率上界。此方法不仅提高了计算效率，还为类似程序的设计提供了理论指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wild+refitting+for+black+box+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21460，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21460&send_immediately=true&force_search=false)

**原文摘要:** We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [99] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias, Angelos Alexopoulos, Siran Liu, Petros Dellaportas*

**主要类别:** stat.ML

**AI概要:** 本论文提出了一种高斯不变采样方法，能够提高统计效率并减少估计量的方差。通过高斯不变性，可以得到精确解析解以构建控制变量，并在高维潜在高斯模型中展示了其优越性。此外，还提供了几何遍历性和最优缩放分析的理论结果。


<details>
  <summary>更多</summary>
  
**动机:** 传统的随机游走Metropolis (RWM) 和 Metropolis调整的Langevin算法 (MALA) 在处理高斯目标分布时表现不佳，缺乏有效的方差缩减技术。因此，需要一种改进的方法来提高估计器的统计效率和准确性。

**方法:** 开发了高斯不变版本的RWM、MALA和二阶Hessian或流形MALA采样方法。利用高斯不变性，得到了Poisson方程的精确解析解，从而构造出有效的控制变量用于减少估计量的方差。这些方法适用于任何难以处理的目标分布。

**结果:** 在多个例子中验证了新采样器和估计器的有效性，特别是在高维潜在高斯模型中，与现有先进方法相比，取得了最前沿的结果。同时，进行了几何遍历性和最优缩放分析，揭示了目标分布高斯性对最优接受率的影响。

**结论:** 高斯不变采样方法为解决高斯目标分布问题提供了一个高效且易于使用的工具，显著提高了估计器的统计效率和精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gaussian+Invariant+Markov+Chain+Monte+Carlo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21511&send_immediately=true&force_search=false)

**原文摘要:** We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>
