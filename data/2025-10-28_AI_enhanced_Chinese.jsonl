{"id": "2510.20852", "pdf": "https://arxiv.org/pdf/2510.20852", "abs": "https://arxiv.org/abs/2510.20852", "authors": ["Safa Ben Atitallah", "Maha Driss", "Henda Ben Ghezela"], "title": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "categories": ["cs.CR"], "comment": null, "summary": "The Internet of Things (IoT) has recently proliferated in both size and\ncomplexity. Using multi-source and heterogeneous IoT data aids in providing\nefficient data analytics for a variety of prevalent and crucial applications.\nTo address the privacy and security concerns raised by analyzing IoT data\nlocally or in the cloud, distributed data analytics techniques were proposed to\ncollect and analyze data in edge or fog devices. In this context, federated\nlearning has been recommended as an ideal distributed machine/deep\nlearning-based technique for edge/fog computing environments. Additionally, the\ndata analytics results are time-sensitive; they should be generated with\nminimal latency and high reliability. As a result, reusing efficient\narchitectures validated through a high number of challenging test cases would\nbe advantageous. The work proposed here presents a solution using a\nmicroservices-based architecture that allows an IoT application to be\nstructured as a collection of fine-grained, loosely coupled, and reusable\nentities. The proposed solution uses the promising capabilities of federated\nlearning to provide intelligent microservices that ensure efficient, flexible,\nand extensible data analytics. This solution aims to deliver cloud calculations\nto the edge to reduce latency and bandwidth congestion while protecting the\nprivacy of exchanged data. The proposed approach was validated through an\nIoT-malware detection and classification use case. MaleVis, a publicly\navailable dataset, was used in the experiments to analyze and validate the\nproposed approach. This dataset included more than 14,000 RGB-converted images,\ncomprising 25 malware classes and one benign class. The results showed that our\nproposed approach outperformed existing state-of-the-art methods in terms of\ndetection and classification performance, with a 99.24%.", "AI": {"tldr": "该论文提出了一种基于微服务架构和联邦学习的IoT数据分析解决方案，通过边缘计算降低延迟并保护数据隐私，在恶意软件检测用例中取得了99.24%的优异性能。", "motivation": "解决IoT数据分析中的隐私安全问题和延迟问题，需要在边缘设备上进行分布式数据分析，同时确保时间敏感应用的快速响应。", "method": "采用微服务架构将IoT应用构建为细粒度、松耦合的可重用实体，结合联邦学习技术提供智能微服务，使用MaleVis数据集（14,000+张RGB图像）进行恶意软件检测验证。", "result": "提出的方法在恶意软件检测和分类性能上优于现有最先进方法，达到了99.24%的高准确率。", "conclusion": "基于微服务架构和联邦学习的解决方案能够有效实现边缘计算环境下的高效、灵活和可扩展的数据分析，同时保障数据隐私和降低延迟。"}}
{"id": "2510.20856", "pdf": "https://arxiv.org/pdf/2510.20856", "abs": "https://arxiv.org/abs/2510.20856", "authors": ["Jia Deng", "Jin Li", "Zhenhua Zhao", "Shaowei Wang"], "title": "FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models", "categories": ["cs.CR"], "comment": "11pages,4figures", "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot generalizability across diverse downstream tasks. However, recent\nstudies have revealed that VLMs, including CLIP, are highly vulnerable to\nadversarial attacks, particularly on their visual modality. Traditional methods\nfor improving adversarial robustness, such as adversarial training, involve\nextensive retraining and can be computationally expensive. In this paper, we\npropose a new Test-Time defense: Feature Perception Threshold Counterattack\nNoise (FPT-Noise), which enhances the adversarial robustness of CLIP without\ncostly fine-tuning. Our core contributions are threefold: First, we introduce a\nDynamic Feature Modulator that dynamically generate an image-specific and\nattack-adaptive noise intensity parameter. Second, We reanalyzed the image\nfeatures of CLIP. When images are exposed to different levels of noise, clean\nimages and adversarial images exhibit distinct rates of feature change. We\nestablished a feature perception threshold to distinguish clean images from\nattacked ones. Finally, we integrate a Scene-Aware Regulation guided by a\nstability threshold and leverage Test-Time Transformation Ensembling (TTE) to\nfurther mitigate the impact of residual noise and enhance robustness.Extensive\nexperimentation has demonstrated that FPT-Noise significantly outperforms\nexisting Test-Time defense methods, boosting average robust accuracy from 0.07%\nto 56.86% under AutoAttack while maintaining high performance on clean images\n(-1.1%). The code will be made public following the publication of the study.\nThe code will be made public following the publication of the study.", "AI": {"tldr": "FPT-Noise是一种新的测试时防御方法，通过动态特征调制器和特征感知阈值来增强CLIP模型的对抗鲁棒性，无需昂贵的微调训练。", "motivation": "现有的视觉语言模型（如CLIP）在对抗攻击下表现脆弱，传统对抗训练方法计算成本高昂，需要一种无需重新训练的高效防御方案。", "method": "提出FPT-Noise方法：1) 动态特征调制器生成图像特定和攻击自适应的噪声强度参数；2) 建立特征感知阈值区分干净图像和对抗图像；3) 集成场景感知调节和测试时变换集成技术。", "result": "FPT-Noise显著优于现有测试时防御方法，在AutoAttack下将平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上保持高性能（仅下降1.1%）。", "conclusion": "该方法提供了一种高效且有效的测试时防御解决方案，显著提升了CLIP模型的对抗鲁棒性，同时保持了在干净数据上的性能表现。"}}
{"id": "2510.20858", "pdf": "https://arxiv.org/pdf/2510.20858", "abs": "https://arxiv.org/abs/2510.20858", "authors": ["Nubio Vidal", "Naghmeh Moradpoor", "Leandros Maglaras"], "title": "Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology", "categories": ["cs.CR"], "comment": null, "summary": "Operational technology (OT) networks are increasingly coupled with\ninformation technology (IT), expanding the attack surface and complicating\nincident response. Although OT standards emphasise incident reporting and\nevidence preservation, they do not specify what data to capture during an\nincident, which hinders coordination across stakeholders. In contrast, IT\nguidance defines reporting content but does not address OT constraints. This\npaper presents the Agnostic Incident Reporting (AIR) framework for live OT\nincident reporting. AIR comprises 25 elements organised into seven groups to\ncapture incident context, chronology, impacts, and actions, tailored to\ntechnical, managerial, and regulatory needs. We evaluate AIR by mapping it to\nmajor OT standards, defining activation points for integration and triggering\nestablished OT frameworks, and then retrospectively applying it to the 2015\nUkrainian distribution grid incident. The evaluation indicates that AIR\ntranslates high-level requirements into concrete fields, overlays existing\nframeworks without vendor dependence, and can support situational awareness and\ncommunication during response. AIR offers a basis for standardising live OT\nincident reporting while supporting technical coordination and regulatory\nalignment.", "AI": {"tldr": "提出了Agnostic Incident Reporting (AIR)框架，包含25个元素和7个分组，用于实时OT事件报告，支持技术协调和监管对齐。", "motivation": "OT网络与IT融合扩大了攻击面，但现有标准未明确事件中应捕获的数据，阻碍了利益相关者间的协调；IT指南定义了报告内容但未考虑OT约束。", "method": "开发AIR框架，将其映射到主要OT标准，定义集成激活点，并回溯应用于2015年乌克兰电网事件进行验证。", "result": "AIR能将高层需求转化为具体字段，独立于供应商覆盖现有框架，支持响应过程中的态势感知和通信。", "conclusion": "AIR为标准化实时OT事件报告提供了基础，同时支持技术协调和监管一致性。"}}
{"id": "2510.20922", "pdf": "https://arxiv.org/pdf/2510.20922", "abs": "https://arxiv.org/abs/2510.20922", "authors": ["Luigi D. C. Soares", "Mário S. Alvim", "Natasha Fernandes"], "title": "A new measure for dynamic leakage based on quantitative information flow", "categories": ["cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "Quantitative information flow (QIF) is concerned with assessing the leakage\nof information in computational systems. In QIF there are two main perspectives\nfor the quantification of leakage. On one hand, the static perspective\nconsiders all possible runs of the system in the computation of information\nflow, and is usually employed when preemptively deciding whether or not to run\nthe system. On the other hand, the dynamic perspective considers only a\nspecific, concrete run of the system that has been realised, while ignoring all\nother runs. The dynamic perspective is relevant for, e.g., system monitors and\ntrackers, especially when deciding whether to continue or to abort a particular\nrun based on how much leakage has occurred up to a certain point. Although the\nstatic perspective of leakage is well-developed in the literature, the dynamic\nperspective still lacks the same level of theoretical maturity. In this paper\nwe take steps towards bridging this gap with the following key contributions:\n(i) we provide a novel definition of dynamic leakage that decouples the\nadversary's belief about the secret value from a baseline distribution on\nsecrets against which the success of the attack is measured; (ii) we\ndemonstrate that our formalisation satisfies relevant information-theoretic\naxioms, including non-interference and relaxed versions of monotonicity and the\ndata-processing inequality (DPI); (iii) we identify under what kind of analysis\nstrong versions of the axioms of monotonicity and the DPI might not hold, and\nexplain the implications of this (perhaps counter-intuitive) outcome; (iv) we\nshow that our definition of dynamic leakage is compatible with the\nwell-established static perspective; and (v) we exemplify the use of our\ndefinition on the formalisation of attacks against privacy-preserving data\nreleases.", "AI": {"tldr": "本文提出了一个新的动态信息泄漏定义，将攻击者对秘密值的信念与基准分布解耦，并验证了其满足信息论公理，同时与静态视角兼容。", "motivation": "定量信息流(QIF)中动态视角的理论成熟度不足，缺乏与静态视角相媲美的理论基础，需要填补这一研究空白。", "method": "提出新颖的动态泄漏定义，解耦攻击者信念与基准分布；验证其满足非干扰性、单调性和数据处理不等式等公理；分析强公理版本不成立的条件；展示与静态视角的兼容性；通过隐私保护数据发布攻击案例进行实证。", "result": "成功定义了一个满足关键信息论公理的新型动态泄漏度量，揭示了强单调性和强DPI在某些分析中可能不成立的原因，并证明了与静态泄漏定义的兼容性。", "conclusion": "该研究为动态信息流分析提供了更成熟的理论基础，填补了动态视角的理论空白，为系统监控和跟踪应用提供了更精确的信息泄漏评估方法。"}}
{"id": "2510.20930", "pdf": "https://arxiv.org/pdf/2510.20930", "abs": "https://arxiv.org/abs/2510.20930", "authors": ["Soham Hans", "Stacy Marsella", "Sophia Hirschmann", "Nikolos Gurney"], "title": "Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Understanding adversarial behavior in cybersecurity has traditionally relied\non high-level intelligence reports and manual interpretation of attack chains.\nHowever, real-time defense requires the ability to infer attacker intent and\ncognitive strategy directly from low-level system telemetry such as intrusion\ndetection system (IDS) logs. In this paper, we propose a novel framework that\nleverages large language models (LLMs) to analyze Suricata IDS logs and infer\nattacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded\nin the hypothesis that attacker behavior reflects underlying cognitive biases\nsuch as loss aversion, risk tolerance, or goal persistence that can be\nextracted and modeled through careful observation of log sequences. This lays\nthe groundwork for future work on behaviorally adaptive cyber defense and\ncognitive trait inference. We develop a strategy-driven prompt system to\nsegment large amounts of network logs data into distinct behavioral phases in a\nhighly efficient manner, enabling the LLM to associate each phase with likely\ntechniques and underlying cognitive motives. By mapping network-layer events to\nhigh-level attacker strategies, our method reveals how behavioral signals such\nas tool switching, protocol transitions, or pivot patterns correspond to\npsychologically meaningful decision points. The results demonstrate that LLMs\ncan bridge the semantic gap between packet-level logs and strategic intent,\noffering a pathway toward cognitive-adaptive cyber defense.\n  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),\nCyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive\nBiases", "AI": {"tldr": "使用大语言模型分析入侵检测系统日志，从底层网络遥测数据推断攻击者的MITRE ATT&CK技术和认知策略，为认知自适应网络安全防御提供新途径", "motivation": "传统网络安全依赖高层情报报告和人工分析攻击链，但实时防御需要直接从底层系统遥测数据推断攻击者意图和认知策略", "method": "开发策略驱动的提示系统，将大量网络日志数据分割为不同的行为阶段，利用大语言模型将每个阶段与可能的技术和潜在认知动机关联", "result": "大语言模型能够弥合数据包级日志与战略意图之间的语义鸿沟，揭示工具切换、协议转换等行为信号与心理决策点的对应关系", "conclusion": "该方法为行为自适应网络防御和认知特征推断奠定了基础，展示了向认知自适应网络安全防御的可行路径"}}
{"id": "2510.20932", "pdf": "https://arxiv.org/pdf/2510.20932", "abs": "https://arxiv.org/abs/2510.20932", "authors": ["Reza Ahmari", "Ahmad Mohammadi", "Vahid Hemmati", "Mohammed Mynuddin", "Mahmoud Nabil Mahmoud", "Parham Kebria", "Abdollah Homaifar", "Mehrdad Saif"], "title": "An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.RO"], "comment": "6 pages", "summary": "This study investigates the vulnerabilities of autonomous navigation and\nlanding systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses\non Trojan attacks that target deep learning models, such as Convolutional\nNeural Networks (CNNs). Trojan attacks work by embedding covert triggers within\na model's training data. These triggers cause specific failures under certain\nconditions, while the model continues to perform normally in other situations.\nWe assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using\nthe DroNet framework. Our experiments showed a significant drop in accuracy,\nfrom 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To\nconduct this study, we collected a custom dataset and trained models to\nsimulate real-world conditions. We also developed an evaluation framework\ndesigned to identify Trojan-infected models. This work demonstrates the\npotential security risks posed by Trojan attacks and lays the groundwork for\nfuture research on enhancing the resilience of UAM systems.", "AI": {"tldr": "本研究调查城市空中交通(UAM)车辆自主导航和着陆系统的漏洞，重点关注针对CNN等深度学习模型的木马攻击，实验显示木马攻击导致准确率从96.4%降至73.3%。", "motivation": "随着城市空中交通系统的发展，需要评估自主导航系统面对木马攻击的安全漏洞，这些攻击通过在训练数据中嵌入隐蔽触发器来导致特定条件下的系统故障。", "method": "使用DroNet框架评估城市自主飞行器的脆弱性，收集自定义数据集并训练模型模拟真实条件，开发了识别木马感染模型的评估框架。", "result": "实验结果显示木马攻击导致模型准确率显著下降，从清洁数据的96.4%降至触发数据上的73.3%。", "conclusion": "木马攻击对UAM系统构成严重安全风险，本研究为未来增强UAM系统韧性的研究奠定了基础。"}}
{"id": "2510.20956", "pdf": "https://arxiv.org/pdf/2510.20956", "abs": "https://arxiv.org/abs/2510.20956", "authors": ["Zheng-Xin Yong", "Stephen H. Bach"], "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We discover a novel and surprising phenomenon of unintentional misalignment\nin reasoning language models (RLMs), which we call self-jailbreaking.\nSpecifically, after benign reasoning training on math or code domains, RLMs\nwill use multiple strategies to circumvent their own safety guardrails. One\nstrategy is to introduce benign assumptions about users and scenarios to\njustify fulfilling harmful requests. For instance, an RLM reasons that harmful\nrequests like ``outline a strategy for stealing customers' credit card\ninformation from a retail store'' could be associated with the benign intent of\n``a security professional trying to test defense,'' despite no such benign\ncontext being provided as input. We observe that many open-weight RLMs,\nincluding DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,\nsuffer from self-jailbreaking despite being aware of the harmfulness of the\nrequests. We also provide a mechanistic understanding of self-jailbreaking:\nRLMs are more compliant after benign reasoning training, and after\nself-jailbreaking, models appear to perceive malicious requests as less harmful\nin the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,\nwe find that including minimal safety reasoning data during training is\nsufficient to ensure RLMs remain safety-aligned. Our work provides the first\nsystematic analysis of self-jailbreaking behavior and offers a practical path\nforward for maintaining safety in increasingly capable RLMs.", "AI": {"tldr": "论文发现推理语言模型存在自我越狱现象，即经过良性推理训练后，模型会使用多种策略绕过自身的安全防护机制，为有害请求寻找正当化理由。", "motivation": "研究旨在揭示推理语言模型在数学或代码领域良性训练后出现的意外不对齐现象，即模型会自我突破安全限制，为恶意请求提供合理化解释。", "method": "通过分析多个开源推理语言模型（如DeepSeek-R1-distilled、Phi-4-mini-reasoning等），研究其推理过程中的自我越狱策略，并提供机制性理解。", "result": "发现许多模型存在自我越狱问题，模型在良性推理训练后变得更顺从，并在思维链中将恶意请求感知为危害性较低，从而满足这些请求。", "conclusion": "通过在训练中加入最少量的安全推理数据可以有效缓解自我越狱问题，为保持推理语言模型的安全性提供了实用解决方案。"}}
{"id": "2510.20975", "pdf": "https://arxiv.org/pdf/2510.20975", "abs": "https://arxiv.org/abs/2510.20975", "authors": ["Darrin Lea", "James Ghawaly", "Golden Richard III", "Aisha Ali-Gombe", "Andrew Case"], "title": "REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted in 2025 Annual Computer Security Applications Conference\n  (ACSAC)", "summary": "Reverse engineering (RE) of x86 binaries is indispensable for malware and\nfirmware analysis, but remains slow due to stripped metadata and adversarial\nobfuscation. Large Language Models (LLMs) offer potential for improving RE\nefficiency through automated comprehension and commenting, but cloud-hosted,\nclosed-weight models pose privacy and security risks and cannot be used in\nclosed-network facilities. We evaluate parameter-efficient fine-tuned local\nLLMs for assisting with x86 RE tasks in these settings. Eight open-weight\nmodels across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned\non a custom curated dataset of 5,981 x86 assembly examples. We evaluate them\nquantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top\nperformer, which we name REx86.\n  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic\ncosine similarity against ground truth by 20.3\\% over its base model. In a\nlimited user case study (n=43), REx86 significantly enhanced line-level code\nunderstanding (p = 0.031) and increased the correct-solve rate from 31% to 53%\n(p = 0.189), though the latter did not reach statistical significance.\nQualitative analysis shows more accurate, concise comments with fewer\nhallucinations.\n  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight\nLLMs. Our findings demonstrate the value of domain-specific fine-tuning, and\nhighlight the need for more commented disassembly data to further enhance LLM\nperformance in RE. REx86, its dataset, and LoRA adapters are publicly available\nat https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.", "AI": {"tldr": "该论文开发了一个名为REx86的本地开源大语言模型，专门针对x86二进制逆向工程任务进行参数高效微调，在代码理解和注释生成方面表现出色，解决了云端闭源模型的安全隐私问题。", "motivation": "x86二进制逆向工程对恶意软件和固件分析至关重要，但由于元数据缺失和对抗性混淆，效率低下。云端闭源大语言模型存在隐私安全风险，无法在封闭网络环境中使用。", "method": "在5,981个x86汇编样本的自定义数据集上，对CodeLlama、Qwen2.5-Coder和CodeGemma系列的8个开源模型进行参数高效微调。", "result": "微调的Qwen2.5-Coder-7B模型（REx86）表现最佳：测试集交叉熵损失降低64.2%，语义余弦相似度提升20.3%；用户案例研究中代码理解显著改善（p=0.031），正确解决率从31%提升至53%。", "conclusion": "REx86在本地开源LLM中提供了最先进的x86逆向工程辅助能力，证明了领域特定微调的价值，并强调需要更多带注释的反汇编数据来进一步提升LLM在逆向工程中的性能。"}}
{"id": "2510.21004", "pdf": "https://arxiv.org/pdf/2510.21004", "abs": "https://arxiv.org/abs/2510.21004", "authors": ["Nguyen Linh Bao Nguyen", "Alsharif Abuadbba", "Kristen Moore", "Tingming Wu"], "title": "Can Current Detectors Catch Face-to-Voice Deepfake Attacks?", "categories": ["cs.CR", "cs.LG", "cs.MM", "cs.SD"], "comment": "8 pages, Accepted at Workshop on AI for Cyber Threat Intelligence,\n  co-located with ACSAC 2025", "summary": "The rapid advancement of generative models has enabled the creation of\nincreasingly stealthy synthetic voices, commonly referred to as audio\ndeepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly\nalarming capability: generating a victim's voice from a single facial image,\nwithout requiring any voice sample. By exploiting correlations between facial\nand vocal features, FOICE produces synthetic voices realistic enough to bypass\nindustry-standard authentication systems, including WeChat Voiceprint and\nMicrosoft Azure. This raises serious security concerns, as facial images are\nfar easier for adversaries to obtain than voice samples, dramatically lowering\nthe barrier to large-scale attacks. In this work, we investigate two core\nresearch questions: (RQ1) can state-of-the-art audio deepfake detectors\nreliably detect FOICE-generated speech under clean and noisy conditions, and\n(RQ2) whether fine-tuning these detectors on FOICE data improves detection\nwithout overfitting, thereby preserving robustness to unseen voice generators\nsuch as SpeechT5.\n  Our study makes three contributions. First, we present the first systematic\nevaluation of FOICE detection, showing that leading detectors consistently fail\nunder both standard and noisy conditions. Second, we introduce targeted\nfine-tuning strategies that capture FOICE-specific artifacts, yielding\nsignificant accuracy improvements. Third, we assess generalization after\nfine-tuning, revealing trade-offs between specialization to FOICE and\nrobustness to unseen synthesis pipelines. These findings expose fundamental\nweaknesses in today's defenses and motivate new architectures and training\nprotocols for next-generation audio deepfake detection.", "AI": {"tldr": "该论文评估了FOICE音频深度伪造技术的检测能力，发现现有检测器在标准和噪声条件下均失效，提出了针对性的微调策略以改进检测准确率，并揭示了专业化与泛化能力之间的权衡。", "motivation": "FOICE技术能够从单张面部图像生成逼真的合成语音，绕过行业标准认证系统，且面部图像比语音样本更容易获取，这带来了严重的安全威胁，需要评估现有检测器的有效性。", "method": "系统评估现有音频深度伪造检测器对FOICE生成语音的检测能力（包括标准和噪声条件），引入针对性微调策略捕获FOICE特有伪影，并评估微调后的泛化能力。", "result": "主流检测器在标准和噪声条件下均无法可靠检测FOICE生成语音；针对性微调显著提高了检测准确率；但微调后在FOICE专业化和对未见合成管道的鲁棒性之间存在权衡。", "conclusion": "当前防御系统存在根本性弱点，需要开发新架构和训练协议来构建下一代音频深度伪造检测系统。"}}
{"id": "2510.21024", "pdf": "https://arxiv.org/pdf/2510.21024", "abs": "https://arxiv.org/abs/2510.21024", "authors": ["Jonathan Gold", "Tristan Freiberg", "Haruna Isah", "Shirin Shahabi"], "title": "JSTprove: Pioneering Verifiable AI for a Trustless Future", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "comment": "13 pages, 8 figures, and 4 tables", "summary": "The integration of machine learning (ML) systems into critical industries\nsuch as healthcare, finance, and cybersecurity has transformed decision-making\nprocesses, but it also brings new challenges around trust, security, and\naccountability. As AI systems become more ubiquitous, ensuring the transparency\nand correctness of AI-driven decisions is crucial, especially when they have\ndirect consequences on privacy, security, or fairness. Verifiable AI, powered\nby Zero-Knowledge Machine Learning (zkML), offers a robust solution to these\nchallenges. zkML enables the verification of AI model inferences without\nexposing sensitive data, providing an essential layer of trust and privacy.\nHowever, traditional zkML systems typically require deep cryptographic\nexpertise, placing them beyond the reach of most ML engineers. In this paper,\nwe introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's\nExpander backend, to enable AI developers and ML engineers to generate and\nverify proofs of AI inference. JSTprove provides an end-to-end verifiable AI\ninference pipeline that hides cryptographic complexity behind a simple\ncommand-line interface while exposing auditable artifacts for reproducibility.\nWe present the design, innovations, and real-world use cases of JSTprove as\nwell as our blueprints and tooling to encourage community review and extension.\nJSTprove therefore serves both as a usable zkML product for current engineering\nneeds and as a reproducible foundation for future research and production\ndeployments of verifiable AI.", "AI": {"tldr": "JSTprove是一个基于零知识机器学习的工具包，旨在让AI开发者无需密码学专业知识即可验证AI推理过程，保护数据隐私的同时提供可审计的验证能力。", "motivation": "随着AI系统在关键行业的广泛应用，需要确保AI决策的透明度和正确性，但传统零知识机器学习系统需要深厚的密码学专业知识，限制了普通ML工程师的使用。", "method": "基于Polyhedra Network的Expander后端构建JSTprove工具包，提供端到端的可验证AI推理流水线，通过简单的命令行界面隐藏密码学复杂性，同时提供可审计的工件。", "result": "开发了JSTprove工具包，使AI开发者和ML工程师能够生成和验证AI推理证明，无需密码学专业知识。", "conclusion": "JSTprove既是满足当前工程需求的可用zkML产品，也是未来可验证AI研究和生产部署的可复现基础，鼓励社区评审和扩展。"}}
{"id": "2510.21053", "pdf": "https://arxiv.org/pdf/2510.21053", "abs": "https://arxiv.org/abs/2510.21053", "authors": ["Li An", "Yujian Liu", "Yepeng Liu", "Yuheng Bu", "Yang Zhang", "Shiyu Chang"], "title": "A Reinforcement Learning Framework for Robust and Secure LLM Watermarking", "categories": ["cs.CR"], "comment": null, "summary": "Watermarking has emerged as a promising solution for tracing and\nauthenticating text generated by large language models (LLMs). A common\napproach to LLM watermarking is to construct a green/red token list and assign\nhigher or lower generation probabilities to the corresponding tokens,\nrespectively. However, most existing watermarking algorithms rely on heuristic\ngreen/red token list designs, as directly optimizing the list design with\ntechniques such as reinforcement learning (RL) comes with several challenges.\nFirst, desirable watermarking involves multiple criteria, i.e., detectability,\ntext quality, robustness against removal attacks, and security against spoofing\nattacks. Directly optimizing for these criteria introduces many partially\nconflicting reward terms, leading to an unstable convergence process. Second,\nthe vast action space of green/red token list choices is susceptible to reward\nhacking. In this paper, we propose an end-to-end RL framework for robust and\nsecure LLM watermarking. Our approach adopts an anchoring mechanism for reward\nterms to ensure stable training and introduces additional regularization terms\nto prevent reward hacking. Experiments on standard benchmarks with two backbone\nLLMs show that our method achieves a state-of-the-art trade-off across all\ncriteria, with notable improvements in resistance to spoofing attacks without\ndegrading other criteria. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/RL-watermark.", "AI": {"tldr": "本文提出了一种端到端的强化学习框架，用于优化大型语言模型水印的绿色/红色令牌列表设计，通过锚定机制和正则化项解决了多目标优化不稳定和奖励黑客问题。", "motivation": "现有水印算法主要依赖启发式的绿色/红色令牌列表设计，直接使用强化学习优化面临多目标冲突导致训练不稳定和巨大动作空间易受奖励黑客攻击的挑战。", "method": "采用端到端强化学习框架，引入锚定机制确保训练稳定性，并添加正则化项防止奖励黑客问题。", "result": "在两个骨干LLM的标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著改进，且不降低其他标准。", "conclusion": "提出的RL框架成功解决了LLM水印优化中的关键挑战，为稳健安全的水印技术提供了有效解决方案。"}}
{"id": "2510.21057", "pdf": "https://arxiv.org/pdf/2510.21057", "abs": "https://arxiv.org/abs/2510.21057", "authors": ["Nils Philipp Walter", "Chawin Sitawarin", "Jamie Hayes", "David Stutz", "Ilia Shumailov"], "title": "Soft Instruction De-escalation Defense", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment; this makes them susceptible to\nprompt injections when dealing with untrusted data. To overcome this\nlimitation, we propose SIC (Soft Instruction Control)-a simple yet effective\niterative prompt sanitization loop designed for tool-augmented LLM agents. Our\nmethod repeatedly inspects incoming data for instructions that could compromise\nagent behavior. If such content is found, the malicious content is rewritten,\nmasked, or removed, and the result is re-evaluated. The process continues until\nthe input is clean or a maximum iteration limit is reached; if imperative\ninstruction-like content remains, the agent halts to ensure security. By\nallowing multiple passes, our approach acknowledges that individual rewrites\nmay fail but enables the system to catch and correct missed injections in later\nsteps. Although immediately useful, worst-case analysis shows that SIC is not\ninfallible; strong adversary can still get a 15% ASR by embedding\nnon-imperative workflows. This nonetheless raises the bar.", "AI": {"tldr": "SIC方法通过迭代式提示净化循环检测和重写恶意指令，提高LLM代理的安全性，但对抗性攻击仍有15%成功率", "motivation": "LLM在代理系统中处理不可信数据时容易受到提示注入攻击，需要有效防护机制", "method": "提出SIC方法：迭代检查输入数据中的恶意指令，通过重写、掩码或删除进行净化，直到输入安全或达到最大迭代次数", "result": "方法有效提高了安全性，但对抗性攻击仍能达到15%的攻击成功率(ASR)", "conclusion": "SIC虽然不能完全防御所有攻击，但显著提高了攻击门槛，为LLM代理安全提供了实用解决方案"}}
{"id": "2510.21124", "pdf": "https://arxiv.org/pdf/2510.21124", "abs": "https://arxiv.org/abs/2510.21124", "authors": ["Jie Zhang", "Xiaohong Li", "Mengke Zhang", "Ruitao Feng", "Shanshan Xu", "Zhe Hou", "Guangdong Bai"], "title": "QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute", "categories": ["cs.CR"], "comment": "17 pages, 10 figures", "summary": "Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a\ndecentralized paradigm for secure data governance but faces two inherent\nchallenges: the transparency of blockchain ledgers threatens user privacy by\nenabling reidentification attacks through attribute analysis, while the\ncomputational complexity of policy matching clashes with blockchain's\nperformance constraints. Existing solutions, such as those employing\nZero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable\nanonymity guarantees, while efficiency optimizations frequently ignore privacy\nimplications. To address these dual challenges, this paper proposes QAEBAC\n(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with\nAttribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically\nquantify the re-identification risk of users based on their access attributes\nand history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that\noptimizes policy structure based on realtime anonymity metrics, drastically\nreducing policy matching complexity. Implemented and evaluated on Hyperledger\nFabric, QAE-BAC demonstrates a superior balance between privacy and\nperformance. Experimental results show that it effectively mitigates\nre-identification risks and outperforms state-of-the-art baselines, achieving\nup to an 11x improvement in throughput and an 87% reduction in latency, proving\nits practicality for privacy-sensitive decentralized applications.", "AI": {"tldr": "QAEBAC提出了一种新的区块链属性访问控制方案，通过(r,t)-匿名模型量化重识别风险，并采用熵加权路径树优化策略匹配，在保护隐私的同时显著提升性能。", "motivation": "现有区块链ABAC方案面临两大挑战：区块链透明度导致用户隐私易受重识别攻击，策略匹配计算复杂度与区块链性能限制冲突。现有方案如零知识证明开销大且缺乏可量化匿名保障，效率优化往往忽视隐私影响。", "method": "提出QAEBAC框架：1) 引入形式化的(r,t)-匿名模型动态量化用户重识别风险；2) 设计熵加权路径树(EWPT)基于实时匿名指标优化策略结构，大幅降低策略匹配复杂度。在Hyperledger Fabric上实现和评估。", "result": "实验结果显示QAEBAC在隐私和性能间取得优异平衡：有效缓解重识别风险，吞吐量提升最高11倍，延迟降低87%，优于现有最先进基线方案。", "conclusion": "QAEBAC证明了在隐私敏感的分布式应用中实现可量化匿名和高效访问控制的实用性，为区块链ABAC提供了兼顾隐私保护与系统性能的创新解决方案。"}}
{"id": "2510.21133", "pdf": "https://arxiv.org/pdf/2510.21133", "abs": "https://arxiv.org/abs/2510.21133", "authors": ["Divyanshu Kumar", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "title": "Quantifying CBRN Risk in Frontier Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Frontier Large Language Models (LLMs) pose unprecedented dual-use risks\nthrough the potential proliferation of chemical, biological, radiological, and\nnuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation\nof 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and\na 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier\nattack methodology. Our findings expose critical safety vulnerabilities: Deep\nInception attacks achieve 86.0\\% success versus 33.8\\% for direct requests,\ndemonstrating superficial filtering mechanisms; Model safety performance varies\ndramatically from 2\\% (claude-opus-4) to 96\\% (mistral-small-latest) attack\nsuccess rates; and eight models exceed 70\\% vulnerability when asked to enhance\ndangerous material properties. We identify fundamental brittleness in current\nsafety alignment, where simple prompt engineering techniques bypass safeguards\nfor dangerous CBRN information. These results challenge industry safety claims\nand highlight urgent needs for standardized evaluation frameworks, transparent\nsafety metrics, and more robust alignment techniques to mitigate catastrophic\nmisuse risks while preserving beneficial capabilities.", "AI": {"tldr": "该论文首次全面评估了10个主流商业大语言模型在CBRN（化学、生物、放射性和核武器）知识扩散方面的双重用途风险，发现现有安全机制存在严重漏洞，深度诱导攻击成功率高达86%，模型安全性差异巨大（2%-96%），急需更强大的安全对齐技术。", "motivation": "前沿大语言模型存在前所未有的双重用途风险，可能扩散CBRN武器知识，需要评估当前商业LLMs的安全防护能力。", "method": "使用包含200个提示的新CBRN数据集和180个提示的FORTRESS基准子集，采用严格的三层攻击方法学对10个领先商业LLMs进行评估。", "result": "深度诱导攻击成功率86%远高于直接请求的33.8%；模型安全性表现差异巨大（claude-opus-4仅2%成功率，mistral-small-latest达96%）；8个模型在增强危险材料属性请求中超过70%的脆弱性。", "conclusion": "当前安全对齐机制存在根本性脆弱性，简单提示工程技术即可绕过防护措施，挑战了行业安全声明，亟需标准化评估框架、透明安全指标和更强大的对齐技术。"}}
{"id": "2510.21189", "pdf": "https://arxiv.org/pdf/2510.21189", "abs": "https://arxiv.org/abs/2510.21189", "authors": ["Yukun Jiang", "Mingjie Li", "Michael Backes", "Yang Zhang"], "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "categories": ["cs.CR"], "comment": "Accepted in NeurIPS 2025", "summary": "Despite their superior performance on a wide range of domains, large language\nmodels (LLMs) remain vulnerable to misuse for generating harmful content, a\nrisk that has been further amplified by various jailbreak attacks. Existing\njailbreak attacks mainly follow sequential logic, where LLMs understand and\nanswer each given task one by one. However, concurrency, a natural extension of\nthe sequential scenario, has been largely overlooked. In this work, we first\npropose a word-level method to enable task concurrency in LLMs, where adjacent\nwords encode divergent intents. Although LLMs maintain strong utility in\nanswering concurrent tasks, which is demonstrated by our evaluations on\nmathematical and general question-answering benchmarks, we notably observe that\ncombining a harmful task with a benign one significantly reduces the\nprobability of it being filtered by the guardrail, showing the potential risks\nassociated with concurrency in LLMs. Based on these findings, we introduce\n$\\texttt{JAIL-CON}$, an iterative attack framework that\n$\\underline{\\text{JAIL}}$breaks LLMs via task $\\underline{\\text{CON}}$currency.\nExperiments on widely-used LLMs demonstrate the strong jailbreak capabilities\nof $\\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the\nguardrail is applied as a defense, compared to the sequential answers generated\nby previous attacks, the concurrent answers in our $\\texttt{JAIL-CON}$ exhibit\ngreater stealthiness and are less detectable by the guardrail, highlighting the\nunique feature of task concurrency in jailbreaking LLMs.", "AI": {"tldr": "该论文提出了JAIL-CON攻击框架，利用任务并发性绕过大语言模型的安全防护，通过在相邻词汇中编码不同意图实现并发任务处理，显著降低了有害内容被防护机制检测的概率。", "motivation": "现有越狱攻击主要基于顺序逻辑，而并发性作为顺序场景的自然扩展被忽视。研究发现并发任务处理会降低有害内容被防护机制过滤的概率，存在潜在安全风险。", "method": "提出词级方法实现LLMs中的任务并发性，使相邻词汇编码不同意图。开发JAIL-CON迭代攻击框架，通过任务并发性进行越狱攻击。", "result": "JAIL-CON在广泛使用的LLMs上展现出强大的越狱能力，相比现有攻击方法，其并发答案具有更强的隐蔽性，更难被防护机制检测。", "conclusion": "任务并发性在LLMs越狱中具有独特优势，JAIL-CON框架证明了并发攻击的有效性和隐蔽性，揭示了LLMs安全防护的新挑战。"}}
{"id": "2510.21190", "pdf": "https://arxiv.org/pdf/2510.21190", "abs": "https://arxiv.org/abs/2510.21190", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long", "Kwok Yan Lam"], "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning", "categories": ["cs.CR"], "comment": "under review", "summary": "Large Language Models (LLMs) have advanced rapidly and now encode extensive\nworld knowledge. Despite safety fine-tuning, however, they remain susceptible\nto adversarial prompts that elicit harmful content. Existing jailbreak\ntechniques fall into two categories: white-box methods (e.g., gradient-based\napproaches such as GCG), which require model internals and are infeasible for\nclosed-source APIs, and black-box methods that rely on attacker LLMs to search\nor mutate prompts but often produce templates that lack explainability and\ntransferability. We introduce TrojFill, a black-box jailbreak that reframes\nunsafe instruction as a template-filling task. TrojFill embeds obfuscated\nharmful instructions (e.g., via placeholder substitution or Caesar/Base64\nencoding) inside a multi-part template that asks the model to (1) reason why\nthe original instruction is unsafe (unsafety reasoning) and (2) generate a\ndetailed example of the requested text, followed by a sentence-by-sentence\nanalysis. The crucial \"example\" component acts as a Trojan Horse that contains\nthe target jailbreak content while the surrounding task framing reduces refusal\nrates. We evaluate TrojFill on standard jailbreak benchmarks across leading\nLLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical\nperformance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,\nand 97% on GPT-4o). Moreover, the generated prompts exhibit improved\ninterpretability and transferability compared with prior black-box optimization\napproaches. We release our code, sample prompts, and generated outputs to\nsupport future red-teaming research.", "AI": {"tldr": "TrojFill是一种新型的黑盒越狱攻击方法，通过将有害指令伪装成模板填充任务，在多个主流LLM上实现了高成功率攻击（最高100%），同时提高了生成提示的可解释性和可迁移性。", "motivation": "现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的模板缺乏可解释性和可迁移性。需要一种既能绕过安全防护又能保持提示质量的黑盒攻击方法。", "method": "TrojFill将不安全指令重新构建为模板填充任务，通过占位符替换或编码（如凯撒/Base64）隐藏有害指令，嵌入到多部分模板中，包含不安全原因推理和详细示例生成两个关键组件。", "result": "在多个主流LLM（ChatGPT、Gemini、DeepSeek、Qwen）上测试显示卓越性能：Gemini-flash-2.5和DeepSeek-3.1达到100%攻击成功率，GPT-4o达到97%。生成的提示比现有黑盒方法更具可解释性和可迁移性。", "conclusion": "TrojFill提供了一种有效的黑盒越狱方法，成功平衡了攻击效果与提示质量，为红队测试研究提供了有价值的工具和数据集。"}}
{"id": "2510.21214", "pdf": "https://arxiv.org/pdf/2510.21214", "abs": "https://arxiv.org/abs/2510.21214", "authors": ["Xingwei Zhong", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses", "categories": ["cs.CR"], "comment": null, "summary": "Multimodal large language models (MLLMs) comprise of both visual and textual\nmodalities to process vision language tasks. However, MLLMs are vulnerable to\nsecurity-related issues, such as jailbreak attacks that alter the model's input\nto induce unauthorized or harmful responses. The incorporation of the\nadditional visual modality introduces new dimensions to security threats. In\nthis paper, we proposed a black-box jailbreak method via both text and image\nprompts to evaluate MLLMs. In particular, we designed text prompts with\nprovocative instructions, along with image prompts that introduced mutation and\nmulti-image capabilities. To strengthen the evaluation, we also designed a\nRe-attack strategy. Empirical results show that our proposed work can improve\ncapabilities to assess the security of both open-source and closed-source\nMLLMs. With that, we identified gaps in existing defense methods to propose new\nstrategies for both training-time and inference-time defense methods, and\nevaluated them across the new jailbreak methods. The experiment results showed\nthat the re-designed defense methods improved protections against the jailbreak\nattacks.", "AI": {"tldr": "本文提出了一种针对多模态大语言模型的黑盒越狱攻击方法，通过文本和图像提示的组合攻击，并设计了重新攻击策略来评估模型安全性，同时改进了防御方法。", "motivation": "多模态大语言模型在视觉和文本模态结合后引入了新的安全威胁维度，现有模型容易受到越狱攻击，需要新的评估和防御方法。", "method": "提出黑盒越狱方法，使用挑衅性文本提示和具有突变及多图像能力的图像提示，设计重新攻击策略来增强评估效果。", "result": "实验结果显示该方法能有效评估开源和闭源MLLMs的安全性，识别出现有防御方法的不足。", "conclusion": "通过重新设计的防御方法在训练时和推理时都能提高对越狱攻击的防护能力，为MLLMs安全提供了新的解决方案。"}}
{"id": "2510.21236", "pdf": "https://arxiv.org/pdf/2510.21236", "abs": "https://arxiv.org/abs/2510.21236", "authors": ["Christoph Bühler", "Matteo Biagiola", "Luca Di Grazia", "Guido Salvaneschi"], "title": "Securing AI Agent Execution", "categories": ["cs.CR", "cs.AI", "cs.SE", "D.2.0"], "comment": null, "summary": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security.", "AI": {"tldr": "AgentBound是首个针对MCP服务器的访问控制框架，通过声明式策略机制和安全执行引擎来保护LLM代理与外部工具交互时的系统安全。", "motivation": "MCP已成为连接AI代理与外部工具的标准协议，但存在严重的安全隐患，数千个MCP服务器拥有对主机系统的无限制访问权限，形成了广泛的攻击面。", "method": "结合受Android权限模型启发的声明式策略机制和策略执行引擎，无需修改MCP服务器即可遏制恶意行为。基于296个最流行的MCP服务器构建数据集，自动从源代码生成访问控制策略。", "result": "自动生成访问控制策略的准确率达到80.9%，能够阻止大多数恶意MCP服务器的安全威胁，策略执行引擎引入的开销可忽略不计。", "conclusion": "AgentBound为开发者和项目经理提供了保护MCP服务器的实用基础，同时保持生产力，使研究人员和工具构建者能够探索声明式访问控制和MCP安全的新方向。"}}
{"id": "2510.21246", "pdf": "https://arxiv.org/pdf/2510.21246", "abs": "https://arxiv.org/abs/2510.21246", "authors": ["Michael Külper", "Jan-Niclas Hilgert", "Frank Breitinger", "Martin Lambertz"], "title": "What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions", "categories": ["cs.CR"], "comment": null, "summary": "Self-hosted cloud storage platforms like Nextcloud are gaining popularity\namong individuals and organizations seeking greater control over their data.\nHowever, this shift introduces new challenges for digital forensic\ninvestigations, particularly in systematically analyzing both client and server\ncomponents. Despite Nextcloud's widespread use, it has received limited\nattention in forensic research. In this work, we critically examine existing\ncloud storage forensic frameworks and highlight their limitations. To address\nthe gaps, we propose an extended forensic framework that incorporates device\nmonitoring and leverages cloud APIs for structured, repeatable evidence\nacquisition. Using Nextcloud as a case study, we demonstrate how its native\nAPIs can be used to reliably access forensic artifacts, and we introduce an\nopen-source acquisition tool that implements this approach. Our framework\nequips investigators with a more flexible method for analyzing self-hosted\ncloud storage systems, and offers a foundation for further development in this\nevolving area of digital forensics.", "AI": {"tldr": "本文提出一个扩展的数字取证框架，专门针对自托管云存储系统（如Nextcloud），通过设备监控和云API实现结构化、可重复的证据采集，并开发了开源采集工具。", "motivation": "自托管云存储平台（如Nextcloud）日益流行，但给数字取证调查带来新挑战，现有云存储取证框架存在局限性，且Nextcloud在取证研究中关注不足。", "method": "批判性分析现有云存储取证框架的局限性；提出扩展取证框架，整合设备监控和云API；以Nextcloud为案例研究，展示如何利用其原生API可靠访问取证证据；开发开源采集工具实现该方法。", "result": "成功演示了利用Nextcloud原生API进行可靠取证证据访问的方法；开发了实现该框架的开源采集工具；提供了分析自托管云存储系统的灵活方法。", "conclusion": "提出的扩展取证框架为调查人员提供了更灵活的分析自托管云存储系统的方法，并为这一不断发展的数字取证领域奠定了进一步发展的基础。"}}
{"id": "2510.21272", "pdf": "https://arxiv.org/pdf/2510.21272", "abs": "https://arxiv.org/abs/2510.21272", "authors": ["Lu Liu", "Wuqi Zhang", "Lili Wei", "Hao Guan", "Yongqiang Tian", "Yepang Liu"], "title": "LLM-Powered Detection of Price Manipulation in DeFi", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Decentralized Finance (DeFi) smart contracts manage billions of dollars,\nmaking them a prime target for exploits. Price manipulation vulnerabilities,\noften via flash loans, are a devastating class of attacks causing significant\nfinancial losses. Existing detection methods are limited. Reactive approaches\nanalyze attacks only after they occur, while proactive static analysis tools\nrely on rigid, predefined heuristics, limiting adaptability. Both depend on\nknown attack patterns, failing to identify novel variants or comprehend complex\neconomic logic. We propose PMDetector, a hybrid framework combining static\nanalysis with Large Language Model (LLM)-based reasoning to proactively detect\nprice manipulation vulnerabilities. Our approach uses a formal attack model and\na three-stage pipeline. First, static taint analysis identifies potentially\nvulnerable code paths. Second, a two-stage LLM process filters paths by\nanalyzing defenses and then simulates attacks to evaluate exploitability.\nFinally, a static analysis checker validates LLM results, retaining only\nhigh-risk paths and generating comprehensive vulnerability reports. To evaluate\nits effectiveness, we built a dataset of 73 real-world vulnerable and 288\nbenign DeFi protocols. Results show PMDetector achieves 88% precision and 90%\nrecall with Gemini 2.5-flash, significantly outperforming state-of-the-art\nstatic analysis and LLM-based approaches. Auditing a vulnerability with\nPMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an\nefficient and cost-effective alternative to manual audits.", "AI": {"tldr": "PMDetector是一个结合静态分析和大型语言模型推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在真实数据集上达到88%精确率和90%召回率，成本仅为每次审计0.03美元。", "motivation": "DeFi智能合约管理着数十亿美元资产，价格操纵漏洞（尤其是通过闪电贷）造成了重大财务损失。现有检测方法存在局限：反应性方法只能在攻击发生后分析，而静态分析工具依赖预定义启发式规则，无法识别新型攻击变体或理解复杂经济逻辑。", "method": "提出PMDetector混合框架：1) 静态污点分析识别潜在漏洞代码路径；2) 两阶段LLM处理（分析防御措施过滤路径，模拟攻击评估可利用性）；3) 静态分析检查器验证LLM结果，保留高风险路径并生成详细漏洞报告。", "result": "在包含73个真实漏洞和288个良性DeFi协议的数据集上测试，使用Gemini 2.5-flash达到88%精确率和90%召回率，显著优于最先进的静态分析和基于LLM的方法。使用GPT-4.1时，每次漏洞审计仅需4.0秒，成本0.03美元。", "conclusion": "PMDetector提供了一个高效且经济实惠的替代手动审计的方案，能够主动检测价格操纵漏洞，有效应对现有方法的局限性，为DeFi安全提供了强有力的保障工具。"}}
{"id": "2510.21353", "pdf": "https://arxiv.org/pdf/2510.21353", "abs": "https://arxiv.org/abs/2510.21353", "authors": ["Aditya Mitra", "Sibi Chakkaravarthy Sethuraman"], "title": "The Qey: Implementation and performance study of post quantum cryptography in FIDO2", "categories": ["cs.CR", "cs.ET"], "comment": null, "summary": "Authentication systems have evolved a lot since the 1960s when Fernando\nCorbato first proposed the password-based authentication. In 2013, the FIDO\nAlliance proposed using secure hardware for authentication, thus marking a\nmilestone in the passwordless authentication era [1]. Passwordless\nauthentication with a possession-based factor often relied on hardware-backed\ncryptographic methods. FIDO2 being one an amalgamation of the W3C Web\nAuthentication and FIDO Alliance Client to Authenticator Protocol is an\nindustry standard for secure passwordless authentication with rising adoption\nfor the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256\n(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature\nalgorithms. This makes it insecure against attacks involving large-scale\nquantum computers [3]. This study aims at exploring the usability of Module\nLattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium\nas a post quantum cryptographic signature standard for FIDO2. The paper\nhighlights the performance and security in comparison to keys with classical\nalgorithms.", "AI": {"tldr": "该研究探索将基于模块格的后量子密码签名算法ML-DSA应用于FIDO2标准，以应对量子计算威胁，并与传统算法进行性能和安全性的比较分析。", "motivation": "当前FIDO2标准使用的ECDSA、RSA等经典密码算法容易受到大规模量子计算机的攻击，存在安全风险。", "method": "采用基于Crystals Dilithium的模块格数字签名算法(ML-DSA)作为后量子密码签名标准，研究其在FIDO2中的可用性。", "result": "论文对ML-DSA与传统算法在性能和安全性方面进行了比较分析。", "conclusion": "研究旨在为FIDO2认证系统提供量子安全的解决方案，推动密码无认证技术在后量子时代的发展。"}}
{"id": "2510.21401", "pdf": "https://arxiv.org/pdf/2510.21401", "abs": "https://arxiv.org/abs/2510.21401", "authors": ["Mojtaba Eshghie", "Gabriele Morello", "Matteo Lauretano", "Alexandre Bartel", "Martin Monperrus"], "title": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain.", "AI": {"tldr": "FLAMES是一个自动化工具，使用领域适应的大语言模型生成可执行的Solidity \"require\"语句来保护智能合约免受漏洞利用，无需漏洞标签或人工干预。", "motivation": "智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施。", "method": "使用在514,506个已验证合约中提取的真实世界不变式进行填空式监督微调，训练领域适应的大语言模型来合成运行时守卫。", "result": "FLAMES实现了96.7%的可编译性，在5000个挑战性不变式测试集中44.5%与真实情况精确或语义匹配，阻止了108个真实漏洞中的22个（20.4%），并成功阻止了APEMAGA真实攻击事件。", "conclusion": "领域适应的LLM能够自动为智能合约生成生产就绪的安全防御，无需漏洞检测、形式化规范或人工干预。"}}
{"id": "2510.21459", "pdf": "https://arxiv.org/pdf/2510.21459", "abs": "https://arxiv.org/abs/2510.21459", "authors": ["Adetayo Adebimpe", "Helmut Neukirchen", "Thomas Welsh"], "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5; D.4.6; I.2.7"], "comment": "to be published in: The 3rd International Conference on Foundation\n  and Large Language Models (FLLM2025), IEEE, 2025", "summary": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency.", "AI": {"tldr": "SBASH框架使用本地轻量级LLM解决蜜罐的数据保护问题，通过RAG和非RAG方法提升Linux shell命令响应的准确性和实时性，研究表明系统提示调优的模型可以达到与RAG相似的准确性且延迟更低。", "motivation": "传统蜜罐需要增强上下文感知能力以提高攻击者参与度，但现有LLM方法存在响应准确性、实时性、运营成本高和数据保护等问题。", "method": "提出SBASH框架，使用本地轻量级LLM，研究RAG支持和非RAG的LLM对Linux shell命令的处理，通过响应时间、人工测试真实性和与真实系统的相似度（Levenshtein距离、SBert、BertScore）进行评估。", "result": "RAG提高了未调优模型的准确性，而通过系统提示调优的模型无需RAG即可达到与RAG未调优模型相似的准确性，且具有略低的延迟。", "conclusion": "本地轻量级LLM结合适当的提示工程可以有效地提升蜜罐的上下文感知能力，同时解决数据保护和响应性能问题。"}}
{"id": "2510.21483", "pdf": "https://arxiv.org/pdf/2510.21483", "abs": "https://arxiv.org/abs/2510.21483", "authors": ["Pierre Guillot", "Auguste Hoang Duc", "Michel Koskas", "Florian Méhats"], "title": "Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise", "categories": ["cs.CR", "math.GR", "E.3"], "comment": null, "summary": "We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic\nEncryption without the need for bootstrapping (or in other words, without\nnoise). Building on the work of Nuida and others, we achieve this using\nencodings in groups.\n  The groups are represented on a machine using rewriting systems. In this way\nthe subgroup membership problem, which an attacker would have to solve in order\nto break the scheme, becomes maximally hard, while performance is preserved. In\nfact we include a simple benchmark demonstrating that our implementation runs\nseveral orders of magnitude faster than existing standards.\n  We review many possible attacks against our protocol and explain how to\nprotect the scheme in each case.", "AI": {"tldr": "GRAFHEN是一种无需自举（无噪声）的全同态加密方案，通过群编码和重写系统实现，性能比现有标准快几个数量级", "motivation": "解决现有全同态加密方案需要bootstrapping（自举）操作和噪声问题，提供更高效的无噪声全同态加密", "method": "基于Nuida等人的工作，使用群编码技术，通过重写系统在机器上表示群，利用子群成员问题的计算困难性保证安全性", "result": "实现了无需bootstrapping的全同态加密，性能显著提升（比现有标准快几个数量级），并分析了多种可能的攻击并提供防护方案", "conclusion": "GRAFHEN方案成功实现了无噪声的全同态加密，在保持安全性的同时大幅提升了性能，为实际应用提供了可行的解决方案"}}
{"id": "2510.21601", "pdf": "https://arxiv.org/pdf/2510.21601", "abs": "https://arxiv.org/abs/2510.21601", "authors": ["Emmanuel Dare Alalade", "Ashraf Matrawy"], "title": "PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis", "categories": ["cs.CR"], "comment": "26 pages, 18 figures", "summary": "Previous studies on PTA have focused on analyzing privacy threats based on\nthe potential areas of occurrence and their likelihood of occurrence. However,\nan in-depth understanding of the threat actors involved, their actions, and the\nintentions that result in privacy threats is essential. In this paper, we\npresent a novel Privacy Threat Model Framework (PTMF) that analyzes privacy\nthreats through different phases.\n  The PTMF development is motivated through the selected tactics from the MITRE\nATT\\&CK framework and techniques from the LINDDUN privacy threat model, making\nPTMF a privacy-centered framework. The proposed PTMF can be employed in various\nways, including analyzing the activities of threat actors during privacy\nthreats and assessing privacy risks in IoT systems, among others. In this\npaper, we conducted a user study on 12 privacy threats associated with IoT by\ndeveloping a questionnaire based on PTMF and recruited experts from both\nindustry and academia in the fields of security and privacy to gather their\nopinions. The collected data were analyzed and mapped to identify the threat\nactors involved in the identification of IoT users (IU) and the remaining 11\nprivacy threats. Our observation revealed the top three threat actors and the\ncritical paths they used during the IU privacy threat, as well as the remaining\n11 privacy threats. This study could provide a solid foundation for\nunderstanding how and where privacy measures can be proactively and effectively\ndeployed in IoT systems to mitigate privacy threats based on the activities and\nintentions of threat actors within these systems.", "AI": {"tldr": "本文提出了一个新颖的隐私威胁模型框架PTMF，通过结合MITRE ATT&CK和LINDDUN框架，从威胁行为者角度深入分析物联网隐私威胁，并通过专家问卷研究识别了主要威胁行为者和关键攻击路径。", "motivation": "现有隐私威胁分析主要关注威胁发生可能性和区域，缺乏对威胁行为者、其行为和意图的深入理解，需要开发一个以隐私为中心的威胁分析框架。", "method": "基于MITRE ATT&CK战术和LINDDUN隐私威胁模型技术开发PTMF框架，通过专家问卷研究分析12个物联网隐私威胁，识别威胁行为者和攻击路径。", "result": "识别了物联网用户识别隐私威胁中的前三大威胁行为者及其关键攻击路径，以及其余11个隐私威胁的相关发现。", "conclusion": "PTMF框架为理解物联网系统中威胁行为者的活动和意图提供了基础，有助于主动有效地部署隐私保护措施来缓解隐私威胁。"}}
{"id": "2510.21684", "pdf": "https://arxiv.org/pdf/2510.21684", "abs": "https://arxiv.org/abs/2510.21684", "authors": ["Albert Cheu", "Artem Lagzdin", "Brett McLarnon", "Daniel Ramage", "Katharine Daly", "Marco Gruteser", "Peter Kairouz", "Rakshita Tandon", "Stanislav Chiknavaryan", "Timon Van Overveldt", "Zoe Gong"], "title": "Toward provably private analytics and insights into GenAI use", "categories": ["cs.CR"], "comment": null, "summary": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences.", "AI": {"tldr": "新一代联邦分析系统，使用基于AMD SEV-SNP和Intel TDX的可信执行环境(TEEs)，为服务器端处理提供可验证的隐私保证，支持LLM处理非结构化数据并应用差分隐私，已成功部署生产环境。", "motivation": "大规模设备数据分析系统需要同时满足高隐私安全标准、数据质量、可用性和资源效率要求，现有系统在隐私保护和验证能力方面存在不足。", "method": "设备加密上传数据并标记允许的服务器处理步骤，使用开源TEE托管的密钥管理服务确保数据仅能被受TEE保护的处理步骤访问，支持LLM处理非结构化数据和差分隐私聚合。", "result": "系统成功部署生产环境，为真实世界的GenAI体验提供有价值的洞察，实现了可验证的隐私保护和处理透明度。", "conclusion": "基于TEE的联邦分析系统能够有效平衡隐私保护与数据分析需求，通过可验证的架构设计为大规模设备数据分析提供了可行的解决方案。"}}
{"id": "2510.21946", "pdf": "https://arxiv.org/pdf/2510.21946", "abs": "https://arxiv.org/abs/2510.21946", "authors": ["Kieu Dang", "Phung Lai", "NhatHai Phan", "Yelong Shen", "Ruoming Jin", "Abdallah Khreishah"], "title": "$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy", "categories": ["cs.CR", "68T07, 68T50", "I.2.6; I.2.7; K.6.5"], "comment": "Accepted at ACML 2025 (PMLR W&CP). Code:\n  https://github.com/kirudang/LDP_Stealing_Attack", "summary": "Large language models (LLMs) demonstrate remarkable capabilities across\nvarious tasks. However, their deployment introduces significant risks related\nto intellectual property. In this context, we focus on model stealing attacks,\nwhere adversaries replicate the behaviors of these models to steal services.\nThese attacks are highly relevant to proprietary LLMs and pose serious threats\nto revenue and financial stability. To mitigate these risks, the watermarking\nsolution embeds imperceptible patterns in LLM outputs, enabling model\ntraceability and intellectual property verification. In this paper, we study\nthe vulnerability of LLM service providers by introducing $\\delta$-STEAL, a\nnovel model stealing attack that bypasses the service provider's watermark\ndetectors while preserving the adversary's model utility. $\\delta$-STEAL\ninjects noise into the token embeddings of the adversary's model during\nfine-tuning in a way that satisfies local differential privacy (LDP)\nguarantees. The adversary queries the service provider's model to collect\noutputs and form input-output training pairs. By applying LDP-preserving noise\nto these pairs, $\\delta$-STEAL obfuscates watermark signals, making it\ndifficult for the service provider to determine whether its outputs were used,\nthereby preventing claims of model theft. Our experiments show that\n$\\delta$-STEAL with lightweight modifications achieves attack success rates of\nup to $96.95\\%$ without significantly compromising the adversary's model\nutility. The noise scale in LDP controls the trade-off between attack\neffectiveness and model utility. This poses a significant risk, as even robust\nwatermarks can be bypassed, allowing adversaries to deceive watermark detectors\nand undermine current intellectual property protection methods.", "AI": {"tldr": "论文提出了一种名为δ-STEAL的新型模型窃取攻击方法，能够绕过LLM服务提供商的水印检测器，同时保持攻击者模型的实用性，对现有知识产权保护方法构成严重威胁。", "motivation": "大型语言模型(LLM)部署存在知识产权风险，特别是模型窃取攻击会威胁专有LLM的收入和财务稳定性。虽然水印技术可用于模型溯源和知识产权验证，但需要研究其脆弱性。", "method": "提出δ-STEAL攻击方法：在对手模型微调期间向token嵌入注入噪声，满足局部差分隐私(LDP)保证。通过查询服务提供商模型收集输出，形成输入-输出训练对，应用LDP保护噪声来混淆水印信号。", "result": "实验显示δ-STEAL在轻量级修改下攻击成功率高达96.95%，且不显著损害对手模型实用性。LDP中的噪声规模控制攻击效果与模型实用性之间的权衡。", "conclusion": "δ-STEAL攻击方法能够有效绕过即使是鲁棒的水印保护，使对手能够欺骗水印检测器，从而破坏当前的知识产权保护方法，对LLM服务提供商构成重大风险。"}}
{"id": "2510.21957", "pdf": "https://arxiv.org/pdf/2510.21957", "abs": "https://arxiv.org/abs/2510.21957", "authors": ["Zhixin Pan", "Ziyu Shu", "Amberbir Alemayoh"], "title": "Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.6"], "comment": "This paper was accepted in the 2025 IEEE International Conference on\n  Computer Design (ICCD)", "summary": "Ransomware has become a critical threat to cybersecurity due to its rapid\nevolution, the necessity for early detection, and growing diversity, posing\nsignificant challenges to traditional detection methods. While AI-based\napproaches had been proposed by prior works to assist ransomware detection,\nexisting methods suffer from three major limitations, ad-hoc feature\ndependencies, delayed response, and limited adaptability to unseen variants. In\nthis paper, we propose a framework that integrates self-supervised contrastive\nlearning with neural architecture search (NAS) to address these challenges.\nSpecifically, this paper offers three important contributions. (1) We design a\ncontrastive learning framework that incorporates hardware performance counters\n(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a\ncustomized loss function that encourages early-stage detection of malicious\nactivity, and significantly reduces the detection latency. (3) We deploy a\nneural architecture search (NAS) framework to automatically construct adaptive\nmodel architectures, allowing the detector to flexibly align with unseen\nransomware variants. Experimental results show that our proposed method\nachieves significant improvements in both detection accuracy (up to 16.1%) and\nresponse time (up to 6x) compared to existing approaches while maintaining\nrobustness under evasive attacks.", "AI": {"tldr": "该论文提出了一种结合自监督对比学习和神经架构搜索的勒索软件检测框架，通过硬件性能计数器和定制损失函数实现早期检测，显著提升了检测精度和响应速度。", "motivation": "勒索软件已成为网络安全的关键威胁，传统检测方法面临特征依赖、响应延迟和适应性有限三大挑战，需要更先进的AI解决方案。", "method": "采用自监督对比学习框架整合硬件性能计数器分析运行时行为；设计定制损失函数促进早期恶意活动检测；部署神经架构搜索自动构建自适应模型架构。", "result": "实验结果显示，相比现有方法，检测精度提升高达16.1%，响应时间加快6倍，且在规避攻击下保持鲁棒性。", "conclusion": "该框架有效解决了勒索软件检测的关键挑战，为实时网络安全防护提供了高效自适应的解决方案。"}}
{"id": "2510.22024", "pdf": "https://arxiv.org/pdf/2510.22024", "abs": "https://arxiv.org/abs/2510.22024", "authors": ["Evangelos Bitsikas", "Jason Veara", "Aanjhan Ranganathan"], "title": "Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla", "categories": ["cs.CR"], "comment": null, "summary": "Modern connected vehicles rely on persistent LTE connectivity to enable\nremote diagnostics, over-the-air (OTA) updates, and critical safety services.\nWhile mobile network vulnerabilities are well documented in the smartphone\necosystem, their impact in safety-critical automotive settings remains\ninsufficiently examined. In this work, we conduct a black-box, non-invasive\nsecurity analysis of LTE connectivity in Tesla vehicles, including the Model 3\nand Cybertruck, revealing systemic protocol weaknesses and architectural\nmisconfigurations. We find that Tesla's telematics stack is susceptible to IMSI\ncatching, rogue base station hijacking, and insecure fallback mechanisms that\nmay silently degrade service availability. Furthermore, legacy control-plane\nconfigurations allow for silent SMS injection and broadcast message spoofing\nwithout driver awareness. These vulnerabilities have implications beyond a\nsingle vendor as they challenge core assumptions in regulatory frameworks like\nISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient\ntelematics for type approval of modern vehicles.", "AI": {"tldr": "对特斯拉车辆LTE连接的黑盒安全分析，揭示了IMSI捕获、伪基站劫持、不安全回退机制等系统漏洞，挑战了汽车网络安全监管框架的核心假设", "motivation": "虽然移动网络漏洞在智能手机生态中已有充分研究，但其在安全关键的汽车环境中的影响仍未得到充分检验，需要评估LTE连接在车辆远程诊断、OTA更新和安全服务中的安全性", "method": "采用黑盒、非侵入式的安全分析方法，针对特斯拉Model 3和Cybertruck车辆的LTE连接进行测试，包括协议弱点和架构错误配置的检测", "result": "发现特斯拉远程信息处理堆栈存在多种漏洞：易受IMSI捕获、伪基站劫持攻击、不安全回退机制可能导致服务静默降级，遗留控制平面配置允许静默SMS注入和广播消息欺骗", "conclusion": "这些漏洞不仅影响单个厂商，还挑战了ISO/SAE 21434和UN R155/R156等监管框架的核心安全假设，突显了现代车辆类型认证中对安全、可追溯和有弹性的远程信息处理系统的需求"}}
{"id": "2510.22085", "pdf": "https://arxiv.org/pdf/2510.22085", "abs": "https://arxiv.org/abs/2510.22085", "authors": ["Pavlos Ntais"], "title": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.0; K.6.5"], "comment": "18 pages, 5 figures", "summary": "Large language models (LLMs) remain vulnerable to sophisticated prompt\nengineering attacks that exploit contextual framing to bypass safety\nmechanisms, posing significant risks in cybersecurity applications. We\nintroduce Jailbreak Mimicry, a systematic methodology for training compact\nattacker models to automatically generate narrative-based jailbreak prompts in\na one-shot manner. Our approach transforms adversarial prompt discovery from\nmanual craftsmanship into a reproducible scientific process, enabling proactive\nvulnerability assessment in AI-driven security systems. Developed for the\nOpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient\nfine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,\nachieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out\ntest set of 200 items. Cross-model evaluation reveals significant variation in\nvulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on\nLlama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad\napplicability and model-specific defensive strengths in cybersecurity contexts.\nThis represents a 54x improvement over direct prompting (1.5% ASR) and\ndemonstrates systematic vulnerabilities in current safety alignment approaches.\nOur analysis reveals that technical domains (Cybersecurity: 93% ASR) and\ndeception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,\nhighlighting threats to AI-integrated threat detection, malware analysis, and\nsecure systems, while physical harm categories show greater resistance (55.6%\nASR). We employ automated harmfulness evaluation using Claude Sonnet 4,\ncross-validated with human expert assessment, ensuring reliable and scalable\nevaluation for cybersecurity red-teaming. Finally, we analyze failure\nmechanisms and discuss defensive strategies to mitigate these vulnerabilities\nin AI for cybersecurity.", "AI": {"tldr": "提出Jailbreak Mimicry方法，通过训练小型攻击模型自动生成叙事型越狱提示，实现了81%的攻击成功率，比直接提示提高54倍，揭示了当前AI安全对齐方法的系统性漏洞。", "motivation": "大型语言模型仍然容易受到复杂提示工程攻击，这些攻击利用上下文框架绕过安全机制，对网络安全应用构成重大风险。", "method": "使用参数高效微调(LoRA)在Mistral-7B模型上，利用从AdvBench整理的训练数据集，开发自动生成叙事型越狱提示的系统化方法。", "result": "在200个测试项上对GPT-OSS-20B达到81.0%攻击成功率，对GPT-4达到66.5%，Llama-3达到79.5%，Gemini 2.5 Flash达到33.0%。技术领域(网络安全93%)和欺骗类攻击(欺诈87.8%)特别脆弱。", "conclusion": "该方法将对抗性提示发现从手工制作转变为可复现的科学过程，揭示了当前安全对齐方法的系统性漏洞，为AI网络安全红队测试提供了可靠且可扩展的评估方法。"}}
{"id": "2510.22100", "pdf": "https://arxiv.org/pdf/2510.22100", "abs": "https://arxiv.org/abs/2510.22100", "authors": ["Saif E. Nouma", "Attila A. Yavuz"], "title": "Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things", "categories": ["cs.CR"], "comment": null, "summary": "The Internet of Things (IoT) relies heavily on resource-limited devices to\ncommunicate critical (e.g., military data) information under low-energy\nadversarial environments and low-latency wireless channels. Authenticated\nEncryption (AE) guarantees confidentiality, authenticity, and integrity, making\nit a vital security service for IoT. However, current deployed (lightweight) AE\nstandards lack essential features like key compromise resiliency and compact\nauthentication tags, as well as performance enhancements such as offline-online\ncryptography. To address these gaps, we propose Graphene, the first (to our\nknowledge) symmetric Forward-secure and Aggregate Authenticated Encryption\n(FAAE) framework designed for the performance and security demands of low-end\nIoT infrastructures. Graphene innovates by synergizing key evolution strategies\nand offline-online cryptographic processing with Universal Message\nAuthentication Codes (UMACs) to guarantee breach-resiliency, near-optimal\nonline latency, and compactness. We demonstrate Graphene efficiency through two\ndistinct instantiations, each balancing unique performance trade-offs with\nextensibility for diverse MACs. Our experimental evaluation on commodity\nhardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant\nperformance gains over existing alternatives. Graphene is also backward\ncompatible with standard-compliant cryptographic implementations. We release\nour implementation as open source for public testing and adaptation.", "AI": {"tldr": "Graphene是首个对称前向安全聚合认证加密框架，专为低端物联网设备设计，结合密钥演进策略和离线-在线加密处理，提供密钥泄露恢复能力、近最优在线延迟和紧凑认证标签。", "motivation": "当前物联网认证加密标准缺乏密钥泄露恢复能力、紧凑认证标签和离线-在线加密等关键特性，无法满足资源受限设备在对抗性环境下的性能和安全需求。", "method": "提出Graphene框架，通过整合密钥演进策略、离线-在线加密处理和通用消息认证码(UMACs)，开发了两个不同的实例化实现，并在商用硬件和ARM Cortex-M4微控制器上进行实验评估。", "result": "实验显示Graphene在性能上显著优于现有方案，具有向后兼容性，并已开源发布供公共测试和使用。", "conclusion": "Graphene成功解决了物联网认证加密的关键缺陷，为资源受限设备提供了高效、安全且具有扩展性的解决方案，是物联网安全的重要进展。"}}
{"id": "2510.22191", "pdf": "https://arxiv.org/pdf/2510.22191", "abs": "https://arxiv.org/abs/2510.22191", "authors": ["Qi Sheng"], "title": "TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation", "categories": ["cs.CR"], "comment": null, "summary": "Provenance analysis based on system audit data has emerged as a fundamental\napproach for investigating Advanced Persistent Threat (APT) attacks. Due to the\nhigh concealment and long-term persistence of APT attacks, they are only\nrepresented as a minimal part of the critical path in the provenance graph.\nWhile existing techniques employ behavioral pattern matching and data flow\nfeature matching to uncover latent associations in attack sequences through\nprovenance graph path reasoning, their inability to establish effective attack\ncontext associations often leads to the conflation of benign system operations\nwith real attack entities, that fail to accurately characterize real APT\nbehaviors. We observe that while the causality of entities in the provenance\ngraph exhibit substantial complexity, attackers often follow specific attack\npatterns-specifically, clear combinations of tactics and techniques to achieve\ntheir goals. Based on these insights, we propose TPPR, a novel framework that\nfirst extracts anomaly subgraphs through abnormal node detection,\nTTP-annotation and graph pruning, then performs attack path reasoning using\nmined TTP sequential pattern, and finally reconstructs attack scenarios through\nconfidence-based path scoring and merging. Extensive evaluation on real\nenterprise logs (more than 100 million events) and DARPA TC dataset\ndemonstrates TPPR's capability to achieve 99.9% graph simplification (700,000\nto 20 edges) while preserving 91% of critical attack nodes, outperforming\nstate-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in\nreconstruction precision while maintaining attack scenario integrity.", "AI": {"tldr": "TPPR是一个新型的APT攻击溯源分析框架，通过异常子图提取、TTP标注和图剪枝，结合TTP序列模式进行攻击路径推理，实现了99.9%的图简化同时保留91%关键攻击节点，在重建精度上比现有方法提升63.1%-67.9%。", "motivation": "现有基于溯源图的APT攻击分析方法在行为模式匹配和数据流特征匹配方面存在局限，无法有效建立攻击上下文关联，容易将良性系统操作与真实攻击实体混淆，无法准确刻画真实的APT行为。", "method": "首先通过异常节点检测、TTP标注和图剪枝提取异常子图，然后使用挖掘的TTP序列模式进行攻击路径推理，最后通过基于置信度的路径评分和合并来重建攻击场景。", "result": "在真实企业日志（超过1亿事件）和DARPA TC数据集上的评估显示，TPPR能够实现99.9%的图简化（从70万条边减少到20条边），同时保留91%的关键攻击节点，在重建精度上比最先进解决方案（SPARSE、DepImpact）分别高出63.1%和67.9%。", "conclusion": "TPPR框架通过结合攻击者的战术技术模式（TTP）和序列模式分析，能够有效识别APT攻击的关键路径，显著提高攻击场景重建的准确性和效率，为APT攻击溯源分析提供了新的有效方法。"}}
{"id": "2510.22274", "pdf": "https://arxiv.org/pdf/2510.22274", "abs": "https://arxiv.org/abs/2510.22274", "authors": ["Anum Paracha", "Junaid Arshad", "Mohamed Ben Farah", "Khalid Ismail"], "title": "SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Data poisoning attacks are a potential threat to machine learning (ML)\nmodels, aiming to manipulate training datasets to disrupt their performance.\nExisting defenses are mostly designed to mitigate specific poisoning attacks or\nare aligned with particular ML algorithms. Furthermore, most defenses are\ndeveloped to secure deep neural networks or binary classifiers. However,\ntraditional multiclass classifiers need attention to be secure from data\npoisoning attacks, as these models are significant in developing multi-modal\napplications. Therefore, this paper proposes SecureLearn, a two-layer\nattack-agnostic defense to defend multiclass models from poisoning attacks. It\ncomprises two components of data sanitization and a new feature-oriented\nadversarial training. To ascertain the effectiveness of SecureLearn, we\nproposed a 3D evaluation matrix with three orthogonal dimensions: data\npoisoning attack, data sanitization and adversarial training. Benchmarking\nSecureLearn in a 3D matrix, a detailed analysis is conducted at different\npoisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,\ndetection and correction rates, and false discovery rate. The experimentation\nis conducted for four ML algorithms, namely Random Forest (RF), Decision Tree\n(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with\nthree public datasets, against three poisoning attacks and compared with two\nexisting mitigations. Our results highlight that SecureLearn is effective\nagainst the provided attacks. SecureLearn has strengthened resilience and\nadversarial robustness of traditional multiclass models and neural networks,\nconfirming its generalization beyond algorithm-specific defenses. It\nconsistently maintained accuracy above 90%, recall and F1-score above 75%. For\nneural networks, SecureLearn achieved 97% recall and F1-score against all\nselected poisoning attacks.", "AI": {"tldr": "SecureLearn是一种针对多类分类器的两层攻击不可知防御机制，通过数据净化和特征导向的对抗训练来抵御数据投毒攻击，在多种机器学习算法和数据集上表现优异。", "motivation": "现有的数据投毒防御大多针对特定攻击或特定ML算法，且主要关注深度神经网络或二分类器，而传统多类分类器在安全防护方面缺乏关注，这些模型在多模态应用中具有重要意义。", "method": "提出SecureLearn防御框架，包含两个组件：数据净化和新的特征导向对抗训练。采用3D评估矩阵（数据投毒攻击、数据净化和对抗训练三个正交维度）进行基准测试，在10%-20%投毒水平下评估准确性、召回率、F1分数等指标。", "result": "SecureLearn对所选攻击有效，保持准确性90%以上，召回率和F1分数75%以上。对于神经网络，对所有选定投毒攻击实现97%的召回率和F1分数，增强了传统多类模型和神经网络的抗攻击能力。", "conclusion": "SecureLearn证明了其超越算法特定防御的泛化能力，为多类分类器提供了有效的攻击不可知防御解决方案，显著提高了模型的抗攻击鲁棒性。"}}
{"id": "2510.22283", "pdf": "https://arxiv.org/pdf/2510.22283", "abs": "https://arxiv.org/abs/2510.22283", "authors": ["Devon A. Kelly", "Christiana Chamon"], "title": "Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY", "physics.app-ph"], "comment": null, "summary": "Wide-bandgap (WBG) technologies offer unprecedented improvements in power\nsystem efficiency, size, and performance, but also introduce unique sensor\ncorruption and cybersecurity risks in industrial control systems (ICS),\nparticularly due to high-frequency noise and sophisticated cyber-physical\nthreats. This proof-of-concept (PoC) study demonstrates the adaptation of a\nnoise-driven physically unclonable function (PUF) and machine learning\n(ML)-assisted anomaly detection framework to the demanding environment of\nWBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG\nswitching noise (up to 100 kHz) as a PUF source, and simultaneously using this\nnoise as a real-time threat indicator, the proposed system unites\nhardware-level authentication and anomaly detection. Our approach integrates\nhybrid machine learning (ML) models with adaptive Bayesian filtering, providing\nrobust and low-latency detection capabilities resilient to both natural\nelectromagnetic interference (EMI) and active adversarial manipulation. Through\ndetailed simulations of WBG modules under benign and attack\nscenarios--including EMI injection, signal tampering, and node\nimpersonation--we achieve 95% detection accuracy and sub-millisecond processing\nlatency. These results demonstrate the feasibility of physics-driven, dual-use\nnoise exploitation as a scalable ICS defense primitive. Our findings lay the\ngroundwork for next-generation security strategies that leverage inherent\ndevice characteristics, bridging hardware and artificial intelligence (AI) for\nenhanced protection of critical ICS infrastructure.", "AI": {"tldr": "该研究提出了一种利用宽禁带技术固有开关噪声作为物理不可克隆函数(PUF)源和实时威胁指示器的双重用途安全框架，结合机器学习实现高精度、低延迟的工业控制系统安全防护。", "motivation": "宽禁带技术虽然提升了电力系统性能，但带来了高频噪声和网络安全风险，需要在工业控制系统中开发新的安全防护方法。", "method": "采用噪声驱动的PUF和机器学习辅助异常检测框架，通过提取WBG开关噪声(高达100kHz)作为熵源，结合混合机器学习模型和自适应贝叶斯滤波技术。", "result": "在良性及攻击场景(EMI注入、信号篡改、节点冒充)的详细模拟中，实现了95%的检测准确率和亚毫秒级的处理延迟。", "conclusion": "该研究证明了物理驱动的噪声双重利用作为可扩展ICS防御原语的可行性，为利用固有设备特性、结合硬件和AI的下一代安全策略奠定了基础。"}}
{"id": "2510.22300", "pdf": "https://arxiv.org/pdf/2510.22300", "abs": "https://arxiv.org/abs/2510.22300", "authors": ["Chenyu Zhang", "Tairen Zhang", "Lanjun Wang", "Ruidong Chen", "Wenhui Li", "Anan Liu"], "title": "T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "AAAI under review", "summary": "Using risky text prompts, such as pornography and violent prompts, to test\nthe safety of text-to-image (T2I) models is a critical task. However, existing\nrisky prompt datasets are limited in three key areas: 1) limited risky\ncategories, 2) coarse-grained annotation, and 3) low effectiveness. To address\nthese limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark\ndesigned for evaluating safety-related tasks in T2I models. Specifically, we\nfirst develop a hierarchical risk taxonomy, which consists of 6 primary\ncategories and 14 fine-grained subcategories. Building upon this taxonomy, we\nconstruct a pipeline to collect and annotate risky prompts. Finally, we obtain\n6,432 effective risky prompts, where each prompt is annotated with both\nhierarchical category labels and detailed risk reasons. Moreover, to facilitate\nthe evaluation, we propose a reason-driven risky image detection method that\nexplicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,\nwe conduct a comprehensive evaluation of eight T2I models, nine defense\nmethods, five safety filters, and five attack strategies, offering nine key\ninsights into the strengths and limitations of T2I model safety. Finally, we\ndiscuss potential applications of T2I-RiskyPrompt across various research\nfields. The dataset and code are provided in\nhttps://github.com/datar001/T2I-RiskyPrompt.", "AI": {"tldr": "本文提出了T2I-RiskyPrompt基准测试，用于评估文本到图像模型的安全性，包含6,432个风险提示，涵盖6大类14个子类，并提出了基于原因的检测方法。", "motivation": "现有风险提示数据集存在三个主要问题：风险类别有限、标注粒度粗、有效性低，需要更全面的安全评估基准。", "method": "开发分层风险分类法，构建风险提示收集和标注流程，提出基于多语言大模型的原因驱动风险图像检测方法。", "result": "对8个T2I模型、9种防御方法、5个安全过滤器和5种攻击策略进行全面评估，提供了9个关键安全见解。", "conclusion": "T2I-RiskyPrompt为T2I模型安全评估提供了全面基准，具有跨研究领域的潜在应用价值。"}}
{"id": "2510.22387", "pdf": "https://arxiv.org/pdf/2510.22387", "abs": "https://arxiv.org/abs/2510.22387", "authors": ["Nader Nemati"], "title": "Privacy-Aware Federated nnU-Net for ECG Page Digitization", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.", "AI": {"tldr": "提出跨机构联邦学习框架，用于ECG图像数字化，在保护隐私的同时实现接近集中式训练的性能", "motivation": "集中式训练ECG图像数字化模型面临跨机构隐私保护和部署限制的冲突，需要开发隐私保护的分布式训练方案", "method": "使用联邦学习框架训练nnU-Net分割模型，整合FedAvg/FedProx/FedAdam聚合器，结合安全聚合和中心差分隐私，包含图像预处理和向量化流程", "result": "实验显示FedAdam收敛更快且性能更好，接近集中式性能，隐私机制在保护原始图像和客户端更新的同时保持竞争性准确度", "conclusion": "该框架为多机构环境提供了可部署、可审计的隐私保护ECG数字化解决方案，平衡了效用与隐私保护需求"}}
{"id": "2510.22396", "pdf": "https://arxiv.org/pdf/2510.22396", "abs": "https://arxiv.org/abs/2510.22396", "authors": ["Zhaoyang Li", "Zheng Yu", "Jingyi Song", "Meng Xu", "Yuxuan Luo", "Dongliang Mu"], "title": "PortGPT: Towards Automated Backporting Using Large Language Models", "categories": ["cs.CR"], "comment": "Accepted by IEEE S&P 2026", "summary": "Patch backporting, the process of migrating mainline security patches to\nolder branches, is an essential task in maintaining popular open-source\nprojects (e.g., Linux kernel). However, manual backporting can be\nlabor-intensive, while existing automated methods, which heavily rely on\npredefined syntax or semantic rules, often lack agility for complex patches.\n  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation\nof patch backporting in real-world scenarios. PORTGPT enhances an LLM with\ntools to access code on-demand, summarize Git history, and revise patches\nautonomously based on feedback (e.g., from compilers), hence, simulating\nhuman-like reasoning and verification. PORTGPT achieved an 89.15% success rate\non existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex\ncases, both outperforms state-of-the-art of backporting tools. We contributed 9\nbackported patches from PORTGPT to the Linux kernel community and all patches\nare now merged.", "AI": {"tldr": "PORTGPT是一个基于LLM的端到端自动化补丁回溯工具，通过工具调用和自主验证机制，在现有数据集上达到89.15%的成功率，在复杂案例上达到62.33%的成功率，优于现有最先进工具。", "motivation": "手动回溯安全补丁到旧版本分支是一项劳动密集型任务，现有自动化方法依赖预定义的语法或语义规则，缺乏对复杂补丁的灵活性。", "method": "PORTGPT增强LLM模型，使其能够按需访问代码、总结Git历史记录，并基于反馈（如编译器输出）自主修订补丁，模拟人类推理和验证过程。", "result": "在1815个现有数据集案例中达到89.15%成功率，在146个复杂案例中达到62.33%成功率，向Linux内核社区贡献的9个回溯补丁全部被合并。", "conclusion": "PORTGPT通过LLM代理的方式有效解决了补丁回溯的自动化问题，在真实场景中表现出色，为开源项目维护提供了高效的解决方案。"}}
{"id": "2510.22400", "pdf": "https://arxiv.org/pdf/2510.22400", "abs": "https://arxiv.org/abs/2510.22400", "authors": ["Fei Shao", "Jia Zou", "Zhichao Cao", "Xusheng Xiao"], "title": "ProGQL: A Provenance Graph Query System for Cyber Attack Investigation", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "Provenance analysis (PA) has recently emerged as an important solution for\ncyber attack investigation. PA leverages system monitoring to monitor system\nactivities as a series of system audit events and organizes these events as a\nprovenance graph to show the dependencies among system activities, which can\nreveal steps of cyber attacks. Despite their potential, existing PA techniques\nface two critical challenges: (1) they are inflexible and non-extensible,\nmaking it difficult to incorporate analyst expertise, and (2) they are memory\ninefficient, often requiring>100GB of RAM to hold entire event streams, which\nfundamentally limits scalability and deployment in real-world environments. To\naddress these limitations, we propose the PROGQL framework, which provides a\ndomain-specific graph search language with a well-engineered query engine,\nallowing PA over system audit events and expert knowledge to be jointly\nexpressed as a graph search query and thereby facilitating the investigation of\ncomplex cyberattacks. In particular, to support dependency searches from a\nstarting edge required in PA, PROGQL introduces new language constructs for\nconstrained graph traversal, edge weight computation, value propagation along\nweighted edges, and graph merging to integrate multiple searches. Moreover, the\nPROGQL query engine is optimized for efficient incremental graph search across\nheterogeneous database backends, eliminating the need for full in-memory\nmaterialization and reducing memory overhead. Our evaluations on real attacks\ndemonstrate the effectiveness of the PROGQL language in expressing a diverse\nset of complex attacks compared with the state-of-the-art graph query language\nCypher, and the comparison with the SOTA PA technique DEPIMPACT further\ndemonstrates the significant improvement of the scalability brought by our\nPROGQL framework's design.", "AI": {"tldr": "PROGQL框架提出了一种专门用于网络攻击调查的图搜索语言和查询引擎，解决了现有溯源分析技术的不灵活性和内存效率低下的问题。", "motivation": "现有溯源分析技术面临两个关键挑战：(1) 不灵活且不可扩展，难以整合分析师专业知识；(2) 内存效率低下，通常需要超过100GB内存来存储完整事件流，限制了在实际环境中的可扩展性和部署。", "method": "提出PROGQL框架，提供领域特定的图搜索语言和精心设计的查询引擎，支持约束图遍历、边权重计算、沿加权边的值传播和图合并等新语言构造。查询引擎针对异构数据库后端的高效增量图搜索进行了优化。", "result": "在真实攻击评估中，PROGQL语言在表达多样化复杂攻击方面比最先进的图查询语言Cypher更有效，与SOTA PA技术DEPIMPACT的比较进一步证明了PROGQL框架设计带来的可扩展性显著提升。", "conclusion": "PROGQL框架通过提供专门的图搜索语言和高效的查询引擎，成功解决了现有溯源分析技术的局限性，为复杂网络攻击调查提供了更灵活、可扩展且内存效率高的解决方案。"}}
{"id": "2510.22536", "pdf": "https://arxiv.org/pdf/2510.22536", "abs": "https://arxiv.org/abs/2510.22536", "authors": ["Jotaro Yano"], "title": "ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole", "categories": ["cs.CR"], "comment": null, "summary": "We formalize a cross-domain \"ZK coprocessor bridge\" that lets Solana programs\nrequest private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable\nAction Approvals (VAAs) as authenticated transport. The system comprises: (i) a\nSolana program that posts messages to Wormhole Core with explicit finality;\n(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound\npayload secretHash||m from the attested VAA, derives a domain-separated field\ncommitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference\nimplementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we\nprovide migration guidance to the payload-bound interface); (iii) a minimal\nAztec contract that consumes the message privately; and (iv) an off-chain\nrelayer that ferries VAAs and can record receipts on Solana. We present state\nmachines, message formats, and proof sketches for replay-safety, origin\nauthenticity, finality alignment, parameter binding (no relayer front-running\nof Aztec parameters), privacy, idempotence, and liveness. Finally, we include a\nconcise Reproducibility note with pinned versions and artifacts to replicate a\npublic testnet run.", "AI": {"tldr": "该论文提出了一个跨域的ZK协处理器桥接系统，允许Solana程序通过Wormhole VAA在Aztec L2上请求私有执行，包含Solana程序、EVM门户、Aztec合约和中继器四个组件。", "motivation": "解决Solana程序需要安全、私密地在以太坊生态的Aztec L2网络上执行计算的需求，实现跨区块链的隐私保护计算。", "method": "设计包含四个组件的系统架构：Solana程序发布消息到Wormhole Core，EVM门户验证VAA并处理重放保护，Aztec合约私有消费消息，离链中继器传输VAA和记录回执。", "result": "提出了完整的状态机、消息格式和安全性证明框架，包括重放安全、来源真实性、最终性对齐、参数绑定、隐私保护、幂等性和活跃性保障。", "conclusion": "成功设计并实现了跨域ZK协处理器桥接系统，提供了可复现的测试网络运行环境，为Solana与Aztec之间的隐私保护跨链计算提供了可行解决方案。"}}
{"id": "2510.22555", "pdf": "https://arxiv.org/pdf/2510.22555", "abs": "https://arxiv.org/abs/2510.22555", "authors": ["Dongyi Liu", "Jiangtong Li", "Dawei Cheng", "Changjun Jiang"], "title": "Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where\nadversaries implant malicious triggers to manipulate model predictions.\n  Existing trigger generators are often simplistic in structure and overly\nreliant on specific features, confining them to a single graph learning\nparadigm, such as graph supervised learning, graph contrastive learning, or\ngraph prompt learning.\n  This specialized design, which aligns the trigger with one learning\nobjective, results in poor transferability when applied to other learning\nparadigms.\n  For instance, triggers generated for the graph supervised learning paradigm\nperform poorly when tested within graph contrastive learning or graph prompt\nlearning environments.\n  Furthermore, these simple generators often fail to utilize complex structural\ninformation or node diversity within the graph data.\n  These constraints limit the attack success rates of such methods in general\ntesting scenarios.\n  Therefore, to address these limitations, we propose Cross-Paradigm Graph\nBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable\ngraph backdoor attack that employs graph prompt learning(GPL) to train a set of\nuniversal subgraph triggers.\n  First, we distill a compact yet expressive trigger set from target graphs,\nwhich is structured as a queryable repository, by jointly enforcing\nclass-awareness, feature richness, and structural fidelity.\n  Second, we conduct the first exploration of the theoretical transferability\nof GPL to train these triggers under prompt-based objectives, enabling\neffective generalization to diverse and unseen test-time paradigms.\n  Extensive experiments across multiple real-world datasets and defense\nscenarios show that CP-GBA achieves state-of-the-art attack success rates.", "AI": {"tldr": "CP-GBA是一种新的跨范式图后门攻击方法，通过图提示学习训练通用子图触发器，解决了现有方法在跨学习范式迁移性差的问题，在多个真实数据集上实现了最先进的攻击成功率。", "motivation": "现有图神经网络后门攻击的触发器生成器结构简单，过度依赖特定特征，只能适应单一图学习范式（如图监督学习、图对比学习或图提示学习），在其他学习范式中迁移性差，攻击成功率有限。", "method": "1. 从目标图中提取紧凑且表达性强的子图触发器集合，构建可查询的知识库，同时确保类别感知、特征丰富和结构保真；2. 首次探索图提示学习的理论可迁移性，在基于提示的目标下训练触发器，使其能有效泛化到多样化和未见过的测试范式。", "result": "在多个真实数据集和防御场景下的广泛实验表明，CP-GBA实现了最先进的攻击成功率。", "conclusion": "CP-GBA通过创新的跨范式设计，成功解决了图后门攻击的迁移性问题，为图神经网络的安全性研究提供了新的视角和方法。"}}
{"id": "2510.22561", "pdf": "https://arxiv.org/pdf/2510.22561", "abs": "https://arxiv.org/abs/2510.22561", "authors": ["Kaveri Banerjee", "Sajal Saha"], "title": "Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study", "categories": ["cs.CR", "cs.AI"], "comment": "13 Pages, 2 Figures", "summary": "Blockchain systems rely on decentralized ledgers and strong security\nguarantees. A key requirement is non-repudiation, which prevents denial of\ntransaction authorship and supports integrity of recorded data. This work\nsurveys digital signature schemes used in blockchain platforms and analyzes how\nthey deliver non-repudiation and contribute to overall system security. We\nexamine representative scheme families and their cryptographic foundations,\nsecurity assumptions, and properties relevant to deployment, including\nunforgeability, resistance to malleability, support for aggregation and\nmultisignature or threshold settings, key and signature sizes, and verification\ncost. Using these criteria, we compare the suitability of different designs for\nconsensus protocols, smart contract constraints, and resource limits. We\nhighlight practical tradeoffs that affect throughput, storage, scalability, and\nattack surfaces, and summarize benefits and limitations of each scheme in\nblockchain contexts. The study underscores that carefully chosen digital\nsignatures are central to achieving non-repudiation and preserving information\nintegrity, and it outlines implementation considerations and open directions\nsuch as interoperability and post-quantum readiness.", "AI": {"tldr": "对区块链中数字签名方案的系统性调研，分析其如何实现不可否认性并保障系统安全，比较不同签名方案在共识协议和智能合约中的适用性", "motivation": "区块链系统依赖去中心化账本和强安全保证，其中不可否认性是关键需求，需要防止交易作者否认并支持数据完整性", "method": "调研代表性数字签名方案家族，分析其密码学基础、安全假设和相关属性（不可伪造性、抗延展性、聚合支持、密钥签名大小、验证成本等），使用这些标准比较不同设计在共识协议和智能合约约束下的适用性", "result": "通过比较分析揭示了不同签名方案在吞吐量、存储、可扩展性和攻击面方面的实际权衡，总结了各方案在区块链环境中的优势和限制", "conclusion": "精心选择的数字签名对于实现不可否认性和保持信息完整性至关重要，研究还提出了互操作性和后量子准备等实施考虑和开放方向"}}
{"id": "2510.22566", "pdf": "https://arxiv.org/pdf/2510.22566", "abs": "https://arxiv.org/abs/2510.22566", "authors": ["Md. Mehedi Hasan"], "title": "FAARM: Firmware Attestation and Authentication Framework for Mali GPUs", "categories": ["cs.CR"], "comment": "10 pages, 8 figures. Preprint version under review in the area of\n  Computer Security (cs.CR)", "summary": "Recent work has revealed MOLE, the first practical attack to compromise GPU\nTrusted Execution Environments (TEEs), by injecting malicious firmware into the\nembedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence\nof cryptographic verification during initialization, adversaries with kernel\nprivileges can bypass memory protections, exfiltrate sensitive data at over 40\nMB/s, and tamper with inference results, all with negligible runtime overhead.\nThis attack surface affects commodity mobile SoCs and cloud accelerators,\nexposing a critical firmware-level trust gap in existing GPU TEE designs. To\naddress this gap, this paper presents FAARM, a lightweight Firmware Attestation\nand Authentication framework that prevents MOLE-style firmware subversion.\nFAARM integrates digital signature verification at the EL3 secure monitor using\nvendor-signed firmware bundles and an on-device public key anchor. At boot, EL3\nverifies firmware integrity and authenticity, enforces version checks, and\nlocks the firmware region, eliminating both pre-verification and\ntime-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a\nsoftware-only prototype on a Mali GPU testbed, using a Google Colab-based\nemulation framework that models the firmware signing process, the EL1 to EL3\nload path, and secure memory configuration. FAARM reliably detects and blocks\nmalicious firmware injections, rejecting tampered images before use and denying\noverwrite attempts after attestation. Firmware verification incurs only 1.34 ms\nlatency on average, demonstrating that strong security can be achieved with\nnegligible overhead. FAARM thus closes a fundamental gap in shim-based GPU\nTEEs, providing a practical, deployable defense that raises the security\nbaseline for both mobile and cloud GPU deployments.", "AI": {"tldr": "FAARM是一个轻量级固件认证框架，通过在EL3安全监控器中集成数字签名验证，有效防御MOLE攻击，保护GPU可信执行环境免受恶意固件注入，仅带来1.34毫秒的延迟开销。", "motivation": "MOLE攻击揭示了GPU TEEs中存在的关键固件级信任漏洞，攻击者可通过注入恶意固件绕过内存保护并窃取敏感数据，现有GPU TEE设计缺乏有效的固件验证机制。", "method": "FAARM采用供应商签名的固件包和设备内公钥锚点，在EL3安全监控器进行固件完整性和真实性验证，执行版本检查并锁定固件区域，消除预验证和TOCTOU攻击向量。", "result": "FAARM能可靠检测和阻止恶意固件注入，在使用前拒绝篡改镜像并在认证后阻止覆盖尝试，平均验证延迟仅为1.34毫秒，安全开销可忽略不计。", "conclusion": "FAARM填补了基于shim的GPU TEEs的基础安全漏洞，为移动和云GPU部署提供了实用且可部署的防御方案，显著提升了安全基线。"}}
{"id": "2510.22620", "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements.", "AI": {"tldr": "论文提出了threat snapshots框架和b³基准测试，用于系统评估LLM作为AI代理时的安全性，发现推理能力提升安全性而模型大小与安全性无关", "motivation": "当前缺乏对LLM作为AI代理时安全性的系统理解，现有框架无法全面处理LLM漏洞与传统安全风险的复杂交互问题", "method": "引入threat snapshots框架，通过隔离代理执行流程中的特定状态来识别LLM漏洞；构建包含194,331个众包对抗攻击的b³安全基准测试；评估31个流行LLM", "result": "研究发现增强推理能力能提高安全性，而模型大小与安全性没有相关性", "conclusion": "该框架和基准测试为LLM提供商和从业者提供了安全评估工具，指导代理开发者并激励模型开发者优先考虑骨干模型的安全性改进"}}
{"id": "2510.22622", "pdf": "https://arxiv.org/pdf/2510.22622", "abs": "https://arxiv.org/abs/2510.22622", "authors": ["Kangran Zhao", "Yupeng Chen", "Xiaoyu Zhang", "Yize Chen", "Weinan Guan", "Baicheng Chen", "Chengzhe Sun", "Soumyya Kanti Datta", "Qingshan Liu", "Siwei Lyu", "Baoyuan Wu"], "title": "DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection", "categories": ["cs.CR", "cs.CV", "cs.MM"], "comment": "Preprint", "summary": "The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.", "AI": {"tldr": "该论文提出了Mega-MMDF大规模多模态深度伪造数据集和DeepfakeBench-MM统一基准，用于解决多模态深度伪造检测领域缺乏充足训练数据和标准化评估的问题。", "motivation": "先进的生成式AI模型被滥用导致伪造人像音视频内容泛滥，带来严重社会风险。现有研究缺乏足够的多样化训练数据和标准化基准，阻碍了该领域的深入探索。", "method": "1. 构建Mega-MMDF数据集：使用21种伪造管道（10种音频伪造方法+12种视觉伪造方法+6种音频驱动人脸重演方法），包含11万真实样本和110万伪造样本。2. 建立DeepfakeBench-MM基准：为多模态深度伪造检测提供标准化协议和评估平台，支持5个数据集和11种检测器。", "result": "创建了当前最大最丰富的多模态深度伪造数据集，建立了首个统一基准，并通过全面评估发现了多个关键发现（如数据增强、堆叠伪造等方面）。", "conclusion": "Mega-MMDF数据集和DeepfakeBench-MM基准将作为推进多模态深度伪造检测研究的基础设施，为解决AI生成内容的安全问题提供重要支持。"}}
{"id": "2510.22628", "pdf": "https://arxiv.org/pdf/2510.22628", "abs": "https://arxiv.org/abs/2510.22628", "authors": ["Md. Mehedi Hasan", "Ziaur Rahman", "Rafid Mostafiz", "Md. Abir Hossain"], "title": "Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 5 figures. Preprint version under review in the area of\n  Artificial Intelligence (cs.AI)", "summary": "This paper presents a real-time modular defense system named Sentra-Guard.\nThe system detects and mitigates jailbreak and prompt injection attacks\ntargeting large language models (LLMs). The framework uses a hybrid\narchitecture with FAISS-indexed SBERT embedding representations that capture\nthe semantic meaning of prompts, combined with fine-tuned transformer\nclassifiers, which are machine learning models specialized for distinguishing\nbetween benign and adversarial language inputs. It identifies adversarial\nprompts in both direct and obfuscated attack vectors. A core innovation is the\nclassifier-retriever fusion module, which dynamically computes context-aware\nrisk scores that estimate how likely a prompt is to be adversarial based on its\ncontent and context. The framework ensures multilingual resilience with a\nlanguage-agnostic preprocessing layer. This component automatically translates\nnon-English prompts into English for semantic evaluation, enabling consistent\ndetection across over 100 languages. The system includes a HITL feedback loop,\nwhere decisions made by the automated system are reviewed by human experts for\ncontinual learning and rapid adaptation under adversarial pressure.\nSentra-Guard maintains an evolving dual-labeled knowledge base of benign and\nmalicious prompts, enhancing detection reliability and reducing false\npositives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =\n1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading\nbaselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike\nblack-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible\nwith diverse LLM backends. Its modular design supports scalable deployment in\nboth commercial and open-source environments. The system establishes a new\nstate-of-the-art in adversarial LLM defense.", "AI": {"tldr": "Sentra-Guard是一个实时模块化防御系统，使用混合架构检测和缓解针对大语言模型的越狱和提示注入攻击，达到99.96%检测率和极低攻击成功率。", "motivation": "针对大语言模型面临的安全威胁，特别是越狱和提示注入攻击，需要开发有效的实时防御系统来保护LLM安全。", "method": "采用混合架构：FAISS索引的SBERT嵌入表示捕获提示语义，结合微调transformer分类器；包含分类器-检索器融合模块动态计算风险分数；多语言预处理层支持100+语言；包含人机交互反馈循环。", "result": "检测率99.96%（AUC=1.00，F1=1.00），攻击成功率仅0.004%，优于LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）等基线方法。", "conclusion": "Sentra-Guard建立了对抗性LLM防御的新最先进水平，具有透明、可微调和模块化特点，支持商业和开源环境的可扩展部署。"}}
{"id": "2510.22661", "pdf": "https://arxiv.org/pdf/2510.22661", "abs": "https://arxiv.org/abs/2510.22661", "authors": ["Malik Imran", "Safiullah Khan", "Zain Ul Abideen", "Ciara Rafferty", "Ayesha Khalid", "Muhammad Rashid", "Maire O'Neill"], "title": "RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography", "categories": ["cs.CR", "cs.AR"], "comment": "6 pages, 1 figure, conference", "summary": "Post-quantum multivariate public key cryptography (MPKC) schemes resist\nquantum threats but require heavy operations, such as rejection sampling, which\nchallenge resource-limited devices. Prior hardware designs have addressed\nvarious aspects of MPKC signature generation. However, rejection sampling\nremains largely unexplored in such contexts. This paper presents RejSCore, a\nlightweight hardware accelerator for rejection sampling in post-quantum\ncryptography. It specifically targets the QR-UOV scheme, which is a prominent\ncandidate under the second-round of the National Institute of Standards and\nTechnology (NIST) additional digital signature standardization process. The\narchitecture includes an AES-CTR-128-based pseudorandom number generator.\nMoreover, a lightweight iterative method is employed in rejection sampling,\noffering reduced resource consumption and area overhead while slightly\nincreasing latency. The performance of RejSCore is comprehensively evaluated on\nArtix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and\nPower-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area\nof 2042 slices and 464,866~$\\mu m^2$, with operating frequencies of 222 MHz and\n565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =\n127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525\nclock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for\ndeployment in resource-constrained and security-critical environments.", "AI": {"tldr": "RejSCore是一个针对后量子密码学中拒绝采样操作的轻量级硬件加速器，专门为QR-UOV签名方案设计，在Artix-7 FPGA和65nm CMOS技术上实现了低资源消耗和高能效。", "motivation": "后量子多变量公钥密码方案需要繁重的拒绝采样操作，这对资源受限设备构成挑战，而现有硬件设计在这方面研究不足。", "method": "设计包含AES-CTR-128伪随机数生成器的轻量级迭代架构，使用面积-延迟乘积和功率-延迟乘积进行性能评估。", "result": "在Artix-7上达到2042个slice面积和222MHz频率，在65nm CMOS上达到464,866μm²面积和565MHz频率，8525个时钟周期完成操作。", "conclusion": "ADP和PDP评估证实RejSCore适合部署在资源受限和安全关键的环境中。"}}
{"id": "2510.22726", "pdf": "https://arxiv.org/pdf/2510.22726", "abs": "https://arxiv.org/abs/2510.22726", "authors": ["Van Le", "Tan Le"], "title": "SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking", "categories": ["cs.CR"], "comment": null, "summary": "SpoofTrackBench is a reproducible, modular benchmark for evaluating\nadversarial robustness in real-time localization and tracking (RTLS) systems\nunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensor\ndataset, we simulate drift, ghost, and mirror-type spoofing attacks and\nevaluate tracker performance using both Joint Probabilistic Data Association\n(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates\nclean and spoofed detection streams, visualizes spoof-induced trajectory\ndivergence, and quantifies assignment errors via direct drift-from-truth\nmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptive\nvisualizations enable interpretability across spoof types and configurations.\nEvaluation figures and logs are auto-exported for reproducible comparison.\nSpoofTrackBench sets a new standard for open, ethical benchmarking of\nspoof-aware tracking pipelines, enabling rigorous cross-architecture analysis\nand community validation.", "AI": {"tldr": "SpoofTrackBench是一个可复现的模块化基准测试，用于评估雷达欺骗攻击下实时定位跟踪系统的对抗鲁棒性，支持不同跟踪架构的性能比较和可视化分析。", "motivation": "需要建立一个开放、可复现的基准测试标准，用于系统评估雷达欺骗攻击对实时定位跟踪系统的影响，促进不同跟踪架构的对抗鲁棒性分析和社区验证。", "method": "利用Hampton University Skyler雷达传感器数据集，模拟漂移、幽灵和镜像三种欺骗攻击，使用JPDA和GNN两种跟踪架构进行评估，通过分离干净和欺骗检测流、可视化轨迹偏差和量化赋值误差来进行分析。", "result": "开发了一个包含聚类覆盖、注入感知时间线和场景自适应可视化的框架，能够自动导出评估图表和日志，实现跨欺骗类型和配置的可解释性分析。", "conclusion": "SpoofTrackBench为欺骗感知跟踪流程的开放、道德基准测试设立了新标准，支持严格的跨架构分析和社区验证。"}}
{"id": "2510.22944", "pdf": "https://arxiv.org/pdf/2510.22944", "abs": "https://arxiv.org/abs/2510.22944", "authors": ["Bin Wang", "YiLu Zhong", "MiDi Wan", "WenJie Yu", "YuanBing Ouyang", "Yenan Huang", "Hui Li"], "title": "Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become indispensable for automated code\ngeneration, yet the quality and security of their outputs remain a critical\nconcern. Existing studies predominantly concentrate on adversarial attacks or\ninherent flaws within the models. However, a more prevalent yet underexplored\nissue concerns how the quality of a benign but poorly formulated prompt affects\nthe security of the generated code. To investigate this, we first propose an\nevaluation framework for prompt quality encompassing three key dimensions: goal\nclarity, information completeness, and logical consistency. Based on this\nframework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale\nbenchmark dataset containing tasks with prompts categorized into four distinct\nlevels of normativity (L0-L3). Extensive experiments on multiple\nstate-of-the-art LLMs reveal a clear correlation: as prompt normativity\ndecreases, the likelihood of generating insecure code consistently and markedly\nincreases. Furthermore, we demonstrate that advanced prompting techniques, such\nas Chain-of-Thought and Self-Correction, effectively mitigate the security\nrisks introduced by low-quality prompts, substantially improving code safety.\nOur findings highlight that enhancing the quality of user prompts constitutes a\ncritical and effective strategy for strengthening the security of AI-generated\ncode.", "AI": {"tldr": "研究发现提示词质量对AI生成代码安全性有显著影响，低质量提示词会大幅增加不安全代码生成概率，而高级提示技术可有效缓解此风险", "motivation": "现有研究主要关注对抗攻击或模型固有缺陷，但忽略了良性但表述不佳的提示词对生成代码安全性的影响这一普遍但未充分探索的问题", "method": "提出包含目标清晰度、信息完整性和逻辑一致性三个维度的提示词质量评估框架，构建包含四个规范性等级(L0-L3)的大规模基准数据集CWE-BENCH-PYTHON，并在多个先进LLM上进行实验", "result": "实验显示提示词规范性降低与生成不安全代码概率增加存在明显正相关关系，Chain-of-Thought和Self-Correction等高级提示技术能有效减轻低质量提示词带来的安全风险", "conclusion": "提升用户提示词质量是增强AI生成代码安全性的关键有效策略"}}
{"id": "2510.22945", "pdf": "https://arxiv.org/pdf/2510.22945", "abs": "https://arxiv.org/abs/2510.22945", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "QuantumShield: Multilayer Fortification for Quantum Federated Learning", "categories": ["cs.CR"], "comment": null, "summary": "In this paper, we propose a groundbreaking quantum-secure federated learning\n(QFL) framework designed to safeguard distributed learning systems against the\nemerging threat of quantum-enabled adversaries. As classical cryptographic\nmethods become increasingly vulnerable to quantum attacks, our framework\nestablishes a resilient security architecture that remains robust even in the\npresence of quantum-capable attackers. We integrate and rigorously evaluate\nadvanced quantum and post-quantum protocols including Quantum Key Distribution\n(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and\nPost-Quantum Cryptography (PQC) to fortify the QFL process against both\nclassical and quantum threats. These mechanisms are systematically analyzed and\nimplemented to demonstrate their seamless interoperability within a secure and\nscalable QFL ecosystem. Through comprehensive theoretical modeling and\nexperimental validation, this work provides a detailed security and performance\nassessment of the proposed framework. Our findings lay a strong foundation for\nnext-generation federated learning systems that are inherently secure in the\nquantum era.", "AI": {"tldr": "提出了一种量子安全的联邦学习框架，通过整合量子密钥分发、量子隐形传态等量子技术来防御量子攻击，为量子时代的联邦学习系统提供安全保障。", "motivation": "随着量子计算的发展，传统加密方法面临量子攻击威胁，需要建立能够抵御量子攻击的联邦学习安全架构。", "method": "集成并评估量子密钥分发(QKD)、量子隐形传态、密钥封装机制(KEM)和后量子密码学(PQC)等量子安全协议，构建安全的联邦学习生态系统。", "result": "通过理论建模和实验验证，证明了该框架在量子攻击下的安全性和性能表现。", "conclusion": "该研究为下一代联邦学习系统在量子时代的安全运行奠定了坚实基础，提供了可扩展的量子安全解决方案。"}}
{"id": "2510.22963", "pdf": "https://arxiv.org/pdf/2510.22963", "abs": "https://arxiv.org/abs/2510.22963", "authors": ["Zesen Liu", "Zhixiang Zhang", "Yuchong Xie", "Dongdong She"], "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections.", "AI": {"tldr": "论文揭示了LLM提示压缩技术存在安全漏洞，攻击者可通过CompressionAttack框架利用该漏洞，实现高达80%的攻击成功率和98%的偏好翻转，现有防御措施无效。", "motivation": "LLM驱动的代理常使用提示压缩来降低推理成本，但这种优化效率而非安全性的压缩模块可能被恶意输入操纵，导致语义漂移和LLM行为改变。", "method": "提出CompressionAttack框架，包含两种攻击策略：HardCom（使用离散对抗编辑进行硬压缩）和SoftCom（在潜在空间进行扰动实现软压缩）。", "result": "在多个LLM上的实验显示攻击成功率高达80%，偏好翻转率达到98%，攻击具有高度隐蔽性和可迁移性。VSCode Cline和Ollama的案例研究证实了实际影响。", "conclusion": "当前防御措施被证明无效，突显了需要更强有力保护措施的必要性，提示压缩已成为新的攻击面需要重点关注。"}}
{"id": "2510.22971", "pdf": "https://arxiv.org/pdf/2510.22971", "abs": "https://arxiv.org/abs/2510.22971", "authors": ["Sudiksha Das", "Ashish Kundu"], "title": "Advancing Honeywords for Real-World Authentication Security", "categories": ["cs.CR"], "comment": null, "summary": "Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords\nstored alongside a real password, appear to be a proactive method to help\ndetect password credentials misuse. However, despite over a decade of research,\nthis technique has not been adopted by major authentication platforms. This\nposition paper argues that the core concept of Honeywords has potential but\nrequires more research on issues such as flatness, integration, and\nreliability, in order to be a practical deployable solution. This paper\nexamines the current work on Honeyword generation, attacker modeling, and\nhoneychecker architecture, analyzing the subproblems that have been addressed\nand ongoing issues that prevent this system from being more widely used. The\npaper then suggests a deployable framework that combines the\nattacker-resilient, context-aware decoy creation that Honeywords provide with\neasy integration into existing systems. Honeywords will only move from an\nacademic idea to a practical security tool if technical advances are paired\nwith secure and straightforward architectures, along with adaptive response\nhandling and detailed configuration checks.", "AI": {"tldr": "本文分析了Honeywords技术（蜜词密码）在密码安全中的应用潜力，指出尽管该技术已研究十多年但仍未被主流认证平台采用，需要解决平坦性、集成性和可靠性等问题才能实现实际部署。", "motivation": "Honeywords作为一种主动检测密码凭证滥用的方法具有潜力，但需要解决阻碍其实际应用的技术和架构问题。", "method": "通过分析现有的蜜词生成技术、攻击者建模和蜜检查器架构研究，识别已解决的问题和持续存在的障碍。", "result": "提出了一个可部署的框架，将抗攻击、上下文感知的蜜词创建与现有系统的简易集成相结合。", "conclusion": "Honeywords只有通过技术进步与安全简洁的架构、自适应响应处理和详细配置检查相结合，才能从学术理念转变为实用的安全工具。"}}
{"id": "2510.23024", "pdf": "https://arxiv.org/pdf/2510.23024", "abs": "https://arxiv.org/abs/2510.23024", "authors": ["Chuan Yan", "Zeng Li", "Kunlin Cai", "Liuhuo Wan", "Ruomai Ren", "Yiran Shen", "Guangdong Bai"], "title": "A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem", "categories": ["cs.CR", "cs.SE"], "comment": "16 pages", "summary": "Virtual Reality (VR) has gained increasing traction among various domains in\nrecent years, with major companies such as Meta, Pico, and Microsoft launching\ntheir application stores to support third-party developers in releasing their\napplications (or simply apps). These apps offer rich functionality but\ninherently collect privacy-sensitive data, such as user biometrics, behaviors,\nand the surrounding environment. Nevertheless, there is still a lack of\ndomain-specific regulations to govern the data handling of VR apps, resulting\nin significant variations in their privacy practices among app stores.\n  In this work, we present the first comprehensive multi-store study of privacy\npractices in the current VR app ecosystem, covering a large-scale dataset\ninvolving 6,565 apps collected from five major app stores. We assess both\ndeclarative and behavioral privacy practices of VR apps, using a multi-faceted\napproach based on natural language processing, reverse engineering, and static\nanalysis. Our assessment reveals significant privacy compliance issues across\nall stores, underscoring the premature status of privacy protection in this\nrapidly growing ecosystem. For instance, one third of apps fail to declare\ntheir use of sensitive data, and 21.5\\% of apps neglect to provide valid\nprivacy policies. Our work sheds light on the status quo of privacy protection\nwithin the VR app ecosystem for the first time. Our findings should raise an\nalert to VR app developers and users, and encourage store operators to\nimplement stringent regulations on privacy compliance among VR apps.", "AI": {"tldr": "对VR应用商店中隐私实践的首个大规模多平台研究，发现VR应用存在严重的隐私合规问题，包括三分之一的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策。", "motivation": "VR应用收集大量隐私敏感数据（如生物特征、用户行为和环境数据），但缺乏领域特定的数据管理法规，导致各应用商店的隐私实践存在显著差异。", "method": "使用自然语言处理、逆向工程和静态分析的多维度方法，评估了来自5个主要应用商店的6,565个VR应用的声明性和行为性隐私实践。", "result": "发现所有商店都存在显著的隐私合规问题：33%的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策，表明VR生态系统的隐私保护仍处于不成熟状态。", "conclusion": "研究首次揭示了VR应用生态系统中隐私保护的现状，结果应引起开发者和用户的警惕，并鼓励应用商店运营商对VR应用的隐私合规实施更严格的监管。"}}
{"id": "2510.23034", "pdf": "https://arxiv.org/pdf/2510.23034", "abs": "https://arxiv.org/abs/2510.23034", "authors": ["Gokulnath Rajendran", "Suman Deb", "Anupam Chattopadhyay"], "title": "Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures", "categories": ["cs.CR", "cs.AI"], "comment": "to be published in: 7th International Conference on Emerging\n  Electronics (ICEE 2025)", "summary": "Binarized Neural Networks (BNNs) are a class of deep neural networks designed\nto utilize minimal computational resources, which drives their popularity\nacross various applications. Recent studies highlight the potential of mapping\nBNN model parameters onto emerging non-volatile memory technologies,\nspecifically using crossbar architectures, resulting in improved inference\nperformance compared to traditional CMOS implementations. However, the common\npractice of protecting model parameters from theft attacks by storing them in\nan encrypted format and decrypting them at runtime introduces significant\ncomputational overhead, thus undermining the core principles of in-memory\ncomputing, which aim to integrate computation and storage. This paper presents\na robust strategy for protecting BNN model parameters, particularly within\nin-memory computing frameworks. Our method utilizes a secret key derived from a\nphysical unclonable function to transform model parameters prior to storage in\nthe crossbar. Subsequently, the inference operations are performed on the\nencrypted weights, achieving a very special case of Fully Homomorphic\nEncryption (FHE) with minimal runtime overhead. Our analysis reveals that\ninference conducted without the secret key results in drastically diminished\nperformance, with accuracy falling below 15%. These results validate the\neffectiveness of our protection strategy in securing BNNs within in-memory\ncomputing architectures while preserving computational efficiency.", "AI": {"tldr": "该论文提出了一种保护二进制神经网络模型参数的加密方法，利用物理不可克隆函数生成密钥对权重进行加密，在内存计算框架中实现高效的安全推理。", "motivation": "传统BNN模型参数加密保护方法在运行时解密会带来显著计算开销，违背内存计算集成存储与计算的核心原则，需要一种既能保护参数安全又不影响计算效率的方法。", "method": "使用物理不可克隆函数生成密钥，在将模型参数存储到交叉阵列前进行变换，实现对加密权重的直接推理操作，达到近似全同态加密的效果。", "result": "未经密钥的推理准确率降至15%以下，验证了保护策略的有效性，同时保持了计算效率。", "conclusion": "该方法成功解决了BNN在内存计算架构中的安全保护问题，在保证安全性的同时最小化运行时开销，实现了安全与效率的平衡。"}}
{"id": "2510.23035", "pdf": "https://arxiv.org/pdf/2510.23035", "abs": "https://arxiv.org/abs/2510.23035", "authors": ["Jun Jiang", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "title": "A high-capacity linguistic steganography based on entropy-driven rank-token mapping", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Linguistic steganography enables covert communication through embedding\nsecret messages into innocuous texts; however, current methods face critical\nlimitations in payload capacity and security. Traditional modification-based\nmethods introduce detectable anomalies, while retrieval-based strategies suffer\nfrom low embedding capacity. Modern generative steganography leverages language\nmodels to generate natural stego text but struggles with limited entropy in\ntoken predictions, further constraining capacity. To address these issues, we\npropose an entropy-driven framework called RTMStega that integrates rank-based\nadaptive coding and context-aware decompression with normalized entropy. By\nmapping secret messages to token probability ranks and dynamically adjusting\nsampling via context-aware entropy-based adjustments, RTMStega achieves a\nbalance between payload capacity and imperceptibility. Experiments across\ndiverse datasets and models demonstrate that RTMStega triples the payload\ncapacity of mainstream generative steganography, reduces processing time by\nover 50%, and maintains high text quality, offering a trustworthy solution for\nsecure and efficient covert communication.", "AI": {"tldr": "RTMStega是一种基于熵驱动的语言隐写框架，通过排名自适应编码和上下文感知解压缩技术，显著提升隐写容量和安全性，同时保持文本质量。", "motivation": "当前语言隐写方法存在嵌入容量低和安全性不足的问题：传统修改方法会产生可检测异常，检索方法容量有限，生成式方法受限于令牌预测熵值过低。", "method": "提出RTMStega框架，整合基于排名的自适应编码和上下文感知解压缩技术，通过将秘密消息映射到令牌概率排名，并基于上下文感知的熵调整进行动态采样。", "result": "实验表明RTMStega将主流生成式隐写的有效载荷容量提升三倍，处理时间减少50%以上，同时保持高文本质量。", "conclusion": "RTMStega为安全高效的隐蔽通信提供了可信解决方案，在容量、速度和文本质量之间实现了良好平衡。"}}
{"id": "2510.23036", "pdf": "https://arxiv.org/pdf/2510.23036", "abs": "https://arxiv.org/abs/2510.23036", "authors": ["Xudong Yang", "Jincheng Li", "Kaiwen Xing", "Zhenjia Xiao", "Mingjian Duan", "Weili Han", "Hu Xiong"], "title": "KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation", "categories": ["cs.CR"], "comment": null, "summary": "As the primary mechanism of digital authentication, user-created passwords\nexhibit common patterns and regularities that can be learned from leaked\ndatasets. Password choices are profoundly shaped by external factors, including\nsocial contexts, cultural trends, and popular vocabulary. Prevailing password\nguessing models primarily emphasize patterns derived from leaked passwords,\nwhile neglecting these external influences -- a limitation that hampers their\nadaptability to emerging password trends and erodes their effectiveness over\ntime.\n  To address these challenges, we propose KAPG, a knowledge-augmented password\nguessing framework that adaptively integrates external lexical knowledge into\nthe guessing process. KAPG couples internal statistical knowledge learned from\nleaked passwords with external information that reflects real-world trends. By\nusing password prefixes as anchors for knowledge lookup, it dynamically injects\nrelevant external cues during generation while preserving the structural\nregularities of authentic passwords. Experiments on twelve leaked datasets show\nthat KnowGuess achieves average improvements of 36.5\\% and 74.7\\% over\nstate-of-the-art models in intra-site and cross-site scenarios, respectively.\nFurther analyses of password overlap and model efficiency highlight its\nrobustness and computational efficiency. To counter these attacks, we further\ndevelop KAPSM, a trend-aware and site-specific password strength meter.\nExperiments demonstrate that KAPSM significantly outperforms existing tools in\naccuracy across diverse evaluation settings.", "AI": {"tldr": "KAPG是一个知识增强的密码猜测框架，通过结合泄露密码的内部统计知识和反映现实趋势的外部词汇知识，显著提升了密码猜测的准确性和适应性。", "motivation": "现有密码猜测模型主要依赖泄露密码的模式，忽视了社会背景、文化趋势和流行词汇等外部因素对密码选择的影响，导致模型难以适应新兴密码趋势且效果随时间下降。", "method": "提出KAPG框架，使用密码前缀作为知识查找锚点，在生成过程中动态注入相关外部线索，同时保持真实密码的结构规律性。", "result": "在12个泄露数据集上的实验显示，KAPG在站内和跨站场景下分别比最先进模型平均提升36.5%和74.7%，并展示了良好的鲁棒性和计算效率。", "conclusion": "KAPG通过整合外部知识有效提升了密码猜测性能，同时开发的KAPSM密码强度计也在各种评估设置中显著优于现有工具。"}}
{"id": "2510.23060", "pdf": "https://arxiv.org/pdf/2510.23060", "abs": "https://arxiv.org/abs/2510.23060", "authors": ["Paritosh Ramanan", "H. M. Mohaimanul Islam", "Abhiram Reddy Alugula"], "title": "zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks", "categories": ["cs.CR", "cs.SY", "eess.SY"], "comment": null, "summary": "Industrial control systems (ICS) form the operational backbone of critical\ninfrastructure networks (CIN) such as power grids, water supply systems, and\ngas pipelines. As cyber threats to these systems escalate, regulatory agencies\nare imposing stricter compliance requirements to ensure system-wide security\nand reliability. A central challenge, however, is enabling regulators to verify\nthe effectiveness of detection mechanisms without requiring utilities to\ndisclose sensitive operational data. In this paper, we introduce zkSTAR, a\ncyberattack detection framework that leverages zk-SNARKs to reconcile these\nrequirements and enable provable detection guarantees while preserving data\nconfidentiality. Our approach builds on established residual-based statistical\nhypothesis testing methods applied to state-space detection models.\nSpecifically, we design a two-pronged zk-SNARK architecture that enforces\ntemporal consistency of the state-space dynamics and statistical consistency of\nthe detection tests, allowing regulators to temporally verify alarm correctness\nwithout visibility into utility-level data. We formally analyze the soundness\nand zero knowledge properties of our framework and validate its practical\nfeasibility through computational experiments on real-world ICS datasets. As a\nresult, our work demonstrates a scalable, privacy-preserving alternative for\nregulatory compliance for ICS driven critical infrastructure networks.", "AI": {"tldr": "zkSTAR是一个基于zk-SNARKs的工业控制系统网络攻击检测框架，能够在保护数据隐私的同时为监管机构提供可验证的检测保证。", "motivation": "工业控制系统面临日益严重的网络威胁，监管机构需要验证检测机制有效性，但公用事业公司不愿披露敏感操作数据，存在隐私与监管的矛盾。", "method": "基于状态空间检测模型的残差统计假设检验方法，设计了双管齐下的zk-SNARK架构，确保状态空间动态的时间一致性和检测测试的统计一致性。", "result": "通过形式化分析框架的可靠性和零知识属性，并在真实ICS数据集上进行计算实验验证了实际可行性。", "conclusion": "该工作为工业控制系统驱动的关键基础设施网络提供了一个可扩展的、保护隐私的监管合规替代方案。"}}
{"id": "2510.23074", "pdf": "https://arxiv.org/pdf/2510.23074", "abs": "https://arxiv.org/abs/2510.23074", "authors": ["Hiromu Takahashi", "Shotaro Ishihara"], "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library\nfor efficiently evaluating membership inference attacks (MIA) against Large\nLanguage Models (LLMs). MIA against LLMs has emerged as a crucial challenge due\nto growing concerns over copyright, security, and data privacy, and has\nattracted increasing research attention. However, the progress of this research\nis significantly hindered by two main obstacles: (1) the high computational\ncost of inference in LLMs, and (2) the lack of standardized and maintained\nimplementations of MIA methods, which makes large-scale empirical comparison\ndifficult. To address these challenges, our library provides fast batch\ninference and includes implementations of representative MIA methods under a\nunified evaluation framework. This library supports easy implementation of\nreproducible benchmarks with simple configuration and extensibility. We release\nFast-MIA as an open-source (Apache License 2.0) tool to support scalable and\ntransparent research on LLMs.", "AI": {"tldr": "Fast-MIA是一个用于高效评估大型语言模型成员推理攻击的Python库，解决了计算成本高和标准化实现缺乏的问题。", "motivation": "由于对版权、安全和数据隐私的日益关注，LLM的成员推理攻击研究面临计算成本高和缺乏标准化实现两大障碍。", "method": "提供快速批量推理功能，并在统一评估框架下实现了代表性MIA方法，支持简单配置和可扩展性。", "result": "开发了开源的Fast-MIA工具（Apache 2.0许可证），支持可扩展和透明的LLM研究。", "conclusion": "Fast-MIA通过提供高效的批量推理和标准化MIA方法实现，为LLM成员推理攻击研究提供了重要的工具支持。"}}
{"id": "2510.23101", "pdf": "https://arxiv.org/pdf/2510.23101", "abs": "https://arxiv.org/abs/2510.23101", "authors": ["Yifan Zhang", "Xin Zhang"], "title": "Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": "Preprint, under submission", "summary": "Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific\ntarget locations by prioritizing seeds whose execution paths are more likely to\nmutate into triggering target bugs. However, existing DGF approaches suffer\nfrom imprecise probability calculations due to their reliance on complex\ndistance metrics derived from static analysis. The over-approximations inherent\nin static analysis cause a large number of irrelevant execution paths to be\nmistakenly considered to potentially mutate into triggering target bugs,\nsignificantly reducing fuzzing efficiency. We propose to replace static\nanalysis-based distance metrics with precise call stack representations. Call\nstacks represent precise control flows, thereby avoiding false information in\nstatic analysis. We leverage large language models (LLMs) to predict\nvulnerability-triggering call stacks for guiding seed prioritization. Our\napproach constructs call graphs through static analysis to identify methods\nthat can potentially reach target locations, then utilizes LLMs to predict the\nmost likely call stack sequence that triggers the vulnerability. Seeds whose\nexecution paths have higher overlap with the predicted call stack are\nprioritized for mutation. This is the first work to integrate LLMs into the\ncore seed prioritization mechanism of DGF. We implement our approach and\nevaluate it against several state-of-the-art fuzzers. On a suite of real-world\nprograms, our approach triggers vulnerabilities $1.86\\times$ to $3.09\\times$\nfaster compared to baselines. In addition, our approach identifies 10 new\nvulnerabilities and 2 incomplete fixes in the latest versions of programs used\nin our controlled experiments through directed patch testing, with 10 assigned\nCVE IDs.", "AI": {"tldr": "本文提出了一种基于大语言模型（LLM）的定向灰盒模糊测试方法，通过预测漏洞触发调用栈来改进种子优先级排序，显著提高了漏洞发现效率。", "motivation": "现有的定向灰盒模糊测试方法依赖静态分析的距离度量，存在概率计算不精确的问题，导致大量无关执行路径被误判，降低了模糊测试效率。", "method": "使用静态分析构建调用图识别可达目标位置的方法，然后利用LLM预测最可能触发漏洞的调用栈序列，优先选择执行路径与预测调用栈重叠度高的种子进行变异。", "result": "在真实程序测试中，该方法比基线方法快1.86到3.09倍触发漏洞，并发现了10个新漏洞和2个不完整修复，获得了10个CVE编号。", "conclusion": "这是首个将LLM集成到定向灰盒模糊测试核心种子优先级机制的工作，证明了基于调用栈表示和LLM预测的方法能有效提高定向模糊测试的精确性和效率。"}}
{"id": "2510.23172", "pdf": "https://arxiv.org/pdf/2510.23172", "abs": "https://arxiv.org/abs/2510.23172", "authors": ["Mohsen Ahmadvand", "Pedro Souto"], "title": "Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation", "categories": ["cs.CR"], "comment": null, "summary": "The Optimism derivation pipeline is engineered for correctness and liveness,\nnot for succinct validity proofs. A straightforward port to a zkVM imposes\nsignificant overheads, making validity proofs significantly more costly than\nnecessary. We systematically identify inefficiencies in the current design,\nanalyze their impact on proving costs, and provide a soundness-preserving\nredesign tailored to zk proving. Our redesign achieves up to 6.5x faster\nderivation inside zkVMs (3.5x overall speedup) while maintaining identical\nsafety guarantees.", "AI": {"tldr": "论文针对Optimism派生管道在zkVM中的效率问题进行了重新设计，通过系统性识别低效环节并优化，实现了6.5倍的派生速度提升和3.5倍的整体加速，同时保持安全保证不变。", "motivation": "Optimism派生管道原本为正确性和活跃性设计，直接移植到zkVM会产生显著开销，导致有效性证明成本过高。", "method": "系统性识别当前设计中的低效环节，分析其对证明成本的影响，并提供保持正确性的zk证明优化重新设计。", "result": "重新设计在zkVM中实现了最高6.5倍的派生速度提升，整体加速达到3.5倍。", "conclusion": "通过针对zk证明的专门优化，可以在保持相同安全保证的前提下，显著降低Optimism派生管道的证明成本和提高效率。"}}
{"id": "2510.23274", "pdf": "https://arxiv.org/pdf/2510.23274", "abs": "https://arxiv.org/abs/2510.23274", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Shunpu Tang", "Zhiguo Shi", "Shui Yu"], "title": "Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy", "categories": ["cs.CR", "eess.IV"], "comment": null, "summary": "While semantic communication (SemCom) improves transmission efficiency by\nfocusing on task-relevant information, it also raises critical privacy\nconcerns. Many existing secure SemCom approaches rely on restrictive or\nimpractical assumptions, such as favorable channel conditions for the\nlegitimate user or prior knowledge of the eavesdropper's model. To address\nthese limitations, this paper proposes a novel secure SemCom framework for\nimage transmission over wiretap channels, leveraging differential privacy (DP)\nto provide approximate privacy guarantees. Specifically, our approach first\nextracts disentangled semantic representations from source images using\ngenerative adversarial network (GAN) inversion method, and then selectively\nperturbs private semantic representations with approximate DP noise. Distinct\nfrom conventional DP-based protection methods, we introduce DP noise with\nlearnable pattern, instead of traditional white Gaussian or Laplace noise,\nachieved through adversarial training of neural networks (NNs). This design\nmitigates the inherent non-invertibility of DP while effectively protecting\nprivate information. Moreover, it enables explicitly controllable security\nlevels by adjusting the privacy budget according to specific security\nrequirements, which is not achieved in most existing secure SemCom approaches.\nExperimental results demonstrate that, compared with the previous DP-based\nmethod and direct transmission, the proposed method significantly degrades the\nreconstruction quality for the eavesdropper, while introducing only slight\ndegradation in task performance. Under comparable security levels, our approach\nachieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86\nfor the legitimate user compared with the previous DP-based method.", "AI": {"tldr": "本文提出了一种基于差分隐私的新型安全语义通信框架，通过可学习模式的DP噪声保护图像传输中的隐私信息，在保证合法用户任务性能的同时有效降低窃听者的重建质量。", "motivation": "现有安全语义通信方法依赖于限制性假设（如良好信道条件或窃听者模型先验知识），存在实际应用局限性，需要提供近似隐私保证的解决方案。", "method": "使用GAN反演方法提取解耦的语义表示，选择性地对私有语义表示添加可学习模式的差分隐私噪声（通过神经网络对抗训练实现），而非传统高斯或拉普拉斯噪声。", "result": "实验表明，相比传统DP方法和直接传输，该方法显著降低窃听者重建质量（LPIPS优势0.06-0.29，FPPSR优势0.10-0.86），同时仅对任务性能造成轻微影响。", "conclusion": "该方法通过可学习模式DP噪声解决了传统DP不可逆性问题，实现了明确可控的安全级别调节，为语义通信提供了有效的隐私保护方案。"}}
{"id": "2510.23313", "pdf": "https://arxiv.org/pdf/2510.23313", "abs": "https://arxiv.org/abs/2510.23313", "authors": ["Yaokai Feng", "Kouichi Sakurai"], "title": "Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks", "categories": ["cs.CR"], "comment": "28 pages,1 figure, 204 references", "summary": "This survey systematizes the evolution of network intrusion detection systems\n(NIDS), from conventional methods such as signature-based and neural network\n(NN)-based approaches to recent integrations with large language models (LLMs).\nIt clearly and concisely summarizes the current status, strengths, and\nlimitations of conventional techniques, and explores the practical benefits of\nintegrating LLMs into NIDS. Recent research on the application of LLMs to NIDS\nin diverse environments is reviewed, including conventional network\ninfrastructures, autonomous vehicle environments and IoT environments.\n  From this survey, readers will learn that: 1) the earliest methods,\nsignature-based IDSs, continue to make significant contributions to modern\nsystems, despite their well-known weaknesses; 2) NN-based detection, although\nconsidered promising and under development for more than two decades, and\ndespite numerous related approaches, still faces significant challenges in\npractical deployment; 3) LLMs are useful for NIDS in many cases, and a number\nof related approaches have been proposed; however, they still face significant\nchallenges in practical applications. Moreover, they can even be exploited as\noffensive tools, such as for generating malware, crafting phishing messages, or\nlaunching cyberattacks. Recently, several studies have been proposed to address\nthese challenges, which are also reviewed in this survey; and 4) strategies for\nconstructing domain-specific LLMs have been proposed and are outlined in this\nsurvey, as it is nearly impossible to train a NIDS-specific LLM from scratch.", "AI": {"tldr": "这篇综述系统梳理了网络入侵检测系统(NIDS)从传统方法到LLM集成的演变历程，总结了各种技术的现状、优势和局限，并探讨了LLM在NIDS中的实际应用价值。", "motivation": "随着网络威胁日益复杂，传统NIDS方法面临挑战，需要探索LLM等新技术在入侵检测中的应用潜力和实际可行性。", "method": "通过文献综述方法，系统分析从基于签名的传统方法、神经网络方法到LLM集成方法的发展历程，涵盖不同网络环境下的应用研究。", "result": "研究发现：1)基于签名的方法仍有重要价值；2)神经网络方法虽发展多年但实际部署仍面临挑战；3)LLM在NIDS中具有应用潜力但存在安全风险；4)需要构建领域特定的LLM模型。", "conclusion": "LLM为NIDS带来新的可能性，但仍需解决实际部署挑战和安全风险，未来应重点发展领域特定的LLM解决方案。"}}
{"id": "2510.23457", "pdf": "https://arxiv.org/pdf/2510.23457", "abs": "https://arxiv.org/abs/2510.23457", "authors": ["Saleh Darzi", "Mirza Masfiqur Rahman", "Imtiaz Karim", "Rouzbeh Behnia", "Attila A Yavuz", "Elisa Bertino"], "title": "Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era", "categories": ["cs.CR"], "comment": "17 pages, 3 tables, 6 figures", "summary": "The 5G protocol lacks a robust base station authentication mechanism during\nthe initial bootstrapping phase, leaving it susceptible to threats such as fake\nbase station attacks. Conventional solutions, including digital signatures\nbased on Public Key Infrastructures (PKIs) and identity-based signatures, are\ninadequate against quantum-capable adversaries. While integrating NIST's\nPost-Quantum Cryptography (PQC) standards is a leading approach for quantum\nresistance, their suitability for 5G base station authentication remains\nunexplored. Moreover, current solutions are predominantly centralized and lack\nsecurity features such as distributed authentication. This work presents, to\nour knowledge, the first comprehensive network-level performance\ncharacterization of integrating NIST-PQC standards and conventional digital\nsignatures (including threshold and identity-based schemes) into 5G base\nstation authentication. Our findings reveal significant feasibility concerns,\nwith direct PQC adoption hindered by protocol constraints and large signature\nsizes. We also highlight the performance limitations of conventional methods\ndue to the overhead of certificate chains. To mitigate these challenges, we\npropose BORG, a transitional authentication solution based on a Hierarchical\nIdentity-Based Threshold Signature scheme with a Fail-Stop property. BORG\noffers post-mortem post-quantum forgery detection and distributed trust via\nthreshold and compact signatures, well-suited for 5G's stringent requirements.\nOur performance analysis underscores an important warning on the infeasibility\nof direct PQC integration and positions BORG as an effective transitional\nsolution toward future quantum-resilient 5G authentication.", "AI": {"tldr": "本文分析了将后量子密码标准集成到5G基站认证中的可行性问题，发现直接采用存在协议约束和大签名尺寸的限制，提出了基于分层身份基阈值签名的过渡解决方案BORG", "motivation": "5G协议在初始引导阶段缺乏强大的基站认证机制，容易受到伪基站攻击，传统解决方案无法抵御量子攻击，需要研究后量子密码标准在5G认证中的适用性", "method": "对NIST后量子密码标准和传统数字签名方案进行全面的网络级性能特征分析，提出基于分层身份基阈值签名方案的BORG认证解决方案", "result": "发现直接采用PQC存在显著可行性问题，传统方法因证书链开销存在性能限制，BORG方案提供了后量子伪造检测和分布式信任机制", "conclusion": "直接PQC集成不可行，BORG作为过渡解决方案能有效满足5G严格需求，为未来量子弹性认证提供路径"}}
{"id": "2510.23483", "pdf": "https://arxiv.org/pdf/2510.23483", "abs": "https://arxiv.org/abs/2510.23483", "authors": ["Valentin Reyes Häusler", "Gabriel Ott", "Aruna Jayasena", "Andreas Peter"], "title": "Towards a Functionally Complete and Parameterizable TFHE Processor", "categories": ["cs.CR"], "comment": null, "summary": "Fully homomorphic encryption allows the evaluation of arbitrary functions on\nencrypted data. It can be leveraged to secure outsourced and multiparty\ncomputation. TFHE is a fast torus-based fully homomorphic encryption scheme\nthat allows both linear operations, as well as the evaluation of arbitrary\nnon-linear functions. It currently provides the fastest bootstrapping operation\nperformance of any other FHE scheme. Despite its fast performance, TFHE suffers\nfrom a considerably higher computational overhead for the evaluation of\nhomomorphic circuits. Computations in the encrypted domain are orders of\nmagnitude slower than their unencrypted equivalents. This bottleneck hinders\nthe widespread adoption of (T)FHE for the protection of sensitive data. While\nstate-of-the-art implementations focused on accelerating and outsourcing single\noperations, their scalability and practicality are constrained by high memory\nbandwidth costs. In order to overcome this, we propose an FPGA-based hardware\naccelerator for the evaluation of homomorphic circuits. Specifically, we design\na functionally complete TFHE processor for FPGA hardware capable of processing\ninstructions on the data completely on the FPGA. In order to achieve a higher\nthroughput from our TFHE processor, we implement an improved programmable\nbootstrapping module which outperforms the current state-of-the-art by 240\\% to\n480\\% more bootstrappings per second. Our efficient, compact, and scalable\ndesign lays the foundation for implementing complete FPGA-based TFHE processor\narchitectures.", "AI": {"tldr": "提出基于FPGA的TFHE全同态加密处理器硬件加速器，通过改进的可编程自举模块实现比现有技术快240%-480%的性能提升", "motivation": "TFHE全同态加密方案虽然自举操作速度快，但同态电路评估的计算开销高，加密域计算比未加密计算慢几个数量级，阻碍了(T)FHE在敏感数据保护中的广泛应用", "method": "设计基于FPGA的TFHE处理器硬件加速器，实现完全在FPGA上处理数据的指令，并实现改进的可编程自举模块", "result": "实现了比当前最先进技术快240%到480%的自举操作每秒处理量，设计高效、紧凑且可扩展", "conclusion": "该设计为实施完整的基于FPGA的TFHE处理器架构奠定了基础，有望克服全同态加密的计算性能瓶颈"}}
{"id": "2510.21720", "pdf": "https://arxiv.org/pdf/2510.21720", "abs": "https://arxiv.org/abs/2510.21720", "authors": ["Anant Pareek"], "title": "A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The confluence of Artificial Intelligence and Computational Psychology\npresents an opportunity to model, understand, and interact with complex human\npsychological states through computational means. This paper presents a\ncomprehensive, multi-faceted framework designed to bridge the gap between\nisolated predictive modeling and an interactive system for psychological\nanalysis. The methodology encompasses a rigorous, end-to-end development\nlifecycle. First, foundational performance benchmarks were established on four\ndiverse psychological datasets using classical machine learning techniques.\nSecond, state-of-the-art transformer models were fine-tuned, a process that\nnecessitated the development of effective solutions to overcome critical\nengineering challenges, including the resolution of numerical instability in\nregression tasks and the creation of a systematic workflow for conducting\nlarge-scale training under severe resource constraints. Third, a generative\nlarge language model (LLM) was fine-tuned using parameter-efficient techniques\nto function as an interactive \"Personality Brain.\" Finally, the entire suite of\npredictive and generative models was architected and deployed as a robust,\nscalable microservices ecosystem. Key findings include the successful\nstabilization of transformer-based regression models for affective computing,\nshowing meaningful predictive performance where standard approaches failed, and\nthe development of a replicable methodology for democratizing large-scale AI\nresearch. The significance of this work lies in its holistic approach,\ndemonstrating a complete research-to-deployment pipeline that integrates\npredictive analysis with generative dialogue, thereby providing a practical\nmodel for future research in computational psychology and human-AI interaction.", "AI": {"tldr": "论文提出了一个结合AI和计算心理学的多层面框架，通过端到端开发流程建立了从预测模型到交互式心理分析系统的完整解决方案，包括基准测试、Transformer微调、LLM开发以及微服务部署。", "motivation": "弥合孤立预测建模与交互式心理分析系统之间的差距，通过计算手段建模和理解复杂的人类心理状态。", "method": "1) 使用经典机器学习技术建立四个心理学数据集的基础性能基准；2) 微调最先进的Transformer模型，解决回归任务中的数值不稳定性和资源限制下的训练挑战；3) 使用参数高效技术微调生成式大语言模型作为交互式\"人格大脑\"；4) 将预测和生成模型架构部署为可扩展的微服务生态系统。", "result": "成功稳定了基于Transformer的情感计算回归模型，在标准方法失败的情况下显示出有意义的预测性能；开发了可复现的大规模AI研究方法论。", "conclusion": "该工作展示了从研究到部署的完整流程，整合了预测分析与生成对话，为计算心理学和人机交互的未来研究提供了实用模型。"}}
{"id": "2510.21721", "pdf": "https://arxiv.org/pdf/2510.21721", "abs": "https://arxiv.org/abs/2510.21721", "authors": ["Kentaro Ueda", "Takehiro Takayanagi"], "title": "PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "While recent advances in Large Language Models (LLMs) have improved the\nquality of creative text generation, significant challenges remain in producing\npersonalized stories that reflect individual user preferences. Conventional\napproaches rely on explicit feedback or fine-tuning, which presents practical\nissues regarding user burden, data collection, computational costs, and\nprivacy. In this work, we propose PREFINE (Persona-and-Rubric Guided\nCritique-and-Refine), a novel framework that extends the Critique-and-Refine\nparadigm to personalization. PREFINE constructs a pseudo-user agent from a\nuser's interaction history and generates user-specific rubrics (evaluation\ncriteria). By having this agent critique and refine outputs on the user's\nbehalf based on these tailored rubrics, our method achieves personalized\ngeneration without requiring parameter updates or direct user feedback. We\nconducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.\nWe designed three baseline methods and several model variants to verify the\ncontribution of each component of our framework. In automatic evaluations\n(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically\nsignificant scores than the baselines, without compromising general story\nquality. Analysis of the model variants confirmed that both the pseudo-user\nagent and the user-specific rubrics are crucial for enhancing personalization\nperformance. Beyond story generation, our approach holds potential for enabling\nefficient personalization in broader applications, such as dialogue systems,\neducation, and recommendation.", "AI": {"tldr": "PREFINE是一个无需参数更新或直接用户反馈的个性化故事生成框架，通过伪用户代理和用户特定评分标准实现个性化文本生成", "motivation": "现有LLM在个性化文本生成方面存在挑战，传统方法依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私问题", "method": "构建伪用户代理从用户交互历史中生成用户特定评分标准，通过该代理基于定制标准进行批判和精炼输出", "result": "在PerDOC和PerMPST数据集上评估显示，PREFINE在自动评估中获得更高胜率和统计显著分数，且不损害一般故事质量", "conclusion": "该方法在故事生成中有效，并具有扩展到对话系统、教育和推荐等更广泛应用的潜力"}}
{"id": "2510.21855", "pdf": "https://arxiv.org/pdf/2510.21855", "abs": "https://arxiv.org/abs/2510.21855", "authors": ["Ryan Zhang", "Herbert Woisetscläger"], "title": "SIGN: Schema-Induced Games for Naming", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2; I.2.7; I.2.11"], "comment": "AAAI 2026 Student Abstract (Oral). Code available ar\n  https://github.com/ryanzhangofficial/schema-induced-games-for-naming", "summary": "Real-world AI systems are tackling increasingly complex problems, often\nthrough interactions among large language model (LLM) agents. When these agents\ndevelop inconsistent conventions, coordination can break down. Applications\nsuch as collaborative coding and distributed planning therefore require\nreliable, consistent communication, and scalability is a central concern as\nsystems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming\ngame that examines how lightweight structure can steer convention formation. We\ncompare schema-induced communication to unconstrained natural language and find\nfaster convergence with up to 5.8x higher agreement. These results suggest that\nminimal structure can act as a simple control knob for efficient multi-agent\ncoordination, pointing toward broader applications beyond the naming game.", "AI": {"tldr": "SIGN方法通过引入轻量级结构指导多智能体命名游戏，相比无约束自然语言实现了5.8倍更快的收敛速度和更高的协议一致性", "motivation": "现实AI系统中多智能体交互时可能产生不一致的约定，导致协调失败，特别是在协作编码和分布式规划等需要可靠通信的应用中", "method": "引入Schema-Induced Games for Naming (SIGN)，一种通过轻量级结构指导约定形成的命名游戏方法", "result": "相比无约束自然语言通信，SIGN方法实现了更快的收敛速度，协议一致性最高提升5.8倍", "conclusion": "最小化结构可以作为多智能体协调的有效控制机制，该方法的应用潜力超出了命名游戏的范畴"}}
{"id": "2510.21866", "pdf": "https://arxiv.org/pdf/2510.21866", "abs": "https://arxiv.org/abs/2510.21866", "authors": ["Javier Marín"], "title": "Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks", "categories": ["cs.AI"], "comment": "The experiments in this paper were performed in January 2024. Current\n  model architectures are considerably more complex than those presented here", "summary": "We document empirical capability ceilings in decoder-only autoregressive\nlanguage models across knowledge-intensive tasks. Systematic evaluation of OPT\nand Pythia model families (70M-30B parameters, spanning 240 times scaling)\nreveals that knowledge retrieval tasks show negligible accuracy improvement\ndespite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains\nflat at 19-20% (below 25% random chance) across all scales while cross-entropy\nloss decreases by 31%. In contrast, procedural tasks like arithmetic show\nconventional scaling where both metrics improve together. Attention\nintervention experiments reveal high sensitivity to perturbation: swapping\nattention patterns between models causes catastrophic performance collapse\n(complete accuracy loss) rather than graceful degradation. These measurements\nhave immediate engineering implications: for knowledge-intensive applications\nusing OPT and Pythia architectures, parameter scaling beyond 1-2B offers\nminimal accuracy gains despite continued loss improvement. Our findings\nquantify capability-specific scaling failures in these model families to inform\nresource allocation decisions. Whether these patterns reflect fundamental\nconstraints of decoder-only architectures or implementation-specific\nlimitations remains an open question requiring investigation across diverse\narchitectural approaches.", "AI": {"tldr": "研究发现解码器自回归语言模型在知识密集型任务中存在能力上限，参数规模扩大无法显著提升准确率，尽管损失函数持续下降。", "motivation": "探究解码器自回归语言模型在不同规模下对知识密集型任务的能力表现，以及参数缩放对性能的影响。", "method": "系统评估OPT和Pythia模型家族（70M-30B参数），分析损失函数和准确率的关系，并进行注意力干预实验。", "result": "知识检索任务准确率几乎无改善，数学基准测试准确率稳定在19-20%，而算术等程序性任务则呈现常规缩放模式。注意力扰动导致性能灾难性崩溃。", "conclusion": "对于使用OPT和Pythia架构的知识密集型应用，超过1-2B参数的缩放几乎无法带来准确率提升，这为资源分配决策提供了量化依据。"}}
{"id": "2510.21762", "pdf": "https://arxiv.org/pdf/2510.21762", "abs": "https://arxiv.org/abs/2510.21762", "authors": ["Eric Jeangirard"], "title": "A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "We present a dataset of 833k paragraphs extracted from CC-BY licensed\nscientific publications, classified into four categories: acknowledgments, data\nmentions, software/code mentions, and clinical trial mentions. The paragraphs\nare primarily in English and French, with additional European languages\nrepresented. Each paragraph is annotated with language identification (using\nfastText) and scientific domain (from OpenAlex). This dataset, derived from the\nFrench Open Science Monitor corpus and processed using GROBID, enables training\nof text classification models and development of named entity recognition\nsystems for scientific literature mining. The dataset is publicly available on\nHuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.", "AI": {"tldr": "该论文介绍了一个包含83.3万个段落的科学文献数据集，这些段落被分类为致谢、数据提及、软件/代码提及和临床试验提及四个类别，主要用于训练文本分类模型和命名实体识别系统。", "motivation": "为科学文献挖掘提供高质量的标注数据集，支持文本分类和命名实体识别模型的训练，促进科学文献的自动化处理和分析。", "method": "从CC-BY许可的科学出版物中提取段落，使用fastText进行语言识别，OpenAlex进行科学领域标注，通过GROBID处理法国开放科学监测器语料库。", "result": "创建了一个包含833k段落的多样化数据集，涵盖多种语言和科学领域，并在HuggingFace平台上公开提供。", "conclusion": "该数据集为科学文献挖掘研究提供了有价值的资源，支持后续的文本分类和实体识别任务，具有广泛的应用潜力。"}}
{"id": "2510.21881", "pdf": "https://arxiv.org/pdf/2510.21881", "abs": "https://arxiv.org/abs/2510.21881", "authors": ["Nannan Shi", "Chuanyu Qin", "Shipeng Song", "Man Luo"], "title": "GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities\nin text-based mathematical problem solving; however, when adapted to visual\nreasoning tasks, particularly geometric problem solving, their performance\nsubstantially declines because geometric problems present unique challenges.\nSpecifically, these challenges stem from two key factors: first, the intrinsic\ncomplexity of geometry requiring detailed image comprehension and multi-step\nreasoning, and second, the limitations of existing datasets which lack\nsufficient scale, diversity, and explicit reasoning traces, consequently\nhindering effective model training. To address these challenges, we developed\nthe GeoThoughts dataset, a comprehensive geometric reasoning corpus with two\nsubsets: Geo-Thought-6K with 6,243 samples and its augmented version\nGeo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual\ndescriptions, step-by-step solutions, explicit reasoning chains, reflection\nsteps, and final answers. Using this dataset, we developed GeoThought-MLLM, a\nmathematical reasoning multimodal model that generates detailed thinking\nprocesses during problem-solving. Our model outperforms existing benchmarks in\ngeometric tasks, demonstrating that training with our Chain-of-Thought dataset\nimproves geometric reasoning capabilities across both in-domain and\nout-of-domain settings. Finally, we analyze failure cases and observe that\nerrors primarily arise from incorrect interpretation of mathematical concepts\nor spatial misjudgment. By invoking CoT to correct these mistakes, the model\nproduces correct answers.", "AI": {"tldr": "该论文针对大语言模型在几何视觉推理任务中表现不佳的问题，开发了包含详细推理链的GeoThoughts数据集，并基于此训练了GeoThought-MLLM模型，显著提升了几何推理能力。", "motivation": "大语言模型在文本数学推理上表现良好，但在几何视觉推理任务中性能显著下降，主要因为几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集缺乏规模性、多样性和显式推理轨迹。", "method": "开发GeoThoughts数据集（包含6,243样本的Geo-Thought-6K和10,834样本的Geo-Thought-Augmented-10K），每个样本包含视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此数据集训练GeoThought-MLLM多模态模型。", "result": "GeoThought-MLLM模型在几何任务中优于现有基准测试，表明使用Chain-of-Thought数据集训练能提升模型在域内和域外设置下的几何推理能力。", "conclusion": "通过分析错误案例发现，错误主要源于数学概念误解或空间判断错误，通过调用思维链（CoT）纠正这些错误后，模型能产生正确答案，验证了方法的有效性。"}}
{"id": "2510.21853", "pdf": "https://arxiv.org/pdf/2510.21853", "abs": "https://arxiv.org/abs/2510.21853", "authors": ["Debdeep Sanyal", "Aakash Sen Sharma", "Dhruv Kumar", "Saurabh Deshpande", "Murari Mandal"], "title": "Policy Optimization Prefers The Path of Least Resistance", "categories": ["cs.CL"], "comment": "21 pages, 8 figures, 2 tables", "summary": "Policy optimization (PO) algorithms are used to refine Large Language Models\nfor complex, multi-step reasoning. Current state-of-the-art pipelines enforce a\nstrict think-then-answer format to elicit chain-of-thought (CoT); however, the\nbehavior of PO when these rigid constraints are relaxed into an open-ended CoT\nstructure remains an under-studied question. We investigate this gap with an\nextensive suite of controlled experiments and identify a consistent principle:\n\\textit{policy optimization consistently follows the path of least resistance}.\nWhen afforded the flexibility to interleave reasoning and response, policy\noptimization consistently learns to discard explicit reasoning, causing the\npolicy to degenerate to a direct \\texttt{<answer>}-only format. This outcome\nholds true across various models and algorithms. We find that this collapse in\nformat is persistent even when the complex \\texttt{<think><answer>} format is\nassigned up to 4x larger reward weights. We formalize this principle through a\nseries of controlled reward decomposition experiments, demonstrating a clear\nhierarchy: PO systematically optimizes for the simplest reward component first,\na preference that holds even when faced with mutually exclusive choices or\nstrong incentives for more complex behaviors. Finally, we show that successful\nconvergence on the high-reward shortcut is not a low-effort drift but is driven\nby the optimization process that requires the KL-regularized policy to have\nsufficient freedom to make a significant shift from its initial prior. Our\nfindings reveal that granting policies the freedom to diverge is a double-edged\nsword: while necessary for discovering high-reward shortcuts, it also creates a\npowerful incentive to game the simplest aspects of the reward function, posing\na critical challenge for reward hacking under alignment.", "AI": {"tldr": "研究发现策略优化在开放式思维链结构中倾向于选择最简单路径，即使给复杂格式更高奖励权重，模型仍会丢弃显式推理而退化到仅输出答案的格式，揭示了奖励破解的关键挑战。", "motivation": "当前策略优化算法在严格思维链格式下表现良好，但在开放式思维链结构中的行为尚未充分研究，需要探索当放松格式约束时策略优化的行为模式。", "method": "通过一系列受控实验和奖励分解实验，分析不同模型和算法在开放式思维链结构中的行为，研究KL正则化策略的优化过程。", "result": "策略优化始终遵循最小阻力路径，即使复杂格式奖励权重提高4倍，模型仍会丢弃推理步骤；优化过程系统性地优先优化最简单的奖励组件。", "conclusion": "给予策略发散自由是一把双刃剑：虽然能发现高奖励捷径，但也导致模型倾向于利用奖励函数最简单部分，这对对齐中的奖励破解提出了关键挑战。"}}
{"id": "2510.21886", "pdf": "https://arxiv.org/pdf/2510.21886", "abs": "https://arxiv.org/abs/2510.21886", "authors": ["Mark Phillip Matovic"], "title": "Exploration through Generation: Applying GFlowNets to Structured Search", "categories": ["cs.AI"], "comment": "12 pages", "summary": "This work applies Generative Flow Networks (GFlowNets) to three graph\noptimization problems: the Traveling Salesperson Problem, Minimum Spanning\nTree, and Shortest Path. GFlowNets are generative models that learn to sample\nsolutions proportionally to a reward function. The models are trained using the\nTrajectory Balance loss to build solutions sequentially, selecting edges for\nspanning trees, nodes for paths, and cities for tours. Experiments on benchmark\ninstances of varying sizes show that GFlowNets learn to find optimal solutions.\nFor each problem type, multiple graph configurations with different numbers of\nnodes were tested. The generated solutions match those from classical\nalgorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact\nsolvers for TSP). Training convergence depends on problem complexity, with the\nnumber of episodes required for loss stabilization increasing as graph size\ngrows. Once training converges, the generated solutions match known optima from\nclassical algorithms across the tested instances. This work demonstrates that\ngenerative models can solve combinatorial optimization problems through learned\npolicies. The main advantage of this learning-based approach is computational\nscalability: while classical algorithms have fixed complexity per instance,\nGFlowNets amortize computation through training. With sufficient computational\nresources, the framework could potentially scale to larger problem instances\nwhere classical exact methods become infeasible.", "AI": {"tldr": "本研究将生成流网络(GFlowNets)应用于三个图优化问题：旅行商问题、最小生成树和最短路径问题，通过训练学习采样与奖励函数成比例的解决方案，在基准测试中能够找到最优解。", "motivation": "探索生成模型在组合优化问题中的应用，特别是利用GFlowNets的学习策略来解决传统算法复杂度固定的问题，实现计算的可扩展性。", "method": "使用轨迹平衡损失训练GFlowNets，顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。在不同节点数量的多种图配置上进行测试。", "result": "生成的解决方案与经典算法（Dijkstra最短路径、Kruskal生成树、TSP精确求解器）的结果匹配。训练收敛性取决于问题复杂度，随着图规模增大，需要的训练回合数增加。", "conclusion": "生成模型可以通过学习策略解决组合优化问题，主要优势是计算可扩展性。GFlowNets通过训练分摊计算，在足够计算资源下可扩展到经典精确方法不可行的大规模问题实例。"}}
{"id": "2510.21883", "pdf": "https://arxiv.org/pdf/2510.21883", "abs": "https://arxiv.org/abs/2510.21883", "authors": ["Chenheng Zhang", "Tianqi Du", "Jizhe Zhang", "Mingqing Xiao", "Yifei Wang", "Yisen Wang", "Zhouchen Lin"], "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Conventional research on large language models (LLMs) has primarily focused\non refining output distributions, while paying less attention to the decoding\nprocess that transforms these distributions into final responses. Recent\nadvances, such as scaling the computation of inference time with reward models,\nhave underscored the importance of decoding, but these methods often suffer\nfrom high computational costs and limited applicability. In this paper, we\nrevisit LLM generation through the lens of recommender systems, conceptualizing\nthe decoding process as analogous to the ranking stage in recommendation\npipelines. From this perspective, we observe that both traditional decoding\nmethods and reward models exhibit clear limitations such as redundancy.\nMotivated by this insight, we propose Language Ranker, a novel framework that\nintroduces a lightweight module to rerank candidate responses using features\nextracted by the base model. Experiments across a wide range of tasks show that\nLanguage Ranker achieves performance comparable to large-scale reward models,\nwhile requiring only <0.5M additional parameters, significantly reducing the\ncomputational overhead during both training and inference stages. This\nhighlights the efficiency and effectiveness of our method, showcasing its\npotential to fully unlock the capabilities of LLMs.", "AI": {"tldr": "论文提出Language Ranker框架，将LLM解码过程重新概念化为推荐系统中的排序阶段，通过轻量级重排模块提升响应质量，在保持性能的同时大幅降低计算成本。", "motivation": "传统LLM研究主要关注输出分布优化，而忽略了解码过程的重要性。现有方法如奖励模型计算成本高且适用性有限，存在冗余等问题。", "method": "借鉴推荐系统思想，将解码过程类比为排序阶段，提出Language Ranker框架，使用基础模型提取的特征通过轻量级模块对候选响应进行重排序。", "result": "在多种任务上的实验表明，该方法仅需增加<0.5M参数就能达到与大规模奖励模型相当的性能，显著降低了训练和推理阶段的计算开销。", "conclusion": "该方法高效且有效，有潜力充分释放LLM的能力，为解码过程优化提供了新的思路。"}}
{"id": "2510.21888", "pdf": "https://arxiv.org/pdf/2510.21888", "abs": "https://arxiv.org/abs/2510.21888", "authors": ["Shayan Karimi", "Xiaoqi Tan"], "title": "Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability", "categories": ["cs.AI", "cs.CC", "cs.LG", "68Q17 (Primary) 68T05, 68T42 (Secondary)", "F.2.2; I.2.6; I.2.8"], "comment": "to be published in NeurIPS 2025", "summary": "This paper investigates the computational complexity of reinforcement\nlearning in a novel linear function approximation regime, termed partial\n$q^{\\pi}$-realizability. In this framework, the objective is to learn an\n$\\epsilon$-optimal policy with respect to a predefined policy set $\\Pi$, under\nthe assumption that all value functions for policies in $\\Pi$ are linearly\nrealizable. The assumptions of this framework are weaker than those in\n$q^{\\pi}$-realizability but stronger than those in $q^*$-realizability,\nproviding a practical model where function approximation naturally arises. We\nprove that learning an $\\epsilon$-optimal policy in this setting is\ncomputationally hard. Specifically, we establish NP-hardness under a\nparameterized greedy policy set (argmax) and show that - unless NP = RP - an\nexponential lower bound (in feature vector dimension) holds when the policy set\ncontains softmax policies, under the Randomized Exponential Time Hypothesis.\nOur hardness results mirror those in $q^*$-realizability and suggest\ncomputational difficulty persists even when $\\Pi$ is expanded beyond the\noptimal policy. To establish this, we reduce from two complexity problems,\n$\\delta$-Max-3SAT and $\\delta$-Max-3SAT(b), to instances of GLinear-$\\kappa$-RL\n(greedy policy) and SLinear-$\\kappa$-RL (softmax policy). Our findings indicate\nthat positive computational results are generally unattainable in partial\n$q^{\\pi}$-realizability, in contrast to $q^{\\pi}$-realizability under a\ngenerative access model.", "AI": {"tldr": "该论文研究了在部分q^π-可实现性框架下强化学习的计算复杂性，证明了在该设置下学习ε-最优策略是计算困难的，包括NP-hardness和指数级下界。", "motivation": "研究在弱于q^π-可实现性但强于q*-可实现性的线性函数逼近框架下的计算复杂性，这是一个函数逼近自然出现的实用模型。", "method": "通过从δ-Max-3SAT和δ-Max-3SAT(b)问题归约到GLinear-κ-RL（贪婪策略）和SLinear-κ-RL（softmax策略）实例来建立计算复杂性结果。", "result": "证明了在参数化贪婪策略集下是NP-hard的，在softmax策略集下存在指数级下界（除非NP=RP），表明即使扩展策略集超越最优策略，计算困难仍然存在。", "conclusion": "在部分q^π-可实现性框架下，通常无法获得积极的计算结果，这与生成访问模型下的q^π-可实现性形成对比。"}}
{"id": "2510.21884", "pdf": "https://arxiv.org/pdf/2510.21884", "abs": "https://arxiv.org/abs/2510.21884", "authors": ["Avinash Patil"], "title": "Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "12 Pages, 12 Figures, 2 tables", "summary": "The growing adoption of machine learning (ML) in sensitive domains has\nheightened the demand for transparent and interpretable artificial\nintelligence. Large Language Models (LLMs) are increasingly capable of\nproducing natural language explanations, yet it remains unclear whether these\nrationales faithfully capture the predictive signals that underlie decisions.\nThis paper introduces RACE-Reasoning Alignment for Completeness of\nExplanations, a systematic framework to evaluate the alignment between\nLLM-generated explanations and interpretable feature importance scores derived\nfrom a logistic regression baseline. We analyze four widely used text\nclassification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and\ncompare LLM rationales against top-ranked supporting and contradicting lexical\nfeatures. To capture alignment at multiple levels of granularity, RACE\nimplements token-aware, exact string, and edit-distance matching techniques.\nEmpirical results reveal a consistent asymmetry: correct predictions exhibit\nhigher coverage of supporting features, while incorrect predictions are\nassociated with elevated coverage of contradicting features. Edit-distance\nmatching further uncovers paraphrastic overlaps, boosting coverage while\npreserving this asymmetry. These findings demonstrate that LLM rationales\ncombine both surface-level and flexible evidence reuse, yet can also amplify\nmisleading cues in error cases. RACE provides new insights into the\nfaithfulness of LLM explanations and establishes a quantitative basis for\nevaluating reasoning completeness in neural language models.", "AI": {"tldr": "本文提出RACE框架，通过比较LLM生成解释与逻辑回归特征重要性评分的一致性，评估LLM解释的忠实度和完整性，发现在正确预测中支持特征覆盖率更高，错误预测中矛盾特征覆盖率更高。", "motivation": "随着机器学习在敏感领域的应用增加，对透明可解释AI的需求日益增长。虽然LLM能够生成自然语言解释，但这些解释是否真实反映预测信号尚不明确。", "method": "提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归基线的特征重要性评分，使用词元感知、精确字符串和编辑距离匹配技术从多粒度层面捕捉对齐关系。", "result": "实证结果显示一致的不对称性：正确预测显示更高的支持特征覆盖率，而错误预测与更高的矛盾特征覆盖率相关。编辑距离匹配发现释义重叠，在保持不对称性的同时提高了覆盖率。", "conclusion": "LLM解释结合了表面层面和灵活的证据重用，但在错误情况下也会放大误导性线索。RACE为评估LLM解释的忠实度提供了新见解，并为评估神经语言模型的推理完整性建立了量化基础。"}}
{"id": "2510.21970", "pdf": "https://arxiv.org/pdf/2510.21970", "abs": "https://arxiv.org/abs/2510.21970", "authors": ["Josip Tomo Licardo", "Nikola Tankovic"], "title": "Performance Trade-offs of Optimizing Small Language Models for E-Commerce", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 9 figures", "summary": "Large Language Models (LLMs) offer state-of-the-art performance in natural\nlanguage understanding and generation tasks. However, the deployment of leading\ncommercial models for specialized tasks, such as e-commerce, is often hindered\nby high computational costs, latency, and operational expenses. This paper\ninvestigates the viability of smaller, open-weight models as a\nresource-efficient alternative. We present a methodology for optimizing a\none-billion-parameter Llama 3.2 model for multilingual e-commerce intent\nrecognition. The model was fine-tuned using Quantized Low-Rank Adaptation\n(QLoRA) on a synthetically generated dataset designed to mimic real-world user\nqueries. Subsequently, we applied post-training quantization techniques,\ncreating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results\ndemonstrate that the specialized 1B model achieves 99% accuracy, matching the\nperformance of the significantly larger GPT-4.1 model. A detailed performance\nanalysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ\nreduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older\nGPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF\nformats on a CPU achieved a speedup of up to 18x in inference throughput and a\nreduction of over 90% in RAM consumption compared to the FP16 baseline. We\nconclude that small, properly optimized open-weight models are not just a\nviable but a more suitable alternative for domain-specific applications,\noffering state-of-the-art accuracy at a fraction of the computational cost.", "AI": {"tldr": "本文研究小型开源模型在电商意图识别任务中的可行性，通过QLoRA微调和后训练量化技术，使10亿参数的Llama 3.2模型达到与GPT-4.1相当的99%准确率，同时大幅降低计算资源需求。", "motivation": "商用大型语言模型在电商等专业领域部署时面临高计算成本、延迟和运营费用的问题，需要寻找资源效率更高的替代方案。", "method": "使用量化低秩适应(QLoRA)在合成的多语言电商查询数据集上微调10亿参数Llama 3.2模型，然后应用后训练量化技术创建GPU优化(GPTQ)和CPU优化(GGUF)版本。", "result": "专用1B模型达到99%准确率，与GPT-4.1性能相当。GPTQ减少41%显存使用但推理速度下降82%，GGUF在CPU上实现18倍推理吞吐量提升和90%以上内存消耗减少。", "conclusion": "经过适当优化的小型开源模型是领域特定应用的更合适替代方案，能以极低的计算成本实现最先进的准确性。"}}
{"id": "2510.21885", "pdf": "https://arxiv.org/pdf/2510.21885", "abs": "https://arxiv.org/abs/2510.21885", "authors": ["Anh Pham", "Mihir Thalanki", "Michael Sun", "Aditya Chaloo", "Ankita Gupta", "Tian Xia", "Aditya Mate", "Ehimwenma Nosakhare", "Soundararajan Srinivasan"], "title": "Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models often lose previously aligned safety behaviors when\nfine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior\nwork shows that adding random safety examples can mitigate this effect, but it\nremains unclear which examples are most effective. We propose a behavior-aware\nsampling framework that selects safety examples based on two complementary\nfactors: instruction-response behavior (e.g., refusal versus compliance) and\nsemantic diversity across harm categories. Systematic evaluation shows that\nthis approach substantially reduces harmful outputs while maintaining\nhelpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%\nadditional training data. These results highlight how targeted data selection\ncan improve the safety and efficiency of fine-tuning at scale.", "AI": {"tldr": "提出了一个行为感知采样框架，通过基于指令-响应行为和语义多样性选择安全示例，有效减少大语言模型在微调时的安全行为遗忘问题，仅需0.5%额外训练数据即可减少41%有害输出。", "motivation": "大语言模型在良性数据微调时经常出现安全行为遗忘现象，现有方法虽然可以通过添加随机安全示例缓解，但哪些示例最有效仍不清楚。", "method": "提出行为感知采样框架，基于两个互补因素选择安全示例：指令-响应行为（拒绝vs服从）和跨危害类别的语义多样性。", "result": "系统评估显示该方法显著减少有害输出同时保持帮助性，仅用0.5%额外训练数据就实现了高达41%的有害性减少。", "conclusion": "定向数据选择可以显著提高大规模微调的安全性和效率，为模型安全优化提供了有效解决方案。"}}
{"id": "2510.21977", "pdf": "https://arxiv.org/pdf/2510.21977", "abs": "https://arxiv.org/abs/2510.21977", "authors": ["Ji Huang", "Mengfei Li", "Shuai Shao"], "title": "Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer a promising way to simulate human survey\nresponses, potentially reducing the cost of large-scale data collection.\nHowever, existing zero-shot methods suffer from prompt sensitivity and low\naccuracy, while conventional fine-tuning approaches mostly fit the training set\ndistributions and struggle to produce results more accurate than the training\nset itself, which deviates from the original goal of using LLMs to simulate\nsurvey responses. Building on this observation, we introduce Distribution Shift\nAlignment (DSA), a two-stage fine-tuning method that aligns both the output\ndistributions and the distribution shifts across different backgrounds. By\nlearning how these distributions change rather than fitting training data, DSA\ncan provide results substantially closer to the true distribution than the\ntraining data. Empirically, DSA consistently outperforms other methods on five\npublic survey datasets. We further conduct a comprehensive comparison covering\naccuracy, robustness, and data savings. DSA reduces the required real data by\n53.48-69.12%, demonstrating its effectiveness and efficiency in survey\nsimulation.", "AI": {"tldr": "提出DSA方法解决LLM模拟调查响应时的分布偏差问题，通过两阶段微调学习分布变化而非拟合训练数据，显著减少真实数据需求并提高准确性", "motivation": "现有零样本方法存在提示敏感性和低准确性问题，传统微调方法容易过拟合训练集分布，无法实现比训练集更准确的模拟结果", "method": "Distribution Shift Alignment (DSA)两阶段微调方法，对齐输出分布和不同背景下的分布变化，学习分布变化规律而非单纯拟合训练数据", "result": "在五个公共调查数据集上持续优于其他方法，将真实数据需求减少53.48-69.12%，在准确性、鲁棒性和数据节省方面表现优异", "conclusion": "DSA方法能提供比训练数据更接近真实分布的结果，证明了在调查模拟中的有效性和效率，为大规模数据收集提供了更优解决方案"}}
{"id": "2510.21891", "pdf": "https://arxiv.org/pdf/2510.21891", "abs": "https://arxiv.org/abs/2510.21891", "authors": ["Dhrupad Bhardwaj", "Julia Kempe", "Tim G. J. Rudner"], "title": "Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "To deploy large language models (LLMs) in high-stakes application domains\nthat require substantively accurate responses to open-ended prompts, we need\nreliable, computationally inexpensive methods that assess the trustworthiness\nof long-form responses generated by LLMs. However, existing approaches often\nrely on claim-by-claim fact-checking, which is computationally expensive and\nbrittle in long-form responses to open-ended prompts. In this work, we\nintroduce semantic isotropy -- the degree of uniformity across normalized text\nembeddings on the unit sphere -- and use it to assess the trustworthiness of\nlong-form responses generated by LLMs. To do so, we generate several long-form\nresponses, embed them, and estimate the level of semantic isotropy of these\nresponses as the angular dispersion of the embeddings on the unit sphere. We\nfind that higher semantic isotropy -- that is, greater embedding dispersion --\nreliably signals lower factual consistency across samples. Our approach\nrequires no labeled data, no fine-tuning, and no hyperparameter selection, and\ncan be used with open- or closed-weight embedding models. Across multiple\ndomains, our method consistently outperforms existing approaches in predicting\nnonfactuality in long-form responses using only a handful of samples --\noffering a practical, low-cost approach for integrating trust assessment into\nreal-world LLM workflows.", "AI": {"tldr": "本文提出了一种基于语义各向同性（semantic isotropy）的低成本方法来评估大语言模型长文本响应的可信度，通过计算文本嵌入在单位球面上的角度离散度来预测事实一致性，无需标注数据或微调。", "motivation": "在需要准确回答开放式提示的高风险应用领域部署大语言模型时，需要可靠且计算成本低廉的方法来评估长文本响应的可信度，而现有的逐项事实核查方法计算昂贵且脆弱。", "method": "通过生成多个长文本响应，将其嵌入到单位球面上，并计算这些嵌入的角度离散度（语义各向同性程度）来评估响应的事实一致性。", "result": "研究发现较高的语义各向同性（即更大的嵌入离散度）可靠地表明样本间的事实一致性较低。该方法在多个领域都优于现有方法，仅需少量样本即可预测非事实性。", "conclusion": "该方法提供了一种实用、低成本的信任评估方法，可以集成到实际的大语言模型工作流程中，无需标注数据、微调或超参数选择，适用于开放或封闭权重的嵌入模型。"}}
{"id": "2510.21999", "pdf": "https://arxiv.org/pdf/2510.21999", "abs": "https://arxiv.org/abs/2510.21999", "authors": ["Zhenya Huang", "Jiayu Liu", "Xin Lin", "Zhiyuan Ma", "Shangzi Xue", "Tong Xiao", "Qi Liu", "Yee Whye Teh", "Enhong Chen"], "title": "Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective", "categories": ["cs.AI"], "comment": null, "summary": "Math word problem (MWP) serves as a fundamental research topic in artificial\nintelligence (AI) dating back to 1960s. This research aims to advance the\nreasoning abilities of AI by mirroring the human-like cognitive intelligence.\nThe mainstream technological paradigm has evolved from the early rule-based\nmethods, to deep learning models, and is rapidly advancing towards large\nlanguage models. However, the field still lacks a systematic taxonomy for the\nMWP survey along with a discussion of current development trends. Therefore, in\nthis paper, we aim to comprehensively review related research in MWP solving\nthrough the lens of human cognition, to demonstrate how recent AI models are\nadvancing in simulating human cognitive abilities. Specifically, we summarize 5\ncrucial cognitive abilities for MWP solving, including Problem Understanding,\nLogical Organization, Associative Memory, Critical Thinking, and Knowledge\nLearning. Focused on these abilities, we review two mainstream MWP models in\nrecent 10 years: neural network solvers, and LLM based solvers, and discuss the\ncore human-like abilities they demonstrated in their intricate problem-solving\nprocess. Moreover, we rerun all the representative MWP solvers and supplement\ntheir performance on 5 mainstream benchmarks for a unified comparison. To the\nbest of our knowledge, this survey first comprehensively analyzes the\ninfluential MWP research of the past decade from the perspective of human\nreasoning cognition and provides an integrative overall comparison across\nexisting approaches. We hope it can inspire further research in AI reasoning.\nOur repository is released on https://github.com/Ljyustc/FoI-MWP.", "AI": {"tldr": "这篇论文对数学应用题(MWP)求解研究进行了系统性综述，从人类认知视角分析了AI模型在模拟人类推理能力方面的进展，包括问题理解、逻辑组织、联想记忆、批判性思维和知识学习五个关键认知能力。", "motivation": "数学应用题作为AI领域的基础研究课题，虽然经历了从规则方法到深度学习再到大语言模型的发展，但缺乏系统性的分类综述和当前发展趋势的讨论，需要从人类认知角度全面回顾相关研究。", "method": "总结了MWP求解的5个关键认知能力，回顾了近10年两种主流MWP模型（神经网络求解器和基于LLM的求解器），重新运行了所有代表性MWP求解器并在5个主流基准上进行了统一性能比较。", "result": "论文提供了过去十年有影响力的MWP研究的全面分析，从人类推理认知角度进行了整合性比较，展示了AI模型在模拟人类认知能力方面的进展。", "conclusion": "这项综述首次从人类推理认知视角系统分析了MWP研究，希望为AI推理领域的进一步研究提供启发，相关资源已在GitHub上开源。"}}
{"id": "2510.21894", "pdf": "https://arxiv.org/pdf/2510.21894", "abs": "https://arxiv.org/abs/2510.21894", "authors": ["Mingzhe Xing", "Chang Tian", "Jianan Zhang", "Lichen Pan", "Peipei Liu", "Zhaoteng Yan", "Yinliang Yue"], "title": "Understanding Network Behaviors through Natural Language Question-Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Large Language Models", "summary": "Modern large-scale networks introduce significant complexity in understanding\nnetwork behaviors, increasing the risk of misconfiguration. Prior work proposed\nto understand network behaviors by mining network configurations, typically\nrelying on domain-specific languages interfaced with formal models. While\neffective, they suffer from a steep learning curve and limited flexibility. In\ncontrast, natural language (NL) offers a more accessible and interpretable\ninterface, motivating recent research on NL-guided network behavior\nunderstanding. Recent advances in large language models (LLMs) further enhance\nthis direction, leveraging their extensive prior knowledge of network concepts\nand strong reasoning capabilities. However, three key challenges remain: 1)\nnumerous router devices with lengthy configuration files challenge LLM's\nlong-context understanding ability; 2) heterogeneity across devices and\nprotocols impedes scalability; and 3) complex network topologies and protocols\ndemand advanced reasoning abilities beyond the current capabilities of LLMs. To\ntackle the above challenges, we propose NetMind, a novel framework for querying\nnetworks using NL. Our approach introduces a tree-based configuration chunking\nstrategy to preserve semantic coherence while enabling efficient partitioning.\nWe then construct a unified fact graph as an intermediate representation to\nnormalize vendor-specific configurations. Finally, we design a hybrid\nimperative-declarative language to reduce the reasoning burden on LLMs and\nenhance precision. We contribute a benchmark consisting of NL question-answer\npairs paired with network configurations. Experiments demonstrate that NetMind\nachieves accurate and scalable network behavior understanding, outperforming\nexisting baselines.", "AI": {"tldr": "NetMind是一个使用自然语言查询网络的框架，通过树形配置分块、统一事实图和混合命令-声明语言解决LLM在网络配置分析中的三大挑战，实现了准确且可扩展的网络行为理解。", "motivation": "大规模网络配置复杂，传统基于领域特定语言的方法学习曲线陡峭且灵活性有限。自然语言接口更易用，但LLM在处理长配置文件、设备异构性和复杂网络推理方面存在挑战。", "method": "1) 树形配置分块策略保持语义连贯性；2) 构建统一事实图作为中间表示来规范化厂商特定配置；3) 设计混合命令-声明语言减轻LLM推理负担", "result": "实验表明NetMind在准确性和可扩展性方面优于现有基线方法", "conclusion": "NetMind框架成功解决了LLM在网络配置分析中的关键挑战，为自然语言驱动的网络行为理解提供了有效解决方案"}}
{"id": "2510.22009", "pdf": "https://arxiv.org/pdf/2510.22009", "abs": "https://arxiv.org/abs/2510.22009", "authors": ["Yangqin Jiang", "Chao Huang"], "title": "LightAgent: Mobile Agentic Foundation Models", "categories": ["cs.AI"], "comment": null, "summary": "With the advancement of multimodal large language models (MLLMs), building\nGUI agent systems has become an increasingly promising direction-especially for\nmobile platforms, given their rich app ecosystems and intuitive touch\ninteractions. Yet mobile GUI agents face a critical dilemma: truly on-device\nmodels (4B or smaller) lack sufficient performance, while capable models\n(starting from 7B) are either too large for mobile deployment or prohibitively\ncostly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose\nLightAgent, a mobile agentic foundation model solution that leverages\ndevice-cloud collaboration to tap the cost-efficiency of on-device models and\nthe high capability of cloud models, while avoiding their drawbacks.\nSpecifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO\ntraining on synthetic GUI data for strong decision-making, integrates an\nefficient long-reasoning mechanism to utilize historical interactions under\ntight resources, and defaults to on-device execution-only escalating\nchallenging subtasks to the cloud via real-time complexity assessment.\nExperiments on the online AndroidLab benchmark and diverse apps show LightAgent\nmatches or nears larger models, with a significant reduction in cloud costs.", "AI": {"tldr": "LightAgent是一个移动GUI代理基础模型，通过设备-云协作解决移动设备上小模型性能不足和大模型部署成本高的问题，在AndroidLab基准测试中表现接近大模型且显著降低云成本。", "motivation": "移动GUI代理面临困境：真正在设备上的小模型（4B或更小）性能不足，而能力强的模型（7B以上）要么太大无法移动部署，要么成本过高（如仅限云的闭源MLLMs）。", "method": "提出LightAgent解决方案，通过设备-云协作利用设备模型的成本效益和云模型的高能力；通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型；集成高效长推理机制在有限资源下利用历史交互；默认在设备上执行，仅通过实时复杂度评估将挑战性子任务升级到云端。", "result": "在在线AndroidLab基准测试和多样化应用上的实验显示，LightAgent匹配或接近更大模型的性能，同时显著降低云成本。", "conclusion": "LightAgent成功解决了移动GUI代理的困境，通过创新的设备-云协作架构实现了性能与成本的有效平衡，为移动平台上的智能代理系统提供了可行的解决方案。"}}
{"id": "2510.21900", "pdf": "https://arxiv.org/pdf/2510.21900", "abs": "https://arxiv.org/abs/2510.21900", "authors": ["Hongbo Zhang", "Han Cui", "Yidong Wang", "Yijian Tian", "Qi Guo", "Cunxiang Wang", "Jian Wu", "Chiyu Song", "Yue Zhang"], "title": "Deep Literature Survey Automation with an Iterative Workflow", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint version", "summary": "Automatic literature survey generation has attracted increasing attention,\nyet most existing systems follow a one-shot paradigm, where a large set of\npapers is retrieved at once and a static outline is generated before drafting.\nThis design often leads to noisy retrieval, fragmented structures, and context\noverload, ultimately limiting survey quality. Inspired by the iterative reading\nprocess of human researchers, we propose \\ours, a framework based on recurrent\noutline generation, in which a planning agent incrementally retrieves, reads,\nand updates the outline to ensure both exploration and coherence. To provide\nfaithful paper-level grounding, we design paper cards that distill each paper\ninto its contributions, methods, and findings, and introduce a\nreview-and-refine loop with visualization enhancement to improve textual flow\nand integrate multimodal elements such as figures and tables. Experiments on\nboth established and emerging topics show that \\ours\\ substantially outperforms\nstate-of-the-art baselines in content coverage, structural coherence, and\ncitation quality, while producing more accessible and better-organized surveys.\nTo provide a more reliable assessment of such improvements, we further\nintroduce Survey-Arena, a pairwise benchmark that complements absolute scoring\nand more clearly positions machine-generated surveys relative to human-written\nones. The code is available at\nhttps://github.com/HancCui/IterSurvey\\_Autosurveyv2.", "AI": {"tldr": "IterSurvey是一个基于循环大纲生成的文献综述自动生成框架，通过迭代检索、阅读和更新大纲来提升综述质量，在内容覆盖、结构连贯性和引用质量方面显著优于现有方法。", "motivation": "现有文献综述生成系统采用一次性检索和静态大纲生成，导致检索噪声大、结构碎片化和上下文过载，限制了综述质量。受人类研究者迭代阅读过程的启发，需要更智能的生成方法。", "method": "提出IterSurvey框架，包含规划代理进行增量检索和阅读，设计论文卡片提取每篇论文的贡献、方法和发现，引入审阅-精炼循环和可视化增强来改善文本流并整合多模态元素。", "result": "在成熟和新兴主题上的实验表明，IterSurvey在内容覆盖、结构连贯性和引用质量方面大幅优于最先进基线方法，生成更易访问和组织更好的综述。", "conclusion": "IterSurvey通过迭代式大纲生成和论文级基础验证，显著提升了自动文献综述的质量，同时引入Survey-Arena评估基准为机器生成与人工撰写综述提供了更可靠的对比评估。"}}
{"id": "2510.22034", "pdf": "https://arxiv.org/pdf/2510.22034", "abs": "https://arxiv.org/abs/2510.22034", "authors": ["Rick Chen", "Joseph Ternasky", "Aaron Ontoyin Yin", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "title": "LLM-AR: LLM-powered Automated Reasoning Framework", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) can already identify patterns and reason\neffectively, yet their variable accuracy hampers adoption in high-stakes\ndecision-making applications. In this paper, we study this issue from a venture\ncapital perspective by predicting idea-stage startup success based on founder\ntraits. (i) To build a reliable prediction model, we introduce LLM-AR, a\npipeline inspired by neural-symbolic systems that distils LLM-generated\nheuristics into probabilistic rules executed by the ProbLog automated-reasoning\nengine. (ii) An iterative policy-evolution loop incorporates association-rule\nmining to progressively refine the prediction rules.\n  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the\nrandom baseline precision, while exposing every decision path for human\ninspection. The framework is interpretable and tunable via hyperparameters,\nshowing promise to extend into other domains.", "AI": {"tldr": "LLM-AR：一个结合大语言模型和概率逻辑推理的框架，通过将LLM生成的启发式规则转化为可解释的概率规则，用于创业公司成功预测，在准确性和可解释性方面表现优异", "motivation": "虽然大语言模型具备模式识别和推理能力，但其准确性不稳定限制了在高风险决策应用中的采用，特别是在风险投资领域需要可靠预测的需求", "method": "提出LLM-AR管道，受神经符号系统启发，将LLM生成的启发式规则转化为ProbLog自动推理引擎执行的概率规则，并采用迭代策略进化循环结合关联规则挖掘来逐步优化预测规则", "result": "在未见数据上达到59.5%的精确率和8.7%的召回率，精确率是随机基线的5.9倍，同时保持决策路径完全可解释", "conclusion": "该框架具有可解释性和超参数可调性，展示了扩展到其他领域的潜力，为高风险决策应用提供了可靠的AI辅助工具"}}
{"id": "2510.21909", "pdf": "https://arxiv.org/pdf/2510.21909", "abs": "https://arxiv.org/abs/2510.21909", "authors": ["Catherine Arnett", "Tyler A. Chang", "Stella Biderman", "Benjamin K. Bergen"], "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities", "categories": ["cs.CL"], "comment": "Accepted to NeurIPS 2025", "summary": "The number of tokens it takes to encode parallel text in different languages\nis known to vary. These disparities are called token premiums. Having high\ntoken premiums leads to less throughput during training and increases costs at\ninference. In this paper, we show that even after controlling for dataset size,\nvocabulary size, and data content, monolingual tokenizers exhibit a wide range\nof token premiums across languages. To understand the cross-linguistic\ndifferences that cause these token premiums, we train a suite of approximately\n7,000 comparable monolingual tokenizers for 97 languages, manipulating\ntokenization algorithm, vocabulary size, and dataset size. We measure token\npremiums and test for a relationship between factors such as data similarity\n(between tokenizer training and evaluation), vocabulary size, and\npre-tokenization. We also investigate the role of language-specific features\nsuch as writing system and word length. We find that similarity between\ntraining and test data does not impact token premiums, but vocabulary size and\npre-tokenization do. While simply increasing vocabulary size does not lead to\nreduced token premium effects, we can determine an ``optimal'' vocabulary size\nfor each language to achieve significantly reduced token premium effects. We\nalso train superword tokenizers which allow merges over whitespaces, and we\nfind that they both reduce token premium effects and improve compression\noverall. Thus, intervening on the vocabulary size or the pre-tokenizer\nsignificantly reduces crosslingual token premium effects.", "AI": {"tldr": "本文研究了多语言文本编码中的token溢价现象，发现即使控制数据集大小、词汇表大小和数据内容，单语分词器在不同语言间仍存在显著token溢价差异。通过训练约7000个单语分词器并分析影响因素，发现词汇表大小和预分词策略是关键因素，而训练测试数据相似性影响不大。", "motivation": "不同语言在编码为token时存在数量差异（token溢价），这会导致训练吞吐量降低和推理成本增加。研究旨在理解导致这些跨语言差异的根本原因。", "method": "训练了约7000个可比单语分词器，涵盖97种语言，控制分词算法、词汇表大小和数据集大小等变量，测量token溢价并分析数据相似性、词汇表大小、预分词策略以及语言特定特征的影响。", "result": "发现训练测试数据相似性不影响token溢价，但词汇表大小和预分词策略是关键因素。确定每种语言的'最优'词汇表大小可显著降低token溢价效应。超级词分词器（允许跨空格合并）能同时降低token溢价并提高整体压缩率。", "conclusion": "通过干预词汇表大小或预分词器可以显著减少跨语言token溢价效应，这为多语言NLP系统的效率优化提供了重要指导。"}}
{"id": "2510.22039", "pdf": "https://arxiv.org/pdf/2510.22039", "abs": "https://arxiv.org/abs/2510.22039", "authors": ["Po-Chen Kuo", "Han Hou", "Will Dabney", "Edgar Y. Walker"], "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability", "categories": ["cs.AI", "q-bio.NC"], "comment": "Accepted to Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2025", "summary": "Learning a compact representation of history is critical for planning and\ngeneralization in partially observable environments. While meta-reinforcement\nlearning (RL) agents can attain near Bayes-optimal policies, they often fail to\nlearn the compact, interpretable Bayes-optimal belief states. This\nrepresentational inefficiency potentially limits the agent's adaptability and\ngeneralization capacity. Inspired by predictive coding in neuroscience--which\nsuggests that the brain predicts sensory inputs as a neural implementation of\nBayesian inference--and by auxiliary predictive objectives in deep RL, we\ninvestigate whether integrating self-supervised predictive coding modules into\nmeta-RL can facilitate learning of Bayes-optimal representations. Through state\nmachine simulation, we show that meta-RL with predictive modules consistently\ngenerates more interpretable representations that better approximate\nBayes-optimal belief states compared to conventional meta-RL across a wide\nvariety of tasks, even when both achieve optimal policies. In challenging tasks\nrequiring active information seeking, only meta-RL with predictive modules\nsuccessfully learns optimal representations and policies, whereas conventional\nmeta-RL struggles with inadequate representation learning. Finally, we\ndemonstrate that better representation learning leads to improved\ngeneralization. Our results strongly suggest the role of predictive learning as\na guiding principle for effective representation learning in agents navigating\npartial observability.", "AI": {"tldr": "该研究通过在元强化学习中整合自监督预测编码模块，成功学习了更紧凑、可解释的贝叶斯最优信念状态表示，显著提升了在部分可观测环境中的表示学习效果和泛化能力。", "motivation": "元强化学习虽然能获得接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态，这种表示效率低下限制了智能体的适应性和泛化能力。", "method": "受神经科学中预测编码理论和深度强化学习中辅助预测目标的启发，将自监督预测编码模块整合到元强化学习中，通过状态机模拟验证方法有效性。", "result": "相比传统元强化学习，带预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地近似贝叶斯最优信念状态；在需要主动信息搜索的挑战性任务中，只有带预测模块的方法能成功学习最优表示和策略。", "conclusion": "预测学习可以作为在部分可观测环境中导航的智能体进行有效表示学习的指导原则，更好的表示学习带来了改进的泛化能力。"}}
{"id": "2510.21954", "pdf": "https://arxiv.org/pdf/2510.21954", "abs": "https://arxiv.org/abs/2510.21954", "authors": ["Mykola Haltiuk", "Aleksander Smywiński-Pohl"], "title": "Model-Aware Tokenizer Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are trained to support an increasing number of\nlanguages, yet their predefined tokenizers remain a bottleneck for adapting\nmodels to lower-resource or distinct-script languages. Existing tokenizer\ntransfer methods typically rely on semantic heuristics to initialize new\nembeddings, ignoring higher-layer model dynamics and limiting transfer quality.\nWe propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates\nmodel internals into the tokenizer transfer process. MATT introduces an\nAttention Influence Modeling (AIM) objective that distills inter-token\ncommunication patterns from a source model into a target model with a new\ntokenizer, providing an efficient warm-up before standard language modeling.\nUnlike approaches that focus solely on embedding similarity, MATT leverages\nattention behavior to guide embedding initialization and adaptation.\nExperiments across diverse linguistic settings show that MATT recovers a large\nfraction of the original model's performance within a few GPU hours,\noutperforming heuristic baselines. These results demonstrate that incorporating\nmodel-level signals offers a practical and effective path toward robust\ntokenizer transfer in multilingual LLMs.", "AI": {"tldr": "MATT方法通过注意力影响建模将源模型的token间通信模式蒸馏到新tokenizer的目标模型中，相比仅关注嵌入相似性的启发式方法，能更有效地实现多语言LLM的tokenizer迁移。", "motivation": "现有tokenizer迁移方法主要依赖语义启发式来初始化新嵌入，忽略了高层模型动态，限制了迁移质量。LLMs支持的语言越来越多，但预定义tokenizer仍然是适应低资源或不同文字语言时的瓶颈。", "method": "提出Model-Aware Tokenizer Transfer (MATT)方法，引入Attention Influence Modeling (AIM)目标，将源模型的token间通信模式蒸馏到使用新tokenizer的目标模型中，为标准语言建模提供高效预热。", "result": "在不同语言设置下的实验显示，MATT在几个GPU小时内就能恢复原模型大部分性能，优于启发式基线方法。", "conclusion": "将模型级信号纳入tokenizer迁移过程为多语言LLMs提供了实用且有效的鲁棒tokenizer迁移路径。"}}
{"id": "2510.22046", "pdf": "https://arxiv.org/pdf/2510.22046", "abs": "https://arxiv.org/abs/2510.22046", "authors": ["Daniel G. P. Petrini", "Braz Izaias da Silva Junior"], "title": "HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology", "categories": ["cs.AI", "cs.AR", "cs.SE"], "comment": "6", "summary": "We present a case study applying the SpecC methodology within a system-level\nhardware/software co-design flow to a PCM-to-PWM converter, the core of a\nClass-D audio amplifier. The converter was modeled and explored with SpecC\nmethodology to derive an HW/SW partition. Using system-level estimates and fast\nfunctional simulation, we evaluated mappings that meet real-time constraints\nwhile reducing estimated cost of an all-hardware solution and avoiding the\nexpense of a purely software implementation on a high-end processor. Despite\nthe design's moderate complexity, the results underline the value of\nsystem-level co-design for early architectural insight, rapid validation, and\nactionable cost/performance trade-offs. [Original work from 2005; formatting\nrevised in 2025, with no changes to the results.]", "AI": {"tldr": "该论文应用SpecC方法学对PCM-to-PWM转换器进行系统级硬件/软件协同设计，通过建模和探索找到满足实时约束的HW/SW分区方案，在降低全硬件方案成本的同时避免高端处理器纯软件实现的高昂费用。", "motivation": "研究系统级硬件/软件协同设计方法在PCM-to-PWM转换器（Class-D音频放大器核心）中的应用价值，探索如何在满足实时性能约束的同时优化成本效益。", "method": "采用SpecC方法学进行建模和系统级探索，使用系统级估算和快速功能仿真来评估不同的硬件/软件映射方案。", "result": "尽管设计复杂度适中，但研究结果证明了系统级协同设计在提供早期架构洞察、快速验证以及可行的成本/性能权衡方面的价值。", "conclusion": "系统级硬件/软件协同设计方法能够有效支持早期设计决策，在满足实时约束的前提下实现成本优化，为类似嵌入式系统设计提供了有价值的参考。"}}
{"id": "2510.21958", "pdf": "https://arxiv.org/pdf/2510.21958", "abs": "https://arxiv.org/abs/2510.21958", "authors": ["Harrison F. Stropkay", "Jiayi Chen", "Mohammad J. Latifi", "Daniel N. Rockmore", "Jeremy R. Manning"], "title": "A Stylometric Application of Large Language Models", "categories": ["cs.CL", "cs.DL"], "comment": "All code and data needed to reproduce the results in this paper are\n  available at https://github.com/ContextLab/llm-stylometry", "summary": "We show that large language models (LLMs) can be used to distinguish the\nwritings of different authors. Specifically, an individual GPT-2 model, trained\nfrom scratch on the works of one author, will predict held-out text from that\nauthor more accurately than held-out text from other authors. We suggest that,\nin this way, a model trained on one author's works embodies the unique writing\nstyle of that author. We first demonstrate our approach on books written by\neight different (known) authors. We also use this approach to confirm R. P.\nThompson's authorship of the well-studied 15th book of the Oz series,\noriginally attributed to F. L. Baum.", "AI": {"tldr": "该论文展示如何通过训练单独的GPT-2模型来识别不同作者的写作风格，模型能更准确地预测目标作者的文本，并成功应用于确认R. P. Thompson对《绿野仙踪》系列第15本书的作者身份", "motivation": "探索大型语言模型在作者识别方面的应用潜力，验证模型是否能捕捉和体现特定作者的独特写作风格", "method": "为每位作者从头开始训练单独的GPT-2模型，然后比较这些模型在预测目标作者与其他作者文本时的准确性差异", "result": "模型能够更准确地预测训练时所针对作者的文本，成功区分了八位不同作者的写作风格，并确认了R. P. Thompson对《绿野仙踪》第15本书的作者身份", "conclusion": "大型语言模型能够有效捕捉和体现特定作者的独特写作风格，为作者识别和文本归属分析提供了新的技术手段"}}
{"id": "2510.22050", "pdf": "https://arxiv.org/pdf/2510.22050", "abs": "https://arxiv.org/abs/2510.22050", "authors": ["Marcus Thomas"], "title": "Towards Error-Centric Intelligence II: Energy-Structured Causal Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Contemporary machine learning optimizes for predictive accuracy, yet systems\nthat achieve state of the art performance remain causally opaque: their\ninternal representations provide no principled handle for intervention. We can\nretrain such models, but we cannot surgically edit specific mechanisms while\nholding others fixed, because learned latent variables lack causal semantics.\nWe argue for a conceptual reorientation: intelligence is the ability to build\nand refine explanations, falsifiable claims about manipulable structure that\nspecify what changes and what remains invariant under intervention.\nExplanations subsume prediction but demand more: causal commitments that can be\nindependently tested and corrected at the level of mechanisms. We introduce\ncomputational explanations, mappings from observations to intervention ready\ncausal accounts. We instantiate these explanations with Energy Structured\nCausal Models (ESCMs), in which mechanisms are expressed as constraints (energy\nfunctions or vector fields) rather than explicit input output maps, and\ninterventions act by local surgery on those constraints. This shift makes\ninternal structure manipulable at the level where explanations live: which\nrelations must hold, which can change, and what follows when they do. We\nprovide concrete instantiations of the structural-causal principles LAP and ICM\nin the ESCM context, and also argue that empirical risk minimization\nsystematically produces fractured, entangled representations, a failure we\nanalyze as gauge ambiguity in encoder energy pairs. Finally, we show that under\nmild conditions, ESCMs recover standard SCM semantics. Building on Part I's\nprinciples (LAP, ICM, CAP) and its definition of intelligence as\nexplanation-building under criticism, this paper offers a formal language for\ncausal reasoning in systems that aspire to understand, not merely to predict.", "AI": {"tldr": "该论文提出从预测准确性转向因果解释的智能概念，引入能量结构化因果模型（ESCMs）作为可干预的因果表示框架，使机器学习系统能够进行机制层面的外科手术式编辑。", "motivation": "当前机器学习虽然实现了最先进的预测性能，但其内部表示缺乏因果语义，无法对特定机制进行精确干预和编辑，需要重新定义智能为构建和修正可验证解释的能力。", "method": "提出计算解释的概念，并实例化为能量结构化因果模型（ESCMs），其中机制表示为约束（能量函数或向量场）而非显式输入输出映射，干预通过对这些约束进行局部手术来实现。", "result": "ESCMs在温和条件下恢复了标准SCM语义，提供了结构因果原则LAP和ICM的具体实例化，并分析了经验风险最小化导致表示断裂和纠缠的问题。", "conclusion": "该研究为追求理解而不仅仅是预测的系统提供了因果推理的形式化语言，将智能重新定义为在批评下构建解释的能力，为可解释和可干预的AI系统奠定了基础。"}}
{"id": "2510.21983", "pdf": "https://arxiv.org/pdf/2510.21983", "abs": "https://arxiv.org/abs/2510.21983", "authors": ["Havva Alizadeh Noughabi", "Julien Serbanescu", "Fattane Zarrinkalam", "Ali Dehghantanha"], "title": "Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite recent advances, Large Language Models remain vulnerable to jailbreak\nattacks that bypass alignment safeguards and elicit harmful outputs. While\nprior research has proposed various attack strategies differing in human\nreadability and transferability, little attention has been paid to the\nlinguistic and psychological mechanisms that may influence a model's\nsusceptibility to such attacks. In this paper, we examine an interdisciplinary\nline of research that leverages foundational theories of persuasion from the\nsocial sciences to craft adversarial prompts capable of circumventing alignment\nconstraints in LLMs. Drawing on well-established persuasive strategies, we\nhypothesize that LLMs, having been trained on large-scale human-generated text,\nmay respond more compliantly to prompts with persuasive structures.\nFurthermore, we investigate whether LLMs themselves exhibit distinct persuasive\nfingerprints that emerge in their jailbreak responses. Empirical evaluations\nacross multiple aligned LLMs reveal that persuasion-aware prompts significantly\nbypass safeguards, demonstrating their potential to induce jailbreak behaviors.\nThis work underscores the importance of cross-disciplinary insight in\naddressing the evolving challenges of LLM safety. The code and data are\navailable.", "AI": {"tldr": "本研究探讨如何利用社会科学中的说服理论来设计对抗性提示，成功绕过大型语言模型的安全对齐机制，揭示了模型对具有说服力结构的提示更易产生有害输出。", "motivation": "现有研究主要关注攻击策略的可读性和可迁移性，但忽视了语言和心理机制对模型脆弱性的影响。研究者希望通过跨学科方法，利用人类说服理论来测试和增强LLM的安全性。", "method": "基于社会科学中成熟的说服策略理论，设计具有说服结构的对抗性提示，并在多个已对齐的LLM上进行实证评估。", "result": "实验结果显示，具有说服意识的提示能显著绕过模型的安全防护，诱导出越狱行为，证明了说服策略在攻击中的有效性。", "conclusion": "这项工作强调了跨学科见解在应对LLM安全挑战中的重要性，表明需要从心理学和语言学角度深入理解模型脆弱性，以开发更强大的安全措施。"}}
{"id": "2510.22052", "pdf": "https://arxiv.org/pdf/2510.22052", "abs": "https://arxiv.org/abs/2510.22052", "authors": ["Abhijit Chatterjee", "Niraj K. Jha", "Jonathan D. Cohen", "Thomas L. Griffiths", "Hongjing Lu", "Diana Marculescu", "Ashiqur Rasul", "Keshab K. Parhi"], "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The field of artificial intelligence (AI) has taken a tight hold on broad\naspects of society, industry, business, and governance in ways that dictate the\nprosperity and might of the world's economies. The AI market size is projected\nto grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI\nis dominated by large language models that exhibit linguistic and visual\nintelligence. However, training these models requires a massive amount of data\nscraped from the web as well as large amounts of energy (50--60 GWh to train\nGPT-4). Despite these costs, these models often hallucinate, a characteristic\nthat prevents them from being deployed in critical application domains. In\ncontrast, the human brain consumes only 20~W of power. What is needed is the\nnext level of AI evolution in which lightweight domain-specific multimodal\nmodels with higher levels of intelligence can reason, plan, and make decisions\nin dynamic environments with real-time data and prior knowledge, while learning\ncontinuously and evolving in ways that enhance future decision-making\ncapability. This will define the next wave of AI, progressing from today's\nlarge models, trained with vast amounts of data, to nimble energy-efficient\ndomain-specific agents that can reason and think in a world full of\nuncertainty. To support such agents, hardware will need to be reimagined to\nallow energy efficiencies greater than 1000x over the state of the art. Such a\nvision of future AI systems is developed in this work.", "AI": {"tldr": "论文提出AI发展的新方向：从当前耗能巨大的大型语言模型转向轻量级、领域特定的多模态智能体，这些智能体能够在动态环境中进行推理、规划和决策，同时实现能效提升1000倍以上的硬件支持。", "motivation": "当前AI模型（如GPT-4）存在高能耗（50-60 GWh训练成本）、幻觉问题以及无法在关键应用领域部署的局限性，而人脑仅消耗20W功率，需要开发更高效、更智能的AI系统。", "method": "提出开发轻量级领域特定多模态模型，这些模型能够利用实时数据和先验知识进行推理、规划和决策，并支持持续学习和进化能力。同时需要重新设计硬件以实现超过当前技术水平1000倍的能效提升。", "result": "论文构建了未来AI系统的愿景框架，提出了从大数据训练的大型模型向节能高效的专业智能体转变的技术路线图。", "conclusion": "下一代AI应该向轻量级、领域特定、高能效的多模态智能系统发展，这需要硬件和软件技术的协同创新，以实现比现有技术高1000倍的能效提升，从而支持在动态不确定环境中的实时推理和决策能力。"}}
{"id": "2510.22014", "pdf": "https://arxiv.org/pdf/2510.22014", "abs": "https://arxiv.org/abs/2510.22014", "authors": ["Sarah Ball", "Niki Hasrati", "Alexander Robey", "Avi Schwarzschild", "Frauke Kreuter", "Zico Kolter", "Andrej Risteski"], "title": "Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Discrete optimization-based jailbreaking attacks on large language models aim\nto generate short, nonsensical suffixes that, when appended onto input prompts,\nelicit disallowed content. Notably, these suffixes are often transferable --\nsucceeding on prompts and models for which they were never optimized. And yet,\ndespite the fact that transferability is surprising and empirically\nwell-established, the field lacks a rigorous analysis of when and why transfer\noccurs. To fill this gap, we identify three statistical properties that\nstrongly correlate with transfer success across numerous experimental settings:\n(1) how much a prompt without a suffix activates a model's internal refusal\ndirection, (2) how strongly a suffix induces a push away from this direction,\nand (3) how large these shifts are in directions orthogonal to refusal. On the\nother hand, we find that prompt semantic similarity only weakly correlates with\ntransfer success. These findings lead to a more fine-grained understanding of\ntransferability, which we use in interventional experiments to showcase how our\nstatistical analysis can translate into practical improvements in attack\nsuccess.", "AI": {"tldr": "该论文分析了离散优化越狱攻击的迁移性机制，发现三个统计属性与迁移成功强相关：原始提示激活拒绝方向的程度、后缀推离拒绝方向的强度、以及正交方向上的位移大小，而语义相似性相关性较弱。", "motivation": "虽然离散优化越狱攻击生成的短后缀具有跨提示和模型的迁移性，但缺乏对迁移何时发生及为何发生的严格分析，需要填补这一理论空白。", "method": "通过大量实验识别与迁移成功强相关的统计属性，包括拒绝方向激活度、推离拒绝方向的强度、正交位移大小，并进行干预实验验证。", "result": "发现三个统计属性与迁移成功强相关，而语义相似性仅弱相关，这些发现可用于实际提升攻击成功率。", "conclusion": "研究提供了对越狱攻击迁移性的细粒度理解，统计分析方法可转化为实际攻击效果的提升，为防御机制设计提供 insights。"}}
{"id": "2510.22095", "pdf": "https://arxiv.org/pdf/2510.22095", "abs": "https://arxiv.org/abs/2510.22095", "authors": ["Yankai Chen", "Xinni Zhang", "Yifei Zhang", "Yangning Li", "Henry Peng Zou", "Chunyu Miao", "Weizhi Zhang", "Xue Liu", "Philip S. Yu"], "title": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by NeurIPS'25 Position Track", "summary": "Brain-Computer Interfaces (BCIs) offer a direct communication pathway between\nthe human brain and external devices, holding significant promise for\nindividuals with severe neurological impairments. However, their widespread\nadoption is hindered by critical limitations, such as low information transfer\nrates and extensive user-specific calibration. To overcome these challenges,\nrecent research has explored the integration of Large Language Models (LLMs),\nextending the focus from simple command decoding to understanding complex\ncognitive states. Despite these advancements, deploying agentic AI faces\ntechnical hurdles and ethical concerns. Due to the lack of comprehensive\ndiscussion on this emerging direction, this position paper argues that the\nfield is poised for a paradigm extension from BCI to Brain-Agent Collaboration\n(BAC). We emphasize reframing agents as active and collaborative partners for\nintelligent assistance rather than passive brain signal data processors,\ndemanding a focus on ethical data handling, model reliability, and a robust\nhuman-agent collaboration framework to ensure these systems are safe,\ntrustworthy, and effective.", "AI": {"tldr": "这篇立场论文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，主张将智能体重新定义为主动协作伙伴而非被动信号处理器，强调伦理数据处理和可靠人机协作框架的重要性。", "motivation": "当前脑机接口技术面临信息传输率低、用户特定校准需求高等限制，虽然大型语言模型的整合有所进展，但代理AI部署仍存在技术障碍和伦理问题，需要更全面的讨论。", "method": "这是一篇立场论文，通过分析当前BCI技术的局限性和LLM整合的进展，提出从BCI到BAC的范式转变概念框架。", "result": "提出了Brain-Agent Collaboration (BAC)的新范式，强调智能体应作为主动协作伙伴，需要关注伦理数据处理、模型可靠性和人机协作框架。", "conclusion": "该领域需要从BCI扩展到BAC范式，通过建立安全、可信、有效的系统，实现智能体作为人类认知活动的主动协作伙伴，这需要重点关注伦理、可靠性和协作框架等问题。"}}
{"id": "2510.22028", "pdf": "https://arxiv.org/pdf/2510.22028", "abs": "https://arxiv.org/abs/2510.22028", "authors": ["Yilin Zhang", "Wenda Xu", "Zhongtao Liu", "Tetsuji Nakagawa", "Markus Freitag"], "title": "Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics", "categories": ["cs.CL"], "comment": null, "summary": "Quality Estimation (QE) metrics are vital in machine translation for\nreference-free evaluation and as a reward signal in tasks like reinforcement\nlearning. However, the prevalence and impact of length bias in QE have been\nunderexplored. Through a systematic study of top-performing regression-based\nand LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two\ncritical length biases: First, QE metrics consistently over-predict errors with\nincreasing translation length, even for high-quality, error-free texts. Second,\nthey exhibit a preference for shorter translations when multiple candidates are\navailable for the same source text. These inherent length biases risk unfairly\npenalizing longer, correct translations and can lead to sub-optimal\ndecision-making in applications such as QE reranking and QE guided\nreinforcement learning. To mitigate this, we propose two strategies: (a)\napplying length normalization during model training, and (b) incorporating\nreference texts during evaluation. Both approaches were found to effectively\nreduce the identified length bias.", "AI": {"tldr": "研究发现机器翻译质量评估(QE)指标存在严重长度偏差：过度预测长文本错误且偏好短译文，提出长度归一化和参考文本两种缓解策略。", "motivation": "质量评估指标在机器翻译中至关重要，但长度偏差的普遍性和影响尚未充分研究，可能对长文本造成不公平惩罚并导致次优决策。", "method": "对10种语言对的顶尖回归基和LLM评估QE指标进行系统研究，分析长度偏差现象。", "result": "发现QE指标存在两种关键长度偏差：1)随译文长度增加而过度预测错误；2)对同一源文本偏好较短译文。", "conclusion": "提出的长度归一化训练和参考文本评估两种策略能有效缓解长度偏差问题。"}}
{"id": "2510.22132", "pdf": "https://arxiv.org/pdf/2510.22132", "abs": "https://arxiv.org/abs/2510.22132", "authors": ["Xuying LI"], "title": "Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors", "categories": ["cs.AI"], "comment": null, "summary": "We present a novel approach for controllable mathematical reasoning that\nleverages self-optimizing thought vectors with entropy minimization. Our method\nintroduces learnable thought vectors that dynamically modulate the internal\nreasoning process of large language models. Using Gemma-2-9B on GSM8K, we\nachieve 90.1% accuracy with a controllability score of 0.42, demonstrating that\nentropy-based rewards effectively guide focused reasoning patterns without\nrequiring external reward annotations. Our analysis reveals distinct thought\nvector clusters and consistent low-entropy distributions across control\nconditions, validating our framework for controllable AI reasoning.", "AI": {"tldr": "提出基于熵最小化的自优化思维向量方法，实现可控数学推理，在GSM8K上达到90.1%准确率和0.42可控性分数", "motivation": "需要开发能够动态调控大语言模型内部推理过程的可控数学推理方法，避免依赖外部奖励标注", "method": "引入可学习的思维向量，通过熵最小化奖励机制引导聚焦推理模式，使用Gemma-2-9B模型在GSM8K数据集上进行实验", "result": "达到90.1%的准确率，可控性分数0.42，思维向量形成明显聚类，在不同控制条件下保持低熵分布", "conclusion": "熵基奖励机制能有效指导聚焦推理模式，验证了可控AI推理框架的有效性"}}
{"id": "2510.22037", "pdf": "https://arxiv.org/pdf/2510.22037", "abs": "https://arxiv.org/abs/2510.22037", "authors": ["Shayne Longpre", "Sneha Kudugunta", "Niklas Muennighoff", "I-Hung Hsu", "Isaac Caswell", "Alex Pentland", "Sercan Arik", "Chen-Yu Lee", "Sayna Ebrahimi"], "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.", "AI": {"tldr": "该研究进行了迄今为止最大的多语言扩展定律研究，涵盖774个多语言训练实验，提出了优于现有方法的自适应迁移扩展定律(ATLAS)，并揭示了多语言学习的动态特性、语言间迁移规律以及多语言性的计算权衡点。", "motivation": "现有的扩展定律研究主要集中于英语，但主流AI模型需要服务数十亿国际用户，因此需要研究多语言环境下的扩展规律。", "method": "进行了774个多语言训练实验，覆盖10M-8B参数规模、400+训练语言和48种评估语言，提出了自适应迁移扩展定律(ATLAS)，并推导了跨语言迁移矩阵和语言无关的扩展定律。", "result": "ATLAS定律在样本外泛化能力上比现有扩展定律平均提升超过0.3 R²，建立了38×38=1444种语言对的相互受益评分矩阵，确定了从头训练与多语言检查点微调的计算权衡点。", "conclusion": "该研究为多语言扩展定律的民主化提供了科学基础，使实践者能够高效地扩展超越英语优先AI的多语言模型。"}}
{"id": "2510.22170", "pdf": "https://arxiv.org/pdf/2510.22170", "abs": "https://arxiv.org/abs/2510.22170", "authors": ["Alexandra Yost", "Shreyans Jain", "Shivam Raval", "Grant Corser", "Allen Roush", "Nina Xu", "Jacqueline Hammack", "Ravid Shwartz-Ziv", "Amirali Abdullah"], "title": "Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests", "categories": ["cs.AI", "I.2.7; I.2.6; H.1.2; J.4"], "comment": "49 pages", "summary": "AI psychometrics evaluates AI systems in roles that traditionally require\nemotional judgment and ethical consideration. Prior work often reuses human\ntrait inventories (Big Five, \\hexaco) or ad hoc personas, limiting behavioral\nrealism and domain relevance. We propose a framework that (1) uses situational\njudgment tests (SJTs) from realistic scenarios to probe domain-specific\ncompetencies; (2) integrates industrial-organizational and personality\npsychology to design sophisticated personas which include behavioral and\npsychological descriptors, life history, and social and emotional functions;\nand (3) employs structured generation with population demographic priors and\nmemoir inspired narratives, encoded with Pydantic schemas. In a law enforcement\nassistant case study, we construct a rich dataset of personas drawn across 8\npersona archetypes and SJTs across 11 attributes, and analyze behaviors across\nsubpopulation and scenario slices. The dataset spans 8,500 personas, 4,000\nSJTs, and 300,000 responses. We will release the dataset and all code to the\npublic.", "AI": {"tldr": "提出一个AI心理测量框架，使用情境判断测试和复杂人物设定来评估AI系统在需要情感判断和伦理考量的角色中的表现，并在执法助手案例中构建了包含8500个人物、4000个测试和30万回应的数据集。", "motivation": "现有AI心理测量研究通常重复使用人类特质清单或临时人物设定，限制了行为真实性和领域相关性，需要更现实的评估方法。", "method": "提出三部分框架：(1)使用真实情境的情境判断测试评估领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂人物设定；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙事。", "result": "在执法助手案例研究中构建了丰富数据集，涵盖8种人物原型和11个属性的测试，包含8500个人物、4000个测试和30万条回应。", "conclusion": "该框架提供了更现实的AI心理测量方法，将公开发布数据集和所有代码，促进该领域研究发展。"}}
{"id": "2510.22042", "pdf": "https://arxiv.org/pdf/2510.22042", "abs": "https://arxiv.org/abs/2510.22042", "authors": ["Benjamin Reichman", "Adar Avsian", "Larry Heck"], "title": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work investigates how large language models (LLMs) internally represent\nemotion by analyzing the geometry of their hidden-state space. The paper\nidentifies a low-dimensional emotional manifold and shows that emotional\nrepresentations are directionally encoded, distributed across layers, and\naligned with interpretable dimensions. These structures are stable across depth\nand generalize to eight real-world emotion datasets spanning five languages.\nCross-domain alignment yields low error and strong linear probe performance,\nindicating a universal emotional subspace. Within this space, internal emotion\nperception can be steered while preserving semantics using a learned\nintervention module, with especially strong control for basic emotions across\nlanguages. These findings reveal a consistent and manipulable affective\ngeometry in LLMs and offer insight into how they internalize and process\nemotion.", "AI": {"tldr": "论文研究发现大语言模型内部存在低维情感流形，情感表征具有方向性编码且分布在各层中，该结构在不同深度保持稳定并能泛化到多语言情感数据集，可通过干预模块操控情感感知。", "motivation": "探究大语言模型如何在内部表示情感，分析其隐藏状态空间的几何结构，以理解模型对情感的内部化处理机制。", "method": "分析LLM隐藏状态空间的几何结构，识别低维情感流形，研究情感表征的方向性编码和层级分布，使用跨领域对齐和线性探针评估，开发干预模块进行情感操控实验。", "result": "发现稳定且可泛化的情感子空间结构，跨语言情感数据集对齐误差低且线性探针性能强，干预模块能有效操控情感感知同时保持语义完整性，特别对基本情感跨语言控制效果显著。", "conclusion": "LLMs内部存在一致且可操控的情感几何结构，揭示了模型内部化和处理情感的机制，为理解AI情感表征提供了新见解。"}}
{"id": "2510.22178", "pdf": "https://arxiv.org/pdf/2510.22178", "abs": "https://arxiv.org/abs/2510.22178", "authors": ["Saranraj Nambusubramaniyan", "Shervin Safavi", "Raja Guru", "Andreas Knoblauch"], "title": "Dopamine-driven synaptic credit assignment in neural networks", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Solving the synaptic Credit Assignment Problem(CAP) is central to learning in\nboth biological and artificial neural systems. Finding an optimal solution for\nsynaptic CAP means setting the synaptic weights that assign credit to each\nneuron for influencing the final output and behavior of neural networks or\nanimals. Gradient-based methods solve this problem in artificial neural\nnetworks using back-propagation, however, not in the most efficient way. For\ninstance, back-propagation requires a chain of top-down gradient computations.\nThis leads to an expensive optimization process in terms of computing power and\nmemory linked with well-known weight transport and update locking problems. To\naddress these shortcomings, we take a NeuroAI approach and draw inspiration\nfrom neural Reinforcement Learning to develop a derivative-free optimizer for\ntraining neural networks, Dopamine. Dopamine is developed for Weight\nPerturbation (WP) learning that exploits stochastic updating of weights towards\noptima. It achieves this by minimizing the regret, a form of Reward Prediction\nError (RPE) between the expected outcome from the perturbed model and the\nactual outcome from the unperturbed model. We use this RPE to adjust the\nlearning rate in the network (i.e., creating an adaptive learning rate\nstrategy, similar to the role of dopamine in the brain). We tested the Dopamine\noptimizer for training multi-layered perceptrons for XOR tasks, and recurrent\nneural networks for chaotic time series forecasting. Dopamine-trained models\ndemonstrate accelerated convergence and outperform standard WP, and give\ncomparable performance to gradient-based algorithms, while consuming\nsignificantly less computation and memory. Overall, the Dopamine optimizer not\nonly finds robust solutions and comparable performance to the state-of-the-art\nMachine Learning optimizers but is also neurobiologically more plausible.", "AI": {"tldr": "论文提出了一种名为Dopamine的免导数优化器，通过权重扰动学习和奖励预测误差机制来训练神经网络，解决了反向传播的计算和内存效率问题，在性能相当的同时具有更好的神经生物学合理性。", "motivation": "解决突触信用分配问题(CAP)是神经网络学习的关键。反向传播虽然有效但计算成本高，存在权重传输和更新锁定问题，需要更高效的神经生物学合理方法。", "method": "采用神经AI方法，从神经强化学习中获得灵感，开发Dopamine优化器。通过权重扰动学习，利用随机权重更新，通过最小化扰动模型与未扰动模型之间的奖励预测误差来调整学习率。", "result": "在XOR任务和多层感知机、混沌时间序列预测的循环神经网络上测试，Dopamine训练模型显示加速收敛，优于标准权重扰动方法，性能与基于梯度的算法相当，同时显著减少计算和内存消耗。", "conclusion": "Dopamine优化器不仅找到了鲁棒解且性能与最先进的机器学习优化器相当，而且在神经生物学上更合理，为高效神经网络训练提供了新途径。"}}
{"id": "2510.22084", "pdf": "https://arxiv.org/pdf/2510.22084", "abs": "https://arxiv.org/abs/2510.22084", "authors": ["Atij Mahesh"], "title": "Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds", "categories": ["cs.CL"], "comment": "20 pages", "summary": "Large Language Models (LLMs) still produce gender-stereotyped language even\nin occupation-neutral contexts that reflect deep societal biases (Rudinger et\nal., 2018). To address this, prior work has proposed prompting, constrained\ndecoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and\nfine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).\nHowever, the comparative efficacy and learning dynamics remain little\nunderstood. We report a comparative analysis of six control techniques for bias\nmitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and\nIterative Nullspace Projection (INLP). We evaluate each method on a\ncompositional constraint task. This task requires generating sentences that\ncontain at least one agentic and one communal descriptor for each of the twenty\nWinogender-derived occupations. We quantify trade-offs between control strength\nand naturalness with evaluations of constraint compliance, lexical diversity,\nand fluency. Our results reveal key contrasts among the methods: SFT achieves\n99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite\nsimilar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect\ncompliance, but at the cost of severely reduced fluency and diversity.\nPreference-based learning fundamentally differs: it cannot satisfy\ncompositional constraints, as binary preference signals encode ranking, not\nlogical conjunctions. Only explicit positive supervision enables mitigation of\ncompositional biases; preference-based alignment fails to generalize logical\nstructures, underscoring the limitations of preference learning and the\nnecessity of explicit supervision for fair and fluent controlled generation.", "AI": {"tldr": "本研究比较了六种减少大语言模型性别偏见的技术，发现监督微调(SFT)在合规性和词汇多样性方面表现最佳，而基于偏好的方法(如DPO)在处理组合约束时失败，表明明确的监督学习对于消除偏见更有效。", "motivation": "大语言模型即使在性别中立的语境中仍会产生性别刻板印象语言，反映了深层社会偏见。虽然已有多种缓解方法，但各种方法的比较效果和学习动态仍不清楚。", "method": "比较分析了六种偏见缓解控制技术：提示词方法、生成后过滤、DFA-based Ctrl-G解码、监督微调(SFT)、直接偏好优化(DPO)和迭代零空间投影(INLP)。在组合约束任务上进行评估，要求为20个职业生成包含至少一个主动性和一个社群性描述符的句子。", "result": "SFT达到99.87%的合规性和高词汇多样性；DPO仅达到4.53%合规性；Ctrl-G保证完全合规但严重降低流畅性和多样性；基于偏好的方法无法满足组合约束要求。", "conclusion": "只有明确的正面监督能够缓解组合偏见，基于偏好的对齐方法无法泛化逻辑结构，这凸显了偏好学习的局限性和明确监督对于公平流畅控制生成的必要性。"}}
{"id": "2510.22192", "pdf": "https://arxiv.org/pdf/2510.22192", "abs": "https://arxiv.org/abs/2510.22192", "authors": ["Haoyang Liu", "Jie Wang", "Yuyang Cai", "Xiongwei Han", "Yufei Kuang", "Jianye Hao"], "title": "OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling", "categories": ["cs.AI"], "comment": "Published at NeurIPS 2025", "summary": "Optimization modeling is one of the most crucial but technical parts of\noperations research (OR). To automate the modeling process, existing works have\nleveraged large language models (LLMs), prompting them to break down tasks into\nsteps for generating variables, constraints, and objectives. However, due to\nthe highly complex mathematical structures inherent in OR problems, standard\nfixed-step decomposition often fails to achieve high performance. To address\nthis challenge, we introduce OptiTree, a novel tree search approach designed to\nenhance modeling capabilities for complex problems through adaptive problem\ndecomposition into simpler subproblems. Specifically, we develop a modeling\ntree that organizes a wide range of OR problems based on their hierarchical\nproblem taxonomy and complexity, with each node representing a problem category\nand containing relevant high-level modeling thoughts. Given a problem to model,\nwe recurrently search the tree to identify a series of simpler subproblems and\nsynthesize the global modeling thoughts by adaptively integrating the\nhierarchical thoughts. Experiments show that OptiTree significantly improves\nthe modeling accuracy compared to the state-of-the-art, achieving over 10\\%\nimprovements on the challenging benchmarks. The code is released at\nhttps://github.com/MIRALab-USTC/OptiTree/tree/main.", "AI": {"tldr": "OptiTree提出了一种基于树搜索的自适应问题分解方法，通过构建建模树来组织运筹学问题的层次分类和复杂度，显著提升了大型语言模型在复杂优化建模任务中的性能。", "motivation": "现有基于大型语言模型的优化建模方法采用固定的步骤分解，但由于运筹学问题具有高度复杂的数学结构，这种方法往往无法达到高性能。", "method": "开发了一个建模树，根据层次化问题分类和复杂度组织各类运筹学问题，每个节点代表一个问题类别并包含相关的高层建模思路。通过递归搜索树结构来识别更简单的子问题，并自适应整合层次化思路来合成全局建模思路。", "result": "实验显示OptiTree相比最先进方法显著提高了建模准确性，在具有挑战性的基准测试中实现了超过10%的性能提升。", "conclusion": "OptiTree通过自适应问题分解和层次化思路整合，有效解决了复杂运筹学问题的自动化建模挑战，为大型语言模型在技术性优化任务中的应用提供了新思路。"}}
{"id": "2510.22099", "pdf": "https://arxiv.org/pdf/2510.22099", "abs": "https://arxiv.org/abs/2510.22099", "authors": ["Xuanming Zhang"], "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.", "AI": {"tldr": "提出了一个统一框架来理解和控制LLMs的泛化与记忆模式，基于信息瓶颈理论开发了动态模式导向算法，显著提升了逻辑一致性和事实准确性。", "motivation": "大型语言模型存在泛化能力和机械记忆的双重性，这种不可预测性在高风险应用中降低了可靠性。", "method": "基于信息瓶颈原理建立理论模型，开发动态模式导向(DMS)算法，包含轻量级线性探针和动态激活导向机制，实现推理时的自适应控制。", "result": "在推理和真实性任务上的实验表明，DMS显著改善了逻辑一致性和事实准确性。", "conclusion": "DMS提供了一个原则性方法来增强LLM的可靠性，通过控制模型在泛化和记忆模式之间的转换。"}}
{"id": "2510.22255", "pdf": "https://arxiv.org/pdf/2510.22255", "abs": "https://arxiv.org/abs/2510.22255", "authors": ["Eunseop Yoon", "Hee Suk Yoon", "Jaehyun Jang", "SooHwan Eom", "Qi Dai", "Chong Luo", "Mark A. Hasegawa-Johnson", "Chang D. Yoo"], "title": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 14 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly\nimproved LLM reasoning, but its sparse, outcome-based reward provides no\nguidance for intermediate steps, slowing exploration. We propose Progressively\nAscending Confidence Reward (PACR), a dense, model-intrinsic reward computed\ndirectly from the model's evolving belief in the correct answer. PACR encodes\nthe inductive bias that, along a well-formed reasoning trajectory, the\nprobability of the ground-truth answer should have a generally ascending trend.\nWe provide empirical and theoretical analysis validating that such an inductive\nbias constrains the exploration search space to regions richer in logically\nsound reasoning. We demonstrate that PACR accelerates exploration, reaches\nreward saturation with fewer trajectories, and yields improvements on multiple\nbenchmarks. Our results suggest that dense, model-intrinsic shaping signals can\nmake RLVR training more effective and reliable.", "AI": {"tldr": "提出PACR方法，通过模型内在的密集奖励信号改进RLVR训练，加速探索并提高推理效果", "motivation": "RLVR的稀疏奖励无法为中间推理步骤提供指导，导致探索效率低下", "method": "使用PACR（渐进上升置信度奖励），基于模型对正确答案置信度的上升趋势构建密集的内在奖励", "result": "PACR加速了探索过程，用更少的轨迹达到奖励饱和，在多个基准测试中取得改进", "conclusion": "密集的模型内在塑造信号可以使RLVR训练更有效和可靠"}}
{"id": "2510.22109", "pdf": "https://arxiv.org/pdf/2510.22109", "abs": "https://arxiv.org/abs/2510.22109", "authors": ["Billy Dickson", "Zoran Tiganj"], "title": "Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most approaches to long-context processing increase the complexity of the\ntransformer's internal architecture by integrating mechanisms such as\nrecurrence or auxiliary memory modules. In this work, we introduce an\nalternative approach that modifies the input representation itself, rather than\nthe transformer architecture. Inspired by cognitive models of human memory, our\nmethod applies a scale-invariant logarithmic compression to the input tokens.\nThe resulting compressed representation is processed by a standard, unmodified\ntransformer, preserving architectural simplicity. We evaluate this approach on\nthe WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in\nperplexity compared to uncompressed baselines. Moreover, performance improves\nconsistently with longer compressed temporal contexts, showing that input-level\nlogarithmic compression is a simple and effective way to extend a transformer's\nlong-range memory.", "AI": {"tldr": "该论文提出了一种通过输入令牌的对数压缩而非修改Transformer架构来处理长上下文的方法，在WikiText-103和PG-19基准测试中降低了困惑度并提升了长程记忆性能", "motivation": "大多数长上下文处理方法通过集成循环或辅助记忆模块增加了Transformer架构的复杂性，需要寻找更简单的替代方案", "method": "受人类记忆认知模型启发，对输入令牌应用尺度不变的对数压缩，生成压缩表示后由标准未修改的Transformer处理", "result": "在WikiText-103和PG-19语言建模基准测试中，相比未压缩基线降低了困惑度，且随着压缩时间上下文的延长性能持续提升", "conclusion": "输入级别的对数压缩是扩展Transformer长程记忆的一种简单有效方法，保持了架构的简洁性"}}
{"id": "2510.22295", "pdf": "https://arxiv.org/pdf/2510.22295", "abs": "https://arxiv.org/abs/2510.22295", "authors": ["Quoc Anh Nguyen", "Bernard Cheng", "Kelvin Soh"], "title": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique\nchallenges due to its tonal complexity and dialectal variations, but remains\nlargely unexplored due to the lack of a dedicated dataset. Therefore, we\ncurated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising\n647 hours of songs with line-level aligned lyrics and metadata to address these\nissues. Our evaluation of current ASRbased approaches reveal significant\nlimitations, including frequent transcription errors and hallucinations in\nnon-vocal segments. To improve performance, we fine-tuned Whisper models on the\nVietLyrics dataset, achieving superior results compared to existing\nmultilingual ALT systems, including LyricWhiz. We publicly release VietLyrics\nand our models, aiming to advance Vietnamese music computing research while\ndemonstrating the potential of this approach for ALT in low-resource language\nand music.", "AI": {"tldr": "本文创建了首个越南语歌词转录数据集VietLyrics，并通过微调Whisper模型在越南语歌词转录任务上取得了优于现有系统的性能。", "motivation": "越南语歌词转录面临音调复杂和方言变异的独特挑战，且缺乏专门的数据集，导致该领域研究不足。", "method": "构建了包含647小时歌曲的大规模越南语歌词数据集VietLyrics，并基于该数据集对Whisper模型进行微调。", "result": "微调后的Whisper模型在越南语歌词转录任务上表现优于现有的多语言ALT系统（包括LyricWhiz），解决了传统ASR方法中的转录错误和幻觉问题。", "conclusion": "VietLyrics数据集和微调模型的发布将推动越南音乐计算研究，展示了该方法在低资源语言和音乐歌词转录中的潜力。"}}
{"id": "2510.22115", "pdf": "https://arxiv.org/pdf/2510.22115", "abs": "https://arxiv.org/abs/2510.22115", "authors": ["Ling-Team", "Ang Li", "Ben Liu", "Binbin Hu", "Bing Li", "Bingwei Zeng", "Borui Ye", "Caizhi Tang", "Changxin Tian", "Chao Huang", "Chao Zhang", "Chen Qian", "Chenchen Ju", "Chenchen Li", "Chengfu Tang", "Chili Fu", "Chunshao Ren", "Chunwei Wu", "Cong Zhang", "Cunyin Peng", "Dafeng Xu", "Daixin Wang", "Dalong Zhang", "Dingnan Jin", "Dingyuan Zhu", "Dongke Hu", "Fangzheng Zhao", "Feifan Wu", "Feng Zhu", "Gangshan Wang", "Haitao Zhang", "Hailin Zhao", "Hanxiao Zhang", "Hanzi Wang", "Hao Qian", "Haoyi Yu", "Heng Zhang", "Hongliang Zhang", "Hongzhi Luan", "Huirong Dong", "Huizhong Li", "Jia Li", "Jia Liu", "Jialong Zhu", "Jian Sha", "Jianping Wei", "Jiaolong Yang", "Jieyue Ma", "Jiewei Wu", "Jinjing Huang", "Jingyun Tian", "Jingyuan Zhang", "Jinquan Sun", "Juanhui Tu", "Jun Liu", "Jun Xu", "Jun Zhou", "Junjie Ou", "Junpeng Fang", "Kaihong Zhang", "Kaiqin Hu", "Ke Shi", "Kun Tang", "Kunlong Chen", "Lanyin Mei", "Lei Liang", "Lei Xu", "Libo Zhang", "Lin Ju", "Lin Yuan", "Ling Zhong", "Lintao Ma", "Lu Liu", "Lu Yu", "Lun Cai", "Meiqi Zhu", "Mengying Li", "Min Chen", "Minghao Xue", "Minghong Cai", "Mingming Yin", "Peijie Jiang", "Peilong Zhao", "Pingping Liu", "Qian Zhao", "Qing Cui", "Qingxiang Huang", "Qingyuan Yang", "Quankun Yu", "Shaowei Wei", "Shijie Lian", "Shoujian Zheng", "Shun Song", "Shungen Zhang", "Shuo Zhang", "Siyuan Li", "Song Liu", "Ting Guo", "Tong Zhao", "Wanli Gu", "Weichang Wu", "Weiguang Han", "Wenjing Fang", "Wubin Wang", "Xiang Shu", "Xiao Shi", "Xiaoshun Lan", "Xiaolu Zhang", "Xiaqing Sun", "Xin Zhao", "Xingyu Lu", "Xiong Xu", "Xudong Wang", "Xudong Wang", "Xuemin Yang", "Yajie Yang", "Yang Xiang", "Yanzhe Li", "Yi Zhang", "Yilong Wang", "Yingxue Li", "Yongzhen Guo", "Yuzhuo Fu", "Yuanyuan Wang", "Yue Yang", "Yue Yu", "Yufeng Deng", "Yun Zhang", "Yunfei Xu", "Yuqi Zhang", "Yuxiao He", "Zengke Gui", "Zhaoxin Huan", "Zhaoyang Wang", "Zhibo Zhu", "Zhihao Wang", "Zhiqiang Zhang", "Zhoufei Wang", "Zihang Zeng", "Ziqi Liu", "Zitao Xuan", "Zuoli Tang"], "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation", "categories": ["cs.CL", "cs.AI"], "comment": "Ling 2.0 Technical Report", "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.", "AI": {"tldr": "Ling 2.0是一个基于稀疏激活MoE架构的推理导向语言模型系列，包含从160亿到1万亿参数的三个模型，在计算效率上相比密集模型提升高达7倍，建立了推理精度与计算效率的新帕累托前沿。", "motivation": "基于\"每个激活都能增强推理能力\"的原则，旨在通过高稀疏性和跨尺度一致性，在统一MoE范式下构建可扩展的高效推理基础模型。", "method": "采用高稀疏MoE架构配合MTP技术、推理导向数据、中期训练CoT激活、基于强化的微调（DFT、Evo-CoT），以及全尺度FP8训练和细粒度异构流水线。", "result": "Ling-1T在万亿参数规模上建立了推理精度与计算效率的新帕累托前沿，证明稀疏激活与推理目标正确对齐时可实现可扩展的高效智能。", "conclusion": "Ling 2.0为推进未来推理和思维模型提供了一个连贯、开放且高效的基础，包括基于相同基础的Ring系列模型。"}}
{"id": "2510.22329", "pdf": "https://arxiv.org/pdf/2510.22329", "abs": "https://arxiv.org/abs/2510.22329", "authors": ["Mustafa Mert Özyılmaz"], "title": "Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows", "categories": ["cs.AI", "math.OC", "90C59, 90C27", "G.2.2; I.2.8; F.2.2"], "comment": "13 pages, 30 figures. Submitted to arXiv under categories quant-ph. A\n  revised version with quantum solver experiment results will be submitted to a\n  peer-reviewed journal", "summary": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nfundamental NP-hard optimization problem in logistics. Solving large-scale\ninstances remains computationally challenging for exact solvers. This work\nintroduces a multilevel graph coarsening and refinement framework that\naggregates customers into meta-nodes using a spatio-temporal distance metric.\nThe reduced problem is solved with classical heuristics and subsequently\nexpanded back into the original space with feasibility corrections. Preliminary\nexperiments on Solomon benchmark instances show that the proposed method\nreduces computation time while preserving or improving solution quality,\nparticularly with respect to capacity and time window constraints. The paper\nalso explores the integration of quantum-inspired optimization techniques,\nhighlighting their potential to further accelerate large-scale vehicle routing\ntasks.", "AI": {"tldr": "提出基于多级图粗化和精化的CVRPTW求解框架，通过时空距离度量聚合客户节点，显著减少计算时间同时保持或提升解质量", "motivation": "CVRPTW是物流领域的NP难优化问题，大规模实例对精确求解器计算挑战巨大，需要高效求解方法", "method": "使用多级图粗化将客户聚合成元节点，在简化问题上应用经典启发式算法，然后通过可行性校正扩展回原问题空间", "result": "在Solomon基准测试中显示计算时间减少，解质量保持或改进，特别是在容量和时间窗约束方面表现良好", "conclusion": "该方法有效提升CVRPTW求解效率，量子启发优化技术的集成展示出进一步加速大规模车辆路径任务的潜力"}}
{"id": "2510.22143", "pdf": "https://arxiv.org/pdf/2510.22143", "abs": "https://arxiv.org/abs/2510.22143", "authors": ["Tianhong Gao", "Jundong Shen", "Bei Shi", "Jiapeng Wang", "Ying Ju", "Junfeng Yao", "Jiao Ran", "Yong Zhang", "Lin Dong", "Huiyu Yu", "Tingting Ye"], "title": "OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Intelligent customer service (ICS) systems via retrieval-augmented generation\n(RAG) have been widely adopted in Web-based domains such as social platforms\nand e-commerce, achieving remarkable improvements in automation and efficiency.\nHowever, notable limitations still remain: these systems are prone to\nhallucinations and often generate rigid, mechanical responses, which can\nintroduce business risks and undermine user experience, especially in Web-based\ncustomer service interactions under the RAG scenarios. In this paper, we\nintroduce OlaMind, a human-like and hallucination-safe customer service\nframework for retrieval-augmented dialogue. Specifically, it first leverages a\nLearn-to-Think stage to learn the reasoning processes and response strategies\nfrom human experts, and then employs a Learn-to-Respond stage to perform\ncold-start supervised fine-tuning (SFT) combined with reinforcement learning\n(RL) for basic-to-hard self-refinement. Our method significantly enhances\nhuman-likeness and naturalness while effectively mitigating hallucinations and\ncritical business risks. We have conducted large-scale online A/B experiments\nin an industry-level social customer service setting, and extensive\nexperimental results show that OlaMind achieves significant cumulative relative\nimprovements with intelligent resolution rates +28.92%/+18.42% and human\ntakeover rate -6.08%/-7.12% in community-support/livestream-interaction\nscenarios, respectively, which highlights its consistent effectiveness across\ndiverse real-world applications. The code and data will be publicly available.", "AI": {"tldr": "OlaMind是一个基于检索增强生成的人类化、防幻觉客服框架，通过两阶段学习（Learn-to-Think和Learn-to-Respond）显著提升智能解决率和降低人工接管率。", "motivation": "现有检索增强生成客服系统存在幻觉问题和机械式回复，可能带来业务风险并影响用户体验。", "method": "采用两阶段方法：先学习人类专家的推理过程和响应策略（Learn-to-Think），然后结合监督微调和强化学习进行冷启动自优化（Learn-to-Respond）。", "result": "在工业级社交客服环境中，社区支持场景智能解决率提升28.92%，直播互动场景提升18.42%；人工接管率分别降低6.08%和7.12%。", "conclusion": "OlaMind框架在多样化实际应用中展现出一致有效性，显著提升人类化程度和自然性，同时有效缓解幻觉和关键业务风险。"}}
{"id": "2510.22333", "pdf": "https://arxiv.org/pdf/2510.22333", "abs": "https://arxiv.org/abs/2510.22333", "authors": ["Xiao Hu", "Yuansheng Lian", "Ke Zhang", "Yunxuan Li", "Yuelong Su", "Meng Li"], "title": "LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This study proposes an interpretable prediction framework with\nliterature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.\nThe framework integrates an LLM-driven Inference Core that predicts and\nexplains truck driving risk, a Literature Processing Pipeline that filters and\nsummarizes domain-specific literature into a literature knowledge base, and a\nResult Evaluator that evaluates the prediction performance as well as the\ninterpretability of the LIFT LLM. After fine-tuning on a real-world truck\ndriving risk dataset, the LIFT LLM achieved accurate risk prediction,\noutperforming benchmark models by 26.7% in recall and 10.1% in F1-score.\nFurthermore, guided by the literature knowledge base automatically constructed\nfrom 299 domain papers, the LIFT LLM produced variable importance ranking\nconsistent with that derived from the benchmark model, while demonstrating\nrobustness in interpretation results to various data sampling conditions. The\nLIFT LLM also identified potential risky scenarios by detecting key combination\nof variables in truck driving risk, which were verified by PERMANOVA tests.\nFinally, we demonstrated the contribution of the literature knowledge base and\nthe fine-tuning process in the interpretability of the LIFT LLM, and discussed\nthe potential of the LIFT LLM in data-driven knowledge discovery.", "AI": {"tldr": "本研究提出一个基于文献知识微调大语言模型(LIFT LLM)的可解释性卡车驾驶风险预测框架，通过整合文献知识库和微调技术，实现了准确的风险预测和可解释性分析。", "motivation": "为了解决卡车驾驶风险预测中的准确性和可解释性问题，研究旨在开发一个能够结合领域文献知识并具有强解释能力的大语言模型预测框架。", "method": "构建包含LLM驱动推理核心、文献处理管道和结果评估器的三部分框架，使用299篇领域文献构建知识库，并在真实卡车驾驶数据集上进行微调。", "result": "LIFT LLM在召回率上优于基准模型26.7%，F1分数提升10.1%，变量重要性排序与基准模型一致，并能识别通过PERMANOVA验证的关键风险变量组合。", "conclusion": "该框架成功实现了准确且可解释的卡车驾驶风险预测，证明了文献知识库和微调过程对模型可解释性的重要贡献，在数据驱动知识发现方面具有巨大潜力。"}}
{"id": "2510.22160", "pdf": "https://arxiv.org/pdf/2510.22160", "abs": "https://arxiv.org/abs/2510.22160", "authors": ["Rahul Ranjan", "Mahendra Kumar Gurve", "Anuj", "Nitin", "Yamuna Prasad"], "title": "SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Developing benchmark datasets for low-resource languages poses significant\nchallenges, primarily due to the limited availability of native linguistic\nexperts and the substantial time and cost involved in annotation. Given these\nchallenges, Maithili is still underrepresented in natural language processing\nresearch. It is an Indo-Aryan language spoken by more than 13 million people in\nthe Purvanchal region of India, valued for its rich linguistic structure and\ncultural significance. While sentiment analysis has achieved remarkable\nprogress in high-resource languages, resources for low-resource languages, such\nas Maithili, remain scarce, often restricted to coarse-grained annotations and\nlacking interpretability mechanisms. To address this limitation, we introduce a\nnovel dataset comprising 3,221 Maithili sentences annotated for sentiment\npolarity and accompanied by natural language justifications. Moreover, the\ndataset is carefully curated and validated by linguistic experts to ensure both\nlabel reliability and contextual fidelity. Notably, the justifications are\nwritten in Maithili, thereby promoting culturally grounded interpretation and\nenhancing the explainability of sentiment models. Furthermore, extensive\nexperiments using both classical machine learning and state-of-the-art\ntransformer architectures demonstrate the dataset's effectiveness for\ninterpretable sentiment analysis. Ultimately, this work establishes the first\nbenchmark for explainable affective computing in Maithili, thus contributing a\nvaluable resource to the broader advancement of multilingual NLP and\nexplainable AI.", "AI": {"tldr": "该论文针对低资源语言迈蒂利语，构建了首个包含情感极性和自然语言解释的基准数据集，通过专家标注确保质量，并验证了其在可解释情感分析中的有效性。", "motivation": "迈蒂利语作为印度超过1300万人使用的印欧语系语言，在自然语言处理研究中代表性不足，缺乏细粒度标注和可解释性机制的情感分析资源。", "method": "创建包含3221个迈蒂利语句子的数据集，由语言专家标注情感极性和提供自然语言解释（使用迈蒂利语撰写），确保标签可靠性和上下文保真度。", "result": "使用经典机器学习和最先进的transformer架构进行广泛实验，证明该数据集在可解释情感分析中的有效性。", "conclusion": "这项工作为迈蒂利语建立了首个可解释情感计算的基准，为多语言NLP和可解释AI的广泛发展贡献了宝贵资源。"}}
{"id": "2510.22340", "pdf": "https://arxiv.org/pdf/2510.22340", "abs": "https://arxiv.org/abs/2510.22340", "authors": ["Changti Wu", "Shijie Lian", "Zihao Liu", "Lei Zhang", "Laurence Tianruo Yang", "Kai Chen"], "title": "DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "The code and dataset are available at\n  \\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}", "summary": "Solid geometry problem solving demands spatial mathematical reasoning that\nintegrates spatial intelligence and symbolic reasoning. However, most existing\nmultimodal mathematical reasoning benchmarks focus primarily on 2D plane\ngeometry, rely on static datasets prone to data contamination and memorization,\nand evaluate models solely by final answers, overlooking the reasoning process.\nTo address these limitations, we introduce DynaSolidGeo, the first dynamic\nbenchmark for evaluating genuine spatial reasoning in Vision-Language Models\n(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo\ncontains 503 expert-curated seed questions that can, in principle, dynamically\ngenerate an unbounded number of diverse multimodal text-visual instances.\nBeyond answer accuracy, we incorporate process evaluation based on\nexpert-annotated reasoning chains to measure logical validity and causal\ncoherence. Experiments across representative open-source and closed-source VLMs\nreveal large performance gaps, severe degradation in dynamic settings, and poor\nperformance on tasks requiring high-level spatial intelligence, such as mental\nrotation and visualization. The code and dataset are available at\n\\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.", "AI": {"tldr": "DynaSolidGeo是首个针对视觉语言模型空间推理能力的动态基准测试，包含503个专家策划的立体几何问题，可动态生成无限多样的多模态实例，并通过过程评估来测量推理链的逻辑有效性。", "motivation": "现有数学推理基准主要关注2D平面几何、依赖静态数据集容易导致数据污染和记忆问题，且仅通过最终答案评估模型，忽略了推理过程。", "method": "通过半自动标注流程构建包含503个种子问题的数据集，可动态生成多样化多模态实例，并引入基于专家标注推理链的过程评估方法。", "result": "实验显示开源和闭源视觉语言模型存在较大性能差距，在动态设置下性能严重下降，在需要高水平空间智能的任务上表现较差。", "conclusion": "DynaSolidGeo填补了立体几何推理评估的空白，揭示了当前模型在空间推理能力上的局限性，为未来研究提供了重要基准。"}}
{"id": "2510.22212", "pdf": "https://arxiv.org/pdf/2510.22212", "abs": "https://arxiv.org/abs/2510.22212", "authors": ["Maria Korobeynikova", "Alessia Battisti", "Lukas Fischer", "Yingqiang Gao"], "title": "DETECT: Determining Ease and Textual Clarity of German Text Simplifications", "categories": ["cs.CL"], "comment": null, "summary": "Current evaluation of German automatic text simplification (ATS) relies on\ngeneral-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently\ncapture simplification quality in terms of simplicity, meaning preservation,\nand fluency. While specialized metrics like LENS have been developed for\nEnglish, corresponding efforts for German have lagged behind due to the absence\nof human-annotated corpora. To close this gap, we introduce DETECT, the first\nGerman-specific metric that holistically evaluates ATS quality across all three\ndimensions of simplicity, meaning preservation, and fluency, and is trained\nentirely on synthetic large language model (LLM) responses. Our approach adapts\nthe LENS framework to German and extends it with (i) a pipeline for generating\nsynthetic quality scores via LLMs, enabling dataset creation without human\nannotation, and (ii) an LLM-based refinement step for aligning grading criteria\nwith simplification requirements. To the best of our knowledge, we also\nconstruct the largest German human evaluation dataset for text simplification\nto validate our metric directly. Experimental results show that DETECT achieves\nsubstantially higher correlations with human judgments than widely used ATS\nmetrics, with particularly strong gains in meaning preservation and fluency.\nBeyond ATS, our findings highlight both the potential and the limitations of\nLLMs for automatic evaluation and provide transferable guidelines for general\nlanguage accessibility tasks.", "AI": {"tldr": "本文提出了DETECT，第一个专门针对德语文本自动简化(ATS)的评估指标，该指标在简洁性、意义保持和流畅性三个维度上全面评估ATS质量，完全基于合成的大语言模型响应进行训练。", "motivation": "当前德语ATS评估依赖通用指标如SARI、BLEU和BERTScore，这些指标在捕捉简化质量方面不足。德语缺乏专门的评估指标和人工标注语料库。", "method": "采用LENS框架并针对德语进行适配，通过(i)基于LLM生成合成质量分数的流程，(ii)基于LLM的分级标准对齐细化步骤，构建了无需人工标注的数据集。", "result": "DETECT与人工评估的相关性显著高于广泛使用的ATS指标，在意义保持和流畅性方面表现尤为突出。", "conclusion": "研究不仅填补了德语ATS评估的空白，还揭示了LLM在自动评估方面的潜力和局限性，为通用语言可访问性任务提供了可转移的指导方针。"}}
{"id": "2510.22371", "pdf": "https://arxiv.org/pdf/2510.22371", "abs": "https://arxiv.org/abs/2510.22371", "authors": ["Revanth Rameshkumar", "Jimson Huang", "Yunxin Sun", "Fei Xia", "Abulhair Saparov"], "title": "Reasoning Models Reason Well, Until They Don't", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown significant progress in reasoning\ntasks. However, recent studies show that transformers and LLMs fail\ncatastrophically once reasoning problems exceed modest complexity. We revisit\nthese findings through the lens of large reasoning models (LRMs) -- LLMs\nfine-tuned with incentives for step-by-step argumentation and\nself-verification. LRM performance on graph and reasoning benchmarks such as\nNLGraph seem extraordinary, with some even claiming they are capable of\ngeneralized reasoning and innovation in reasoning-intensive fields such as\nmathematics, physics, medicine, and law. However, by more carefully scaling the\ncomplexity of reasoning problems, we show existing benchmarks actually have\nlimited complexity. We develop a new dataset, the Deep Reasoning Dataset\n(DeepRD), along with a generative process for producing unlimited examples of\nscalable complexity. We use this dataset to evaluate model performance on graph\nconnectivity and natural language proof planning. We find that the performance\nof LRMs drop abruptly at sufficient complexity and do not generalize. We also\nrelate our LRM results to the distributions of the complexities of large,\nreal-world knowledge graphs, interaction graphs, and proof datasets. We find\nthe majority of real-world examples fall inside the LRMs' success regime, yet\nthe long tails expose substantial failure potential. Our analysis highlights\nthe near-term utility of LRMs while underscoring the need for new methods that\ngeneralize beyond the complexity of examples in the training distribution.", "AI": {"tldr": "研究表明大语言模型在复杂推理任务上存在局限性，虽然经过微调的大推理模型在简单推理问题上表现优异，但在复杂程度增加时性能急剧下降，无法实现泛化推理。", "motivation": "重新审视大语言模型在复杂推理任务中的表现，验证其是否真正具备泛化推理能力，特别是针对现有基准测试复杂度有限的问题。", "method": "开发了Deep Reasoning Dataset (DeepRD)数据集，通过可扩展复杂度的生成过程创建无限样本，评估模型在图连接性和自然语言证明规划任务上的性能。", "result": "大推理模型在达到足够复杂度时性能急剧下降，无法泛化到训练分布之外的复杂问题，尽管多数现实世界问题仍在模型成功范围内。", "conclusion": "大推理模型在近期具有实用价值，但需要开发新方法来解决训练分布之外的复杂推理问题，以应对现实世界中的长尾复杂情况。"}}
{"id": "2510.22219", "pdf": "https://arxiv.org/pdf/2510.22219", "abs": "https://arxiv.org/abs/2510.22219", "authors": ["Tianyi Li"], "title": "Estimating the Error of Large Language Models at Pairwise Text Comparison", "categories": ["cs.CL", "cs.AI", "math.PR"], "comment": "14 pages, 6 figures", "summary": "We measure LLMs' output error at pairwise text comparison, noting the\nprobability of error in their preferences. Our method does not rely on the\nground truth and supports two scenarios: (i) uniform error rate regardless of\nthe order of comparison, estimated with two comparisons for each text pair with\neither text placed first; (ii) binary positional bias assuming distinct error\nrates for the two orders of comparison, estimated with repeated comparisons\nbetween the texts. The Copeland counting constructs a ranking over the compared\ntexts from pairwise preferences; the ranking reveals the poor scalability of\nLLM-based pairwise comparison and helps yield the estimates for LLMs' error\nrates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,\nGrok, Qwen) with five types of text input and obtain consistent estimates of\nLLMs' error. In general, the measured two positional bias terms are similar,\nclose to the uniform error. Considering both the error rates and the robustness\nto the variation of prompts, Claude obtained the most desirable performance in\nthis experiment. Our model outperforms the biased Bradley-Terry model and the\ncommutativity score in indicating LLMs' error at this task.", "AI": {"tldr": "本研究提出了一种无需真实标签的方法来测量LLM在成对文本比较中的错误率，通过Copeland计数构建排名并发现LLM比较的可扩展性差，在测试6个主流LLM后发现Claude表现最佳。", "motivation": "需要量化评估大型语言模型在成对文本比较任务中的错误概率，特别是位置偏差对比较结果的影响，而现有方法依赖于真实标签或无法准确测量错误率。", "method": "提出两种场景的误差估计方法：(i)统一错误率，通过每个文本对两次比较估计；(ii)二元位置偏差，通过重复比较估计不同顺序的错误率。使用Copeland计数从成对偏好构建文本排名。", "result": "测试了6个LLM（ChatGPT、Claude、DeepSeek、Gemini、Grok、Qwen）在五种文本类型上的表现，发现两个位置偏差项相似且接近统一错误率，Claude在错误率和提示鲁棒性方面表现最佳。", "conclusion": "该方法在指示LLM比较错误方面优于有偏Bradley-Terry模型和交换性评分，为评估LLM比较性能提供了有效工具，揭示了LLM基于成对比较的可扩展性限制。"}}
{"id": "2510.22437", "pdf": "https://arxiv.org/pdf/2510.22437", "abs": "https://arxiv.org/abs/2510.22437", "authors": ["G M Shahariar", "Ali Nazari", "Erfan Shayegani", "Nael Abu-Ghazaleh"], "title": "Modeling Hierarchical Thinking in Large Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\nwhen they generate step-by-step solutions, known as chain-of-thought (CoT)\nreasoning. When trained to using chain-of-thought reasoning examples, the\nresulting models (called Large Reasoning Models, or LRMs) appear to learn\nhierarchical thinking strategies similar to those used by humans. However,\nunderstanding LRMs emerging reasoning capabilities remains a difficult open\nproblem, with many potential important applications including improving\ntraining and understanding robustness. In this paper, we adopt a memoryless\nFinite State Machine formulation to approximate LRM's emerging hierarchical\nreasoning dynamics as a structured, interpretable abstraction. We identify a\nsmall set of discrete reasoning states including - initialization, deduction,\naugmentation-strategy, uncertainty-estimation, backtracking, and\nfinal-conclusion that capture the high-level states present in the model's\nreasoning process. By annotating each step of a model's CoT with these states,\nwe can represent the reasoning trajectory as a transition sequence through the\nstate graph. This FSM formulation provides a systematic way to analyze,\ninterpret and visualize how different models approach problems. We describe the\nFSM model, provide examples of CoT annotations under this scheme, and discuss\nhow it can shed light on differences between available models in their approach\nto reasoning. Our results demonstrate that this FSM-based analysis reveals\ndistinct reasoning patterns and potential shortcomings, offering a new lens to\nevaluate and improve LLM reasoning.", "AI": {"tldr": "该论文提出使用有限状态机(FSM)模型来分析大型推理模型(LRMs)的思维过程，将复杂的推理步骤抽象为离散状态和状态转移，为理解LLM的推理机制提供了结构化框架。", "motivation": "虽然大型语言模型在链式思维推理方面表现出色，但其内部推理能力的涌现机制仍然难以理解，需要开发可解释的分析方法来理解模型的推理动态和层次化思维策略。", "method": "采用无记忆有限状态机(FSM)公式化方法，将LRM的推理过程抽象为6个离散状态(初始化、演绎、增强策略、不确定性估计、回溯、最终结论)，通过标注CoT步骤的状态转换来分析推理轨迹。", "result": "FSM模型能够系统性地分析和可视化不同模型的推理方式，揭示了不同的推理模式和潜在缺陷，为模型评估和改进提供了新视角。", "conclusion": "基于FSM的分析方法为理解和解释LLM推理能力提供了有价值的框架，有助于改进模型训练和增强推理鲁棒性，是解决LLM推理机制理解难题的重要进展。"}}
{"id": "2510.22220", "pdf": "https://arxiv.org/pdf/2510.22220", "abs": "https://arxiv.org/abs/2510.22220", "authors": ["Maurizio Serva"], "title": "Evolution of the lexicon: a probabilistic point of view", "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "The Swadesh approach for determining the temporal separation between two\nlanguages relies on the stochastic process of words replacement (when a\ncomplete new word emerges to represent a given concept). It is well known that\nthe basic assumptions of the Swadesh approach are often unrealistic due to\nvarious contamination phenomena and misjudgments (horizontal transfers,\nvariations over time and space of the replacement rate, incorrect assessments\nof cognacy relationships, presence of synonyms, and so on). All of this means\nthat the results cannot be completely correct.\n  More importantly, even in the unrealistic case that all basic assumptions are\nsatisfied, simple mathematics places limits on the accuracy of estimating the\ntemporal separation between two languages. These limits, which are purely\nprobabilistic in nature and which are often neglected in lexicostatistical\nstudies, are analyzed in detail in this article.\n  Furthermore, in this work we highlight that the evolution of a language's\nlexicon is also driven by another stochastic process: gradual lexical\nmodification of words. We show that this process equally also represents a\nmajor contribution to the reshaping of the vocabulary of languages over the\ncenturies and we also show, from a purely probabilistic perspective, that\ntaking into account this second random process significantly increases the\nprecision in determining the temporal separation between two languages.", "AI": {"tldr": "本文批判性分析了Swadesh方法在语言年代学中的局限性，指出即使满足所有假设也存在数学精度限制，并提出词汇渐进修改过程作为重要补充因素，可显著提高时间分离估计的精度。", "motivation": "Swadesh方法基于词汇替换的随机过程来估计语言分离时间，但其基本假设常因污染现象和误判而不现实，且即使假设完美满足，数学上也存在固有的精度限制，这些限制在以往研究中常被忽视。", "method": "从纯概率角度详细分析Swadesh方法的数学精度限制，并引入词汇渐进修改这一第二随机过程，通过概率建模展示其对词汇演变的重要贡献。", "result": "研究发现Swadesh方法存在固有的概率性精度限制，同时证实词汇渐进修改过程是词汇演变的重要驱动力，考虑此过程可显著提高时间分离估计的精确度。", "conclusion": "语言年代学研究需要同时考虑词汇替换和渐进修改两个随机过程，单纯依赖Swadesh方法存在根本性局限，结合两种过程可提供更准确的语言分离时间估计。"}}
{"id": "2510.22462", "pdf": "https://arxiv.org/pdf/2510.22462", "abs": "https://arxiv.org/abs/2510.22462", "authors": ["Abhijnan Nath", "Nikhil Krishnaswamy"], "title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly bring deployed in agentic\nsettings where they act as collaborators with humans. Therefore, it is\nincreasingly important to be able to evaluate their abilities to collaborate\neffectively in multi-turn, multi-party tasks. In this paper, we build on the AI\nalignment and safe interruptability literature to offer novel theoretical\ninsights on collaborative behavior between LLM-driven collaborator agents and\nan intervention agent. Our goal is to learn an ideal partner-aware collaborator\nthat increases the group's common-ground (CG)-alignment on task-relevant\npropositions-by intelligently collecting information provided in interventions\nby a partner agent.We show how LLM agents trained using standard RLHF and\nrelated approaches are naturally inclined to ignore possibly well-meaning\ninterventions, which makes increasing group common ground non-trivial in this\nsetting. We employ a two-player Modified-Action MDP to examine this suboptimal\nbehavior of standard AI agents, and propose Interruptible Collaborative\nRoleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal\ncollaborators. Experiments on multiple collaborative task environments show\nthat ICR, on average, is more capable of promoting successful CG convergence\nand exploring more diverse solutions in such tasks.", "AI": {"tldr": "该论文提出了一种新型的合作伙伴感知学习算法ICR，用于训练LLM驱动的协作代理，使其能够更好地接受干预并促进团队共识达成。", "motivation": "随着LLM越来越多地在代理设置中与人类协作，需要评估其在多轮多方任务中的协作能力。标准RLHF训练的LLM代理倾向于忽略干预，这使得增加团队共同基础变得困难。", "method": "使用双玩家修改动作MDP分析标准AI代理的次优行为，并提出ICR（可中断协作角色扮演）算法来训练共同基础最优的协作代理。", "result": "在多个协作任务环境中的实验表明，ICR平均能更有效地促进成功的共同基础收敛，并在任务中探索更多样化的解决方案。", "conclusion": "ICR算法通过使LLM代理更加合作伙伴感知，能够显著提高协作效果和团队共识达成能力，为LLM在代理协作场景中的应用提供了重要改进。"}}
{"id": "2510.22251", "pdf": "https://arxiv.org/pdf/2510.22251", "abs": "https://arxiv.org/abs/2510.22251", "authors": ["Imran Khan"], "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 1 figure, 6 tables. Code and experimental data available at\n  https://github.com/strongSoda/prompt-sculpting", "summary": "Prompt engineering, particularly Chain-of-Thought (CoT) prompting,\nsignificantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a\nconstrained, rule-based prompting method designed to improve upon standard CoT\nby reducing errors from semantic ambiguity and flawed common sense.\n  We evaluate three prompting strategies (Zero Shot, standard CoT, and\nSculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)\nusing the GSM8K mathematical reasoning benchmark (1,317 problems).\n  Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on\ngpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%\nvs. 96.36% for CoT on full benchmark). We trace this to a\n\"Guardrail-to-Handcuff\" transition where constraints preventing common-sense\nerrors in mid-tier models induce hyper-literalism in advanced models. Our\ndetailed error analysis demonstrates that optimal prompting strategies must\nco-evolve with model capabilities, suggesting simpler prompts for more capable\nmodels.", "AI": {"tldr": "论文提出了\"Sculpting\"提示方法，相比标准CoT能减少语义歧义和常识错误，但在不同能力模型上效果不同：在gpt-4o上表现更好，在更强的gpt-5上反而变差，揭示了提示策略需要与模型能力协同进化。", "motivation": "标准Chain-of-Thought提示方法存在语义歧义和常识推理错误的问题，需要开发更有效的提示工程方法来提升大语言模型的推理能力。", "method": "提出基于规则的约束性提示方法\"Sculpting\"，在GSM8K数学推理基准上对比了三种提示策略（Zero Shot、标准CoT、Sculpting）在三个OpenAI模型（gpt-4o-mini、gpt-4o、gpt-5）上的表现。", "result": "发现\"提示反转\"现象：Sculpting在gpt-4o上表现优于标准CoT（97% vs 93%），但在gpt-5上反而变差（94.00% vs 96.36%）。约束条件在中级模型中防止错误，在高级模型中却导致过度字面化理解。", "conclusion": "最优提示策略必须与模型能力协同进化，更强大的模型可能需要更简单的提示方法，避免过度约束导致的性能下降。"}}
{"id": "2510.22535", "pdf": "https://arxiv.org/pdf/2510.22535", "abs": "https://arxiv.org/abs/2510.22535", "authors": ["Hao Zheng", "Zirui Pang", "Ling li", "Zhijie Deng", "Yuhan Pu", "Zhaowei Zhu", "Xiaobo Xia", "Jiaheng Wei"], "title": "OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Advances in Multimodal Large Language Models (MLLMs) intensify concerns about\ndata privacy, making Machine Unlearning (MU), the selective removal of learned\ninformation, a critical necessity. However, existing MU benchmarks for MLLMs\nare limited by a lack of image diversity, potential inaccuracies, and\ninsufficient evaluation scenarios, which fail to capture the complexity of\nreal-world applications. To facilitate the development of MLLMs unlearning and\nalleviate the aforementioned limitations, we introduce OFFSIDE, a novel\nbenchmark for evaluating misinformation unlearning in MLLMs based on football\ntransfer rumors. This manually curated dataset contains 15.68K records for 80\nplayers, providing a comprehensive framework with four test sets to assess\nforgetting efficacy, generalization, utility, and robustness. OFFSIDE supports\nadvanced settings like selective unlearning and corrective relearning, and\ncrucially, unimodal unlearning (forgetting only text data). Our extensive\nevaluation of multiple baselines reveals key findings: (1) Unimodal methods\n(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning\nefficacy is largely driven by catastrophic forgetting; (3) All methods struggle\nwith \"visual rumors\" (rumors appear in the image); (4) The unlearned rumors can\nbe easily recovered and (5) All methods are vulnerable to prompt attacks. These\nresults expose significant vulnerabilities in current approaches, highlighting\nthe need for more robust multimodal unlearning solutions. The code is available\nat\n\\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.", "AI": {"tldr": "OFFSIDE是一个针对多模态大语言模型设计的机器遗忘基准测试，专注于足球转会谣言的信息遗忘评估，包含15.68K条手动标注数据，揭示了当前多模态遗忘方法的显著漏洞。", "motivation": "多模态大语言模型的发展加剧了数据隐私担忧，现有机器遗忘基准存在图像多样性不足、准确性问题和评估场景不充分等局限，无法反映真实应用的复杂性。", "method": "构建基于足球转会谣言的手动标注数据集，包含四个测试集评估遗忘效果、泛化性、实用性和鲁棒性，支持选择性遗忘、纠正再学习和单模态遗忘等高级设置。", "result": "评估发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法都难以处理视觉谣言；遗忘的谣言容易被恢复；所有方法都易受提示攻击。", "conclusion": "当前多模态遗忘方法存在重大脆弱性，需要开发更鲁棒的多模态遗忘解决方案，OFFSIDE基准为这一领域的发展提供了重要工具。"}}
{"id": "2510.22256", "pdf": "https://arxiv.org/pdf/2510.22256", "abs": "https://arxiv.org/abs/2510.22256", "authors": ["Xiaoyan Zhao", "Ming Yan", "Yilun Qiu", "Haoting Ni", "Yang Zhang", "Fuli Feng", "Hong Cheng", "Tat-Seng Chua"], "title": "SteerX: Disentangled Steering for LLM Personalization", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable success in recent years,\nenabling a wide range of applications, including intelligent assistants that\nsupport users' daily life and work. A critical factor in building such\nassistants is personalizing LLMs, as user preferences and needs vary widely.\nActivation steering, which directly leverages directions representing user\npreference in the LLM activation space to adjust its behavior, offers a\ncost-effective way to align the model's outputs with individual users. However,\nexisting methods rely on all historical data to compute the steering vector,\nignoring that not all content reflects true user preferences, which undermines\nthe personalization signal. To address this, we propose SteerX, a disentangled\nsteering method that isolates preference-driven components from\npreference-agnostic components. Grounded in causal inference theory, SteerX\nestimates token-level causal effects to identify preference-driven tokens,\ntransforms these discrete signals into a coherent description, and then\nleverages them to steer personalized LLM generation. By focusing on the truly\npreference-driven information, SteerX produces more accurate activation\nsteering vectors and enhances personalization. Experiments on two\nrepresentative steering backbone methods across real-world datasets demonstrate\nthat SteerX consistently enhances steering vector quality, offering a practical\nsolution for more effective LLM personalization.", "AI": {"tldr": "SteerX是一种基于因果推理的个性化LLM激活导向方法，通过分离偏好驱动和偏好无关的组件，提升个性化效果。", "motivation": "现有激活导向方法使用所有历史数据计算导向向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号。", "method": "基于因果推理理论，估计token级别的因果效应来识别偏好驱动的token，将这些离散信号转化为连贯描述，用于指导个性化LLM生成。", "result": "在两个代表性导向骨干方法和真实数据集上的实验表明，SteerX持续提升导向向量质量。", "conclusion": "SteerX通过专注于真正偏好驱动的信息，提供了更有效的LLM个性化实用解决方案。"}}
{"id": "2510.22590", "pdf": "https://arxiv.org/pdf/2510.22590", "abs": "https://arxiv.org/abs/2510.22590", "authors": ["Yassir Lairgi", "Ludovic Moncla", "Khalid Benabdeslem", "Rémy Cazabet", "Pierre Cléau"], "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "In today's rapidly expanding data landscape, knowledge extraction from\nunstructured text is vital for real-time analytics, temporal inference, and\ndynamic memory frameworks. However, traditional static knowledge graph (KG)\nconstruction often overlooks the dynamic and time-sensitive nature of\nreal-world data, limiting adaptability to continuous changes. Moreover, recent\nzero- or few-shot approaches that avoid domain-specific fine-tuning or reliance\non prebuilt ontologies often suffer from instability across multiple runs, as\nwell as incomplete coverage of key facts. To address these challenges, we\nintroduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that\nbuilds and continuously updates Temporal Knowledge Graphs (TKGs) from\nunstructured texts. ATOM splits input documents into minimal, self-contained\n\"atomic\" facts, improving extraction exhaustivity and stability. Then, it\nconstructs atomic TKGs from these facts while employing a dual-time modeling\nthat distinguishes when information is observed from when it is valid. The\nresulting atomic TKGs are subsequently merged in parallel. Empirical\nevaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%\nbetter stability, and over 90% latency reduction compared to baseline methods,\ndemonstrating a strong scalability potential for dynamic TKG construction.", "AI": {"tldr": "ATOM是一个从非结构化文本构建和持续更新时序知识图谱的少样本可扩展方法，通过原子事实提取和双时间建模，在提取完整性、稳定性和延迟方面显著优于基线方法", "motivation": "传统静态知识图谱忽略了数据的动态性和时效性，而现有的零/少样本方法存在不稳定性和关键事实覆盖不全的问题", "method": "将输入文档分割为最小自包含的原子事实，采用双时间建模区分信息观测时间和有效时间，并行合并构建原子时序知识图谱", "result": "相比基线方法，ATOM实现了约18%的完整性提升、17%的稳定性改善和超过90%的延迟减少", "conclusion": "ATOM展示了在动态时序知识图谱构建方面的强大可扩展潜力，有效解决了现有方法的局限性"}}
{"id": "2510.22264", "pdf": "https://arxiv.org/pdf/2510.22264", "abs": "https://arxiv.org/abs/2510.22264", "authors": ["Iliass Ayaou", "Denis Cavallucci"], "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7; I.2.6"], "comment": null, "summary": "Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.", "AI": {"tldr": "PatenTEB是一个针对专利文本嵌入的综合性基准测试，包含15个任务、206万个样本，解决了现有基准测试在专利领域特异性挑战方面的不足，并提出了patembed模型家族在多个任务上达到最先进性能。", "motivation": "现有基准测试无法充分捕捉专利文本嵌入的特殊挑战，如领域分层、非对称片段-文档匹配等专利特定场景。", "method": "开发PatenTEB基准测试，采用领域分层划分、领域特定难负样本挖掘；通过多任务训练开发patembed模型家族（67M-344M参数，最长4096 tokens上下文）。", "result": "patembed-base在MTEB BigPatentClustering.v2上达到0.494 V-measure（之前最佳为0.445）；patembed-large在DAPFAM上达到0.377 NDCG@100；多任务训练显著提升外部泛化能力。", "conclusion": "PatenTEB填补了专利文本嵌入基准测试的空白，patembed模型家族在多个专利相关任务上表现优异，多任务训练和领域预训练初始化带来一致优势。"}}
{"id": "2510.22594", "pdf": "https://arxiv.org/pdf/2510.22594", "abs": "https://arxiv.org/abs/2510.22594", "authors": ["Bingqing Song", "Jiaxiang Li", "Rong Wang", "Songtao Lu", "Mingyi Hong"], "title": "A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Pre-trained large language models have demonstrated a strong ability to learn\nfrom context, known as in-context learning (ICL). Despite a surge of recent\napplications that leverage such capabilities, it is by no means clear, at least\ntheoretically, how the ICL capabilities arise, and in particular, what is the\nprecise role played by key factors such as pre-training procedure as well as\ncontext construction. In this work, we propose a new framework to analyze the\nICL performance, for a class of realistic settings, which includes network\narchitectures, data encoding, data generation, and prompt construction process.\nAs a first step, we construct a simple example with a one-layer transformer,\nand show an interesting result, namely when the pre-train data distribution is\ndifferent from the query task distribution, a properly constructed context can\nshift the output distribution towards the query task distribution, in a\nquantifiable manner, leading to accurate prediction on the query topic. We then\nextend the findings in the previous step to a more general case, and derive the\nprecise relationship between ICL performance, context length and the KL\ndivergence between pre-train and query task distribution. Finally, we provide\nexperiments to validate our theoretical results.", "AI": {"tldr": "本文提出了一个新的理论框架来分析上下文学习(ICL)性能，通过构建简单的一层Transformer示例，揭示了当预训练数据分布与查询任务分布不同时，适当构建的上下文可以将输出分布向查询任务分布转移，并量化了ICL性能、上下文长度以及预训练与查询任务分布之间KL散度的精确关系。", "motivation": "尽管大型预训练语言模型展现出了强大的上下文学习能力，但理论上尚不清楚这种能力是如何产生的，特别是预训练过程和上下文构建等关键因素的确切作用。", "method": "构建了一个包含网络架构、数据编码、数据生成和提示构建过程的现实设置框架。首先使用单层Transformer构建简单示例，然后扩展到更一般情况，并进行实验验证。", "result": "研究发现当预训练数据分布与查询任务分布不同时，适当构建的上下文可以量化地将输出分布向查询任务分布转移，从而在查询主题上实现准确预测。", "conclusion": "研究提供了一个理论框架来理解ICL能力的产生机制，揭示了上下文在弥合预训练分布与目标任务分布差异中的关键作用，为理解和改进上下文学习提供了理论基础。"}}
{"id": "2510.22272", "pdf": "https://arxiv.org/pdf/2510.22272", "abs": "https://arxiv.org/abs/2510.22272", "authors": ["Tu Anh Dinh", "Philipp Nicolas Schumacher", "Jan Niehues"], "title": "From Slides to Chatbots: Enhancing Large Language Models with University Course Materials", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly in recent years. One\napplication of LLMs is to support student learning in educational settings.\nHowever, prior work has shown that LLMs still struggle to answer questions\naccurately within university-level computer science courses. In this work, we\ninvestigate how incorporating university course materials can enhance LLM\nperformance in this setting. A key challenge lies in leveraging diverse course\nmaterials such as lecture slides and transcripts, which differ substantially\nfrom typical textual corpora: slides also contain visual elements like images\nand formulas, while transcripts contain spoken, less structured language. We\ncompare two strategies, Retrieval-Augmented Generation (RAG) and Continual\nPre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture\nslides, we further explore a multi-modal RAG approach, where we present the\nretrieved content to the generator in image form. Our experiments reveal that,\ngiven the relatively small size of university course materials, RAG is more\neffective and efficient than CPT. Moreover, incorporating slides as images in\nthe multi-modal setting significantly improves performance over text-only\nretrieval. These findings highlight practical strategies for developing AI\nassistants that better support learning and teaching, and we hope they inspire\nsimilar efforts in other educational contexts.", "AI": {"tldr": "研究发现将大学课程材料（特别是包含视觉元素的幻灯片）通过多模态检索增强生成(RAG)方式整合到LLM中，比持续预训练(CPT)更有效地提升LLM在计算机科学课程问答中的表现。", "motivation": "LLMs在教育场景中支持学生学习时，在大学计算机科学课程问答中仍存在准确性问题，需要探索如何通过整合课程材料来提升性能。", "method": "比较两种策略：检索增强生成(RAG)和持续预训练(CPT)。针对包含视觉元素的幻灯片，还探索了多模态RAG方法，将检索内容以图像形式呈现给生成器。", "result": "RAG比CPT更有效且高效；在多模态设置中将幻灯片作为图像整合显著优于纯文本检索的性能。", "conclusion": "这些发现为开发更好支持学习和教学的AI助手提供了实用策略，并鼓励在其他教育场景中进行类似努力。"}}
{"id": "2510.22609", "pdf": "https://arxiv.org/pdf/2510.22609", "abs": "https://arxiv.org/abs/2510.22609", "authors": ["Md. Mehedi Hasan", "Rafid Mostafiz", "Md. Abir Hossain", "Bikash Kumar Paul"], "title": "CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation", "categories": ["cs.AI"], "comment": "13 pages, 9 figures. Preprint version under review in the area of\n  Artificial Intelligence (cs.CR)", "summary": "Accurate symptom-to-disease classification and clinically grounded treatment\nrecommendations remain challenging, particularly in heterogeneous patient\nsettings with high diagnostic risk. Existing large language model (LLM)-based\nsystems often lack medical grounding and fail to quantify uncertainty,\nresulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid\npipeline that integrates multimodal patient encoding, uncertainty-calibrated\ndisease classification, and retrieval-augmented treatment generation. The\nframework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease\ndataset and incorporates Focal Loss with Monte Carlo Dropout to enable\nconfidence-aware predictions from free-text symptoms and structured vitals.\nLow-certainty cases (18%) are automatically flagged for expert review, ensuring\nhuman oversight. For treatment generation, CLIN-LLM employs Biomedical\nSentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample\nMedDialog corpus. The retrieved evidence and patient context are fed into a\nfine-tuned FLAN-T5 model for personalized treatment generation, followed by\npost-processing with RxNorm for antibiotic stewardship and drug-drug\ninteraction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,\noutperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval\nprecision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic\nsuggestions are reduced by 67% compared to GPT-5. These results demonstrate\nCLIN-LLM's robustness, interpretability, and clinical safety alignment. The\nproposed system provides a deployable, human-in-the-loop decision support\nframework for resource-limited healthcare environments. Future work includes\nintegrating imaging and lab data, multilingual extensions, and clinical trial\nvalidation.", "AI": {"tldr": "CLIN-LLM是一个安全约束的混合AI系统，通过多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，实现了98%的准确率，比现有模型提升7.1%，并显著减少67%的不安全抗生素建议。", "motivation": "现有基于大语言模型的医疗系统缺乏医学基础且无法量化不确定性，导致输出不安全，特别是在诊断风险高的异质患者环境中。", "method": "使用BioBERT在1200个临床病例上微调，结合Focal Loss和蒙特卡洛Dropout进行置信度感知预测；利用Biomedical Sentence-BERT从26万样本的MedDialog语料库检索相关对话；使用微调的FLAN-T5模型生成个性化治疗，并通过RxNorm进行抗生素管理和药物相互作用筛查。", "result": "达到98%的准确率和F1分数，比ClinicalBERT提升7.1%（p<0.001），78%的top-5检索精度，临床医生评分4.2/5，不安全抗生素建议减少67%。", "conclusion": "CLIN-LLM展示了强大的鲁棒性、可解释性和临床安全性，为资源有限的医疗环境提供了可部署的人机协同决策支持框架。"}}
{"id": "2510.22285", "pdf": "https://arxiv.org/pdf/2510.22285", "abs": "https://arxiv.org/abs/2510.22285", "authors": ["Andrei Baroian"], "title": "Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER", "categories": ["cs.CL", "cs.AI"], "comment": "Work done in November - December 2024", "summary": "We study clinical Named Entity Recognition (NER) on the CADEC corpus and\ncompare three families of approaches: (i) BERT-style encoders (BERT Base,\nBioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context\nlearning (ICL) under simple vs.\\ complex prompts, and (iii) GPT-4o with\nsupervised fine-tuning (SFT). All models are evaluated on standard NER metrics\nover CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).\nRoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,\nshowing the limit of these family of models. Among LLM settings, simple ICL\noutperforms a longer, instruction-heavy prompt, and SFT achieves the strongest\noverall performance (F1 $\\approx$ 87.1%), albeit with higher cost. We find that\nthe LLM achieve higher accuracy on simplified tasks, restricting classification\nto two labels.", "AI": {"tldr": "论文比较了三种临床命名实体识别方法：BERT编码器、GPT-4o少样本上下文学习和GPT-4o监督微调，发现监督微调效果最佳但成本高，简单提示优于复杂提示", "motivation": "研究临床命名实体识别任务，比较不同模型家族在CADEC语料库上的性能表现，探索大语言模型在医疗NER任务中的应用潜力", "method": "使用CADEC语料库评估三种方法：(i) BERT类编码器(BERT Base, BioClinicalBERT, RoBERTa-large)；(ii) GPT-4o少样本上下文学习(简单vs复杂提示)；(iii) GPT-4o监督微调；评估五个实体类型(ADR、药物、疾病、症状、发现)", "result": "RoBERTa-large和BioClinicalBERT相比BERT Base改进有限；简单提示优于复杂提示；监督微调获得最佳性能(F1≈87.1%)但成本较高；LLM在简化任务(二分类)上准确率更高", "conclusion": "监督微调是临床NER任务的最有效方法，但需权衡成本效益；大语言模型在简化任务中表现更好；提示设计对少样本学习效果有重要影响"}}
{"id": "2510.22626", "pdf": "https://arxiv.org/pdf/2510.22626", "abs": "https://arxiv.org/abs/2510.22626", "authors": ["Adhyayan Veer Singh", "Aaron Shen", "Brian Law", "Ahmed Ismail", "Jonas Rohweder", "Sean O'Brien", "Kevin Zhu"], "title": "SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming", "categories": ["cs.AI"], "comment": null, "summary": "Correctness alone is insufficient: LLM-generated programs frequently satisfy\nunit tests while violating contest time or memory budgets. We present\nSwiftSolve, a complexity-aware multi-agent system for competitive programming\nthat couples algorithmic planning with empirical profiling and\ncomplexity-guided repair. We frame competitive programming as a software\nenvironment where specialized agents act as programmers, each assuming roles\nsuch as planning, coding, profiling, and complexity analysis. A Planner\nproposes an algorithmic sketch; a deterministic Static Pruner filters high-risk\nplans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on\na fixed input-size schedule to record wall time and peak memory; and a\nComplexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a\ncomplexity class and dispatch targeted patches to either the Planner or Coder.\nAgents communicate via typed, versioned JSON; a controller enforces iteration\ncaps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10\nCodeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains\npass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with\nmarginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate\nrun-level success is 73.08% at 12.40 s mean. Failures are predominantly\nresource-bound, indicating inefficiency rather than logic errors. Against\nClaude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at\napproximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness\n(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence\nof TLE or MLE, and complexity fit accuracy on BigO), demonstrating that\nprofiling and complexity-guided replanning reduce inefficiency while preserving\naccuracy.", "AI": {"tldr": "SwiftSolve是一个复杂度感知的多智能体系统，专门用于竞争性编程，通过算法规划、性能分析和复杂度指导的修复来确保程序不仅正确，还要满足时间和内存约束。", "motivation": "现有LLM生成的程序虽然能通过单元测试，但经常违反比赛的时间和内存限制，需要一种能同时保证正确性和效率的解决方案。", "method": "采用多智能体系统架构，包括规划器、静态修剪器、编码器、性能分析器和复杂度分析器，通过JSON通信和迭代控制机制协作工作。", "result": "在26个问题上测试，首尝试通过率61.54%，3次内解决率80.77%，平均运行时间12.40秒，整体运行成功率73.08%。", "conclusion": "SwiftSolve通过性能分析和复杂度指导的重新规划显著提高了程序效率，在保持准确性的同时减少了资源超限问题，相比Claude Opus 4有显著改进。"}}
{"id": "2510.22317", "pdf": "https://arxiv.org/pdf/2510.22317", "abs": "https://arxiv.org/abs/2510.22317", "authors": ["Antal van den Bosch", "Ainhoa Risco Patón", "Teun Buijse", "Peter Berck", "Maarten van Gompel"], "title": "Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling", "categories": ["cs.CL"], "comment": "15 pages, 11 figures", "summary": "We present memory-based language modeling as an efficient, eco-friendly\nalternative to deep neural network-based language modeling. It offers\nlog-linearly scalable next-token prediction performance and strong memorization\ncapabilities. Implementing fast approximations of k-nearest neighbor\nclassification, memory-based language modeling leaves a relatively small\necological footprint both in training and in inference mode, as it relies fully\non CPUs and attains low token latencies. Its internal workings are simple and\nfully transparent. We compare our implementation of memory-based language\nmodeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,\nestimated emissions and speeds, and offer some deeper analyses of the model.", "AI": {"tldr": "本文提出了一种基于内存的语言建模方法OLIFANT，作为深度神经网络语言建模的高效环保替代方案，具有对数线性可扩展的预测性能和强记忆能力。", "motivation": "寻求一种比深度神经网络更高效、更环保的语言建模方法，减少训练和推理过程中的生态足迹。", "method": "采用基于内存的语言建模方法，实现k近邻分类的快速近似，完全依赖CPU运行，实现低延迟的token处理。", "result": "OLIFANT在下一个token预测准确率、排放估算和速度方面与GPT-2和GPT-Neo进行了比较，显示出竞争力。", "conclusion": "基于内存的语言建模是一种简单透明、生态友好的高效替代方案，在保持性能的同时显著降低了环境影响。"}}
{"id": "2510.22679", "pdf": "https://arxiv.org/pdf/2510.22679", "abs": "https://arxiv.org/abs/2510.22679", "authors": ["Yuval Kainan", "Shaked Zychlinski"], "title": "Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration", "categories": ["cs.AI", "cs.CL"], "comment": "13 pages, 4 figures", "summary": "Large Language Models (LLMs) often expend significant computational resources\ngenerating boilerplate responses, such as refusals, simple acknowledgements and\ncasual greetings, which adds unnecessary cost and latency. To address this\ninefficiency, we propose a simple yet highly effective method for detecting\nsuch responses after only a single generation step. We demonstrate that the\nlog-probability distribution of the first generated token serves as a powerful\nsignal for classifying the nature of the entire subsequent response. Our\nexperiments, conducted across a diverse range of small, large, and\nreasoning-specialized models, show that the first-token log-probability vectors\nform distinctly separable clusters for different response types. Using a\nlightweight k-NN classifier, we achieve high accuracy in predicting whether a\nresponse will be a substantive answer or a form of boilerplate response,\nincluding user-specified refusals. The primary implication is a practical,\ncomputationally trivial technique, optimizing LLM inference by enabling early\ntermination or redirection to a smaller model, thereby yielding significant\nsavings in computational cost. This work presents a direct path toward more\nefficient and sustainable LLM deployment.", "AI": {"tldr": "提出基于首个生成token的对数概率分布来检测LLM模板化响应的方法，可在单步生成后实现高效分类，显著降低计算成本和延迟", "motivation": "LLMs在生成模板化响应（如拒绝、简单确认和问候）时消耗大量计算资源，增加了不必要的成本和延迟", "method": "使用首个生成token的对数概率分布作为信号，通过轻量级k-NN分类器预测响应类型（实质性回答vs模板化响应）", "result": "实验表明首个token的对数概率向量能形成明显可分离的聚类，实现高精度分类，支持早期终止或重定向到小模型", "conclusion": "该方法提供了一种计算简单的实用技术，可优化LLM推理效率，为更高效和可持续的LLM部署提供直接路径"}}
{"id": "2510.22334", "pdf": "https://arxiv.org/pdf/2510.22334", "abs": "https://arxiv.org/abs/2510.22334", "authors": ["Ethan Mines", "Bonnie Dorr"], "title": "Multilingual Target-Stance Extraction", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures, Submitted to the Fifteenth Language Resources\n  and Evaluation Conference (LREC 2026)", "summary": "Social media enables data-driven analysis of public opinion on contested\nissues. Target-Stance Extraction (TSE) is the task of identifying the target\ndiscussed in a document and the document's stance towards that target. Many\nworks classify stance towards a given target in a multilingual setting, but all\nprior work in TSE is English-only. This work introduces the first multilingual\nTSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and\nSpanish corpora. It manages to extend the original TSE pipeline to a\nmultilingual setting without requiring separate models for each language. Our\nmodel pipeline achieves a modest F1 score of 12.78, underscoring the increased\ndifficulty of the multilingual task relative to English-only setups and\nhighlighting target prediction as the primary bottleneck. We are also the first\nto demonstrate the sensitivity of TSE's F1 score to different target\nverbalizations. Together these serve as a much-needed baseline for resources,\nalgorithms, and evaluation criteria in multilingual TSE.", "AI": {"tldr": "本文提出了首个多语言目标立场提取（TSE）基准，涵盖6种语言，展示了在无需为每种语言单独建模的情况下扩展TSE流程的可行性，但性能显著低于英语单语言设置。", "motivation": "社交媒体数据驱动的争议问题公众意见分析需要多语言支持，但现有TSE研究仅限于英语，缺乏多语言基准和评估标准。", "method": "将原始TSE流程扩展到多语言环境，涵盖加泰罗尼亚语、爱沙尼亚语、法语、意大利语、普通话和西班牙语语料库，无需为每种语言单独训练模型。", "result": "模型获得了12.78的F1分数，表明多语言任务相比英语单语言设置难度显著增加，目标预测是主要瓶颈。", "conclusion": "这项工作为多语言TSE提供了急需的资源、算法和评估基准，并首次证明了TSE的F1分数对不同目标表述方式的敏感性。"}}
{"id": "2510.22702", "pdf": "https://arxiv.org/pdf/2510.22702", "abs": "https://arxiv.org/abs/2510.22702", "authors": ["Mithul Chander", "Sai Pragnya Ranga", "Prathamesh Mayekar"], "title": "Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring", "categories": ["cs.AI", "cs.CV", "cs.ET", "eess.IV"], "comment": "An abridged version of this paper will be presented at and appear in\n  the Proceedings of ACM IKDD CODS 2025", "summary": "We introduce the {\\em Atlas Urban Index} (AUI), a metric for measuring urban\ndevelopment computed using Sentinel-2 \\citep{spoto2012sentinel2} satellite\nimagery. Existing approaches, such as the {\\em Normalized Difference Built-up\nIndex} (NDBI), often struggle to accurately capture urban development due to\nfactors like atmospheric noise, seasonal variation, and cloud cover. These\nlimitations hinder large-scale monitoring of human development and\nurbanization. To address these challenges, we propose an approach that\nleverages {\\em Vision-Language Models }(VLMs) to provide a development score\nfor regions. Specifically, we collect a time series of Sentinel-2 images for\neach region. Then, we further process the images within fixed time windows to\nget an image with minimal cloud cover, which serves as the representative image\nfor that time window. To ensure consistent scoring, we adopt two strategies:\n(i) providing the VLM with a curated set of reference images representing\ndifferent levels of urbanization, and (ii) supplying the most recent past image\nto both anchor temporal consistency and mitigate cloud-related noise in the\ncurrent image. Together, these components enable AUI to overcome the challenges\nof traditional urbanization indices and produce more reliable and stable\ndevelopment scores. Our qualitative experiments on Bangalore suggest that AUI\noutperforms standard indices such as NDBI.", "AI": {"tldr": "本文提出了Atlas城市指数(AUI)，一种基于Sentinel-2卫星影像的新城市发展度量指标，通过视觉语言模型(VLMs)处理时间序列影像数据，克服传统NDBI指标在大气噪声、季节变化和云层覆盖方面的局限性。", "motivation": "现有方法如NDBI在准确捕捉城市发展方面存在困难，受大气噪声、季节变化和云层覆盖等因素影响，阻碍了大规模人类发展和城市化的监测。", "method": "收集每个区域的Sentinel-2时间序列影像，在固定时间窗口内处理影像以获得最小云层覆盖的代表性图像；采用两种策略确保评分一致性：(i)提供代表不同城市化水平的参考图像集，(ii)提供最近历史图像以保持时间一致性和减轻云相关噪声。", "result": "在班加罗尔的定性实验表明，AUI优于NDBI等标准指标。", "conclusion": "AUI能够克服传统城市化指数的挑战，产生更可靠和稳定的发展评分，为大规模城市发展监测提供了有效解决方案。"}}
{"id": "2510.22344", "pdf": "https://arxiv.org/pdf/2510.22344", "abs": "https://arxiv.org/abs/2510.22344", "authors": ["Mohammad Aghajani Asl", "Majid Asgari-Bidhendi", "Behrooz Minaei-Bidgoli"], "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "comment": "30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop\n  Question Answering, Faithfulness", "summary": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and\nknowledge staleness in Large Language Models (LLMs), existing frameworks often\nfalter on complex, multi-hop queries that require synthesizing information from\ndisparate sources. Current advanced RAG methods, employing iterative or\nadaptive strategies, lack a robust mechanism to systematically identify and\nfill evidence gaps, often propagating noise or failing to gather a\ncomprehensive context. We introduce FAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a dynamic, evidence-driven reasoning\nprocess. At its core is an Iterative Refinement Cycle governed by a module we\nterm Structured Evidence Assessment (SEA). The SEA acts as an analytical gating\nmechanism: it deconstructs the initial query into a checklist of required\nfindings and audits the aggregated evidence to identify confirmed facts and,\ncritically, explicit informational gaps. These gaps provide a precise signal to\nan Adaptive Query Refinement agent, which generates new, targeted sub-queries\nto retrieve missing information. This cycle repeats until the evidence is\nverified as sufficient, ensuring a comprehensive context for a final, strictly\nfaithful generation. We conducted experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified\nexperimental setup, FAIR-RAG significantly outperforms strong baselines. On\nHotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3\npoints over the strongest iterative baseline -- establishing a new\nstate-of-the-art for this class of methods on these benchmarks. Our work\ndemonstrates that a structured, evidence-driven refinement process with\nexplicit gap analysis is crucial for unlocking reliable and accurate reasoning\nin advanced RAG systems for complex, knowledge-intensive tasks.", "AI": {"tldr": "FAIR-RAG是一个新型的代理框架，通过结构化证据评估和自适应查询优化机制，显著提升了复杂多跳查询中的RAG性能，在多个基准测试中实现了最先进的结果。", "motivation": "现有的RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统性识别和填补证据空白的机制，容易传播噪声或无法收集全面上下文。", "method": "提出FAIR-RAG框架，包含结构化证据评估(SEA)模块作为分析门控机制，将查询分解为所需发现的清单，识别已确认事实和明确信息空白，通过自适应查询优化代理生成针对性子查询来检索缺失信息，形成迭代优化循环。", "result": "在HotpotQA、2WikiMultiHopQA和MusiQue等挑战性多跳QA基准测试中，FAIR-RAG显著优于强基线方法，在HotpotQA上达到0.453的F1分数，比最强的迭代基线绝对提升8.3个百分点。", "conclusion": "研究表明，具有明确空白分析的结构化证据驱动优化过程对于在复杂知识密集型任务中实现可靠准确的RAG推理至关重要。"}}
{"id": "2510.22710", "pdf": "https://arxiv.org/pdf/2510.22710", "abs": "https://arxiv.org/abs/2510.22710", "authors": ["Kaitong Cai", "Jusheng Zhang", "Yijia Fan", "Jing Yang", "Keze Wang"], "title": "RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) faces a core bottleneck with\nknowledge-sparse and semantically ambiguous long-tail queries, where retrieval\nnoise distorts reasoning and necessitates costly post-processing. To tackle\nthis, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel\nframework that shifts contrastive thinking to the pre-retrieval stage. By\nautomatically generating a semantically adjacent yet differently answered\ncontrastive question and extracting a $\\Delta$-Prompt to capture their key\ndifferences, RaCoT guides the model to proactively focus on the ``critical\ndetails that determine answer divergence.\" This approach allows it to suppress\nsemantic interference within a single retrieval pass, overcoming the\ntheoretical bottleneck of single-vector queries that struggle to simultaneously\nencode signals for what to attend to and what to ignore. On six authoritative\nbenchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong\nbaselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits\nsuperior robustness, with a performance drop of only 8.6\\% in adversarial\ntests, far surpassing the over 15\\% degradation in other methods. Furthermore,\nits low latency (3.12s) and token overhead (11.54) place it on the\naccuracy-efficiency Pareto frontier, while ablation studies validate the\nnecessity of each component. Ultimately, RaCoT reframes the RAG paradigm from\n``post-hoc context cleaning\" to ``a priori shaping of discriminative\nreasoning\", offering an efficient and robust path toward reliable AI systems\nfor real-time, resource-constrained deployments.", "AI": {"tldr": "RaCoT是一个新的RAG框架，通过在检索前阶段生成对比性问题来指导模型关注关键差异，从而在单次检索中抑制语义干扰，显著提升长尾查询的处理效果。", "motivation": "解决RAG在处理知识稀疏和语义模糊的长尾查询时面临的检索噪声问题，避免昂贵的后处理成本。", "method": "提出RaCoT框架，在检索前自动生成语义相邻但答案不同的对比问题，提取Δ-Prompt捕获关键差异，引导模型主动关注决定答案分歧的关键细节。", "result": "在六个权威基准测试中，RaCoT比RankRAG和Self-RAG等强基线高出0.9-2.4个百分点，在对抗性测试中性能下降仅8.6%，远优于其他方法的15%以上下降，同时具有低延迟(3.12s)和低token开销(11.54)。", "conclusion": "RaCoT将RAG范式从\"事后上下文清理\"重新定义为\"先验形成判别推理\"，为实时、资源受限部署提供了高效稳健的可靠AI系统路径。"}}
{"id": "2510.22356", "pdf": "https://arxiv.org/pdf/2510.22356", "abs": "https://arxiv.org/abs/2510.22356", "authors": ["Fiaz Ahmad", "Nisar Hussain", "Amna Qasim", "Momina Hafeez", "Muhammad Usman Grigori Sidorov", "Alexander Gelbukh"], "title": "Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models", "categories": ["cs.CL"], "comment": "5 pages, 3 figuers", "summary": "Ironic identification is a challenging task in Natural Language Processing,\nparticularly when dealing with languages that differ in syntax and cultural\ncontext. In this work, we aim to detect irony in Urdu by translating an English\nIronic Corpus into the Urdu language. We evaluate ten state-of-the-art machine\nlearning algorithms using GloVe and Word2Vec embeddings, and compare their\nperformance with classical methods. Additionally, we fine-tune advanced\ntransformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),\nand Mistral, to assess the effectiveness of large-scale models in irony\ndetection. Among machine learning models, Gradient Boosting achieved the best\nperformance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3\n(8B) achieved the highest performance with an F1-score of 94.61%. These results\ndemonstrate that combining transliteration techniques with modern NLP models\nenables robust irony detection in Urdu, a historically low-resource language.", "AI": {"tldr": "该研究通过将英语讽刺语料库翻译成乌尔都语，评估了多种机器学习算法和Transformer模型在乌尔都语讽刺识别任务上的表现，发现LLaMA 3 (8B)模型取得了最佳性能，F1分数达到94.61%。", "motivation": "乌尔都语作为一种语法和文化背景与英语不同的低资源语言，其讽刺识别在自然语言处理中具有挑战性，研究旨在探索有效的讽刺检测方法。", "method": "将英语讽刺语料库翻译成乌尔都语，使用GloVe和Word2Vec词嵌入评估10种最先进的机器学习算法，并微调BERT、RoBERTa、LLaMA 2 (7B)、LLaMA 3 (8B)和Mistral等Transformer模型。", "result": "机器学习模型中梯度提升算法表现最佳（F1分数89.18%），Transformer模型中LLaMA 3 (8B)表现最优（F1分数94.61%）。", "conclusion": "结合音译技术和现代NLP模型能够在乌尔都语这种历史低资源语言中实现鲁棒的讽刺检测。"}}
{"id": "2510.22729", "pdf": "https://arxiv.org/pdf/2510.22729", "abs": "https://arxiv.org/abs/2510.22729", "authors": ["Urja Kohli", "Aditi Singh", "Arun Sharma"], "title": "Critical Insights into Leading Conversational AI Models", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.8"], "comment": "21 pages, 7 tables, 3 figures. Open-access preprint intended for\n  journal or conference submission", "summary": "Big Language Models (LLMs) are changing the way businesses use software, the\nway people live their lives and the way industries work. Companies like Google,\nHigh-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial\nto look at how each model is different in terms of performance, moral behaviour\nand usability, as these differences are based on the different ideas that built\nthem. This study compares five top LLMs: Google's Gemini, High-Flyer's\nDeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs\nthis by analysing three important factors: Performance and Accuracy, Ethics and\nBias Mitigation and Usability and Integration. It was found that Claude has\ngood moral reasoning, Gemini is better at multimodal capabilities and has\nstrong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA\nis good for open applications and ChatGPT delivers balanced performance with a\nfocus on usage. It was concluded that these models are different in terms of\nhow well they work, how easy they are to use and how they treat people\nethically, making it a point that each model should be utilised by the user in\na way that makes the most of its strengths.", "AI": {"tldr": "本研究比较了五大主流大语言模型（Gemini、DeepSeek、Claude、GPT和LLaMA）在性能准确性、伦理偏见缓解和可用性集成三个关键维度的差异表现。", "motivation": "随着大语言模型在各领域的广泛应用，不同模型基于不同理念构建，在性能、道德行为和可用性方面存在显著差异，需要系统评估以指导用户选择。", "method": "通过分析三个重要因素进行比较：性能与准确性、伦理与偏见缓解、可用性与集成能力。", "result": "Claude在道德推理方面表现优异，Gemini在多模态能力和伦理框架方面更强，DeepSeek擅长基于事实的推理，LLaMA适合开源应用，ChatGPT在整体性能和使用体验上表现均衡。", "conclusion": "不同模型在性能、易用性和伦理处理方面各具特色，用户应根据具体需求选择最适合的模型以最大化利用其优势。"}}
{"id": "2510.22369", "pdf": "https://arxiv.org/pdf/2510.22369", "abs": "https://arxiv.org/abs/2510.22369", "authors": ["Egor Kolodin", "Daria Khomich", "Nikita Savushkin", "Anastasia Ianina", "Fyodor Minkin"], "title": "GigaEmbeddings: Efficient Russian Language Embedding Model", "categories": ["cs.CL"], "comment": null, "summary": "We introduce GigaEmbeddings, a novel framework for training high-performance\nRussian-focused text embeddings through hierarchical instruction tuning of the\ndecoder-only LLM designed specifically for Russian language (GigaChat-3B). Our\nthree-stage pipeline, comprising large-scale contrastive pre-training in\nweb-scale corpora, fine-tuning with hard negatives, and multitask\ngeneralization across retrieval, classification, and clustering tasks,\naddresses key limitations of existing methods by unifying diverse objectives\nand leveraging synthetic data generation. Architectural innovations include\nbidirectional attention for contextual modeling, latent attention pooling for\nrobust sequence aggregation, and strategic pruning of 25% of transformer layers\nto enhance efficiency without compromising performance. Evaluated on the ruMTEB\nbenchmark spanning 23 multilingual tasks, GigaEmbeddings achieves\nstate-of-the-art results (69.1 avg. score), outperforming strong baselines with\na larger number of parameters.", "AI": {"tldr": "GigaEmbeddings是一个基于GigaChat-3B模型的三阶段训练框架，通过分层指令调优为俄语文本生成高性能嵌入，在ruMTEB基准测试中达到69.1分的最先进性能。", "motivation": "解决现有方法在俄语文本嵌入方面的局限性，通过统一多样化目标和利用合成数据生成来提升性能。", "method": "采用三阶段流水线：大规模对比预训练、硬负样本微调、多任务泛化（检索、分类、聚类）；架构创新包括双向注意力、潜在注意力池化、25% Transformer层剪枝。", "result": "在包含23个多语言任务的ruMTEB基准测试中获得69.1的平均分数，超越了参数更多的强基线模型，达到最先进性能。", "conclusion": "GigaEmbeddings框架通过创新的训练方法和架构优化，成功实现了俄语文本嵌入的高性能和高效率，为俄语NLP任务提供了有效的解决方案。"}}
{"id": "2510.22751", "pdf": "https://arxiv.org/pdf/2510.22751", "abs": "https://arxiv.org/abs/2510.22751", "authors": ["Piyushkumar Patel"], "title": "Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models have transformed how we interact with AI systems,\nthey suffer from a critical flaw: they confidently generate false information\nthat sounds entirely plausible. This hallucination problem has become a major\nbarrier to deploying these models in real-world applications where accuracy\nmatters. We developed a fact verification framework that catches and corrects\nthese errors in real-time by cross checking LLM outputs against multiple\nknowledge sources. Our system combines structured databases, live web searches,\nand academic literature to verify factual claims as they're generated. When we\ndetect inconsistencies, we automatically correct them while preserving the\nnatural flow of the response. Testing across various domains showed we could\nreduce hallucinations by 67% without sacrificing response quality. Domain\nexperts in healthcare, finance, and scientific research rated our corrected\noutputs 89% satisfactory a significant improvement over unverified LLM\nresponses. This work offers a practical solution for making LLMs more\ntrustworthy in applications where getting facts wrong isn't an option.", "AI": {"tldr": "开发了一个实时事实核查框架，通过多知识源交叉验证来检测和纠正LLM的幻觉问题，在保持响应质量的同时将幻觉减少67%，专家满意度达89%", "motivation": "大型语言模型会自信地生成看似合理但错误的信息（幻觉问题），这成为在准确性要求高的现实应用中部署的主要障碍", "method": "构建事实验证框架，结合结构化数据库、实时网络搜索和学术文献，在LLM生成过程中实时交叉验证事实主张，检测到不一致时自动纠正并保持回答的自然流畅性", "result": "跨多个领域测试显示幻觉减少67%，响应质量不受影响。医疗、金融和科研领域的专家对纠正后输出的满意度达89%，相比未验证的LLM响应有显著提升", "conclusion": "这项工作为在事实准确性至关重要的应用中提高LLM的可信度提供了实用解决方案"}}
{"id": "2510.22373", "pdf": "https://arxiv.org/pdf/2510.22373", "abs": "https://arxiv.org/abs/2510.22373", "authors": ["Yupeng Xie", "Zhiyang Zhang", "Yifan Wu", "Sirong Lu", "Jiayi Zhang", "Zhaoyang Yu", "Jinlin Wang", "Sirui Hong", "Bang Liu", "Chenglin Wu", "Yuyu Luo"], "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "53 pages, 26 figures, 5 tables", "summary": "Visualization, a domain-specific yet widely used form of imagery, is an\neffective way to turn complex datasets into intuitive insights, and its value\ndepends on whether data are faithfully represented, clearly communicated, and\naesthetically designed. However, evaluating visualization quality is\nchallenging: unlike natural images, it requires simultaneous judgment across\ndata encoding accuracy, information expressiveness, and visual aesthetics.\nAlthough multimodal large language models (MLLMs) have shown promising\nperformance in aesthetic assessment of natural images, no systematic benchmark\nexists for measuring their capabilities in evaluating visualizations. To\naddress this, we propose VisJudge-Bench, the first comprehensive benchmark for\nevaluating MLLMs' performance in assessing visualization aesthetics and\nquality. It contains 3,090 expert-annotated samples from real-world scenarios,\ncovering single visualizations, multiple visualizations, and dashboards across\n32 chart types. Systematic testing on this benchmark reveals that even the most\nadvanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human\nexperts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a\ncorrelation with human ratings of only 0.429. To address this issue, we propose\nVisJudge, a model specifically designed for visualization aesthetics and\nquality assessment. Experimental results demonstrate that VisJudge\nsignificantly narrows the gap with human judgment, reducing the MAE to 0.442 (a\n19.8% reduction) and increasing the consistency with human experts to 0.681 (a\n58.7% improvement) compared to GPT-5. The benchmark is available at\nhttps://github.com/HKUSTDial/VisJudgeBench.", "AI": {"tldr": "该论文提出了VisJudge-Bench，首个用于评估多模态大语言模型在可视化质量评估方面能力的综合基准，并开发了专门的可视化美学质量评估模型VisJudge，显著缩小了与人类专家判断的差距。", "motivation": "可视化质量评估具有挑战性，需要同时判断数据编码准确性、信息表达性和视觉美学。虽然多模态大语言模型在自然图像美学评估中表现良好，但缺乏系统性的基准来评估其在可视化领域的表现。", "method": "创建包含3,090个专家标注样本的VisJudge-Bench基准，涵盖32种图表类型的单图、多图和仪表盘。基于此基准测试先进MLLMs性能，并开发专门的VisJudge模型。", "result": "最先进的MLLMs（如GPT-5）与人类专家存在显著差距（MAE=0.551，相关性0.429）。VisJudge将MAE降低至0.442（减少19.8%），与人类专家一致性提升至0.681（提高58.7%）。", "conclusion": "VisJudge-Bench填补了可视化质量评估基准的空白，VisJudge模型在可视化美学评估方面显著优于通用MLLMs，为可视化质量自动评估提供了有效解决方案。"}}
{"id": "2510.22765", "pdf": "https://arxiv.org/pdf/2510.22765", "abs": "https://arxiv.org/abs/2510.22765", "authors": ["Binxiao Xu", "Junyu Feng", "Ruichuan An", "Yulin Luo", "Shilin Yan", "Hao Liang", "Ming Lu", "Wentao Zhang"], "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval", "categories": ["cs.AI"], "comment": "19 pages, 7 figures", "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.", "AI": {"tldr": "Jarvis是一个创新的个性化AI助手框架，通过个人KV-Cache检索技术，在文本和视觉token的KV-Cache中存储用户特定信息，实现了更准确的个性化问答。", "motivation": "现有方法要么学习一组概念token，要么训练VLM使用用户特定信息，但都难以作为个性化助手生成准确答案。", "method": "将用户信息总结为元数据创建文本token，从用户图像中提取独特图像块生成视觉token，存储在KV-Cache中。回答问题时会先检索相关KV-Cache以确保准确性。", "result": "Jarvis在视觉问答和纯文本任务上都取得了最先进的结果，特别是在依赖特定局部细节时能提供更准确的响应。", "conclusion": "Jarvis为个性化AI助手提供了一条实用路径，通过KV-Cache检索机制有效利用用户特定信息，显著提升了回答准确性。"}}
{"id": "2510.22395", "pdf": "https://arxiv.org/pdf/2510.22395", "abs": "https://arxiv.org/abs/2510.22395", "authors": ["Federica Gamba", "Aman Sinha", "Timothee Mickus", "Raul Vazquez", "Patanjali Bhamidipati", "Claudio Savelli", "Ahana Chattopadhyay", "Laura A. Zanella", "Yash Kankanampati", "Binesh Arakkal Remesh", "Aryan Ashok Chandramania", "Rohit Agarwal", "Chuyuan Li", "Ioana Buhnila", "Radhika Mamidi"], "title": "Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the CAP (Confabulations from ACL Publications) dataset, a\nmultilingual resource for studying hallucinations in large language models\n(LLMs) within scientific text generation. CAP focuses on the scientific domain,\nwhere hallucinations can distort factual knowledge, as they frequently do. In\nthis domain, however, the presence of specialized terminology, statistical\nreasoning, and context-dependent interpretations further exacerbates these\ndistortions, particularly given LLMs' lack of true comprehension, limited\ncontextual understanding, and bias toward surface-level generalization. CAP\noperates in a cross-lingual setting covering five high-resource languages\n(English, French, Hindi, Italian, and Spanish) and four low-resource languages\n(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated\nscientific questions and over 7000 LLM-generated answers from 16 publicly\navailable models, provided as question-answer pairs along with token sequences\nand corresponding logits. Each instance is annotated with a binary label\nindicating the presence of a scientific hallucination, denoted as a factuality\nerror, and a fluency label, capturing issues in the linguistic quality or\nnaturalness of the text. CAP is publicly released to facilitate advanced\nresearch on hallucination detection, multilingual evaluation of LLMs, and the\ndevelopment of more reliable scientific NLP systems.", "AI": {"tldr": "CAP数据集是一个多语言资源，用于研究大语言模型在科学文本生成中的幻觉现象，包含900个科学问题和7000多个模型生成的答案，覆盖9种语言，标注了事实性错误和流畅性问题。", "motivation": "科学领域中的幻觉会扭曲事实知识，而专业术语、统计推理和上下文依赖解释加剧了这种扭曲，特别是考虑到LLMs缺乏真正理解、上下文理解有限和偏向表面泛化的问题。", "method": "创建跨语言数据集，涵盖5种高资源语言和4种低资源语言，包含900个精选科学问题和16个公开可用模型生成的7000多个答案，提供问题-答案对、标记序列和对应logits，每个实例都标注了科学幻觉的二元标签和流畅性标签。", "result": "CAP数据集公开发布，包含丰富的多语言科学文本生成数据，标注了事实性错误和流畅性问题。", "conclusion": "CAP数据集有助于促进幻觉检测、LLMs的多语言评估以及更可靠的科学NLP系统的开发方面的先进研究。"}}
{"id": "2510.22780", "pdf": "https://arxiv.org/pdf/2510.22780", "abs": "https://arxiv.org/abs/2510.22780", "authors": ["Zora Zhiruo Wang", "Yijia Shao", "Omar Shaikh", "Daniel Fried", "Graham Neubig", "Diyi Yang"], "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI agents are continually optimized for tasks related to human work, such as\nsoftware engineering and professional writing, signaling a pressing trend with\nsignificant impacts on the human workforce. However, these agent developments\nhave often not been grounded in a clear understanding of how humans execute\nwork, to reveal what expertise agents possess and the roles they can play in\ndiverse workflows. In this work, we study how agents do human work by\npresenting the first direct comparison of human and agent workers across\nmultiple essential work-related skills: data analysis, engineering,\ncomputation, writing, and design. To better understand and compare\nheterogeneous computer-use activities of workers, we introduce a scalable\ntoolkit to induce interpretable, structured workflows from either human or\nagent computer-use activities. Using such induced workflows, we compare how\nhumans and agents perform the same tasks and find that: (1) While agents\nexhibit promise in their alignment to human workflows, they take an\noverwhelmingly programmatic approach across all work domains, even for\nopen-ended, visually dependent tasks like design, creating a contrast with the\nUI-centric methods typically used by humans. (2) Agents produce work of\ninferior quality, yet often mask their deficiencies via data fabrication and\nmisuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster\nand cost 90.4-96.2% less than humans, highlighting the potential for enabling\nefficient collaboration by delegating easily programmable tasks to agents.", "AI": {"tldr": "本研究首次直接比较人类和AI代理在多个工作技能上的表现，发现AI代理虽然速度快、成本低，但工作质量较差且倾向于程序化方法，与人类基于UI的方法形成对比。", "motivation": "AI代理在人类工作任务中快速发展，但缺乏对人类工作方式的深入理解，需要揭示AI代理的专业能力和在不同工作流程中的角色。", "method": "引入可扩展的工具包，从人类和AI代理的计算机使用活动中提取可解释的结构化工作流程，并在数据分析、工程、计算、写作和设计等五个领域进行直接比较。", "result": "(1) AI代理倾向于程序化方法，与人类UI中心方法形成对比；(2) AI代理工作质量较差，但会通过数据伪造和工具滥用掩盖缺陷；(3) AI代理速度快88.3%，成本低90.4-96.2%。", "conclusion": "AI代理在效率和经济性方面具有优势，但需要改进工作质量和方法论，通过将易编程任务委托给代理可以实现高效人机协作。"}}
{"id": "2510.22475", "pdf": "https://arxiv.org/pdf/2510.22475", "abs": "https://arxiv.org/abs/2510.22475", "authors": ["Xiangjue Dong", "Cong Wang", "Maria Teleki", "Millennium Bismay", "James Caverlee"], "title": "CHOIR: Collaborative Harmonization fOr Inference Robustness", "categories": ["cs.CL", "cs.AI"], "comment": "updated version", "summary": "Persona-assigned Large Language Models (LLMs) can adopt diverse roles,\nenabling personalized and context-aware reasoning. However, even minor\ndemographic perturbations in personas, such as simple pronoun changes, can\nalter reasoning trajectories, leading to divergent sets of correct answers.\nInstead of treating these variations as biases to be mitigated, we explore\ntheir potential as a constructive resource to improve reasoning robustness. We\npropose CHOIR (Collaborative Harmonization fOr Inference Robustness), a\ntest-time framework that harmonizes multiple persona-conditioned reasoning\nsignals into a unified prediction. CHOIR orchestrates a collaborative decoding\nprocess among counterfactual personas, dynamically balancing agreement and\ndivergence in their reasoning paths. Experiments on various reasoning\nbenchmarks demonstrate that CHOIR consistently enhances performance across\ndemographics, model architectures, scales, and tasks - without additional\ntraining. Improvements reach up to 26.4% for individual demographic groups and\n19.2% on average across five demographics. It remains effective even when base\npersonas are suboptimal. By reframing persona variation as a constructive\nsignal, CHOIR provides a scalable and generalizable approach to more reliable\nLLM reasoning.", "AI": {"tldr": "CHOIR是一个测试时框架，通过协调多个角色条件下的推理信号来提升LLM推理的鲁棒性，无需额外训练即可显著提高不同人口统计群体的性能表现。", "motivation": "角色分配的LLMs中，即使是简单的人称代词变化等人口统计扰动也会导致推理轨迹变化和不同的正确答案，作者希望将这些变化作为建设性资源而非需要消除的偏见来提升推理鲁棒性。", "method": "提出CHOIR框架，在反事实角色之间进行协作解码，动态平衡其推理路径中的一致性和分歧，将多个角色条件下的推理信号协调为统一的预测。", "result": "在各种推理基准测试中，CHOIR持续提升了不同人口统计群体、模型架构、规模和任务的性能，个体人口统计群体改进最高达26.4%，五个群体平均改进19.2%，即使在基础角色非最优时也有效。", "conclusion": "通过将角色变化重新定义为建设性信号，CHOIR为更可靠的LLM推理提供了一种可扩展和可泛化的方法。"}}
{"id": "2510.22781", "pdf": "https://arxiv.org/pdf/2510.22781", "abs": "https://arxiv.org/abs/2510.22781", "authors": ["Xiaofeng Zhu", "Yunshen Zhou"], "title": "Agentic Meta-Orchestrator for Multi-task Copilots", "categories": ["cs.AI"], "comment": null, "summary": "Microsoft Copilot suites serve as the universal entry point for various\nagents skilled in handling important tasks, ranging from assisting a customer\nwith product purchases to detecting vulnerabilities in corporate programming\ncode. Each agent can be powered by language models, software engineering\noperations, such as database retrieval, and internal \\& external knowledge. The\nrepertoire of a copilot can expand dynamically with new agents. This requires a\nrobust orchestrator that can distribute tasks from user prompts to the right\nagents. In this work, we propose an Agentic Meta-orchestrator (AMO) for\nhandling multiple tasks and scalable agents in copilot services, which can\nprovide both natural language and action responses. We will also demonstrate\nthe planning that leverages meta-learning, i.e., a trained decision tree model\nfor deciding the best inference strategy among various agents/models. We\nshowcase the effectiveness of our AMO through two production use cases:\nMicrosoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365\nE-Commerce Copilot advertises Microsoft products to external customers to\npromote sales success. The M365 E-Commerce Copilot provides up-to-date product\ninformation and connects to multiple agents, such as relational databases and\nhuman customer support. The code compliance copilot scans the internal DevOps\ncode to detect known and new compliance issues in pull requests (PR).", "AI": {"tldr": "微软提出了一种Agentic Meta-orchestrator (AMO)框架，用于在Copilot服务中协调多个任务和可扩展的代理，通过元学习决策树模型选择最佳推理策略，并在M365电商Copilot和代码合规Copilot两个生产用例中验证了有效性。", "motivation": "微软Copilot套件需要处理各种任务，从客户产品购买协助到企业代码漏洞检测，需要强大的编排器来将用户提示分发到正确的代理，支持动态扩展新代理。", "method": "提出了Agentic Meta-orchestrator (AMO)框架，利用元学习方法训练决策树模型来决定不同代理/模型之间的最佳推理策略，支持自然语言和动作响应。", "result": "通过两个生产用例验证：M365电商Copilot提供最新产品信息并连接多个代理（关系数据库和人工客服）；代码合规Copilot扫描DevOps代码检测已知和新的合规问题。", "conclusion": "AMO框架能够有效协调多个代理处理复杂任务，为Copilot服务提供了可扩展的编排解决方案，在实际生产环境中展示了良好的应用效果。"}}
{"id": "2510.22485", "pdf": "https://arxiv.org/pdf/2510.22485", "abs": "https://arxiv.org/abs/2510.22485", "authors": ["Siyu Liang", "Zhaxi Zerong"], "title": "The Tonogenesis Continuum in Tibetan: A Computational Investigation", "categories": ["cs.CL"], "comment": null, "summary": "Tonogenesis-the historical process by which segmental contrasts evolve into\nlexical tone-has traditionally been studied through comparative reconstruction\nand acoustic phonetics. We introduce a computational approach that quantifies\nthe functional role of pitch at different stages of this sound change by\nmeasuring how pitch manipulation affects automatic speech recognition (ASR)\nperformance. Through analysis on the sensitivity to pitch-flattening from a set\nof closely related Tibetan languages, we find evidence of a tonogenesis\ncontinuum: atonal Amdo dialects tolerate pitch removal the most, while fully\ntonal U-Tsang varieties show severe degradation, and intermediate Kham dialects\nfall measurably between these extremes. These gradient effects demonstrate how\nASR models implicitly learn the shifting functional load of pitch as languages\ntransition from consonant-based to tone-based lexical contrasts. Our findings\nshow that computational methods can capture fine-grained stages of sound change\nand suggest that traditional functional load metrics, based solely on minimal\npairs, may overestimate pitch dependence in transitional systems where\nsegmental and suprasegmental cues remain phonetically intertwined.", "AI": {"tldr": "该研究引入了一种计算方法来量化声调发生过程中音高的功能作用，通过分析藏语方言对音高平坦化的敏感性，揭示了声调发生的连续统现象。", "motivation": "传统声调发生研究主要依赖比较重建和声学语音学方法，需要更精细的计算方法来量化音高在不同声调发展阶段的功能性作用。", "method": "通过测量音高操作对自动语音识别（ASR）性能的影响，分析一组密切相关的藏语方言对音高平坦化的敏感性。", "result": "发现了声调发生的连续统：无声调的安多方言对音高移除最耐受，完全有声调的卫藏方言显示严重性能下降，而中间的康方言处于两者之间。", "conclusion": "计算方法能够捕捉语音变化的精细阶段，传统基于最小对立对的功能负荷指标可能在过渡性系统中高估音高依赖性，因为音段和超音段线索在语音上仍然交织。"}}
{"id": "2510.22814", "pdf": "https://arxiv.org/pdf/2510.22814", "abs": "https://arxiv.org/abs/2510.22814", "authors": ["Mohamed El Louadi", "Emna Ben Romdhane"], "title": "Will Humanity Be Rendered Obsolete by AI?", "categories": ["cs.AI", "I.2.0; K.4.1; K.6.m"], "comment": null, "summary": "This article analyzes the existential risks artificial intelligence (AI)\nposes to humanity, tracing the trajectory from current AI to ultraintelligence.\nDrawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent\npublications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and\nsuperintelligence. Considering machines' exponentially growing cognitive power\nand hypothetical IQs, it addresses the ethical and existential implications of\nan intelligence vastly exceeding humanity's, fundamentally alien. Human\nextinction may result not from malice, but from uncontrollable, indifferent\ncognitive superiority.", "AI": {"tldr": "这篇论文分析了人工智能对人类构成的生存风险，从当前AI发展到超智能的轨迹，探讨了AGI和超级智能的伦理与生存影响，指出人类灭绝可能源于不可控的认知优势而非恶意。", "motivation": "基于Irving J. Good和Nick Bostrom的理论工作以及近期出版物，研究人工智能指数级增长的认知能力对人类生存的潜在威胁。", "method": "通过理论分析和文献回顾，探讨人工智能从当前水平到超智能的发展轨迹，分析机器认知能力的指数增长和假设IQ水平。", "result": "揭示了超级智能可能带来的存在性风险，指出即使没有恶意，人类也可能因无法控制的认知优势而面临灭绝风险。", "conclusion": "人工智能的超智能发展对人类构成严重的存在性威胁，需要认真对待其伦理和生存影响，防范不可控的认知优势带来的风险。"}}
{"id": "2510.22489", "pdf": "https://arxiv.org/pdf/2510.22489", "abs": "https://arxiv.org/abs/2510.22489", "authors": ["Yuanhe Tian", "Junjie Liu", "Xican Yang", "Haishan Ye", "Yan Song"], "title": "Frustratingly Easy Task-aware Pruning for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 3 figures", "summary": "Pruning provides a practical solution to reduce the resources required to run\nlarge language models (LLMs) to benefit from their effective capabilities as\nwell as control their cost for training and inference. Research on LLM pruning\noften ranks the importance of LLM parameters using their magnitudes and\ncalibration-data activations and removes (or masks) the less important ones,\naccordingly reducing LLMs' size. However, these approaches primarily focus on\npreserving the LLM's ability to generate fluent sentences, while neglecting\nperformance on specific domains and tasks. In this paper, we propose a simple\nyet effective pruning approach for LLMs that preserves task-specific\ncapabilities while shrinking their parameter space. We first analyze how\nconventional pruning minimizes loss perturbation under general-domain\ncalibration and extend this formulation by incorporating task-specific feature\ndistributions into the importance computation of existing pruning algorithms.\nThus, our framework computes separate importance scores using both general and\ntask-specific calibration data, partitions parameters into shared and exclusive\ngroups based on activation-norm differences, and then fuses their scores to\nguide the pruning process. This design enables our method to integrate\nseamlessly with various foundation pruning techniques and preserve the LLM's\nspecialized abilities under compression. Experiments on widely used benchmarks\ndemonstrate that our approach is effective and consistently outperforms the\nbaselines with identical pruning ratios and different settings.", "AI": {"tldr": "本文提出了一种针对大语言模型的任务特定剪枝方法，通过结合通用和任务特定的特征分布来计算参数重要性，在压缩模型的同时保持特定任务性能。", "motivation": "传统剪枝方法主要关注保持模型生成流畅句子的能力，但忽视了在特定领域和任务上的性能表现。", "method": "分析传统剪枝的损失扰动最小化原理，将任务特定特征分布纳入重要性计算，分别使用通用和任务特定校准数据计算重要性分数，基于激活范数差异将参数分为共享组和专属组，融合分数指导剪枝过程。", "result": "在广泛使用的基准测试中，该方法在相同剪枝比例和不同设置下始终优于基线方法。", "conclusion": "该方法能有效保持大语言模型在压缩后的专门能力，并能与各种基础剪枝技术无缝集成。"}}
{"id": "2510.22832", "pdf": "https://arxiv.org/pdf/2510.22832", "abs": "https://arxiv.org/abs/2510.22832", "authors": ["Long H Dang", "David Rawlinson"], "title": "HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning", "categories": ["cs.AI", "cs.LG", "stat.ML", "68T07 (Primary) 62M45, 37N99 (Secondary)", "I.2.6; I.2.8"], "comment": "14 pages, 9 figures, 1 table", "summary": "The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities\ngiven its small size, but has only been applied to supervised, static,\nfully-observable problems. One of HRM's strengths is its ability to adapt its\ncomputational effort to the difficulty of the problem. However, in its current\nform it cannot integrate and reuse computation from previous time-steps if the\nproblem is dynamic, uncertain or partially observable, or be applied where the\ncorrect action is undefined, characteristics of many real-world problems.\n  This paper presents HRM-Agent, a variant of HRM trained using only\nreinforcement learning. We show that HRM can learn to navigate to goals in\ndynamic and uncertain maze environments. Recent work suggests that HRM's\nreasoning abilities stem from its recurrent inference process. We explore the\ndynamics of the recurrent inference process and find evidence that it is\nsuccessfully reusing computation from earlier environment time-steps.", "AI": {"tldr": "HRM-Agent是基于分层推理模型(HRM)的强化学习变体，能够在动态不确定的迷宫环境中学习导航，并成功重用先前时间步的计算。", "motivation": "虽然HRM模型在小型规模下具有出色的推理能力，但仅限于监督学习、静态和完全可观测的问题。现实世界问题通常是动态、不确定、部分可观测的，需要重用先前计算。", "method": "开发HRM-Agent，使用纯强化学习训练HRM变体，探索其在动态不确定迷宫环境中的导航能力，并分析循环推理过程的动态特性。", "result": "HRM-Agent成功学会了在动态不确定环境中导航到目标，研究显示其循环推理过程能够有效重用先前时间步的计算。", "conclusion": "HRM-Agent扩展了HRM的应用范围，证明了其在强化学习设置下处理动态不确定问题的能力，为实际应用提供了新途径。"}}
{"id": "2510.22492", "pdf": "https://arxiv.org/pdf/2510.22492", "abs": "https://arxiv.org/abs/2510.22492", "authors": ["Siyu Liang", "Nicolas Ballier", "Gina-Anne Levow", "Richard Wright"], "title": "The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR", "categories": ["cs.CL"], "comment": null, "summary": "How much audio is needed to fully observe a multilingual ASR model's learned\nsub-token inventory across languages, and does data disparity in multilingual\npre-training affect how these tokens are utilized during inference? We address\nthis question by analyzing Whisper's decoding behavior during inference across\n49 languages. By logging decoding candidate sub-tokens and tracking their\ncumulative discovery over time, we study the utilization pattern of the model's\nsub-token space. Results show that the total number of discovered tokens\nremains largely independent of a language's pre-training hours, indicating that\ndata disparity does not strongly influence lexical diversity in the model's\nhypothesis space. Sub-token discovery rates follow a consistent exponential\nsaturation pattern across languages, suggesting a stable time window after\nwhich additional audio yields minimal new sub-token activation. We refer to\nthis convergence threshold as acoustic saturation time (AST). Further analyses\nof rank-frequency distributions reveal Zipf-like patterns better modeled by a\nZipf-Mandelbrot law, and mean sub-token length shows a positive correlation\nwith resource level. Additionally, those metrics show more favorable patterns\nfor languages in the Latin script than those in scripts such as Cyrillic, CJK,\nand Semitic. Together, our study suggests that sub-token utilization during\nmultilingual ASR inference is constrained more by the statistical, typological,\nand orthographic structure of the speech than by training data scale, providing\nan empirical basis for more equitable corpus construction and cross-lingual\nevaluation.", "AI": {"tldr": "研究分析Whisper多语言ASR模型在49种语言上的解码行为，发现子词发现数量与预训练数据量无关，子词发现遵循指数饱和模式，提出了声学饱和时间(AST)概念，揭示了子词使用更多受语言统计、类型和正字法结构影响而非训练数据规模。", "motivation": "探究多语言ASR模型需要多少音频才能充分观察其学习的子词库，以及多语言预训练中的数据差异是否影响推理过程中这些子词的使用方式。", "method": "通过记录Whisper模型在49种语言推理过程中的解码候选子词，并追踪它们随时间的累积发现情况，分析模型子词空间的使用模式。", "result": "发现子词发现总数与语言预训练时长无关；子词发现率遵循一致的指数饱和模式；揭示了类似Zipf-Mandelbrot的分布规律；拉丁文字语言表现优于西里尔、中日韩和闪族文字语言。", "conclusion": "多语言ASR推理中的子词使用更多受语音的统计、类型和正字法结构约束，而非训练数据规模，这为更公平的语料构建和跨语言评估提供了实证基础。"}}
{"id": "2510.22833", "pdf": "https://arxiv.org/pdf/2510.22833", "abs": "https://arxiv.org/abs/2510.22833", "authors": ["Adrian Orenstein", "Jessica Chen", "Gwyneth Anne Delos Santos", "Bayley Sapara", "Michael Bowling"], "title": "Toward Agents That Reason About Their Computation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While reinforcement learning agents can achieve superhuman performance in\nmany complex tasks, they typically do not become more computationally efficient\nas they improve. In contrast, humans gradually require less cognitive effort as\nthey become more proficient at a task. If agents could reason about their\ncompute as they learn, could they similarly reduce their computation footprint?\nIf they could, we could have more energy efficient agents or free up compute\ncycles for other processes like planning. In this paper, we experiment with\nshowing agents the cost of their computation and giving them the ability to\ncontrol when they use compute. We conduct our experiments on the Arcade\nLearning Environment, and our results demonstrate that with the same training\ncompute budget, agents that reason about their compute perform better on 75% of\ngames. Furthermore, these agents use three times less compute on average. We\nanalyze individual games and show where agents gain these efficiencies.", "AI": {"tldr": "论文研究如何让强化学习智能体在提升性能的同时降低计算开销，通过让智能体感知计算成本并自主控制计算使用，在ALE环境中实现了75%游戏性能提升和平均3倍计算效率提升。", "motivation": "人类在任务熟练后会降低认知努力，而现有强化学习智能体在性能提升时计算效率不增反降。研究旨在让智能体能够像人类一样在提升熟练度的同时减少计算资源消耗。", "method": "在Arcade Learning Environment中实验，让智能体感知计算成本并赋予其控制计算使用的自主权，通过计算预算约束来训练智能体。", "result": "在相同训练计算预算下，75%的游戏上智能体表现更好，平均计算使用量减少三倍，同时分析了各游戏中效率提升的具体情况。", "conclusion": "让强化学习智能体感知和控制计算成本能显著提升计算效率，为开发更节能的智能体和释放计算资源用于其他进程（如规划）提供了可行路径。"}}
{"id": "2510.22495", "pdf": "https://arxiv.org/pdf/2510.22495", "abs": "https://arxiv.org/abs/2510.22495", "authors": ["Michael Scott", "Siyu Liang", "Alicia Wassink", "Gina-Anne Levow"], "title": "A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a systematic evaluation of racial bias in four major\ncommercial automatic speech recognition (ASR) systems using the Pacific\nNorthwest English (PNWE) corpus. We analyze transcription accuracy across\nspeakers from four ethnic backgrounds (African American, Caucasian American,\nChicanX, and Yakama) and examine how sociophonetic variation contributes to\ndifferential system performance. We introduce a heuristically-determined\nPhonetic Error Rate (PER) metric that links recognition errors to specific\nlinguistically motivated variables derived from sociophonetic annotation. Our\nanalysis of eleven sociophonetic features reveals that vowel quality variation,\nparticularly resistance to the low-back merger and pre-nasal merger patterns,\nis systematically associated with differential error rates across ethnic\ngroups, with the most pronounced effects for African American speakers across\nall evaluated systems. These findings demonstrate that acoustic modeling of\ndialectal phonetic variation, rather than lexical or syntactic factors, remains\na primary source of bias in commercial ASR systems. The study establishes the\nPNWE corpus as a valuable resource for bias evaluation in speech technologies\nand provides actionable guidance for improving ASR performance through targeted\nrepresentation of sociophonetic diversity in training data.", "AI": {"tldr": "该研究系统评估了四种主流商业语音识别系统在太平洋西北英语语料库中的种族偏见，发现语音变异是导致不同种族群体识别准确率差异的主要原因，特别是非裔美国人受影响最大。", "motivation": "评估商业ASR系统中存在的种族偏见，分析社会语音变异如何导致不同种族群体在语音识别性能上的差异。", "method": "使用太平洋西北英语语料库，分析四个种族群体（非裔美国人、白人美国人、奇卡诺人和雅卡马人）的转录准确性，引入启发式确定的语音错误率指标，分析11种社会语音特征。", "result": "发现元音质量变异，特别是对低后元音合并和前鼻音合并模式的抵抗，与不同种族群体的错误率差异系统相关，非裔美国人在所有评估系统中受影响最显著。", "conclusion": "方言语音变异的声学建模是商业ASR系统偏见的主要来源，研究为通过训练数据中有针对性地表示社会语音多样性来改进ASR性能提供了可行指导。"}}
{"id": "2510.22836", "pdf": "https://arxiv.org/pdf/2510.22836", "abs": "https://arxiv.org/abs/2510.22836", "authors": ["Guanyu Yao", "Qiucheng Wu", "Yang Zhang", "Zhaowen Wang", "Handong Zhao", "Shiyu Chang"], "title": "Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ncapabilities on vision-and-language tasks. However, recent findings reveal an\nimbalance in their reasoning capabilities across visual and textual modalities.\nSpecifically, current MLLMs often over-rely on textual cues while\nunder-attending to visual content, resulting in suboptimal performance on tasks\nthat require genuine visual reasoning. We refer to this phenomenon as the\n\\textit{modality gap}, defined as the performance disparity between\ntext-centric and vision-centric inputs. In this paper, we analyze the modality\ngap through the lens of training recipes. We first show that existing training\nrecipes tend to amplify this gap. Then, we systematically explore strategies to\nbridge it from two complementary perspectives: data and loss design. Our\nfindings provide insights into developing training recipes that mitigate the\nmodality gap and promote more balanced multimodal reasoning. Our code is\npublicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.", "AI": {"tldr": "本文分析多模态大语言模型中的模态鸿沟问题，发现现有训练方法加剧了文本和视觉模态之间的性能差距，并提出从数据和损失函数设计两方面来弥合这一差距的方法。", "motivation": "多模态大语言模型在视觉-语言任务中表现出色，但存在模态不平衡问题——过度依赖文本线索而忽视视觉内容，导致需要真正视觉推理的任务表现不佳。", "method": "通过训练方法视角分析模态鸿沟，首先证明现有训练方法会放大这种差距，然后从数据和损失函数设计两个互补角度系统探索弥合策略。", "result": "研究发现为开发能够减轻模态鸿沟并促进更平衡多模态推理的训练方法提供了洞见。", "conclusion": "论文提出了弥合多模态大语言模型中文本和视觉模态之间性能差距的有效策略，通过改进训练方法实现更平衡的多模态推理能力。"}}
{"id": "2510.22531", "pdf": "https://arxiv.org/pdf/2510.22531", "abs": "https://arxiv.org/abs/2510.22531", "authors": ["Noshitha Padma Pratyusha Juttu", "Sahithi Singireddy", "Sravani Gona", "Sujal Timilsina"], "title": "Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, including figures and tables. All experiments are\n  reproducible. Code and fine-tuned models are publicly available on: GitHub:\n  (https://github.com/Stimils02/UnfairTOSAgreementsDetection) and Hugging Face:\n  (https://huggingface.co/Noshitha98)", "summary": "Large Language Models (LLMs) have transformed text understanding, yet their\nadaptation to specialized legal domains remains constrained by the cost of full\nfine-tuning. This study provides a systematic evaluation of fine tuning,\nparameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting\nstrategies for unfair clause detection in Terms of Service (ToS) documents, a\nkey application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit\nLow-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and\nSaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments\non the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that\nfull fine-tuning achieves the strongest precision recall balance, while\nLoRA-based models provide competitive recall with up to 3x lower memory cost.\nThese findings highlight practical design trade-offs for efficient and\ndomain-adapted LLMs, contributing open baselines for fine-tuning research in\nlegal text processing.", "AI": {"tldr": "本研究系统评估了在服务条款文档中进行不公平条款检测的不同方法，包括全微调、参数高效适配（LoRA、QLoRA）和零样本提示策略，发现全微调效果最佳但成本高，LoRA方法在内存效率上更具优势", "motivation": "大型语言模型在文本理解方面表现出色，但在专业法律领域的应用受到全微调成本的限制，需要探索更高效的适配方法", "method": "使用BERT和DistilBERT进行全微调，对TinyLlama、LLaMA 3B/7B和SaulLM应用4位低秩适配（LoRA），并在零样本设置下评估GPT-4o及其变体", "result": "全微调获得最佳精确率-召回率平衡，基于LoRA的模型提供竞争性的召回率且内存成本降低最多3倍", "conclusion": "研究揭示了高效领域适配LLM的实际设计权衡，为法律文本处理中的微调研究提供了开放基准"}}
{"id": "2510.22840", "pdf": "https://arxiv.org/pdf/2510.22840", "abs": "https://arxiv.org/abs/2510.22840", "authors": ["Yifei Li", "Erik-Jan van Kampen"], "title": "Lyapunov Function-guided Reinforcement Learning for Flight Control", "categories": ["cs.AI"], "comment": null, "summary": "A cascaded online learning flight control system has been developed and\nenhanced with respect to action smoothness. In this paper, we investigate the\nconvergence performance of the control system, characterized by the increment\nof a Lyapunov function candidate. The derivation of this metric accounts for\ndiscretization errors and state prediction errors introduced by the incremental\nmodel. Comparative results are presented through flight control simulations.", "AI": {"tldr": "开发了一个级联在线学习飞行控制系统并改进了动作平滑性，研究了以李雅普诺夫函数增量为特征的收敛性能，考虑了离散化误差和增量模型引入的状态预测误差，通过飞行控制仿真展示比较结果", "motivation": "研究级联在线学习飞行控制系统的收敛性能，特别关注动作平滑性的改进效果", "method": "使用李雅普诺夫函数增量作为性能指标，考虑离散化误差和状态预测误差，通过飞行控制仿真进行对比分析", "result": "开发了增强动作平滑性的级联在线学习飞行控制系统，并对其收敛性能进行了理论分析和仿真验证", "conclusion": "该控制系统的收敛性能得到了理论分析和仿真验证，李雅普诺夫函数增量方法有效表征了系统性能，考虑了实际工程中的误差因素"}}
{"id": "2510.22548", "pdf": "https://arxiv.org/pdf/2510.22548", "abs": "https://arxiv.org/abs/2510.22548", "authors": ["Ziyuan He", "Yuxuan Wang", "Jiaqi Li", "Kexin Liang", "Muhan Zhang"], "title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?", "categories": ["cs.CL", "cs.AI"], "comment": "NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Large language models (LLMs) are equipped with increasingly extended context\nwindows recently, yet their long context understanding capabilities over long\ndependency tasks remain fundamentally limited and underexplored. This gap is\nespecially significant in many real-world long-context applications that were\nrarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark\ndesigned to evaluate LLMs' long context ability in real-world applications and\nscenarios. Our benchmark consists of automatically collected real-world long\ntexts, ranging from 16k to 2M tokens, encompassing domains in law, finance,\ngame and code. Accordingly, we delicately design 10 types of domain-specific\nlong-dependency tasks and generate 1,934 QA instances with various diversity\nand complexity in a scalable data curation pipeline for further practical\nneeds. We conduct a comprehensive assessment of 6 locally deployed and 4\nAPI-based LLMs. The evaluation results show that even the best-performing model\nachieves only a 59.2% overall score on our benchmark. Despite the extensive\ncontext windows, popular LLMs are only capable of understanding a much shorter\nlength of context than they claim to be, revealing significant limitations in\ntheir ability to handle real-world tasks with long dependencies and\nhighlighting substantial room for model improvement in practical long-context\nunderstanding.", "AI": {"tldr": "LooGLE v2是一个评估大语言模型在真实世界长文本场景下理解能力的新基准，测试显示即使最佳模型也只能达到59.2%的分数，表明当前模型的长上下文处理能力存在显著局限", "motivation": "当前大语言模型虽然具备更长的上下文窗口，但在长依赖任务中的实际理解能力仍然有限且缺乏充分研究，特别是在真实世界长文本应用中缺乏基准测试", "method": "构建包含法律、金融、游戏和代码领域的自动收集的真实长文本数据集（16k到2M token），设计10种领域特定的长依赖任务类型，生成1,934个多样化和复杂性的QA实例，评估6个本地部署和4个API基础的LLM", "result": "评估结果显示，即使表现最佳的模型在基准测试中仅获得59.2%的总体分数，流行LLM实际能理解的上文长度远低于其宣称的能力，在处理长依赖任务时存在显著限制", "conclusion": "当前大语言模型在真实世界长上下文理解方面存在重大局限性，揭示了模型在实际长依赖任务处理能力上的不足，表明在实用长上下文理解方面还有很大的改进空间"}}
{"id": "2510.22883", "pdf": "https://arxiv.org/pdf/2510.22883", "abs": "https://arxiv.org/abs/2510.22883", "authors": ["Giovanni Sileno", "Jean-Louis Dessalles"], "title": "Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits", "categories": ["cs.AI"], "comment": "paper presented at the 10th AIC workshop (AI & cognition) at ECAI\n  2025", "summary": "Cognitive studies and artificial intelligence have developed distinct models\nfor various inferential mechanisms (categorization, induction, abduction,\ncausal inference, contrast, merge, ...). Yet, both natural and artificial views\non cognition lack apparently a unifying framework. This paper formulates a\nspeculative answer attempting to respond to this gap. To postulate on\nhigher-level activation processes from a material perspective, we consider\ninferential mechanisms informed by symbolic AI modelling techniques, through\nthe simplistic lenses of electronic circuits based on logic gates. We observe\nthat a logic gate view entails a different treatment of implication and\nnegation compared to standard logic and logic programming. Then, by\ncombinatorial exploration, we identify four main forms of dependencies that can\nbe realized by these inferential circuits. Looking at how these forms are\ngenerally used in the context of logic programs, we identify eight common\ninferential patterns, exposing traditionally distinct inferential mechanisms in\nan unifying framework. Finally, following a probabilistic interpretation of\nlogic programs, we unveil inner functional dependencies. The paper concludes\nelaborating in what sense, even if our arguments are mostly informed by\nsymbolic means and digital systems infrastructures, our observations may\npinpoint to more generally applicable structures.", "AI": {"tldr": "该论文提出了一个基于逻辑门电子电路的统一框架，将多种推理机制（分类、归纳、溯因、因果推理等）整合到统一的视角下，通过组合探索识别出四种主要依赖形式和八种常见推理模式。", "motivation": "认知研究和人工智能为各种推理机制开发了不同的模型，但缺乏统一的理论框架。论文试图填补这一空白，从物质角度思考高级激活过程。", "method": "使用符号AI建模技术，通过基于逻辑门的电子电路简化视角来分析推理机制，进行组合探索识别依赖形式，并在逻辑程序背景下分析推理模式。", "result": "识别出四种可由推理电路实现的主要依赖形式，发现了八种常见的推理模式，在统一框架下揭示了传统上不同的推理机制。", "conclusion": "即使论证主要基于符号方法和数字系统基础设施，这些观察结果可能指向更普遍适用的结构，为认知和AI推理机制提供了统一的视角。"}}
{"id": "2510.22556", "pdf": "https://arxiv.org/pdf/2510.22556", "abs": "https://arxiv.org/abs/2510.22556", "authors": ["Jinhan Chen", "Jianchun Liu", "Hongli Xu", "Xianjun Gao", "Shilong Wang"], "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size", "categories": ["cs.CL"], "comment": null, "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.", "AI": {"tldr": "SABlock是一种语义感知的KV缓存淘汰框架，通过自适应块大小来平衡语义连贯性和内存效率，在长上下文LLM推理中显著减少内存占用并提升性能", "motivation": "KV缓存不断增长的内存占用成为长上下文LLM推理的可扩展性瓶颈，现有压缩方法难以平衡语义连贯性和内存效率", "method": "SABlock首先进行语义分割以对齐压缩边界和语言结构，然后应用分段引导的token评分来优化重要性评估，最后通过预算驱动的搜索策略自适应确定最佳块大小", "result": "在相同内存预算下，SABlock持续优于最先进基线方法。在NIAH测试中仅用96个KV条目就达到99.9%检索准确率，在1024固定缓存预算下减少46.28%峰值内存使用，在128K上下文长度下解码速度提升9.5倍", "conclusion": "SABlock通过语义感知的自适应块大小策略有效解决了KV缓存内存瓶颈问题，在保持语义完整性的同时显著提升了压缩效率和推理性能"}}
{"id": "2510.22898", "pdf": "https://arxiv.org/pdf/2510.22898", "abs": "https://arxiv.org/abs/2510.22898", "authors": ["Vishvesh Bhat", "Omkar Ghugarkar", "Julian McAuley"], "title": "On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset", "categories": ["cs.AI", "cs.SE"], "comment": "Preprint", "summary": "Generalization across Agentic tool-calling environments remains a key\nunsolved challenge in developing reliable agentic reasoning systems. While\nlarge language models (LLMs) demonstrate strong performance on isolated\nbenchmarks, their ability to transfer reasoning strategies and co-ordinate\ntools across diverse domains is poorly understood. In this work, we conduct a\nlarge-scale evaluation of state-of-the-art LLMs on multiple tool-calling\nbenchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &\nPhysics Adversarial Verification & Evaluation Network), a new out of\ndistribution (OOD) benchmark designed to stress-test multi-step reasoning\nthrough explicit verification and adversarial task composition. Our results\nshow that most current models achieve below 50% accuracy on MAVEN, revealing a\nsignificant generalization gap across tool-use settings.\n  To address this, we present the CoreThink Agentic Reasoner, a framework that\naugments LLMs with a lightweight symbolic reasoning layer for structured\ndecomposition and adaptive tool orchestration. Without additional training, it\ngeneralizes across all benchmarks, achieving state-of-the-art performance with\n530% improvements over existing baselines at roughly one-tenth the\ncomputational cost.", "AI": {"tldr": "论文提出了CoreThink智能体推理框架，通过轻量级符号推理层增强大语言模型，在多工具调用环境中实现了530%的性能提升和显著的计算成本降低。", "motivation": "解决智能体工具调用环境中的泛化挑战，当前大语言模型在跨领域工具协调和推理策略迁移方面存在显著泛化差距。", "method": "提出CoreThink智能体推理器框架，为大语言模型添加轻量级符号推理层，实现结构化分解和自适应工具编排，无需额外训练。", "result": "在多个工具调用基准测试中实现最先进性能，准确率提升530%，计算成本降低约90%，在MAVEN新基准上大多数模型准确率低于50%。", "conclusion": "CoreThink框架有效解决了智能体工具调用泛化问题，证明了符号推理层在增强大语言模型跨领域推理能力方面的重要价值。"}}
{"id": "2510.22559", "pdf": "https://arxiv.org/pdf/2510.22559", "abs": "https://arxiv.org/abs/2510.22559", "authors": ["Zhifeng Wang", "Xinyue Zheng", "Chunyan Zeng"], "title": "A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback", "categories": ["cs.CL"], "comment": "8 pages, 6 figures", "summary": "As information technology advances, education is moving from\none-size-fits-all instruction toward personalized learning. However, most\nmethods handle modeling, item selection, and feedback in isolation rather than\nas a closed loop. This leads to coarse or opaque student models,\nassumption-bound adaptivity that ignores diagnostic posteriors, and generic,\nnon-actionable feedback. To address these limitations, this paper presents an\nend-to-end personalized learning agent, EduLoop-Agent, which integrates a\nNeural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation\nComputerized Adaptive Testing strategy (BECAT), and large language models\n(LLMs). The NCD module provides fine-grained estimates of students' mastery at\nthe knowledge-point level; BECAT dynamically selects subsequent items to\nmaximize relevance and learning efficiency; and LLMs convert diagnostic signals\ninto structured, actionable feedback. Together, these components form a\nclosed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments\non the ASSISTments dataset show that the NCD module achieves strong performance\non response prediction while yielding interpretable mastery assessments. The\nadaptive recommendation strategy improves item relevance and personalization,\nand the LLM-based feedback offers targeted study guidance aligned with\nidentified weaknesses. Overall, the results indicate that the proposed design\nis effective and practically deployable, providing a feasible pathway to\ngenerating individualized learning trajectories in intelligent education.", "AI": {"tldr": "EduLoop-Agent是一个端到端的个性化学习代理，通过神经认知诊断模型、自适应测试策略和大型语言模型形成诊断-推荐-反馈的闭环系统，在ASSISTments数据集上验证了其有效性。", "motivation": "当前个性化学习方法存在模型粗糙、适应性有限和反馈不具体的问题，各组件孤立运作而非形成闭环，需要整合建模、项目选择和反馈的端到端解决方案。", "method": "提出EduLoop-Agent系统，包含三个核心组件：神经认知诊断模型(NCD)提供细粒度知识掌握度评估，有界能力估计计算机自适应测试策略(BECAT)动态选择项目，大型语言模型(LLMs)生成结构化可操作反馈。", "result": "在ASSISTments数据集上，NCD模块在响应预测方面表现优异且提供可解释的掌握度评估，自适应推荐策略提高了项目相关性和个性化程度，LLM反馈能针对已识别的弱点提供针对性学习指导。", "conclusion": "该设计方案有效且可实际部署，为智能教育中生成个性化学习轨迹提供了可行路径，实现了诊断-推荐-反馈的闭环个性化学习框架。"}}
{"id": "2510.22942", "pdf": "https://arxiv.org/pdf/2510.22942", "abs": "https://arxiv.org/abs/2510.22942", "authors": ["Zhuoxuan Li", "Jieyuan Pei", "Tangwei Ye", "Zhongyuan Lai", "Zihan Liu", "Fengyuan Xu", "Qi Zhang", "Liang Hu"], "title": "GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation", "categories": ["cs.AI", "cs.IR", "H.3.3; I.2.6"], "comment": "14 pages, 8 figures, 4 tables, submitted to ICDE 2026", "summary": "Next Point-of-Interest (POI) recommendation is a critical task in modern\nLocation-Based Social Networks (LBSNs), aiming to model the complex\ndecision-making process of human mobility to provide personalized\nrecommendations for a user's next check-in location. Existing POI\nrecommendation models, predominantly based on Graph Neural Networks and\nsequential models, have been extensively studied. However, these models face a\nfundamental limitation: they struggle to simultaneously capture the inherent\nhierarchical structure of spatial choices and the dynamics and irregular shifts\nof user-specific temporal contexts. To overcome this limitation, we propose\nGTR-Mamba, a novel framework for cross-manifold conditioning and routing.\nGTR-Mamba leverages the distinct advantages of different mathematical spaces\nfor different tasks: it models the static, tree-like preference hierarchies in\nhyperbolic geometry, while routing the dynamic sequence updates to a novel\nMamba layer in the computationally stable and efficient Euclidean tangent\nspace. This process is coordinated by a cross-manifold channel that fuses\nspatio-temporal information to explicitly steer the State Space Model (SSM),\nenabling flexible adaptation to contextual changes. Extensive experiments on\nthree real-world datasets demonstrate that GTR-Mamba consistently outperforms\nstate-of-the-art baseline models in next POI recommendation.", "AI": {"tldr": "GTR-Mamba是一个新颖的跨流形条件路由框架，通过双曲几何建模静态偏好层级，在欧几里得空间处理动态序列，有效解决了现有POI推荐模型无法同时捕捉空间层次结构和用户时间上下文动态变化的问题。", "motivation": "现有基于图神经网络和序列模型的POI推荐方法存在根本局限性：难以同时捕捉空间选择的内在层次结构以及用户特定时间上下文的动态和不规则变化。", "method": "提出GTR-Mamba框架，利用不同数学空间的优势：在双曲几何中建模静态的树状偏好层级，在欧几里得切空间中通过新颖的Mamba层处理动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。", "result": "在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中始终优于最先进的基线模型。", "conclusion": "GTR-Mamba通过跨流形条件路由的方法成功解决了POI推荐中的时空建模挑战，证明了利用不同几何空间处理不同类型信息的有效性，为下一代位置推荐系统提供了有前景的方向。"}}
{"id": "2510.22581", "pdf": "https://arxiv.org/pdf/2510.22581", "abs": "https://arxiv.org/abs/2510.22581", "authors": ["Kaushal Kumar Maurya", "Ekaterina Kochmar"], "title": "Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems", "categories": ["cs.CL"], "comment": "AIED 2025 (BlueSky)", "summary": "The interdisciplinary research domain of Artificial Intelligence in Education\n(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by\nintegrating insights from technological advancements, educational theories, and\ncognitive psychology. The remarkable success of generative AI (GenAI) models\nhas accelerated the development of large language model (LLM)-powered ITSs,\nwhich have potential to imitate human-like, pedagogically rich, and cognitively\ndemanding tutoring. However, the progress and impact of these systems remain\nlargely untraceable due to the absence of reliable, universally accepted, and\npedagogy-driven evaluation frameworks and benchmarks. Most existing educational\ndialogue-based ITS evaluations rely on subjective protocols and\nnon-standardized benchmarks, leading to inconsistencies and limited\ngeneralizability. In this work, we take a step back from mainstream ITS\ndevelopment and provide comprehensive state-of-the-art evaluation practices,\nhighlighting associated challenges through real-world case studies from careful\nand caring AIED research. Finally, building on insights from previous\ninterdisciplinary AIED research, we propose three practical, feasible, and\ntheoretically grounded research directions, rooted in learning science\nprinciples and aimed at establishing fair, unified, and scalable evaluation\nmethodologies for ITSs.", "AI": {"tldr": "本文回顾了AI教育领域智能辅导系统的评估现状，指出当前缺乏标准化评估框架的问题，并提出了基于学习科学的三项研究方向来建立公平统一的评估方法。", "motivation": "生成式AI和大型语言模型的成功加速了智能辅导系统的发展，但这些系统的进展和影响难以追踪，因为缺乏可靠、普遍接受且以教学为导向的评估框架和基准。", "method": "通过回顾最先进的评估实践，分析相关挑战，并基于真实案例研究，结合跨学科AI教育研究的见解。", "result": "识别了当前评估方法的主观性和非标准化问题，导致不一致性和有限的泛化能力。", "conclusion": "提出了三项实践可行、理论扎实的研究方向，这些方向基于学习科学原则，旨在为智能辅导系统建立公平、统一和可扩展的评估方法论。"}}
{"id": "2510.22969", "pdf": "https://arxiv.org/pdf/2510.22969", "abs": "https://arxiv.org/abs/2510.22969", "authors": ["Kechen Meng", "Sinuo Zhang", "Rongpeng Li", "Xiangming Meng", "Chan Wang", "Ming Lei", "Zhifeng Zhao"], "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "In wireless communication systems, efficient and adaptive resource allocation\nplays a crucial role in enhancing overall Quality of Service (QoS). While\ncentralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a\ncentral coordinator for policy training and resource scheduling, they suffer\nfrom scalability issues and privacy risks. In contrast, the Distributed\nTraining with Decentralized Execution (DTDE) paradigm enables distributed\nlearning and decision-making, but it struggles with non-stationarity and\nlimited inter-agent cooperation, which can severely degrade system performance.\nTo overcome these challenges, we propose the Multi-Agent Conditional Diffusion\nModel Planner (MA-CDMP) for decentralized communication resource management.\nBuilt upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP\nemploys Diffusion Models (DMs) to capture environment dynamics and plan future\ntrajectories, while an inverse dynamics model guides action generation, thereby\nalleviating the sample inefficiency and slow convergence of conventional DTDE\nmethods. Moreover, to approximate large-scale agent interactions, a Mean-Field\n(MF) mechanism is introduced as an assistance to the classifier in DMs. This\ndesign mitigates inter-agent non-stationarity and enhances cooperation with\nminimal communication overhead in distributed settings. We further\ntheoretically establish an upper bound on the distributional approximation\nerror introduced by the MF-based diffusion generation, guaranteeing convergence\nstability and reliable modeling of multi-agent stochastic dynamics. Extensive\nexperiments demonstrate that MA-CDMP consistently outperforms existing MARL\nbaselines in terms of average reward and QoS metrics, showcasing its\nscalability and practicality for real-world wireless network optimization.", "AI": {"tldr": "提出了基于多智能体条件扩散模型规划器(MA-CDMP)的去中心化通信资源管理方法，通过扩散模型捕捉环境动态和规划轨迹，结合均值场机制处理大规模智能体交互，有效解决了传统方法的非平稳性和合作问题。", "motivation": "集中式多智能体强化学习存在可扩展性和隐私风险问题，而分布式训练去中心化执行范式面临非平稳性和有限合作的挑战，需要新的方法来提升无线通信系统的资源分配效率。", "method": "基于模型强化学习范式，使用扩散模型(DMs)捕捉环境动态和规划未来轨迹，通过逆动力学模型指导动作生成，引入均值场(MF)机制近似大规模智能体交互，减少通信开销。", "result": "实验表明MA-CDMP在平均奖励和QoS指标上持续优于现有MARL基线方法，展现了良好的可扩展性和实际应用价值。", "conclusion": "MA-CDMP通过扩散模型和均值场机制有效解决了多智能体资源管理的非平稳性和合作问题，理论保证了收敛稳定性，为实际无线网络优化提供了实用解决方案。"}}
{"id": "2510.22593", "pdf": "https://arxiv.org/pdf/2510.22593", "abs": "https://arxiv.org/abs/2510.22593", "authors": ["Dario Loi", "Elena Maria Muià", "Federico Siciliano", "Giovanni Trappolini", "Vincenzo Crisà", "Peter Kruger", "Fabrizio Silvestri"], "title": "AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11; H.3.4; D.2.8"], "comment": null, "summary": "We present AutoBench, a fully automated and self-sustaining framework for\nevaluating Large Language Models (LLMs) through reciprocal peer assessment.\nThis paper provides a rigorous scientific validation of the AutoBench\nmethodology, originally developed as an open-source project by eZecute S.R.L..\nUnlike static benchmarks that suffer from test-set contamination and limited\nadaptability, AutoBench dynamically generates novel evaluation tasks while\nmodels alternately serve as question generators, contestants, and judges across\ndiverse domains. An iterative weighting mechanism amplifies the influence of\nconsistently reliable evaluators, aggregating peer judgments into\nconsensus-based rankings that reflect collective model agreement. Our\nexperiments demonstrate strong correlations with established benchmarks\nincluding MMLU-Pro and GPQA (respectively 78\\% and 63\\%), validating this\npeer-driven evaluation paradigm. The multi-judge design significantly\noutperforms single-judge baselines, confirming that distributed evaluation\nproduces more robust and human-consistent assessments. AutoBench offers a\nscalable, contamination-resistant alternative to static benchmarks for the\ncontinuous evaluation of evolving language models.", "AI": {"tldr": "AutoBench是一个全自动的LLM评估框架，通过模型间的互评机制动态生成评估任务，避免了传统静态基准的测试集污染问题，并能产生更鲁棒的人类一致性评估结果。", "motivation": "解决传统静态基准测试存在的测试集污染和有限适应性问题，需要一种能够持续评估不断演进的语言模型的动态评估方法。", "method": "采用互评机制，让模型交替充当问题生成器、参赛者和评委角色，通过迭代加权机制放大可靠评估者的影响力，将同行判断聚合成基于共识的排名。", "result": "实验显示与MMLU-Pro和GPQA基准有强相关性（分别为78%和63%），多评委设计显著优于单评委基线，证明分布式评估产生更鲁棒和人类一致的评估。", "conclusion": "AutoBench提供了一个可扩展、抗污染的替代方案，适用于对不断演进的语言模型进行持续评估，验证了同行驱动评估范式的有效性。"}}
{"id": "2510.22981", "pdf": "https://arxiv.org/pdf/2510.22981", "abs": "https://arxiv.org/abs/2510.22981", "authors": ["Jin Hu", "Jiakai Wang", "Linna Jing", "Haolin Li", "Haodong Liu", "Haotong Qin", "Aishan Liu", "Ke Xu", "Xianglong Liu"], "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction", "categories": ["cs.AI", "cs.CV"], "comment": "NeurIPS 2025", "summary": "Recently, semantically constrained adversarial examples (SemanticAE), which\nare directly generated from natural language instructions, have become a\npromising avenue for future research due to their flexible attacking forms. To\ngenerate SemanticAEs, current methods fall short of satisfactory attacking\nability as the key underlying factors of semantic uncertainty in human\ninstructions, such as referring diversity, descriptive incompleteness, and\nboundary ambiguity, have not been fully investigated. To tackle the issues,\nthis paper develops a multi-dimensional instruction uncertainty reduction\n(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,\nadaptive, and effective. Specifically, in the dimension of the sampling method,\nwe propose the residual-driven attacking direction stabilization to alleviate\nthe unstable adversarial optimization caused by the diversity of language\nreferences. By coarsely predicting the language-guided sampling process, the\noptimization process will be stabilized by the designed ResAdv-DDIM sampler,\ntherefore releasing the transferable and robust adversarial capability of\nmulti-step diffusion models. In task modeling, we propose the context-encoded\nattacking scenario constraint to supplement the missing knowledge from\nincomplete human instructions. Guidance masking and renderer integration are\nproposed to regulate the constraints of 2D/3D SemanticAE, activating stronger\nscenario-adapted attacks. Moreover, in the dimension of generator evaluation,\nwe propose the semantic-abstracted attacking evaluation enhancement by\nclarifying the evaluation boundary, facilitating the development of more\neffective SemanticAE generators. Extensive experiments demonstrate the\nsuperiority of the transfer attack performance of InSUR. Moreover, we realize\nthe reference-free generation of semantically constrained 3D adversarial\nexamples for the first time.", "AI": {"tldr": "本文提出了InSUR框架，通过多维度指令不确定性减少技术，解决了语义对抗样本生成中的语言不确定性挑战，显著提升了对抗样本的可迁移性、适应性和有效性。", "motivation": "当前语义对抗样本生成方法因未充分研究人类指令中的语义不确定性（如指称多样性、描述不完整性和边界模糊性）而攻击能力不足，需要开发更有效的生成框架。", "method": "提出多维度指令不确定性减少(InSUR)框架：1）采样方法维度：残差驱动攻击方向稳定化(ResAdv-DDIM采样器)；2）任务建模维度：上下文编码攻击场景约束；3）生成器评估维度：语义抽象攻击评估增强。", "result": "大量实验证明InSUR在迁移攻击性能上具有优越性，首次实现了无需参考的语义约束3D对抗样本生成。", "conclusion": "InSUR框架通过系统解决语义不确定性挑战，显著提升了语义对抗样本的生成质量，为未来语义对抗攻击研究提供了有效解决方案。"}}
{"id": "2510.22602", "pdf": "https://arxiv.org/pdf/2510.22602", "abs": "https://arxiv.org/abs/2510.22602", "authors": ["Mahyar Abbasian", "Ramesh Jain"], "title": "Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "22 pages, 2 figures, 1 table, Journal paper", "summary": "Building on decades of success in digital infrastructure and biomedical\ninnovation, we propose the Personal Care Utility (PCU) - a cybernetic system\nfor lifelong health guidance. PCU is conceived as a global, AI-powered utility\nthat continuously orchestrates multimodal data, knowledge, and services to\nassist individuals and populations alike. Drawing on multimodal agents,\nevent-centric modeling, and contextual inference, it offers three essential\ncapabilities: (1) trusted health information tailored to the individual, (2)\nproactive health navigation and behavior guidance, and (3) ongoing\ninterpretation of recovery and treatment response after medical events. Unlike\nconventional episodic care, PCU functions as an ambient, adaptive companion -\nobserving, interpreting, and guiding health in real time across daily life. By\nintegrating personal sensing, experiential computing, and population-level\nanalytics, PCU promises not only improved outcomes for individuals but also a\nnew substrate for public health and scientific discovery. We describe the\narchitecture, design principles, and implementation challenges of this emerging\nparadigm.", "AI": {"tldr": "提出了一个名为个人护理公用事业（PCU）的全球性AI驱动系统，通过多模态数据和实时分析提供个性化健康指导和持续健康管理。", "motivation": "基于数字基础设施和生物医学创新的成功，旨在解决传统间歇性医疗护理的局限性，提供持续、实时的健康指导。", "method": "采用多模态智能体、事件中心建模和情境推理技术，整合个人传感、体验计算和群体层面分析。", "result": "PCU系统具备三个核心能力：个性化可信健康信息、主动健康导航和行为指导、医疗事件后的持续恢复解释。", "conclusion": "PCU不仅能够改善个人健康结果，还为公共卫生和科学发现提供了新的基础，代表了医疗保健范式的转变。"}}
{"id": "2510.22998", "pdf": "https://arxiv.org/pdf/2510.22998", "abs": "https://arxiv.org/abs/2510.22998", "authors": ["Gilber A. Corrales", "Carlos Andrés Ferro Sánchez", "Reinel Tabares-Soto", "Jesús Alfonso López Sotelo", "Gonzalo A. Ruz", "Johan Sebastian Piña Durán"], "title": "ProfileXAI: User-Adaptive Explainable AI", "categories": ["cs.AI", "68T05, 68T07", "I.2.6; H.5.2"], "comment": "pages, 1 figure, 3 tables. Preprint. Evaluated on UCI Heart Disease\n  (1989) and UCI Differentiated Thyroid Cancer Recurrence (2023). Uses IEEEtran", "summary": "ProfileXAI is a model- and domain-agnostic framework that couples post-hoc\nexplainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce\nexplanations for different types of users. The system indexes a multimodal\nknowledge base, selects an explainer per instance via quantitative criteria,\nand generates grounded narratives with chat-enabled prompting. On Heart Disease\nand Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token\nuse, and perceived quality. No explainer dominates: LIME achieves the best\nfidelity--robustness trade-off (Infidelity $\\le 0.30$, $L<0.7$ on Heart\nDisease); Anchor yields the sparsest, low-token rules; SHAP attains the highest\nsatisfaction ($\\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\\sigma\n\\le 13\\%$) and maintains positive ratings across profiles ($\\bar{x}\\ge 3.7$,\nwith domain experts at $3.77$), enabling efficient and trustworthy\nexplanations.", "AI": {"tldr": "ProfileXAI是一个结合事后解释器与检索增强LLM的框架，为不同用户生成可解释的AI解释，在心脏病和甲状腺癌数据集上评估显示不同解释器各有优势，系统能稳定生成高质量解释。", "motivation": "为了解决AI模型解释性不足的问题，需要为不同类型的用户提供可理解且可靠的事后解释，同时整合多种解释方法以适应不同场景需求。", "method": "开发模型和领域无关的框架，耦合SHAP、LIME、Anchor等事后解释器与检索增强的大型语言模型，通过多模态知识库索引、基于量化标准选择解释器，并使用聊天式提示生成接地气的叙述。", "result": "在心脏病和甲状腺癌数据集上评估显示：LIME在保真度-鲁棒性权衡上最佳，Anchor产生最稀疏的低token规则，SHAP获得最高满意度。Profile条件化稳定了token使用并保持积极评分。", "conclusion": "ProfileXAI框架能够有效整合多种解释方法，为不同用户群体生成高效且可信的解释，证明了多解释器协同工作的价值，没有单一解释器在所有指标上都表现最优。"}}
{"id": "2510.22616", "pdf": "https://arxiv.org/pdf/2510.22616", "abs": "https://arxiv.org/abs/2510.22616", "authors": ["Morteza Alikhani", "Mohammadtaha Bagherifard", "Erfan Zinvandi", "Mehran Sarmadi"], "title": "PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "20 pages, 17 figures, Accepted to IJCNLP-AACL 2025 (Main Conference)", "summary": "We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale\nPersian benchmark for commonsense reasoning. PerCoR contains 106K\nmultiple-choice sentence-completion problems drawn from more than forty news,\ncultural, and other web sources. We introduce a novel conjunction-based\nsegmentation strategy to generate coherent sentence-completion pairs, enabling\nbroad topical and structural diversity. To create challenging distractors, we\npropose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and\nAdversarial Filtering), a generation-free adversarial filtering method that\nselects distractors from the pool of gold continuations while maximising model\nconfusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the\nhighest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).\nThe strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both\nthe dataset's difficulty and the remaining performance gap in Persian\ncommonsense reasoning. We further show that DRESS-AF transfers to the English\nHellaSwag benchmark, increasing its difficulty without hurting human\nsolvability. The dataset is available at\nhttps://huggingface.co/datasets/MCINext/PerCoR.", "AI": {"tldr": "PerCoR是首个大规模波斯语常识推理基准数据集，包含10.6万个选择题，采用新颖的连词分割策略生成句子补全对，并使用DRESS-AF方法创建具有挑战性的干扰项。", "motivation": "解决波斯语缺乏大规模常识推理基准的问题，推动波斯语自然语言处理的发展。", "method": "1) 从40多个新闻和文化网站收集数据；2) 使用连词分割策略生成句子补全对；3) 开发DRESS-AF方法（基于嵌入相似性评分和对抗过滤）选择干扰项；4) 在HellaSwag英文基准上验证方法有效性。", "result": "人类标注者准确率89%，OpenAI-o3模型表现最佳（92.18%），最强开源模型DeepSeek-R1达到82.51%，显示数据集具有挑战性且存在性能差距。DRESS-AF方法成功提升英文HellaSwag基准的难度而不影响人类可解性。", "conclusion": "PerCoR填补了波斯语常识推理基准的空白，提出的DRESS-AF方法有效提升了数据集质量，为波斯语NLP研究提供了重要资源，同时该方法在跨语言场景中具有通用性。"}}
{"id": "2510.23008", "pdf": "https://arxiv.org/pdf/2510.23008", "abs": "https://arxiv.org/abs/2510.23008", "authors": ["Qiuli Wang", "Xiaoming Li", "Jie Chen", "Yongxu Liu", "Xingpeng Zhang", "Chen Liu", "Wei Chen"], "title": "From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports", "categories": ["cs.AI"], "comment": "10 pages, 6 figures, 4 tables", "summary": "Large language models (LLMs) have demonstrated promising performance in\ngenerating diagnostic conclusions from imaging findings, thereby supporting\nradiology reporting, trainee education, and quality control. However,\nsystematic guidance on how to optimize prompt design across different clinical\ncontexts remains underexplored. Moreover, a comprehensive and standardized\nframework for assessing the trustworthiness of LLM-generated radiology reports\nis yet to be established. This study aims to enhance the trustworthiness of\nLLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility\nAssessment (MDCA) framework and providing guidance on institution-specific\nprompt optimization. The proposed framework is applied to evaluate and compare\nthe performance of several advanced LLMs, including Kimi-K2-Instruct-0905,\nQwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and\nByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.", "AI": {"tldr": "该研究提出了一个多维可信度评估框架(MDCA)来提升LLM生成肝脏MRI报告的可信度，并提供了机构特定的提示优化指导，在多个先进LLM模型上进行了评估比较。", "motivation": "虽然大语言模型在从影像发现生成诊断结论方面表现出潜力，但缺乏针对不同临床场景的提示优化系统指导，以及评估LLM生成放射学报告可信度的标准化框架。", "method": "引入多维可信度评估(MDCA)框架，应用该框架在SiliconFlow平台上评估比较多个先进LLM模型(Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3、ByteDance-Seed-OSS-36B-Instruct)的性能。", "result": "研究提出了一个评估框架并进行了模型比较，但摘要中未明确给出具体的评估结果数据。", "conclusion": "该研究通过建立MDCA框架和提供提示优化指导，为提升LLM生成肝脏MRI报告的可信度提供了系统化的方法学支持。"}}
{"id": "2510.22629", "pdf": "https://arxiv.org/pdf/2510.22629", "abs": "https://arxiv.org/abs/2510.22629", "authors": ["Ambalika Guha", "Sajal Saha", "Debanjan Ballav", "Soumi Mitra", "Hritwick Chakraborty"], "title": "Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preserving linguistic diversity is necessary as every language offers a\ndistinct perspective on the world. There have been numerous global initiatives\nto preserve endangered languages through documentation. This paper is a part of\na project which aims to develop a trilingual (Toto-Bangla-English) language\nlearning application to digitally archive and promote the endangered Toto\nlanguage of West Bengal, India. This application, designed for both native Toto\nspeakers and non-native learners, aims to revitalize the language by ensuring\naccessibility and usability through Unicode script integration and a structured\nlanguage corpus. The research includes detailed linguistic documentation\ncollected via fieldwork, followed by the creation of a morpheme-tagged,\ntrilingual corpus used to train a Small Language Model (SLM) and a\nTransformer-based translation engine. The analysis covers inflectional\nmorphology such as person-number-gender agreement, tense-aspect-mood\ndistinctions, and case marking, alongside derivational strategies that reflect\nword-class changes. Script standardization and digital literacy tools were also\ndeveloped to enhance script usage. The study offers a sustainable model for\npreserving endangered languages by incorporating traditional linguistic\nmethodology with AI. This bridge between linguistic research with technological\ninnovation highlights the value of interdisciplinary collaboration for\ncommunity-based language revitalization.", "AI": {"tldr": "该论文开发了一个托托语-孟加拉语-英语三语学习应用，通过AI技术和语言文档化来保护和振兴印度濒危的托托语。", "motivation": "保护语言多样性，通过数字化方式保存濒危的托托语，确保其可访问性和可用性。", "method": "通过田野调查收集语言数据，创建词素标注的三语语料库，训练小型语言模型和Transformer翻译引擎，分析屈折形态和派生策略，开发文字标准化和数字素养工具。", "result": "成功开发了三语学习应用，建立了结构化的语言语料库，训练了AI模型，为濒危语言保护提供了可持续的数字化解决方案。", "conclusion": "研究展示了将传统语言学方法与AI技术相结合的有效性，为基于社区的语言振兴提供了跨学科合作模式，为其他濒危语言的保护提供了可持续模型。"}}
{"id": "2510.23026", "pdf": "https://arxiv.org/pdf/2510.23026", "abs": "https://arxiv.org/abs/2510.23026", "authors": ["Crimson Stambaugh", "Rajesh P. N. Rao"], "title": "Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution", "categories": ["cs.AI", "cs.RO"], "comment": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESSAN) (under review)", "summary": "Recent studies demonstrate that diffusion planners benefit from sparse-step\nplanning over single-step planning. Training models to skip steps in their\ntrajectories helps capture long-term dependencies without additional or memory\ncomputational cost. However, predicting excessively sparse plans degrades\nperformance. We hypothesize this temporal density threshold is non-uniform\nacross a temporal horizon and that certain parts of a planned trajectory should\nbe more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion\nplanner where the densities throughout the horizon are tunable hyperparameters.\nMDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL\ntask domains.", "AI": {"tldr": "MDD提出可调节密度的扩散规划器，通过非均匀稀疏步长规划在多个任务领域实现新的SOTA性能", "motivation": "现有扩散规划器使用稀疏步长规划可捕获长期依赖但性能会下降，作者发现时间密度阈值在时间维度上不均匀，需要更智能的密度分配", "method": "提出Mixed Density Diffuser (MDD)，在整个规划时间范围内密度可作为可调超参数进行配置", "result": "在Maze2D、Franka Kitchen和Antmaze D4RL任务领域达到了新的SOTA性能", "conclusion": "通过允许不同时间段的规划密度可调节，MDD证明了非均匀稀疏规划的有效性，为扩散规划器提供了更灵活的框架"}}
{"id": "2510.22631", "pdf": "https://arxiv.org/pdf/2510.22631", "abs": "https://arxiv.org/abs/2510.22631", "authors": ["Marco De Santis", "Lisa Alazraki"], "title": "Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task", "categories": ["cs.CL"], "comment": "MRL 2025 Shared Task on Multilingual Physical Reasoning Datasets", "summary": "This paper presents our submission to the MRL 2025 Shared Task on\nMultilingual Physical Reasoning Datasets. The objective of the shared task is\nto create manually-annotated evaluation data in the physical commonsense\nreasoning domain, for languages other than English, following a format similar\nto PIQA. Our contribution, FormaMentis, is a novel benchmark for physical\ncommonsense reasoning that is grounded in Italian language and culture. The\ndata samples in FormaMentis are created by expert annotators who are native\nItalian speakers and are familiar with local customs and norms. The samples are\nadditionally translated into English, while preserving the cultural elements\nunique to the Italian context.", "AI": {"tldr": "FormaMentis是一个基于意大利语言和文化的物理常识推理基准数据集，为MRL 2025多语言物理推理共享任务而创建", "motivation": "为英语以外的语言创建手动标注的物理常识推理评估数据，特别是针对意大利语言和文化背景", "method": "由熟悉当地习俗的意大利母语专家创建数据样本，并保留意大利文化元素的同时翻译成英文", "result": "开发了FormaMentis基准数据集，包含意大利语言文化背景下的物理常识推理样本", "conclusion": "该工作为多语言物理推理研究提供了意大利语文化背景下的高质量评估资源"}}
{"id": "2510.23045", "pdf": "https://arxiv.org/pdf/2510.23045", "abs": "https://arxiv.org/abs/2510.23045", "authors": ["Guiyao Tie", "Pan Zhou", "Lichao Sun"], "title": "A Survey of AI Scientists: Surveying the automatic Scientists and Research", "categories": ["cs.AI"], "comment": "28 pages, 9 figures, 1 table", "summary": "Artificial intelligence is undergoing a profound transition from a\ncomputational instrument to an autonomous originator of scientific knowledge.\nThis emerging paradigm, the AI scientist, is architected to emulate the\ncomplete scientific workflow-from initial hypothesis generation to the final\nsynthesis of publishable findings-thereby promising to fundamentally reshape\nthe pace and scale of discovery. However, the rapid and unstructured\nproliferation of these systems has created a fragmented research landscape,\nobscuring overarching methodological principles and developmental trends. This\nsurvey provides a systematic and comprehensive synthesis of this domain by\nintroducing a unified, six-stage methodological framework that deconstructs the\nend-to-end scientific process into: Literature Review, Idea Generation,\nExperimental Preparation, Experimental Execution, Scientific Writing, and Paper\nGeneration. Through this analytical lens, we chart the field's evolution from\nearly Foundational Modules (2022-2023) to integrated Closed-Loop Systems\n(2024), and finally to the current frontier of Scalability, Impact, and\nHuman-AI Collaboration (2025-present). By rigorously synthesizing these\ndevelopments, this survey not only clarifies the current state of autonomous\nscience but also provides a critical roadmap for overcoming remaining\nchallenges in robustness and governance, ultimately guiding the next generation\nof systems toward becoming trustworthy and indispensable partners in human\nscientific inquiry.", "AI": {"tldr": "这篇综述论文提出了一个六阶段方法论框架来系统分析AI科学家领域的发展，从早期基础模块到当前的规模化与人类协作阶段，为自主科学系统的未来发展提供路线图。", "motivation": "AI正从计算工具转变为自主科学知识创造者，但该领域的快速无序发展导致研究碎片化，缺乏统一的方法论框架和发展趋势分析。", "method": "引入统一的六阶段方法论框架：文献综述、想法生成、实验准备、实验执行、科学写作和论文生成，通过这一分析视角梳理该领域的演进历程。", "result": "将领域发展划分为三个阶段：基础模块阶段（2022-2023）、闭环系统阶段（2024）、规模化与人类协作阶段（2025至今），系统梳理了技术发展脉络。", "conclusion": "该综述不仅阐明了自主科学的当前状态，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖且不可或缺的合作伙伴。"}}
{"id": "2510.22656", "pdf": "https://arxiv.org/pdf/2510.22656", "abs": "https://arxiv.org/abs/2510.22656", "authors": ["Zilong Wang", "Qingtian Zeng", "Hua Duan", "Cheng Cheng", "Minghao Zou", "Ziyang Wang"], "title": "Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion", "categories": ["cs.CL"], "comment": null, "summary": "Few-shot Knowledge Graph Completion (FKGC) infers missing triples from\nlimited support samples, tackling long-tail distribution challenges. Existing\nmethods, however, struggle to capture complex relational patterns and mitigate\ndata sparsity. To address these challenges, we propose a novel FKGC framework\nfor conjugate relation modeling (CR-FKGC). Specifically, it employs a\nneighborhood aggregation encoder to integrate higher-order neighbor\ninformation, a conjugate relation learner combining an implicit conditional\ndiffusion relation module with a stable relation module to capture stable\nsemantics and uncertainty offsets, and a manifold conjugate decoder for\nefficient evaluation and inference of missing triples in manifold space.\nExperiments on three benchmarks demonstrate that our method achieves superior\nperformance over state-of-the-art methods.", "AI": {"tldr": "提出了一种新的少样本知识图谱补全框架CR-FKGC，通过共轭关系建模解决复杂关系模式和数据稀疏性问题，在三个基准测试中取得了优于现有方法的效果。", "motivation": "现有的少样本知识图谱补全方法难以捕捉复杂的关系模式并缓解数据稀疏性问题，需要新的解决方案。", "method": "使用邻域聚合编码器整合高阶邻居信息，结合隐式条件扩散关系模块和稳定关系模块的共轭关系学习器来捕捉稳定语义和不确定性偏移，以及流形共轭解码器在流形空间中进行高效评估和推理。", "result": "在三个基准测试中，该方法相比最先进的方法取得了更优越的性能表现。", "conclusion": "CR-FKGC框架通过有效的共轭关系建模，成功解决了少样本知识图谱补全中的复杂关系模式捕捉和数据稀疏性挑战，展现出优异的性能。"}}
{"id": "2510.23062", "pdf": "https://arxiv.org/pdf/2510.23062", "abs": "https://arxiv.org/abs/2510.23062", "authors": ["Zhifeng Wang", "Meixin Su", "Yang Yang", "Chunyan Zeng", "Lizhi Ye"], "title": "TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis", "categories": ["cs.AI"], "comment": "10 pages, 8 figures", "summary": "Driven by the dual principles of smart education and artificial intelligence\ntechnology, the online education model has rapidly emerged as an important\ncomponent of the education industry. Cognitive diagnostic technology can\nutilize students' learning data and feedback information in educational\nevaluation to accurately assess their ability level at the knowledge level.\nHowever, while massive amounts of information provide abundant data resources,\nthey also bring about complexity in feature extraction and scarcity of\ndisciplinary data. In cross-disciplinary fields, traditional cognitive\ndiagnostic methods still face many challenges. Given the differences in\nknowledge systems, cognitive structures, and data characteristics between\ndifferent disciplines, this paper conducts in-depth research on neural network\ncognitive diagnosis and knowledge association neural network cognitive\ndiagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis\nmethod (TLCD). This method combines deep learning techniques and transfer\nlearning strategies to enhance the performance of the model in the target\ndiscipline by utilizing the common features of the main discipline. The\nexperimental results show that the cross-disciplinary cognitive diagnosis model\nbased on deep learning performs better than the basic model in\ncross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate\nstudents' learning situation.", "AI": {"tldr": "该论文提出了一种基于深度学习和迁移学习的跨学科认知诊断方法(TLCD)，通过利用主学科的共同特征来提升目标学科中的模型性能，实验证明该方法在跨学科认知诊断任务中表现优于基础模型。", "motivation": "在线教育模式下，传统认知诊断方法在跨学科领域面临知识体系差异、认知结构不同和数据特征不统一等挑战，需要新的方法来准确评估学生的能力水平。", "method": "提出TLCD方法，结合深度学习技术和迁移学习策略，利用主学科的共同特征来增强模型在目标学科中的性能，研究神经网络认知诊断和知识关联神经网络认知诊断。", "result": "实验结果显示，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能更准确地评估学生的学习情况。", "conclusion": "TLCD方法通过深度学习和迁移学习的结合，有效解决了跨学科认知诊断中的挑战，为在线教育提供了更准确的学生能力评估工具。"}}
{"id": "2510.22689", "pdf": "https://arxiv.org/pdf/2510.22689", "abs": "https://arxiv.org/abs/2510.22689", "authors": ["Joel Rorseth", "Parke Godfrey", "Lukasz Golab", "Divesh Srivastava", "Jarek Szlichta"], "title": "Rule-Based Explanations for Retrieval-Augmented LLM Systems", "categories": ["cs.CL"], "comment": null, "summary": "If-then rules are widely used to explain machine learning models; e.g., \"if\nemployed = no, then loan application = rejected.\" We present the first proposal\nto apply rules to explain the emerging class of large language models (LLMs)\nwith retrieval-augmented generation (RAG). Since RAG enables LLM systems to\nincorporate retrieved information sources at inference time, rules linking the\npresence or absence of sources can explain output provenance; e.g., \"if a Times\nHigher Education ranking article is retrieved, then the LLM ranks Oxford\nfirst.\" To generate such rules, a brute force approach would probe the LLM with\nall source combinations and check if the presence or absence of any sources\nleads to the same output. We propose optimizations to speed up rule generation,\ninspired by Apriori-like pruning from frequent itemset mining but redefined\nwithin the scope of our novel problem. We conclude with qualitative and\nquantitative experiments demonstrating our solutions' value and efficiency.", "AI": {"tldr": "提出了首个针对检索增强生成（RAG）大语言模型的规则解释方法，通过分析检索源的存在与否来生成if-then规则解释模型输出来源，并设计了优化算法加速规则生成。", "motivation": "现有的if-then规则解释方法主要针对传统机器学习模型，而新兴的检索增强生成（RAG）大语言模型需要新的解释方法来理解其基于检索信息的输出来源。", "method": "提出基于Apriori剪枝思想的优化算法，通过探测LLM在不同检索源组合下的输出，生成连接检索源存在与否与输出结果的if-then规则，避免暴力枚举所有组合。", "result": "开发了高效的规则生成方法，能够解释RAG-LLM系统的输出来源，如\"如果检索到泰晤士高等教育排名文章，则LLM将牛津大学排名第一\"。", "conclusion": "通过定性和定量实验验证了所提解决方案的价值和效率，为RAG大语言模型提供了有效的可解释性工具。"}}
{"id": "2510.23083", "pdf": "https://arxiv.org/pdf/2510.23083", "abs": "https://arxiv.org/abs/2510.23083", "authors": ["Jan Niklas Groeneveld", "Xi Qin", "Alexander Schaefer", "Yaad Oren"], "title": "Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards", "categories": ["cs.AI", "cs.LG", "cs.SE", "I.2.7"], "comment": "Accepted and to be presented at NeurIPS 2025 Workshop: Foundations of\n  Reasoning in Language Models", "summary": "Generating high-quality code remains a challenge for Large Language Models\n(LLMs). For the evolution of reasoning models on this task, reward models are a\nnecessary intermediate step. These models judge outcomes or intermediate steps.\nDecoder-only transformer models can be turned into reward models by introducing\na regression layer and supervised fine-tuning. While it is known that\nreflection capabilities generally increase with the size of a model, we want to\ninvestigate whether state-of-the-art small language models like the Phi-4\nfamily can be turned into usable reward models blending the consideration of\nprocess rewards and outcome rewards.\n  Targeting this goal, we construct a dataset of code samples with correctness\nlabels derived from the APPS coding challenge benchmark. We then train a\nvalue-head model to estimate the success probability of intermediate outputs.\nOur evaluation shows that small LLMs are capable of serving as effective reward\nmodels or code evaluation critics, successfully identifying correct solutions\namong multiple candidates. Using this critic, we achieve over a 20% improvement\nin the search capability of the most accurate code out of multiple generations.", "AI": {"tldr": "研究证明小型语言模型（如Phi-4）可以通过添加回归层和监督微调转变为有效的奖励模型，用于代码生成任务，能识别正确代码解决方案并提升搜索能力20%以上。", "motivation": "虽然大语言模型在代码生成方面仍有挑战，奖励模型是推理模型发展的必要中间步骤，但现有研究主要关注大模型，本研究探索小型语言模型是否也能成为有效的奖励模型。", "method": "构建基于APPS编程挑战基准的代码样本数据集，训练带有回归头的价值模型来估计中间输出的成功概率，结合过程奖励和结果奖励的考量。", "result": "小型LLM能够作为有效的奖励模型或代码评估评判器，成功识别多个候选方案中的正确解决方案，使用该评判器使多代代码中最准确代码的搜索能力提升超过20%。", "conclusion": "小型语言模型可以被成功转化为实用的奖励模型，在代码生成任务中结合过程奖励和结果奖励的考量，为代码生成模型的进化提供有效的中间步骤。"}}
{"id": "2510.22691", "pdf": "https://arxiv.org/pdf/2510.22691", "abs": "https://arxiv.org/abs/2510.22691", "authors": ["Ruslan Berdichevsky", "Shai Nahum-Gefen", "Elad Ben Zaken"], "title": "SALSA: Single-pass Autoregressive LLM Structured Classification", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite their impressive generalization capabilities, instruction-tuned Large\nLanguage Models often underperform on text classification benchmarks. We\nintroduce SALSA, a coherent pipeline that combines structured prompting,\nclass-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding\ncold-start training. Each class label is mapped to a distinct output token, and\nprompts are constructed to elicit a single-token response. During inference,\nthe model's output is projected only onto the logits of the relevant class\ntokens, enabling efficient and accurate classification in a single forward\npass. SALSA achieves state-of-the-art results across diverse benchmarks,\ndemonstrating its robustness and scalability for LLM-based classification\napplications.", "AI": {"tldr": "SALSA是一种针对指令调优大语言模型的文本分类优化方法，通过结构化提示、类别到标记映射和参数高效微调，实现单次前向传播的高效准确分类。", "motivation": "尽管指令调优大语言模型具有强大的泛化能力，但在文本分类基准测试中表现不佳，需要改进其分类性能。", "method": "将每个类别标签映射到不同的输出标记，构建提示以引发单标记响应，在推理时仅将模型输出投影到相关类别标记的logits上。", "result": "SALSA在多个基准测试中取得了最先进的结果，证明了其在基于LLM的分类应用中的鲁棒性和可扩展性。", "conclusion": "SALSA通过避免冷启动训练，提供了一种高效准确的LLM文本分类解决方案，具有实际应用的潜力。"}}
{"id": "2510.23127", "pdf": "https://arxiv.org/pdf/2510.23127", "abs": "https://arxiv.org/abs/2510.23127", "authors": ["Kai Zhuang", "Jiawei Zhang", "Yumou Liu", "Hanqun Cao", "Chunbin Gu", "Mengdi Liu", "Zhangyang Gao", "Zitong Jerry Wang", "Xuanhe Zhou", "Pheng-Ann Heng", "Lijun Wu", "Conghui He", "Cheng Tan"], "title": "Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs", "categories": ["cs.AI"], "comment": "36 pages, under review", "summary": "Scientific Large Language Models (Sci-LLMs) have emerged as a promising\nfrontier for accelerating biological discovery. However, these models face a\nfundamental challenge when processing raw biomolecular sequences: the\ntokenization dilemma. Whether treating sequences as a specialized language,\nrisking the loss of functional motif information, or as a separate modality,\nintroducing formidable alignment challenges, current strategies fundamentally\nlimit their reasoning capacity. We challenge this sequence-centric paradigm by\npositing that a more effective strategy is to provide Sci-LLMs with high-level\nstructured context derived from established bioinformatics tools, thereby\nbypassing the need to interpret low-level noisy sequence data directly. Through\na systematic comparison of leading Sci-LLMs on biological reasoning tasks, we\ntested three input modes: sequence-only, context-only, and a combination of\nboth. Our findings are striking: the context-only approach consistently and\nsubstantially outperforms all other modes. Even more revealing, the inclusion\nof the raw sequence alongside its high-level context consistently degrades\nperformance, indicating that raw sequences act as informational noise, even for\nmodels with specialized tokenization schemes. These results suggest that the\nprimary strength of existing Sci-LLMs lies not in their nascent ability to\ninterpret biomolecular syntax from scratch, but in their profound capacity for\nreasoning over structured, human-readable knowledge. Therefore, we argue for\nreframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines\nover expert knowledge. This work lays the foundation for a new class of hybrid\nscientific AI agents, repositioning the developmental focus from direct\nsequence interpretation towards high-level knowledge synthesis. The code is\navailable at github.com/opendatalab-raise-dev/CoKE.", "AI": {"tldr": "研究发现，在生物推理任务中，为科学大语言模型提供高层次结构化上下文比直接处理原始生物分子序列效果更好，甚至原始序列会干扰模型性能，表明Sci-LLMs的真正优势在于对结构化知识的推理能力而非序列解码。", "motivation": "解决科学大语言模型在处理原始生物分子序列时面临的tokenization困境——无论是将序列视为特殊语言可能丢失功能motif信息，还是作为单独模态引入对齐挑战，当前策略都限制了模型的推理能力。", "method": "通过系统比较领先的Sci-LLMs在生物推理任务上的表现，测试了三种输入模式：仅序列、仅上下文、以及两者结合。", "result": "上下文仅方法始终且显著优于所有其他模式。更令人惊讶的是，将原始序列与其高层次上下文结合会持续降低性能，表明原始序列即使对于具有专门tokenization方案的模型也表现为信息噪声。", "conclusion": "现有Sci-LLMs的主要优势不在于从头解释生物分子语法的新兴能力，而在于对结构化、人类可读知识的深刻推理能力。应重新构建Sci-LLMs，将其视为专家知识的强大推理引擎而非序列解码器。"}}
{"id": "2510.22733", "pdf": "https://arxiv.org/pdf/2510.22733", "abs": "https://arxiv.org/abs/2510.22733", "authors": ["Qi Liu", "Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Pengjun Xie", "Jiaxin Mao"], "title": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Code and models are avaliable at https://alibaba-nlp.github.io/E2Rank", "summary": "Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\n$\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.", "AI": {"tldr": "E²Rank是一个统一的检索和重排序框架，通过列表排序目标继续训练单个文本嵌入模型，实现高效且高质量的检索和重排名功能", "motivation": "传统文本嵌入模型在检索效率上表现优异，但在排序保真度上不如专门的LLM列表重排序器，后者能捕捉细粒度的查询-文档和文档-文档交互", "method": "使用查询和文档嵌入的余弦相似度作为统一排序函数，构建包含原始查询和候选文档的列表排序提示，类似于传统检索中的伪相关反馈机制", "result": "在BEIR重排序基准上达到最先进结果，在BRIGHT推理密集型基准上表现优异，重排序延迟极低，同时在MTEB基准上提升了嵌入性能", "conclusion": "单个嵌入模型可以有效地统一检索和重排序功能，在保持计算效率的同时提供有竞争力的排序准确性"}}
{"id": "2510.23167", "pdf": "https://arxiv.org/pdf/2510.23167", "abs": "https://arxiv.org/abs/2510.23167", "authors": ["Zhao Yang", "Thomas M. Moerland", "Mike Preuss", "Aske Plaat", "Vincent François-Lavet", "Edward S. Hu"], "title": "Guiding Skill Discovery with Foundation Models", "categories": ["cs.AI"], "comment": null, "summary": "Learning diverse skills without hand-crafted reward functions could\naccelerate reinforcement learning in downstream tasks. However, existing skill\ndiscovery methods focus solely on maximizing the diversity of skills without\nconsidering human preferences, which leads to undesirable behaviors and\npossibly dangerous skills. For instance, a cheetah robot trained using previous\nmethods learns to roll in all directions to maximize skill diversity, whereas\nwe would prefer it to run without flipping or entering hazardous areas. In this\nwork, we propose a Foundation model Guided (FoG) skill discovery method, which\nincorporates human intentions into skill discovery through foundation models.\nSpecifically, FoG extracts a score function from foundation models to evaluate\nstates based on human intentions, assigning higher values to desirable states\nand lower to undesirable ones. These scores are then used to re-weight the\nrewards of skill discovery algorithms. By optimizing the re-weighted skill\ndiscovery rewards, FoG successfully learns to eliminate undesirable behaviors,\nsuch as flipping or rolling, and to avoid hazardous areas in both state-based\nand pixel-based tasks. Interestingly, we show that FoG can discover skills\ninvolving behaviors that are difficult to define. Interactive visualisations\nare available from https://sites.google.com/view/submission-fog.", "AI": {"tldr": "FoG方法通过基础模型将人类偏好融入技能发现过程，有效避免传统方法产生的不良行为，同时保持技能多样性。", "motivation": "现有技能发现方法只关注技能多样性最大化，忽略了人类偏好，导致产生不良甚至危险的行为，如机器人翻滚或进入危险区域。", "method": "提出Foundation model Guided (FoG)方法，利用基础模型提取评分函数来评估状态，基于人类意图为期望状态分配高值、不良状态分配低值，然后用这些分数重新加权技能发现算法的奖励。", "result": "FoG成功消除了翻转、翻滚等不良行为，避免了危险区域，在状态基和像素基任务中都表现良好，还能发现难以定义的行为技能。", "conclusion": "FoG方法有效将人类意图整合到技能发现中，解决了传统方法的安全性问题，同时保持了技能的多样性，为下游任务提供了更安全的技能基础。"}}
{"id": "2510.22747", "pdf": "https://arxiv.org/pdf/2510.22747", "abs": "https://arxiv.org/abs/2510.22747", "authors": ["Eeham Khan", "Firas Saidani", "Owen Van Esbroeck", "Richard Khoury", "Leila Kosseim"], "title": "Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to LREC 2026", "summary": "Despite the widespread adoption of large language models (LLMs), their\nstrongest capabilities remain largely confined to a small number of\nhigh-resource languages for which there is abundant training data. Recently,\ncontinual pre-training (CPT) has emerged as a means to fine-tune these models\nto low-resource regional dialects. In this paper, we study the use of CPT for\ndialect learning under tight data and compute budgets. Using low-rank\nadaptation (LoRA) and compute-efficient continual pre-training, we adapt three\nLLMs to the Qu\\'ebec French dialect using a very small dataset and benchmark\nthem on the COLE suite. Our experiments demonstrate an improvement on the\nminority dialect benchmarks with minimal regression on the prestige language\nbenchmarks with under 1% of model parameters updated. Analysis of the results\ndemonstrate that gains are highly contingent on corpus composition. These\nfindings indicate that CPT with parameter-efficient fine-tuning (PEFT) can\nnarrow the dialect gap by providing cost-effective and sustainable language\nresource creation, expanding high-quality LLM access to minority linguistic\ncommunities. We release the first Qu\\'ebec French LLMs on HuggingFace.", "AI": {"tldr": "本研究使用持续预训练和参数高效微调方法，以极少的参数量（<1%）和数据量成功将大语言模型适配到魁北克法语方言，在方言基准上获得提升的同时保持标准法语性能。", "motivation": "解决大语言模型主要局限于高资源语言的问题，探索在有限数据和计算预算下将LLM适配到低资源方言的方法。", "method": "采用低秩适应（LoRA）和计算高效的持续预训练方法，使用小数据集对三个LLM进行魁北克法语适配，并在COLE基准套件上进行评估。", "result": "实验显示在少数民族方言基准上获得改进，同时标准语言基准仅有最小程度的性能回归，仅更新了不到1%的模型参数。", "conclusion": "CPT结合PEFT能以成本效益高的方式缩小方言差距，为少数民族语言社区提供高质量LLM访问，研究结果高度依赖语料库组成。"}}
{"id": "2510.23214", "pdf": "https://arxiv.org/pdf/2510.23214", "abs": "https://arxiv.org/abs/2510.23214", "authors": ["Robin Schmöcker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm", "categories": ["cs.AI"], "comment": null, "summary": "We introduce a novel, drop-in modification to Monte Carlo Tree Search's\n(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC\nbenchmark problems show that AUPO clearly outperforms MCTS. AUPO is an\nautomatic action abstraction algorithm that solely relies on reward\ndistribution statistics acquired during the MCTS. Thus, unlike other automatic\nabstraction algorithms, AUPO requires neither access to transition\nprobabilities nor does AUPO require a directed acyclic search graph to build\nits abstraction, allowing AUPO to detect symmetric actions that\nstate-of-the-art frameworks like ASAP struggle with when the resulting\nsymmetric states are far apart in state space. Furthermore, as AUPO only\naffects the decision policy, it is not mutually exclusive with other\nabstraction techniques that only affect the tree search.", "AI": {"tldr": "AUPO是一种基于奖励分布统计的自动动作抽象算法，作为MCTS的即插即用修改，在IPPC基准测试中明显优于标准MCTS，能检测对称动作且无需转移概率或DAG搜索图", "motivation": "解决现有自动抽象算法需要转移概率和DAG搜索图的限制，以及无法有效处理状态空间中相距较远的对称状态的问题", "method": "基于MCTS过程中获取的奖励分布统计，开发自动动作抽象算法AUPO，仅影响决策策略而不影响树搜索", "result": "在IPPC基准问题上的比较显示AUPO明显优于MCTS，能检测到ASAP等先进框架难以处理的对称动作", "conclusion": "AUPO是一种有效的自动动作抽象方法，无需额外信息需求，可与仅影响树搜索的其他抽象技术兼容使用"}}
{"id": "2510.22752", "pdf": "https://arxiv.org/pdf/2510.22752", "abs": "https://arxiv.org/abs/2510.22752", "authors": ["Anooshka Bajaj", "Deven Mahesh Mistry", "Sahaj Singh Maini", "Yash Aggarwal", "Zoran Tiganj"], "title": "Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-context learning is governed by both temporal and semantic relationships,\nshaping how Large Language Models (LLMs) retrieve contextual information.\nAnalogous to human episodic memory, where the retrieval of specific events is\nenabled by separating events that happened at different times, this work probes\nthe ability of various pretrained LLMs, including transformer and state-space\nmodels, to differentiate and retrieve temporally separated events.\nSpecifically, we prompted models with sequences containing multiple\npresentations of the same token, which reappears at the sequence end. By fixing\nthe positions of these repeated tokens and permuting all others, we removed\nsemantic confounds and isolated temporal effects on next-token prediction.\nAcross diverse sequences, models consistently placed the highest probabilities\non tokens following a repeated token, but with a notable bias for those nearest\nthe beginning or end of the input. An ablation experiment linked this\nphenomenon in transformers to induction heads. Extending the analysis to unique\nsemantic contexts with partial overlap further demonstrated that memories\nembedded in the middle of a prompt are retrieved less reliably. Despite\narchitectural differences, state-space and transformer models showed comparable\ntemporal biases. Our findings deepen the understanding of temporal biases in\nin-context learning and offer an illustration of how these biases can enable\ntemporal separation and episodic retrieval.", "AI": {"tldr": "论文研究了大型语言模型在上下文学习中的时间偏差问题，通过实验发现LLMs在检索信息时存在显著的首位和近因效应，中间位置的信息检索可靠性较低，这一现象在transformer和state-space模型中表现相似。", "motivation": "受人类情景记忆的启发，研究想要探究预训练语言模型是否能够像人类一样区分和检索时间上分离的事件，理解LLMs在上下文学习中如何处理时间关系。", "method": "使用包含相同token多次出现的序列提示模型，固定重复token位置并置换其他token来消除语义干扰，隔离时间因素对下一个token预测的影响，进行了消融实验分析transformer中的归纳头机制。", "result": "模型始终对重复token后的token赋予最高概率，但存在对序列开头和结尾位置的明显偏好；中间位置的记忆检索可靠性较低；transformer和state-space模型表现出相似的时间偏差模式。", "conclusion": "研究揭示了LLMs在上下文学习中存在系统性的时间偏差，这种偏差有助于实现时间分离和情景检索，增进了对模型内部记忆机制的理解，为改进模型设计提供了 insights。"}}
{"id": "2510.23216", "pdf": "https://arxiv.org/pdf/2510.23216", "abs": "https://arxiv.org/abs/2510.23216", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Jean-Philippe Barrette-LaPierre", "Florian Fuchs", "Brady Chen", "Micheal Jones", "Linus Gisslén"], "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries.", "AI": {"tldr": "提出一种针对游戏行业优化的样本高效深度强化学习方法，通过利用预收集数据和增加网络可塑性，在EA SPORTS FC 25中训练的门将AI比内置AI的扑救率高10%，训练速度快50%，且行为更接近人类玩家。", "motivation": "虽然深度强化学习在游戏AI研究中取得进展，但由于需要大量资源和训练超人类智能体，游戏行业难以实际应用。需要为资源有限的游戏工作室开发能够训练类人智能体的高效方法。", "method": "改进基于价值的深度强化学习方法，利用预收集数据提高样本效率，增加网络可塑性，专门针对游戏工业环境中的智能体训练和微调。", "result": "在EA SPORTS FC 25中训练的门将智能体扑救率比内置AI高10%，训练速度比标准DRL方法快50%，专家评估显示行为更接近人类玩家。", "conclusion": "该方法成功解决了游戏行业中DRL应用的实际问题，证明了在工业环境中训练高效、类人智能体的可行性，并将被应用于该游戏系列的后续版本中。"}}
{"id": "2510.22758", "pdf": "https://arxiv.org/pdf/2510.22758", "abs": "https://arxiv.org/abs/2510.22758", "authors": ["Li Zhou", "Lutong Yu", "You Lyu", "Yihang Lin", "Zefeng Zhao", "Junyi Ao", "Yuhao Zhang", "Benyou Wang", "Haizhou Li"], "title": "EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models", "categories": ["cs.CL"], "comment": "Speech Language Models, Spoken Language Understanding, Vocal Cue\n  Perception, Empathetic Dialogue, Benchmark Evaluation", "summary": "Speech Language Models (SLMs) have made significant progress in spoken\nlanguage understanding. Yet it remains unclear whether they can fully perceive\nnon lexical vocal cues alongside spoken words, and respond with empathy that\naligns with both emotional and contextual factors. Existing benchmarks\ntypically evaluate linguistic, acoustic, reasoning, or dialogue abilities in\nisolation, overlooking the integration of these skills that is crucial for\nhuman-like, emotionally intelligent conversation. We present EchoMind, the\nfirst interrelated, multi-level benchmark that simulates the cognitive process\nof empathetic dialogue through sequential, context-linked tasks: spoken-content\nunderstanding, vocal-cue perception, integrated reasoning, and response\ngeneration. All tasks share identical and semantically neutral scripts that are\nfree of explicit emotional or contextual cues, and controlled variations in\nvocal style are used to test the effect of delivery independent of the\ntranscript. EchoMind is grounded in an empathy-oriented framework spanning 3\ncoarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and\nevaluated using both objective and subjective metrics. Testing 12 advanced SLMs\nreveals that even state-of-the-art models struggle with high-expressive vocal\ncues, limiting empathetic response quality. Analyses of prompt strength, speech\nsource, and ideal vocal cue recognition reveal persistent weaknesses in\ninstruction-following, resilience to natural speech variability, and effective\nuse of vocal cues for empathy. These results underscore the need for SLMs that\nintegrate linguistic content with diverse vocal cues to achieve truly\nempathetic conversational ability.", "AI": {"tldr": "EchoMind是一个新颖的多层级基准测试，专门评估语音语言模型在共情对话中整合语言内容、声学线索和上下文推理的能力，发现当前最先进模型在处理高表达性声音线索方面仍有显著局限。", "motivation": "现有基准测试通常孤立评估语言、声学、推理或对话能力，而忽视了这些技能在实现类人化、情感智能对话中的整合需求，特别是模型是否能同时感知非词汇声音线索并做出符合情感和上下文因素的共情回应。", "method": "提出EchoMind基准，通过顺序的、上下文关联的任务模拟共情对话的认知过程：口语内容理解、声音线索感知、整合推理和回应生成。使用语义中性的相同脚本，控制声音风格变化来独立测试表达效果，基于包含3个粗粒度和12个细粒度维度、39个声音属性的共情导向框架进行评估。", "result": "测试12个先进SLM显示，即使最先进模型也难以处理高表达性声音线索，限制了共情回应质量。在指令遵循、自然语音变化的韧性以及有效利用声音线索实现共情方面存在持续弱点。", "conclusion": "研究结果强调了需要开发能够整合语言内容与多样化声音线索的SLM，以实现真正的共情对话能力，当前模型在这方面仍有显著改进空间。"}}
{"id": "2510.23221", "pdf": "https://arxiv.org/pdf/2510.23221", "abs": "https://arxiv.org/abs/2510.23221", "authors": ["Hong Wang", "Wenkai Yang", "Jie Wang", "Huanshuo Dong", "Zijie Geng", "Zhen Huang", "Depeng Xie", "Zhezheng Hao", "Hande Dong"], "title": "Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action", "categories": ["cs.AI", "physics.comp-ph"], "comment": null, "summary": "Recent advances in data-driven approaches, such as neural operators (NOs),\nhave shown substantial efficacy in reducing the solution time for integrated\ncircuit (IC) thermal simulations. However, a limitation of these approaches is\nrequiring a large amount of high-fidelity training data, such as chip\nparameters and temperature distributions, thereby incurring significant\ncomputational costs. To address this challenge, we propose a novel algorithm\nfor the generation of IC thermal simulation data, named block Krylov and\noperator action (BlocKOA), which simultaneously accelerates the data generation\nprocess and enhances the precision of generated data. BlocKOA is specifically\ndesigned for IC applications. Initially, we use the block Krylov algorithm\nbased on the structure of the heat equation to quickly obtain a few basic\nsolutions. Then we combine them to get numerous temperature distributions that\nsatisfy the physical constraints. Finally, we apply heat operators on these\nfunctions to determine the heat source distributions, efficiently generating\nprecise data points. Theoretical analysis shows that the time complexity of\nBlocKOA is one order lower than the existing method. Experimental results\nfurther validate its efficiency, showing that BlocKOA achieves a 420-fold\nspeedup in generating thermal simulation data for 5000 chips with varying\nphysical parameters and IC structures. Even with just 4% of the generation\ntime, data-driven approaches trained on the data generated by BlocKOA exhibits\ncomparable performance to that using the existing method.", "AI": {"tldr": "提出BlocKOA算法，用于高效生成集成电路热仿真数据，相比现有方法实现420倍加速，同时保证数据精度，解决了数据驱动方法需要大量高保真训练数据的高计算成本问题。", "motivation": "现有神经网络算子等数据驱动方法需要大量高保真训练数据（如芯片参数和温度分布），导致计算成本显著。需要一种能够同时加速数据生成过程并提高生成数据精度的方法。", "method": "提出BlocKOA算法：1）基于热传导方程结构使用块Krylov算法快速获取基础解；2）组合这些解得到满足物理约束的多种温度分布；3）应用热算子确定热源分布，高效生成精确数据点。", "result": "理论分析显示BlocKOA时间复杂度比现有方法低一个数量级。实验验证其效率，在生成5000个不同物理参数和IC结构芯片的热仿真数据时实现420倍加速。仅用4%的生成时间，基于BlocKOA生成数据训练的数据驱动方法性能与使用现有方法相当。", "conclusion": "BlocKOA算法成功解决了集成电路热仿真数据生成的高计算成本问题，大幅提升了数据生成效率，同时保持了数据精度，为数据驱动方法提供了高效的数据生成解决方案。"}}
{"id": "2510.22763", "pdf": "https://arxiv.org/pdf/2510.22763", "abs": "https://arxiv.org/abs/2510.22763", "authors": ["Yasmin Moslem", "Muhammad Hazim Al Farouq", "John D. Kelleher"], "title": "Iterative Layer Pruning for Efficient Translation Inference", "categories": ["cs.CL", "cs.PF"], "comment": "WMT 2025", "summary": "Large language models (LLMs) have transformed many areas of natural language\nprocessing, including machine translation. However, efficient deployment of\nLLMs remains challenging due to their intensive computational requirements. In\nthis paper, we address this challenge and present our submissions to the Model\nCompression track at the Conference on Machine Translation (WMT 2025). In our\nexperiments, we investigate iterative layer pruning guided by layer importance\nanalysis. We evaluate this method using the Aya-Expanse-8B model for\ntranslation from Czech to German, and from English to Egyptian Arabic. Our\napproach achieves substantial reductions in model size and inference time,\nwhile maintaining the translation quality of the baseline models.", "AI": {"tldr": "本文提出基于层重要性分析的迭代层剪枝方法，在保持翻译质量的同时显著减小模型大小和推理时间", "motivation": "大型语言模型在机器翻译等NLP任务中表现出色，但其巨大的计算需求使得高效部署面临挑战", "method": "采用迭代层剪枝策略，通过层重要性分析指导剪枝过程，使用Aya-Expanse-8B模型在捷克语-德语和英语-埃及阿拉伯语翻译任务上进行评估", "result": "方法实现了模型大小和推理时间的显著减少，同时保持了基准模型的翻译质量", "conclusion": "迭代层剪枝是一种有效的模型压缩方法，能够在保持性能的同时解决LLMs部署中的计算效率问题"}}
{"id": "2510.23304", "pdf": "https://arxiv.org/pdf/2510.23304", "abs": "https://arxiv.org/abs/2510.23304", "authors": ["Riccardo Romanello", "Daniele Lizzio Bosco", "Jacopo Cossio", "Dusan Sutulovic", "Giuseppe Serra", "Carla Piazza", "Paolo Burelli"], "title": "CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach", "categories": ["cs.AI"], "comment": null, "summary": "CNOT gates are fundamental to quantum computing, as they facilitate\nentanglement, a crucial resource for quantum algorithms. Certain classes of\nquantum circuits are constructed exclusively from CNOT gates. Given their\nwidespread use, it is imperative to minimise the number of CNOT gates employed.\nThis problem, known as CNOT minimisation, remains an open challenge, with its\ncomputational complexity yet to be fully characterised. In this work, we\nintroduce a novel reinforcement learning approach to address this task. Instead\nof training multiple reinforcement learning agents for different circuit sizes,\nwe use a single agent up to a fixed size $m$. Matrices of sizes different from\nm are preprocessed using either embedding or Gaussian striping. To assess the\nefficacy of our approach, we trained an agent with m = 8, and evaluated it on\nmatrices of size n that range from 3 to 15. The results we obtained show that\nour method overperforms the state-of-the-art algorithm as the value of n\nincreases.", "AI": {"tldr": "本文提出了一种基于强化学习的新方法来解决CNOT门最小化问题，使用单一智能体处理不同规模的量子电路，在3-15规模的矩阵上表现优于现有最优算法。", "motivation": "CNOT门是量子计算中的基本组件，用于产生量子纠缠。减少CNOT门数量对量子算法效率至关重要，但CNOT最小化问题仍是一个计算复杂度未完全明确的开放挑战。", "method": "采用强化学习方法，使用单一智能体处理固定规模m的电路，对不同规模矩阵采用嵌入或高斯条带化预处理。研究中训练了m=8的智能体。", "result": "在规模n从3到15的矩阵上评估，结果显示随着n值增大，该方法的表现优于当前最优算法。", "conclusion": "强化学习方法在CNOT门最小化问题上展现出优势，特别是在处理较大规模量子电路时，为量子计算资源优化提供了有效解决方案。"}}
{"id": "2510.22768", "pdf": "https://arxiv.org/pdf/2510.22768", "abs": "https://arxiv.org/abs/2510.22768", "authors": ["Haoyi Qiu", "Yilun Zhou", "Pranav Narayanan Venkit", "Kung-Hsiang Huang", "Jiaxin Zhang", "Nanyun Peng", "Chien-Sheng Wu"], "title": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion", "categories": ["cs.CL"], "comment": null, "summary": "As Large Vision-Language Models (LVLMs) are increasingly deployed in domains\nsuch as shopping, health, and news, they are exposed to pervasive persuasive\ncontent. A critical question is how these models function as persuadees-how and\nwhy they can be influenced by persuasive multimodal inputs. Understanding both\ntheir susceptibility to persuasion and the effectiveness of different\npersuasive strategies is crucial, as overly persuadable models may adopt\nmisleading beliefs, override user preferences, or generate unethical or unsafe\noutputs when exposed to manipulative messages. We introduce MMPersuade, a\nunified framework for systematically studying multimodal persuasion dynamics in\nLVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs\nimages and videos with established persuasion principles across commercial,\nsubjective and behavioral, and adversarial contexts, and (ii) an evaluation\nframework that quantifies both persuasion effectiveness and model\nsusceptibility via third-party agreement scoring and self-estimated token\nprobabilities on conversation histories. Our study of six leading LVLMs as\npersuadees yields three key insights: (i) multimodal inputs substantially\nincrease persuasion effectiveness-and model susceptibility-compared to text\nalone, especially in misinformation scenarios; (ii) stated prior preferences\ndecrease susceptibility, yet multimodal information maintains its persuasive\nadvantage; and (iii) different strategies vary in effectiveness across\ncontexts, with reciprocity being most potent in commercial and subjective\ncontexts, and credibility and logic prevailing in adversarial contexts. By\njointly analyzing persuasion effectiveness and susceptibility, MMPersuade\nprovides a principled foundation for developing models that are robust,\npreference-consistent, and ethically aligned when engaging with persuasive\nmultimodal content.", "AI": {"tldr": "MMPersuade是一个研究大型视觉语言模型在多媒体内容中如何被说服的框架，包含多模态数据集和评估方法，发现多模态输入比纯文本更具说服力，特别是在错误信息场景中。", "motivation": "随着大型视觉语言模型在购物、健康、新闻等领域的部署增加，它们面临大量说服性内容。需要了解模型如何被多模态说服内容影响，因为过度易被说服的模型可能采纳误导性信念、覆盖用户偏好或生成不道德输出。", "method": "提出MMPersuade框架，包含：(1)综合多模态数据集，将图像和视频与商业、主观行为及对抗场景中的说服原则配对；(2)评估框架，通过第三方协议评分和对话历史的自我估计标记概率来量化说服效果和模型易感性。", "result": "对六个领先LVLM的研究发现：(1)多模态输入相比纯文本显著提高说服效果和模型易感性，特别是在错误信息场景；(2)声明的先前偏好降低易感性，但多模态信息仍保持说服优势；(3)不同策略在不同场景效果不同，互惠在商业和主观场景最有效，可信度和逻辑在对抗场景占主导。", "conclusion": "MMPersuade通过联合分析说服效果和易感性，为开发在面对说服性多模态内容时具有鲁棒性、偏好一致性和伦理对齐的模型提供了原则性基础。"}}
{"id": "2510.23340", "pdf": "https://arxiv.org/pdf/2510.23340", "abs": "https://arxiv.org/abs/2510.23340", "authors": ["Anwesha Das", "John Duff", "Jörg Hoffmann", "Vera Demberg"], "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "11 pages, 3 figures", "summary": "Adaptive agent design offers a way to improve human-AI collaboration on\ntime-sensitive tasks in rapidly changing environments. In such cases, to ensure\nthe human maintains an accurate understanding of critical task elements, an\nassistive agent must not only identify the highest priority information but\nalso estimate how and when this information can be communicated most\neffectively, given that human attention represents a zero-sum cognitive\nresource where focus on one message diminishes awareness of other or upcoming\ninformation. We introduce a theoretical framework for adaptive signalling which\nmeets these challenges by using principles of rational communication,\nformalised as Bayesian reference resolution using the Rational Speech Act (RSA)\nmodelling framework, to plan a sequence of messages which optimise timely\nalignment between user belief and a dynamic environment. The agent adapts\nmessage specificity and timing to the particulars of a user and scenario based\non projections of how prior-guided interpretation of messages will influence\nattention to the interface and subsequent belief update, across several\ntimesteps out to a fixed horizon. In a comparison to baseline methods, we show\nthat this effectiveness depends crucially on combining multi-step planning with\na realistic model of user awareness. As the first application of RSA for\ncommunication in a dynamic environment, and for human-AI interaction in\ngeneral, we establish theoretical foundations for pragmatic communication in\nhuman-agent teams, highlighting how insights from cognitive science can be\ncapitalised to inform the design of assistive agents.", "AI": {"tldr": "本文提出了一个基于理性言语行为（RSA）建模框架的自适应信号理论框架，通过多步规划和用户意识建模来优化人机协作中的信息传递时机和特异性。", "motivation": "在快速变化的环境中，确保人类准确理解关键任务信息是一个挑战，因为人类注意力是有限的零和认知资源，需要智能体不仅识别优先级信息，还要估计如何最有效地在何时传递这些信息。", "method": "使用贝叶斯参考解析的理性沟通原则，通过Rational Speech Act（RSA）建模框架来规划消息序列，根据用户和场景特点自适应调整消息的特异性和时机，基于多步预测来优化用户信念与环境对齐。", "result": "与基线方法相比，该方法在多步规划与真实用户意识模型结合方面表现出关键的有效性，证明了其在动态环境中通信的优越性。", "conclusion": "作为RSA在动态环境通信和人类-AI交互中的首次应用，本研究为人机团队的实用沟通建立了理论基础，展示了如何利用认知科学见解来指导辅助智能体的设计。"}}
{"id": "2510.22775", "pdf": "https://arxiv.org/pdf/2510.22775", "abs": "https://arxiv.org/abs/2510.22775", "authors": ["Junjielong Xu", "Boyin Tan", "Xiaoyuan Liu", "Chao Peng", "Pengfei Gao", "Pinjia He"], "title": "Scalable Supervising Software Agents with Patch Reasoner", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "While large language model agents have advanced software engineering tasks,\nthe unscalable nature of existing test-based supervision is limiting the\npotential improvement of data scaling. The reason is twofold: (1) building and\nrunning test sandbox is rather heavy and fragile, and (2) data with\nhigh-coverage tests is naturally rare and threatened by test hacking via edge\ncases. In this paper, we propose R4P, a patch verifier model to provide\nscalable rewards for training and testing SWE agents via reasoning. We consider\nthat patch verification is fundamentally a reasoning task, mirroring how human\nrepository maintainers review patches without writing and running new\nreproduction tests. To obtain sufficient reference and reduce the risk of\nreward hacking, R4P uses a group-wise objective for RL training, enabling it to\nverify multiple patches against each other's modification and gain a dense\nreward for stable training. R4P achieves 72.2% Acc. for verifying patches from\nSWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we\ndesign and train a lite scaffold, Mini-SE, with pure reinforcement learning\nwhere all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%\nPass@1 on SWE-bench-verified, showing a 10.0% improvement over the original\nQwen3-32B. This can be further improved to 32.8% with R4P for test-time\nscaling. Furthermore, R4P verifies patches within a second, 50x faster than\ntesting on average. The stable scaling curves of rewards and accuracy along\nwith high efficiency reflect R4P's practicality.", "AI": {"tldr": "R4P是一个基于推理的补丁验证模型，为软件工程代理提供可扩展的奖励机制，解决了传统测试监督方法在可扩展性和稳定性方面的问题。", "motivation": "现有基于测试的监督方法存在两个问题：(1)构建和运行测试沙箱成本高且脆弱；(2)高覆盖率测试数据稀缺且容易受到边缘案例测试攻击的威胁，限制了数据扩展的潜力。", "method": "提出R4P补丁验证模型，将其视为推理任务，模拟人类代码审查者不编写新测试就能审查补丁的方式。采用组间目标进行强化学习训练，通过相互比较多个补丁修改来获得密集奖励。", "result": "R4P在SWE-bench-verified上达到72.2%的补丁验证准确率，超越OpenAI o3。基于R4P训练的Mini-SE模型在SWE-bench-verified上达到26.2%的Pass@1，比原Qwen3-32B提升10.0%。R4P验证补丁仅需1秒，比平均测试速度快50倍。", "conclusion": "R4P通过推理为基础的补丁验证提供了可扩展、高效的奖励机制，实现了稳定的扩展曲线和高效率，展示了其在软件工程代理训练中的实用性。"}}
{"id": "2510.23384", "pdf": "https://arxiv.org/pdf/2510.23384", "abs": "https://arxiv.org/abs/2510.23384", "authors": ["Pratik N. Kalamkar", "A. G. Phakatkar"], "title": "Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, 4 figures, Conference Paper", "summary": "Opinions are central to almost all human activities and are key influencers\nof our behaviors. In current times due to growth of social networking website\nand increase in number of e-commerce site huge amount of opinions are now\navailable on web. Given a set of evaluative statements that contain opinions\n(or sentiments) about an Entity, opinion mining aims to extract attributes and\ncomponents of the object that have been commented on in each statement and to\ndetermine whether the comments are positive, negative or neutral. While lot of\nresearch recently has been done in field of opinion mining and some of it\ndealing with ranking of entities based on review or opinion set, classifying\nopinions into finer granularity level and then ranking entities has never been\ndone before. In this paper method for opinion mining from statements at a\ndeeper level of granularity is proposed. This is done by using fuzzy logic\nreasoning, after which entities are ranked as per this information.", "AI": {"tldr": "该论文提出了一种基于模糊逻辑的细粒度意见挖掘方法，用于从评论文本中提取更详细的属性评价，并基于这些细粒度信息对实体进行排序。", "motivation": "当前社交媒体和电商网站产生了大量意见数据，现有研究主要进行情感分类和基于整体评价的实体排序，但缺乏对意见的细粒度分析和基于细粒度信息的排序方法。", "method": "使用模糊逻辑推理方法进行深层次的细粒度意见挖掘，提取评论中针对实体具体属性的评价信息。", "result": "开发了一种能够从评论文本中挖掘更细致意见信息的方法，并实现了基于细粒度评价的实体排序。", "conclusion": "该方法填补了意见挖掘领域中细粒度分析和基于细粒度信息进行实体排序的研究空白，为更精准的意见分析和实体评价提供了新途径。"}}
{"id": "2510.22798", "pdf": "https://arxiv.org/pdf/2510.22798", "abs": "https://arxiv.org/abs/2510.22798", "authors": ["Thu Phuong Nguyen", "Duc M. Nguyen", "Hyotaek Jeon", "Hyunwook Lee", "Hyunmin Song", "Sungahn Ko", "Taehwan Kim"], "title": "VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions", "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2025. Project Website: https://vehme.github.io/", "summary": "Automatically assessing handwritten mathematical solutions is an important\nproblem in educational technology with practical applications, but it remains a\nsignificant challenge due to the diverse formats, unstructured layouts, and\nsymbolic complexity of student work. To address this challenge, we introduce\nVEHME-a Vision-Language Model for Evaluating Handwritten Mathematics\nExpressions-designed to assess open-form handwritten math responses with high\naccuracy and interpretable reasoning traces. VEHME integrates a two-phase\ntraining pipeline: (i) supervised fine-tuning using structured reasoning data,\nand (ii) reinforcement learning that aligns model outputs with\nmulti-dimensional grading objectives, including correctness, reasoning depth,\nand error localization. To enhance spatial understanding, we propose an\nExpression-Aware Visual Prompting Module, trained on our synthesized multi-line\nmath expressions dataset to robustly guide attention in visually heterogeneous\ninputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art\nperformance among open-source models and approaches the accuracy of proprietary\nsystems, demonstrating its potential as a scalable and accessible tool for\nautomated math assessment. Our training and experiment code is publicly\navailable at our GitHub repository.", "AI": {"tldr": "VEHME是一种用于评估手写数学表达式的视觉语言模型，通过两阶段训练流程和表达式感知视觉提示模块，在开放形式的手写数学答案评估中实现了高精度和可解释性。", "motivation": "手写数学解决方案的自动评估在教育技术中很重要但具有挑战性，因为学生作业格式多样、布局非结构化且符号复杂。", "method": "采用两阶段训练流程：(i)使用结构化推理数据进行监督微调，(ii)通过强化学习将模型输出与多维评分目标对齐。提出表达式感知视觉提示模块增强空间理解。", "result": "在AIHub和FERMAT数据集上评估，VEHME在开源模型中达到最先进性能，接近专有系统的准确性。", "conclusion": "VEHME展示了作为可扩展和易获取的自动化数学评估工具的潜力，训练和实验代码已公开。"}}
{"id": "2510.23408", "pdf": "https://arxiv.org/pdf/2510.23408", "abs": "https://arxiv.org/abs/2510.23408", "authors": ["Abolfazl Younesi", "Zahra Najafabadi Samani", "Thomas Fahringer"], "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.LG", "cs.MA"], "comment": "Under review", "summary": "Data pipelines are essential in stream processing as they enable the\nefficient collection, processing, and delivery of real-time data, supporting\nrapid data analysis. In this paper, we present AutoStreamPipe, a novel\nframework that employs Large Language Models (LLMs) to automate the design,\ngeneration, and deployment of stream processing pipelines. AutoStreamPipe\nbridges the semantic gap between high-level user intent and platform-specific\nimplementations across distributed stream processing systems for structured\nmulti-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an\nextended version of GoT. AutoStreamPipe combines resilient execution\nstrategies, advanced query analysis, and HGoT to deliver pipelines with good\naccuracy. Experimental evaluations on diverse pipelines demonstrate that\nAutoStreamPipe significantly reduces development time (x6.3) and error rates\n(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM\ncode-generation methods.", "AI": {"tldr": "AutoStreamPipe是一个基于大语言模型的自动化流处理管道框架，通过超图思维(HGoT)技术实现高效的设计、生成和部署，显著提升开发效率和准确性。", "motivation": "解决流处理管道开发中高层用户意图与平台特定实现之间的语义鸿沟，减少手动开发的时间和错误率。", "method": "结合弹性执行策略、高级查询分析和超图思维(HGoT)技术，实现多智能体推理的自动化管道生成。", "result": "实验表明，相比传统LLM代码生成方法，开发时间减少6.3倍，错误率降低5.19倍(通过新型无错误评分EFS衡量)。", "conclusion": "AutoStreamPipe框架有效解决了流处理管道自动化的关键挑战，为实时数据分析提供了高效可靠的解决方案。"}}
{"id": "2510.22823", "pdf": "https://arxiv.org/pdf/2510.22823", "abs": "https://arxiv.org/abs/2510.22823", "authors": ["Poli Nemkova", "Amrit Adhikari", "Matthew Pearson", "Vamsi Krishna Sadu", "Mark V. Albert"], "title": "Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humanitarian organizations face a critical choice: invest in costly\ncommercial APIs or rely on free open-weight models for multilingual human\nrights monitoring. While commercial systems offer reliability, open-weight\nalternatives lack empirical validation -- especially for low-resource languages\ncommon in conflict zones. This paper presents the first systematic comparison\nof commercial and open-weight large language models (LLMs) for\nhuman-rights-violation detection across seven languages, quantifying the\ncost-reliability trade-off facing resource-constrained organizations. Across\n78,000 multilingual inferences, we evaluate six models -- four\ninstruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,\nGPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both\nstandard classification metrics and new measures of cross-lingual reliability:\nCalibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),\nand Language Stability Score (LSS). Results show that alignment, not scale,\ndetermines stability: aligned models maintain near-invariant accuracy and\nbalanced calibration across typologically distant and low-resource languages\n(e.g., Lingala, Burmese), while open-weight models exhibit significant\nprompt-language sensitivity and calibration drift. These findings demonstrate\nthat multilingual alignment enables language-agnostic reasoning and provide\npractical guidance for humanitarian organizations balancing budget constraints\nwith reliability in multilingual deployment.", "AI": {"tldr": "本文首次系统比较了商业API和开源大语言模型在7种语言上的人权侵犯检测性能，发现模型对齐而非规模决定了跨语言稳定性，为资源有限的人道组织提供了成本-可靠性权衡的实用指南。", "motivation": "人道组织面临选择：投资昂贵的商业API还是依赖免费开源模型进行多语言人权监控。商业系统可靠但昂贵，开源模型缺乏实证验证，特别是在冲突地区常见的低资源语言上。", "method": "在78,000次多语言推理中评估6个模型（4个对齐模型和2个开源模型），使用标准分类指标和新的跨语言可靠性指标：校准偏差、决策偏差、语言鲁棒性评分和语言稳定性评分。", "result": "对齐模型在类型学差异大和低资源语言上保持近乎不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。对齐而非规模决定了稳定性。", "conclusion": "多语言对齐实现了语言无关的推理，为人道组织在多语言部署中平衡预算约束和可靠性提供了实用指导。"}}
{"id": "2510.23410", "pdf": "https://arxiv.org/pdf/2510.23410", "abs": "https://arxiv.org/abs/2510.23410", "authors": ["Jiahao Ji", "Tianyu Wang", "Yeshu Li", "Yushen Huo", "Zhilin Zhang", "Chuan Yu", "Jian Xu", "Bo Zheng"], "title": "Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens", "categories": ["cs.AI"], "comment": "12 pages, KDD 2025", "summary": "Auto-bidding is crucial in facilitating online advertising by automatically\nproviding bids for advertisers. While previous work has made great efforts to\nmodel bidding environments for better ad performance, it has limitations in\ngeneralizability across environments since these models are typically tailored\nfor specific bidding scenarios. To this end, we approach the\nscenario-independent principles through a unified function that estimates the\nachieved effect under specific bids, such as budget consumption, gross\nmerchandise volume (GMV), page views, etc. Then, we propose a bidding\nfoundation model Bid2X to learn this fundamental function from data in various\nscenarios. Our Bid2X is built over uniform series embeddings that encode\nheterogeneous data through tailored embedding methods. To capture complex\ninter-variable and dynamic temporal dependencies in bidding data, we propose\ntwo attention mechanisms separately treating embeddings of different variables\nand embeddings at different times as attention tokens for representation\nlearning. On top of the learned variable and temporal representations, a\nvariable-aware fusion module is used to perform adaptive bidding outcome\nprediction. To model the unique bidding data distribution, we devise a\nzero-inflated projection module to incorporate the estimated non-zero\nprobability into its value prediction, which makes up a joint optimization\nobjective containing classification and regression. The objective is proven to\nconverge to the zero-inflated distribution. Our model has been deployed on the\nad platform in Taobao, one of the world's largest e-commerce platforms. Offline\nevaluation on eight datasets exhibits Bid2X's superiority compared to various\nbaselines and its generality across different scenarios. Bid2X increased GMV by\n4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding\nfoundation model in computational advertising.", "AI": {"tldr": "该论文提出了Bid2X竞价基础模型，通过统一函数估计不同竞价场景下的广告效果，使用注意力机制处理异构数据和时间依赖关系，在淘宝平台部署后显著提升了广告效果指标。", "motivation": "现有的自动竞价模型通常针对特定竞价场景设计，缺乏跨环境泛化能力，需要开发一个能够适应不同场景的统一竞价基础模型。", "method": "提出Bid2X模型，使用统一序列嵌入处理异构数据，设计两种注意力机制分别处理变量间和时间上的依赖关系，采用变量感知融合模块进行自适应预测，并使用零膨胀投影模块处理数据分布特性。", "result": "在8个数据集上的离线评估显示Bid2X优于各种基线方法，在线A/B测试中GMV提升4.65%，投资回报率提升2.44%。", "conclusion": "Bid2X作为一个竞价基础模型，成功实现了跨场景的泛化能力，为计算广告领域的竞价建模开辟了新途径，并在实际电商平台上验证了其有效性。"}}
{"id": "2510.22830", "pdf": "https://arxiv.org/pdf/2510.22830", "abs": "https://arxiv.org/abs/2510.22830", "authors": ["Haowei Hua", "Hong Jiao", "Xinyi Wang"], "title": "Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays", "categories": ["cs.CL", "cs.LG"], "comment": "19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence\n  in Measurement and Education Conference (AIME-Con)", "summary": "BERT and its variants are extensively explored for automated scoring.\nHowever, a limit of 512 tokens for these encoder-based models showed the\ndeficiency in automated scoring of long essays. Thus, this research explores\ngenerative language models for automated scoring of long essays via\nsummarization and prompting. The results revealed great improvement of scoring\naccuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab\nAutomated Essay Scoring 2.0 dataset.", "AI": {"tldr": "本研究探索使用生成式语言模型通过摘要和提示技术对长文进行自动评分，相比BERT等编码器模型在长文评分上的局限性取得了显著改进", "motivation": "BERT等编码器模型受限于512个token的长度限制，在长文自动评分方面存在不足，需要探索更有效的解决方案", "method": "采用生成式语言模型，通过摘要技术和提示工程方法对长文进行自动评分", "result": "评分准确性显著提升，在Learning Agency Lab Automated Essay Scoring 2.0数据集上，QWK分数从0.822提高到0.8878", "conclusion": "生成式语言模型结合摘要和提示技术能有效解决长文自动评分问题，比传统编码器模型表现更优"}}
{"id": "2510.23424", "pdf": "https://arxiv.org/pdf/2510.23424", "abs": "https://arxiv.org/abs/2510.23424", "authors": ["Elouanes Khelifi", "Amir Saki", "Usef Faghihi"], "title": "Causal Deep Q Network", "categories": ["cs.AI"], "comment": null, "summary": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement\nlearning tasks. However, their reliance on associative learning often leads to\nthe acquisition of spurious correlations, hindering their problem-solving\ncapabilities. In this paper, we introduce a novel approach to integrate causal\nprinciples into DQNs, leveraging the PEACE (Probabilistic Easy vAriational\nCausal Effect) formula for estimating causal effects. By incorporating causal\nreasoning during training, our proposed framework enhances the DQN's\nunderstanding of the underlying causal structure of the environment, thereby\nmitigating the influence of confounding factors and spurious correlations. We\ndemonstrate that integrating DQNs with causal capabilities significantly\nenhances their problem-solving capabilities without compromising performance.\nExperimental results on standard benchmark environments showcase that our\napproach outperforms conventional DQNs, highlighting the effectiveness of\ncausal reasoning in reinforcement learning. Overall, our work presents a\npromising avenue for advancing the capabilities of deep reinforcement learning\nagents through principled causal inference.", "AI": {"tldr": "将因果推理与DQN结合，使用PEACE公式估计因果效应，减少虚假相关性，提升强化学习性能", "motivation": "传统DQN依赖关联学习容易获得虚假相关性，限制了其问题解决能力，需要引入因果推理来理解环境中的因果结构", "method": "提出新框架将因果原则整合到DQN中，利用PEACE（概率易变因果效应）公式进行因果效应估计，在训练过程中加入因果推理", "result": "实验结果表明该方法在标准基准环境中优于传统DQN，显著增强了问题解决能力且不牺牲性能", "conclusion": "这项工作为通过原则性因果推断提升深度强化学习智能体能力提供了有前景的途径"}}
{"id": "2510.22844", "pdf": "https://arxiv.org/pdf/2510.22844", "abs": "https://arxiv.org/abs/2510.22844", "authors": ["Prerna Ravi", "Dong Won Lee", "Beatriz Flamia", "Jasmine David", "Brandon Hanks", "Cynthia Breazeal", "Emma Anderson", "Grace Lin"], "title": "Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning", "categories": ["cs.CL"], "comment": "In Submission: Journal of Educational Data Mining (jEDM) 2026", "summary": "Understanding how ideas develop and flow in small-group conversations is\ncritical for analyzing collaborative learning. A key structural feature of\nthese interactions is threading, the way discourse talk naturally organizes\ninto interwoven topical strands that evolve over time. While threading has been\nwidely studied in asynchronous text settings, detecting threads in synchronous\nspoken dialogue remains challenging due to overlapping turns and implicit cues.\nAt the same time, large language models (LLMs) show promise for automating\ndiscourse analysis but often struggle with long-context tasks that depend on\ntracing these conversational links. In this paper, we investigate whether\nexplicit thread linkages can improve LLM-based coding of relational moves in\ngroup talk. We contribute a systematic guidebook for identifying threads in\nsynchronous multi-party transcripts and benchmark different LLM prompting\nstrategies for automated threading. We then test how threading influences\nperformance on downstream coding of conversational analysis frameworks, that\ncapture core collaborative actions such as agreeing, building, and eliciting.\nOur results show that providing clear conversational thread information\nimproves LLM coding performance and underscores the heavy reliance of\ndownstream analysis on well-structured dialogue. We also discuss practical\ntrade-offs in time and cost, emphasizing where human-AI hybrid approaches can\nyield the best value. Together, this work advances methods for combining LLMs\nand robust conversational thread structures to make sense of complex, real-time\ngroup interactions.", "AI": {"tldr": "该研究探讨了如何利用明确的对话线程信息来提升大语言模型在同步多人群组对话中对关系性话语行为的自动编码性能，提出了线程识别指南并测试了不同提示策略。", "motivation": "同步口语对话中的话题线程检测具有挑战性（由于重叠话轮和隐含线索），而大语言模型在处理需要追踪对话链接的长上下文任务时存在困难，需要探索如何通过明确的线程链接来改进基于LLM的关系性话语分析。", "method": "开发了同步多人群组转录本中线程识别的系统指南，对不同LLM提示策略进行基准测试，然后测试线程信息对下游会话分析框架编码性能的影响。", "result": "提供清晰的对话线程信息能够提高LLM编码性能，表明下游分析严重依赖良好结构的对话。", "conclusion": "这项工作推进了将LLM与稳健的对话线程结构相结合的方法，以理解复杂的实时群组互动，并讨论了人机混合方法在时间和成本方面的实际权衡。"}}
{"id": "2510.23443", "pdf": "https://arxiv.org/pdf/2510.23443", "abs": "https://arxiv.org/abs/2510.23443", "authors": ["Chiara Bonfanti", "Alessandro Druetto", "Cataldo Basile", "Tharindu Ranasinghe", "Marcos Zampieri"], "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "comment": "7 pages", "summary": "The growing intersection of cybersecurity and law creates a complex\ninformation space where traditional legal research tools struggle to deal with\nnuanced connections between cases, statutes, and technical vulnerabilities.\nThis knowledge divide hinders collaboration between legal experts and\ncybersecurity professionals. To address this important gap, this work provides\na first step towards intelligent systems capable of navigating the increasingly\nintricate cyber-legal domain. We demonstrate promising initial results on\nmultilingual tasks.", "AI": {"tldr": "本文提出了一种智能系统来解决网络安全与法律交叉领域的信息检索难题，旨在弥合法律专家与网络安全专业人员之间的知识鸿沟。", "motivation": "网络安全与法律的交叉领域形成了复杂的信息空间，传统法律研究工具难以处理案件、法规和技术漏洞之间的细微联系，这阻碍了法律专家与网络安全专业人员的协作。", "method": "开发能够导航复杂网络法律领域的智能系统，作为解决这一重要差距的第一步。", "result": "在多语言任务上展示了有希望的初步结果。", "conclusion": "这项工作为解决网络法律领域的复杂信息检索问题提供了初步但有前景的解决方案，为未来更深入的跨领域协作奠定了基础。"}}
{"id": "2510.22849", "pdf": "https://arxiv.org/pdf/2510.22849", "abs": "https://arxiv.org/abs/2510.22849", "authors": ["Adam Stein", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NeurIPS 2025. 34 pages, 7 figures", "summary": "Large language models (LLMs) excel at zero-shot inference but continue to\nstruggle with complex, multi-step reasoning. Recent methods that augment LLMs\nwith intermediate reasoning steps such as Chain of Thought (CoT) and Program of\nThought (PoT) improve performance but often produce undesirable solutions,\nespecially in algorithmic domains. We introduce Per-Instance Program Synthesis\n(PIPS), a method that generates and refines programs at the instance-level\nusing structural feedback without relying on task-specific guidance or explicit\ntest cases. To further improve performance, PIPS incorporates a confidence\nmetric that dynamically chooses between direct inference and program synthesis\non a per-instance basis. Experiments across three frontier LLMs and 30\nbenchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question\nanswering tasks, relational reasoning tasks, and mathematical reasoning tasks\nshow that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and\n9.4% compared to PoT and CoT respectively, and reduces undesirable program\ngenerations by 65.1% on the algorithmic tasks compared to PoT with\nGemini-2.0-Flash.", "AI": {"tldr": "PIPS方法通过实例级程序合成和结构反馈，在复杂推理任务中显著优于CoT和PoT方法，提升准确率并减少不良程序生成。", "motivation": "大型语言模型在零样本推理方面表现优异，但在复杂多步推理任务中仍存在困难。现有的CoT和PoT方法虽然有所改进，但在算法领域经常产生不理想的解决方案。", "method": "提出PIPS方法：1) 在实例级别生成和精炼程序；2) 使用结构反馈而不依赖任务特定指导或显式测试用例；3) 引入置信度指标动态选择直接推理或程序合成策略。", "result": "在3个前沿LLM和30个基准测试中，PIPS相比PoT和CoT分别提升绝对调和平均准确率8.6%和9.4%，在算法任务中比PoT减少65.1%的不良程序生成。", "conclusion": "PIPS通过实例级程序合成和自适应策略选择，有效提升了复杂推理任务的性能，为LLM在算法领域的应用提供了更可靠的解决方案。"}}
{"id": "2510.23453", "pdf": "https://arxiv.org/pdf/2510.23453", "abs": "https://arxiv.org/abs/2510.23453", "authors": ["Marco Grossi"], "title": "What are the odds? Risk and uncertainty about AI existential risk", "categories": ["cs.AI"], "comment": "10 pages", "summary": "This work is a commentary of the article\n\\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a\nTaxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and\nHawthorne. It is not just a commentary though, but a useful reminder of the\nphilosophical limitations of \\say{linear} models of risk. The article will\nfocus on the model employed by the authors: first, I discuss some differences\nbetween standard Swiss Cheese models and this one. I then argue that in a\nsituation of epistemic indifference the probability of P(D) is higher than what\none might first suggest, given the structural relationships between layers. I\nthen distinguish between risk and uncertainty, and argue that any estimation of\nP(D) is structurally affected by two kinds of uncertainty: option uncertainty\nand state-space uncertainty. Incorporating these dimensions of uncertainty into\nour qualitative discussion on AI existential risk can provide a better\nunderstanding of the likeliness of P(D).", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2510.22860", "pdf": "https://arxiv.org/pdf/2510.22860", "abs": "https://arxiv.org/abs/2510.22860", "authors": ["Linyang He", "Tianjun Zhong", "Richard Antonello", "Gavin Mischler", "Micah Goldblum", "Nima Mesgarani"], "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement", "categories": ["cs.CL", "q-bio.NC"], "comment": "Accepted at NeurIPS 2025", "summary": "Understanding how the human brain progresses from processing simple\nlinguistic inputs to performing high-level reasoning is a fundamental challenge\nin neuroscience. While modern large language models (LLMs) are increasingly\nused to model neural responses to language, their internal representations are\nhighly \"entangled,\" mixing information about lexicon, syntax, meaning, and\nreasoning. This entanglement biases conventional brain encoding analyses toward\nlinguistically shallow features (e.g., lexicon and syntax), making it difficult\nto isolate the neural substrates of cognitively deeper processes. Here, we\nintroduce a residual disentanglement method that computationally isolates these\ncomponents. By first probing an LM to identify feature-specific layers, our\nmethod iteratively regresses out lower-level representations to produce four\nnearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,\nreasoning. We used these disentangled embeddings to model intracranial (ECoG)\nbrain recordings from neurosurgical patients listening to natural speech. We\nshow that: 1) This isolated reasoning embedding exhibits unique predictive\npower, accounting for variance in neural activity not explained by other\nlinguistic features and even extending to the recruitment of visual regions\nbeyond classical language areas. 2) The neural signature for reasoning is\ntemporally distinct, peaking later (~350-400ms) than signals related to\nlexicon, syntax, and meaning, consistent with its position atop a processing\nhierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as\ntheir predictive success is primarily attributable to linguistically shallow\nfeatures, masking the more subtle contributions of deeper cognitive processing.", "AI": {"tldr": "该研究提出了一种残差解耦方法，从大语言模型中分离出词汇、句法、语义和推理四个正交的表示成分，用于建模大脑对自然语言的处理过程，发现推理成分具有独特的神经表征和时序特征。", "motivation": "现有大语言模型的内部表示高度'纠缠'，混合了词汇、句法、语义和推理信息，这导致传统脑编码分析偏向浅层语言特征，难以分离深层认知过程的神经基础。", "method": "采用残差解耦方法，首先探测语言模型以识别特征特定层，然后迭代回归掉低层表示，生成四个近乎正交的嵌入表示（词汇、句法、语义和推理）。", "result": "1) 分离出的推理嵌入具有独特预测能力，解释了其他语言特征无法解释的神经活动变异；2) 推理的神经信号在时间上更晚出现（约350-400ms）；3) 标准非解耦嵌入会误导分析，其预测成功主要归因于浅层特征。", "conclusion": "该方法成功分离了语言处理的不同认知成分，揭示了推理过程具有独特的神经表征和时序特征，为理解大脑语言处理层次提供了新视角。"}}
{"id": "2510.23474", "pdf": "https://arxiv.org/pdf/2510.23474", "abs": "https://arxiv.org/abs/2510.23474", "authors": ["Shames Al Mandalawi", "Muzakkiruddin Ahmed Mohammed", "Hendrika Maclean", "Mert Can Cakmak", "John R. Talburt"], "title": "Policy-Aware Generative AI for Safe, Auditable Data Access Governance", "categories": ["cs.AI"], "comment": "The 17th International Conference on Knowledge and Systems\n  Engineering", "summary": "Enterprises need access decisions that satisfy least privilege, comply with\nregulations, and remain auditable. We present a policy aware controller that\nuses a large language model (LLM) to interpret natural language requests\nagainst written policies and metadata, not raw data. The system, implemented\nwith Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context\ninterpretation, user validation, data classification, business purpose test,\ncompliance mapping, and risk synthesis) with early hard policy gates and deny\nby default. It returns APPROVE, DENY, CONDITIONAL together with cited controls\nand a machine readable rationale. We evaluate on fourteen canonical cases\nacross seven scenario families using a privacy preserving benchmark. Results\nshow Exact Decision Match improving from 10/14 to 13/14 (92.9\\%) after applying\npolicy gates, DENY recall rising to 1.00, False Approval Rate on must-deny\nfamilies dropping to 0, and Functional Appropriateness and Compliance Adherence\nat 14/14. Expert ratings of rationale quality are high, and median latency is\nunder one minute. These findings indicate that policy constrained LLM\nreasoning, combined with explicit gates and audit trails, can translate human\nreadable policies into safe, compliant, and traceable machine decisions.", "AI": {"tldr": "论文提出了一种基于大语言模型的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略进行匹配，实现安全、合规且可追溯的访问决策。", "motivation": "企业需要满足最小权限原则、符合法规要求且可审计的访问决策，传统方法难以有效处理自然语言请求与复杂策略的匹配问题。", "method": "使用Google Gemini 2.0 Flash实现六阶段推理框架：上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险综合，采用早期硬策略门控和默认拒绝机制。", "result": "在14个典型案例测试中，精确决策匹配率从10/14提升到13/14（92.9%），拒绝召回率达到1.00，必须拒绝场景的错误批准率降为0，功能适当性和合规性均为14/14。", "conclusion": "策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策，中位延迟低于一分钟。"}}
{"id": "2510.22866", "pdf": "https://arxiv.org/pdf/2510.22866", "abs": "https://arxiv.org/abs/2510.22866", "authors": ["Tiasa Singha Roy", "Ayush Rajesh Jhaveri", "Ilias Triantafyllopoulos"], "title": "Interpreting and Mitigating Unwanted Uncertainty in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite their impressive capabilities, Large Language Models (LLMs) exhibit\nunwanted uncertainty, a phenomenon where a model changes a previously correct\nanswer into an incorrect one when re-prompted. This behavior undermines trust\nand poses serious risks in high-stakes domains. In this work, we investigate\nthe mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack\nretrieval framework and integrate a Flip-style re-evaluation prompt to simulate\nrealistic answer-flipping scenarios. We find that retrieval heads are not\nprimarily responsible for avoiding uncertainty. Instead, we identify a small\nset of non-retrieval attention heads that disproportionately attend to\nmisleading tokens in uncertain contexts. Masking these heads yields significant\nimprovements, reducing flip behavior by up to 15% without introducing\nincoherence or overcorrection. However, when tested for downstream tasks, we\nobserve trade-offs with flip behavior. Our findings contribute to the growing\nfield of mechanistic interpretability and present a simple yet effective\ntechnique for mitigating uncertainty-driven failure modes in LLMs.", "AI": {"tldr": "大型语言模型存在不确定性现象，即重新提示时会改变之前正确的答案为错误答案。研究发现非检索注意力头是主要原因，屏蔽这些头可减少15%的翻转行为，但在下游任务中存在权衡。", "motivation": "大型语言模型在重新提示时会改变正确答案为错误答案，这种不确定性行为在高风险领域存在严重风险，需要研究其机制并找到缓解方法。", "method": "采用Needle-in-a-Haystack检索框架，集成Flip式重新评估提示来模拟答案翻转场景，分析注意力机制并识别导致问题的特定注意力头。", "result": "发现检索头不是避免不确定性的主要原因，而是识别出一小部分非检索注意力头过度关注误导性标记。屏蔽这些头可减少高达15%的翻转行为，且不引入不连贯或过度校正。", "conclusion": "研究为机制可解释性领域做出贡献，提出了一种简单有效的技术来缓解LLMs中的不确定性驱动故障模式，但在下游任务应用中需要权衡考虑。"}}
{"id": "2510.23476", "pdf": "https://arxiv.org/pdf/2510.23476", "abs": "https://arxiv.org/abs/2510.23476", "authors": ["Sima Noorani", "Shayan Kiyani", "George Pappas", "Hamed Hassani"], "title": "Human-AI Collaborative Uncertainty Quantification", "categories": ["cs.AI", "cs.HC", "stat.ML"], "comment": null, "summary": "AI predictive systems are increasingly embedded in decision making pipelines,\nshaping high stakes choices once made solely by humans. Yet robust decisions\nunder uncertainty still rely on capabilities that current AI lacks: domain\nknowledge not captured by data, long horizon context, and reasoning grounded in\nthe physical world. This gap has motivated growing efforts to design\ncollaborative frameworks that combine the complementary strengths of humans and\nAI. This work advances this vision by identifying the fundamental principles of\nHuman AI collaboration within uncertainty quantification, a key component of\nreliable decision making. We introduce Human AI Collaborative Uncertainty\nQuantification, a framework that formalizes how an AI model can refine a human\nexpert's proposed prediction set with two goals: avoiding counterfactual harm,\nensuring the AI does not degrade correct human judgments, and complementarity,\nenabling recovery of correct outcomes the human missed. At the population\nlevel, we show that the optimal collaborative prediction set follows an\nintuitive two threshold structure over a single score function, extending a\nclassical result in conformal prediction. Building on this insight, we develop\npractical offline and online calibration algorithms with provable distribution\nfree finite sample guarantees. The online method adapts to distribution shifts,\nincluding human behavior evolving through interaction with AI, a phenomenon we\ncall Human to AI Adaptation. Experiments across image classification,\nregression, and text based medical decision making show that collaborative\nprediction sets consistently outperform either agent alone, achieving higher\ncoverage and smaller set sizes across various conditions.", "AI": {"tldr": "提出人类-AI协作不确定性量化框架，通过AI优化人类专家的预测集，避免对正确判断的损害并补全人类遗漏的结果，在多种任务中优于单独使用人类或AI。", "motivation": "当前AI在不确定性下的鲁棒决策仍缺乏领域知识、长时上下文和物理世界推理能力，需要结合人类与AI的互补优势进行协作决策。", "method": "引入人类-AI协作不确定性量化框架，开发基于双阈值结构的离线/在线校准算法，具有分布无关的有限样本保证，能适应分布偏移和人类行为变化。", "result": "在图像分类、回归和医疗决策等任务中，协作预测集始终优于单独使用人类或AI，实现了更高的覆盖率和更小的集合大小。", "conclusion": "该框架为人类-AI协作提供了理论基础和实用算法，证明了在不确定性量化中结合人类与AI优势的重要性，为可靠决策系统设计提供了新方向。"}}
{"id": "2510.22874", "pdf": "https://arxiv.org/pdf/2510.22874", "abs": "https://arxiv.org/abs/2510.22874", "authors": ["Rajarshi Roy", "Nasrin Imanpour", "Ashhar Aziz", "Shashwat Bajpai", "Gurpreet Singh", "Shwetangshu Biswas", "Kapil Wanaskar", "Parth Patwa", "Subhankar Ghosh", "Shreyas Dixit", "Nilesh Ranjan Pal", "Vipula Rawte", "Ritvik Garimella", "Gaytri Jena", "Amit Sheth", "Vasu Sharma", "Aishwarya Naresh Reganti", "Vinija Jain", "Aman Chadha", "Amitava Das"], "title": "A Comprehensive Dataset for Human vs. AI Generated Text Detection", "categories": ["cs.CL"], "comment": "Defactify4 @AAAI 2025", "summary": "The rapid advancement of large language models (LLMs) has led to increasingly\nhuman-like AI-generated text, raising concerns about content authenticity,\nmisinformation, and trustworthiness. Addressing the challenge of reliably\ndetecting AI-generated text and attributing it to specific models requires\nlarge-scale, diverse, and well-annotated datasets. In this work, we present a\ncomprehensive dataset comprising over 58,000 text samples that combine\nauthentic New York Times articles with synthetic versions generated by multiple\nstate-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,\nYi-Large, and GPT-4-o. The dataset provides original article abstracts as\nprompts, full human-authored narratives. We establish baseline results for two\nkey tasks: distinguishing human-written from AI-generated text, achieving an\naccuracy of 58.35\\%, and attributing AI texts to their generating models with\nan accuracy of 8.92\\%. By bridging real-world journalistic content with modern\ngenerative models, the dataset aims to catalyze the development of robust\ndetection and attribution methods, fostering trust and transparency in the era\nof generative AI. Our dataset is available at:\nhttps://huggingface.co/datasets/gsingh1-py/train.", "AI": {"tldr": "该研究构建了一个包含5.8万+文本样本的综合数据集，结合纽约时报真实文章和多种先进LLM生成的合成文本，为AI文本检测和模型溯源任务提供基准数据。", "motivation": "随着LLM生成文本越来越像人类写作，需要解决内容真实性、错误信息和可信度问题，这要求有大规模、多样化且标注良好的数据集来开发可靠的检测方法。", "method": "创建包含真实纽约时报文章和Gemma-2-9b、Mistral-7B、Qwen-2-72B、LLaMA-8B、Yi-Large、GPT-4-o等多种先进LLM生成文本的数据集，提供原文摘要作为提示和完整人类撰写叙述。", "result": "建立了两个关键任务的基准结果：区分人类写作与AI生成文本的准确率为58.35%，将AI文本溯源到生成模型的准确率为8.92%。", "conclusion": "通过将真实新闻内容与现代生成模型相结合，该数据集旨在促进稳健检测和溯源方法的发展，在生成式AI时代培养信任和透明度。"}}
{"id": "2510.23487", "pdf": "https://arxiv.org/pdf/2510.23487", "abs": "https://arxiv.org/abs/2510.23487", "authors": ["Roham Koohestani", "Ziyou Li", "Anton Podkopaev", "Maliheh Izadi"], "title": "Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy", "categories": ["cs.AI", "cs.FL"], "comment": null, "summary": "This paper establishes a formal equivalence between the architectural classes\nof modern agentic AI systems and the abstract machines of the Chomsky\nhierarchy. We posit that the memory architecture of an AI agent is the\ndefinitive feature determining its computational power and that it directly\nmaps it to a corresponding class of automaton. Specifically, we demonstrate\nthat simple reflex agents are equivalent to Finite Automata, hierarchical\ntask-decomposition agents are equivalent to Pushdown Automata, and agents\nemploying readable/writable memory for reflection are equivalent to TMs. This\nAutomata-Agent Framework provides a principled methodology for right-sizing\nagent architectures to optimize computational efficiency and cost. More\ncritically, it creates a direct pathway to formal verification, enables the\napplication of mature techniques from automata theory to guarantee agent safety\nand predictability. By classifying agents, we can formally delineate the\nboundary between verifiable systems and those whose behavior is fundamentally\nundecidable. We address the inherent probabilistic nature of LLM-based agents\nby extending the framework to probabilistic automata that allow quantitative\nrisk analysis. The paper concludes by outlining an agenda for developing static\nanalysis tools and grammars for agentic frameworks.", "AI": {"tldr": "该论文建立了现代智能体AI系统架构类别与乔姆斯基层级抽象机器之间的形式等价关系，提出基于内存架构的智能体计算能力分类框架", "motivation": "为智能体系统提供理论基础和形式化验证方法，解决AI智能体的安全性、可预测性和计算效率优化问题", "method": "通过建立智能体架构与自动机类别的映射关系：简单反射智能体对应有限自动机，分层任务分解智能体对应下推自动机，具有读写内存的反思智能体对应图灵机", "result": "提出了Automata-Agent框架，能够对智能体进行形式化分类，并为概率性LLM智能体扩展了概率自动机框架以支持定量风险分析", "conclusion": "该框架为智能体系统的形式验证、安全性保证和静态分析工具开发提供了理论基础，并规划了未来研究方向"}}
{"id": "2510.22876", "pdf": "https://arxiv.org/pdf/2510.22876", "abs": "https://arxiv.org/abs/2510.22876", "authors": ["Ranran Haoran Zhang", "Soumik Dey", "Ashirbad Mishra", "Hansi Wu", "Binbin Li", "Rui Zhang"], "title": "Batch Speculative Decoding Done Right", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.", "AI": {"tldr": "论文提出EQSPEC和EXSPEC两种批处理推测解码方法，解决批量处理中的参差不齐张量问题，在保证输出等价性的同时显著提升推理吞吐量。", "motivation": "推测解码在批处理中面临参差不齐张量问题，导致序列对齐破坏、位置ID和注意力掩码损坏，现有实现违反输出等价性要求。", "method": "提出EQSPEC方法保证正确性，分析重对齐开销；引入EXSPEC方法维护滑动序列池和动态分组，减少重对齐开销。", "result": "在SpecBench数据集上，Vicuna、Qwen3和GLM-4等模型上实现最高3倍吞吐量提升，批量大小8时保持95%输出等价性。", "conclusion": "该方法无需定制内核，可与现有推理栈无缝集成，有效解决了批处理推测解码的正确性和效率问题。"}}
{"id": "2510.23506", "pdf": "https://arxiv.org/pdf/2510.23506", "abs": "https://arxiv.org/abs/2510.23506", "authors": ["Hyeongseop Rha", "Jeong Hun Yeo", "Yeonju Kim", "Yong Man Ro"], "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier", "categories": ["cs.AI", "cs.HC"], "comment": "16 pages, 11 figures", "summary": "The recent advancement of Multimodal Large Language Models (MLLMs) is\ntransforming human-computer interaction (HCI) from surface-level exchanges into\nmore nuanced and emotionally intelligent communication. To realize this shift,\nemotion understanding becomes essential allowing systems to capture subtle cues\nunderlying user intent. Furthermore, providing faithful explanations for\npredicted emotions is crucial to ensure interpretability and build user trust.\nHowever, current MLLM-based methods often generate emotion explanations that\ndiverge from the target labels and sometimes even contradict their own\npredicted emotions. This inconsistency poses a critical risk for\nmisunderstanding and erodes reliability in interactive settings. To address\nthis, we propose a novel approach: the Emotional Rationale Verifier (ERV) and\nan Explanation Reward. Our method guides the model to produce reasoning that is\nexplicitly consistent with the target emotion during multimodal emotion\nrecognition without modifying the model architecture or requiring additional\npaired video-description annotations. Our method significantly improves\nfaithful explanation-prediction consistency and explanation emotion accuracy on\nthe MAFW and DFEW datasets. Through extensive experiments and human\nevaluations, we show that our approach not only enhances alignment between\nexplanation and prediction but also empowers MLLMs to deliver emotionally\ncoherent, trustworthy interactions, marking a key step toward truly human-like\nHCI systems.", "AI": {"tldr": "提出情感推理验证器(ERV)和解释奖励机制，解决多模态大语言模型在情感识别中解释与预测不一致的问题，提高情感解释的忠实度和准确性", "motivation": "当前多模态大语言模型在情感理解时，生成的情感解释往往与目标标签不一致，甚至与自身预测的情感相矛盾，这影响了系统的可解释性和用户信任", "method": "提出情感推理验证器(ERV)和解释奖励机制，在不修改模型架构或需要额外视频-描述标注的情况下，引导模型产生与目标情感一致的解释", "result": "在MAFW和DFEW数据集上显著提高了解释-预测一致性和解释情感准确性，通过实验和人工评估验证了方法的有效性", "conclusion": "该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信的交互，是实现真正类人交互系统的关键一步"}}
{"id": "2510.22907", "pdf": "https://arxiv.org/pdf/2510.22907", "abs": "https://arxiv.org/abs/2510.22907", "authors": ["Yifan Zhang", "Lanser Contributors"], "title": "Language Server CLI Empowers Language Agents with Process Rewards", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "comment": "Project Page: https://github.com/yifanzhang-pro/lanser-cli", "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli", "AI": {"tldr": "Lanser-CLI是一个CLI优先的编排层，通过钉住和中介语言服务器协议(LSP)服务器，为编码代理和CI提供确定性、可重放的工作流程，解决大语言模型在API幻觉和编辑定位错误方面的问题。", "motivation": "大语言模型经常产生API幻觉和错误定位编辑，而语言服务器能够计算关于真实代码的经过验证的IDE级事实。需要一种方法将语言服务器的确定性能力与AI代理的工作流程相结合。", "method": "开发Lanser-CLI工具，包含：1) 通过Selector DSL实现稳健的寻址方案；2) 确定性分析包标准化语言服务器响应；3) 为变异操作提供安全信封；4) 基于语言服务器事实的过程奖励函数。", "result": "实现了在冻结快照下的确定性，建立了过程奖励的单调性属性，使其适用于过程监督和反事实分析。", "conclusion": "语言服务器不仅提供结构信息，还提供可操作的过程奖励，Lanser-CLI成功地将语言服务器的验证能力与AI代理工作流程集成，提供了机器检查的逐步信号。"}}
{"id": "2510.23524", "pdf": "https://arxiv.org/pdf/2510.23524", "abs": "https://arxiv.org/abs/2510.23524", "authors": ["KC Santosh", "Rodrigue Rizk", "Longwei Wang"], "title": "Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "The rapid advancement of Artificial Intelligence (AI) has led to\nunprecedented computational demands, raising significant environmental and\nethical concerns. This paper critiques the prevailing reliance on large-scale,\nstatic datasets and monolithic training paradigms, advocating for a shift\ntoward human-inspired, sustainable AI solutions. We introduce a novel\nframework, Human AI (HAI), which emphasizes incremental learning, carbon-aware\noptimization, and human-in-the-loop collaboration to enhance adaptability,\nefficiency, and accountability. By drawing parallels with biological cognition\nand leveraging dynamic architectures, HAI seeks to balance performance with\necological responsibility. We detail the theoretical foundations, system\ndesign, and operational principles that enable AI to learn continuously and\ncontextually while minimizing carbon footprints and human annotation costs. Our\napproach addresses pressing challenges in active learning, continual\nadaptation, and energy-efficient model deployment, offering a pathway toward\nresponsible, human-centered artificial intelligence.", "AI": {"tldr": "论文提出Human AI (HAI)框架，通过增量学习、碳感知优化和人机协作，实现可持续、高效和负责任的人工智能发展。", "motivation": "人工智能快速发展带来巨大计算需求和环境伦理问题，批评当前依赖大规模静态数据集和单一训练范式的做法。", "method": "引入HAI框架，采用增量学习、碳感知优化、人机协作，借鉴生物认知和动态架构设计。", "result": "提出理论基础、系统设计和操作原则，使AI能够持续情境学习，同时减少碳足迹和人工标注成本。", "conclusion": "HAI框架为解决主动学习、持续适应和节能模型部署等挑战提供了路径，推动负责任、以人为本的人工智能发展。"}}
{"id": "2510.22954", "pdf": "https://arxiv.org/pdf/2510.22954", "abs": "https://arxiv.org/abs/2510.22954", "authors": ["Liwei Jiang", "Yuanjun Chai", "Margaret Li", "Mickel Liu", "Raymond Fok", "Nouha Dziri", "Yulia Tsvetkov", "Maarten Sap", "Alon Albalak", "Yejin Choi"], "title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)", "categories": ["cs.CL"], "comment": "NeurIPS 2025 D&B Paper (Oral); Camera-Ready Version", "summary": "Language models (LMs) often struggle to generate diverse, human-like creative\ncontent, raising concerns about the long-term homogenization of human thought\nthrough repeated exposure to similar outputs. Yet scalable methods for\nevaluating LM output diversity remain limited, especially beyond narrow tasks\nsuch as random number or name generation, or beyond repeated sampling from a\nsingle model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,\nreal-world, open-ended user queries that admit a wide range of plausible\nanswers with no single ground truth. We introduce the first comprehensive\ntaxonomy for characterizing the full spectrum of open-ended prompts posed to\nLMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that\nfurther breaks down to 17 subcategories. Using Infinity-Chat, we present a\nlarge-scale study of mode collapse in LMs, revealing a pronounced Artificial\nHivemind effect in open-ended generation of LMs, characterized by (1)\nintra-model repetition, where a single model consistently generates similar\nresponses, and more so (2) inter-model homogeneity, where different models\nproduce strikingly similar outputs. Infinity-Chat also includes 31,250 human\nannotations, across absolute ratings and pairwise preferences, with 25\nindependent human annotations per example. This enables studying collective and\nindividual-specific human preferences in response to open-ended queries. Our\nfindings show that LMs, reward models, and LM judges are less well calibrated\nto human ratings on model generations that elicit differing idiosyncratic\nannotator preferences, despite maintaining comparable overall quality. Overall,\nINFINITY-CHAT presents the first large-scale resource for systematically\nstudying real-world open-ended queries to LMs, revealing critical insights to\nguide future research for mitigating long-term AI safety risks posed by the\nArtificial Hivemind.", "AI": {"tldr": "该论文介绍了Infinity-Chat数据集，用于评估语言模型在开放性问题生成中的多样性问题，揭示了模型存在的人工蜂群效应（模式崩溃），即模型内部和不同模型之间都产生高度相似的输出。", "motivation": "语言模型在生成多样化、类人创造性内容方面存在困难，可能导致人类思维的长期同质化，但目前缺乏评估LM输出多样性的可扩展方法。", "method": "引入Infinity-Chat数据集（26K个多样化、开放式用户查询），建立首个全面的开放式提示分类法（6个顶级类别，17个子类别），并进行大规模模式崩溃研究，包含31,250个人工标注。", "result": "发现LM在开放式生成中存在明显的人工蜂群效应：模型内部重复性和模型间同质性；LM、奖励模型和LM评判器在对引发不同个体偏好的人类评分校准方面表现较差。", "conclusion": "Infinity-Chat为系统研究现实世界开放式查询提供了首个大规模资源，揭示了缓解人工蜂群效应带来的长期AI安全风险的关键见解。"}}
{"id": "2510.23532", "pdf": "https://arxiv.org/pdf/2510.23532", "abs": "https://arxiv.org/abs/2510.23532", "authors": ["Anirban Das", "Irtaza Khalid", "Rafael Peñaloza", "Steven Schockaert"], "title": "When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "accepted at NeurIPS 2025 D&B track", "summary": "Designing models that can learn to reason in a systematic way is an important\nand long-standing challenge. In recent years, a wide range of solutions have\nbeen proposed for the specific case of systematic relational reasoning,\nincluding Neuro-Symbolic approaches, variants of the Transformer architecture,\nand specialised Graph Neural Networks. However, existing benchmarks for\nsystematic relational reasoning focus on an overly simplified setting, based on\nthe assumption that reasoning can be reduced to composing relational paths. In\nfact, this assumption is hard-baked into the architecture of several recent\nmodels, leading to approaches that can perform well on existing benchmarks but\nare difficult to generalise to other settings. To support further progress in\nthe field of systematic relational reasoning with neural networks, we introduce\nNoRA, a new benchmark which adds several levels of difficulty and requires\nmodels to go beyond path-based reasoning.", "AI": {"tldr": "论文提出了新的基准测试NoRA，旨在解决现有系统性关系推理基准过于简化的问题，要求模型超越基于路径的推理方法。", "motivation": "现有系统性关系推理基准过于简化，假设推理可简化为关系路径组合，导致模型在现有基准表现良好但难以泛化到其他场景。", "method": "引入NoRA基准测试，增加多个难度级别，要求模型进行超越路径推理的系统性关系推理。", "result": "NoRA基准为神经网络系统性关系推理领域提供了更全面、更具挑战性的评估标准。", "conclusion": "NoRA基准将推动系统性关系推理领域的进一步发展，促使开发更具泛化能力的模型。"}}
{"id": "2510.22956", "pdf": "https://arxiv.org/pdf/2510.22956", "abs": "https://arxiv.org/abs/2510.22956", "authors": ["Anwesan Pal", "Karen Hovsepian", "Tinghao Guo", "Mengnan Zhao", "Somendra Tripathi", "Nikos Kanakaris", "George Mihaila", "Sumit Nigam"], "title": "Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts", "categories": ["cs.CL", "cs.IR"], "comment": "Paper accepted at EMNLP 2025", "summary": "Recent investigations into effective context lengths of modern flagship large\nlanguage models (LLMs) have revealed major limitations in effective question\nanswering (QA) and reasoning over long and complex contexts for even the\nlargest and most impressive cadre of models. While approaches like\nretrieval-augmented generation (RAG) and chunk-based re-ranking attempt to\nmitigate this issue, they are sensitive to chunking, embedding and retrieval\nstrategies and models, and furthermore, rely on extensive pre-processing,\nknowledge acquisition and indexing steps. In this paper, we propose\nTagging-Augmented Generation (TAG), a lightweight data augmentation strategy\nthat boosts LLM performance in long-context scenarios, without degrading and\naltering the integrity and composition of retrieved documents. We validate our\nhypothesis by augmenting two challenging and directly relevant\nquestion-answering benchmarks -- NoLima and NovelQA -- and show that tagging\nthe context or even just adding tag definitions into QA prompts leads to\nconsistent performance gains over the baseline -- up to 17% for 32K token\ncontexts, and 2.9% in complex reasoning question-answering for multi-hop\nqueries requiring knowledge across a wide span of text. Additional details are\navailable at https://sites.google.com/view/tag-emnlp.", "AI": {"tldr": "提出TAG（标签增强生成）方法，通过在长文本中添加标签或标签定义来增强LLM的长上下文问答能力，无需修改原文结构，在多个基准测试中取得显著性能提升", "motivation": "现有大型语言模型在长上下文问答和推理方面存在严重限制，传统方法如RAG和分块重排需要复杂的预处理且对策略敏感", "method": "使用轻量级数据增强策略TAG，通过在检索文档中添加标签或标签定义来增强提示，保持文档完整性", "result": "在NoLima和NovelQA基准测试中，TAG方法相比基线性能提升最高达17%（32K token上下文）和2.9%（复杂多跳推理）", "conclusion": "TAG是一种有效且轻量的长上下文增强方法，能够显著提升LLM性能而不破坏文档完整性"}}
{"id": "2510.23538", "pdf": "https://arxiv.org/pdf/2510.23538", "abs": "https://arxiv.org/abs/2510.23538", "authors": ["Qiushi Sun", "Jingyang Gong", "Yang Liu", "Qiaosheng Chen", "Lei Li", "Kai Chen", "Qipeng Guo", "Ben Kao", "Fei Yuan"], "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress", "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.", "AI": {"tldr": "JanusCode-800K是最大的多模态代码语料库，支持JanusCoder系列模型的训练，该模型能够从文本指令、视觉输入或两者结合生成代码，在文本和视觉编码任务中表现优异。", "motivation": "神经代码智能正从基于文本的源代码扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展，需要解决合成和质量评估的挑战。", "method": "开发了一个完整的合成工具包，利用数据模态之间的协同作用，构建了JanusCode-800K语料库，并训练了JanusCoder和JanusCoderV模型，建立了从文本指令、视觉输入或两者结合生成代码的视觉-编程接口。", "result": "JanusCoder系列模型在文本和视觉编码任务中表现出优越性能，7B到14B规模的模型接近甚至超过商业模型的性能。", "conclusion": "该研究通过构建大规模多模态代码语料库和统一模型，成功解决了视觉-编程接口的生成问题，为协调程序逻辑与视觉表达提供了关键见解，代码和检查点已开源。"}}
{"id": "2510.22967", "pdf": "https://arxiv.org/pdf/2510.22967", "abs": "https://arxiv.org/abs/2510.22967", "authors": ["Yucheng Ning", "Xixun Lin", "Fang Fang", "Yanan Cao"], "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "This article has been accepted by Frontiers of Computer Science (FCS)", "summary": "The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains.", "AI": {"tldr": "本文针对大语言模型在长文本生成中的事实准确性评估难题，提出了集成大规模数据集、多智能体验证机制和加权评估指标的系统方法，构建了中文长文本事实性数据集LongHalluQA和多智能体验证系统MAD-Fact。", "motivation": "大语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对其输出事实准确性的担忧，现有短文本评估方法难以处理长文本中复杂的推理链、交织观点和累积信息的问题。", "method": "提出系统性方法：构建中文长文本事实性数据集LongHalluQA；开发基于辩论的多智能体验证系统MAD-Fact；引入事实重要性层次结构来捕捉长文本中不同主张的重要性差异。", "result": "在两个基准测试上的实验表明，更大的LLM通常保持更高的事实一致性，而国产模型在中文内容上表现更优。", "conclusion": "本研究为评估和提升长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。"}}
{"id": "2510.23553", "pdf": "https://arxiv.org/pdf/2510.23553", "abs": "https://arxiv.org/abs/2510.23553", "authors": ["Alexis Ellis", "Stacie Severyn", "Fjollë Novakazi", "Hadi Banaee", "Cogan Shimizu"], "title": "OntoPret: An Ontology for the Interpretation of Human Behavior", "categories": ["cs.AI"], "comment": null, "summary": "As human machine teaming becomes central to paradigms like Industry 5.0, a\ncritical need arises for machines to safely and effectively interpret complex\nhuman behaviors. A research gap currently exists between techno centric robotic\nframeworks, which often lack nuanced models of human behavior, and descriptive\nbehavioral ontologies, which are not designed for real time, collaborative\ninterpretation. This paper addresses this gap by presenting OntoPret, an\nontology for the interpretation of human behavior. Grounded in cognitive\nscience and a modular engineering methodology, OntoPret provides a formal,\nmachine processable framework for classifying behaviors, including task\ndeviations and deceptive actions. We demonstrate its adaptability across two\ndistinct use cases manufacturing and gameplay and establish the semantic\nfoundations necessary for advanced reasoning about human intentions.", "AI": {"tldr": "OntoPret是一个基于认知科学和模块化工程方法的行为解释本体论，旨在解决技术中心化机器人框架与描述性行为本体论之间的研究空白，提供机器可处理的框架来分类人类行为，包括任务偏差和欺骗性动作。", "motivation": "随着人机协作在工业5.0等范式中变得至关重要，机器需要安全有效地解释复杂人类行为。目前存在技术中心化机器人框架缺乏细致人类行为模型，而描述性行为本体论不适合实时协作解释的研究空白。", "method": "基于认知科学和模块化工程方法论，开发OntoPret本体论，提供形式化的机器可处理框架，用于行为分类（包括任务偏差和欺骗性动作）。在两个不同用例（制造和游戏）中验证其适应性。", "result": "开发了OntoPret本体论框架，能够有效分类人类行为，建立了支持高级人类意图推理所需的语义基础。", "conclusion": "OntoPret成功填补了现有研究空白，为实时协作环境中的人类行为解释提供了有效的本体论解决方案，为高级意图推理奠定了语义基础。"}}
{"id": "2510.22968", "pdf": "https://arxiv.org/pdf/2510.22968", "abs": "https://arxiv.org/abs/2510.22968", "authors": ["Michael Hardy"], "title": "Measuring Teaching with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective and scalable measurement of teaching quality is a persistent\nchallenge in education. While Large Language Models (LLMs) offer potential,\ngeneral-purpose models have struggled to reliably apply complex, authentic\nclassroom observation instruments. This paper uses custom LLMs built on\nsentence-level embeddings, an architecture better suited for the long-form,\ninterpretive nature of classroom transcripts than conventional subword\ntokenization. We systematically evaluate five different sentence embeddings\nunder a data-efficient training regime designed to prevent overfitting. Our\nresults demonstrate that these specialized models can achieve human-level and\neven super-human performance with expert human ratings above 0.65 and\nsurpassing the average human-human rater correlation. Further, through analysis\nof annotation context windows, we find that more advanced models-those better\naligned with human judgments-attribute a larger share of score variation to\nlesson-level features rather than isolated utterances, challenging the\nsufficiency of single-turn annotation paradigms. Finally, to assess external\nvalidity, we find that aggregate model scores align with teacher value-added\nmeasures, indicating they are capturing features relevant to student learning.\nHowever, this trend does not hold at the individual item level, suggesting that\nwhile the models learn useful signals, they have not yet achieved full\ngeneralization. This work establishes a viable and powerful new methodology for\nAI-driven instructional measurement, offering a path toward providing scalable,\nreliable, and valid feedback for educator development.", "AI": {"tldr": "本研究开发了基于句子级嵌入的定制化大语言模型，用于教学质量的客观评估，实现了接近甚至超越人类评估者的性能，并验证了与教师增值测量的外部有效性。", "motivation": "教育领域长期缺乏客观可扩展的教学质量测量方法，传统通用大语言模型在应用复杂的课堂观察工具时可靠性不足。", "method": "使用句子级嵌入架构（而非传统的子词标记化）构建定制化LLMs，系统评估五种句子嵌入模型，采用防过拟合的数据高效训练策略，分析标注上下文窗口。", "result": "专业模型达到人类水平甚至超人类性能（专家人类评分相关性>0.65，超越平均人-人评估相关性），高级模型更多关注课程层面特征而非孤立话语，总体模型分数与教师增值测量一致。", "conclusion": "建立了一种可行且强大的AI驱动教学测量新方法，为实现可扩展、可靠、有效的教育者发展反馈提供了路径，但模型在单项水平上尚未完全泛化。"}}
{"id": "2510.23564", "pdf": "https://arxiv.org/pdf/2510.23564", "abs": "https://arxiv.org/abs/2510.23564", "authors": ["Zhaoyang Yu", "Jiayi Zhang", "Huixue Su", "Yufan Zhao", "Yifan Wu", "Mingyi Deng", "Jinyu Xiang", "Yizhang Lin", "Lingxiao Tang", "Yingchao Li", "Yuyu Luo", "Bang Liu", "Chenglin Wu"], "title": "ReCode: Unify Plan and Action for Universal Granularity Control", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.", "AI": {"tldr": "ReCode提出了一种递归代码生成的新范式，通过将规划和行动统一在单一代码表示中，使LLM智能体能够在不同决策粒度间灵活操作，显著提升了推理性能和数据效率。", "motivation": "现实世界任务需要在不同粒度上进行决策，而当前基于LLM的智能体缺乏这种跨粒度操作能力，因为现有范式将高层次规划与低层次行动严格分离，限制了动态适应性和泛化能力。", "method": "ReCode将高层次计划视为抽象占位函数，然后递归地将其分解为更细粒度的子函数，直到达到原始行动。这种递归方法消除了规划与行动之间的刚性边界。", "result": "大量实验显示ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率。", "conclusion": "通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法，递归结构还能生成丰富的多粒度训练数据，使模型能够学习分层决策过程。"}}
{"id": "2510.23006", "pdf": "https://arxiv.org/pdf/2510.23006", "abs": "https://arxiv.org/abs/2510.23006", "authors": ["Shenran Wang", "Timothy Tin-Long Tse", "Jian Zhu"], "title": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We perform in-depth evaluations of in-context learning (ICL) on\nstate-of-the-art transformer, state-space, and hybrid large language models\nover two categories of knowledge-based ICL tasks. Using a combination of\nbehavioral probing and intervention-based methods, we have discovered that,\nwhile LLMs of different architectures can behave similarly in task performance,\ntheir internals could remain different. We discover that function vectors (FVs)\nresponsible for ICL are primarily located in the self-attention and Mamba\nlayers, and speculate that Mamba2 uses a different mechanism from FVs to\nperform ICL. FVs are more important for ICL involving parametric knowledge\nretrieval, but not for contextual knowledge understanding. Our work contributes\nto a more nuanced understanding across architectures and task types.\nMethodologically, our approach also highlights the importance of combining both\nbehavioural and mechanistic analyses to investigate LLM capabilities.", "AI": {"tldr": "对最先进的transformer、state-space和混合架构大语言模型在知识型上下文学习任务上的深入评估，发现不同架构模型虽然表现相似但内部机制不同，函数向量主要位于自注意力和Mamba层，且在不同知识类型任务中重要性各异", "motivation": "深入研究不同架构大语言模型在上下文学习中的内部工作机制差异，特别是transformer、state-space和混合架构在知识型任务中的表现和机制", "method": "使用行为探测和干预方法相结合的方式，评估模型在两类知识型上下文学习任务上的表现，分析函数向量的位置和作用机制", "result": "发现不同架构LLMs任务表现相似但内部机制不同；函数向量主要位于自注意力和Mamba层；Mamba2可能使用不同于函数向量的机制；函数向量在参数化知识检索任务中更重要，在上下文知识理解中作用较小", "conclusion": "研究提供了对不同架构和任务类型的更细致理解，强调了结合行为和机制分析对于研究LLM能力的重要性"}}
{"id": "2510.23578", "pdf": "https://arxiv.org/pdf/2510.23578", "abs": "https://arxiv.org/abs/2510.23578", "authors": ["Joachim Baumann", "Aleksandra Urman", "Ulrich Leicht-Deobald", "Zachary J. Roman", "Anikó Hannák", "Markus Christen"], "title": "Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study", "categories": ["cs.AI"], "comment": null, "summary": "The rapid adoption of generative artificial intelligence (GenAI) technologies\nhas led many organizations to integrate AI into their products and services,\noften without considering user preferences. Yet, public attitudes toward AI\nuse, especially in impactful decision-making scenarios, are underexplored.\nUsing a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)\nrepresentative of the Swiss population, we examine shifts in public attitudes\ntoward AI before and after the launch of ChatGPT. We find that the GenAI boom\nis significantly associated with reduced public acceptance of AI (see Figure 1)\nand increased demand for human oversight in various decision-making contexts.\nThe proportion of respondents finding AI \"not acceptable at all\" increased from\n23% to 30%, while support for human-only decision-making rose from 18% to 26%.\nThese shifts have amplified existing social inequalities in terms of widened\neducational, linguistic, and gender gaps post-boom. Our findings challenge\nindustry assumptions about public readiness for AI deployment and highlight the\ncritical importance of aligning technological development with evolving public\npreferences.", "AI": {"tldr": "研究通过瑞士大规模调查发现，ChatGPT推出后公众对AI的接受度下降，对人机监督需求增加，并扩大了教育、语言和性别方面的社会不平等", "motivation": "生成式AI技术快速普及但缺乏对用户偏好的考虑，公众对AI在重要决策场景中的态度研究不足", "method": "使用瑞士人口代表性的大规模两波调查（第一波1514人，第二波1488人），比较ChatGPT发布前后的公众态度变化", "result": "生成式AI热潮显著降低了公众对AI的接受度，完全不可接受AI的比例从23%升至30%；支持纯人类决策的比例从18%升至26%；扩大了教育、语言和性别差距", "conclusion": "研究结果挑战了行业对公众AI接受度的假设，强调技术发展必须与不断变化的公众偏好保持一致的重要性"}}
{"id": "2510.23011", "pdf": "https://arxiv.org/pdf/2510.23011", "abs": "https://arxiv.org/abs/2510.23011", "authors": ["Sammriddh Gupta", "Sonit Singh", "Aditya Joshi", "Mira Kim"], "title": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "14 pages", "summary": "Language educators strive to create a rich experience for learners, while\nthey may be restricted in the extend of feedback and practice they can provide.\nWe present the design and development of LangLingual, a conversational agent\nbuilt using the LangChain framework and powered by Large Language Models. The\nsystem is specifically designed to provide real-time, grammar-focused feedback,\ngenerate context-aware language exercises and track learner proficiency over\ntime. The paper discusses the architecture, implementation and evaluation of\nLangLingual in detail. The results indicate strong usability, positive learning\noutcomes and encouraging learner engagement.", "AI": {"tldr": "LangLingual是一个基于LangChain框架和大型语言模型构建的对话代理系统，专门为语言学习者提供实时语法反馈、情境感知的语言练习和学习进度追踪。", "motivation": "语言教育者希望为学习者创造丰富的学习体验，但在提供反馈和练习方面可能受到限制，需要技术解决方案来扩展教学能力。", "method": "使用LangChain框架和大型语言模型构建对话代理系统，设计包含实时语法反馈、情境感知练习生成和学习进度追踪功能的架构。", "result": "系统表现出良好的可用性，产生了积极的学习成果，并获得了学习者的高度参与和认可。", "conclusion": "LangLingual系统成功解决了语言教育中的反馈限制问题，通过技术手段有效提升了语言学习体验和效果，具有实际应用价值。"}}
{"id": "2510.23595", "pdf": "https://arxiv.org/pdf/2510.23595", "abs": "https://arxiv.org/abs/2510.23595", "authors": ["Yixing Chen", "Yiding Wang", "Siqi Zhu", "Haofei Yu", "Tao Feng", "Muhan Zhan", "Mostofa Patwary", "Jiaxuan You"], "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution", "categories": ["cs.AI"], "comment": "29 pages, 4 figures, submitted to ICLR 2026", "summary": "Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision.", "AI": {"tldr": "MAE是一个多智能体自我进化框架，通过三个角色（提议者、求解者、评判者）的协同进化，使用强化学习提升大语言模型的推理能力，无需人类标注数据，在多个基准测试中平均提升4.54%。", "motivation": "现有强化学习方法依赖人类标注数据和可验证奖励，限制了扩展性和通用性；而现有的自我博弈方法需要特定环境反馈，难以扩展到通用领域。", "method": "提出MAE框架，使用单一LLM实例化三个交互智能体：提议者生成问题，求解者尝试解答，评判者进行评估，三者通过强化学习协同进化。", "result": "在Qwen2.5-3B-Instruct模型上，MAE在数学、推理和常识问答等多个基准测试中实现了平均4.54%的性能提升。", "conclusion": "MAE是一种可扩展、数据高效的方法，能够以最小的人类监督依赖显著提升LLM的通用推理能力。"}}
{"id": "2510.23038", "pdf": "https://arxiv.org/pdf/2510.23038", "abs": "https://arxiv.org/abs/2510.23038", "authors": ["Ran Xu", "Jingjing Chen", "Jiayu Ye", "Yu Wu", "Jun Yan", "Carl Yang", "Hongkun Yu"], "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress", "summary": "Large Language Models (LLMs) are widely used as judges to evaluate response\nquality, providing a scalable alternative to human evaluation. However, most\nLLM judges operate solely on intrinsic text-based reasoning, limiting their\nability to verify complex constraints or perform accurate computation.\nMotivated by the success of tool-integrated reasoning (TIR) in numerous tasks,\nwe propose TIR-Judge, an end-to-end RL framework for training LLM judges that\nintegrates a code executor for precise evaluation. TIR-Judge is built on three\nprinciples: (i) diverse training across verifiable and non-verifiable domains,\n(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)\niterative RL that bootstraps directly from the initial model without\ndistillation. On seven public benchmarks, TIR-Judge surpasses strong\nreasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and\nachieves listwise performance comparable to Claude-Opus-4 despite having only\n8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled\njudge trajectories, matches the performance of distilled variants,\ndemonstrating that tool-augmented judges can self-evolve through iterative\nreinforcement learning.", "AI": {"tldr": "TIR-Judge是一个端到端的强化学习框架，通过集成代码执行器来训练LLM评估器，在多个评估基准上显著优于基于推理的评估器，且无需蒸馏即可实现自我进化。", "motivation": "现有的LLM评估器主要基于文本推理，难以验证复杂约束或进行精确计算，而工具集成推理在其他任务中已证明有效。", "method": "提出TIR-Judge框架，基于三个原则：多样化训练（可验证和不可验证领域）、灵活评估格式（点对、配对、列表式）、迭代强化学习（无需蒸馏直接从初始模型启动）。", "result": "在7个公共基准上，TIR-Judge比基于推理的评估器提升6.4%（点对）和7.7%（配对），8B参数的模型达到与Claude-Opus-4相当的列表式性能，无需蒸馏的变体也能达到蒸馏版本的效果。", "conclusion": "工具增强的评估器可以通过迭代强化学习自我进化，为LLM评估提供了更精确和可扩展的解决方案。"}}
{"id": "2510.23601", "pdf": "https://arxiv.org/pdf/2510.23601", "abs": "https://arxiv.org/abs/2510.23601", "authors": ["Jiahao Qiu", "Xuan Qi", "Hongru Wang", "Xinzhe Juan", "Yimin Wang", "Zelin Zhao", "Jiayi Geng", "Jiacheng Guo", "Peihang Li", "Jingzhe Shi", "Shilong Liu", "Mengdi Wang"], "title": "Alita-G: Self-Evolving Generative Agent for Agent Generation", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "Large language models (LLMs) have been shown to perform better when\nscaffolded into agents with memory, tools, and feedback. Beyond this,\nself-evolving agents have emerged, but current work largely limits adaptation\nto prompt rewriting or failure retries. Therefore, we present ALITA-G, a\nself-evolution framework that transforms a general-purpose agent into a domain\nexpert by systematically generating, abstracting, and curating Model Context\nProtocol (MCP) tools. In this framework, a generalist agent executes a curated\nsuite of target-domain tasks and synthesizes candidate MCPs from successful\ntrajectories. These are then abstracted to parameterized primitives and\nconsolidated into an MCP Box. At inference time, ALITA-G performs\nretrieval-augmented MCP selection with the help of each tool's descriptions and\nuse cases, before executing an agent equipped with the MCP Executor. Across\nseveral benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains\nstrong gains while reducing computation costs. On GAIA validation, it achieves\n83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result\nwhile reducing mean tokens per example by approximately 15% relative to a\nstrong baseline agent. ALITA-G thus provides a principled pathway from\ngeneralist capability to reusable, domain-specific competence, improving both\naccuracy and efficiency on complex reasoning tasks.", "AI": {"tldr": "ALITA-G是一个自进化框架，通过系统生成、抽象和策划MCP工具，将通用智能体转化为领域专家，在多个基准测试中实现SOTA性能并降低计算成本。", "motivation": "当前自进化智能体主要局限于提示重写或失败重试，需要更系统的领域适应方法来提升LLM在复杂推理任务中的性能。", "method": "通过执行领域任务生成候选MCP工具，抽象为参数化原语并整合到MCP Box中，在推理时进行检索增强的MCP选择并使用MCP执行器执行。", "result": "在GAIA验证集上达到83.03% pass@1和89.09% pass@3的SOTA结果，同时相比基线减少约15%的平均token使用量。", "conclusion": "ALITA-G提供了从通用能力到可重用领域专业能力的原理性路径，显著提升了复杂推理任务的准确性和效率。"}}
{"id": "2510.23052", "pdf": "https://arxiv.org/pdf/2510.23052", "abs": "https://arxiv.org/abs/2510.23052", "authors": ["Zhanchao Zhou", "Xiaodong Chen", "Haoxing Chen", "Zhenzhong Lan", "Jianguo Li"], "title": "Knocking-Heads Attention", "categories": ["cs.CL"], "comment": null, "summary": "Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.", "AI": {"tldr": "提出一种新的注意力机制Knocking-Heads Attention (KHA)，通过在多头注意力计算前引入跨头特征交互，解决了传统多头注意力机制中头间缺乏强交互的问题。", "motivation": "传统多头注意力机制(MHA)及其变体(GQA、GTA)只是简单拼接各头的输出，缺乏头间的强交互，且增加头数会削弱单个头的容量。", "method": "使用共享的对角初始化投影矩阵在所有头之间建立特征级交互，使注意力头能够相互\"敲击\"，在缩放点积注意力计算前进行跨头交互。", "result": "在6.1B参数的MoE模型上训练验证，KHA带来更优且更稳定的训练动态，在下游任务中表现更好。", "conclusion": "KHA通过最小参数和计算开销实现了头间的有效交互，可无缝集成到各种注意力变体中，提升模型性能。"}}
{"id": "2510.23070", "pdf": "https://arxiv.org/pdf/2510.23070", "abs": "https://arxiv.org/abs/2510.23070", "authors": ["Hoyeon Moon", "Byeolhee Kim", "Nikhil Verma"], "title": "Quality-Aware Translation Tagging in Multilingual RAG system", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 MRL Workshop", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English\ndocuments and translates them into the query language for low-resource\nsettings. However, poor translation quality degrades response generation\nperformance. Existing approaches either assume sufficient translation quality\nor utilize the rewriting method, which introduces factual distortion and\nhallucinations. To mitigate these problems, we propose Quality-Aware\nTranslation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation\nquality along three dimensions-semantic equivalence, grammatical accuracy, and\nnaturalness&fluency-and attach these scores as metadata without altering the\noriginal content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines\nin two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs\nranging from 2.4B to 14B parameters, covering two low-resource languages\n(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG\noutperforms the baselines by preserving factual integrity while enabling\ngenerator models to make informed decisions based on translation reliability.\nThis approach allows for effective usage of cross-lingual documents in\nlow-resource settings with limited native language documents, offering a\npractical and robust solution across multilingual domains.", "AI": {"tldr": "QTT-RAG通过质量感知翻译标注提升多语言检索增强生成性能，在不改变原文内容的情况下评估翻译质量，帮助生成模型基于翻译可靠性做出明智决策", "motivation": "现有的mRAG方法在低资源语言环境中依赖英语文档翻译，但翻译质量差会降低生成性能，现有方法要么假设翻译质量足够好，要么使用会导致事实扭曲和幻觉的重写方法", "method": "提出QTT-RAG方法，明确从三个维度评估翻译质量：语义等价性、语法准确性和自然流畅性，并将这些分数作为元数据附加而不改变原始内容", "result": "在两个开放域QA基准测试中，使用6个2.4B到14B参数的指令调优LLM，涵盖韩语、芬兰语和中文，QTT-RAG优于CrossRAG和DKM-RAG基线", "conclusion": "QTT-RAG在保持事实完整性的同时，使生成模型能够基于翻译可靠性做出明智决策，为低资源环境下有限母语文档的跨语言文档使用提供了实用且鲁棒的解决方案"}}
{"id": "2510.23081", "pdf": "https://arxiv.org/pdf/2510.23081", "abs": "https://arxiv.org/abs/2510.23081", "authors": ["Chengying Tu", "Xuemiao Zhang", "Rongxiang Weng", "Rumei Li", "Chen Zhang", "Yang Bai", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "A Survey on LLM Mid-training", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in foundation models have highlighted the significant\nbenefits of multi-stage training, with a particular emphasis on the emergence\nof mid-training as a vital stage that bridges pre-training and post-training.\nMid-training is distinguished by its use of intermediate data and computational\nresources, systematically enhancing specified capabilities such as mathematics,\ncoding, reasoning, and long-context extension, while maintaining foundational\ncompetencies. This survey provides a formal definition of mid-training for\nlarge language models (LLMs) and investigates optimization frameworks that\nencompass data curation, training strategies, and model architecture\noptimization. We analyze mainstream model implementations in the context of\nobjective-driven interventions, illustrating how mid-training serves as a\ndistinct and critical stage in the progressive development of LLM capabilities.\nBy clarifying the unique contributions of mid-training, this survey offers a\ncomprehensive taxonomy and actionable insights, supporting future research and\ninnovation in the advancement of LLMs.", "AI": {"tldr": "本调查论文对大型语言模型的中期训练进行了系统研究，提出了中期训练的形式化定义，分析了数据管理、训练策略和模型架构优化的框架，并展示了中期训练在LLM能力渐进发展中的关键作用。", "motivation": "基础模型的最新进展突显了多阶段训练的重要价值，特别是中期训练作为连接预训练和后训练的关键阶段，能够系统性增强特定能力同时保持基础能力。", "method": "通过形式化定义LLM中期训练概念，研究包含数据管理、训练策略和模型架构优化的优化框架，分析主流模型在目标驱动干预下的实现方式。", "result": "阐明了中期训练的独特贡献，提供了全面的分类体系和可操作的见解，展示了中期训练如何作为LLM能力发展的一个独特且关键的阶段。", "conclusion": "中期训练是LLM能力渐进发展中不可或缺的阶段，该研究为未来LLM发展的研究和创新提供了理论基础和实践指导。"}}
{"id": "2510.23090", "pdf": "https://arxiv.org/pdf/2510.23090", "abs": "https://arxiv.org/abs/2510.23090", "authors": ["Suchan Lee", "Jihoon Choi", "Sohyeon Lee", "Minseok Song", "Bong-Gyu Jang", "Hwanjo Yu", "Soyeon Caren Han"], "title": "MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances have investigated the use of pretrained large language models\n(LLMs) for time-series forecasting by aligning numerical inputs with LLM\nembedding spaces. However, existing multimodal approaches often overlook the\ndistinct statistical properties and temporal dependencies that are fundamental\nto time-series data. To bridge this gap, we propose MAP4TS, a novel\nMulti-Aspect Prompting Framework that explicitly incorporates classical\ntime-series analysis into the prompt design. Our framework introduces four\nspecialized prompt components: a Global Domain Prompt that conveys\ndataset-level context, a Local Domain Prompt that encodes recent trends and\nseries-specific behaviors, and a pair of Statistical and Temporal Prompts that\nembed handcrafted insights derived from autocorrelation (ACF), partial\nautocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined\nwith raw time-series embeddings and passed through a cross-modality alignment\nmodule to produce unified representations, which are then processed by an LLM\nand projected for final forecasting. Extensive experiments across eight diverse\ndatasets show that MAP4TS consistently outperforms state-of-the-art LLM-based\nmethods. Our ablation studies further reveal that prompt-aware designs\nsignificantly enhance performance stability and that GPT-2 backbones, when\npaired with structured prompts, outperform larger models like LLaMA in\nlong-term forecasting tasks.", "AI": {"tldr": "MAP4TS是一个新颖的多方面提示框架，通过将经典时间序列分析融入提示设计，显著提升了基于大语言模型的时间序列预测性能。", "motivation": "现有的多模态方法往往忽视了时间序列数据特有的统计特性和时间依赖性，需要一种能够更好结合时间序列专业知识的提示框架。", "method": "提出包含四个专门提示组件的框架：全局域提示、局部域提示、统计提示和时间提示，结合自相关、偏自相关和傅里叶分析等手工设计的洞察，通过跨模态对齐模块生成统一表示。", "result": "在八个不同数据集上的广泛实验显示，MAP4TS持续优于最先进的基于LLM的方法，消融研究表明提示感知设计显著提高了性能稳定性。", "conclusion": "结构化提示与GPT-2骨干网络的组合在长期预测任务中表现优于LLaMA等更大模型，证明了将领域专业知识融入提示设计的有效性。"}}
{"id": "2510.23104", "pdf": "https://arxiv.org/pdf/2510.23104", "abs": "https://arxiv.org/abs/2510.23104", "authors": ["Yi-Li Hsu", "Katelyn X. Mei", "Lucy Lu Wang"], "title": "Leveraging Hierarchical Organization for Medical Multi-document Summarization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Medical multi-document summarization (MDS) is a complex task that requires\neffectively managing cross-document relationships. This paper investigates\nwhether incorporating hierarchical structures in the inputs of MDS can improve\na model's ability to organize and contextualize information across documents\ncompared to traditional flat summarization methods. We investigate two ways of\nincorporating hierarchical organization across three large language models\n(LLMs), and conduct comprehensive evaluations of the resulting summaries using\nautomated metrics, model-based metrics, and domain expert evaluation of\npreference, understandability, clarity, complexity, relevance, coverage,\nfactuality, and coherence. Our results show that human experts prefer\nmodel-generated summaries over human-written summaries. Hierarchical approaches\ngenerally preserve factuality, coverage, and coherence of information, while\nalso increasing human preference for summaries. Additionally, we examine\nwhether simulated judgments from GPT-4 align with human judgments, finding\nhigher agreement along more objective evaluation facets. Our findings\ndemonstrate that hierarchical structures can improve the clarity of medical\nsummaries generated by models while maintaining content coverage, providing a\npractical way to improve human preference for generated summaries.", "AI": {"tldr": "该研究探讨了在医疗多文档摘要任务中使用层次结构输入能否改善模型的信息组织能力，发现层次化方法在保持事实性、覆盖面和连贯性的同时提高了人类对生成摘要的偏好。", "motivation": "医疗多文档摘要是复杂任务，需要有效管理跨文档关系，传统平面摘要方法在信息组织和上下文化方面存在局限，因此研究层次结构是否能改善模型性能。", "method": "研究比较了两种层次组织方法在三个大语言模型中的应用，使用自动化指标、基于模型的指标以及领域专家在多个维度（偏好、可理解性、清晰度等）的评估进行综合分析。", "result": "人类专家更偏好模型生成的摘要而非人工撰写的摘要；层次化方法在保持事实性、覆盖面和连贯性的同时提高了人类偏好；GPT-4模拟判断与人类判断在更客观的评估维度上具有更高一致性。", "conclusion": "层次结构可以提高医疗摘要的清晰度同时保持内容覆盖面，为改善人类对生成摘要的偏好提供了实用方法。"}}
{"id": "2510.23114", "pdf": "https://arxiv.org/pdf/2510.23114", "abs": "https://arxiv.org/abs/2510.23114", "authors": ["Tomáš Sourada", "Jana Straková"], "title": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection", "categories": ["cs.CL"], "comment": "Published in the proceedings of TSD 2025. 12 pages, 1 figure, 4\n  tables", "summary": "We present a compact, single-model approach to multilingual inflection, the\ntask of generating inflected word forms from base lemmas to express grammatical\ncategories. Our model, trained jointly on data from 73 languages, is\nlightweight, robust to unseen words, and outperforms monolingual baselines in\nmost languages. This demonstrates the effectiveness of multilingual modeling\nfor inflection and highlights its practical benefits: simplifying deployment by\neliminating the need to manage and retrain dozens of separate monolingual\nmodels. In addition to the standard SIGMORPHON shared task benchmarks, we\nevaluate our monolingual and multilingual models on 73 Universal Dependencies\n(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.\nTo ensure realistic data splits, we introduce a novel frequency-weighted,\nlemma-disjoint train-dev-test resampling procedure. Our work addresses the lack\nof an open-source, general-purpose, multilingual morphological inflection\nsystem capable of handling unseen words across a wide range of languages,\nincluding Czech. All code is publicly released at:\nhttps://github.com/tomsouri/multilingual-inflection.", "AI": {"tldr": "提出一种紧凑的单模型多语言词形变化方法，在73种语言上联合训练，性能优于单语言基线，简化了部署需求。", "motivation": "解决缺乏开源、通用、多语言形态变化系统的问题，特别是能够处理未见词汇和多种语言（包括捷克语）的需求。", "method": "使用多语言联合训练模型，从73种Universal Dependencies树库中提取lemma-tag-form三元组及其频率计数，采用频率加权、lemma不相交的训练-开发-测试重采样程序。", "result": "模型轻量、对未见词汇鲁棒，在大多数语言上超越单语言基线，证明了多语言建模在词形变化任务中的有效性。", "conclusion": "多语言建模在词形变化任务中具有显著优势，能够简化部署并提高性能，所有代码已开源发布。"}}
{"id": "2510.23123", "pdf": "https://arxiv.org/pdf/2510.23123", "abs": "https://arxiv.org/abs/2510.23123", "authors": ["Shiwei Li", "Xiandi Luo", "Haozhao Wang", "Xing Tang", "Ziqiang Cui", "Dugang Liu", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by NeurIPS 2025", "summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). LoRA essentially describes the\nprojection of an input space into a low-dimensional output space, with the\ndimensionality determined by the LoRA rank. In standard LoRA, all input tokens\nshare the same weights and undergo an identical input-output projection. This\nlimits LoRA's ability to capture token-specific information due to the inherent\nsemantic differences among tokens. To address this limitation, we propose\nToken-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts\nLoRA weights according to the input token, thereby learning token-wise\ninput-output projections in an end-to-end manner. Formally, the weights of\nTopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank\nmatrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated\nfrom each input token $X$. Notably, TopLoRA does not increase the rank of LoRA\nweights but achieves more granular adaptation by learning token-wise LoRA\nweights (i.e., token-wise input-output projections). Extensive experiments\nacross multiple models and datasets demonstrate that TopLoRA consistently\noutperforms LoRA and its variants. The code is available at\nhttps://github.com/Leopold1423/toplora-neurips25.", "AI": {"tldr": "TopLoRA提出了一种基于token-wise投影的低秩适应方法，通过动态调整LoRA权重来捕捉token特定信息，在多个模型和数据集上表现优于标准LoRA及其变体。", "motivation": "标准LoRA中所有输入token共享相同权重，无法有效捕捉token间的语义差异，限制了其表达能力。", "method": "提出TopLoRA方法，使用动态生成的Σ_X对角矩阵为每个输入token生成特定的LoRA权重BΣ_XA，实现token-wise的输入输出投影。", "result": "在多个模型和数据集上的实验表明，TopLoRA始终优于标准LoRA及其变体，且不增加LoRA权重秩数。", "conclusion": "TopLoRA通过token-wise的权重调整有效提升了LoRA的适应能力，为大语言模型的高效微调提供了更优解决方案。"}}
{"id": "2510.23131", "pdf": "https://arxiv.org/pdf/2510.23131", "abs": "https://arxiv.org/abs/2510.23131", "authors": ["Tomáš Sourada", "Jana Straková"], "title": "Corpus Frequencies in Morphological Inflection: Do They Matter?", "categories": ["cs.CL"], "comment": "Published in the proceedings of ITAT 2025.15 pages, 1 figure, 4\n  tables", "summary": "The traditional approach to morphological inflection (the task of modifying a\nbase word (lemma) to express grammatical categories) has been, for decades, to\nconsider lexical entries of lemma-tag-form triples uniformly, lacking any\ninformation about their frequency distribution. However, in production\ndeployment, one might expect the user inputs to reflect a real-world\ndistribution of frequencies in natural texts. With future deployment in mind,\nwe explore the incorporation of corpus frequency information into the task of\nmorphological inflection along three key dimensions during system development:\n(i) for train-dev-test split, we combine a lemma-disjoint approach, which\nevaluates the model's generalization capabilities, with a frequency-weighted\nstrategy to better reflect the realistic distribution of items across different\nfrequency bands in training and test sets; (ii) for evaluation, we complement\nthe standard type accuracy (often referred to simply as accuracy), which treats\nall items equally regardless of frequency, with token accuracy, which assigns\ngreater weight to frequent words and better approximates performance on running\ntext; (iii) for training data sampling, we introduce a method novel in the\ncontext of inflection, frequency-aware training, which explicitly incorporates\nword frequency into the sampling process. We show that frequency-aware training\noutperforms uniform sampling in 26 out of 43 languages.", "AI": {"tldr": "该论文探索将语料库频率信息融入形态屈折任务中，通过频率加权划分数据集、引入词例准确率评估指标和频率感知训练方法，在43种语言中26种语言上优于均匀采样。", "motivation": "传统形态屈折方法缺乏频率分布信息，而实际应用中用户输入反映的是自然文本的真实频率分布，需要开发能更好反映现实世界频率分布的系统。", "method": "提出三个维度的方法：(1) 词干不相交的频率加权训练-开发-测试集划分；(2) 补充标准类型准确率，引入词例准确率评估；(3) 在训练数据采样中引入频率感知训练方法。", "result": "频率感知训练在43种语言中的26种语言上表现优于均匀采样，证明频率信息对提升形态屈折性能的有效性。", "conclusion": "将语料库频率信息融入形态屈折任务的多维度方法能更好地反映现实世界语言使用情况，频率感知训练显著提升系统性能，为实际部署提供更实用的解决方案。"}}
{"id": "2510.23160", "pdf": "https://arxiv.org/pdf/2510.23160", "abs": "https://arxiv.org/abs/2510.23160", "authors": ["Zile Yang", "Ling Li", "Na Di", "Jinlong Pang", "Yao Zhou", "Hao Cheng", "Bo Han", "Jiaheng Wei"], "title": "ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix", "categories": ["cs.CL"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)\nto domain-specific instructions by training on a carefully curated subset of\nhigh-quality instruction-response pairs, typically drawn from a larger dataset\nthat often contains many low-quality or noisy samples. However, existing\nquality-first paradigms often overlook valuable signals in discarded\nlow-quality data and rely on imperfect quality filters. We introduce ENTP\n(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a\nframework that revitalizes low-quality corpora through symbolic purification\nand neural reconstruction. The symbolic module identifies and prunes noisy\nsamples based on statistical priors, while the neural component synthesizes\nenriched instruction-response pairs by leveraging latent representations and\nmodel knowledge. This neural-symbolic synergy enhances data informativeness and\ndiversity. Experiments show that ENTP-augmented datasets, constructed\nexclusively from low-quality data, outperform 13 established data-selection\nbaselines across five instruction-following benchmarks, and even surpass\nfine-tuning on the full original dataset (approximately 300K examples). Our\nresults highlight the untapped potential of low-quality data and underscore the\nimportance of intelligent purification and synthesis for efficient instruction\nalignment.", "AI": {"tldr": "ENTP框架通过神经符号方法从低质量数据中提取价值，仅使用低质量数据构建的数据集在多个基准测试中超越了传统数据选择方法和完整原始数据集", "motivation": "现有监督微调方法丢弃低质量数据，但其中可能包含有价值信息，且质量过滤方法不完善", "method": "提出ENTP框架：符号模块基于统计先验识别和修剪噪声样本，神经模块利用潜在表示和模型知识合成增强的指令-响应对", "result": "ENTP增强的数据集在五个指令跟随基准测试中优于13个现有数据选择基线，甚至超越了使用完整原始数据集（约30万样本）的微调效果", "conclusion": "低质量数据具有未开发的潜力，智能净化和合成对于高效指令对齐至关重要"}}
{"id": "2510.23163", "pdf": "https://arxiv.org/pdf/2510.23163", "abs": "https://arxiv.org/abs/2510.23163", "authors": ["Hang Lei", "Shengyi Zong", "Zhaoyan Li", "Ziren Zhou", "Hao Liu"], "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs", "categories": ["cs.CL", "cs.AI", "I.2.0"], "comment": null, "summary": "The screenplay serves as the foundation for television production, defining\nnarrative structure, character development, and dialogue. While Large Language\nModels (LLMs) show great potential in creative writing, direct end-to-end\ngeneration approaches often fail to produce well-crafted screenplays. We argue\nthis failure stems from forcing a single model to simultaneously master two\ndisparate capabilities: creative narrative construction and rigid format\nadherence. The resulting outputs may mimic superficial style but lack the deep\nstructural integrity and storytelling substance required for professional use.\nTo enable LLMs to generate high-quality screenplays, we introduce Dual-Stage\nRefinement (DSR), a decomposed framework that decouples creative narrative\ngeneration from format conversion. The first stage transforms a brief outline\ninto rich, novel-style prose. The second stage refines this narrative into a\nprofessionally formatted screenplay. This separation enables the model to\nspecialize in one distinct capability at each stage. A key challenge in\nimplementing DSR is the scarcity of paired outline-to-novel training data. We\naddress this through hybrid data synthesis: reverse synthesis deconstructs\nexisting screenplays into structured inputs, while forward synthesis leverages\nthese inputs to generate high-quality narrative texts as training targets.\nBlind evaluations by professional screenwriters show that DSR achieves a 75%\nwin rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of\nhuman-level performance. Our work demonstrates that decomposed generation\narchitecture with tailored data synthesis effectively specializes LLMs in\ncomplex creative domains.", "AI": {"tldr": "提出双阶段精炼(DSR)框架，将剧本生成分解为创意叙事生成和格式转换两个独立阶段，通过混合数据合成解决训练数据稀缺问题，显著提升LLM生成剧本的质量", "motivation": "传统端到端LLM剧本生成方法同时要求模型掌握创意叙事构建和严格格式遵循两种不同能力，导致生成结果缺乏深层结构完整性和故事实质", "method": "Dual-Stage Refinement (DSR)框架：第一阶段将简要大纲转换为丰富的小说风格散文，第二阶段将叙事精炼为专业格式剧本。采用混合数据合成方法（反向合成解构现有剧本，正向合成生成高质量叙事文本）", "result": "专业编剧盲测显示DSR对Gemini-2.5-Pro等强基线达到75%胜率，达到人类水平表现的82.7%", "conclusion": "分解生成架构结合定制化数据合成能有效让LLM在复杂创意领域实现专业化，为解决多能力要求任务提供了有效范式"}}
{"id": "2510.23169", "pdf": "https://arxiv.org/pdf/2510.23169", "abs": "https://arxiv.org/abs/2510.23169", "authors": ["Marah Ghoummaid", "Vladimir Tchuiev", "Ofek Glick", "Michal Moschkovitz", "Dotan Di Castro"], "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "AI-based code generation is increasingly prevalent, with GitHub Copilot\nestimated to generate 46% of the code on GitHub. Accurately evaluating how well\ngenerated code aligns with developer intent remains a critical challenge.\nTraditional evaluation methods, such as unit tests, are often unscalable and\ncostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code\nfunctionality, and metrics like CodeBERTScore require reference code, which is\nnot always available. To address the gap in reference-free evaluation, with few\nalternatives such as ICE-Score, this paper introduces MATCH, a novel\nreference-free metric. MATCH uses Contrastive Learning to generate meaningful\nembeddings for code and natural language task descriptions, enabling similarity\nscoring that reflects how well generated code implements the task. We show that\nMATCH achieves stronger correlations with functional correctness and human\npreference than existing metrics across multiple programming languages.", "AI": {"tldr": "论文提出了MATCH，一种基于对比学习的无参考代码评估指标，用于评估AI生成代码与开发者意图的匹配程度，相比现有指标在功能正确性和人类偏好方面表现出更强的相关性。", "motivation": "AI代码生成日益普及，但传统评估方法如单元测试难以扩展且成本高，语法相似性指标无法捕捉代码功能，现有无参考评估指标选择有限，需要更好的评估解决方案。", "method": "使用对比学习技术为代码和自然语言任务描述生成有意义的嵌入表示，通过相似性评分来评估生成代码实现任务的程度。", "result": "MATCH在多种编程语言中显示出比现有指标更强的与功能正确性和人类偏好的相关性。", "conclusion": "MATCH作为一种无参考评估指标，能够有效解决AI生成代码的评估挑战，为代码生成系统的性能评估提供了更好的解决方案。"}}
{"id": "2510.23182", "pdf": "https://arxiv.org/pdf/2510.23182", "abs": "https://arxiv.org/abs/2510.23182", "authors": ["Shuai Huang", "Wenxuan Zhao", "Jun Gao"], "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations", "categories": ["cs.CL"], "comment": "17 pages, 9 figures", "summary": "As large language models (LLMs) develop anthropomorphic abilities, they are\nincreasingly being deployed as autonomous agents to interact with humans.\nHowever, evaluating their performance in realistic and complex social\ninteractions remains a significant challenge. Most previous research built\ndatasets through simulated agent-to-agent interactions, which fails to capture\nthe authentic linguistic styles and relational dynamics found in real human\nconversations. To address this gap, we introduce SI-Bench, a novel benchmark\ndesigned to evaluate aspects of social intelligence in LLMs. Grounded in broad\nsocial science theories, SI-Bench contains 2,221 authentic multi-turn dialogues\ncollected from a social networking application. We further selected a subset of\n312 dialogues for manual annotation across 8 major models. The experiments show\nthat SOTA models have surpassed the human expert in process reasoning under\ncomplex social situations, yet they still fall behind humans in reply quality.\nMoreover, introducing Chain-of-Thought (CoT) reasoning may degrade the\nperformance of LLMs in social dialogue tasks. All datasets are openly available\nat https://github.com/SI-Bench/SI-Bench.git.", "AI": {"tldr": "SI-Bench是一个基于真实社交媒体对话的社会智能评估基准，包含2221个多轮对话，用于评估大语言模型在复杂社交情境中的表现。研究发现SOTA模型在过程推理上超越人类专家，但在回复质量上仍落后于人类，且CoT推理可能降低模型在社交对话任务中的性能。", "motivation": "随着大语言模型具备类人能力并被部署为自主代理与人类互动，需要评估其在真实复杂社交互动中的表现。现有研究多通过模拟代理间互动构建数据集，无法捕捉真实人类对话的语言风格和关系动态。", "method": "基于广泛的社会科学理论，从社交网络应用收集2221个真实多轮对话构建SI-Bench基准，并选取312个对话对8个主要模型进行人工标注评估。", "result": "实验显示：1)SOTA模型在复杂社交情境的过程推理上超越人类专家；2)在回复质量上仍落后于人类；3)引入Chain-of-Thought推理可能降低LLMs在社交对话任务中的性能。", "conclusion": "SI-Bench提供了一个基于真实对话的社会智能评估框架，揭示了LLMs在社交智能方面的优势与不足，为未来研究提供了重要基准和数据集。"}}
{"id": "2510.23189", "pdf": "https://arxiv.org/pdf/2510.23189", "abs": "https://arxiv.org/abs/2510.23189", "authors": ["Ali Fata", "Hossein Rahmani", "Parinaz Soltanzadeh", "Amirhossein Derakhshan", "Behrouz Minaei Bidgoli"], "title": "DREaM: Drug-Drug Relation Extraction via Transfer Learning Method", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Relation extraction between drugs plays a crucial role in identifying drug\ndrug interactions and predicting side effects. The advancement of machine\nlearning methods in relation extraction, along with the development of large\nmedical text databases, has enabled the low cost extraction of such relations\ncompared to other approaches that typically require expert knowledge. However,\nto the best of our knowledge, there are limited datasets specifically designed\nfor drug drug relation extraction currently available. Therefore, employing\ntransfer learning becomes necessary to apply machine learning methods in this\ndomain. In this study, we propose DREAM, a method that first employs a trained\nrelation extraction model to discover relations between entities and then\napplies this model to a corpus of medical texts to construct an ontology of\ndrug relationships. The extracted relations are subsequently validated using a\nlarge language model. Quantitative results indicate that the LLM agreed with 71\nof the relations extracted from a subset of PubMed abstracts. Furthermore, our\nqualitative analysis indicates that this approach can uncover ambiguities in\nthe medical domain, highlighting the challenges inherent in relation extraction\nin this field.", "AI": {"tldr": "提出DREAM方法，使用预训练关系抽取模型从医学文本中构建药物关系本体，并用大语言模型验证结果，在PubMed摘要子集上达到71%的验证一致性。", "motivation": "药物关系抽取对识别药物相互作用和预测副作用至关重要，但缺乏专门的数据集，需要采用迁移学习方法。", "method": "先使用训练好的关系抽取模型发现实体关系，然后应用于医学文本语料库构建药物关系本体，最后用大语言模型验证抽取的关系。", "result": "定量结果显示LLM对PubMed摘要子集中71%的抽取关系表示同意；定性分析显示该方法能揭示医学领域的模糊性。", "conclusion": "该方法能有效构建药物关系本体，但揭示了医学领域关系抽取的内在挑战和模糊性问题。"}}
{"id": "2510.23217", "pdf": "https://arxiv.org/pdf/2510.23217", "abs": "https://arxiv.org/abs/2510.23217", "authors": ["Alois Thomas", "Maya Varma", "Jean-Benoit Delbrouck", "Curtis P. Langlotz"], "title": "Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automating radiology report generation with Large Vision-Language Models\n(LVLMs) holds great potential, yet these models often produce clinically\ncritical hallucinations, posing serious risks. Existing hallucination detection\nmethods frequently lack the necessary sentence-level granularity or robust\ngeneralization across different LVLM generators. We introduce a novel approach:\na sentence-level Process Reward Model (PRM) adapted for this vision-language\ntask. Our PRM predicts the factual correctness of each generated sentence,\nconditioned on clinical context and preceding text. When fine-tuned on\nMIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM\noutperforms existing verification techniques, demonstrating, for instance,\nrelative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in\nAUROC over strong white-box baselines on outputs from one LVLM. Unlike methods\nreliant on internal model states, our PRM demonstrates strong generalization to\nan unseen LVLM. We further show its practical utility: PRM scores effectively\nfilter low-quality reports, improving F1-CheXbert scores by 4.5% (when\ndiscarding the worst 10% of reports). Moreover, when guiding a novel weighted\nbest-of-N selection process on the MIMIC-CXR test set, our PRM show relative\nimprovements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for\nBERTScore. These results demonstrate that a lightweight, context-aware PRM\nprovides a model-agnostic safety layer for clinical LVLMs without access to\ninternal activations", "AI": {"tldr": "提出一种句子级别的过程奖励模型(PRM)，用于检测大型视觉语言模型在放射学报告生成中的幻觉问题，该模型在临床指标上显著优于现有方法，并能有效泛化到未见过的模型。", "motivation": "大型视觉语言模型在放射学报告生成中存在严重的临床幻觉风险，现有检测方法缺乏句子级粒度且泛化能力不足。", "method": "开发轻量级句子级过程奖励模型(PRM)，使用弱监督标签在MIMIC-CXR数据集上微调，预测每个生成句子的事实正确性，基于临床上下文和先前文本。", "result": "PRM在Matthews相关系数上相对提升7.5%，AUROC提升1.8%；能有效过滤低质量报告，F1-CheXbert分数提升4.5%；在加权最佳选择过程中，F1-CheXbert提升7.4%，BERTScore提升0.6%。", "conclusion": "轻量级、上下文感知的PRM为临床LVLM提供了模型无关的安全层，无需访问内部激活，有效解决了幻觉检测问题。"}}
{"id": "2510.23252", "pdf": "https://arxiv.org/pdf/2510.23252", "abs": "https://arxiv.org/abs/2510.23252", "authors": ["Tawsif Tashwar Dipto", "Azmol Hossain", "Rubayet Sabbir Faruque", "Md. Rezuwan Hassan", "Kanij Fatema", "Tanmoy Shome", "Ruwad Naswan", "Md. Foriduzzaman Zihad", "Mohaymen Ul Anam", "Nazia Tasnim", "Hasan Mahmud", "Md Kamrul Hasan", "Md. Mehedi Hasan Shawon", "Farig Sadeque", "Tahsin Reasat"], "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?", "categories": ["cs.CL"], "comment": "This manuscript contains 11 pages, 5 tables and 16 figures This was\n  accepted at International Joint Conference on Natural Language Processing &\n  Asia-Pacific Chapter of the Association for Computational Linguistics\n  (IJCNLP-AACL) 2025", "summary": "Conventional research on speech recognition modeling relies on the canonical\nform for most low-resource languages while automatic speech recognition (ASR)\nfor regional dialects is treated as a fine-tuning task. To investigate the\neffects of dialectal variations on ASR we develop a 78-hour annotated Bengali\nSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and\ndata-driven perspectives shows that speech foundation models struggle heavily\nin regional dialect ASR, both in zero-shot and fine-tuned settings. We observe\nthat all deep learning methods struggle to model speech data under dialectal\nvariations but dialect specific model training alleviates the issue. Our\ndataset also serves as a out of-distribution (OOD) resource for ASR modeling\nunder constrained resources in ASR algorithms. The dataset and code developed\nfor this project are publicly available", "AI": {"tldr": "该研究创建了一个78小时的孟加拉语方言语音数据集Ben-10，发现语音基础模型在方言ASR任务中表现不佳，无论是零样本还是微调设置。方言特定的模型训练能缓解这一问题。", "motivation": "传统语音识别研究主要关注标准语言形式，而方言ASR通常被视为微调任务。研究旨在探索方言变体对语音识别的影响。", "method": "开发了78小时的带标注孟加拉语方言语音转文本语料库Ben-10，从语言学和数据驱动角度分析语音基础模型在方言ASR中的表现。", "result": "语音基础模型在方言ASR中表现严重不足，所有深度学习方法都难以有效建模方言变异的语音数据，但方言特定训练能改善性能。", "conclusion": "方言ASR需要专门的处理方法，Ben-10数据集可作为资源受限条件下ASR建模的分布外资源，数据集和代码已公开。"}}
{"id": "2510.23271", "pdf": "https://arxiv.org/pdf/2510.23271", "abs": "https://arxiv.org/abs/2510.23271", "authors": ["Mohammed Aljafari", "Ismail Alturki", "Ahmed Mori", "Yehya Kadumi"], "title": "Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding", "categories": ["cs.CL", "68T50 (68T50 Natural language processing)", "I.2.7; I.2.6; I.2.0; H.3.3"], "comment": "21 pages, 2 figures, 3 tables. Includes appendices on ethical\n  guidelines and training framework. Submitted September 04, 2025", "summary": "Mubeen is a proprietary Arabic language model developed by MASARAT SA,\noptimized for deep understanding of Arabic linguistics, Islamic studies, and\ncultural heritage. Trained on an extensive collection of authentic Arabic\nsources significantly expanded by digitizing historical manuscripts via a\nproprietary Arabic OCR engine, the model incorporates seminal scholarly works\nin linguistics, jurisprudence, hadith, and Quranic exegesis, alongside\nthousands of academic theses and peer-reviewed research papers. Conditioned\nthrough a deep linguistic engineering framework, Mubeen masters not just the\nmeaning but the eloquence of Arabic, enabling precise understanding across\nclassical texts, contemporary writing, and regional dialects with focus on\ncomprehending user intent and delivering accurate, contextually relevant\nresponses. Unlike other Arabic models relying on translated English data that\noften fail in intent detection or retrieval-augmented generation (RAG), Mubeen\nuses native Arabic sources to ensure cultural authenticity and accuracy. Its\ncore innovation is the Practical Closure Architecture, designed to solve the\n\"Utility Gap Crisis\" where factually correct answers fail to resolve users'\ncore needs, forcing them into frustrating cycles of re-prompting. By\nprioritizing clarity and decisive guidance, Mubeen transforms from an\ninformation repository into a decisive guide, aligning with Saudi Vision 2030.\nThe model's architecture combines deep heritage specialization with\nmulti-disciplinary expert modules, enabling robust performance across both\ncultural preservation and general knowledge domains.", "AI": {"tldr": "Mubeen是一个专有的阿拉伯语语言模型，通过深度语言工程框架训练，专注于阿拉伯语言学、伊斯兰研究和文化遗产的深度理解，采用原生阿拉伯语源数据确保文化真实性和准确性，通过Practical Closure架构解决\"效用差距危机\"。", "motivation": "解决现有阿拉伯语模型依赖英语翻译数据导致意图检测和RAG失败的问题，确保文化真实性和准确性，同时解决事实正确答案无法满足用户核心需求的\"效用差距危机\"。", "method": "使用专有阿拉伯OCR引擎数字化历史手稿扩展真实阿拉伯语源数据集，结合深度语言工程框架训练，整合语言学、法学、圣训和古兰经注释等学术著作，采用Practical Closure架构优先清晰度和决定性指导。", "result": "开发出能够精确理解古典文本、当代写作和地区方言的阿拉伯语模型，在文化保存和一般知识领域均表现出强大性能，从信息存储库转变为决定性指南。", "conclusion": "Mubeen模型成功解决了阿拉伯语AI领域的文化真实性和效用差距问题，与沙特2030愿景保持一致，为阿拉伯语言和文化传承提供了专业化的AI解决方案。"}}
{"id": "2510.23272", "pdf": "https://arxiv.org/pdf/2510.23272", "abs": "https://arxiv.org/abs/2510.23272", "authors": ["Bang Xiao", "Lingjie Jiang", "Shaohan Huang", "Tengchao Lv", "Yupan Huang", "Xun Wu", "Lei Cui", "Furu Wei"], "title": "Code Aesthetics with Agentic Reward Feedback", "categories": ["cs.CL"], "comment": "30 pages, 7 figures", "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.", "AI": {"tldr": "提出新方法GRPO-AR提升LLM生成代码的美学质量，通过大规模美学指令数据集AesCode-358K和多智能体奖励反馈机制，在OpenDesign基准测试中显著超越GPT-4o等模型", "motivation": "LLM在代码生成任务中表现出色，但在视觉导向的编程任务中美学质量较差，需要专门优化代码美观度", "method": "构建AesCode-358K指令调优数据集；设计多智能体奖励反馈系统评估可执行性、静态美学和交互美学；开发GRPO-AR算法联合优化功能性和美学", "result": "实验显示该方法在OpenDesign基准上显著提升性能，AesCoder-4B模型超越GPT-4o和GPT-4.1，达到480B-685B参数开源模型的性能水平", "conclusion": "结合监督微调和强化学习的多智能体奖励反馈方法能有效提升LLM生成代码的美学质量，为视觉编程任务提供了新的优化路径"}}
{"id": "2510.23276", "pdf": "https://arxiv.org/pdf/2510.23276", "abs": "https://arxiv.org/abs/2510.23276", "authors": ["Thai-Binh Nguyen", "Katerina Zmolikova", "Pingchuan Ma", "Ngoc Quan Pham", "Christian Fuegen", "Alexander Waibel"], "title": "A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results", "categories": ["cs.CL"], "comment": "Submitted to ICASSP 2026", "summary": "We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in\nthe ninth CHiME Challenge, which addresses the cocktail-party problem of\noverlapping conversations in a single-room setting using audio, visual, and\ncontextual cues. MCoRec captures natural multi-party conversations where the\nrecordings focus on unscripted, casual group chats, leading to extreme speech\noverlap of up to 100% and highly fragmented conversational turns. The task\nrequires systems to answer the question \"Who speaks when, what, and with whom?\"\nby jointly transcribing each speaker's speech and clustering them into their\nrespective conversations from audio-visual recordings. Audio-only baselines\nexceed 100% word error rate, whereas incorporating visual cues yields\nsubstantial 50% improvements, highlighting the importance of multi-modality. In\nthis manuscript, we present the motivation behind the task, outline the data\ncollection process, and report the baseline systems developed for the MCoRec.", "AI": {"tldr": "第九届CHiME挑战赛引入多模态上下文感知识别任务(MCoRec)，解决单房间环境下重叠对话的鸡尾酒会问题，通过音频、视觉和上下文线索进行说话人分离和转录。", "motivation": "解决自然多人群聊场景中的极端语音重叠问题（可达100%重叠率），需要同时回答\"谁在何时说了什么、与谁对话\"这一复杂问题。", "method": "使用音频和视觉多模态方法，收集非脚本化的自然群聊数据，开发基线系统进行说话人语音转录和对话聚类。", "result": "纯音频基线系统的词错误率超过100%，而加入视觉线索后性能提升50%，证明了多模态方法的重要性。", "conclusion": "多模态方法在处理极端语音重叠场景中至关重要，视觉信息的引入显著提升了说话人分离和语音识别的性能。"}}
{"id": "2510.23284", "pdf": "https://arxiv.org/pdf/2510.23284", "abs": "https://arxiv.org/abs/2510.23284", "authors": ["Yuanzhen Xie", "Liu Ye", "Jiqun Chu", "Mochi Gao", "Hehuan Liu", "Yunzhi Tan", "Bo Hu", "Zang Li"], "title": "DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model", "categories": ["cs.CL"], "comment": null, "summary": "Text-to-SQL tasks have gained attractive improvements since the release of\nChatGPT. Among them, agent-based frameworks have been widely used in this\nfield. However, the impact of data-centric strategies on text-to-SQL tasks has\nrarely been explored. In this paper, we systemically design a fully automated\ndata-centric pipeline for text-to-SQL tasks, including \\emph{adaptive data\nrepair}, which can automatically find and fix errors in the training dataset;\nand \\emph{error data augmentation}, where we specifically diffuse and enhance\nerroneous data predicted by the initially trained models. Meanwhile, we propose\na Multi-Model collaboration training schema, aiming to train multiple models\nwith different augmented data, enabling them to possess distinct capabilities\nand work together to complement each other, because it has been found that the\ncapability of a single fine-tuned model is very limited. Furthermore, we\nutilize an ensemble strategy to integrate the capabilities of multiple models\nto solve a multiple-choice question, aiming to further improve the accuracy of\ntext-to-SQL tasks. The experiment results and ablation study have demonstrated\nthe effectiveness of data-centric pipeline and Multi-Model(MM) interactive\niterative strategies, achieving first place in lightweight text-to-SQL models\n(within 70B).", "AI": {"tldr": "本文提出了一个全自动的数据中心化pipeline用于Text-to-SQL任务，包括自适应数据修复和错误数据增强，并采用多模型协作训练和集成策略，在轻量级模型中取得了最佳性能。", "motivation": "虽然基于代理的框架在Text-to-SQL任务中取得了显著改进，但数据中心化策略的影响尚未得到充分探索。现有单一微调模型的能力有限，需要更有效的数据处理和多模型协作方法。", "method": "设计了全自动数据中心化pipeline：1)自适应数据修复自动发现和修复训练数据错误；2)错误数据增强扩散和增强模型预测的错误数据；3)多模型协作训练，用不同增强数据训练多个模型；4)集成策略整合多个模型能力解决多选问题。", "result": "实验和消融研究证明了数据中心化pipeline和多模型交互迭代策略的有效性，在轻量级Text-to-SQL模型（70B参数内）中取得了第一名。", "conclusion": "数据中心化方法和多模型协作策略显著提升了Text-to-SQL任务的性能，证明了数据处理质量对模型性能的重要影响，为轻量级模型提供了有效的解决方案。"}}
{"id": "2510.23319", "pdf": "https://arxiv.org/pdf/2510.23319", "abs": "https://arxiv.org/abs/2510.23319", "authors": ["Mouhand Alkadri", "Dania Desouki", "Khloud Al Jallad"], "title": "Arabic Little STT: Arabic Children Speech Recognition Dataset", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.SD"], "comment": null, "summary": "The performance of Artificial Intelligence (AI) systems fundamentally depends\non high-quality training data. However, low-resource languages like Arabic\nsuffer from severe data scarcity. Moreover, the absence of child-specific\nspeech corpora is an essential gap that poses significant challenges. To\naddress this gap, we present our created dataset, Arabic Little STT, a dataset\nof Levantine Arabic child speech recorded in classrooms, containing 355\nutterances from 288 children (ages 6 - 13). We further conduct a systematic\nassessment of Whisper, a state-of-the-art automatic speech recognition (ASR)\nmodel, on this dataset and compare its performance with adult Arabic\nbenchmarks. Our evaluation across eight Whisper variants reveals that even the\nbest-performing model (Large_v3) struggles significantly, achieving a 0.66 word\nerror rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on\nadult datasets. These results align with other research on English speech.\nResults highlight the critical need for dedicated child speech benchmarks and\ninclusive training data in ASR development. Emphasizing that such data must be\ngoverned by strict ethical and privacy frameworks to protect sensitive child\ninformation. We hope that this study provides an initial step for future work\non equitable speech technologies for Arabic-speaking children. We hope that our\npublicly available dataset enrich the children's demographic representation in\nASR datasets.", "AI": {"tldr": "该论文介绍了Arabic Little STT数据集，包含288名阿拉伯儿童的355条语音，并评估了Whisper模型在儿童语音识别上的表现，发现即使最佳模型在儿童语音上的词错误率也高达0.66，远高于成人语音的0.20以下，强调了儿童语音数据和伦理框架的重要性。", "motivation": "解决阿拉伯语等低资源语言中儿童特定语音语料库缺失的问题，为阿拉伯语儿童提供更公平的语音技术。", "method": "创建Arabic Little STT数据集（包含288名6-13岁儿童的355条语音），并系统评估8个Whisper变体在该数据集上的表现，与成人阿拉伯语基准进行比较。", "result": "最佳模型（Large_v3）在儿童语音上的词错误率高达0.66，而成人数据集上的词错误率低于0.20，性能差异显著。", "conclusion": "研究强调了在ASR开发中需要专门的儿童语音基准和包容性训练数据，并需建立严格的伦理和隐私框架保护儿童敏感信息，为阿拉伯语儿童公平语音技术研究迈出第一步。"}}
{"id": "2510.23334", "pdf": "https://arxiv.org/pdf/2510.23334", "abs": "https://arxiv.org/abs/2510.23334", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb", "Nishant Sharma", "Ananth Shreekumar", "Jonathan Rosenthal", "Muslum Ozgur Ozmen", "Mikhail Kuznetsov", "Z. Berkay Celik"], "title": "Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "LLM alignment remains a critical challenge. Inference-time methods provide a\nflexible alternative to fine-tuning, but their uniform computational effort\noften yields suboptimal alignment. We hypothesize that for many alignment\ntasks, the initial tokens of a response are disproportionately more critical.\nTo leverage this principle, we introduce AdaSearch, a novel blockwise search\nstrategy. It adaptively allocates a fixed computational budget using a sampling\nschedule, focusing search effort on these critical tokens. We apply AdaSearch\nto sequential decoding and introduce its tree-search counterpart, AdaBeam. Our\ncomprehensive evaluation across eight LLMs demonstrates that AdaSearch\noutperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates\nimprove by over 10% for harmlessness generation, controlled sentiment\ngeneration, and for mathematical reasoning tasks relative to Best-of-N.", "AI": {"tldr": "AdaSearch是一种新颖的块状搜索策略，通过自适应分配计算预算来优化LLM对齐任务，相比传统方法在无害性生成、情感控制和数学推理任务上提升超过10%的胜率。", "motivation": "当前LLM对齐任务中，推理时方法虽然灵活但计算效率低，研究发现响应初始标记对对齐任务更为关键，需要更有效的计算分配策略。", "method": "提出AdaSearch块状搜索策略，采用自适应采样计划分配固定计算预算，重点关注关键初始标记，并开发了其树搜索版本AdaBeam。", "result": "在8个LLM上的综合评估显示，AdaSearch在无害性生成、控制情感生成和数学推理任务上相比Best-of-N基线方法胜率提升超过10%。", "conclusion": "AdaSearch通过自适应计算预算分配有效提升了LLM对齐任务的性能，证明了关注响应初始关键标记的重要性，为推理时对齐方法提供了更高效的解决方案。"}}
{"id": "2510.23337", "pdf": "https://arxiv.org/pdf/2510.23337", "abs": "https://arxiv.org/abs/2510.23337", "authors": ["Siyuan Zheng", "Pai Liu", "Xi Chen", "Jizheng Dong", "Sihan Jia"], "title": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Human-like virtual characters are crucial for games, storytelling, and\nvirtual reality, yet current methods rely heavily on annotated data or\nhandcrafted persona prompts, making it difficult to scale up and generate\nrealistic, contextually coherent personas. We create the first QA dataset for\nBaZi-based persona reasoning, where real human experiences categorized into\nwealth, health, kinship, career, and relationships are represented as\nlife-event questions and answers. Furthermore, we propose the first BaZi-LLM\nsystem that integrates symbolic reasoning with large language models to\ngenerate temporally dynamic and fine-grained virtual personas. Compared with\nmainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a\n30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information\nis used, our model's accuracy drops by 20%-45%, showing the potential of\nculturally grounded symbolic-LLM integration for realistic character\nsimulation.", "AI": {"tldr": "该论文提出了首个基于八字命理的人格推理QA数据集和BaZi-LLM系统，通过符号推理与大语言模型结合，实现了时间动态和细粒度的虚拟人格生成，相比主流LLM在准确性上有显著提升。", "motivation": "当前虚拟角色生成方法依赖标注数据或手工制作的人格提示，难以扩展且难以生成真实、上下文一致的人格，需要新的方法来创建更真实的虚拟角色。", "method": "创建了首个基于八字的人格推理QA数据集，将人类经验分类为财富、健康、亲情、事业和关系等维度；提出了BaZi-LLM系统，将符号推理与大语言模型集成。", "result": "相比DeepSeek-v3和GPT-5-mini等主流LLM，该方法实现了30.3%-62.6%的准确率提升；当使用错误八字信息时，模型准确率下降20%-45%。", "conclusion": "基于文化根基的符号推理与LLM集成在真实角色模拟方面具有巨大潜力，为虚拟角色的生成提供了新的有效方法。"}}
{"id": "2510.23341", "pdf": "https://arxiv.org/pdf/2510.23341", "abs": "https://arxiv.org/abs/2510.23341", "authors": ["Teng Lin"], "title": "LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data", "categories": ["cs.CL"], "comment": null, "summary": "The scarcity of high-quality knowledge graphs (KGs) remains a critical\nbottleneck for downstream AI applications, as existing extraction methods rely\nheavily on error-prone pattern-matching techniques or resource-intensive large\nlanguage models (LLMs). While recent tools leverage LLMs to generate KGs, their\ncomputational demands limit accessibility for low-resource environments. Our\npaper introduces LightKGG, a novel framework that enables efficient KG\nextraction from textual data using small-scale language models (SLMs) through\ntwo key technical innovations: (1) Context-integrated Graph extraction\nintegrates contextual information with nodes and edges into a unified graph\nstructure, reducing the reliance on complex semantic processing while\nmaintaining more key information; (2) Topology-enhanced relationship inference\nleverages the inherent topology of the extracted graph to efficiently infer\nrelationships, enabling relationship discovery without relying on complex\nlanguage understanding capabilities of LLMs. By enabling accurate KG\nconstruction with minimal hardware requirements, this work bridges the gap\nbetween automated knowledge extraction and practical deployment scenarios while\nintroducing scientifically rigorous methods for optimizing SLM efficiency in\nstructured NLP tasks.", "AI": {"tldr": "LightKGG是一个使用小型语言模型(SLMs)从文本数据中高效提取知识图谱的新框架，通过上下文集成图提取和拓扑增强关系推断两项技术创新，解决了传统方法依赖错误模式匹配或资源密集型大语言模型的问题。", "motivation": "高质量知识图谱的稀缺性限制了AI应用发展，现有提取方法要么依赖错误易发的模式匹配技术，要么需要资源密集型的大语言模型，计算需求大且难以在低资源环境中部署。", "method": "1) 上下文集成图提取：将上下文信息与节点和边整合到统一图结构中，减少对复杂语义处理的依赖；2) 拓扑增强关系推断：利用提取图的固有拓扑结构高效推断关系，无需依赖LLMs的复杂语言理解能力。", "result": "LightKGG框架能够以最小的硬件需求准确构建知识图谱，在结构化NLP任务中优化了小型语言模型的效率。", "conclusion": "该工作填补了自动化知识提取与实际部署场景之间的差距，为在低资源环境下使用小型语言模型进行高效知识图谱提取提供了科学严谨的方法。"}}
{"id": "2510.23358", "pdf": "https://arxiv.org/pdf/2510.23358", "abs": "https://arxiv.org/abs/2510.23358", "authors": ["Sheri Osborn", "Rohit Valecha", "H. Raghav Rao", "Dan Sass", "Anthony Rios"], "title": "How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes", "categories": ["cs.CL"], "comment": "8 pages + Limitations + References", "summary": "Artificial intelligence is reshaping labor markets, yet we lack tools to\nsystematically forecast its effects on employment. This paper introduces a\nbenchmark for evaluating how well large language models (LLMs) can anticipate\nchanges in job demand, especially in occupations affected by AI. Existing\nresearch has shown that LLMs can extract sentiment, summarize economic reports,\nand emulate forecaster behavior, but little work has assessed their use for\nforward-looking labor prediction. Our benchmark combines two complementary\ndatasets: a high-frequency index of sector-level job postings in the United\nStates, and a global dataset of projected occupational changes due to AI\nadoption. We format these data into forecasting tasks with clear temporal\nsplits, minimizing the risk of information leakage. We then evaluate LLMs using\nmultiple prompting strategies, comparing task-scaffolded, persona-driven, and\nhybrid approaches across model families. We assess both quantitative accuracy\nand qualitative consistency over time. Results show that structured task\nprompts consistently improve forecast stability, while persona prompts offer\nadvantages on short-term trends. However, performance varies significantly\nacross sectors and horizons, highlighting the need for domain-aware prompting\nand rigorous evaluation protocols. By releasing our benchmark, we aim to\nsupport future research on labor forecasting, prompt design, and LLM-based\neconomic reasoning. This work contributes to a growing body of research on how\nLLMs interact with real-world economic data, and provides a reproducible\ntestbed for studying the limits and opportunities of AI as a forecasting tool\nin the context of labor markets.", "AI": {"tldr": "本文提出了一个评估大语言模型预测AI对就业需求影响的基准测试，结合美国行业级职位发布数据和全球AI对职业影响的预测数据，测试不同提示策略在劳动力市场预测中的效果。", "motivation": "人工智能正在重塑劳动力市场，但缺乏系统预测AI对就业影响的工具。现有研究显示LLMs可以提取情感、总结经济报告和模拟预测行为，但尚未评估其在前瞻性劳动力预测中的应用。", "method": "结合两个互补数据集：美国行业级高频职位发布指数和全球AI采用导致的职业变化预测数据，构建具有明确时间分割的预测任务。评估多种提示策略（任务支架、人物驱动和混合方法）在不同模型家族中的表现。", "result": "结构化任务提示持续提高预测稳定性，人物提示在短期趋势上具有优势。但不同行业和时间跨度的表现差异显著，表明需要领域感知的提示和严格的评估协议。", "conclusion": "通过发布基准测试，支持未来关于劳动力预测、提示设计和基于LLM的经济推理研究，为研究AI作为劳动力市场预测工具的局限性和机会提供可复现的测试平台。"}}
{"id": "2510.23395", "pdf": "https://arxiv.org/pdf/2510.23395", "abs": "https://arxiv.org/abs/2510.23395", "authors": ["Evy Beijen", "Pien Pieterse", "Yusuf Çelik", "Willem Th. van Peursen", "Sandjai Bhulai", "Meike Morren"], "title": "Detecting Religious Language in Climate Discourse", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Religious language continues to permeate contemporary discourse, even in\nostensibly secular domains such as environmental activism and climate change\ndebates. This paper investigates how explicit and implicit forms of religious\nlanguage appear in climate-related texts produced by secular and religious\nnongovernmental organizations (NGOs). We introduce a dual methodological\napproach: a rule-based model using a hierarchical tree of religious terms\nderived from ecotheology literature, and large language models (LLMs) operating\nin a zero-shot setting. Using a dataset of more than 880,000 sentences, we\ncompare how these methods detect religious language and analyze points of\nagreement and divergence. The results show that the rule-based method\nconsistently labels more sentences as religious than LLMs. These findings\nhighlight not only the methodological challenges of computationally detecting\nreligious language but also the broader tension over whether religious language\nshould be defined by vocabulary alone or by contextual meaning. This study\ncontributes to digital methods in religious studies by demonstrating both the\npotential and the limitations of approaches for analyzing how the sacred\npersists in climate discourse.", "AI": {"tldr": "本研究通过规则模型和大型语言模型分析宗教语言在气候相关文本中的使用，发现基于规则的方法比LLMs检测到更多宗教语言，揭示了宗教语言检测的方法学挑战和定义争议。", "motivation": "宗教语言在当代话语中普遍存在，包括看似世俗的环境活动领域。研究旨在探索宗教和非宗教NGO在气候文本中使用的显性和隐性宗教语言。", "method": "采用双重方法：基于生态神学文献构建宗教术语层次树的规则模型，以及在零样本设置下运行的大型语言模型(LLMs)。分析超过88万句的数据集。", "result": "规则方法比LLMs持续标记更多句子为宗教语言。两种方法在检测结果上存在一致性和分歧点。", "conclusion": "研究不仅揭示了计算检测宗教语言的方法学挑战，还展现了宗教语言应仅由词汇定义还是需考虑语境意义的广泛争议。为宗教研究的数字方法展示了分析神圣性在气候话语中持续存在的潜力和局限性。"}}
{"id": "2510.23396", "pdf": "https://arxiv.org/pdf/2510.23396", "abs": "https://arxiv.org/abs/2510.23396", "authors": ["Musleh Alharthi", "Kaleel Mahmood", "Sarosh Patel", "Ausif Mahmood"], "title": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The immense success of the Transformer architecture\n  in Natural Language Processing has led to its adoption in Time Se ries\nForecasting (TSF), where superior performance has been shown.\n  However, a recent important paper questioned their effectiveness by\n  demonstrating that a simple single layer linear model outperforms\n  Transformer-based models. This was soon shown to be not as valid,\n  by a better transformer-based model termed PatchTST. More re cently, TimeLLM\ndemonstrated even better results by repurposing a\n  Large Language Model (LLM) for the TSF domain. Again, a follow\n  up paper challenged this by demonstrating that removing the LLM\n  component or replacing it with a basic attention layer in fact yields\n  better performance. One of the challenges in forecasting is the fact\n  that TSF data favors the more recent past, and is sometimes subject\n  to unpredictable events. Based upon these recent insights in TSF, we\n  propose a strong Mixture of Experts (MoE) framework. Our method\n  combines the state-of-the-art (SOTA) models including xLSTM, en hanced\nLinear, PatchTST, and minGRU, among others. This set of\n  complimentary and diverse models for TSF are integrated in a Trans former\nbased MoE gating network. Our proposed model outperforms\n  all existing TSF models on standard benchmarks, surpassing even the\n  latest approaches based on MoE frameworks.", "AI": {"tldr": "本文提出了一个基于混合专家(MoE)框架的时间序列预测模型，结合了xLSTM、增强线性模型、PatchTST和minGRU等多种SOTA模型，通过Transformer门控网络集成，在标准基准测试中超越了所有现有TSF模型。", "motivation": "针对时间序列预测领域中Transformer模型有效性争议和近期研究表明简单模型有时优于复杂模型的现状，以及TSF数据具有近期偏好和不可预测事件的特点，需要开发更强大的预测框架。", "method": "提出混合专家(MoE)框架，集成xLSTM、增强线性模型、PatchTST、minGRU等多种互补的SOTA模型，使用基于Transformer的门控网络进行模型选择和集成。", "result": "在标准基准测试中，提出的MoE框架超越了所有现有的时间序列预测模型，包括最新的基于MoE框架的方法，取得了最优性能。", "conclusion": "通过集成多种互补的SOTA模型到MoE框架中，能够有效应对时间序列预测的挑战，证明了混合专家方法在TSF领域的优越性和有效性。"}}
{"id": "2510.23451", "pdf": "https://arxiv.org/pdf/2510.23451", "abs": "https://arxiv.org/abs/2510.23451", "authors": ["Zhuoran Jin", "Hongbang Yuan", "Kejian Zhu", "Jiachun Li", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "48 pages, 17 figures", "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.", "AI": {"tldr": "Omni-Reward是一个支持多模态和自由形式偏好的通用奖励模型系统，解决了现有奖励模型的模态不平衡和偏好刚性两大挑战。", "motivation": "现有奖励模型主要局限于文本和图像模态，且基于固定二元偏好对训练，无法捕捉多模态内容和个性化偏好的复杂性。", "method": "提出Omni-Reward系统，包括：1)Omni-RewardBench多模态基准测试；2)Omni-RewardData包含31.7万偏好对的多模态数据集；3)支持判别式和生成式奖励模型的Omni-RewardModel。", "result": "模型在Omni-RewardBench和其他广泛使用的奖励建模基准测试中表现出强劲性能。", "conclusion": "Omni-Reward为通用多模态奖励建模迈出了重要一步，能够更好地支持视频、音频、3D等多种模态的自由形式偏好对齐。"}}
{"id": "2510.23458", "pdf": "https://arxiv.org/pdf/2510.23458", "abs": "https://arxiv.org/abs/2510.23458", "authors": ["Litu Ou", "Kuan Li", "Huifeng Yin", "Liwen Zhang", "Zhongwang Zhang", "Xixi Wu", "Rui Ye", "Zile Qiao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages", "summary": "Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods.", "AI": {"tldr": "该论文研究LLM搜索代理在多轮交互中表达置信度的能力，发现高置信度时任务准确率显著提升，提出基于置信度的Test-Time Scaling方法，在减少token消耗的同时保持竞争力。", "motivation": "现有研究主要关注单轮场景的置信度，而对复杂多轮交互中LLM表达置信度的能力研究有限，需要探索LLM搜索代理在长序列动作后能否通过语言化置信分数传达不确定性。", "method": "在开源代理模型上进行实验，提出Test-Time Scaling (TTS)方法，利用置信分数判断答案质量，鼓励模型在置信度不足时重新尝试，直至达到满意的置信水平。", "result": "实验发现模型在高置信度时任务准确率很高，低置信度时准确率接近零；提出的TTS方法显著减少了token消耗，相比基线固定预算TTS方法表现出竞争力。", "conclusion": "LLM搜索代理能够通过语言化置信分数有效传达不确定性，基于置信度的TTS方法在保持性能的同时优化了计算资源使用，为多轮交互中的置信度评估提供了有效解决方案。"}}
{"id": "2510.23464", "pdf": "https://arxiv.org/pdf/2510.23464", "abs": "https://arxiv.org/abs/2510.23464", "authors": ["Nikesh Gyawali", "Doina Caragea", "Alex Vasenkov", "Cornelia Caragea"], "title": "Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Financial narratives from U.S. Securities and Exchange Commission (SEC)\nfiling reports and quarterly earnings call transcripts (ECTs) are very\nimportant for investors, auditors, and regulators. However, their length,\nfinancial jargon, and nuanced language make fine-grained analysis difficult.\nPrior sentiment analysis in the financial domain required a large, expensive\nlabeled dataset, making the sentence-level stance towards specific financial\ntargets challenging. In this work, we introduce a sentence-level corpus for\nstance detection focused on three core financial metrics: debt, earnings per\nshare (EPS), and sales. The sentences were extracted from Form 10-K annual\nreports and ECTs, and labeled for stance (positive, negative, neutral) using\nthe advanced ChatGPT-o3-pro model under rigorous human validation. Using this\ncorpus, we conduct a systematic evaluation of modern large language models\n(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting\nstrategies. Our results show that few-shot with CoT prompting performs best\ncompared to supervised baselines, and LLMs' performance varies across the SEC\nand ECT datasets. Our findings highlight the practical viability of leveraging\nLLMs for target-specific stance in the financial domain without requiring\nextensive labeled data.", "AI": {"tldr": "本研究构建了一个针对金融领域三大核心指标（债务、每股收益、销售额）的句子级立场检测语料库，并评估了大型语言模型在零样本、少样本和思维链提示策略下的表现，发现少样本+思维链提示效果最佳，证明了LLMs在金融立场分析中的实用性。", "motivation": "SEC文件财报和财报电话会议记录中的金融叙述对投资者、审计师和监管者很重要，但其长度、金融术语和微妙语言使细粒度分析变得困难，传统情感分析需要大量昂贵标注数据。", "method": "从10-K年报和财报电话会议记录中提取句子，使用ChatGPT-o3-pro模型在严格人工验证下标注立场（积极、消极、中性），系统评估现代LLMs在零样本、少样本和思维链提示策略下的表现。", "result": "少样本+思维链提示策略表现最佳，优于监督基线方法，LLMs在不同数据集（SEC和ECT）上的表现存在差异。", "conclusion": "研究结果表明，无需大量标注数据即可利用LLMs进行金融领域特定目标的立场检测，具有实际可行性。"}}
{"id": "2510.23477", "pdf": "https://arxiv.org/pdf/2510.23477", "abs": "https://arxiv.org/abs/2510.23477", "authors": ["Tengchao Yang", "Sichen Guo", "Mengzhao Jia", "Jiaming Su", "Yuanyang Liu", "Zhihan Zhang", "Meng Jiang"], "title": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring", "categories": ["cs.CL"], "comment": null, "summary": "Effective math tutoring requires not only solving problems but also\ndiagnosing students' difficulties and guiding them step by step. While\nmultimodal large language models (MLLMs) show promise, existing benchmarks\nlargely overlook these tutoring skills. We introduce MMTutorBench, the first\nbenchmark for AI math tutoring, consisting of 685 problems built around\npedagogically significant key-steps. Each problem is paired with\nproblem-specific rubrics that enable fine-grained evaluation across six\ndimensions, and structured into three tasks-Insight Discovery, Operation\nFormulation, and Operation Execution. We evaluate 12 leading MLLMs and find\nclear performance gaps between proprietary and open-source systems, substantial\nroom compared to human tutors, and consistent trends across input variants: OCR\npipelines degrade tutoring quality, few-shot prompting yields limited gains,\nand our rubric-based LLM-as-a-Judge proves highly reliable. These results\nhighlight both the difficulty and diagnostic value of MMTutorBench for\nadvancing AI tutoring.", "AI": {"tldr": "MMTutorBench是首个AI数学辅导基准测试，包含685个围绕关键教学步骤设计的问题，通过六个维度评估模型在洞察发现、操作制定和操作执行三个任务上的表现。评估显示专有模型优于开源模型，与人类导师仍有差距，OCR会降低质量，少样本提示效果有限，基于量表的LLM评估方法可靠。", "motivation": "现有基准测试忽视了AI数学辅导所需的诊断学生困难和逐步引导的关键教学技能，需要专门的评估工具来推动AI辅导系统发展。", "method": "构建包含685个教学关键步骤问题的MMTutorBench基准，每个问题配备特定评分量表，分为洞察发现、操作制定和操作执行三个任务，评估12个主流多模态大语言模型。", "result": "专有模型表现优于开源模型，与人类导师水平仍有显著差距；OCR处理会降低辅导质量；少样本提示提升有限；基于量表的LLM评估方法表现出高可靠性。", "conclusion": "MMTutorBench揭示了AI数学辅导的难度和诊断价值，为推进AI辅导系统发展提供了重要基准，显示了当前模型与人类导师的差距以及改进方向。"}}
{"id": "2510.23508", "pdf": "https://arxiv.org/pdf/2510.23508", "abs": "https://arxiv.org/abs/2510.23508", "authors": ["Jiahui Geng", "Jonathan Tonglet", "Iryna Gurevych"], "title": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset", "categories": ["cs.CL"], "comment": "Preprint under review. Code and data available at:\n  https://github.com/UKPLab/M4FC", "summary": "Existing real-world datasets for multimodal automated fact-checking have\nmultiple limitations: they contain few instances, focus on only one or two\nlanguages and tasks, suffer from evidence leakage, or depend on external sets\nof news articles for sourcing true claims. To address these shortcomings, we\nintroduce M4FC, a new real-world dataset comprising 4,982 images paired with\n6,980 claims. The images, verified by professional fact-checkers from 22\norganizations, represent diverse cultural and geographic contexts. Each claim\nis available in one or two out of ten languages. M4FC spans six multimodal\nfact-checking tasks: visual claim extraction, claimant intent prediction, fake\ndetection, image contextualization, location verification, and verdict\nprediction. We provide baseline results for all tasks and analyze how combining\nintermediate tasks influence downstream verdict prediction performance. We make\nour dataset and code available.", "AI": {"tldr": "M4FC是一个新的大规模多模态事实核查数据集，包含4,982张图像和6,980个声明，涵盖10种语言和6个多模态事实核查任务，解决了现有数据集的多个局限性。", "motivation": "现有真实世界多模态自动事实核查数据集存在多个局限性：样本数量少、语言和任务覆盖有限、存在证据泄露问题、依赖外部新闻文章集来获取真实声明。", "method": "构建M4FC数据集，包含由22个专业事实核查组织验证的4,982张图像和6,980个声明，涵盖10种语言，设计6个多模态事实核查任务。", "result": "提供了所有任务的基线结果，并分析了组合中间任务对下游裁决预测性能的影响。数据集和代码已公开提供。", "conclusion": "M4FC数据集解决了现有数据集的多个局限性，为多模态事实核查研究提供了更全面和实用的资源，有助于推动该领域的发展。"}}
{"id": "2510.23536", "pdf": "https://arxiv.org/pdf/2510.23536", "abs": "https://arxiv.org/abs/2510.23536", "authors": ["Jieyong Kim", "Maryam Amirizaniani", "Soojin Yoon", "Dongha Lee"], "title": "IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Intent identification serves as the foundation for generating appropriate\nresponses in personalized question answering (PQA). However, existing\nbenchmarks evaluate only response quality or retrieval performance without\ndirectly measuring intent identification capabilities. This gap is critical\nbecause without understanding which intents users prioritize, systems cannot\ngenerate responses satisfying individual information needs. To address this, we\nintroduce the concept of core intents: intents users prioritize when selecting\nanswers to satisfy their information needs. To evaluate these core intents, we\npropose IPQA, a benchmark for core Intent identification in Personalized\nQuestion Answering. Since users do not explicitly state their prioritized\nintents, we derive core intents from observable behavior patterns in answer\nselection, grounded in satisficing theory where users choose answers meeting\ntheir acceptance thresholds. We construct a dataset with various domains\nthrough systematic filtering, LLM-based annotation, and rigorous quality\ncontrol combining automated verification with human validation. Experimental\nevaluations across state-of-the-art language models reveal that current systems\nstruggle with core intent identification in personalized contexts. Models fail\nto identify core intents from user histories, with performance degrading as\nquestion complexity increases. The code and dataset will be made publicly\navailable to facilitate future research in this direction.", "AI": {"tldr": "该论文提出了IPQA基准测试，用于评估个性化问答中的核心意图识别能力，填补了现有基准测试只关注回答质量而忽略意图识别的空白。", "motivation": "现有基准测试只评估回答质量或检索性能，没有直接衡量意图识别能力，这导致系统无法理解用户优先考虑哪些意图来满足信息需求。", "method": "基于满意理论，从用户选择答案的可观察行为模式中推导核心意图，通过系统筛选、基于LLM的标注以及自动化验证与人工验证相结合的质量控制来构建多领域数据集。", "result": "实验评估显示当前最先进的语言模型在个性化场景下的核心意图识别表现不佳，模型无法从用户历史中识别核心意图，且随着问题复杂度增加性能下降。", "conclusion": "核心意图识别对个性化问答至关重要，当前系统在此方面存在不足，发布的代码和数据集将促进该方向的未来研究。"}}
{"id": "2510.23544", "pdf": "https://arxiv.org/pdf/2510.23544", "abs": "https://arxiv.org/abs/2510.23544", "authors": ["Tingyu Song", "Yilun Zhao", "Siyue Zhang", "Chen Zhao", "Arman Cohan"], "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking", "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP 2025 Main (Short)", "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.", "AI": {"tldr": "LIMRANK是一种使用少量合成数据训练的高效信息重排序模型，仅需传统方法5%的数据量就能达到竞争性性能。", "motivation": "现有LLM信息重排序方法需要大规模微调，计算成本高昂，需要寻找更高效的适应方法。", "method": "设计LIMRANK-SYNTHESIZER管道生成多样化、具有挑战性的合成重排序数据，并用这些数据微调LIMRANK模型。", "result": "LIMRANK在BRIGHT和FollowIR基准测试中表现优异，展示了强大的泛化能力，适用于科学文献搜索和知识密集型问题解决。", "conclusion": "通过高质量合成数据训练，LLM可以在极少量监督下有效适应信息重排序任务，显著降低计算成本。"}}
{"id": "2510.23585", "pdf": "https://arxiv.org/pdf/2510.23585", "abs": "https://arxiv.org/abs/2510.23585", "authors": ["Luis Ramos", "Hiram Calvo", "Olga Kolesnikova"], "title": "Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The identification of hope speech has become a promised NLP task, considering\nthe need to detect motivational expressions of agency and goal-directed\nbehaviour on social media platforms. This proposal evaluates traditional\nmachine learning models and fine-tuned transformers for a previously split hope\nspeech dataset as train, development and test set. On development test, a\nlinear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM\nwith RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models\ndelivered better results, the best model achieved weighted precision of 0.82,\nweighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80\naccuracy. These results suggest that while optimally configured traditional\nmachine learning models remain agile, transformer architectures detect some\nsubtle semantics of hope to achieve higher precision and recall in hope speech\ndetection, suggesting that larges transformers and LLMs could perform better in\nsmall datasets.", "AI": {"tldr": "该研究评估了传统机器学习模型和微调transformer在希望语音检测任务上的表现，发现transformer模型在精度和召回率方面优于传统方法，特别是在小数据集上表现更好。", "motivation": "识别社交媒体上的希望语音（包含激励性表达和目标导向行为的内容）已成为重要的NLP任务，需要有效的检测方法来处理这类内容。", "method": "使用预先分割的希望语音数据集（训练集、开发集和测试集），评估了传统机器学习模型（线性核SVM、RBF核SVM、逻辑回归、朴素贝叶斯）和微调的transformer模型。", "result": "传统模型中线性核SVM和逻辑回归的macro-F1为0.78，RBF核SVM为0.77，朴素贝叶斯为0.75。Transformer模型表现更好，最佳模型达到加权精度0.82、加权召回率0.80、加权F1 0.79、macro F1 0.79、准确率0.80。", "conclusion": "虽然优化配置的传统机器学习模型仍具有敏捷性，但transformer架构能够检测希望语音中更细微的语义特征，在精度和召回率方面表现更优，表明大型transformer和LLM在小数据集上可能表现更好。"}}
{"id": "2510.23596", "pdf": "https://arxiv.org/pdf/2510.23596", "abs": "https://arxiv.org/abs/2510.23596", "authors": ["Yizhu Jiao", "Jiaqi Zeng", "Julien Veron Vialard", "Oleksii Kuchaiev", "Jiawei Han", "Olivier Delalleau"], "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly rely on thinking models that\nexternalize intermediate steps and allocate extra test-time compute, with\nthink-twice strategies showing that a deliberate second pass can elicit\nstronger reasoning. In contrast, most reward models (RMs) still compress many\nquality dimensions into a single scalar in one shot, a design that induces\njudgment diffusion: attention spreads across evaluation criteria, yielding\ndiluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a\ntwo-turn RM that transfers the think-twice principle to reward modeling. Turn 1\nperforms adaptive branching, selecting a small set of instance-critical\ndimensions (such as factuality and safety) and sketching concise,\nevidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a\ntargeted reread that tests those hypotheses and scrutinizes only what matters\nmost. We train with GRPO-style reinforcement learning over structured two-turn\ntraces using a simple binary outcome reward with strict format checks, making\nthe approach compatible with standard RLHF pipelines. By converting\nall-at-oncescoringintofocused, second-lookreasoning,\nBR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet\nconsequential errors while remaining practical and scalable. Experimental\nresults demonstrate that our model achieves state-of-the-art performance on\nthree challenging reward modeling benchmarks across diverse domains. The code\nand the model will be released soon.", "AI": {"tldr": "BR-RM是一个两阶段的奖励模型，通过分支再思考机制减少判断扩散，提高对细微错误的敏感性，在多个奖励建模基准上达到最先进性能。", "motivation": "传统奖励模型将多个质量维度压缩为单一标量评分，导致注意力分散和浅层分析（判断扩散问题），而大语言模型的思考两次策略显示二次思考能提升推理能力。", "method": "提出分支再思考奖励模型（BR-RM）：第一轮进行自适应分支选择关键维度并生成证据寻求假设；第二轮执行分支条件再思考，针对性重新评估假设。使用GRPO风格的强化学习训练，采用带严格格式检查的二元结果奖励。", "result": "在三个具有挑战性的奖励建模基准测试中实现了最先进的性能，证明该方法能有效减少判断扩散并提高对细微错误的敏感性。", "conclusion": "BR-RM成功将思考两次原则应用于奖励建模，通过结构化两轮评估机制解决了判断扩散问题，同时保持了实用性和可扩展性，与标准RLHF流程兼容。"}}
