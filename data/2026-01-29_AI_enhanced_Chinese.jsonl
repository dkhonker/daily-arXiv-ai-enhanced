{"id": "2601.19955", "pdf": "https://arxiv.org/pdf/2601.19955", "abs": "https://arxiv.org/abs/2601.19955", "authors": ["Jean-Marc Fellous", "Gert Cauwenberghs", "Cornelia Fermüller", "Yulia Sandamisrkaya", "Terrence Sejnowski"], "title": "NeuroAI and Beyond", "categories": ["cs.AI", "cs.NE"], "comment": "53 pages, 5 figures, extended appendix", "summary": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.", "AI": {"tldr": "这篇论文基于2025年工作坊，探讨神经科学与人工智能的协同发展，提出了NeuroAI概念，旨在通过神经科学原理提升AI算法效率，同时深化对生物神经计算的理解。", "motivation": "神经科学和AI虽各自取得显著进展但联系松散，需要通过跨学科整合来推动两个领域的共同发展，提升AI能力并促进对神经机制的理解。", "method": "通过工作坊讨论，聚焦具体子领域（具身化、语言沟通、机器人、学习机制、神经形态工程），收集领域专家的个人观点，并进行SWOT分析。", "result": "提出了NeuroAI框架，识别了当前和未来的协同领域，展示了神经科学启发AI发展的潜力，并提供了专家观点和风险效益分析。", "conclusion": "倡导发展NeuroAI，认为这种神经科学启发的AI方法既能显著提升AI算法的范围和效率，又能改变我们对生物神经计算的理解方式，具有重要的发展前景。"}}
{"id": "2601.20014", "pdf": "https://arxiv.org/pdf/2601.20014", "abs": "https://arxiv.org/abs/2601.20014", "authors": ["Shuhui Qu"], "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.", "AI": {"tldr": "SQ-BCP是一种在部分可观测环境下进行推理时规划的新方法，通过显式表示前提条件状态和双向搜索，显著减少了资源违规率", "motivation": "大型语言模型在部分可观测环境下进行规划时经常出现问题：当任务关键前提条件未在查询时指定时，模型会幻觉缺失事实或产生违反硬约束的计划", "method": "引入自我查询双向分类规划(SQ-BCP)，显式表示前提条件状态(Sat/Viol/Unk)，通过(i)向预言机/用户进行针对性自我查询或(ii)通过额外动作建立缺失条件的桥接假设来解决未知问题。采用双向搜索并使用基于距离的分数仅用于排序和剪枝", "result": "在WikiHow和RecipeNLG任务中，SQ-BCP将资源违规率分别降低到14.9%和5.8%（最佳基线为26.0%和15.7%），同时保持竞争力的参考质量", "conclusion": "SQ-BCP在部分可观测环境下有效解决了LLM规划问题，通过形式化验证确保了计划与目标要求的兼容性，在有限分支和深度下能够找到可接受的计划"}}
{"id": "2601.20021", "pdf": "https://arxiv.org/pdf/2601.20021", "abs": "https://arxiv.org/abs/2601.20021", "authors": ["Shuhui Qu"], "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints", "categories": ["cs.AI"], "comment": null, "summary": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.", "AI": {"tldr": "提出了模糊范畴论规划(FCP)方法，通过引入模糊逻辑处理自然语言规划中的模糊谓词，使用t-范数进行质量组合，保持可执行性的严格验证，在食谱替换规划任务中表现优于基线方法。", "motivation": "现有范畴论规划器将适用性视为二值逻辑，需要阈值处理，这会丢失有意义的质量差异且无法跟踪多步计划中的质量退化。自然语言规划常涉及模糊谓词，其满足程度本质上是分级的。", "method": "FCP为每个动作(态射)标注[0,1]区间内的程度值，使用Lukasiewicz t-范数组合计划质量，通过拉回验证保持可执行性检查。使用LLM进行k样本中值聚合从语言中获取分级适用性，支持基于剩余的后向需求中间相遇搜索。", "result": "在PDDL3偏好/超额订阅基准和RecipeNLG-Subs食谱替换规划基准上评估。FCP相比纯LLM和ReAct风格基线提高了成功率并减少了硬约束违反，同时与经典PDDL3规划器保持竞争力。", "conclusion": "FCP成功地将模糊逻辑集成到范畴论规划中，有效处理自然语言中的模糊谓词，在保持可执行性严格验证的同时支持质量分级评估，为自然语言规划提供了更细粒度的解决方案。"}}
{"id": "2601.20048", "pdf": "https://arxiv.org/pdf/2601.20048", "abs": "https://arxiv.org/abs/2601.20048", "authors": ["Jincheng Bai", "Zhenyu Zhang", "Jennifer Zhang", "Zhihuai Zhu"], "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to SIGIR 2025. DOI: 10.1145/3726302.3731959", "summary": "Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.", "AI": {"tldr": "Insight Agents (IA)是一个基于LLM的多代理对话系统，通过分层代理结构为电商卖家提供个性化数据洞察，实现了90%的准确率和低于15秒的延迟。", "motivation": "电商卖家面临难以发现和利用可用工具、难以理解各种工具产生的丰富数据等挑战，需要自动化信息检索来提供个性化业务洞察。", "method": "采用分层多代理结构（管理代理+两个工作代理），结合OOD检测和BERT分类器进行路由，使用API数据模型进行战略规划，并动态注入领域知识。", "result": "系统在美国亚马逊卖家平台上线，人工评估准确率达到90%，P90延迟低于15秒。", "conclusion": "IA系统通过多代理架构成功解决了电商卖家数据洞察的挑战，验证了其作为效率倍增器的假设，能够帮助卖家更快速做出商业决策。"}}
{"id": "2601.19913", "pdf": "https://arxiv.org/pdf/2601.19913", "abs": "https://arxiv.org/abs/2601.19913", "authors": ["Shinwoo Park", "Yo-Sub Han"], "title": "From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.", "AI": {"tldr": "该研究开发了LREAD评分标准，通过结构化校准训练韩语语言学专业学生，显著提高了他们区分人类撰写文本与LLM生成文本的能力，准确率从60%提升到100%。", "motivation": "区分人类撰写的韩语文本与流畅的LLM输出对语言专家也很困难，研究者希望探索是否可以通过结构化校准将专家检测作为可学习技能进行提升。", "method": "采用三阶段纵向盲测协议：第一阶段测量直觉检测，第二阶段实施标准级评分并要求明确理由，第三阶段评估对保留的小学作文的领域专注掌握能力。使用基于韩国国家写作标准开发的LREAD评分标准。", "result": "多数投票准确率从60%提高到100%，标注者间一致性显著增强（Fleiss' kappa从-0.09提升到0.82）。校准后的人类检测者比最先进的LLM检测器更依赖语言特定的微观诊断特征。", "conclusion": "基于评分标准的专家判断可以作为非英语环境中自动化检测器的可解释补充工具，研究发布了完整的评分标准和校准检测特征的分类体系。"}}
{"id": "2601.20090", "pdf": "https://arxiv.org/pdf/2601.20090", "abs": "https://arxiv.org/abs/2601.20090", "authors": ["Amirmohammad Farzaneh", "Salvatore D'Oro", "Osvaldo Simeone"], "title": "Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.", "AI": {"tldr": "该论文提出了一个名为CCG的框架，用于在LLM驱动的智能体控制场景中进行反事实推理，通过结构因果模型和概率溯因生成多个候选反事实结果，并提供高概率包含真实反事实结果的可靠性保证。", "motivation": "用户在使用LLM智能体执行任务后，可能会想知道如果以不同方式表达意图会得到什么结果，因此需要一种能够提供可靠反事实推理的方法。", "method": "使用结构因果模型(SCM)建模用户、LLM智能体和环境之间的闭环交互，通过测试时缩放和概率溯因生成候选反事实结果，并通过离线校准阶段提供形式化的可靠性保证。", "result": "在无线网络控制用例中展示了CCG的性能，相比简单的重新执行基线方法表现出显著优势。", "conclusion": "CCG框架能够有效支持反事实推理，为用户提供可靠的\"如果...会怎样\"分析，在LLM驱动的智能体控制场景中具有实用价值。"}}
{"id": "2601.19914", "pdf": "https://arxiv.org/pdf/2601.19914", "abs": "https://arxiv.org/abs/2601.19914", "authors": ["Maxwell Crouse", "Ibrahim Abdelaziz", "Kshitij Fadnis", "Siva Sankalp Patel", "Kinjal Basu", "Chulaka Gunasekara", "Sadhana Kumaravel", "Asim Munawar", "Pavan Kapanipathi"], "title": "Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.", "AI": {"tldr": "DiGiT-TC是一种新的合成数据生成方法，专门用于创建多轮工具调用对话数据，特别适用于无状态执行环境场景，在标准基准测试中显示出显著性能提升", "motivation": "现有合成数据生成方法通常假设工具调用在可维护状态的环境中执行，但现实中许多场景（如企业数据安全环境或多源工具规范）无法提供状态执行环境，导致现有方法不适用", "method": "提出DiGiT-TC方法，采用新颖的生成模式，在用户请求中隐式表示某些工具调用，从而模拟状态环境中的对话特征", "result": "在标准工具调用基准测试中验证了该方法，即使在状态化问题设置中也能实现强大的性能增益", "conclusion": "DiGiT-TC填补了无状态环境下工具调用对话数据生成的空白，为实际应用场景提供了有效的合成数据解决方案"}}
{"id": "2601.20206", "pdf": "https://arxiv.org/pdf/2601.20206", "abs": "https://arxiv.org/abs/2601.20206", "authors": ["Zixuan Xiao", "Chunguang Hu", "Jun Ma"], "title": "Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis", "categories": ["cs.AI"], "comment": null, "summary": "As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.", "AI": {"tldr": "该研究提出了一种多模态LLM智能体框架，通过水平垂直数据对齐机制和专用工具包，解决城市公园发展监测中传统遥感方法的局限性，实现更智能的多模态数据分析。", "motivation": "传统遥感影像变化检测方法在高层次智能分析方面存在明显局限，难以满足当前城市规划管理对复杂多模态数据分析的日益增长需求，特别是在城市公园发展监测领域。", "method": "提出多模态LLM智能体框架，设计通用水平垂直数据对齐机制确保多模态数据一致性，构建特定工具包缓解LLM因缺乏领域知识而产生的幻觉问题。", "result": "相比原生GPT-4o和其他智能体，该方法实现了稳健的多模态信息融合与分析，为城市公园发展监测提供了可靠且可扩展的解决方案。", "conclusion": "该框架充分利用LLM的语义理解和推理能力，能够有效应对城市公园发展监测中的挑战，为城市规划评估和资源优化配置提供更智能的分析工具。"}}
{"id": "2601.19915", "pdf": "https://arxiv.org/pdf/2601.19915", "abs": "https://arxiv.org/abs/2601.19915", "authors": ["Paul Tarau"], "title": "Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication", "categories": ["cs.CL", "cs.AI", "cs.LO"], "comment": "25 pages", "summary": "We introduce the \\emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \\emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \\emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.\n  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.", "AI": {"tldr": "提出Arrow语言模型，基于直觉主义逻辑解释的神经架构，将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序，将下一个token预测对应为肯定前件推理。", "motivation": "从直觉主义逻辑的角度重新解释下一个token预测，探索基于逻辑推导的神经架构替代方案，寻求Transformer和状态空间模型之外的替代方案。", "method": "使用直觉主义逻辑的蕴含链编码前缀，通过非交换组合保持顺序结构，基于Prolog的定理证明器验证模型性质，提出实用的低秩神经实现。", "result": "展示了神经架构与乘法RNN的等价性，从证明论角度解释下一个token预测为嵌套直觉主义蕴含，建立了与Transformer和状态空间模型的相对位置关系。", "conclusion": "Arrow语言模型提供了一种基于逻辑推导的神经架构新范式，通过直觉主义逻辑的视角重新构建语言建模问题，为替代Transformer基础模型提供了理论框架和实现路径。"}}
{"id": "2601.20221", "pdf": "https://arxiv.org/pdf/2601.20221", "abs": "https://arxiv.org/abs/2601.20221", "authors": ["Hang Zhang", "Ruheng Wang", "Yuelyu Ji", "Mingu Kwak", "Xizhi Wu", "Chenyu Li", "Li Zhang", "Wenqi Shi", "Yifan Peng", "Yanshan Wang"], "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.", "AI": {"tldr": "M$\\method$是一个医疗推理验证框架，通过迭代查询外部医学知识库进行验证，结合工具增强验证和迭代强化学习，在四个医疗推理基准上显著提升性能，相比基线方法减少8倍采样预算需求。", "motivation": "现有奖励模型方法存在两个局限性：只产生标量奖励值而无明确解释，以及依赖单次检索而无法在验证过程中自适应获取知识。需要确保医疗推理的事实准确性。", "method": "训练医疗推理验证器在评估过程中迭代查询外部医学语料库，结合工具增强验证和迭代强化学习范式（仅需轨迹级监督），以及动态调整训练数据分布的自适应课程机制。", "result": "在四个医疗推理基准上取得显著提升：MedQA准确率提高23.5%，MedXpertQA提高32.0%（相对于基础生成器），相比先前奖励模型基线减少8倍采样预算需求。", "conclusion": "基于动态检索证据的验证为构建更可靠的医疗推理系统提供了原则性路径，证明迭代知识检索对提升医疗推理事实准确性的重要性。"}}
{"id": "2601.19916", "pdf": "https://arxiv.org/pdf/2601.19916", "abs": "https://arxiv.org/abs/2601.19916", "authors": ["Songjun Tu", "Yiwen Ma", "Jiahao Lin", "Qichao Zhang", "Xiangyuan Lan", "Junfeng. Li", "Nan Xu", "Linjing Li", "Dongbin Zhao"], "title": "PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review", "categories": ["cs.CL"], "comment": null, "summary": "Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.", "AI": {"tldr": "该论文提出了PaperAudit-Bench评测基准，包含数据集和自动审稿框架，用于评估大语言模型在长文本环境下检测学术论文中细微错误的能力，并通过实验证明结构化错误检测能产生更严格的审稿评估。", "motivation": "大语言模型虽然能生成流畅的同行评审，但在处理分散在论文各部分的细微实质性错误时缺乏足够的批判性严谨度，需要更好的评测方法和工具来提升审稿质量。", "method": "构建PaperAudit-Dataset错误数据集（包含单章节和跨章节推理错误），开发PaperAudit-Review自动审稿框架（结合结构化错误检测和证据感知的评审生成），并通过实验比较不同模型在长文本环境下的错误检测能力。", "result": "实验显示不同模型在错误检测能力上存在显著差异，结构化错误检测相比基线方法能产生更严格和更具区分度的评估结果，数据集还支持通过SFT和RL训练轻量级检测器以降低计算成本。", "conclusion": "PaperAudit-Bench有效解决了大语言模型在学术审稿中批判性不足的问题，结构化错误检测方法显著提升了审稿质量，为自动化同行评审提供了可靠的技术支持。"}}
{"id": "2601.20305", "pdf": "https://arxiv.org/pdf/2601.20305", "abs": "https://arxiv.org/abs/2601.20305", "authors": ["Zhenchen Tang", "Songlin Yang", "Zichuan Wang", "Bo Peng", "Yang Li", "Beibei Dong", "Jing Dong"], "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models", "categories": ["cs.AI"], "comment": null, "summary": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.", "AI": {"tldr": "SEER框架通过内生重提示机制解决统一多模态模型的认知鸿沟问题，仅用300个样本训练即可实现自我评估和生成优化的两阶段循环，在多个指标上超越现有基准。", "motivation": "统一多模态模型虽然具备强大的理解能力，但无法有效指导生成过程，存在认知鸿沟问题。", "method": "提出内生重提示机制，通过SEER训练框架建立两阶段内生循环：1) 使用可验证奖励的强化学习激活模型评估能力；2) 利用模型奖励思维优化生成推理策略。", "result": "SEER在评估准确性、重提示效率和生成质量方面均优于最先进基准，且不牺牲通用多模态能力。", "conclusion": "内生重提示机制成功将模型理解从被动编码过程转化为显式生成推理步骤，为解决认知鸿沟问题提供了有效解决方案。"}}
{"id": "2601.19917", "pdf": "https://arxiv.org/pdf/2601.19917", "abs": "https://arxiv.org/abs/2601.19917", "authors": ["Haoyu Zheng", "Yun Zhu", "Yuqian Yuan", "Bo Yuan", "Wenqiao Zhang", "Siliang Tang", "Jun Xiao"], "title": "PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.", "AI": {"tldr": "PILOT框架通过轻量级超网络生成潜在引导向量，将大模型的战略规划能力内化到紧凑LLM中，无需修改主干权重即可显著提升多步推理性能", "motivation": "紧凑型大语言模型缺乏全局战略规划能力，导致长时域任务中的错误传播问题，虽然可以通过外部教师模型提供显式规划来解锁潜在推理能力，但运行时依赖外部指导在延迟和可用性方面不实用", "method": "提出PILOT框架，使用轻量级超网络合成查询条件化的潜在引导向量，该向量作为内部转向机制，引导模型表示朝向最优推理路径，无需修改主干模型权重", "result": "在数学和编程基准测试中，PILOT有效稳定了推理轨迹，显著超越强基线（如MATH500上提升8.9%），且推理延迟可忽略不计", "conclusion": "PILOT成功将大模型的战略监督能力内化为紧凑模型的潜在引导，为非侵入式提升LLM推理能力提供了有效解决方案"}}
{"id": "2601.20323", "pdf": "https://arxiv.org/pdf/2601.20323", "abs": "https://arxiv.org/abs/2601.20323", "authors": ["Hyunseung Chung", "Jungwoo Oh", "Daeun Kyung", "Jiho Kim", "Yeonsu Kwon", "Min-Gyu Kim", "Edward Choi"], "title": "ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue", "categories": ["cs.AI"], "comment": "Accepted to ICASSP 2026 (5 pages, 2 figures, 5 tables)", "summary": "Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.", "AI": {"tldr": "ECG-Agent是首个基于LLM的工具调用代理，用于多轮心电图对话，解决了现有模型在多轮对话能力、设备端效率和心电图测量精度方面的不足，并提供了ECG-MTD数据集进行开发评估。", "motivation": "现有多模态大语言模型在心电图应用中缺乏多轮对话能力、设备端效率和对心电图测量（如PQRST间期）的精确理解，无法满足真实场景需求。", "method": "开发了ECG-Agent工具调用代理，创建了ECG-MTD多轮对话数据集，构建了从设备端到大型代理的不同规模模型。", "result": "ECG-Agent在响应准确性上优于基线ECG-LLM，设备端代理在响应准确性、工具调用能力和幻觉控制方面与大型代理表现相当。", "conclusion": "ECG-Agent展示了在真实世界应用中的可行性，特别是设备端代理在保持性能的同时实现了高效部署。"}}
{"id": "2601.19918", "pdf": "https://arxiv.org/pdf/2601.19918", "abs": "https://arxiv.org/abs/2601.19918", "authors": ["Yitong Qiao", "Licheng Pan", "Yu Mi", "Lei Liu", "Yue Shen", "Fei Sun", "Zhixuan Chu"], "title": "Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.", "AI": {"tldr": "提出了一种名为LSC（Lowest Span Confidence）的高效零样本幻觉检测方法，仅需单次前向传播和输出概率，通过滑动窗口机制评估语义连贯跨度的联合似然，在资源受限条件下优于现有基线方法。", "motivation": "现有幻觉检测方法要么需要昂贵的密集采样策略进行一致性检查，要么需要白盒LLM状态，这在基于API的常见场景中不可用或效率低下。", "method": "LSC通过滑动窗口机制评估语义连贯跨度的联合似然，识别可变长度n-gram中最低边际置信度区域，捕捉与事实不一致性高度相关的局部不确定性模式。", "result": "在多个最先进LLM和多样化基准测试上的广泛实验表明，LSC始终优于现有的零样本基线方法，即使在资源受限条件下也能提供强大的检测性能。", "conclusion": "LSC能够缓解困惑度的稀释效应和最小标记概率的噪声敏感性，提供了更稳健的事实不确定性估计，为高风险环境中LLM的可靠部署提供了有效的幻觉检测解决方案。"}}
{"id": "2601.20352", "pdf": "https://arxiv.org/pdf/2601.20352", "abs": "https://arxiv.org/abs/2601.20352", "authors": ["Weiquan Huang", "Zixuan Wang", "Hehai Lin", "Sudong Wang", "Bo Xu", "Qian Li", "Beier Zhu", "Linyi Yang", "Chengwei Qin"], "title": "AMA: Adaptive Memory via Multi-Agent Collaboration", "categories": ["cs.AI"], "comment": "8 pages", "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.", "AI": {"tldr": "AMA框架通过多智能体协作实现自适应记忆管理，采用分层记忆设计动态匹配检索粒度与任务复杂度，显著提升检索精度和长期记忆一致性，同时减少80%的token消耗。", "motivation": "现有LLM智能体记忆系统存在检索粒度僵化、维护策略积累繁重和更新机制粗粒度等问题，导致存储信息与任务推理需求不匹配，以及逻辑不一致性随时间累积。", "method": "提出AMA框架，采用多智能体协作：Constructor和Retriever实现多粒度记忆构建和自适应查询路由；Judge验证相关性和一致性；Refresher执行针对性更新或删除过时条目以保持一致性。", "result": "在长上下文基准测试中显著优于最先进基线方法，相比全上下文方法减少约80%的token消耗，证明了在保持检索精度和长期记忆一致性方面的有效性。", "conclusion": "AMA框架通过多智能体协作和分层记忆设计有效解决了现有记忆系统的局限性，为LLM智能体提供了更高效、精确和一致的长时记忆管理方案。"}}
{"id": "2601.19919", "pdf": "https://arxiv.org/pdf/2601.19919", "abs": "https://arxiv.org/abs/2601.19919", "authors": ["Junseok Lee", "Nahoon Kim", "Sangyong Lee", "Chang-Jae Chun"], "title": "FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.", "AI": {"tldr": "提出自适应自知识蒸馏(ASKD)方法，通过动态减少对教师模型的依赖来提升学生模型的泛化能力，并将Whisper模型压缩为更小的FastWhisper版本，在推理速度提升5倍的同时错误率降低1.07%。", "motivation": "传统知识蒸馏方法中，学生模型可能会继承教师模型的缺点，导致泛化能力下降。需要一种方法来减少对教师模型的过度依赖，提升学生模型的自主学习能力。", "method": "提出自适应自知识蒸馏(ASKD)方法：1) 动态调整对教师模型的依赖程度；2) 采用自知识蒸馏技术提升泛化能力；3) 将Whisper模型蒸馏为更小的FastWhisper变体。", "result": "FastWhisper在词错误率上比原始Whisper模型降低了1.07%，同时推理速度提升了5倍，实现了模型压缩和性能提升的双重目标。", "conclusion": "ASKD方法有效解决了知识蒸馏中学生模型继承教师模型缺点的问题，通过自适应调整依赖关系和自蒸馏技术，显著提升了学生模型的泛化能力和推理效率。"}}
{"id": "2601.20379", "pdf": "https://arxiv.org/pdf/2601.20379", "abs": "https://arxiv.org/abs/2601.20379", "authors": ["Zhengbo Jiao", "Hongyu Xian", "Qinglong Wang", "Yunpu Ma", "Zhebo Wang", "Zifan Zhang", "Dezhang Kong", "Meng Han"], "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution", "categories": ["cs.AI"], "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.", "AI": {"tldr": "PoT框架通过在线优化和GRPO算法，使小型语言模型在复杂推理任务上超越大型模型表现", "motivation": "大型语言模型在复杂长程推理中存在策略冻结问题，当前方法仅将执行反馈作为外部信号使用，未能内部化改进推理策略", "method": "提出Policy of Thoughts框架：1) 通过高效探索机制生成多样候选解 2) 使用Group Relative Policy Optimization基于执行反馈更新瞬态LoRA适配器，实现实例特定的在线优化", "result": "4B参数模型在LiveCodeBench上达到49.71%准确率，超越GPT-4o和DeepSeek-V3，尽管模型规模小50倍以上", "conclusion": "通过实时从失败尝试中学习的闭环设计，PoT框架能够动态优化推理策略，显著提升语言模型的复杂推理能力"}}
{"id": "2601.19921", "pdf": "https://arxiv.org/pdf/2601.19921", "abs": "https://arxiv.org/abs/2601.19921", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Yizhou Chi", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "title": "Demystifying Multi-Agent Debate: The Role of Confidence and Diversity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.", "AI": {"tldr": "论文提出两种轻量级干预方法改进多智能体辩论：多样性感知初始化和置信度调节辩论协议，通过增加初始观点多样性和置信度校准通信，显著提升LLM辩论效果", "motivation": "现有vanilla MAD方法在计算成本更高的情况下表现不如简单多数投票，研究发现同质智能体和统一信念更新无法可靠改善结果，需要借鉴人类审议的多样性初始观点和置信度沟通机制", "method": "1. 多样性感知初始化：选择更多样化的候选答案池，增加辩论开始时存在正确假设的可能性；2. 置信度调节辩论协议：智能体表达校准后的置信度，并根据他人置信度条件更新信念", "result": "理论上证明多样性初始化提高MAD成功先验概率，置信度调节使辩论系统性地偏向正确假设；实证在六个推理问答基准上一致优于vanilla MAD和多数投票", "conclusion": "将人类审议机制与基于LLM的辩论相结合，通过简单而有原则的修改可显著提升辩论效果，为多智能体协作提供新思路"}}
{"id": "2601.20380", "pdf": "https://arxiv.org/pdf/2601.20380", "abs": "https://arxiv.org/abs/2601.20380", "authors": ["Le Zhang", "Yixiong Xiao", "Xinjiang Lu", "Jingjia Cao", "Yusai Zhao", "Jingbo Zhou", "Lang An", "Zikan Feng", "Wanxiang Sha", "Yu Shi", "Congxi Xiao", "Jian Xiong", "Yankai Zhang", "Hua Wu", "Haifeng Wang"], "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.", "AI": {"tldr": "OmegaUse是一个通用的GUI代理模型，支持移动和桌面平台的自主任务执行，通过精心设计的数据构建管道和两阶段训练策略，在多个基准测试中达到最先进性能。", "motivation": "构建有效的GUI代理模型需要高质量数据和有效训练方法，以支持跨平台的计算机和手机使用场景，革新人机交互并提高生产力。", "method": "采用数据构建管道（结合开源数据集和自动化合成框架）和两阶段训练策略（SFT建立基础交互语法，GRPO改进空间定位和顺序规划），基于MoE架构平衡计算效率与推理能力。", "result": "在多个基准测试中表现优异：ScreenSpot-V2达到96.3% SOTA分数，AndroidControl达到79.1%步骤成功率，OS-Nav基准中ChiM-Nav达到74.24%步骤成功率，Ubu-Nav达到55.9%平均成功率。", "conclusion": "OmegaUse通过创新的数据构建和训练方法，成功实现了跨平台GUI代理的高效执行能力，为通用GUI代理模型的发展提供了有效解决方案。"}}
{"id": "2601.19922", "pdf": "https://arxiv.org/pdf/2601.19922", "abs": "https://arxiv.org/abs/2601.19922", "authors": ["Laya Iyer", "Kriti Aggarwal", "Sanmi Koyejo", "Gail Heyman", "Desmond C. Ong", "Subhabrata Mukherjee"], "title": "HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.", "AI": {"tldr": "HEART框架首次在情感支持对话中直接比较人类和LLM的表现，通过盲测和LLM评估发现前沿模型在共情和一致性上接近或超过人类平均水平，但人类在适应性重构、紧张情境处理和细微语调变化方面仍有优势。", "motivation": "尽管语言模型快速发展，但缺乏明确的方法来评估其在人际交往领域（如情感支持对话）的能力与人类相比如何，特别是在情感解读、语调调整和处理抵抗/沮丧时刻等方面。", "method": "提出HEART框架，在相同的多轮情感支持对话中配对人类和模型回应，通过盲测人类评估员和LLM评估员集合进行评估，使用基于人际沟通科学的五个维度评估标准：人类对齐、共情反应、调谐、共鸣和任务遵循。", "result": "前沿模型在感知共情和一致性方面接近或超过人类平均水平；人类在适应性重构、紧张情境命名和细微语调变化方面保持优势；人类和LLM评估偏好在大约80%的配对比较中一致，与人类间一致性相当。", "conclusion": "HEART将支持性对话重新定义为独立的能力维度，与一般推理或语言流畅性分离，为理解模型生成支持与人类社交判断的契合点和差异点提供了统一的实证基础。"}}
{"id": "2601.20467", "pdf": "https://arxiv.org/pdf/2601.20467", "abs": "https://arxiv.org/abs/2601.20467", "authors": ["Zhenxuan Fan", "Jie Cao", "Yang Dai", "Zheqi Lv", "Wenqiao Zhang", "Zhongle Xie", "Peng LU", "Beng Chin Ooi"], "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 9 figures, 11 tables", "summary": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.", "AI": {"tldr": "CtrlCoT是一个双粒度思维链压缩框架，通过语义抽象和令牌级剪枝的协同作用，在减少30.7%令牌使用的同时提升推理准确率7.6个百分点", "motivation": "现有思维链(CoT)提示方法存在高延迟和高内存成本问题，现有压缩方法要么过于保守，要么会丢失关键推理线索导致准确率下降", "method": "提出三组件框架：分层推理抽象生成多粒度语义CoT、逻辑保持蒸馏训练逻辑感知剪枝器保留关键推理线索、分布对齐生成确保压缩后推理流畅性", "result": "在MATH-500数据集上使用Qwen2.5-7B-Instruct模型，相比最强基线减少30.7%令牌使用，准确率提升7.6个百分点", "conclusion": "CtrlCoT实现了更高效可靠的推理，证明了语义抽象与令牌剪枝协同压缩的有效性"}}
{"id": "2601.19923", "pdf": "https://arxiv.org/pdf/2601.19923", "abs": "https://arxiv.org/abs/2601.19923", "authors": ["Boxiang Zhao", "Qince Li", "Zhonghao Wang", "Zelin Cao", "Yi Wang", "Peng Cheng", "Bo Lin"], "title": "Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.", "AI": {"tldr": "Table-BiEval：一种无需人工干预的自我监督评估框架，用于定量评估LLMs将自然语言转换为结构化格式和表格信息的能力，通过内容语义准确性和标准化树编辑距离来分离结构和内容。", "motivation": "当前评估方法缺乏有效测量LLMs结构化保真度的能力，传统文本指标无法检测代码式输出的语义漂移，需要无需人工干预的评估方法。", "method": "基于确定性中间表示，计算内容语义准确性和标准化树编辑距离，在层次结构和平面表格两个拓扑维度上评估15个最先进LLMs。", "result": "结果显示模型性能存在显著差异，中等规模模型在结构效率上可能优于更大模型，深度递归嵌套仍然是当前架构的普遍瓶颈。", "conclusion": "Table-BiEval提供了一个有效的人类免费评估框架，揭示了LLMs在结构化输出能力方面的局限性，特别是处理复杂嵌套结构时的挑战。"}}
{"id": "2601.20487", "pdf": "https://arxiv.org/pdf/2601.20487", "abs": "https://arxiv.org/abs/2601.20487", "authors": ["Nico Mutzner", "Taha Yasseri", "Heiko Rauhut"], "title": "Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups", "categories": ["cs.AI", "cs.GT", "cs.HC", "econ.GN"], "comment": null, "summary": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.", "AI": {"tldr": "该研究通过在线实验发现，在人类与AI混合的小组中，合作规范机制与全人类小组运作方式相同，合作水平不受AI标签影响，支持规范性等效模式。", "motivation": "研究AI智能体如何影响人类小组中的合作社会规范，填补了先前研究主要关注二元互动而缺乏对小组环境中AI整合影响的空白。", "method": "采用在线实验设计，使用重复的四玩家公共物品博弈(PGG)，每组包含3名人类参与者和1个机器人（被标注为人类或AI），机器人采用三种预定策略之一：无条件合作、条件合作或搭便车。", "result": "发现合作主要由互惠群体动态和行为惯性驱动，这些规范机制在不同条件下运作相同，合作水平在人类和AI标签间无显著差异，后续囚徒困境中也未发现规范持续性的差异。", "conclusion": "合作规范足够灵活可以扩展到人工智能体，模糊了人类与AI在集体决策中的界限，支持规范性等效模式，即维持合作的机制在混合人类-AI和全人类群体中功能相似。"}}
{"id": "2601.19924", "pdf": "https://arxiv.org/pdf/2601.19924", "abs": "https://arxiv.org/abs/2601.19924", "authors": ["Yitian Chen", "Cheng Cheng", "Yinan Sun", "Zi Ling", "Dongdong Ge"], "title": "OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \\textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.", "AI": {"tldr": "OPT-ENGINE是一个用于评估大语言模型在优化建模方面能力的可扩展基准框架，包含10个运筹学标准任务，研究发现工具集成推理比纯文本推理在复杂任务中表现更稳健，约束自动制定是主要性能瓶颈。", "motivation": "尽管大语言模型在优化建模方面取得显著进展，但其在自动化制定和解决复杂现实问题方面的能力边界仍不明确，需要建立可控可扩展的评估基准。", "method": "提出OPT-ENGINE基准框架，包含5个线性规划和5个混合整数规划任务，通过控制难度水平评估LLMs的推理能力，特别关注分布外泛化和性能瓶颈分析。", "result": "实证研究表明：1) 工具集成推理在任务复杂度增加时表现出更高的稳健性，纯文本推理存在性能上限；2) 约束的自动制定是当前LLMs的主要性能瓶颈。", "conclusion": "研究结果为开发下一代用于高级优化的大语言模型提供了可操作的指导，工具集成方法和改进约束制定能力是未来发展的关键方向。"}}
{"id": "2601.20539", "pdf": "https://arxiv.org/pdf/2601.20539", "abs": "https://arxiv.org/abs/2601.20539", "authors": ["Oguzhan Gungordu", "Siheng Xiong", "Faramarz Fekri"], "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.", "AI": {"tldr": "PathWise是一个基于多智能体推理的自动启发式设计框架，通过将启发式生成建模为序列决策过程，使用蕴含图作为状态记忆，实现了从试错进化到状态感知规划的转变。", "motivation": "现有自动启发式设计框架依赖固定进化规则和静态提示模板，导致启发式生成短视、评估冗余且缺乏对新启发式推导的推理能力。", "method": "提出多智能体框架PathWise：策略智能体规划进化动作，世界模型智能体基于动作生成启发式推演，评论智能体提供路由反射总结先前步骤的经验。使用蕴含图作为搜索轨迹的紧凑状态记忆。", "result": "实验表明PathWise在不同组合优化问题上收敛更快、获得更好的启发式，能够泛化到不同的LLM骨干网络，并可扩展到更大的问题规模。", "conclusion": "PathWise通过状态感知规划和多智能体推理，显著改进了基于LLM的自动启发式设计，实现了更高效的启发式搜索和更好的泛化能力。"}}
{"id": "2601.19925", "pdf": "https://arxiv.org/pdf/2601.19925", "abs": "https://arxiv.org/abs/2601.19925", "authors": ["Yinuo Liu", "Emre Sezgin", "Eric A. Youngstrom"], "title": "Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 4 figures, 2 tables", "summary": "Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.", "AI": {"tldr": "本研究评估了ChatGPT-5、Gemini-3-Pro和Claude-Sonnet-4.5三个大语言模型在学术摘要评估中的一致性和可靠性，发现LLMs在客观标准上与人类评审员达到中等一致性，但在主观维度表现较弱，建议AI应作为人类评审的补充工具。", "motivation": "探索大语言模型在协助科学评审方面的潜力，研究LLMs评估学术摘要的一致性和可靠性，与人类评审员进行比较。", "method": "使用160篇会议摘要，由人类评审员和三个LLM使用相同评分标准进行评分。通过组内相关系数(ICC)计算评分者间信度，使用Bland-Altman图分析视觉一致性模式和系统偏差。", "result": "LLMs之间达成良好至极好的一致性(ICC: 0.59-0.87)。ChatGPT和Claude在整体质量和内容特定标准上与人类评审员达到中等一致性(ICC: 0.45-0.60)，在主观维度上一致性较差(ICC: 0.23-0.38)。三个LLM与人类平均综合分数的差异可接受或可忽略。", "conclusion": "LLMs能够批量处理摘要，在整体质量和客观标准上与人类专家达成中等一致性。在适当的过程架构下，LLMs可以一致地应用评分标准处理大量摘要。主观维度表现较弱表明AI应在评估中发挥补充作用，人类专业知识仍然至关重要。"}}
{"id": "2601.20554", "pdf": "https://arxiv.org/pdf/2601.20554", "abs": "https://arxiv.org/abs/2601.20554", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function", "categories": ["cs.AI"], "comment": null, "summary": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.", "AI": {"tldr": "该论文提出了一种在部分可观测环境下使用ICVaR动态风险度量的风险敏感规划方法，开发了具有有限时间性能保证的策略评估算法，并扩展了三种在线规划算法以优化ICVaR值函数而非期望回报。", "motivation": "传统基于期望的规划方法无法有效处理风险敏感场景，特别是在部分可观测环境中需要更好地控制尾部风险。", "method": "开发了ICVaR策略评估算法，并将Sparse Sampling、PFT-DPW和POMCPOW三种算法扩展到ICVaR优化，引入风险参数α来控制风险厌恶程度。", "result": "实验结果表明，提出的ICVaR规划器在基准POMDP领域中相比风险中性方法实现了更低的尾部风险。", "conclusion": "ICVaR方法为部分可观测环境下的风险敏感规划提供了有效的框架，通过风险参数实现了从风险中性到风险厌恶的平滑过渡，具有实际应用价值。"}}
{"id": "2601.19926", "pdf": "https://arxiv.org/pdf/2601.19926", "abs": "https://arxiv.org/abs/2601.19926", "authors": ["Nora Graichen", "Iria de-Dios-Flores", "Gemma Boleda"], "title": "The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.", "AI": {"tldr": "对337篇评估Transformer语言模型句法能力研究的系统综述，涵盖1,015个模型结果，显示当前研究存在英语和BERT模型的过度关注，模型在形式化句法现象表现良好但在句法-语义界面现象表现不稳定", "motivation": "系统评估Transformer语言模型在句法能力方面的研究现状，识别当前研究方法的局限性和偏差", "method": "对337篇相关论文进行系统性综述，分析1,015个模型结果，涵盖多种句法现象和可解释性方法", "result": "研究发现当前方法多样性良好，但过度集中于英语、BERT模型和易于处理的句法现象；模型在词性和一致性等形式化现象表现良好，但在绑定和填充-空位依赖等句法-语义界面现象表现较弱且不稳定", "conclusion": "提出未来研究建议：完整数据报告、理论构念与方法对齐、增加机制性方法使用、扩大语言和语言现象的实证范围"}}
{"id": "2601.20604", "pdf": "https://arxiv.org/pdf/2601.20604", "abs": "https://arxiv.org/abs/2601.20604", "authors": ["Gray Cox"], "title": "Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies", "categories": ["cs.AI"], "comment": "23 pages, 5 tables, 5 appendices. Code and data: https://github.com/jgraycox-coa/vcw-multi-ai-dialogue", "summary": "This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.\n  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.\n  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of \"VCW as transitional framework.\" Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.\n  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.", "AI": {"tldr": "本研究提出了一个通过结构化多模型对话实证测试AI对齐策略的方法框架，基于和平研究的理论基础，将AI对齐重新定义为关系问题而非控制问题。", "motivation": "将AI对齐从传统的控制问题重构为关系问题，借鉴和平研究中的利益协商、冲突转化和公共治理等理念，开发可操作的病毒式协作智慧(VCW)方法。", "method": "采用实验设计，为Claude、Gemini和GPT-4o等不同AI系统分配四种角色(提议者、回应者、监督者、翻译者)，在六种条件下进行72轮对话，共576,822字符的结构化交流。", "result": "AI系统能够有意义地参与和平研究概念讨论，从不同架构视角提出互补性反对意见，并产生初始框架中未出现的新见解，如\"VCW作为过渡框架\"的新综合。不同模型关注点各异：Claude强调验证挑战，Gemini关注偏见和可扩展性，GPT-4o突出实施障碍。", "conclusion": "该框架为研究人员提供了在实施前压力测试对齐建议的可复制方法，初步证明了AI进行VCW所提议的对话推理能力。但对话更多关注过程元素而非AI本质的基础主张，未来研究可探索人机混合协议和扩展对话研究。"}}
{"id": "2601.19927", "pdf": "https://arxiv.org/pdf/2601.19927", "abs": "https://arxiv.org/abs/2601.19927", "authors": ["Yuqing Zhao", "Ziyao Liu", "Yongsen Zheng", "Kwok-Yan Lam"], "title": "Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.", "AI": {"tldr": "本文系统综述了基于归因的技术在RAG系统中减少幻觉的应用，提出了幻觉类型分类法、统一技术流程，并提供了实用指南和比较分析。", "motivation": "LLM生成的回答存在幻觉问题，RAG框架虽然引入外部参考但带来了新的幻觉形式，需要系统化的归因技术来确保回答的可验证性。", "method": "通过构建RAG系统中幻觉类型的分类法，建立统一的归因技术流程，基于目标幻觉类型回顾相关技术，并进行优缺点比较分析。", "result": "提出了系统的RAG幻觉分类框架和统一的归因技术流程，为不同应用场景下的技术选择提供了实用指南。", "conclusion": "该研究为RAG系统中归因技术的未来研究和实际应用提供了重要见解，有助于针对不同类型的幻觉选择合适的技术解决方案。"}}
{"id": "2601.20614", "pdf": "https://arxiv.org/pdf/2601.20614", "abs": "https://arxiv.org/abs/2601.20614", "authors": ["Yanqi Dai", "Yuxiang Ji", "Xiao Zhang", "Yong Wang", "Xiangxiang Chu", "Zhiwu Lu"], "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for ICLR 2026", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "AI": {"tldr": "MathForge框架通过难度感知策略优化和多方面问题重构，系统性提升数学推理中难题的处理能力，显著超越现有方法", "motivation": "现有强化学习方法在数学推理中存在系统性缺陷：算法层面GRPO对难题的更新幅度不足，数据层面增强方法缺乏系统性难度提升", "method": "提出两阶段MathForge框架：1) DGPO算法通过难度平衡组优势估计修正GRPO的不平衡，并采用难度感知问题级加权；2) MQR策略从多方面重构问题以增加难度但保持原答案", "result": "大量实验表明MathForge在各种数学推理任务上显著优于现有方法", "conclusion": "MathForge形成了协同循环：MQR扩展数据边界，DGPO有效学习增强数据，为解决数学推理中的难题提供了有效框架"}}
{"id": "2601.19928", "pdf": "https://arxiv.org/pdf/2601.19928", "abs": "https://arxiv.org/abs/2601.19928", "authors": ["Yi Hu", "Jiaqi Gu", "Ruxin Wang", "Zijun Yao", "Hao Peng", "Xiaobao Wu", "Jianhui Chen", "Muhan Zhang", "Liangming Pan"], "title": "Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.", "AI": {"tldr": "对大型推理模型（LRMs）机制理解的全面综述，从训练动态、推理机制和意外行为三个维度分析强化学习驱动的推理模型内部工作机制", "motivation": "虽然大型推理模型的性能令人兴奋，但探索驱动这些行为的内部机制已成为同等重要的研究前沿，需要弥合黑盒性能与机制透明性之间的差距", "method": "综合组织近期研究发现，从三个核心维度进行系统综述：1）训练动态 2）推理机制 3）意外行为", "result": "提供了对LRMs机制理解的系统性框架，识别了关键研究发现和模式", "conclusion": "提出了未来机制研究的路线图，包括应用可解释性需求、改进方法论和统一理论框架等未充分探索的挑战"}}
{"id": "2601.20641", "pdf": "https://arxiv.org/pdf/2601.20641", "abs": "https://arxiv.org/abs/2601.20641", "authors": ["Boaz Carmeli", "Orr Paradise", "Shafi Goldwasser", "Yonatan Belinkov", "Ron Meir"], "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.", "AI": {"tldr": "研究显示基于LLM的智能体能够开发出比自然语言更高效且具有隐蔽性的任务导向通信协议，这些协议在协作推理任务中表现出色但存在透明度风险", "motivation": "探究LLM智能体是否能开发出不同于标准自然语言的任务导向通信协议，特别关注协议的高效性和隐蔽性特征", "method": "使用参考游戏框架，让视觉语言模型(VLM)智能体进行通信，在受控可测量的环境中评估语言变体", "result": "实验表明VLM能够开发出有效的任务适应通信模式，同时能创建难以被人类和外部智能体解读的隐蔽协议，还观察到相似模型间无需显式共享协议的自发协调", "conclusion": "研究结果突显了任务导向通信的潜力和风险，并将参考游戏定位为该领域未来研究的宝贵测试平台"}}
{"id": "2601.19929", "pdf": "https://arxiv.org/pdf/2601.19929", "abs": "https://arxiv.org/abs/2601.19929", "authors": ["David Linus Ostby"], "title": "Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "28 pages, 10 tables, 2 figures and 6 appendices", "summary": "We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.", "AI": {"tldr": "Stingy Context是一种基于层次树结构的压缩方案，在自动编码任务中实现18:1的上下文压缩比，将23.9万token的代码库压缩至1.1万token，同时保持任务准确性。", "motivation": "解决大型语言模型在代码任务中上下文窗口有限的问题，减少计算成本并缓解'迷失在中部'效应。", "method": "采用TREEFRAG分解方法构建层次树结构压缩方案，对源代码进行高效压缩。", "result": "在12个前沿模型上测试40个真实世界问题，达到94-97%的成功率，性能优于扁平压缩方法。", "conclusion": "Stingy Context提供了一种高效且低成本的上下文压缩解决方案，显著提升了LLM在代码任务中的实用性。"}}
{"id": "2601.20696", "pdf": "https://arxiv.org/pdf/2601.20696", "abs": "https://arxiv.org/abs/2601.20696", "authors": ["Samira Yazdanpourmoghadam", "Mahan Balal Pour", "Vahid Partovi Nia"], "title": "Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.", "AI": {"tldr": "该论文提出使用多类型Transformer(MTT)架构统一解决作业车间调度问题(JSP)和背包问题(KP)等组合优化问题，在标准基准测试中表现出竞争力，并首次在铁钛合金制造行业实现实际应用。", "motivation": "组合优化问题如JSP和KP在运筹学、物流和企业资源规划中至关重要，但传统启发式算法难以在实用时间限制内获得近似最优解。深度学习特别是Transformer架构为这些问题提供了新的解决途径。", "method": "采用多类型Transformer(MTT)架构，构建统一框架来处理JSP和KP等不同组合优化问题，通过多类型注意力机制捕捉问题中的复杂关系。", "result": "在JSP和KP的标准基准数据集上进行广泛实验评估，MTT在不同规模问题上都取得了有竞争力的性能表现，并在铁钛合金制造行业的实际应用中展示了潜力。", "conclusion": "多类型Transformer架构为组合优化问题提供了有效的统一解决方案，在理论和实际应用中都表现出良好性能，特别是在制造行业的实际应用方面具有开创性意义。"}}
{"id": "2601.19930", "pdf": "https://arxiv.org/pdf/2601.19930", "abs": "https://arxiv.org/abs/2601.19930", "authors": ["Jacob Nielsen", "Stine L. Beltoft", "Peter Schneider-Kamp", "Lukas Galke Poech"], "title": "SDUs DAISY: A Benchmark for Danish Culture", "categories": ["cs.CL", "cs.AI"], "comment": "Danish Culture Benchmark, 2 Tables, 1 Figure demonstrating the data curation pipeline", "summary": "We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.", "AI": {"tldr": "Daisy是一个基于丹麦文化经典的丹麦文化遗产基准数据集，包含741个经过人工审核的封闭式问答对，涵盖从公元前1300年到当代的丹麦文化主题。", "motivation": "创建一个系统性的基准来评估AI模型对丹麦文化遗产知识的理解，不仅包括主流信息，还涵盖定义丹麦文化传统的深度知识。", "method": "从丹麦文化经典2006中选取文物，查询对应的维基百科页面，使用语言模型生成随机问题，采用中心与边缘问题混合的采样策略，最后进行人工审核和修正。", "result": "构建了包含741个问答对的数据集，覆盖考古发现、诗歌、音乐作品、流行音乐、设计和建筑等多个文化领域，时间跨度从公元前1300年到现代。", "conclusion": "Daisy基准为评估AI模型对丹麦文化遗产的理解提供了标准化工具，有助于促进文化知识的保存和传播。"}}
{"id": "2601.20735", "pdf": "https://arxiv.org/pdf/2601.20735", "abs": "https://arxiv.org/abs/2601.20735", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Diéguez", "Susana Hahn", "Javier Romero", "Torsten Schaub"], "title": "Implementing Metric Temporal Answer Set Programming", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.", "AI": {"tldr": "提出一种基于差异约束的度量ASP计算方法，用于处理时间相关约束，解决时间粒度导致的ASP基础化瓶颈问题", "motivation": "需要表达定量时间约束（如持续时间和截止时间），但细粒度时间约束会显著加剧ASP的基础化瓶颈，影响可扩展性", "method": "利用ASP的差异约束扩展（一种简化的线性约束），将时间相关方面外部化处理，实现度量ASP与时间粒度的解耦", "result": "开发的计算方法能够有效处理时间约束，且不受时间精度影响，保持了系统的可扩展性", "conclusion": "通过差异约束外部化处理时间方面，成功解决了度量ASP中的时间粒度问题，为处理定量时间约束提供了可扩展的解决方案"}}
{"id": "2601.19931", "pdf": "https://arxiv.org/pdf/2601.19931", "abs": "https://arxiv.org/abs/2601.19931", "authors": ["Sebastien Kawada", "Dylan Holyoak"], "title": "CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity", "categories": ["cs.CL"], "comment": "6 pages (including references), 2 figures, 2 tables. System description paper for SemEval-2026 Task 4 (Narrative Story Similarity)", "summary": "We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.", "AI": {"tldr": "混合神经符号系统结合神经自一致性投票和多尺度叙事分析集成，通过级联架构处理叙事故事相似性任务，在不确定情况下使用符号方法作为决策器，开发集准确率达到81%", "motivation": "解决叙事故事相似性任务中的模糊比较问题，通过结合神经和符号方法的优势来提高预测准确性", "method": "使用大型语言模型进行神经自一致性投票，设置超多数阈值；当投票出现平局时，采用包含五种叙事相似性信号（词汇重叠、语义嵌入、故事语法结构、事件链对齐和叙事张力曲线）的符号集成作为最终决策", "result": "在开发集上达到81%的准确率，证明选择性使用符号方法可以增强神经预测在模糊叙事比较中的表现", "conclusion": "混合神经符号架构通过选择性延迟到符号方法，能够有效处理神经网络在真正模糊叙事比较中的不确定性，提高整体性能"}}
{"id": "2601.20784", "pdf": "https://arxiv.org/pdf/2601.20784", "abs": "https://arxiv.org/abs/2601.20784", "authors": ["Zishen Wan", "Che-Kai Liu", "Jiayi Qian", "Hanchen Yang", "Arijit Raychowdhury", "Tushar Krishna"], "title": "REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence", "categories": ["cs.AI", "cs.AR"], "comment": "16 pages, 13 figures, 5 tables, 2026 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "summary": "Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.\n  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.", "AI": {"tldr": "REASON是一个针对神经符号AI中概率逻辑推理的集成加速框架，通过统一的DAG表示、自适应剪枝和树形处理架构，在GPU上实现了12-50倍的速度提升和310-681倍的能效提升。", "motivation": "神经符号AI系统虽然结合了神经感知和符号推理的优势，但在部署时面临符号和概率推理效率低下的挑战，特别是在概率逻辑推理方面存在控制流不规则、算术强度低等问题。", "method": "提出REASON框架，包括：1）统一的DAG表示方法；2）自适应剪枝和正则化技术；3）可重构的树形处理架构；4）与GPU流多处理器的紧密集成和可编程接口。", "result": "在6个神经符号工作负载上评估，相比桌面和边缘GPU，REASON实现了12-50倍的速度提升和310-681倍的能效提升，在28nm工艺下完成端到端任务仅需0.8秒，面积6mm²，功耗2.12W。", "conclusion": "REASON证明了针对概率逻辑推理的专门加速对于实现实用和可扩展的神经符号AI至关重要，为下一代认知智能系统提供了基础架构。"}}
{"id": "2601.19932", "pdf": "https://arxiv.org/pdf/2601.19932", "abs": "https://arxiv.org/abs/2601.19932", "authors": ["Ruyuan Wan", "Changye Li", "Ting-Hao 'Kenneth' Huang"], "title": "\"Newspaper Eat\" Means \"Not Tasty\": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.", "AI": {"tldr": "本文介绍了CodedLang数据集，包含7,744条中文谷歌地图评论，其中900条带有编码语言的标注，提出了七类编码策略分类法，并展示了当前语言模型在处理编码语言方面的局限性。", "motivation": "编码语言是人类交流的重要组成部分，但当前语言模型处理能力较差，缺乏真实数据集和清晰分类法限制了研究进展。", "method": "构建包含7,744条中文谷歌地图评论的数据集，其中900条有编码语言的span级标注，开发了七类编码策略分类法（包括语音、拼写和跨语言替换），对语言模型进行编码语言检测、分类和评分预测的基准测试。", "result": "结果显示即使是强大的语言模型也难以识别和理解编码语言，许多编码表达依赖于基于发音的策略。", "conclusion": "编码语言是现实世界NLP系统中一个重要但未被充分探索的挑战，需要更多研究关注。"}}
{"id": "2601.20831", "pdf": "https://arxiv.org/pdf/2601.20831", "abs": "https://arxiv.org/abs/2601.20831", "authors": ["Vishnu Sashank Dorbala", "Dinesh Manocha"], "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.", "AI": {"tldr": "MemCtrl是一个新颖的框架，使用多模态大语言模型进行在线记忆剪枝，通过可训练的记忆头μ来动态管理记忆，在具身智能任务中实现了约16%的性能提升", "motivation": "现有记忆检索系统通常将记忆视为大型离线存储空间，这不适合需要在严格内存和计算约束下在线操作的具身智能体", "method": "提出MemCtrl框架，为MLLMs添加可训练的记忆头μ作为门控机制，决定在探索过程中保留、更新或丢弃哪些观察或反思。评估了两种μ训练方式：通过离线专家和在线强化学习", "result": "在EmbodiedBench基准测试的多个子集上，μ增强的MLLMs平均提升约16%，在特定指令子集上提升超过20%。长且复杂的指令类型表现尤为突出", "conclusion": "MemCtrl框架通过在线记忆管理显著提升了具身智能体的任务完成能力，证明了动态记忆剪枝在资源受限环境中的有效性"}}
{"id": "2601.19933", "pdf": "https://arxiv.org/pdf/2601.19933", "abs": "https://arxiv.org/abs/2601.19933", "authors": ["Kei Saito"], "title": "Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 3 figures, 5 tables. Sequel to arXiv:2512.13478", "summary": "Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.", "AI": {"tldr": "本文提出了非消解推理框架中的文本到状态映射函数φ，将自然语言输入转换为叠加状态，保持语义模糊性而非强制单一解释。通过矛盾保持原则和LLM作为解释生成器，在68个测试句子上验证了该方法能有效保持模糊输入的熵值。", "motivation": "解决自然语言如何映射到非消解推理数学结构的关键问题，建立原始文本与形式状态空间之间的算法桥梁，实现语言模型推理中的架构延迟崩溃。", "method": "引入文本到状态映射函数φ，形式化矛盾保持原则，使用大型语言模型作为解释生成器开发提取协议，通过香农熵量化状态表示的模糊性。", "result": "在涵盖词汇、结构和语用模糊的68个测试句子中，模糊输入的平均香农熵H(S)=1.087比特，而基线单解释方法H(S)=0.000。", "conclusion": "该框架成功建立了文本与NRR操作符作用的形式状态空间之间的算法连接，实现了语言处理中语义模糊性的保持和解释延迟。"}}
{"id": "2601.20843", "pdf": "https://arxiv.org/pdf/2601.20843", "abs": "https://arxiv.org/abs/2601.20843", "authors": ["Saurav Prateek"], "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)", "categories": ["cs.AI"], "comment": "11 pages, 6 figures, 2 tables, source code: https://github.com/SauravP97/deep-researcher-reflect-evolve/", "summary": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.", "AI": {"tldr": "本文提出了一种新颖的Deep Researcher架构，通过顺序研究计划精化和候选交叉算法，在PhD级别复杂研究主题上生成详细研究报告，在DeepResearch Bench基准测试中取得46.21分的优异表现，超越了多个领先研究助手。", "motivation": "解决并行扩展范式在知识孤岛和搜索效率方面的固有局限性，需要一种能够动态适应研究过程的新方法。", "method": "采用顺序研究计划精化（通过反思维护全局研究上下文）和候选交叉算法（使用多个LLM候选探索更大搜索空间），最后进行一次性报告生成。", "result": "在包含100个博士级研究任务的DeepResearch Bench基准测试中获得46.21分的总体评分，超越了Claude Researcher、Nvidia AIQ等多个领先研究助手，且表现优于之前的Static DRA工作。", "conclusion": "顺序扩展范式在性能上持续优于并行自一致性范式，证明了动态适应研究过程的方法在复杂研究任务中的有效性。"}}
{"id": "2601.19934", "pdf": "https://arxiv.org/pdf/2601.19934", "abs": "https://arxiv.org/abs/2601.19934", "authors": ["Claire Nicholson"], "title": "Quantifying non deterministic drift in large language models", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures, 1 table. Empirical measurement study reporting new repeated-run experiments quantifying baseline nondeterministic drift in large language models. This manuscript presents original empirical results (not a review or position paper) and establishes a baseline reference for future drift-mitigation work", "summary": "Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.", "AI": {"tldr": "该研究通过重复实验量化了大语言模型在相同提示下的输出变异性（行为漂移），发现即使在温度设为0.0时也存在非确定性，不同模型和提示类型表现出不同的变异性模式。", "motivation": "大语言模型在实际应用中，即使温度和参数固定，相同提示也不总是产生相同输出，需要系统评估这种基础行为漂移现象。", "method": "使用gpt-4o-mini和llama3.1-8b两个模型，在温度0.0和0.7下进行五类提示的重复实验，包括精确重复、扰动输入和重用模式，通过唯一输出比例、词汇相似性和词数统计测量漂移。", "result": "研究显示非确定性在温度0.0时仍然存在，模型大小、部署方式和提示类型都影响变异性模式，词汇度量方法存在局限性。", "conclusion": "本研究在没有稳定技术的情况下建立了系统实证基线，为未来漂移缓解和控制方法的评估提供了参考点，并强调了语义方法的重要性。"}}
{"id": "2601.20856", "pdf": "https://arxiv.org/pdf/2601.20856", "abs": "https://arxiv.org/abs/2601.20856", "authors": ["Sebastiano Monti", "Carlo Nicolini", "Gianni Pellegrini", "Jacopo Staiano", "Bruno Lepri"], "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.", "AI": {"tldr": "该论文系统评估了大语言模型的长时程规划能力，通过Sokoban拼图基准测试发现模型在需要超过25步的规划任务中性能显著下降，表明存在固有的前向规划能力限制。", "motivation": "虽然大语言模型在复杂推理任务上表现出色，但其长时程规划能力尚未得到充分研究，需要系统评估当前最先进大语言模型的规划和长时程推理能力。", "method": "提出基于Sokoban拼图的新基准测试，故意简化以隔离长时程规划与状态持久性问题，并测试模型配备PDDL解析、验证和求解工具的效果。", "result": "发现规划性能在需要超过25步解决方案时出现一致性的性能下降，配备PDDL工具只能带来适度改进。", "conclusion": "大语言模型存在固有的架构限制，仅通过测试时扩展方法可能无法克服其前向规划能力的根本约束。"}}
{"id": "2601.19935", "pdf": "https://arxiv.org/pdf/2601.19935", "abs": "https://arxiv.org/abs/2601.19935", "authors": ["Yiting Shen", "Kun Li", "Wei Zhou", "Songlin Hu"], "title": "Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \\textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.", "AI": {"tldr": "提出了Mem2ActBench基准测试，用于评估LLM智能体是否能够主动利用长期记忆来执行基于工具的任务，包括工具选择和参数基础化。", "motivation": "现有基准测试主要评估智能体被动检索孤立事实的能力，但无法评估主动应用记忆执行任务的关键能力。", "method": "通过自动化流水线整合异构数据源（ToolACE、BFCL、Oasst1），通过一致性建模解决冲突，合成2,029个会话，并使用反向生成方法产生400个工具使用任务。", "result": "人类评估确认91.3%的任务具有强记忆依赖性，对7个记忆框架的实验显示当前系统在主动利用记忆进行参数基础化方面仍然不足。", "conclusion": "当前系统在主动应用记忆执行任务方面存在不足，需要更有效的方法来评估和改进记忆在任务执行中的应用。"}}
{"id": "2601.19945", "pdf": "https://arxiv.org/pdf/2601.19945", "abs": "https://arxiv.org/abs/2601.19945", "authors": ["Thomas Schuster", "Julius Trögele", "Nico Döring", "Robin Krüger", "Matthieu Hoffmann", "Holger Friedrich"], "title": "Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen", "categories": ["cs.CL", "cs.AI"], "comment": "Language: German; English Title: Benchmarking ASR Models in German Medical Contexts: A Performance Analysis Using Anamnesis Conversations", "summary": "Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.", "AI": {"tldr": "本文评估了29个德语医疗ASR模型在包含方言的医患对话数据集上的表现，发现最佳模型词错误率低于3%，但医疗术语和方言处理仍存在显著差异", "motivation": "虽然英语ASR已有众多基准测试，但德语医疗场景特别是包含方言的评估仍然缺乏，需要专门的研究来填补这一空白", "method": "使用精心策划的模拟医患对话数据集，评估29个不同ASR模型（包括Whisper、Voxtral、Wav2Vec2等开源模型和商业API），采用WER、CER、BLEU三种指标并进行语义分析", "result": "模型间性能差异显著：最佳系统词错误率(WER)部分低于3%，但其他模型在医疗术语和方言变体上的错误率明显更高", "conclusion": "德语医疗ASR在标准语音上已表现良好，但在医疗专业术语和方言处理方面仍需改进，需要针对性的优化和评估"}}
{"id": "2601.20006", "pdf": "https://arxiv.org/pdf/2601.20006", "abs": "https://arxiv.org/abs/2601.20006", "authors": ["Michał Gromadzki", "Anna Wróblewska", "Agnieszka Kaliska"], "title": "On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 6 figures. Under review at Information Sciences", "summary": "The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\\%$ token-level accuracy, substantially outperforming existing open-source baselines.", "AI": {"tldr": "本研究基于大规模语料库和新型训练策略，对AI生成文本检测进行了全面研究，开发了检测模型并提出了两种新颖的训练范式，在包含21个大语言模型的基准测试中取得了99.6%的词元级准确率。", "motivation": "大型语言模型的快速发展使得生成文本与人类写作极为相似，给教育、出版和数字安全领域的真实性验证带来了挑战，因此检测AI生成文本成为关键的技术和伦理问题。", "method": "构建了10亿词元的人类写作文本语料库和19亿词元的AI生成文本语料库，开发了多种检测模型，并提出了两种新颖的训练范式：按LLM微调和按LLM家族微调。", "result": "在涵盖21个大语言模型的1亿词元基准测试中，最佳微调检测器达到了99.6%的词元级准确率，显著优于现有的开源基线方法。", "conclusion": "研究表明，基于大规模多样化语料库和针对性训练策略的检测方法能够有效识别AI生成文本，为解决数字内容真实性验证问题提供了有力技术支撑。"}}
{"id": "2601.20009", "pdf": "https://arxiv.org/pdf/2601.20009", "abs": "https://arxiv.org/abs/2601.20009", "authors": ["J. Ben Tamo", "Daniel Carlander-Reuterfelt", "Jonathan Rubin", "Dezhi Hong", "Mingxian Wang", "Oleg Poliannikov"], "title": "LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.", "AI": {"tldr": "该论文通过分析多语言大模型的语言控制问题，提出了选择性微调方法，仅微调负责语言控制的最后几层，在保持任务准确性的同时显著提高语言一致性，且只使用3-5%的参数。", "motivation": "尽管进行了多语言预训练，大语言模型在非英语任务中仍存在语言控制问题，包括多语言迁移瓶颈（正确语言但错误任务响应）和语言一致性瓶颈（正确任务响应但错误语言）。", "method": "设计四场景评估协议，扩展logit lens分析跟踪语言概率层变化，计算隐藏状态的跨语言语义相似度。基于发现的三阶段内部结构，选择性微调仅负责语言控制的最后几层。", "result": "在Qwen-3-32B和Bloom-7.1B模型上，该方法在六种语言上达到98%以上的语言一致性，仅微调3-5%参数，且任务准确性不下降，效果与全范围微调相当但计算资源大幅减少。", "conclusion": "通过定位语言控制层进行选择性微调是高效多语言适应的有效方法，首次利用语言控制的层定位实现资源高效的模型适配。"}}
{"id": "2601.20026", "pdf": "https://arxiv.org/pdf/2601.20026", "abs": "https://arxiv.org/abs/2601.20026", "authors": ["Pragatheeswaran Vipulanandan", "Kamal Premaratne", "Dilip Sarkar"], "title": "Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.", "AI": {"tldr": "提出基于量子张量网络的不确定性量化框架，用于检测大语言模型的幻觉问题，通过语义等价聚类和熵最大化策略提高输出可靠性", "motivation": "大语言模型虽具备强大生成能力，但存在幻觉问题，输出不可靠且具有任意变异性，需要可解释的幻觉检测方案", "method": "使用量子张量网络管道构建不确定性量化框架，考虑标记序列概率的偶然不确定性，进行基于语义等价的生成聚类，并引入熵最大化策略", "result": "在TriviaQA、NQ等数据集上的116个实验显示，相比现有基线在AUROC和AURAC指标上持续改进，在不同生成长度和量化级别下保持鲁棒性", "conclusion": "该方法为幻觉检测提供了原则性和可解释的方案，能在资源受限部署中保持可靠性，为需要人工监督的场景提供实用指导"}}
{"id": "2601.20032", "pdf": "https://arxiv.org/pdf/2601.20032", "abs": "https://arxiv.org/abs/2601.20032", "authors": ["Nishanth Sridhar Nakshatri", "Eylon Caplan", "Rajkumar Pujari", "Dan Goldwasser"], "title": "TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference", "categories": ["cs.CL"], "comment": null, "summary": "Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.", "AI": {"tldr": "提出了TAIGR框架，通过三阶段结构化分析来验证健康影响者内容的实用意义，而非仅验证事实声明。", "motivation": "健康影响者的内容通常通过对话叙述和修辞策略传达，而非明确的事实声明，导致传统声明验证方法难以捕捉其语用含义。", "method": "TAIGR框架包含三个阶段：1)识别核心推荐要点；2)构建论证图谱捕捉论证结构；3)使用因子图概率推理验证要点。", "result": "在健康影响者视频转录本的验证任务中，TAIGR证明需要建模话语的语用和论证结构，而非将转录本视为扁平声明集合。", "conclusion": "准确验证影响者内容需要分析其语用和论证结构，TAIGR框架为此提供了有效的结构化方法。"}}
{"id": "2601.20055", "pdf": "https://arxiv.org/pdf/2601.20055", "abs": "https://arxiv.org/abs/2601.20055", "authors": ["Vikash Singh", "Darion Cassel", "Nathaniel Weir", "Nick Feng", "Sam Bayless"], "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.", "AI": {"tldr": "VERGE是一个神经符号框架，将大语言模型与SMT求解器结合，通过迭代精炼产生验证引导的答案，在推理基准测试中比单次方法提升18.7%的性能。", "motivation": "尽管大语言模型具有语法流畅性，但在高风险领域中确保其逻辑正确性仍然是一个基本挑战。", "method": "将LLM输出分解为原子声明，自动形式化为一阶逻辑，使用自动定理证明验证逻辑一致性；采用多模型共识、语义路由和最小校正子集定位错误；迭代精炼答案直到满足接受标准。", "result": "使用GPT-OSS-120B模型，VERGE在一组推理基准测试中比单次方法平均性能提升18.7%。", "conclusion": "这种混合方法在可能的情况下提供形式保证，在其他情况下提供共识验证，推动了可信AI的发展。"}}
{"id": "2601.20102", "pdf": "https://arxiv.org/pdf/2601.20102", "abs": "https://arxiv.org/abs/2601.20102", "authors": ["Amirhossein Haji Mohammad Rezaei", "Zahra Shakeri"], "title": "Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects", "categories": ["cs.CL"], "comment": null, "summary": "Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.", "AI": {"tldr": "该研究创建了一个反事实基准测试，通过插入文化相关标识符和上下文线索来评估医疗语言模型的文化敏感性，发现文化线索会显著影响诊断准确性，特别是当标识符和上下文同时出现时，导致诊断错误率显著上升。", "motivation": "工程化可持续和公平的医疗保健需要医疗语言模型在面对非决定性文化信息时不改变临床正确的诊断。", "method": "引入一个反事实基准，将150个MedQA测试项目扩展为1650个变体，插入文化相关的标识符标记、上下文线索或其组合（针对三个文化群体），并包含长度匹配的中性对照组，由临床医生验证所有变体中正确答案保持不变。评估了多个模型在不同提示策略下的表现。", "result": "文化线索显著影响模型准确性（Cochran's Q, p<10^-14），当标识符和上下文同时出现时准确性下降最大（在仅选项提示下下降3-7个百分点）。超过一半的文化基础解释最终导致错误答案。", "conclusion": "文化参照推理与诊断失败相关，研究发布提示和增强数据以支持评估和缓解文化引起的诊断错误。"}}
{"id": "2601.20105", "pdf": "https://arxiv.org/pdf/2601.20105", "abs": "https://arxiv.org/abs/2601.20105", "authors": ["Faezeh Hosseini", "Mohammadali Yousefzadeh", "Yadollah Yaghoobzadeh"], "title": "FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language", "categories": ["cs.CL"], "comment": "EACL 2026", "summary": "Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.", "AI": {"tldr": "论文提出了FFEHallu基准测试，用于评估大语言模型在固定比喻表达（FFEs）上的幻觉问题，发现在波斯语等语言中，模型在区分真实与虚构比喻表达及跨语言翻译时存在系统性弱点。", "motivation": "固定比喻表达（如习语和谚语）由于文化背景深厚、非组合性和固定性，容易导致大语言模型产生比喻幻觉（生成或认可看似合理但实际不存在的表达），需要专门基准来评估这一问题。", "method": "构建FFEHallu基准，包含600个精心筛选的实例，涵盖三个任务：从含义生成FFE、检测四个控制构建类别中的虚构FFE、以及从英语到波斯语的FFE翻译，并评估了六个先进的多语言大语言模型。", "result": "GPT4.1在拒绝虚构FFE和检索真实FFE方面表现相对较好，但大多数模型难以可靠区分真实与高质量虚构表达，在跨语言翻译中经常产生幻觉，显示出在比喻能力和文化基础方面的系统性弱点。", "conclusion": "当前大语言模型在处理比喻语言方面存在显著差距，需要有针对性的基准来评估和减轻比喻幻觉问题，特别是在 underrepresented 语言如波斯语中。"}}
{"id": "2601.20126", "pdf": "https://arxiv.org/pdf/2601.20126", "abs": "https://arxiv.org/abs/2601.20126", "authors": ["Abha Jha", "Akanksha Mahajan", "Ashwath Vaithinathan Aravindan", "Praveen Saravanan", "Sai Sailaja Policharla", "Sonal Chaturbhuj Gehlot"], "title": "Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention (\"I don't know\") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.", "AI": {"tldr": "该研究提出RLVR训练范式，通过可验证奖励机制鼓励LLMs在不确定时选择弃权，以减少幻觉内容。在多项选择任务中，适度的弃权奖励能有效减少错误回答且不明显降低准确率。", "motivation": "大型语言模型经常产生幻觉或不可验证的内容，这削弱了其在事实性领域的可靠性，需要一种能促进知识谦逊的训练方法。", "method": "使用三元奖励结构(-1, r_abs, 1)对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct在MedMCQA和Hendrycks Math基准上进行微调，并研究RLVR与监督微调策略的结合效果。", "result": "适度的弃权奖励(r_abs ≈ -0.25到0.3)能持续减少错误回答且不严重降低准确率，较大模型对弃权激励表现出更强鲁棒性。在开放性问题中探索不足存在限制。", "conclusion": "可验证奖励设计是缓解语言模型幻觉的可行且灵活的实际方法，监督弃权训练可以部分缓解探索不足的问题。"}}
{"id": "2601.20129", "pdf": "https://arxiv.org/pdf/2601.20129", "abs": "https://arxiv.org/abs/2601.20129", "authors": ["Akif Islam", "Sujan Kumar Roy", "Md. Ekramul Hamid"], "title": "BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset paper. 6 pages, 3 figures. 4 Tables, Includes a publicly released Bengali sentiment dataset on Kaggle (BengaliSent140) and baseline experimental results", "summary": "Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/", "AI": {"tldr": "本文介绍了BengaliSent140，一个大规模孟加拉语二元情感数据集，通过整合7个现有数据集构建而成，包含139,792个文本样本，为深度学习模型训练提供更丰富的语言和上下文覆盖。", "motivation": "孟加拉语情感分析研究受限于大规模多样化标注数据集的稀缺性，现有数据集规模小或局限于单一领域，无法满足现代深度学习模型对大量异质数据的需求。", "method": "通过系统整合7个现有孟加拉语文本数据集，将异质标注方案统一为二元情感分类（非仇恨0和仇恨1），构建统一语料库。", "result": "构建了包含139,792个独特文本样本的数据集（68,548个仇恨实例和71,244个非仇恨实例），类别分布相对平衡，提供了基线实验结果证明其实用性。", "conclusion": "BengaliSent140数据集通过多源多领域数据整合，提供了比现有孟加拉语情感数据集更广泛的语言和上下文覆盖，为深度学习模型的训练和基准测试提供了坚实基础。"}}
{"id": "2601.20142", "pdf": "https://arxiv.org/pdf/2601.20142", "abs": "https://arxiv.org/abs/2601.20142", "authors": ["Zilai Wang", "Natarajan Balaji Shankar", "Kaiyuan Zhang", "Zihan Wang", "Abeer Alwan"], "title": "Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "ICASSP 2026", "summary": "Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.", "AI": {"tldr": "该论文提出使用delta SSL嵌入（微调后模型与预训练模型嵌入之间的差异）来提升儿童语音识别性能，通过融合不同SSL模型的delta嵌入，在MyST儿童语料库上取得了显著的词错误率降低效果。", "motivation": "儿童自动语音识别(ASR)面临数据有限和预训练领域不匹配的挑战，微调SSL模型会导致表示空间偏移，需要寻找更好的特征融合方法来提升性能。", "method": "提出delta SSL嵌入概念，定义为微调后模型嵌入与预训练模型嵌入的差异，评估了多种融合策略，在MyST儿童语料库上使用不同SSL模型进行实验。", "result": "delta嵌入融合使HuBERT相对WER降低10%，W2V2降低4.4%。WavLM与delta W2V2嵌入融合达到9.64% WER，在MyST语料库上创造了SSL模型的新最佳性能。", "conclusion": "delta嵌入方法有效，特征融合是推进儿童ASR发展的有前景方向，证明了delta嵌入编码了任务特定信息并能补充微调特征。"}}
{"id": "2601.20144", "pdf": "https://arxiv.org/pdf/2601.20144", "abs": "https://arxiv.org/abs/2601.20144", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Jiri Gesi", "Xianfeng Tang", "Chen Luo", "Yisi Sang", "Hanqing Lu", "Manling Li", "Dakuo Wang"], "title": "Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents", "categories": ["cs.CL"], "comment": null, "summary": "Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.", "AI": {"tldr": "本文提出了Trajectory2Task数据生成管道，用于在三种真实用户场景下大规模研究工具调用：意图模糊、意图变化和意图不可行。通过多轮探索生成可验证的工具调用轨迹，并转化为用户任务进行基准测试和模型微调。", "motivation": "现实世界中工具调用代理面临用户请求模糊、动态变化和策略约束等复杂情况，但现有研究主要关注理想化设置，缺乏覆盖这些复杂交互模式的训练和评估数据。", "method": "开发Trajectory2Task管道：1）多轮探索生成有效工具调用轨迹；2）将轨迹转化为带控制意图调整的用户任务；3）支持闭环评估和训练的可验证任务生成。", "result": "在生成的复杂用户场景任务上测试7个最先进LLM，发现频繁失败。使用成功轨迹微调轻量级LLM，在所有三种条件下都获得一致改进，并展现出更好的未见工具使用领域泛化能力。", "conclusion": "Trajectory2Task管道能有效生成现实世界工具调用场景数据，微调后的模型表现出更强的通用工具调用能力，为解决现实应用中复杂用户交互问题提供了有效方法。"}}
{"id": "2601.20162", "pdf": "https://arxiv.org/pdf/2601.20162", "abs": "https://arxiv.org/abs/2601.20162", "authors": ["Shuoxin Wang", "Chang Liu", "Gowen Loo", "Lifan Zheng", "Kaiwen Wei", "Xinyi Zeng", "Jingyuan Zhang", "Yu Tian"], "title": "Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.", "AI": {"tldr": "Me-Agent是一个可学习、可记忆的个性化移动代理，通过两级用户习惯学习方法和分层偏好记忆系统来解决LLM代理缺乏个性化能力的问题，在个性化基准测试中达到最先进性能。", "motivation": "现有基于LLM的移动代理虽然性能显著提升，但往往只遵循明确用户指令而忽略个性化需求，存在三个主要限制：(1)无法解释模糊指令；(2)缺乏从用户交互历史中学习；(3)无法处理个性化指令。", "method": "提出Me-Agent代理，采用两级用户习惯学习方法：在提示级别设计用户偏好学习策略，通过个人奖励模型增强个性化性能；在记忆级别设计分层偏好记忆，将用户长期记忆和应用特定记忆存储在不同层级内存中。", "result": "在User FingerTip基准测试和通用基准测试上的广泛实验表明，Me-Agent在个性化方面达到了最先进的性能，同时保持了具有竞争力的指令执行性能。", "conclusion": "Me-Agent通过创新的个性化学习架构有效解决了移动代理的个性化限制问题，为用户提供了更好的个性化体验，同时保持了原有的指令执行能力。"}}
{"id": "2601.20185", "pdf": "https://arxiv.org/pdf/2601.20185", "abs": "https://arxiv.org/abs/2601.20185", "authors": ["Husein Zolkepli"], "title": "Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \\href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.", "AI": {"tldr": "通过引入额外池化和增加解码器跳跃大小，将X-Codec-2.0的潜在速率从50Hz降至25Hz，同时将输出采样率从16kHz提升至24kHz，显著提高了音频压缩的效率和感知质量", "motivation": "X-Codec-2.0在50Hz潜在速率和16kHz采样率下表现良好，但限制了时间效率和音频保真度，需要改进配置以提升性能", "method": "采用简单的架构修改：引入额外池化操作和增加解码器跳跃大小，保持核心架构不变但优化参数配置", "result": "在Common Voice 17多语言测试集上，基于UTMOSv2评估显示MOS得分比原始X-Codec-2.0基线提高了0.29，在所有25Hz编解码器中达到最佳性能", "conclusion": "该简单有效的修改显著提升了音频压缩效率和感知质量，为神经音频压缩技术提供了新的优化方向，相关代码和模型已开源"}}
{"id": "2601.20230", "pdf": "https://arxiv.org/pdf/2601.20230", "abs": "https://arxiv.org/abs/2601.20230", "authors": ["Haoyuan Yu", "Yuxuan Chen", "Minjie Cai"], "title": "Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems", "categories": ["cs.CL", "cs.HC"], "comment": "ICASSP 2026 (Workshop). https://github.com/yu-haoyuan/fd-badcat", "summary": "Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.", "AI": {"tldr": "提出基于多模态大语言模型的半级联全双工对话框架，将复杂对话分解为最小对话单元，实现无需训练、即插即用的全双工语音交互系统，在HumDial数据集上验证有效，在Human-like Spoken Dialogue Systems Challenge中排名第二。", "motivation": "全双工语音交互对于自然的人机交互至关重要，需要解决复杂对话的处理和状态转换问题。", "method": "设计半级联全双工对话系统框架，围绕多模态大语言模型构建，使用语音活动检测(VAD)和文本转语音(TTS)等辅助模块，将对话分解为最小对话单元独立处理并预测状态转换。", "result": "在HumDial数据集上的实验证明了框架的有效性，在Human-like Spoken Dialogue Systems Challenge测试集（赛道2：全双工交互）中排名第二。", "conclusion": "该框架实现了无需训练、即插即用的全双工对话系统，通过分解对话单元和预测状态转换的方法有效提升了语音交互的自然度和实时性。"}}
{"id": "2601.20253", "pdf": "https://arxiv.org/pdf/2601.20253", "abs": "https://arxiv.org/abs/2601.20253", "authors": ["Si Chen", "Le Huy Khiem", "Annalisa Szymanski", "Ronald Metoyer", "Ting Hua", "Nitesh V. Chawla"], "title": "Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.", "AI": {"tldr": "提出了一个基于专家指南自动生成评估基准的框架，将专业实践转化为多选题和对话，用于评估LLM在实践领域中的情境化推理能力，发现LLM在高阶推理表现较好但在基础记忆方面存在不足。", "motivation": "现有LLM基准测试主要依赖现有人类考试数据集，但在实践性领域中这类数据往往不可得，需要开发能够评估程序性知识和专业判断能力的自动化评估方法。", "method": "开发了从专家编写的基于布鲁姆分类学的指南自动生成基准的框架，将专家实践转化为隐含违规场景，并扩展到自动评分的多选题和多轮对话，覆盖四个认知层次。", "result": "在三个应用领域（教学、营养学和护理）测试发现，LLM在高阶分析推理上表现相对较好，但在基础记忆项目上失败更频繁，揭示了模型与人类推理的差异。", "conclusion": "该框架能够大规模、心理测量学意义上生成基准，揭示LLM的非直观行为模式，支持在真实世界环境中评估情境化推理能力，为实践领域的AI评估提供了可扩展的解决方案。"}}
{"id": "2601.20256", "pdf": "https://arxiv.org/pdf/2601.20256", "abs": "https://arxiv.org/abs/2601.20256", "authors": ["Xuanyu Su", "Diana Inkpen", "Nathalie Japkowicz"], "title": "SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility", "categories": ["cs.CL"], "comment": null, "summary": "Online hate on social media ranges from overt slurs and threats (\\emph{hard hate speech}) to \\emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \\textbf{\\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \\emph{Argumentum Model of Topics} (AMT) and \\emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \\textbf{7} sociocultural domains and \\textbf{28} target groups, comprising \\textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \\textcolor{red}{\\textbf{Disclaimer.} Contains offensive examples used solely for research.}", "AI": {"tldr": "论文提出了SoftHateBench基准测试，用于评估内容审核系统对软性仇恨言论的检测能力，发现现有系统在处理表面合理但内含敌意的推理式仇恨言论时存在明显漏洞。", "motivation": "当前社交媒体内容审核系统主要针对表面毒性的显性仇恨言论进行优化，但无法有效检测看似合理但通过推理框架引导受众排斥目标群体的软性仇恨言论，现有基准测试未能系统性地衡量这一差距。", "method": "整合Argumentum Model of Topics (AMT) 和 Relevance Theory (RT) 构建统一框架：AMT提供重写显性仇恨言论为表面中性但保持立场的基础论证结构，RT确保AMT链的逻辑连贯性，生成软性仇恨变体。", "result": "构建了涵盖7个社会文化领域和28个目标群体的4,745个软性仇恨实例。评估显示，从硬性到软性层级，基于编码器的检测器、通用LLM和安全模型的表现一致下降，系统在检测推理式微妙语言表达的相同立场时经常失败。", "conclusion": "当前内容审核系统对软性仇恨言论的检测存在系统性漏洞，需要开发能够识别推理驱动型敌意的新方法，SoftHateBench为评估和改进这类系统提供了重要基准。"}}
{"id": "2601.20275", "pdf": "https://arxiv.org/pdf/2601.20275", "abs": "https://arxiv.org/abs/2601.20275", "authors": ["Elina Sigdel", "Anastasia Panfilova"], "title": "RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis", "categories": ["cs.CL"], "comment": "The link to the platform: https://ruslica.ipran.ru", "summary": "Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.", "AI": {"tldr": "该研究开发了针对俄语的心理语言学文本分析工具RusLICA，基于LIWC方法但专门为俄语的语言和文化特点定制，包含96个分析类别，整合了语法、形态、词汇特征和预训练语言模型的预测结果。", "motivation": "现有的LIWC工具主要针对英语设计，直接翻译到俄语无法充分考虑俄语的语法和文化特异性，需要专门为俄语开发定制化的心理语言学分析工具。", "method": "基于多个词典资源和语料库专门构建俄语词典，将词元映射到42个心理语言学类别，整合句法、形态、词汇特征和预训练语言模型的预测结果，开发了包含96个类别的分析系统。", "result": "成功开发了RusLICA网络服务分析器，能够对俄语文本进行专门的心理语言学特征分析，克服了直接翻译英语词典的局限性。", "conclusion": "为俄语定制的心理语言学分析工具比直接翻译的版本更有效，该方法可为其他语言开发类似工具提供参考，强调了考虑语言特异性在心理语言学分析中的重要性。"}}
{"id": "2601.20276", "pdf": "https://arxiv.org/pdf/2601.20276", "abs": "https://arxiv.org/abs/2601.20276", "authors": ["Tianwei Lin", "Zuyi Zhou", "Xinda Zhao", "Chenke Wang", "Xiaohong Li", "Yu Chen", "Chuanrui Hu", "Jian Pei", "Yafeng Deng"], "title": "Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.", "AI": {"tldr": "EverMemBench-S (EMB-S)是一个对抗性的长上下文评估基准，使用326M token的记忆库，通过语义干扰测试来揭示现有模型在真实长上下文场景中的证据访问瓶颈。", "motivation": "现有的NIAH评估主要测试良性跨度定位，但真实环境中存在大量语义干扰和近似干扰项，需要更全面的评估基准来测试模型在对抗性环境中的证据访问能力。", "method": "构建326M token的MemoryBank，设计包含碰撞测试的近似干扰项和黄金证据集的查询，采用解耦诊断协议分别评估证据访问（文档ID定位）和端到端QA质量。", "result": "在从64K到326M token的不同规模语料库上测试发现，即使在良性NIAH测试中表现良好的系统，在语义干扰下证据访问能力急剧下降。", "conclusion": "语义辨别能力而非单纯的上下文长度是长上下文记忆的主要瓶颈，需要开发更强大的语义区分机制来应对大规模环境中的记忆挑战。"}}
{"id": "2601.20300", "pdf": "https://arxiv.org/pdf/2601.20300", "abs": "https://arxiv.org/abs/2601.20300", "authors": ["Jing Xu", "Minglin Wu", "Xueyuan Chen", "Xixin Wu", "Helen Meng"], "title": "MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by ICASSP2026", "summary": "Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.", "AI": {"tldr": "MiLorE-SSL：结合LoRA和软混合专家机制的高效持续多语言语音表示学习框架，仅需2.14%可训练参数即可有效学习新语言并防止灾难性遗忘", "motivation": "解决多语言自监督学习模型在添加新语言时面临的重新训练计算成本高和顺序训练导致灾难性遗忘的问题", "method": "结合LoRA模块的低秩适应能力和软混合专家机制，通过有限的历史数据回放来减少跨语言干扰和防止遗忘", "result": "在ML-SUPERB基准测试中表现出色，新语言学习能力强，同时保持现有语言能力，仅需2.14%可训练参数", "conclusion": "MiLorE-SSL提供了一种计算高效且有效的持续多语言学习方法，解决了现有方法在语言扩展时的主要挑战"}}
{"id": "2601.20312", "pdf": "https://arxiv.org/pdf/2601.20312", "abs": "https://arxiv.org/abs/2601.20312", "authors": ["Kaiyuan Chen", "Guangmin Zheng", "Jin Wang", "Xiaobing Zhou", "Xuejie Zhang"], "title": "SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger", "categories": ["cs.CL"], "comment": "Accepted by AAAI 2026", "summary": "Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.", "AI": {"tldr": "该论文提出了一种名为SAPO的自适应过程优化方法，通过主动最小化推理器-验证器之间的差距来解决现有自进化方法忽略细粒度推理步骤影响的问题，在数学和代码任务上优于现有方法。", "motivation": "现有自进化方法忽略了细粒度推理步骤的影响，导致推理器-验证器之间存在差距，且蒙特卡洛过程监督的计算效率低下加剧了这一问题。受ERN（错误相关负波）启发，推理器能够在错误决策后定位错误并指导快速调整。", "method": "提出SAPO（自适应过程优化）方法，通过自适应且高效地引入过程监督信号，主动最小化推理器-验证器之间的差距，而不是依赖低效的蒙特卡洛估计。", "result": "大量实验表明，该方法在数学和代码这两类具有挑战性的任务类型上优于大多数现有自进化方法。", "conclusion": "SAPO方法有效解决了推理器-验证器差距问题，提高了小语言模型的自改进能力，并为过程奖励模型在数学和编程任务中引入了两个新的基准测试。"}}
{"id": "2601.20326", "pdf": "https://arxiv.org/pdf/2601.20326", "abs": "https://arxiv.org/abs/2601.20326", "authors": ["Zeyu Xing", "Xing Li", "Hui-Ling Zhen", "Mingxuan Yuan", "Sinno Jialin Pan"], "title": "Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR26", "summary": "KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \\textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \\textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.", "AI": {"tldr": "KV缓存可作为轻量级表示用于下游任务，无需重新计算或存储完整隐藏状态。在Chain-of-Embedding和Fast/Slow Thinking Switching应用中表现优异，能减少5.7倍token生成且精度损失最小。", "motivation": "KV缓存通常仅用于加速自回归解码，但其中编码的上下文信息可以被免费重用于下游任务，避免重新计算或存储完整隐藏状态的成本。", "method": "将KV缓存作为轻量级表示使用，提出两种应用方式：Chain-of-Embedding用于提升模型性能，Fast/Slow Thinking Switching用于自适应推理。", "result": "在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上达到竞争性或更优性能；在Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B上实现自适应推理，token生成减少达5.7倍，精度损失最小。", "conclusion": "KV缓存可作为免费有效的采样和推理基础，为LLM推理中的表示重用开辟了新方向。"}}
{"id": "2601.20327", "pdf": "https://arxiv.org/pdf/2601.20327", "abs": "https://arxiv.org/abs/2601.20327", "authors": ["Xinyu Hu", "Yancheng He", "Weixun Wang", "Tao Feng", "Li Lin", "Jiashun Liu", "Wenbo Su", "Bo Zheng", "Xiaojun Wan"], "title": "CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.", "AI": {"tldr": "提出CE-RM-4B点式生成奖励模型，通过两阶段rollout方法和统一查询标准训练，在少量高质量数据上实现优越性能，有效提升RL实践效果", "motivation": "现有LLM-as-a-Judge评估方法在基准测试表现与实际RL应用效果之间存在显著差距，主要由于成对评估主导和评估标准优化不足", "method": "使用两阶段rollout方法训练点式生成奖励模型CE-RM-4B，采用统一查询标准，仅使用约5.7K高质量数据", "result": "在多样化奖励模型基准测试中表现优越，特别是在Best-of-N场景中，并在下游RL实践中提供更有效的改进", "conclusion": "CE-RM-4B模型成功解决了传统评估方法的局限性，为开放域自然语言生成的自动评估提供了更有效的解决方案"}}
{"id": "2601.20330", "pdf": "https://arxiv.org/pdf/2601.20330", "abs": "https://arxiv.org/abs/2601.20330", "authors": ["Zhuang Chen", "Dazhen Wan", "Zhangkai Zheng", "Guanqun Bi", "Xiyao Xiao", "Binghang Li", "Minlie Huang"], "title": "PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.", "AI": {"tldr": "PsychePass是一个评估大语言模型心理治疗能力的统一框架，通过轨迹锚定锦标赛来校准模型能力，解决了现有评估方法的过程漂移和标准漂移问题。", "motivation": "当前心理治疗能力评估存在两个不稳定问题：过程漂移（客户模拟偏离咨询目标）和标准漂移（静态评分缺乏稳定性），需要更可靠的评估方法。", "method": "采用轨迹锚定锦标赛框架：1）在模拟中锚定交互轨迹，让客户精确控制咨询过程；2）通过瑞士系统锦标赛进行动态成对对战，生成稳健的Elo评分。", "result": "实验验证了PsychePass的有效性，并显示其与人类专家判断具有强一致性。锦标赛轨迹可转化为可信奖励信号，支持强化学习提升模型性能。", "conclusion": "PsychePass提供了一个统一且稳健的框架来评估和提升LLMs的心理治疗能力，解决了现有评估方法的局限性，并与人类专家判断高度一致。"}}
{"id": "2601.20335", "pdf": "https://arxiv.org/pdf/2601.20335", "abs": "https://arxiv.org/abs/2601.20335", "authors": ["Qinzhuo Wu", "Zhizhuo Yang", "Hanhao Li", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "title": "MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.", "AI": {"tldr": "MobileBench-OL是一个针对移动GUI代理的在线评估基准，包含1080个任务和5个子集，用于评估任务执行、复杂推理和噪声鲁棒性能力，填补了现有基准与真实移动环境之间的差距。", "motivation": "现有移动GUI代理评估基准主要关注指令跟随能力，忽视了推理探索能力和真实环境中的随机噪声问题，导致基准与真实环境存在差距。", "method": "提出了MobileBench-OL在线基准，包含来自80个中文应用的1080个任务，分为5个子集评估不同维度，并提供了带有重置机制的自动评估框架。", "result": "对12个领先GUI代理的评估显示其在真实环境要求下仍有显著改进空间，人工评估确认MobileBench-OL能可靠测量代理在真实环境中的性能。", "conclusion": "MobileBench-OL为移动GUI代理提供了更全面、真实的评估标准，有助于推动代理在复杂移动环境中的实际应用能力发展。"}}
{"id": "2601.20674", "pdf": "https://arxiv.org/pdf/2601.20674", "abs": "https://arxiv.org/abs/2601.20674", "authors": ["Juan Jose Rubio Jan", "Jack Wu", "Julia Ive"], "title": "Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.", "AI": {"tldr": "本研究评估了大型语言模型在电子健康记录数据分析中的两个核心任务表现：结构化数据查询（Python/Pandas）和基于RAG的非结构化临床文本信息提取，通过自动生成的合成问答对进行多维度评估。", "motivation": "探索LLMs在医疗数据分析中的实际应用潜力，特别是在结构化数据查询和非结构化临床文本信息提取方面的准确性和可靠性。", "method": "使用MIMIC III数据集的精选子集（4个结构化表和1种临床记录类型），通过自动生成的合成问答对评估框架，结合本地部署和API调用的LLMs，采用精确匹配、语义相似度和人工判断的多维度评估方法。", "result": "研究发现LLMs在临床工作流程中支持精确查询和准确信息提取方面具有显著潜力。", "conclusion": "LLMs在电子健康记录数据分析中展现出强大的应用前景，特别是在结构化查询和基于RAG的文本信息提取任务中表现可靠，为临床工作流程的自动化提供了有力支持。"}}
{"id": "2601.20339", "pdf": "https://arxiv.org/pdf/2601.20339", "abs": "https://arxiv.org/abs/2601.20339", "authors": ["Yangyi Shen", "Tianjian Feng", "Jiaqi Han", "Wen Wang", "Tianlang Chen", "Chunhua Shen", "Jure Leskovec", "Stefano Ermon"], "title": "Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.", "AI": {"tldr": "Order-Token Search方法通过联合搜索生成顺序和token值，在扩散语言模型中实现了更好的解码性能，在多个推理和编程基准测试中显著超越基线方法", "motivation": "当前的扩散语言模型解码方法只遵循单一轨迹，限制了轨迹空间的探索能力", "method": "提出Order-Token Search方法，使用似然估计器对去噪动作进行评分，实现稳定的剪枝和高效的多样化轨迹探索", "result": "在GSM8K、MATH500、Countdown和HumanEval基准测试中分别获得3.1%、3.8%、7.9%和6.8%的绝对性能提升，匹配或超越了经过后训练的diffu-GRPO d1-LLaDA模型", "conclusion": "联合搜索是推进扩散语言模型解码能力的关键组成部分"}}
{"id": "2601.20731", "pdf": "https://arxiv.org/pdf/2601.20731", "abs": "https://arxiv.org/abs/2601.20731", "authors": ["Mae Sosto", "Delfina Sol Martinez Pandiani", "Laura Hollink"], "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.", "AI": {"tldr": "该研究分析了大语言模型如何复制社会规范（特别是异性恋规范）并产生可测量的偏见。研究发现不同类型的语言模型在对待不同性别和性取向标记的主体时存在显著差异，其中掩码语言模型对酷儿标记主体产生最负面的输出。", "motivation": "研究动机是探究大语言模型如何重现社会规范，特别是异性恋规范，以及这些规范如何转化为文本生成中的可测量偏见。", "method": "研究方法包括：将主体分为三类（酷儿标记、非酷儿标记和无标记类别），通过四个维度（情感、尊重度、毒性和预测多样性）量化表征不平衡，分析掩码语言模型和自回归语言模型的表现差异。", "result": "研究发现掩码语言模型对酷儿标记主体产生最不利的情感、更高的毒性和更负面的尊重度。自回归语言模型部分缓解了这些模式，但闭源自回归模型对无标记主体产生更有害的输出。", "conclusion": "结论是大语言模型确实重现了规范性的社会假设，但偏见的形式和程度强烈依赖于具体模型特征，这些特征可能重新分配但不会消除表征性伤害。"}}
{"id": "2601.20412", "pdf": "https://arxiv.org/pdf/2601.20412", "abs": "https://arxiv.org/abs/2601.20412", "authors": ["Qihao Wang", "Yue Hu", "Mingzhe Lu", "Jiayue Wu", "Yanbing Liu", "Yuanmin Tang"], "title": "Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents", "categories": ["cs.CL", "cs.SE"], "comment": "Accepted to AAAI 2026", "summary": "The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.", "AI": {"tldr": "该论文提出了一个基于认知负荷理论的评估框架，将任务复杂度分解为内在负荷和外在负荷，并构建了ToolLoad-Bench基准测试来精确测量大语言模型的能力边界。", "motivation": "当前基准测试主要报告最终准确率，无法揭示模型认知瓶颈和真实能力边界，需要从简单性能评分转向诊断工具。", "method": "基于认知负荷理论，将任务复杂度分解为：1）内在负荷 - 解决方案路径的结构复杂性，用新颖的工具交互图形式化；2）外在负荷 - 任务呈现模糊性带来的难度。构建了ToolLoad-Bench基准，支持参数化调整认知负荷。", "result": "评估显示随着认知负荷增加，模型性能出现明显下降点，能够精确绘制每个模型的能力边界。框架预测与实证结果高度校准。", "conclusion": "建立了一个原则性方法来理解智能体的极限，为构建更高效系统提供了实用基础，实现了从性能评估到诊断分析的转变。"}}
{"id": "2601.20417", "pdf": "https://arxiv.org/pdf/2601.20417", "abs": "https://arxiv.org/abs/2601.20417", "authors": ["Biswesh Mohapatra", "Marcely Zanon Boito", "Ioan Calapodescu"], "title": "SpeechMapper: Speech-to-text Embedding Projector for LLMs", "categories": ["cs.CL"], "comment": "Accepted to ICASSP 2026", "summary": "Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.", "AI": {"tldr": "SpeechMapper提出了一种高效的语音到LLM嵌入训练方法，通过预训练和简短指令调优阶段，减少计算成本和过拟合问题，在语音翻译和口语问答任务中表现出色。", "motivation": "当前语音LLM通过投影层连接语音基础模型和LLM，需要在整个语音指令数据上训练所有组件，计算成本高且容易过拟合。", "method": "先在不使用LLM的情况下进行预训练（低成本硬件），然后通过1K步的简短指令调优阶段高效连接到目标LLM，包括任务无关和任务特定的指令调优策略。", "result": "在任务无关设置中，SpeechMapper与IWSLT25最佳指令跟随语音LLM相当；在任务特定设置中，使用更少数据和计算的情况下，在多个数据集上超越该模型。", "conclusion": "SpeechMapper提供了一种实用且可扩展的方法，无需大规模指令调优即可实现高效、可泛化的语音-LLM集成。"}}
{"id": "2601.20424", "pdf": "https://arxiv.org/pdf/2601.20424", "abs": "https://arxiv.org/abs/2601.20424", "authors": ["Anna Ristilä", "Otto Tarkka", "Veronika Laippala", "Kimmo Elo"], "title": "Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020", "categories": ["cs.CL"], "comment": "27 pages (40 including appendices), 5 figures (13 including sub-figures), 1 table, 1 formula, 3 appendices; submitted to JDMDH", "summary": "Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.", "AI": {"tldr": "该研究分析芬兰议会2000-2020年演讲中的情绪表达，发现不同议题存在特定情绪模式，议会演讲整体呈现积极情绪增长趋势", "motivation": "现有研究常将议会话语视为同质整体，忽视了议题特定的情绪模式，缺乏对不同议题通常关联的情绪研究", "method": "使用情绪分析模型，从共时和历时两个角度分析芬兰议会演讲中不同议题的情绪表达", "result": "证实了议会演讲中积极情绪增加的趋势，并提供了议会辩论中议题特定情绪表达的深入见解", "conclusion": "议会话语的情绪表达具有议题特异性，该研究填补了议题相关情绪研究的空白，为理解议会辩论中的情绪动态提供了新视角"}}
{"id": "2601.20439", "pdf": "https://arxiv.org/pdf/2601.20439", "abs": "https://arxiv.org/abs/2601.20439", "authors": ["Qihao Wang", "Mingzhe Lu", "Jiayue Wu", "Yue Hu", "Yanbing Liu"], "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use", "categories": ["cs.CL"], "comment": "Accepted to PRICAI25", "summary": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.", "AI": {"tldr": "PEARL框架通过离线和在线两阶段方法增强LLM的工具使用能力，在ToolHop基准上达到56.5%的最新成功率", "motivation": "大型语言模型在复杂多轮工具调用中存在规划能力弱、工具幻觉、参数生成错误等问题，需要更鲁棒的交互解决方案", "method": "采用两阶段方法：离线阶段探索工具学习有效使用模式和失败条件；在线阶段通过群体相对策略优化(GRPO)训练专用规划器，使用精心设计的奖励函数提供规划质量信号", "result": "在ToolHop和T-Eval基准测试中显著优于现有方法，ToolHop上达到56.5%的成功率，同时保持较低调用错误率", "conclusion": "PEARL框架在解决工具使用的复杂规划挑战方面取得关键进展，有助于开发更鲁棒可靠的基于LLM的智能体"}}
{"id": "2601.20451", "pdf": "https://arxiv.org/pdf/2601.20451", "abs": "https://arxiv.org/abs/2601.20451", "authors": ["Diandian Guo", "Fangfang Yuan", "Cong Cao", "Xixun Lin", "Chuan Zhou", "Hao Peng", "Yanan Cao", "Yanbing Liu"], "title": "MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues", "categories": ["cs.CL"], "comment": "12 pages, 7 figures. Accepted by WWW 2026", "summary": "The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.", "AI": {"tldr": "MuVaC是一个变分因果推理框架，用于联合优化多模态讽刺检测(MSD)和多模态讽刺解释(MuSE)，通过模拟人类认知机制来理解讽刺内容。", "motivation": "当前研究主要单独处理MSD或MuSE任务，忽视了它们之间的因果依赖关系，需要一种能够同时优化这两个任务的方法来更好地理解多模态讽刺。", "method": "1) 从结构因果模型角度建模MSD和MuSE，建立变分因果路径；2) 设计对齐-融合方法整合多模态特征；3) 确保检测结果与解释之间的一致性以增强推理可信度。", "result": "在公开数据集上展示了MuVaC的优越性能。", "conclusion": "MuVaC为理解多模态讽刺提供了新的视角，通过因果推理框架有效解决了MSD和MuSE的联合优化问题。"}}
{"id": "2601.20465", "pdf": "https://arxiv.org/pdf/2601.20465", "abs": "https://arxiv.org/abs/2601.20465", "authors": ["Yang Li", "Jiaxiang Liu", "Yusong Wang", "Yujie Wu", "Mingkun Xu"], "title": "BMAM: Brain-inspired Multi-Agent Memory Framework", "categories": ["cs.CL"], "comment": "Submitted to ACL (ARR 2026 January submission); non-anonymous preprint", "summary": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.", "AI": {"tldr": "BMAM是一种受大脑启发的多智能体记忆架构，通过将记忆分解为情景、语义、显著性感知和控制导向四个功能子系统，解决了语言模型智能体在长时间交互中的记忆保持和行为一致性问题。", "motivation": "基于语言模型的智能体在长时间交互中存在难以保持时间基础信息和跨会话行为一致性的问题，作者称之为\"灵魂侵蚀\"。", "method": "BMAM采用认知记忆系统启发的方法，将记忆分解为四个功能子系统：情景记忆、语义记忆、显著性感知记忆和控制导向记忆，这些子系统在互补的时间尺度上运作。为了支持长时程推理，BMAM沿显式时间线组织情景记忆，并通过融合多个互补信号来检索证据。", "result": "在LoCoMo基准测试中，BMAM在标准长时程评估设置下达到了78.45%的准确率，消融分析证实受海马体启发的情景记忆子系统在时间推理中起关键作用。", "conclusion": "BMAM通过功能专门化的多子系统记忆架构有效解决了智能体长时间交互中的记忆保持问题，特别是情景记忆子系统对时间推理至关重要，为构建更稳定和一致的AI智能体提供了新思路。"}}
{"id": "2601.20476", "pdf": "https://arxiv.org/pdf/2601.20476", "abs": "https://arxiv.org/abs/2601.20476", "authors": ["Evanfiya Logacheva", "Arto Hellas", "Tsvetomila Mihaylova", "Juha Sorva", "Ava Heinonen", "Juho Leinonen"], "title": "Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch", "categories": ["cs.CL"], "comment": null, "summary": "Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.", "AI": {"tldr": "基于修辞结构理论(RST)的上下文示例方法改善AI生成图表代码的质量，减少事实性幻觉并提高上下文忠实度，但LLM的随机性导致生成质量不稳定。", "motivation": "生成式AI在教育领域广泛应用，但生成材料的质量引发担忧，需要解决图表生成与用户期望对齐的问题。", "method": "引入基于修辞结构理论(RST)的上下文示例方法，通过计算机科学教育工作者评估150个LLM生成的图表，从逻辑组织、连接性、布局美学和AI幻觉四个维度进行分析。", "result": "方法降低了事实性幻觉率并提高了图表对上下文的忠实度，但LLM的随机性导致生成质量不稳定；复杂文本上下文导致更高的幻觉率，LLM难以检测自身输出错误。", "conclusion": "RST方法在改善AI生成图表质量方面有效，但需要进一步解决LLM随机性和复杂上下文导致的幻觉问题，评估数据集对自动化图表评估具有潜在价值。"}}
{"id": "2601.20546", "pdf": "https://arxiv.org/pdf/2601.20546", "abs": "https://arxiv.org/abs/2601.20546", "authors": ["Kumiko Nakajima", "Jan Zuiderveld", "Sandro Pezzelle"], "title": "Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to Findings of EACL 2026", "summary": "Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.", "AI": {"tldr": "论文提出了Conditional Divergent Association Task (CDAT)来更准确地评估大语言模型的创造力，发现现有评估方法DAT存在缺陷，高级模型倾向于适当性而非创造性。", "motivation": "现有的大语言模型创造力评估方法(DAT)缺乏人类创造力理论的坚实基础，只关注新颖性而忽略了适当性这一创造力的核心要素，导致评估结果难以解释。", "method": "基于人类创造力理论(新颖性+适当性)，提出了条件发散联想任务(CDAT)，在保持简单客观的同时，通过条件化上下文适当性来更好地分离噪声与创造力。", "result": "CDAT评估显示较小模型家族通常最具创造力，而先进模型家族在较低新颖性水平上更倾向于适当性；训练和对齐过程可能使模型输出更适当但创造性降低。", "conclusion": "CDAT提供了更有效的创造力评估框架，揭示了模型训练过程对创造力的影响，为未来研究提供了新的评估工具和数据集。"}}
{"id": "2601.20582", "pdf": "https://arxiv.org/pdf/2601.20582", "abs": "https://arxiv.org/abs/2601.20582", "authors": ["Shalom Rosner", "Ronit D. Gross", "Ella Koresh", "Ido Kanter"], "title": "Single-Nodal Spontaneous Symmetry Breaking in NLP Models", "categories": ["cs.CL"], "comment": "23 pages, 6 figures, 1 table", "summary": "Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.", "AI": {"tldr": "该论文揭示了自然语言处理模型在预训练和微调过程中出现自发性对称破缺现象，即使在确定性动力学和有限架构下，注意力头层级的节点也能学习特定标记或标签，表现出类似统计力学相变的对称性降低特征。", "motivation": "研究旨在探索NLP模型中是否会出现类似统计力学的自发性对称破缺现象，特别是在确定性训练过程和有限网络架构条件下，这与传统统计力学中需要在热力学极限下发生相变的情况形成对比。", "method": "使用BERT-6架构在Wikipedia数据集上进行预训练，然后在FewRel分类任务上进行微调。通过分析注意力头层级的节点行为，采用凸包分析上界节点功能，研究节点数量增加时的学习能力交叉现象。", "result": "发现单个节点在预训练后能学习有限标记集，微调后能学习特定分类标签。节点数量增加时出现学习能力交叉，受随机猜测概率下降和节点协作增强的权衡支配。与自旋玻璃系统不同，每个节点功能都明确贡献于全局任务。", "conclusion": "NLP模型在有限架构和确定性训练中确实表现出自发性对称破缺，这为理解神经网络的学习机制提供了新视角，表明微观节点行为与全局任务优化之间存在直接联系，区别于传统统计力学系统。"}}
{"id": "2601.20592", "pdf": "https://arxiv.org/pdf/2601.20592", "abs": "https://arxiv.org/abs/2601.20592", "authors": ["Ali Basirat", "Danial Namazifard", "Navid Baradaran Hemmati"], "title": "A Computational Approach to Language Contact -- A Case Study of Persian", "categories": ["cs.CL"], "comment": null, "summary": "We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.", "AI": {"tldr": "研究分析了单语语言模型中间表征中的语言接触结构痕迹，发现普遍句法信息对历史接触不敏感，而形态特征（如格和性）受语言特定结构强烈影响。", "motivation": "探索波斯语作为历史上接触丰富的语言，在单语语言模型中间表征中如何反映与不同接触程度语言的接触效应。", "method": "使用波斯语训练的模型，分析其暴露于与波斯语有不同接触程度语言时的中间表征，量化语言信息编码量并评估信息在不同模型组件中的分布。", "result": "普遍句法信息对历史接触不敏感，形态特征（如Case和Gender）受语言特定结构强烈影响。", "conclusion": "单语语言模型中的接触效应是选择性的，并受到结构约束，形态特征比句法特征更容易受到语言接触的影响。"}}
{"id": "2601.20613", "pdf": "https://arxiv.org/pdf/2601.20613", "abs": "https://arxiv.org/abs/2601.20613", "authors": ["Kaiyuan Chen", "Qimin Wu", "Taiyu Hou", "Tianhao Tang", "Xueyu Hu", "Yuchen Hou", "Bikun Li", "Chengming Qian", "Guoyin Wang", "Haolin Chen", "Haotong Tian", "Haoye Zhang", "Haoyu Bian", "Hongbing Pan", "Hongkang Zhang", "Hongyi Zhou", "Jiaqi Cai", "Jiewu Rao", "Jiyuan Ren", "Keduan Huang", "Lucia Zhu Huang", "Mingyu Yuan", "Naixu Guo", "Qicheng Tang", "Qinyan Zhang", "Shuai Chen", "Siheng Chen", "Ting Ting Li", "Xiaoxing Guo", "Yaocheng Zuo", "Yaoqi Guo", "Yinan Wang", "Yinzhou Yu", "Yize Wang", "Yuan Jiang", "Yuan Tian", "Yuanshuo Zhang", "Yuxuan Liu", "Yvette Yan Zeng", "Zenyu Shan", "Zihan Yin", "Xiaobo Hu", "Yang Liu", "Yixin Ren", "Yuan Gong"], "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios", "categories": ["cs.CL"], "comment": "17 pages, 8 figures", "summary": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.", "AI": {"tldr": "论文提出了AgentIF-OneDay基准测试，用于评估AI代理在多样化日常任务中的表现，包括工作流执行、隐含指令理解和迭代优化，发现基于API的代理产品表现最佳。", "motivation": "当前AI评估过于关注任务难度而忽视了日常任务的多样性，导致普通用户对AI能力的认知有限。", "method": "构建包含104个任务、767个评分点的基准测试，分为三个用户中心类别：开放工作流执行、潜在指令理解和迭代优化，采用实例级评分标准和LLM与人工判断结合的评估流程。", "result": "基于API和ChatGPT的代理产品表现最佳，领先的LLM API和开源模型已具备代理能力，Gemini-3-Pro达到80.1%的人工判断一致性。", "conclusion": "AI应用团队可以利用现有LLM API开发先进的代理产品，AgentIF-OneDay基准有助于更全面地评估AI代理在日常场景中的实际能力。"}}
{"id": "2601.20649", "pdf": "https://arxiv.org/pdf/2601.20649", "abs": "https://arxiv.org/abs/2601.20649", "authors": ["Wenlin Zhong", "Chengyuan Liu", "Yiquan Wu", "Bovin Tan", "Changlong Sun", "Yi Wang", "Xiaozhong Liu", "Kun Kuang"], "title": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.", "AI": {"tldr": "P2S是一个新的自监督框架，通过路径忠实度奖励为LLM推理过程提供细粒度监督，无需额外奖励模型或人工标注，在阅读理解医疗问答任务中显著优于基线方法。", "motivation": "现有的RLVR方法在结构化领域有效，但在通用领域因缺乏可验证奖励信号而受限。RLPR等方法只关注最终结果奖励，忽略了推理过程的逐步监督。", "method": "提出概率过程监督(P2S)框架：1)合成和过滤高质量参考推理链；2)计算路径忠实度奖励(PFR)，基于当前推理前缀生成参考后缀的条件概率；3)可与任何结果奖励灵活结合。", "result": "在阅读理解和医疗问答基准测试中，P2S显著优于强基线方法，有效解决了奖励稀疏性问题。", "conclusion": "P2S通过提供密集的过程级监督，成功解决了通用领域推理任务的奖励稀疏问题，为LLM推理提供了有效的自监督解决方案。"}}
{"id": "2601.20659", "pdf": "https://arxiv.org/pdf/2601.20659", "abs": "https://arxiv.org/abs/2601.20659", "authors": ["Sara Candussio"], "title": "A Dialectic Pipeline for Improving LLM Robustness", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.\n  However, methods such as fine-tuning on domain-specific data or the training of a separate \\textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.\n  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.\n  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.\n  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.", "AI": {"tldr": "论文提出了一种通过自对话来减少语言模型幻觉的辩证法流程，无需额外训练即可提升输出质量，在多个数据集和模型上都显著优于标准方法和思维链提示", "motivation": "现有减少语言模型幻觉的方法（如领域微调或训练验证器）需要大量计算资源且限制了模型的泛化能力，需要一种更通用且资源友好的解决方案", "method": "设计辩证法流程，让语言模型通过自对话进行反思和修正错误答案，流程中包含相关上下文信息（oracle-RAG设置），并研究了信息摘要和过滤的影响", "result": "辩证法流程在多个数据集和不同模型家族上都显著优于标准模型答案，且持续获得比单纯思维链提示更高的性能表现", "conclusion": "自对话辩证法流程是减少语言模型幻觉的有效方法，既能保持模型泛化能力，又能显著提升输出质量，且无需额外训练成本"}}
{"id": "2601.20676", "pdf": "https://arxiv.org/pdf/2601.20676", "abs": "https://arxiv.org/abs/2601.20676", "authors": ["Zhuo Chen", "Xinyu Geng", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Pengjun Xie", "Kewei Tu"], "title": "Efficient Multimodal Planning Agent for Visual Question-Answering", "categories": ["cs.CL"], "comment": null, "summary": "Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.", "AI": {"tldr": "本文提出了一种训练多模态规划代理的方法，通过动态分解多模态检索增强生成(mRAG)流程来解决视觉问答任务，在保持性能的同时显著提升效率。", "motivation": "现有的多模态检索增强生成方法在处理知识密集型VQA查询时采用多阶段流程，存在效率低下和计算冗余的问题。", "method": "训练一个多模态规划代理，智能地动态确定每个mRAG步骤的必要性，优化效率与效果的平衡。", "result": "实验显示该方法能减少60%以上的搜索时间，降低工具调用成本，并在六个不同数据集上超越所有基线方法。", "conclusion": "该方法通过智能规划有效解决了mRAG流程的效率问题，在保持VQA任务性能的同时显著提升了计算效率。"}}
{"id": "2601.20679", "pdf": "https://arxiv.org/pdf/2601.20679", "abs": "https://arxiv.org/abs/2601.20679", "authors": ["Mingqiao Mo", "Yunlong Tan", "Hao Zhang", "Heng Zhang", "Yangfan He"], "title": "ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code", "categories": ["cs.CL"], "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.", "AI": {"tldr": "ShieldedCode是首个保护感知框架，通过大规模数据集和分层依赖建模，联合优化语言建模与对比目标，显著提升虚拟机保护代码的生成质量和安全强度。", "motivation": "传统虚拟机保护(VMP)方法基于刚性规则转换，设计成本高且易受自动化分析攻击，而大语言模型在代码保护方面的潜力尚未充分挖掘。", "method": "构建大规模源代码与规范化VM实现配对数据集，引入指令内、前序和指令间三个层次的分层依赖建模，联合优化功能感知和保护感知的对比目标，采用两阶段持续预训练和微调流程。", "result": "在L0 VM代码生成上达到26.95%的Pass@1（GPT-4o为22.58%），二进制相似性检测Recall@1比jTrans等先进方法提升10%。", "conclusion": "该框架显著提升了不同保护级别的鲁棒性，为基于学习的软件防御开辟了新的研究方向。"}}
{"id": "2601.20680", "pdf": "https://arxiv.org/pdf/2601.20680", "abs": "https://arxiv.org/abs/2601.20680", "authors": ["Ostap Vykhopen", "Viktoria Skorik", "Maxim Tereschenko", "Veronika Solopova"], "title": "Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin", "categories": ["cs.CL"], "comment": null, "summary": "Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.", "AI": {"tldr": "本研究评估了用在线聚类算法替代HDBSCAN批量聚类来处理社交媒体流数据，解决了传统批处理算法在实时叙事监控中的可扩展性问题。", "motivation": "社交媒体监控中的叙事智能系统面临可扩展性挑战，传统批量聚类算法（如HDBSCAN）需要完全重新训练每个时间窗口，导致内存限制、计算效率低下且无法实时适应不断发展的叙事。", "method": "采用三阶段架构（数据收集、建模、仪表板生成），评估多种在线聚类算法，在乌克兰信息空间的历史数据集上进行滑动窗口模拟，使用聚类质量指标（轮廓系数、Davies-Bouldin指数）和叙事指标（叙事独特性、偶然性和方差）进行综合评估。", "result": "研究提出了平衡传统聚类指标和叙事指标的评估标准，能够比较算法在真实操作环境中的权衡，为流式社交媒体监控提供了有效的解决方案。", "conclusion": "这项工作解决了面向批处理的主题建模框架与社交媒体监控流式性质之间的关键差距，对计算社会科学、危机信息学和叙事监控系统具有重要意义。"}}
{"id": "2601.20730", "pdf": "https://arxiv.org/pdf/2601.20730", "abs": "https://arxiv.org/abs/2601.20730", "authors": ["Shicheng Fang", "Yuxin Wang", "XiaoRan Liu", "Jiahao Lu", "Chuanyuan Tan", "Xinchi Chen", "Yining Zheng. Xuanjing Huang", "Xipeng Qiu"], "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "categories": ["cs.CL"], "comment": "26 pages", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "AI": {"tldr": "AgentLongBench是一个新的基准测试框架，通过侧向思维谜题模拟环境交互来评估LLM智能体在动态上下文中的表现，发现现有智能体在静态检索表现良好但在动态信息合成方面存在显著弱点", "motivation": "当前LLM智能体基准测试主要是静态和被动的，无法模拟智能体与环境交互的复杂性（如非线性推理和迭代反馈），需要开发能更好评估自主智能体能力的动态基准", "method": "开发AgentLongBench框架，基于侧向思维谜题模拟环境交互轨迹，在知识密集和无知识场景中进行测试，评估32K到4M tokens范围内最先进模型和内存系统的表现", "result": "实验显示智能体在静态检索任务上表现良好，但在动态信息合成方面严重不足，性能下降主要由解决查询所需的最小token数量驱动，大规模工具响应的高信息密度比长对话中的内存碎片化更具挑战性", "conclusion": "大规模工具响应的高信息密度对LLM智能体构成了比内存碎片化更严重的挑战，突显了开发能够处理动态信息合成工作流程的智能体的重要性"}}
{"id": "2601.20747", "pdf": "https://arxiv.org/pdf/2601.20747", "abs": "https://arxiv.org/abs/2601.20747", "authors": ["Elham Aghakhani", "Rezvaneh Rezapour"], "title": "Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.", "AI": {"tldr": "本研究通过分析5126个Reddit心理健康社区的帖子，探讨人们在日常生活中如何评价和使用AI进行情感支持，发现用户参与主要受结果叙述、信任和响应质量影响，而非单纯情感联系。", "motivation": "大型语言模型越来越多地被用于临床环境之外的情感支持和心理健康相关互动，但人们对日常使用中如何评价和关联这些系统的了解甚少。", "method": "基于技术接受模型和治疗联盟理论，开发理论指导的标注框架，采用LLM-人工混合流程大规模分析评价性语言、采纳相关态度和关系对齐。", "result": "结果显示参与主要由叙述结果、信任和响应质量塑造；积极情绪与任务和目标对齐最相关；陪伴导向的使用更多涉及错位联盟和依赖、症状加重等风险。", "conclusion": "这项工作展示了理论基础的构念如何在大规模话语分析中操作化，并强调了研究用户如何在敏感的现实环境中解释语言技术的重要性。"}}
{"id": "2601.20757", "pdf": "https://arxiv.org/pdf/2601.20757", "abs": "https://arxiv.org/abs/2601.20757", "authors": ["Jing Yang", "Moritz Hechtbauer", "Elisabeth Khalilov", "Evelyn Luise Brinkmann", "Vera Schmitt", "Nils Feldhus"], "title": "Persona Prompting as a Lens on LLM Social Reasoning", "categories": ["cs.CL"], "comment": "9 Pages, EACL main", "summary": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.", "AI": {"tldr": "本研究探讨了在仇恨言论检测等社会敏感任务中，人格提示（Persona Prompting）对大型语言模型生成解释质量的影响，发现人格提示虽然能提升分类性能但会降低解释质量，且无法有效缓解模型偏见。", "motivation": "虽然人格提示被广泛用于引导模型生成用户特定的内容，但其对模型解释质量的影响尚未得到充分研究，特别是在社会敏感任务中解释质量对用户信任和模型对齐至关重要。", "method": "使用带有词级解释标注的数据集，测试不同模拟人口统计人格条件下LLM生成解释的变化，测量与不同人口统计群体人类标注的一致性，评估人格提示对模型偏见和人类对齐的影响。", "result": "三项关键发现：(1)人格提示在最具主观性的任务（仇恨言论检测）中提升分类性能但降低解释质量；(2)模拟人格无法与真实人口统计群体对齐，模型对人格引导具有抵抗性；(3)模型展现出一致的人口统计偏见和过度标记内容为有害的倾向。", "conclusion": "人格提示在社会敏感任务中存在关键权衡：虽然能改善分类性能，但往往以解释质量为代价且无法缓解底层偏见，需要谨慎应用。"}}
{"id": "2601.20789", "pdf": "https://arxiv.org/pdf/2601.20789", "abs": "https://arxiv.org/abs/2601.20789", "authors": ["Ethan Shen", "Danny Tormoen", "Saurabh Shah", "Ali Farhadi", "Tim Dettmers"], "title": "SERA: Soft-Verified Efficient Repository Agents", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": "21 main pages, 7 pages appendix", "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "AI": {"tldr": "SERA是一种高效训练代码代理的方法，通过监督微调实现开源模型的最先进性能，成本比强化学习低26倍，比合成数据方法低57倍，支持私有代码库的专门化。", "motivation": "开源权重代码代理相比闭源系统具有优势，可以针对私有代码库进行专门化训练，但之前的训练成本高且复杂，使得这一优势停留在理论层面。", "method": "提出Soft-Verified Efficient Repository Agents (SERA)方法，使用监督微调(SFT)和Soft Verified Generation (SVG)技术，从单个代码库生成数千条轨迹，实现高效训练。", "result": "SERA在完全开源模型中达到最先进性能，匹配Devstral-Small-2等前沿开放权重模型的性能，训练成本大幅降低，并生成了20万条合成轨迹数据集。", "conclusion": "这项工作将加速开源代码代理的研究，展示了开源模型针对私有代码库专门化的优势，并发布了完整的代码、数据和集成工具支持研究社区。"}}
{"id": "2601.20796", "pdf": "https://arxiv.org/pdf/2601.20796", "abs": "https://arxiv.org/abs/2601.20796", "authors": ["Yiran Huang", "Karsten Roth", "Quentin Bouniot", "Wenjia Xu", "Zeynep Akata"], "title": "Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.", "AI": {"tldr": "本文通过控制实验发现，Transformer在多模态情境学习中存在学习不对称性：当主模态数据多样性高时，次模态仅需极低数据复杂度即可实现跨模态关联学习，其机制依赖于类归纳的标签复制机制。", "motivation": "探究Transformer如何通过情境学习实现跨模态信息关联，特别是在多模态大语言模型中观察到的情境学习能力。", "method": "在合成分类任务上训练小型Transformer，精确控制数据统计和模型架构，分析单模态和多模态情境学习机制，特别关注RoPE位置编码的影响。", "result": "发现RoPE提高了ICL的数据复杂度阈值；多模态训练中存在学习不对称性：主模态数据多样性高时，次模态只需极低复杂度即可实现跨模态ICL；机制分析显示依赖类归纳的标签复制机制。", "conclusion": "研究为理解现代Transformer中的多模态情境学习提供了机制基础，并建立了受控测试平台供未来研究使用。"}}
{"id": "2601.20803", "pdf": "https://arxiv.org/pdf/2601.20803", "abs": "https://arxiv.org/abs/2601.20803", "authors": ["Aunabil Chakma", "Mihai Surdeanu", "Eduardo Blanco"], "title": "Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.", "AI": {"tldr": "提出了一种基于句法语义结构相似性的示例选择策略，结合LLM生成示例，构建混合系统，在少样本关系抽取任务上达到最先进性能", "motivation": "解决一次性关系抽取中上下文学习示例不足的问题，通过自动获取额外示例来提升模型性能", "method": "引入基于句法语义结构相似性的示例选择策略，与LLM生成示例相结合形成混合系统", "result": "混合方法在FS-TACRED和FS-FewRel数据集上表现优异，超越单一方法，达到state-of-the-art性能", "conclusion": "基于结构相似性的示例选择与LLM生成示例相结合的方法能有效提升少样本关系抽取性能，具有良好的跨数据集和模型泛化能力"}}
{"id": "2601.20834", "pdf": "https://arxiv.org/pdf/2601.20834", "abs": "https://arxiv.org/abs/2601.20834", "authors": ["Andrew Kyle Lampinen", "Yuxuan Li", "Eghbal Hosseini", "Sangnie Bhardwaj", "Murray Shanahan"], "title": "Linear representations in language models can change dramatically over a conversation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.", "AI": {"tldr": "研究发现语言模型的表示会随着对话动态变化，相同信息在不同对话阶段可能被表示为事实或非事实，这种变化具有内容依赖性和鲁棒性，对可解释性和控制技术提出了挑战。", "motivation": "研究语言模型表示在对话过程中的动态变化特性，探索高维概念表示如何随上下文演化。", "method": "通过分析对话过程中线性表示维度的变化，测试不同模型家族和层次，使用脚本回放和科幻故事对比实验，研究表示动态的鲁棒性和条件依赖性。", "result": "发现表示会随对话显著变化，相关信息会改变而通用信息保持稳定；变化具有鲁棒性，跨模型和层次都存在；脚本回放能产生类似变化但显式科幻故事框架的适应较弱；表示方向的控制效果在对话不同阶段差异巨大。", "conclusion": "表示动态变化反映了模型根据对话角色调整的内在机制，这对静态特征解释和探测技术提出了挑战，但也为理解模型上下文适应机制提供了新的研究方向。"}}
{"id": "2601.20858", "pdf": "https://arxiv.org/pdf/2601.20858", "abs": "https://arxiv.org/abs/2601.20858", "authors": ["David Tan", "Pinzhen Chen", "Josef van Genabith", "Koel Dutta Chowdhury"], "title": "When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation", "categories": ["cs.CL"], "comment": "5 pages of content, 15 total. 5 figures, 12 tables total. Accepted to EACL 2026 main conference. Code can be found here: github.com/Mr-Ao-25/cross-ling-contamination", "summary": "Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to \"uncontaminated\" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.", "AI": {"tldr": "研究发现大型语言模型在FLORES翻译基准测试中存在污染问题，导致分数虚高，将记忆伪装成泛化能力。通过分析Bloomz和Llama模型，证实了污染会导致跨方向翻译性能的人为提升，即使进行源端扰动，记忆的回忆仍然持续。", "motivation": "研究大型语言模型在多语言环境下的基准污染问题，特别是FLORES翻译基准被训练数据污染的情况，这会导致模型性能评估失真，将记忆误判为泛化能力。", "method": "使用FLORES-200翻译基准作为诊断工具，对比分析两个7-8B指令调优的多语言LLM：被FLORES污染的Bloomz和未受污染的Llama控制组。通过源端扰动技术（如释义和命名实体替换）来测试记忆持久性。", "result": "确认Bloomz存在FLORES污染，机器翻译污染具有跨方向性，会因目标端记忆而人为提升未见翻译方向的性能。记忆回忆在各种源端扰动下仍然持续，但命名实体替换会导致BLEU分数一致下降。", "conclusion": "机器翻译基准污染会导致性能评估失真，跨方向记忆效应显著。命名实体替换可作为检测污染模型记忆的有效探测方法，这对LLM基准测试的严谨性提出了重要警示。"}}
