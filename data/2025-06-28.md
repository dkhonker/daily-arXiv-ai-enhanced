<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 77]
- [cs.AI](#cs.AI) [总数: 13]
- [stat.ML](#stat.ML) [总数: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain, Muhammad Sohail, Nauman Ali Khan, Naima Iltaf, Ihtesham ul Islam*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种基于数据集大小的自适应联邦学习（SAFL）框架，揭示了数据集大小对联邦学习效果的影响，并提供了理论和实践指导。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法主要关注模型异构性和聚合技术，而忽略了数据集大小特征对联邦训练动态的基本影响。

**方法:** 引入了Size-Based Adaptive Federated Learning (SAFL)，这是一个基于数据集大小特征的渐进式训练框架，系统地组织了跨异构多模态数据的联邦学习过程。

**结果:** 通过在13个不同数据集上的实验，发现：1）联邦学习的有效性最佳数据集大小范围为1000-1500个样本；2）结构化数据的表现显著优于非结构化数据；3）超过2000个样本的大数据集性能会系统性下降。SAFL在所有数据集上平均准确率达到87.68%，并且具有较高的通信效率。

**结论:** 这项工作填补了如何利用数据特性驱动联邦学习策略的关键空白，为神经网络和学习系统的实际联邦学习部署提供了理论见解和实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Progressive+Size-Adaptive+Federated+Learning%3A+A+Comprehensive+Framework+for+Heterogeneous+Multi-Modal+Data+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20685&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [2] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio, Tommaso Mazza, Pierangelo Veltri, Pietro Hiram Guzzi*

**主要类别:** cs.LG

**AI概要:** 随着大规模组学数据的日益增多，需要能够处理复杂基因表达数据集并提供可解释结果的稳健分析框架。人工智能的进步使得识别区分疾病状态和健康对照的异常分子模式成为可能，并支持确定潜在驱动疾病表型的基因。然而，当前的基因异常检测方法通常仅限于单一数据集且缺乏易用的图形界面。本文介绍了一种通用且可解释的生物网络异常检测框架E-ABIN。E-ABIN结合了经典机器学习和基于图的深度学习技术，能够在统一、用户友好的平台上实现从基因表达或甲基化衍生网络中检测和解释异常。通过整合如支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，E-ABIN确保了高预测准确性的同时保持可解释性。通过膀胱癌和乳糜泻的案例研究，证明了E-ABIN在有效揭示生物学相关异常并提供疾病机制见解方面的实用性。


<details>
  <summary>更多</summary>
  
**动机:** 现有基因异常检测方法通常局限于单一数据集，且缺乏易于使用的图形界面，限制了其广泛应用和易用性。

**方法:** E-ABIN框架结合了经典机器学习（如支持向量机、随机森林）和基于图的深度学习技术（如图自编码器和图对抗属性网络），在一个统一且用户友好的平台上进行基因表达或甲基化衍生网络中的异常检测与解释。

**结果:** 通过膀胱癌和乳糜泻的案例研究，E-ABIN成功揭示了生物学相关的异常，并提供了对疾病机制的深入见解。

**结论:** E-ABIN是一个通用且可解释的框架，适用于生物网络中的异常检测，具有高预测准确性及良好的可解释性，为疾病机制研究提供了有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是E-ABIN%3A+an+Explainable+module+for+Anomaly+detection+in+BIological+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20693，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20693&send_immediately=true&force_search=false)

**原文摘要:** The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [3] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了基于上下文-内容不确定性原则（CCUP）的分层计算框架，揭示了大脑和机器通过递归结构特异性对齐来最小化不确定性的统一理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 为理解大脑和机器如何通过结构与特异性的递归对齐来最小化不确定性提供统一理论基础。

**方法:** 开发了一个基于CCUP的分层计算框架，包括四个层级的操作原则：核心推理约束、资源分配原则、时间引导动力学和空间层次组合，并进行了正式等价定理、依赖关系晶格和计算模拟的研究。

**结果:** 展示了CCUP对齐推理的效率提升，并证明了各操作原则之间的相互可约性。

**结论:** 大脑不仅仅是推理机器，而是一个周期一致的熵梯度解析器，通过路径依赖的内容驱动模拟来对齐结构与特异性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Context-Content+Uncertainty+Principle，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20699&send_immediately=true&force_search=false)

**原文摘要:** The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [4] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain, Kusha Sareen, Mohammad Pedramfar, Siamak Ravanbakhsh*

**主要类别:** cs.LG

**AI概要:** 通过重用过去计算，将推理时对齐问题视为搜索问题，提出树状方法（DTS和DTS$^\star$），在MNIST、CIFAR-10等任务上显著减少计算量并提升样本质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的引导方法在高噪声水平下价值估计不准确，并且未有效利用过去的运行信息来改进样本质量，导致计算效率低下。为解决这些问题，受蒙特卡洛树搜索成功的启发，将推理时间对齐视为一个重用过去计算的搜索问题。

**方法:** 引入基于树的方法，通过沿扩散链传播终端奖励，并随着每次额外生成迭代地改进价值估计，从奖励对齐的目标密度中采样。提出了两种方法：Diffusion Tree Sampling (DTS) 和其贪婪变体 Diffusion Tree Search (DTS$^\star$)。

**结果:** 在MNIST和CIFAR-10类条件生成任务中，DTS用至多10倍少的计算量达到最佳基线的FID；在文本到图像生成和语言补全任务中，DTS$^\star$用至多5倍少的计算量找到与最佳-of-N匹配的高奖励样本。

**结论:** 提出的DTS和DTS$^\star$方法提供了一种可扩展的扩散模型推理时间对齐方法，能够通过额外计算逐步改善样本质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Tree+Sampling%3A+Scalable+inference-time+alignment+of+diffusion+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20701&send_immediately=true&force_search=false)

**原文摘要:** Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [5] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung, Rasa Hosseinzadeh, Gabriel Loaiza-Ganem*

**主要类别:** cs.LG

**AI概要:** 这篇论文主要探讨了扩散模型（DMs）在学习高维数据空间中低维子流形的能力，并提出了一个名为FLIPD的局部固有维度（LID）估计器。作者通过理论证明，验证了FLIPD在更现实假设下的正确性，并讨论了将其从高斯卷积扩展到均匀卷积的类似结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已知扩散模型可以学习低维分布支持的数据，但目前关于FLIPD（一种基于扩散模型的LID估计器）的理论基础仍不完善，因为之前的证明仅基于不切实际的仿射子流形假设。因此，需要对FLIPD的正确性进行更广泛的理论验证，以增强其应用的可靠性。

**方法:** 作者通过引入更现实的假设，形式化地证明了FLIPD在非仿射子流形情况下的正确性。此外，还研究了当高斯卷积被均匀卷积替代时，类似的理论结果是否仍然成立，并分析了该结果的意义。

**结果:** 作者成功地在更一般的假设条件下证明了FLIPD的正确性，弥补了先前理论的不足。同时，他们展示了均匀卷积的等效结果，进一步拓展了LID估计器的理论框架。

**结论:** 本研究为FLIPD提供了坚实的理论基础，并扩展了其适用范围。这不仅增强了FLIPD作为LID估计器的可信度，还为进一步研究扩散模型和LID估计器之间的关系奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Convolutions%2C+Intrinsic+Dimension%2C+and+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20705&send_immediately=true&force_search=false)

**原文摘要:** The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [6] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao, Tianyi Li, Yurii Kvasiuk, Sai Chaitanya Tadepalli, Maja Rudolph, Daniel J. H. Chung, Frederic Sala, Moritz Münchmeyer*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了数学推理基准测试中学到的经验是否适用于高级理论物理领域，并提出了一种新的符号弱验证框架，该方法在TPBench物理数据集和AIME数学基准上均表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在复杂推理方面表现出强大的能力，并且测试时扩展技术能以较低成本提升其性能，但这些方法主要是在数学推理基准（如AIME）上开发和评估的。本研究旨在探讨这些经验是否可以推广到高级理论物理领域。

**方法:** 研究者在TPBench物理数据集上评估了一系列常见的测试时扩展方法，并与AIME上的结果进行比较。此外，为了更好地利用物理问题的结构，研究者开发了一种新的符号弱验证框架，以改进并行扩展的结果。

**结果:** 实验结果表明，新提出的符号弱验证框架在TPBench上的表现显著优于现有的测试时扩展方法。同时，在AIME上的评估也证实了该方法在解决高级数学问题方面的有效性。

**结论:** 研究表明，逐步符号验证对于解决复杂的科学问题具有强大的作用，所提出的方法不仅适用于高级理论物理，也能有效解决高级数学问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-time+Scaling+Techniques+in+Theoretical+Physics+--+A+Comparison+of+Methods+on+the+TPBench+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20729&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [7] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van, Prateek Verma, Chen Zhao, Xintao Wu*

**主要类别:** cs.LG

**AI概要:** Foundation models (FMs) are transforming materials science by providing scalable, general-purpose AI systems. This survey covers their applications, recent advances, challenges, and future directions.


<details>
  <summary>更多</summary>
  
**动机:** To provide a comprehensive overview of foundation models in materials science, highlighting their capabilities, supporting tools, and datasets, while identifying limitations and outlining future research directions.

**方法:** A task-driven taxonomy is introduced, encompassing six application areas: data extraction, atomistic simulation, property prediction, materials design, process planning, and multiscale modeling. The survey discusses unimodal and multimodal FMs, LLM agents, standardized datasets, open-source tools, and experimental platforms.

**结果:** Foundation models show early successes but face challenges such as generalizability, interpretability, data imbalance, safety, and limited multimodal fusion.

**结论:** Future research should focus on scalable pretraining, continual learning, data governance, and trustworthiness to overcome current limitations and fully integrate FMs into materials science workflows.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+AI+for+Materials+Science%3A+Foundation+Models%2C+LLM+Agents%2C+Datasets%2C+and+Tools，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20743，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20743&send_immediately=true&force_search=false)

**原文摘要:** Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [8] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief, David Reber, Sean Richardson, Ari Holtzman*

**主要类别:** cs.LG

**AI概要:** 在微调期间，大型语言模型(LLM)学习的关系信息会被提取并在生成预测时被回忆。通过动态权重移植方法，研究发现模型在处理实体时提取关系信息，并在后续层中回忆这些信息以生成预测。不同情况下，模型可能需要一个或两个信息通路来正确生成微调信息。


<details>
  <summary>更多</summary>
  
**动机:** 了解大型语言模型在微调期间学习到的关系信息是如何存储和使用的。现有方法（如激活修补）不适合这种分析，因为它们可能会删除信息的一部分。

**方法:** 提出了一种动态权重移植方法，在微调和预训练的语言模型之间进行比较，以展示微调语言模型如何在处理实体时提取关系信息，并在生成预测时回忆这些信息。

**结果:** 微调语言模型在处理实体时提取关系信息，并在生成预测时回忆这些信息。有些情况下，模型需要这两个通路来正确生成微调信息；而在其他情况下，单一的“丰富”或“回忆”通路就足够了。

**结论:** 微调语言模型通过任务特定的注意力机制和关系提取步骤，在生成预测时回忆信息。这些信息通路出现在不同的层中，表现出一定的冗余性，并涉及模型的不同组件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiple+Streams+of+Relation+Extraction%3A+Enriching+and+Recalling+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20746&send_immediately=true&force_search=false)

**原文摘要:** When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [9] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek, Guy Zamir, Yudong Chen*

**主要类别:** cs.LG

**AI概要:** 本文研究了平均奖励马尔可夫决策过程中的离线强化学习问题，提出了基于悲观折扣值迭代的算法，并引入新的量化技术以减少复杂度和样本需求。该研究首次实现了仅依赖目标策略的完全单一策略样本复杂度边界，并适用于通用弱通信MDPs。此外，通过困难实例证明了超越目标策略平稳分布的覆盖假设的必要性，同时开发了几乎匹配主要结果的下界。


<details>
  <summary>更多</summary>
  
**动机:** 离线强化学习在平均奖励马尔可夫决策过程中面临分布偏移和非均匀覆盖等挑战，且理论研究相对较少。已有工作通常需要单策略数据覆盖假设，并使用对所有策略统一的复杂度度量（如均匀混合时间），这限制了其适用性和效率。

**方法:** 作者提出了一种基于悲观折扣值迭代的算法，结合一种新的分位数裁剪技术，从而可以使用更精确的经验跨度为基础的惩罚函数。该方法无需任何先验参数知识即可实现，并依赖于目标策略的具体偏差跨度和新提出的策略命中半径。

**结果:** 该算法首次实现了仅依赖目标策略的完全单一策略样本复杂度边界，并能够处理一般的弱通信马尔可夫决策过程。通过构造困难实例，进一步验证了超越目标策略平稳分布的覆盖假设的必要性，并开发了几乎与主要结果相匹配的下界。

**结论:** 本文为平均奖励离线强化学习提供了新的理论框架和算法设计，显著减少了样本复杂度并扩展了适用范围。研究成果强调了单一策略复杂度度量的独特性，并为进一步优化离线强化学习算法提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Single-Policy+Sample+Complexity+and+Transient+Coverage+for+Average-Reward+Offline+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20904&send_immediately=true&force_search=false)

**原文摘要:** We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [10] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su, Mujin Kwun, Stephanie Gil, Sham Kakade, Nikhil Anand*

**主要类别:** cs.LG

**AI概要:** 本论文研究了在模型训练中使用块缩放精度格式（如NVIDIA Blackwell架构中的Microscaling (MX) 格式）的挑战和可行性。通过近一千个从头开始训练的语言模型，作者观察到在较大计算规模下，MX格式训练表现出尖锐且随机的损失不稳定现象。通过控制实验和消融分析，作者提出了一种简单模型解释这种不稳定性，并展示了通过训练中修改精度方案可以避免或延迟这些不稳定性。最终，作者评估了稳定化策略，发现某些混合配置可以恢复与全精度训练相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着大规模语言模型的扩展、算法改进和新数据的收集，训练过程变得昂贵且计算密集。为了应对这一问题，下一代硬件加速器支持更低精度的算术格式（如MX格式），以提高效率。然而，这种低精度格式在训练过程中可能会引发不稳定性，因此需要研究其可行性和潜在解决方案。

**方法:** 1. 使用近一千个从头开始训练的语言模型，涵盖不同的计算预算和权重-激活精度组合。
2. 观察MX格式训练中的损失不稳定性，特别是在较大计算规模下。
3. 在较小的代理模型上进行受控实验和消融分析，探索架构设置、超参数和精度格式的影响。
4. 提出一种简单模型，解释量化引入的乘法梯度偏差如何导致失控发散。
5. 通过训练中的干预实验验证修改精度方案对不稳定性的影响。
6. 在LLM设置中评估稳定化策略，测试混合配置的效果。

**结果:** 1. 发现MX格式训练在较大计算规模下表现出尖锐且随机的损失不稳定现象。
2. 通过控制实验验证了量化引入的乘法梯度偏差是导致不稳定性的重要因素。
3. 展示了通过训练中修改精度方案可以避免或延迟不稳定性。
4. 某些混合配置能够恢复与全精度训练相当的性能。

**结论:** 块缩放精度格式（如MX格式）在训练大规模语言模型时面临不稳定性挑战，但通过适当的稳定化策略和混合配置，可以在一定程度上缓解这些问题并实现与全精度训练相当的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Characterization+and+Mitigation+of+Training+Instabilities+in+Microscaling+Formats，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20752&send_immediately=true&force_search=false)

**原文摘要:** Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [11] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan, Peng Wang, Jing Yang, Cong Shen*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的浅层Transformer框架CHOOSE，用于无线符号检测，通过在隐藏空间中引入自回归潜在推理步骤，提高了浅层模型的推理能力，使其性能可与深层模型媲美，同时保持存储和计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于ICL的Transformer模型虽然有效，但依赖于深层架构，导致存储和计算成本高，不适用于资源受限的设备。

**方法:** 设计了CHOOSE框架，通过在隐藏空间中引入自回归潜在推理步骤，增强了浅层Transformer（1-2层）的推理能力，无需增加模型深度。

**结果:** 实验结果表明，该方法优于传统的浅层Transformer，并且在保持存储和计算效率的同时，其性能可与深层Transformer相媲美。

**结论:** CHOOSE为在计算资源有限的无线接收器中实现基于Transformer的算法提供了一个有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chain-of-Thought+Enhanced+Shallow+Transformers+for+Wireless+Symbol+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21093&send_immediately=true&force_search=false)

**原文摘要:** Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [12] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong, Huchen Yang, Jin-Long Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种潜在分数生成式AI框架，用于学习计算力学非线性动力系统中的随机、非局部闭合模型和本构定律。通过联合训练卷积自动编码器与条件扩散模型，显著降低了采样过程的维度，同时保留了关键物理特性。实验结果表明该方法能有效发现合适的潜在空间，保证小的重建误差，并在潜在空间中确保扩散模型的良好性能。将此框架集成到数值模拟中，可实现显著的计算加速，同时保持与标准扩散模型相当的预测精度。


<details>
  <summary>更多</summary>
  
**动机:** 建模复杂多尺度动力系统（如工程湍流）面临挑战，因为解析所有尺度在计算上过于昂贵。经典闭合模型依赖领域知识近似亚网格现象，但在缺乏明确尺度分离的情况下，其确定性和局部假设可能过于限制。尽管基于扩散的随机模型在闭合建模方面显示出潜力，但其高昂的计算推断成本限制了实际应用。

**方法:** 提出了一个潜在分数生成式AI框架，联合训练卷积自动编码器和条件扩散模型。通过降低采样过程的维度，同时保留关键物理特性，解决了扩散模型计算成本过高的问题。

**结果:** 数值结果证明了联合训练方法能够发现合适的潜在空间，不仅保证了小的重建误差，而且确保了扩散模型在潜在空间中的良好性能。将该框架集成到数值模拟中，实现了显著的计算加速，同时保持了与标准扩散模型相当的预测精度。

**结论:** 所提出的潜在分数生成式AI框架为解决复杂多尺度动力系统的建模问题提供了一种新的解决方案，能够在显著降低计算成本的同时，保持较高的预测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+and+Non-local+Closure+Modeling+for+Nonlinear+Dynamical+Systems+via+Latent+Score-based+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20771&send_immediately=true&force_search=false)

**原文摘要:** We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [13] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler, Florian Lemmerich*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于线性度的神经网络压缩新方法，通过合并几乎总是线性激活的神经元来减少权重，实验表明该方法可以在大多数测试模型中实现无损压缩至原模型大小的1/4，并且与现有的重要性剪枝方法结合良好。


<details>
  <summary>更多</summary>
  
**动机:** 当前神经网络压缩方法主要通过测量参数的重要性与冗余度来减少不必要的参数，但仍有进一步优化的空间。

**方法:** 提出一种基于线性度的压缩方法，利用ReLU类激活函数的特点，识别几乎总是线性行为的神经元，进而合并后续层以减少权重。

**结果:** 在多数测试模型中，该方法实现了无损压缩至原模型大小的1/4；与基于重要性的剪枝模型结合时，表现出很小的干扰，具有良好的兼容性。

**结论:** 本研究为神经网络压缩提供了一种新的方法，可有效减小模型规模并提升效率，同时能够与其他技术成功结合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Linearity-based+neural+network+compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21146&send_immediately=true&force_search=false)

**原文摘要:** In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [14] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq, Dan Braun, Lee Sharkey*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的神经网络参数分解方法Stochastic Parameter Decomposition (SPD)，相较于现有方法Attribution-based Parameter Decomposition (APD)，SPD在计算效率和鲁棒性方面有显著提升。该方法能够避免参数收缩问题，并在玩具模型中更好地识别真实机制，为更大规模模型的线性参数分解提供了新可能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经网络分解方法存在计算成本高、对超参数敏感的问题，特别是Attribution-based Parameter Decomposition (APD)方法在实际应用中受限。因此，需要一种更高效、更鲁棒的分解方法来研究复杂神经网络。

**方法:** 引入了Stochastic Parameter Decomposition (SPD) 方法，该方法通过将神经网络参数分解为参数空间中稀疏使用的向量之和，解决了APD方法计算成本高和对超参数敏感的问题。SPD还避免了参数收缩等其他问题，并能更好地识别玩具模型中的真实机制。

**结果:** SPD成功分解了比APD方法可处理的规模稍大且更复杂的模型，验证了其在扩展性和鲁棒性方面的优势。此外，SPD在玩具模型中表现优异，能够更准确地识别出真实的因果机制。

**结论:** Stochastic Parameter Decomposition (SPD) 提供了一种更高效的神经网络参数分解方法，克服了现有方法的局限性，为大规模模型的机制解释性研究铺平了道路。同时，相关代码已开源以促进进一步研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Parameter+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20790&send_immediately=true&force_search=false)

**原文摘要:** A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [15] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews, Sam Witteveen*

**主要类别:** cs.LG

**AI概要:** 本研究介绍了一种基于LLM的自动化方法，用于优化GPU内核。通过多阶段进化过程，包括选择有潜力的代码版本、生成优化假设并自主实现实验，这种方法能够在资源受限或快速变化的硬件环境中加速GPU内核优化。


<details>
  <summary>更多</summary>
  
**动机:** 优化GPU内核以实现高性能是一项复杂任务，需要深入的架构知识、广泛的分析和迭代实验，尤其是在针对更新或文档较少的GPU架构时，传统开发辅助工具稀缺。

**方法:** 该方法使用LLM进行多阶段进化过程：(a) 选择有潜力的先前代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中获取的知识生成优化实验假设；(c) 通过代码修改和提交外部评估系统自主实施这些实验，仅使用观察到的时间数据作为性能反馈。

**结果:** 由于在论文提交日期前定量结果被禁运，作者展示了架构设计、操作流程和定性见解，强调了LLM驱动代理在民主化和加速GPU内核优化方面的潜力。

**结论:** LLM驱动的方法能够补偿特定领域人类专家的有限知识，尤其在资源受限或硬件快速发展的环境中，展现出巨大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GPU+Kernel+Scientist%3A+An+LLM-Driven+Framework+for+Iterative+Kernel+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20807&send_immediately=true&force_search=false)

**原文摘要:** Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [16] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal, Jakoba Petri-Koenig, Thomas B. Preußer, Michaela Blott, Shreejith Shanker*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于FINN框架的方法，利用FPGA加速LSTM网络。通过引入ONNX规范中的Scan操作符和FINN编译器的自定义转换，支持混合量化和功能验证，生成了用于中期股票价格预测任务的ConvLSTM加速器，实现了性能与资源消耗的平衡，同时保持或提高了推理精度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的FPGA工具主要针对前馈网络，而LSTM加速通常需要完全定制实现，难以在资源受限环境中实现实时部署。

**方法:** 使用FINN框架和ONNX规范中的Scan操作符来模拟LSTM计算的递归性质，支持混合量化和功能验证；在FINN编译器中引入自定义转换，将量化后的ONNX计算图映射到HLS内核库中的硬件块；训练一个量化的ConvLSTM模型进行中期股票价格预测，并生成相应的硬件IP。

**结果:** 生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时以降低精度为代价匹配或超越最先进的模型的推理准确性。

**结论:** 所提出的流程具有通用性，将为FPGA上高效的RNN加速器设计铺平道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FINN-GL%3A+Generalized+Mixed-Precision+Extensions+for+FPGA-Accelerated+LSTMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20810&send_immediately=true&force_search=false)

**原文摘要:** Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [17] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko, Jędrzej Ruciński, Dawid Płudowski, Antoni Zajko, Patryzja Żak, Mateusz Zacharecki, Anna Kozak, Katarzyna Woźnica*

**主要类别:** cs.LG

**AI概要:** Hellsemble是一种新的集成框架，通过利用数据集复杂性来提高二分类的预测性能，同时保持计算效率和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的集成方法（如bagging、boosting和DES）在计算成本高且对异构数据分布适应性有限的问题上存在不足。

**方法:** Hellsemble通过将数据集按难度逐步划分为不同的'圈层'，并将错误分类的实例从简单模型传递给后续模型，形成一个专门化的基础学习者委员会。此外，一个独立的路由模型负责根据推断出的难度将新实例分配给最合适的基础模型。

**结果:** 实验结果表明，在OpenML-CC18和Tabzilla基准测试中，Hellsemble通常优于经典的集成方法。

**结论:** 研究结果表明，采用实例级别的难度为构建高效和鲁棒的集成系统提供了一个有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Divide%2C+Specialize%2C+and+Route%3A+A+New+Approach+to+Efficient+Ensemble+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20814&send_immediately=true&force_search=false)

**原文摘要:** Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [18] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu, Yasin Yilmaz*

**主要类别:** cs.LG

**AI概要:** 深度神经网络（DNNs）易受对抗样本攻击，尽管已有许多针对改进输入的微妙修改的攻击方法，但防御技术仍相对不足。本文研究了通过检测对抗攻击的防御方法，提出了一种新颖且高效的检测方法，该方法通过分析不同DNN层对攻击影响的不同程度来检测对抗样本。具体来说，我们的方法训练一个轻量级回归模型，从早期层特征预测深层特征，并使用预测误差来检测对抗样本。实验表明，该方法具有高度有效性、实时处理效率高、与任何DNN架构兼容，并可应用于图像、视频和音频等多个领域。


<details>
  <summary>更多</summary>
  
**动机:** 现有的防御方法要么专注于提高DNN的鲁棒性以抵消扰动的影响，要么使用辅助模型来检测对抗数据。然而，这些方法可能对最先进的攻击技术无效或计算效率低下，无法进行实时处理。因此，需要一种新的、有效的、高效的对抗样本检测方法。

**方法:** 提出了一种新的通用且高效的方法，通过分析攻击对不同DNN层影响的程度来检测对抗样本。具体而言，该方法训练一个轻量级回归模型，从早期层特征预测深层特征，并利用预测误差检测对抗样本。

**结果:** 通过理论分析和广泛的实验验证，证明了所提出的方法在检测对抗样本方面具有高度有效性，同时具备实时处理的计算效率，与任何DNN架构兼容，并可跨多个领域应用，如图像、视频和音频。

**结论:** 本文提出的对抗样本检测方法是一种通用且高效的解决方案，适用于各种DNN架构和多种数据类型，为实时防御对抗攻击提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+and+Efficient+Detection+of+Adversarial+Data+through+Nonuniform+Impact+on+Network+Layers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20816&send_immediately=true&force_search=false)

**原文摘要:** Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [19] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang, Chul-Ho Lee*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了分布式图神经网络（GNN）在链路预测中的性能下降问题，并提出了一种名为SpLPG的方法，通过图稀疏化技术以较低的通信成本缓解了这一问题。实验表明，SpLPG能减少高达约80%的通信开销，同时基本保持链路预测的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 分布式GNN框架和系统增强了GNN的可扩展性并加速模型训练，但其在链路预测任务上的表现尚未得到充分研究。特别是在每个工作节点仅基于分配的子图进行训练时，会出现性能下降的问题，需要找到一种有效方法来解决这一问题，同时降低通信成本。

**方法:** 作者提出了SpLPG方法，该方法利用图稀疏化技术，在降低通信成本的同时缓解了由于图划分导致的信息丢失以及负采样方式不当带来的性能下降问题。

**结果:** 实验结果表明，SpLPG方法能够在多个公开的真实世界数据集上有效减少通信开销（最多减少约80%），并且基本保持链路预测的准确性。

**结论:** SpLPG是一种有效的解决方案，可以在分布式GNN链路预测任务中显著降低通信成本，同时缓解性能下降问题。这为实际应用中的大规模分布式GNN训练提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+Distributed+Training+of+Graph+Neural+Networks+for+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20818&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [20] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** A constrained deep reinforcement learning (CDRL) approach is proposed to optimize time allocation for tracking and communication in radar-communication systems, improving communication quality within time constraints.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of efficiently allocating limited time resources between target tracking and communication in dual-functional radar-communication systems.

**方法:** A novel constrained deep reinforcement learning (CDRL) approach is introduced to optimize resource allocation under time budget constraints.

**结果:** Numerical results show that the CDRL framework can effectively maximize communication quality while respecting time limitations in dynamic environments.

**结论:** The CDRL method successfully enhances communication quality in radar-communication systems by optimally allocating time between tracking and communication tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning-Based+Resource+Management+in+Integrated+Sensing+and+Communication+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20849&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [21] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu, Subodh Kalia, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了多功能认知雷达系统中的时间分配问题，提出了一个多目标优化问题，并使用深度强化学习方法（DDPG和SAC）寻找Pareto最优解。实验结果表明SAC比DDPG更稳定且采样效率更高。此外，还使用NSGA-II算法估计了问题的Pareto前沿的上界，为开发更高效、适应性更强的认知雷达系统提供了基础。


<details>
  <summary>更多</summary>
  
**动机:** 多功能认知雷达系统需要在扫描新出现的目标和跟踪已检测到的目标之间进行权衡，这促使研究者将这一时间分配问题建模为一个多目标优化问题，以期提高雷达系统的效率和适应性。

**方法:** 作者采用深度强化学习技术来解决时间分配问题，具体比较了DDPG和SAC两种算法的性能。同时，使用NSGA-II算法估计该多目标优化问题的Pareto前沿的上界。

**结果:** 两种算法（DDPG和SAC）都能有效适应不同场景，但SAC表现出更高的稳定性和样本效率。NSGA-II算法成功估计了Pareto前沿的上界，验证了方法的有效性。

**结论:** 通过深度强化学习和NSGA-II算法，本研究为实现更高效、更具适应性的认知雷达系统提供了一种可行的方法，能够更好地平衡动态环境中的多个竞争目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Objective+Reinforcement+Learning+for+Cognitive+Radar+Resource+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20853&send_immediately=true&force_search=false)

**原文摘要:** The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [22] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang, Baochun Li*

**主要类别:** cs.LG

**AI概要:** 本文重新审视了微调中的记忆现象，发现LoRA微调在记忆效应上与先前的研究结果存在差异，并且相比完全微调，LoRA显著降低了记忆风险，同时保持了良好的任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）由于记忆训练数据而容易受到数据提取攻击，但目前对微调阶段的记忆影响研究较少，特别是针对广泛使用的低秩适应（LoRA）方法。因此需要进一步探索不同微调策略下的记忆效应及其影响因素。

**方法:** 作者通过重新评估微调过程中的记忆现象，比较了不同微调策略（如LoRA微调和全量微调）在记忆效应上的差异。使用一种基于相似性的宽松记忆度量标准，分析了模型规模、数据重复等因素对记忆的影响。

**结果:** 研究表明，在LoRA微调中，模型规模和数据重复等传统影响记忆的因素并未表现出相同的趋势。相比全量微调，LoRA微调显著降低了记忆风险，同时仍能保持强大的任务性能。

**结论:** LoRA微调作为一种参数高效的微调方法，在降低记忆风险方面表现优异，为提高模型安全性和减少数据提取攻击提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leaner+Training%2C+Lower+Leakage%3A+Revisiting+Memorization+in+LLM+Fine-Tuning+with+LoRA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20856&send_immediately=true&force_search=false)

**原文摘要:** Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [23] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery*

**主要类别:** cs.LG

**AI概要:** 本研究提出了Omniwise，一种端到端、自监督的微调管道，首次将大语言模型应用于GPU内核性能预测。通过轻量级设计和模型无关性，即使使用3B参数的小型模型也能准确预测多个性能指标，误差控制在10%以内，并集成了在线推理服务器和VS Code插件以方便开发者使用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度神经网络在许多领域取得了巨大成功，但现有的GPU性能预测方法通常依赖于代码执行或专用工具，效率较低且不够灵活。因此，需要一种新的方法来直接从代码中预测性能指标，而无需实际运行代码或使用复杂工具。

**方法:** 提出了一种名为Omniwise的端到端、自监督微调管道，利用大语言模型（LLM）进行GPU内核性能预测。该方法无需执行代码或使用专门的分析工具，能够直接从内核代码中预测关键性能指标（如内存带宽、缓存命中率等）。此外，还开发了在线推理服务器和VS Code插件，便于开发者集成到工作流程中。

**结果:** 实验结果表明，在AMD MI250和MI300X架构上运行的GPU内核中，超过90%的预测值相对误差小于10%，证明了该方法的高精度和有效性。

**结论:** Omniwise作为一种轻量级、模型无关的解决方案，成功地将大语言模型应用于GPU内核性能预测，显著提高了预测精度和效率，同时简化了开发者的使用流程。这一创新为性能分析领域开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omniwise%3A+Predicting+GPU+Kernels+Performance+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20886&send_immediately=true&force_search=false)

**原文摘要:** In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [24] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang, Ali Ebrahimpour-Boroojeny, Hari Sundaram*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为RWFT的输出重加权遗忘方法，该方法能够在无需完全重新训练的情况下从已训练好的分类器中删除整个类。通过引入新的基于总变异（TV）距离的度量标准来量化残余泄漏，并防止未来的方法受到新攻击的影响。与现有最先进方法相比，在先前使用的度量标准上提高了2.79%，在新提出的TV基度量标准上提高了111.45%。


<details>
  <summary>更多</summary>
  
**动机:** 从训练好的模型中遗忘特定类对于执行用户删除权利和减少有害或有偏见的预测至关重要。然而，完整重新训练成本高昂，现有的遗忘方法无法在预测未学习类样本时复制重新训练模型的行为。

**方法:** 设计了一个成员推断攻击变体MIA-NN，揭示现有方法在遗忘类上的失败。提出了对遗忘类样本预测的概率质量进行简单再分配，使其对MIA-NN具有鲁棒性。并引入了基于总变异（TV）距离的新度量标准，以量化残余泄漏。

**结果:** 通过与机器遗忘领域的最新基线进行广泛实验比较，表明该方法在先前工作用于评估的两种度量标准上均达到完全重新训练的结果。

**结论:** RWFT方法不仅在先前度量标准上表现优异，还在新提出的TV基度量标准上大幅超越现有最佳方法，有效实现了类遗忘且防止信息泄露。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Necessity+of+Output+Distribution+Reweighting+for+Effective+Class+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20893&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [25] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi, Yanning Shen*

**主要类别:** cs.LG

**AI概要:** 在线共形预测方法通过从有效模型子集中选择模型，结合反馈机制和预测集大小优化，实现了更小的预测集和有效的覆盖率保证。实验表明该方法优于现有的多模型在线共形预测方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模型在线共形预测虽然提高了灵活性，但预选模型集的选择带来了挑战：过多模型增加计算复杂度，而性能差的无关模型可能影响效果并导致过大的预测集。

**方法:** 提出了一种新的多模型在线共形预测算法，利用二分图收集反馈，在每个时间步识别出有效模型子集，并从中选择模型构建预测集。同时使用预测集大小和模型损失作为反馈，以提高效率。

**结果:** 理论证明该算法能够确保有效的覆盖率并实现次线性后悔界。实验结果表明，所提方法构建的预测集更小，且在真实和合成数据集上均优于现有方法。

**结论:** 新提出的多模型在线共形预测算法通过动态选择有效模型子集，显著降低了计算复杂度和预测集大小，同时保持了所需的覆盖率保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Structured+Feedback+Multimodel+Ensemble+Online+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20898&send_immediately=true&force_search=false)

**原文摘要:** Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [26] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于多智能体大语言模型（LLM）的框架，用于从简要过程描述中自动推断操作约束，并通过协作优化化学生产过程。该框架在氢脱烷基化过程中验证了其性能，相较于传统方法具有更高的计算效率和更快的收敛速度，同时展示了对过程理解的深度和正确识别效用权衡的能力。


<details>
  <summary>更多</summary>
  
**动机:** 传统的化学过程优化方法在操作约束不明确或不可用时变得不切实际，工程师需要依赖主观启发式方法估计可行参数范围。为了解决这一瓶颈，需要一种可以自主推断操作约束并指导优化的方法。

**方法:** 提出了一个多智能体框架，使用大语言模型（LLM）代理从最少的过程描述中自主推断操作约束，并通过这些推断出的约束协作引导优化。框架包含专门用于约束生成、参数验证、模拟执行和优化指导的代理，采用OpenAI的o3模型。优化过程分为两个阶段：自主约束生成和迭代多智能体优化。

**结果:** 在氢脱烷基化过程中的验证表明，该框架在成本、产率和产率比成本等指标上表现出与传统优化方法竞争的性能，同时具有更好的计算效率，仅需较少的迭代次数即可收敛。实验中，该方法在不到20分钟内收敛，相较于网格搜索提高了31倍的速度。

**结论:** 提出的多智能体框架展示了在操作约束不明确或不可用情况下的显著潜力，特别是在新兴过程和改造应用中，能够提高优化效率并减少计算资源需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-guided+Chemical+Process+Optimization+with+a+Multi-Agent+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20921，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20921&send_immediately=true&force_search=false)

**原文摘要:** Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [27] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh, Pierre Le Bodic, Geoffrey I. Webb, Mario Boley*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Representation+Learning+for+Additive+Rule+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20927&send_immediately=true&force_search=false)

**原文摘要:** Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [28] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种改进的LIME方法，称为DL-LIME，该方法将深度学习集成到采样过程中，并应用于雷达资源管理的深度强化学习中。实验结果表明，DL-LIME在保真度和任务性能方面均优于传统LIME，并揭示了雷达资源管理决策中的关键因素。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络的“黑箱”特性限制了其在需要解释性场景中的应用，而现有的LIME方法在采样过程中忽略了特征之间的相关性，因此需要一种改进的方法来提高解释性和性能。

**方法:** 提出了一种名为DL-LIME的方法，通过将深度学习集成到LIME的采样过程中，以考虑特征之间的相关性。此方法被应用于雷达资源管理的深度强化学习中。

**结果:** 数值结果显示，DL-LIME在保真度和任务性能上均优于传统的LIME方法，并且能够提供有关雷达资源管理决策中哪些因素更重要的见解。

**结论:** DL-LIME不仅提高了解释性AI技术的性能，还为理解雷达资源管理中的决策过程提供了有价值的洞察。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+AI+for+Radar+Resource+Management%3A+Modified+LIME+in+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20916&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [29] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen, Xiantao Cai, Jia Wu, Wenbin Hu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为AbMEGD的端到端框架，用于抗体序列和结构的共同设计。该方法通过多尺度等变图扩散技术结合原子级几何特征与残基级嵌入，确保了几何精度、计算效率和对复杂抗原的强大泛化能力。实验表明，与现有模型相比，AbMEGD在关键CDR-H3区域中显著提高了氨基酸恢复率、改善百分比并降低了均方根偏差。


<details>
  <summary>更多</summary>
  
**动机:** 当前计算方法在抗体设计领域存在两大局限性：（1）捕捉几何特征同时保持对称性；（2）推广新型抗原界面。这些方法往往无法准确捕获分子相互作用并维持结构完整性，特别是在处理具有多样化结合界面的复杂抗原时。

**方法:** AbMEGD是一个整合了多尺度等变图扩散技术的端到端框架。它利用先进的几何深度学习技术，将原子级几何特征与残基级嵌入相结合，从而捕捉局部原子细节和全局序列-结构相互作用。其E(3)-等变扩散方法确保了几何精度、计算效率和对复杂抗原的强大泛化能力。

**结果:** 使用SAbDab数据库进行的实验显示，与领先的抗体设计模型DiffAb相比，AbMEGD实现了以下改进：氨基酸恢复率增加10.13%，改善百分比提升3.32%，关键CDR-H3区域内的均方根偏差降低0.062 Å。

**结论:** AbMEGD展示了在平衡结构完整性和提高功能方面的卓越能力，为序列-结构共同设计和亲和力优化设定了新的基准。代码已在GitHub上公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Antibody+Design+and+Optimization+with+Multi-scale+Equivariant+Graph+Diffusion+Models+for+Accurate+Complex+Antigen+Binding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20957&send_immediately=true&force_search=false)

**原文摘要:** Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [30] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han*

**主要类别:** cs.LG

**AI概要:** 论文提出了Strict Subgoal Execution (SSE)，一个用于解决长时域目标条件任务的图基分层强化学习框架，通过结构化约束高层决策、采用分离探索策略和失败感知路径细化等技术，在多种基准测试中超越现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 长时域目标条件任务对于强化学习来说是一个基本挑战，特别是当目标遥远且奖励稀疏时。虽然分层和基于图的方法提供了部分解决方案，但它们通常遭受子目标不可行性和规划效率低下的问题。

**方法:** 我们引入了严格的子目标执行（SSE），这是一种基于图的分层强化学习框架，通过结构化约束高层决策来强制执行单步子目标可达性。为了增强探索，SSE采用了一种分离的探索策略，系统地遍历目标空间中未充分探索的区域。此外，一种失败感知路径细化方法通过根据观察到的低级成功率动态调整边成本来改进基于图的规划，从而提高子目标的可靠性。

**结果:** 在各种长时域基准测试中的实验结果表明，SSE在效率和成功率方面始终优于现有的目标条件强化学习和分层强化学习方法。

**结论:** SSE是一种有效的基于图的分层RL框架，能通过结构化约束、分离探索策略和失败感知路径细化来解决子目标不可达性和规划效率低下的问题，并在长时域任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strict+Subgoal+Execution%3A+Reliable+Long-Horizon+Planning+in+Hierarchical+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21039&send_immediately=true&force_search=false)

**原文摘要:** Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [31] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang, Ming Zhou, Shaopeng Zhai, Ying Sun, Hui Xiong*

**主要类别:** cs.LG

**AI概要:** 在无监督技能发现领域，本文提出了一种基于后悔感知的方法，通过将技能发现视为生成与策略学习的极小极大博弈问题，有效提升了高维环境下的效率和多样性。实验表明该方法在零样本学习中比现有方法高出15%。


<details>
  <summary>更多</summary>
  
**动机:** 当前无监督技能发现方法虽然在探索多样性方面表现出色，但在高维情况下效率有限。需要一种更高效的技能发现方法以应对复杂环境挑战。

**方法:** 将技能发现建模为技能生成与策略学习的极小极大博弈，并引入后悔感知机制指导技能生成器的学习过程，优先探索较弱技能并减少对已收敛技能的探索。技能生成器采用可升级群体设计以避免退化。

**结果:** 在不同复杂度和维度的环境中进行实验，结果表明该方法在效率和多样性上均优于基线方法，尤其在高维环境下实现了15%的零样本性能提升。

**结论:** 提出的后悔感知技能发现方法能够显著提高无监督技能发现的效率和多样性，特别适用于高维环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Skill+Discovery+via+Regret-Aware+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21044&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [32] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei, Mehrdad Saberi, Abhilasha Ravichander, Soheil Feizi*

**主要类别:** cs.LG

**AI概要:** 大型语言模型可能受到包含在训练数据中的有问题的数据点（如私密数据、版权材料等）的影响。完全通过重新训练来消除这些影响计算成本过高，因此出现了机器遗忘算法。本文提出了一种新的算法MSA，利用模型检查点来估计和撤销数据点的影响。实验结果表明，MSA在多个基准测试中优于现有的机器遗忘算法，可能是实现更灵活的大规模语言模型的有效方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的训练数据可能包含有问题的数据，例如私密数据、受版权保护的材料或降低模型性能的数据。完全重新训练以消除这些数据点的影响计算上不可行，因此需要一种低计算成本的方法来消除特定数据点的影响，同时保留模型的其他部分。

**方法:** 本文提出了一种名为MSA的新算法，该算法通过利用模型检查点（即捕获预训练不同阶段模型状态的人工制品）来估计和撤销数据点的影响。

**结果:** 实验结果表明，MSA在多个基准测试、模型和评估指标上始终优于现有的机器遗忘算法。

**结论:** MSA可能是实现更灵活的大型语言模型的有效方法，这些模型能够进行数据擦除。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+State+Arithmetic+for+Machine+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20941&send_immediately=true&force_search=false)

**原文摘要:** Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [33] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann, Luca Corbucci, Mattia Cerrato, Anna Monreale*

**主要类别:** cs.LG

**AI概要:** Federated Learning (FL) is a method for collaborative model training without sharing private data. However, fairness remains a key concern as biases in local datasets can impact the entire federated system. This paper contributes three ways to support more robust and reproducible fairness research in FL: introducing FeDa4Fair library, releasing four bias-heterogeneous datasets and benchmarks, and providing ready-to-use functions for evaluating fairness outcomes.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper lies in addressing the limitations of existing fairness-enhancing solutions in Federated Learning, which mostly focus on mitigating bias for a single sensitive attribute while ignoring the diverse and sometimes conflicting fairness needs of different clients.

**方法:** This paper introduces FeDa4Fair, a library for generating tabular datasets tailored to evaluating fair FL methods under heterogeneous client bias. It also releases four bias-heterogeneous datasets and corresponding benchmarks for comparing fairness mitigation methods in a controlled environment. Ready-to-use functions are provided for evaluating fairness outcomes.

**结果:** The results include the creation of a library (FeDa4Fair), release of four datasets with benchmarks, and provision of evaluation functions that contribute to more robust and reproducible fairness research in FL.

**结论:** This paper concludes by contributing significantly to fairness research in FL through its novel library, datasets, benchmarks, and evaluation functions, enabling consistent benchmarking of fairness-aware FL methods at both global and client levels.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FeDa4Fair%3A+Client-Level+Federated+Datasets+for+Fairness+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21095&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [34] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的概念基础模型H-CMR，它通过学习的概念图和神经注意力机制来解释概念和任务预测。实验表明，该模型在性能上达到最先进水平，并且可以通过概念和模型干预显著提高准确性和训练的数据效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前的概念基础模型（CBMs）虽然提供了对最终任务预测的可解释性，但其对概念本身的预测通常是通过黑箱神经网络进行的，缺乏透明度。因此需要一种能够同时解释概念预测和任务预测的新模型。

**方法:** 提出了分层概念记忆推理器（H-CMR），这是一种新的CBM，利用学习到的有向无环图建模概念间的关系，其中边代表定义概念的逻辑规则。在推理过程中，H-CMR使用神经注意力机制选择一部分这些规则，以分层方式预测所有概念及最终任务。

**结果:** 实验结果证明，H-CMR达到了最先进的性能水平。此外，通过概念干预可以在推理时显著提高准确性，而通过模型干预则在训练时提高了数据效率（特别是在有背景知识的情况下）。

**结论:** H-CMR不仅实现了与现有最优方法相匹配的性能，还增强了人类与模型的交互能力，使得在推理和训练阶段都能通过干预提高模型的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Hierarchical+Concept+Reasoning+through+Attention-Guided+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21102&send_immediately=true&force_search=false)

**原文摘要:** Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [35] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang, Zhen Zhang, Rupak Vignesh Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang*

**主要类别:** cs.LG

**AI概要:** fine-tuning vision language models without backpropagation using SharpZO method


<details>
  <summary>更多</summary>
  
**动机:** Fine-tuning VLMs has great performance but needs backpropagation which is not suitable for edge devices with limited memory. Previous BP-free methods have limitations in performance.

**方法:** Propose SharpZO, a hybrid optimization approach with two stages: sharpness-aware ES for initialization and sparse ZO for local search. It only uses forward passes.

**结果:** SharpZO improves accuracy and convergence speed of CLIP models by up to 7% over current forward-only methods.

**结论:** SharpZO is an effective BP-free method for fine-tuning VLMs on edge devices.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SharpZO%3A+Hybrid+Sharpness-Aware+Vision+Language+Model+Prompt+Tuning+via+Forward-Only+Passes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20990&send_immediately=true&force_search=false)

**原文摘要:** Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [36] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** An antifragile RL framework with discounted Thompson sampling is proposed to enhance UAV navigation resilience against adversarial attacks, showing superior performance in simulations.


<details>
  <summary>更多</summary>
  
**动机:** Existing robust RL methods have limited generalization for out-of-distribution shifts due to adversarial attacks exploiting vulnerabilities through sensor manipulation.

**方法:** The paper introduces an antifragile RL framework that incorporates a switching mechanism based on discounted Thompson sampling. It derives a diverse ensemble of action robust policies and models them as a multi-armed bandit problem, enabling dynamic selection among multiple robust policies to minimize adversarially induced state-action-value distribution shifts.

**结果:** Extensive numerical simulations demonstrate the effectiveness of the proposed framework in complex navigation environments with dynamic obstacles and strong adversarial attacks. The antifragile approach achieves shorter navigation path lengths and higher conflict-free navigation rates compared to existing robust RL techniques.

**结论:** The antifragile RL framework effectively enhances adaptability to broader distributional shifts and improves resilience against unseen adversarial attacks, offering a promising solution for secure UAV navigation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Policy+Switching+for+Antifragile+Reinforcement+Learning+for+UAV+Deconfliction+in+Adversarial+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21127&send_immediately=true&force_search=false)

**原文摘要:** The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [37] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton, Valeriy Klyukin, Maksim Artemev, Denis Derkach, Nikita Orlov, Humphrey Shi*

**主要类别:** cs.LG

**AI概要:** 显式密度学习者因其对概率分布的更好建模能力而成为生成模型中越来越受欢迎的技术。尽管它们具有密度估计和精确的潜在变量推理的优势，但这些模型往往更难训练且采样质量较低。本文提出了一种新的知识蒸馏技术，以提高较小的学生正则流的采样质量和密度估计。通过蒸馏，可以在显著减小学生模型的同时获得实质性的性能提升，从而提高吞吐量。


<details>
  <summary>更多</summary>
  
**动机:** 显式密度模型（如正则流）在生成模型中有优势，例如可以进行密度估计和精确的潜在变量推断，但其训练难度较大且采样质量较低。因此，研究如何通过知识蒸馏技术改进较小的学生正则流的性能是有意义的。

**方法:** 使用新颖的知识蒸馏技术来提高较小的学生正则流的采样质量和密度估计能力。特别地，利用正则流的独特性质，实现非传统的知识传递形式，即在中间层之间进行知识转移。

**结果:** 通过知识蒸馏，可以使学生模型显著缩小，同时在性能上取得实质性提升。更小的模型带来了更高的吞吐量，因为吞吐量与网络中的双射器数量成反比。

**结论:** 知识蒸馏在组合正则流中具有显著的潜力，可以有效减少模型大小并提高性能，这为高效生成模型的设计提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilling+Normalizing+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21003&send_immediately=true&force_search=false)

**原文摘要:** Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [38] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda, Adolfo Perrusquia, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种抗脆弱强化学习框架，通过模拟攻击者逐步增加观测空间扰动强度，使RL代理适应更广泛的OOD观测并预测未见过的攻击。方法通过迭代专家引导的批评者对齐（使用Wasserstein距离最小化）来限制价值函数分布的变化。实验表明，在PGD和GPS欺骗攻击下，该策略比标准和鲁棒RL基线表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习（RL）在关键安全系统中的应用容易受到观察空间中的OOD对抗攻击，这些攻击会显著降低价值估计，导致不安全或次优决策。因此需要一种方法来增强RL政策的抗脆弱性。

**方法:** 提出抗脆弱RL框架，引入模拟攻击者逐步增加观察空间扰动强度，让RL代理适应更广泛的OOD观测，并预测未见过的攻击。定义了脆弱性和抗脆弱性，推导出遗忘稳定的适应条件，并通过迭代专家引导的批评者对齐（使用Wasserstein距离最小化）来限制价值函数分布的变化。

**结果:** 在涉及动态3D障碍物的无人机避碰场景中进行实证评估，结果表明在PGD和GPS欺骗攻击下，抗脆弱策略比标准和鲁棒RL基线高出15%的累积奖励，冲突事件减少超过30%。

**结论:** 抗脆弱强化学习在具有演变威胁情景的环境中提供了实际和理论上的可行性，用于安全和有弹性的决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Curriculum-Guided+Antifragile+Reinforcement+Learning+for+Secure+UAV+Deconfliction+under+Observation-Space+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21129&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [39] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang, Mangal Prakash, Hehuan Ma, Jianyuan Deng, Yuzhi Guo, Amina Mollaysa, Tommaso Mansi, Rui Liao, Junzhou Huang*

**主要类别:** cs.LG

**AI概要:** TRIDENT 是一个新框架，整合了分子 SMILES、文本描述和分类功能注释以学习丰富的分子表示，并在 11 个下游任务中达到最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多模态学习已成为学习分子表示的强大范式，但先前的研究大多忽略了分子的文本和分类信息对表示学习的作用。

**方法:** TRIDENT 利用基于体积的对齐目标在全球范围内联合对齐三模态特征，并引入了一种新的局部对齐目标，捕捉分子子结构与相应子文本描述之间的详细关系。通过动量机制动态平衡全局和局部对齐，模型可以学习广泛的功能语义和精细的结构-功能映射。

**结果:** TRIDENT 在 11 个下游任务上实现了最先进的性能，证明了结合 SMILES、文本和分类功能注释对分子性质预测的价值。

**结论:** 将文本和分类信息纳入分子表示学习可以显著提升分子性质预测的效果，TRIDENT 提供了一个有效的框架来实现这一点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRIDENT%3A+Tri-Modal+Molecular+Representation+Learning+with+Taxonomic+Annotations+and+Local+Correspondence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21028&send_immediately=true&force_search=false)

**原文摘要:** Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [40] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang, Hongbin Wang, Tianwang Jia, Xingyi He, Siyang Li, Dongrui Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为DBConformer的双分支卷积Transformer网络，专门用于脑电图（EEG）解码。它通过时间Conformer建模长时间依赖性，空间Conformer提取通道间交互，并使用轻量级通道注意力模块进一步优化空间表示。实验表明，DBConformer在五个运动想象数据集和两个癫痫检测数据集上表现优于10个基线模型，同时参数量仅为高容量EEG Conformer基线的八分之一。此外，可视化结果证实了DBConformer提取的特征具有生理可解释性，与运动想象中的感觉运动先验一致。


<details>
  <summary>更多</summary>
  
**动机:** 当前用于EEG解码的卷积神经网络（CNNs）由于其短感受野难以捕捉长距离时间依赖性和全局通道间关系。尽管最近的CNN-Transformer混合模型部分解决了这个问题，但它们大多采用串行设计，导致局部和全局特征的次优集成，且常常忽视显式的通道建模。

**方法:** DBConformer是一种双分支卷积Transformer网络，包含一个时间Conformer用于建模长时间依赖性，一个空间Conformer用于提取通道间交互，以及一个轻量级通道注意力模块来分配数据驱动的重要性给EEG通道。

**结果:** DBConformer在多个数据集和评估设置下显著优于10个竞争性基线模型，同时拥有更少的参数量。可视化结果表明，DBConformer提取的特征具有生理可解释性，与运动想象的感觉运动先验一致。

**结论:** DBConformer在性能和可解释性方面表现出色，为稳健和可解释的EEG解码提供了可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DBConformer%3A+Dual-Branch+Convolutional+Transformer+for+EEG+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21140&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [41] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, Dong Gong*

**主要类别:** cs.LG

**AI概要:** MoRA是一种新的混合专家方法，通过自激活和稀疏秩激活解决持续学习中的灾难性遗忘和任务干扰问题。它将每个低秩更新分解为多个独立的秩-1专家，从而减少干扰和冗余，并通过自适应选择输入的稀疏混合秩来避免模糊路由。实验表明，MoRA在增强预训练模型的持续学习能力方面非常有效。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于LoRA的混合专家方法虽然通过分配和冻结任务特定的适配器减轻了遗忘问题，但仍然存在干扰、冗余和路由模糊的问题。这些问题限制了其在持续学习中的效果。因此需要一种更精细的方法来解决这些问题。

**方法:** 提出了一种名为MoRA的方法，该方法将每个低秩更新分解为多个独立的秩-1专家，允许细粒度地利用这些专家，同时减少干扰和冗余。此外，MoRA通过中间激活推断每个秩-1专家的相关性，并结合秩剪枝和激活预算，自适应地为每个输入选择稀疏的混合秩。

**结果:** MoRA在使用CLIP和大语言模型进行持续学习任务时表现出显著的有效性，不仅提高了持续学习的效果，还改善了泛化能力，同时减轻了遗忘问题。

**结论:** MoRA作为一种新的混合秩自适应学习方法，在增强预训练模型的持续学习能力和改善泛化性能方面具有重要作用，同时有效地缓解了灾难性遗忘和任务干扰。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Little+By+Little%3A+Continual+Learning+via+Self-Activated+Sparse+Mixture-of-Rank+Adaptive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21035&send_immediately=true&force_search=false)

**原文摘要:** Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [42] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng, Meng Zhang, Ming Tang*

**主要类别:** cs.LG

**AI概要:** 近期研究提出了一种新算法，通过信息论分析联邦学习在概念漂移下的性能，并有效缓解了性能下降。


<details>
  <summary>更多</summary>
  
**动机:** 当前联邦学习研究多基于静态数据集，但实际中数据流分布会随时间变化（即概念漂移），导致模型性能下降。因此需要分析并解决这一问题。

**方法:** 将概念漂移建模为马尔可夫链，引入“平稳泛化误差”评估模型对未来未知数据的捕捉能力，使用KL散度和互信息推导其上界；提出一种结合KL散度和互信息正则化的经验风险最小化算法以缓解性能下降；并通过构建帕累托前沿探索性能与成本的权衡。

**结果:** 实验结果验证了理论分析，证明了三种漂移模式（周期性、渐进性和随机性）对联邦学习性能的影响，所提方法在这三种模式下均优于现有方法。

**结论:** 该研究通过信息论方法分析了联邦学习在概念漂移下的性能，并提出了有效的缓解算法，为适应动态数据环境提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Information-Theoretic+Analysis+for+Federated+Learning+under+Concept+Drift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21036&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [43] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为DiLoCoX的低通信大规模去中心化集群训练框架，结合了管道并行性、双优化策略、一步延迟通信与本地训练重叠以及自适应梯度压缩方案。该框架显著提升了参数规模和模型预训练速度，并首次成功应用于超过100亿参数的模型的去中心化训练。


<details>
  <summary>更多</summary>
  
**动机:** 当前基础模型（特别是大语言模型）的分布式训练高度依赖于快速可靠的集中式集群，但在慢速网络环境下进行训练的需求促使研究者探索去中心化集群的潜力。

**方法:** 提出DiLoCoX框架，包含管道并行性、双优化策略、一步延迟通信与本地训练重叠以及自适应梯度压缩方案。通过理论分析证明了一步延迟通信与本地训练重叠及自适应梯度压缩方案的收敛性优势。

**结果:** 实证结果表明，DiLoCoX能够在1Gbps网络上对107亿参数的基础模型进行预训练，并相较于传统的AllReduce方法实现了357倍的速度提升，同时保持模型收敛性能几乎无损。

**结论:** DiLoCoX是首个成功应用于超过100亿参数模型的去中心化训练框架，为慢速网络环境下的大规模模型训练提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiLoCoX%3A+A+Low-Communication+Large-Scale+Training+Framework+for+Decentralized+Cluster，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21263，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21263&send_immediately=true&force_search=false)

**原文摘要:** The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [44] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang, Peijia Li, Furao Shen, Jian Zhao*

**主要类别:** cs.LG

**AI概要:** Modern deep learning models often need large datasets for training, which can be computationally expensive. This paper addresses this issue by proposing a new method called RL-Selector that uses reinforcement learning to select the most useful data samples, reducing redundancy and improving training efficiency without sacrificing performance.


<details>
  <summary>更多</summary>
  
**动机:** Current methods for selecting data samples either use static scoring metrics or pretrained models, failing to account for the changing dynamics of the dataset during training. The authors aim to address this limitation by developing a more dynamic and efficient data selection approach.

**方法:** The authors introduce the concept of epsilon-sample cover to measure sample redundancy based on inter-sample relationships. They then reformulate data selection as a reinforcement learning problem and propose RL-Selector, where an RL agent optimizes data selection policies using the epsilon-sample cover as a reward signal derived from the evolving dataset distribution.

**结果:** Through extensive experiments on benchmark datasets and various architectures, the proposed method outperforms existing state-of-the-art baselines in terms of both generalization performance and training efficiency.

**结论:** RL-Selector provides a novel and effective way to perform data-efficient training by dynamically selecting the most representative samples, leading to better model performance with reduced computational costs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL-Selector%3A+Reinforcement+Learning-Guided+Data+Selection+via+Redundancy+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21037&send_immediately=true&force_search=false)

**原文摘要:** Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [45] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman, Jing Bi*

**主要类别:** cs.LG

**AI概要:** rQdia通过在像素级深度强化学习中使用增强图像来规范化Q值分布，提高了算法在多个任务中的样本效率和长期训练表现，并使无模型连续控制超越了状态编码基线。


<details>
  <summary>更多</summary>
  
**动机:** 在像素级深度强化学习中，利用图像增强技术来改进Q值分布的正则化，从而提升算法性能。

**方法:** rQdia引入了一个简单的辅助损失函数，通过均方误差（MSE）使Q值分布在增强图像上趋于一致。

**结果:** rQdia提升了DrQ和SAC在MuJoCo Continuous Control Suite中的9/12和10/12任务的表现，同时增强了Data-Efficient Rainbow在Atari Arcade环境中的18/26任务上的效果。这些提升体现在样本效率和长期训练两个方面。

**结论:** rQdia的成功应用表明其能够显著改善基于像素的深度强化学习算法的性能，并使得无模型连续控制方法首次超越了状态编码基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是rQdia%3A+Regularizing+Q-Value+Distributions+With+Image+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21367&send_immediately=true&force_search=false)

**原文摘要:** rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [46] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou, Tom Jacobs, Advait Gadhikar, Rebekka Burkholz*

**主要类别:** cs.LG

**AI概要:** 在微调大型预训练神经网络时，资源消耗大。通过分析梯度与权重的关系，发现大梯度通常与小权重相关联。基于此观察，提出NANOADAM方法，在微调过程中动态更新小权重参数，具有无需梯度计算、保留大权重参数、允许更大学习率等优点，并在NLP和视觉任务中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 微调大型预训练神经网络资源消耗大，限制训练的参数子集是常见做法。观察到微调时大梯度通常与小权重相关联，这种关联比从头开始训练更明显。

**方法:** 提出NANOADAM方法，动态更新小权重参数，无需梯度计算即可确定参数子集，保留大权重参数以减少灾难性遗忘风险，允许使用更大的学习率。

**结果:** 在NLP和视觉任务中的实验表明，NANOADAM方法可以更好地泛化性能。

**结论:** NANOADAM方法通过动态更新小权重参数，在微调大型预训练神经网络时表现出更好的泛化性能和实际优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pay+Attention+to+Small+Weights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21374&send_immediately=true&force_search=false)

**原文摘要:** Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [47] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng, Ming Tang*

**主要类别:** cs.LG

**AI概要:** 在联邦学习中，概念漂移（数据异质性）是一个重要问题。大多数现有方法主要针对真实漂移，但在面对虚拟或标签漂移时容易出现灾难性遗忘。本文提出FedDAA框架，通过动态聚类区分不同漂移源，并结合历史知识进行适应，有效提升多源概念漂移下的模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的数据分布可能随时间变化，导致时间和空间上的数据异质性（概念漂移）。然而，现有的方法大多仅关注真实漂移，无法有效应对虚拟或标签漂移，从而可能导致灾难性遗忘。因此，需要一种能够区分不同漂移源并保留有用历史知识的方法。

**方法:** FedDAA框架包含三个模块：1) 聚类数量确定模块，用于找到最优聚类数量；2) 真实漂移检测模块，用于区分真实漂移与虚拟/标签漂移；3) 概念漂移适应模块，用于在适应新数据的同时保留有用的历史信息。此外，该方法提供了理论收敛保证。

**结果:** 实验结果表明，FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上相比现有方法提升了7.84%到8.52%的准确率。

**结论:** FedDAA是一种有效的动态聚类联邦学习框架，能够在多源概念漂移场景下显著提升模型性能，同时保留有价值的历史知识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedDAA%3A+Dynamic+Client+Clustering+for+Concept+Drift+Adaptation+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21054&send_immediately=true&force_search=false)

**原文摘要:** In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [48] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng, Bochuan Zhou, Yuping Song*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种增强的时间感知图注意力网络（ATGAT），通过三个模块提升加密货币交易欺诈检测性能：1）高级时间嵌入模块；2）时间感知三重注意力机制；3）加权BCE损失以应对类别不平衡。实验表明，ATGAT在Elliptic++数据集上AUC达到0.9130，相较于传统方法有显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 加密货币交易欺诈检测面临日益复杂的交易模式和严重的类别不平衡问题。传统方法依赖于人工特征工程，难以捕捉交易网络中的时间和结构依赖关系。

**方法:** 该方法包括三个模块：1）设计了一个高级时间嵌入模块，将多尺度时间差特征与周期位置编码融合；2）构建了时间感知三重注意力机制，联合优化结构、时间和全局上下文注意力；3）采用加权BCE损失来解决类别不平衡问题。

**结果:** 在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到了0.9130，相比最佳的传统方法XGBoost提高了9.2%，比GCN提高了12.0%，比标准GAT提高了10.0%。

**结论:** 此方法验证了时间感知和三重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，并且其设计原则可以推广到其他时间图异常检测任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal-Aware+Graph+Attention+Network+for+Cryptocurrency+Transaction+Fraud+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21382，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21382&send_immediately=true&force_search=false)

**原文摘要:** Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [49] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种利用知识图谱生成高质量指令数据的新方法，以提升大型语言模型的工具使用能力和整体性能。


<details>
  <summary>更多</summary>
  
**动机:** 教导大型语言模型（LLMs）使用工具对于提高其解决问题的能力和扩展应用至关重要，但有效使用工具需要深刻理解工具功能和用户意图，而以往的方法生成的指令数据质量往往不足。

**方法:** 该方法通过从给定的知识图谱中提取多种查询路径，并将其转换为广泛的用户查询，然后将实体间的关系转化为可操作工具，解析每个查询的路径为详细的解决方案步骤，从而生成高质量的指令数据。

**结果:** 实验表明，仅对少量这种合成数据进行微调就可以显著提高LLMs的工具使用能力及整体能力。

**结论:** 使用知识图谱生成高质量指令数据可以有效增强LLMs的工具使用效率与整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+LLM+Tool+Use+with+High-quality+Instruction+Data+from+Knowledge+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21071&send_immediately=true&force_search=false)

**原文摘要:** Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [50] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的方法ScalaBL，通过在低维子空间中进行贝叶斯推理，结合LoRA参数作为投影矩阵，将样本映射到LLM的完整权重空间。这种方法仅需约1000个额外参数，即可与最先进的方法竞争，并扩展到了目前最大的贝叶斯LLM。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）存在产生错误信息和校准不良的问题，这使得不确定性量化尤为重要，特别是在高风险领域（如自主系统和医疗保健）。然而，现有的贝叶斯深度学习方法虽然有效，但在应用于更大规模的LLM时面临扩展性问题，因为它们需要比LoRA更多的额外参数。

**方法:** 作者提出了ScalaBL方法，该方法在r维子空间中执行贝叶斯推理，其中r为LoRA的秩。通过将LoRA参数重新用作投影矩阵，可以将子空间中的样本映射到LLM的完整权重空间。利用随机变分推断学习所有参数，从而解决了扩展性问题。

**结果:** ScalaBL能够在低维子空间中实现与最先进方法相当的性能，同时只需要约1000个额外参数。此外，该方法成功扩展到了目前为止最大的贝叶斯LLM，其基础参数数量是先前工作的四倍。

**结论:** ScalaBL提供了一种有效的解决方案，用于对大规模LLM进行贝叶斯推理和不确定性量化。它不仅显著减少了所需的额外参数数量，还实现了对更大模型的支持，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Bayesian+Low-Rank+Adaptation+of+Large+Language+Models+via+Stochastic+Variational+Subspace+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21408&send_immediately=true&force_search=false)

**原文摘要:** Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [51] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship, Luis Miralles-Pechuan, Stephen O'Sullivan*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于优化低存储需求的扩展稳定性龙格-库塔（ESRK）方法。通过系统性参数减少，在保持四阶精度的同时显著提高了计算效率。实验结果表明，与传统ESRK优化方法相比，该方法可以将IPOPT运行时间减少25%，同时保持数值稳定性和准确性。这项工作为数值方法中的启发式优化建立了一个新的范例，并为使用深度强化学习和自动机器学习进行进一步探索奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 扩展稳定性龙格-库塔（ESRK）方法在科学和工程的大规模计算问题中至关重要，但在高阶、低存储方案中，平衡精度、稳定性和计算效率仍然具有挑战性。

**方法:** 研究采用了一种结合遗传算法（GA）和强化学习（RL）的混合方法。GA驱动的变异用于搜索空间探索，而RL启发的状态转换机制则动态优化启发式选择。这种方法允许系统性地减少参数，同时保持四阶精度并提高计算效率。

**结果:** 通过基准问题测试，包括1D和2D Brusselator系统以及稳态Navier-Stokes方程，结果表明该方法可以将IPOPT运行时间减少25%，同时保持数值稳定性和准确性。

**结论:** 该研究表明，自适应启发式发现能够提高高保真模拟的资源效率，并拓宽低存储Runge-Kutta方法在实际计算流体动力学、物理模拟等领域的应用。这为数值方法的启发式优化提供了新的范例，并为未来的研究指明了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimising+4th-Order+Runge-Kutta+Methods%3A+A+Dynamic+Heuristic+Approach+for+Efficiency+and+Low+Storage，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21465&send_immediately=true&force_search=false)

**原文摘要:** Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [52] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale, Nicola Dall'Ora, Sebastiano Gaiardelli, Enrico Fraccaroli, Nicola Mazzocca, Franco Fummi*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的无监督故障诊断方法，结合多变量时间序列分析、过程挖掘和随机模拟来检测异常并生成可解释的过程模型，通过实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 手动建模故障行为需要大量领域专业知识，且模型复杂、易出错、难以解释。因此需要一种新的方法来解决这一问题。

**方法:** 该方法首先使用多变量时间序列分析从低级传感器数据中检测集体异常，然后将这些异常转化为结构化事件日志，利用过程挖掘发现可解释的过程模型，并通过将时间分布纳入提取的Petri网进行随机模拟以增强根本原因分析和行为理解。

**结果:** 实验结果表明，该方法在对CPS中的故障行为进行建模、模拟和分类方面非常有效，能够创建全面的故障字典，支持预测性维护和工业环境数字孪生的开发。

**结论:** 所提出的无监督故障诊断方法为智能制造业中的CPS提供了有效的故障行为分析手段，有助于提升系统可靠性和操作效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Process+mining-driven+modeling+and+simulation+to+enhance+fault+diagnosis+in+cyber-physical+systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21502&send_immediately=true&force_search=false)

**原文摘要:** Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [53] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou, Constantin Brif, Ismini Lourentzou*

**主要类别:** cs.LG

**AI概要:** 摘要介绍了一个名为mTSBench的大规模基准测试，涵盖了344个标记的时间序列、19个数据集和12个不同的应用领域。它评估了24种异常检测方法，包括基于大型语言模型（LLM）的多变量时间序列检测器，并在标准化条件下系统地对无监督模型选择技术进行了基准测试。结果表明，没有任何单一检测器在所有数据集上表现最佳，强调了模型选择的重要性。然而，即使是最先进的选择方法也远非最优，揭示了关键差距。mTSBench提供了一个统一的评估套件，以促进适应性异常检测和稳健模型选择的未来进步。


<details>
  <summary>更多</summary>
  
**动机:** 多变量时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于变量间复杂的依赖关系、时间动态特性和稀疏的异常标签而具有挑战性。

**方法:** 引入mTSBench，这是目前最大的MTS-AD和无监督模型选择基准，包含344个标记时间序列、19个数据集和12个不同应用领域。评估24种异常检测方法，包括基于大型语言模型（LLM）的多变量时间序列检测器，并在标准化条件下系统地对无监督模型选择技术进行基准测试。

**结果:** 结果证实，没有任何单一检测器在所有数据集上表现最佳，强调了模型选择的重要性。最先进选择方法仍远未达到最优，揭示了关键差距。

**结论:** mTSBench提供了一个统一的评估套件，以实现严谨、可重复的比较，并推动适应性异常检测和稳健模型选择的未来发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mTSBench%3A+Benchmarking+Multivariate+Time+Series+Anomaly+Detection+and+Model+Selection+at+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21550，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21550&send_immediately=true&force_search=false)

**原文摘要:** Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [54] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson, Laurence Aitchison*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的架构，通过动态跳过中间层来提高Transformer效率，尽管对于简单令牌减少了计算需求，但与密集基线相比，在验证交叉熵和估计FLOPs之间的权衡没有改善。


<details>
  <summary>更多</summary>
  
**动机:** 现有的条件计算方法通常针对单个模块或独立跳过层，而研究表明Transformer的中间层具有更高的冗余度，早期层将信息聚合到令牌位置中，因此需要一种新方法来动态地跳过中间层以提高效率。

**方法:** 提出了一种通过学习门机制动态跳过中间层的方法，并采用门控注意力机制防止后续令牌关注被跳过的令牌位置；使用'sandwich'或'perilayernorm'方案控制残差范数，并用自适应正则化损失控制门稀疏性。

**结果:** 该方法减少了简单令牌的计算需求，但未能在所研究的规模上实现比具有较少层数的密集基线更好的验证交叉熵和估计FLOPs之间的权衡。

**结论:** 虽然提出的方法为简化令牌减少了计算需求，但在所研究的规模上未能优于具有较少层数的密集基线，代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Skip+the+Middle+Layers+of+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21103，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21103&send_immediately=true&force_search=false)

**原文摘要:** Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [55] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi, Jun Xia, Yufei Huang, Jingbo Zhou, Siyuan Li, Yunfan Liu, Chang Yu, Stan Z. Li*

**主要类别:** cs.LG

**AI概要:** 为解决单细胞扰动数据未配对问题，提出基于双重扩散隐式桥（DDIB）的框架，结合基因调控网络（GRN）信息和掩码机制预测沉默基因，引入新的评估指标反映单细胞响应的内在异质性。


<details>
  <summary>更多</summary>
  
**动机:** 单细胞测序破坏性导致无法捕获同一细胞扰动前后的表型，现有方法强行配对或忽视未扰动与扰动细胞间关系，无法有效处理未配对数据。

**方法:** 提出DDIB框架学习不同数据分布间的映射，将框架解释为数据增强形式，整合基因调控网络信息传播扰动信号，并引入掩码机制预测沉默基因，同时提出适合评估单细胞响应异质性的新指标。

**结果:** 提出的Unlasting模型克服了未配对单细胞扰动数据的问题，增强了对扰动的理解，提高了生成质量，并通过新评估指标更好地反映了单细胞响应的内在异质性。

**结论:** 该研究提供了一种有效的方法来处理未配对单细胞扰动数据，结合生物意义的数据增强和评估方法，提升了对单细胞响应的理解和实验效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlasting%3A+Unpaired+Single-Cell+Multi-Perturbation+Estimation+by+Dual+Conditional+Diffusion+Implicit+Bridges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21107&send_immediately=true&force_search=false)

**原文摘要:** Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [56] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng, Yadan Luo, Liangyu Huo, Yaowei Wang, Xin Li, Zheng Zhang*

**主要类别:** cs.LG

**AI概要:** Linear attention通过降低序列长度的复杂度从二次到线性，成为softmax attention的一个可行替代方案。然而，当前的工作忽视了查询范数，导致熵差距，并且抑制了查询和键向量的负值，导致内积交互丢失。为了解决这些问题，本文提出了一种新的Norm-Aware Linear Attention机制，可以恢复范数引导的动态尖锐性和核扰动的范数分布。具体来说，我们将查询和键矩阵分解为范数和方向两个部分，以实现范数感知的尖锐性控制和范数一致性。此外，我们还采用了一个保持范数的映射方法，将角度矩阵的所有元素投影到正值，利用余弦相似性来抑制方向相反的维度。实验表明，NaLaFormer在视觉和语言任务上提高了性能，表达能力和效率提升了4.2%。


<details>
  <summary>更多</summary>
  
**动机:** Linear attention虽然降低了计算复杂度，但存在两个主要问题：1）查询范数被忽略，导致熵差距；2）抑制了查询和键向量的负值，导致内积交互丢失。这些问题促使研究者寻找一种新的注意力机制来解决这些缺陷。

**方法:** 本文提出了Norm-Aware Linear Attention（NaLa）机制，该机制将查询和键矩阵分解为范数和方向两个部分，分别进行处理。通过这种方式，可以实现范数感知的尖锐性控制和范数一致性。同时，为了确保范数一致性和非负性约束，采用了保持范数的映射方法，将角度矩阵的所有元素投影到正值，并利用余弦相似性来抑制方向相反的维度。

**结果:** 通过广泛的实验验证，NaLaFormer在视觉和语言任务上表现出色，相比现有模型，其表达能力和效率最高可提升4.2%。这表明提出的机制不仅解决了现有linear attention的问题，还进一步增强了模型的性能。

**结论:** Norm-Aware Linear Attention是一种有效的新型注意力机制，能够解决现有linear attention中存在的范数忽略和内积交互丢失的问题。它通过分解查询和键矩阵、引入范数感知的动态控制以及保持范数的映射方法，显著提升了模型的表达能力和效率，在视觉和语言任务中均取得了优异的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NaLaFormer%3A+Norm-Aware+Linear+Attention+for+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21137&send_immediately=true&force_search=false)

**原文摘要:** Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [57] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** The paper presents a cGAN-based framework to craft stealthy adversarial attacks that evade IDS and a CVAE-based method to detect them, highlighting the need for advanced probabilistic modeling in IDS.


<details>
  <summary>更多</summary>
  
**动机:** Traditional anomaly detection methods fail to identify novel threats and struggle with stealthy adversarial attacks.

**方法:** A cGAN-based framework generates adversarial samples by perturbing known attacks to misclassify as benign. A CVAE-based method uses negative log-likelihood to distinguish adversarial inputs from authentic OOD samples.

**结果:** CVAE-based regret scores outperform Mahalanobis distance-based detectors in identifying stealthy adversarial threats.

**结论:** Advanced probabilistic modeling is crucial to enhance IDS capabilities against adaptive, generative-model-based cyber intrusions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Adversarial+Evasion+and+Out-of-Distribution+Detection+for+UAV+Cyber-Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21142&send_immediately=true&force_search=false)

**原文摘要:** The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [58] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang, Kuangpu Guo, Zhihe Lu, Yunbo Wang, Jian Liang*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为pFedDC的个性化联邦学习框架，结合双提示学习和交叉融合模块，在多种异构数据集上表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦提示学习方法仅依赖文本提示，并未解决联合标签域分布偏移的问题，因此需要一种新的框架来更好地适应数据、计算和通信的异构性。

**方法:** 提出了pFedDC框架，其中每个客户端维护视觉和语言模态的全局和局部提示：全局提示捕捉联邦共享的通用知识，局部提示编码客户端特定的语义和领域特征；同时设计了一个交叉融合模块，自适应地整合不同层次的提示，生成与各客户端独特数据分布对齐的个性化表示。

**结果:** 在九个具有不同类型异构性的数据集上的广泛实验证明，pFedDC持续优于最先进的方法。

**结论:** pFedDC框架通过结合双提示学习和交叉融合，解决了现有方法的局限性，为联邦学习中的个性化模型训练提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Federated+Learning+via+Dual-Prompt+Optimization+and+Cross+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21144，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21144&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [59] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson, Ola Engkvist, Jon Paul Janet, Christian Tyrchan, Morteza Haghir Chehreghani*

**主要类别:** cs.LG

**AI概要:** 在强化学习中引入了使用行列式点过程的多样化小批量选择框架，并在药物发现任务中进行了实验，结果表明该方法可以在保持解的质量的同时显著提高解的多样性。


<details>
  <summary>更多</summary>
  
**动机:** 在许多实际应用中，评估实例的好坏往往成本高昂且耗时，例如人类反馈和物理模拟，而在强化学习中，与环境的新交互（即新实例）需要被评估以提供奖励信号进行学习。因此，充分探索至关重要，而从多样化的小批量数据中学习可以产生重大影响并帮助缓解模式崩溃。

**方法:** 作者提出了一个基于行列式点过程（Determinantal Point Processes, DPP）的多样化小批量选择框架，应用于强化学习领域。通过在药物发现的实际问题中测试这一框架，研究其在去 novo 药物设计中的效果。具体来说，作者通过三个已建立的分子生成模型，在多个生成步骤上进行全面评估。

**结果:** 实验结果表明，所提出的多样化小批量选择框架可以显著提高解的多样性，同时仍然获得高质量的解。这对于药物发现任务而言，可能会更快地满足未满足的医疗需求。

**结论:** 本文提出了一种基于行列式点过程的多样化小批量选择方法，用于强化学习中的化学探索任务。该方法不仅提高了解的多样性，还保持了解的质量，为药物发现等实际问题提供了潜在的加速解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diverse+Mini-Batch+Selection+in+Reinforcement+Learning+for+Efficient+Chemical+Exploration+in+de+novo+Drug+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21158&send_immediately=true&force_search=false)

**原文摘要:** In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [60] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah, Axel Abels, Ann Nowé, Tom Lenaerts*

**主要类别:** cs.LG

**AI概要:** 永久投票通过评估随时间推移的代表性公平来解决连续集体决策中的公平性问题。然而，现有的永久投票规则依赖于完全参与和完整的批准信息，这在实践中很少见。本文研究了将人工代表（Artificial Delegates）整合到永久投票系统中，这些代表是经过训练以表示缺席选民的偏好学习代理。我们探讨了缺席主义如何影响不同投票方法下的公平性和代表性，并评估了人工代表在多大程度上可以弥补缺失的参与。结果表明，虽然缺席主义显著影响公平性，但人工代表可靠地减轻了这些影响并增强了各种场景下的稳健性。


<details>
  <summary>更多</summary>
  
**动机:** 永久投票系统的公平性受到缺席投票的影响，而现有系统假设完全参与和完整信息，这与实际情况不符。因此，需要探索一种方法来减少缺席投票对公平性的影响，同时保持系统的代表性。

**方法:** 研究将人工代表（Artificial Delegates）引入永久投票系统，这些代表通过学习缺席选民的偏好来进行投票。通过分析缺席投票对公平性和代表性的影响，以及评估人工代表在不同投票方法下的补偿效果，来验证该方法的有效性。

**结果:** 研究表明，缺席投票显著影响公平性，而人工代表能够有效减轻这种影响，并在各种场景下提高系统的稳健性。

**结论:** 人工代表是一种有效的解决方案，可以减少缺席投票对永久投票系统公平性的影响，增强系统的代表性和稳健性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Delegates+Resolve+Fairness+Issues+in+Perpetual+Voting+with+Partial+Turnout，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21186&send_immediately=true&force_search=false)

**原文摘要:** Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [61] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev*

**主要类别:** cs.LG

**AI概要:** 通过使用基于熵的复杂度分类方法，论文提出了一种高效微调大语言模型（LLMs）的新方法，该方法在减少数据需求的同时显著提升了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 通用大语言模型（LLMs）通常通过监督微调（SFT）提升特定领域的性能，但传统蒸馏方法需要大量昂贵的计算和数据。因此，研究者试图寻找一种更高效的微调方法。

**方法:** 利用单个标记答案熵对训练数据进行复杂度分类，并仅对复杂数据应用推理。具体来说，通过将两个约30亿参数的小型开源模型的数据按复杂度分类后，采用SFT与蒸馏结合的方法进行微调。

**结果:** 相较于标准SFT方法，该方法的平均准确率从0.43提升至0.55，同时使用的数据量减少了62%，且其性能与传统蒸馏方法相当。

**结论:** 提出的基于熵的高效微调方法能够在减少数据需求的情况下显著提升模型性能，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Complexity-aware+fine-tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21220&send_immediately=true&force_search=false)

**原文摘要:** General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [62] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad, Aya Mrabah, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种使用零样本学习与大语言模型的电子元件过时风险预测方法，并通过实际数据集验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 电子元件的过时给依赖电子元件的行业带来了显著挑战，包括增加成本和影响系统的安全性和可用性。然而，由于缺乏可靠的数据，准确预测过时风险变得困难。

**方法:** 本研究提出了一种新的方法，利用零样本学习（ZSL）和大型语言模型（LLMs），通过从表格数据集中利用领域特定知识来解决数据限制问题，以预测过时风险。

**结果:** 该方法在两个真实世界的数据集上进行了应用，展示了有效的风险预测能力。此外，对四种大语言模型的比较评估表明，在特定预测任务中选择合适的模型的重要性。

**结论:** 提出的基于零样本学习和大语言模型的方法可以有效地用于电子元件过时风险预测，并强调了为特定任务选择合适的大语言模型的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Learning+for+Obsolescence+Risk+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21240&send_immediately=true&force_search=false)

**原文摘要:** Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [63] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière, Frédéric Cazals*

**主要类别:** cs.LG

**AI概要:** 本文重新审视了k-means聚类和k-GMM的随机化种子技术，提出了利用前瞻原则和多遍策略的新初始化方法族。实验表明，这些方法在最终度量标准上比经典方法有显著改进，并揭示了k-means的一些微妙特性。实际应用中，这些方法有望成为新的标准技术，而从理论上，该研究为新的分析方法铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 现有的k-means聚类和k-GMM的随机化种子技术存在改进空间，特别是在种子采样度量、候选种子数量和种子选择度量方面。此外，需要更深入地理解这些技术与最终算法评估度量之间的关系。

**方法:** 作者通过引入前瞻原则（即根据最终评估度量来调整种子选择）和多遍策略（减少随机化的影响），提出了新型初始化方法族。这些方法在理论上有明确的形式化描述，并且在实践中进行了广泛的实验验证。

**结果:** 实验结果表明，新提出的方法在最终度量标准（如k-means的SSE或k-GMM的对数似然）上比经典方法有显著改进，且计算开销较小。此外，还揭示了k-means的一些重要特性，例如种子阶段与最终SSE的相关性较低、迭代种子方法中的方差减少现象等。

**结论:** 本文提出的新初始化方法在性能上优于现有技术，具有广泛的实际应用价值。同时，对种子技术的形式化描述为未来理论研究开辟了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+seeding+strategies+for+k-means+and+k-GMM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21291&send_immediately=true&force_search=false)

**原文摘要:** We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [64] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

**主要类别:** cs.LG

**AI概要:** 研究提出了一种新的路由框架LPR，解决了Mixture-of-Experts系统中严重的负载不平衡问题，显著提高了模型容量和计算资源的利用率，同时不会影响下游性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的Mixture-of-Experts (MoE) 系统在训练和推理过程中存在严重的负载不平衡问题，只有少量专家被激活，导致模型容量和计算资源的极大浪费。

**方法:** 通过聚类视角重新审视专家路由，并提出了潜在原型路由（Latent Prototype Routing, LPR）这一新框架。该框架推广了现有方法，促进了专家使用的平衡性。

**结果:** 在多个开源MoE模型上的广泛实验表明，LPR将专家负载的基尼系数从0.70降低到0.035，平均改善了最小最大专家负载比从1e-6提升到0.70，接近完美的负载均衡。

**结论:** LPR框架不仅有效解决了MoE系统的负载不平衡问题，而且保持了下游任务的性能，为更高效的LLM扩展提供了策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Prototype+Routing%3A+Achieving+Near-Perfect+Load+Balancing+in+Mixture-of-Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21328&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [65] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的图时空卷积网络（AGTCNet），用于改进脑机接口（BCI）系统中的运动想象EEG分类。该模型通过减少模型大小、加速推理时间以及使用更短的输入EEG信号，显著提高了跨主体和特定主体的分类准确率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的脑机接口系统在捕捉多通道EEG信号的复杂时空依赖关系方面存在不足，特别是在处理个体间和时间上的神经活动变化时效果不佳。这促使研究者开发一种更有效的模型来解决这些问题。

**方法:** 研究引入了AGTCNet，这是一种结合图卷积注意力网络（GCAT）的新型图时空模型。该模型利用EEG电极的地形配置作为归纳偏差，学习表达性的时空EEG表示。它具有紧凑的架构，减少了模型大小并加快了推理速度。

**结果:** AGTCNet在多个数据集上表现出色：在BCI Competition IV Dataset 2a上，跨主体分类的移动平均准确率为66.82%，特定主体分类为82.88%；在EEG Motor Movement/Imagery Dataset上，4类和2类跨主体分类的移动平均准确率分别为64.14%和85.22%，特定主体分类则分别提高到72.13%和90.54%。此外，模型大小减少了49.87%，推理时间加快了64.65%。

**结论:** AGTCNet显著提升了运动想象EEG分类的性能，同时保持了模型的紧凑性和高效性，为未来BCI系统的实际部署提供了有力支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AGTCNet%3A+A+Graph-Temporal+Approach+for+Principled+Motor+Imagery+EEG+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21338，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21338&send_immediately=true&force_search=false)

**原文摘要:** Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [66] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia*

**主要类别:** cs.LG

**AI概要:** 提出DynamicBench评估大型语言模型处理实时信息的能力，并引入先进的报告生成系统，实验结果表明该方法在无文档和有文档辅助情况下均优于GPT4o。


<details>
  <summary>更多</summary>
  
**动机:** 传统的大型语言模型评估基准通常依赖于静态评估，如讲故事或表达观点，无法捕捉当代应用中实时信息处理的动态需求。

**方法:** DynamicBench利用双路径检索管道，结合网络搜索与本地报告数据库，需要领域特定知识以确保专业领域内准确的报告生成。通过在提供或不提供外部文档的情况下评估模型，有效测量模型独立处理近期信息或利用上下文增强的能力。

**结果:** 实验结果证实了该方法的有效性，在无文档和有文档辅助的情况下分别超越GPT4o 7.0%和5.8%。

**结论:** 代码和数据将公开发布，DynamicBench能有效评估大型语言模型处理实时信息的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynamicBench%3A+Evaluating+Real-Time+Report+Generation+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21343，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21343&send_immediately=true&force_search=false)

**原文摘要:** Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [67] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh, Mehmet Dik, Nguyen Viet Anh*

**主要类别:** cs.LG

**AI概要:** 本文研究了持久拉普拉斯算子在单个单纯形添加时特征值的变化，证明了一个统一的Lipschitz界，并提供了谱拓扑数据分析中特征值级别的稳健性保证。


<details>
  <summary>更多</summary>
  
**动机:** 持久拉普拉斯算子被广泛应用于生物学、物理学和机器学习等领域，其特征值是描述数据几何和拓扑特征的重要工具。然而，对于添加一个单纯形（如顶点、边或三角形）时单个特征值的具体变化尚未有明确结果，这直接影响到下游工具（如热核签名和谱神经网络）的性能。

**方法:** 作者通过证明一个统一的Lipschitz界来填补这一空白：当插入一个单纯形时，每个上持久拉普拉斯特征值最多可以变化为该单纯形边界欧几里得范数的两倍，且这一变化与过滤尺度和复形大小无关。

**结果:** 该研究提供了首个谱拓扑数据分析中特征值级别的稳健性保证，确保了谱特征在局部更新下保持稳定，并实现了动态数据环境中的可靠误差控制。

**结论:** 这项工作不仅为持久拉普拉斯算子的特征值变化提供了理论支持，还增强了谱拓扑数据分析在实际应用中的可靠性与鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lipschitz+Bounds+for+Persistent+Laplacian+Eigenvalues+under+One-Simplex+Insertions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21352&send_immediately=true&force_search=false)

**原文摘要:** Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [68] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff, Maya Varma, Ossian Rabow, Subathra Adithan, Julie Kim, Ken Chang, Hannah Lee, Nidhi Rohatgi, Christian Bluethgen, Mohamed S. Muneer, Jean-Benoit Delbrouck, Michael Moor*

**主要类别:** cs.LG

**AI概要:** 尽管多模态大模型在医学视觉问答任务中表现出色，但它们在医疗任务中的多模态情境学习（ICL）能力仍有限。研究发现，当前的模型对无关示例敏感，并存在近期偏差的问题，这表明其在医疗多模态任务中的表现亟待提升。


<details>
  <summary>更多</summary>
  
**动机:** 由于医疗领域需要从少量样本中适应多样化、专业化的任务，因此探索多模态大模型在医疗任务中的情境学习能力具有重要意义。

**方法:** 研究人员引入了SMMILE，这是首个针对医疗任务的专家驱动多模态情境学习基准，并进一步扩展为SMMILE++。通过评估15个多模态大模型，分析它们在处理医疗问题时的情境学习表现。

**结果:** 大多数模型在医疗任务中的多模态情境学习能力表现一般或较差。情境学习相较于零样本学习仅带来8%和9.4%的平均改进。此外，模型对无关示例敏感，并表现出近期偏差。

**结论:** 当前多模态大模型在医疗任务中的情境学习能力存在关键限制和偏差，需进一步改进以提高性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SMMILE%3A+An+Expert-Driven+Benchmark+for+Multimodal+Medical+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21355&send_immediately=true&force_search=false)

**原文摘要:** Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [69] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon, Georgios Makris, Sotirios Xydis, Kiamal Pekmestzi, Dimitrios Soudris*

**主要类别:** cs.LG

**AI概要:** 本文研究了在低功耗DNN计算中，利用硬件近似技术结合DNN任务的细粒度误差弹性，以提高能源效率。通过在ResNet-8模型上使用CIFAR-10数据集进行评估，提出的方法在牺牲少量精度的情况下实现了显著的能效提升。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度神经网络（DNN）架构的快速发展，它们已经成为提供高精度机器学习任务的事实标准。然而，DNN计算通常需要大量的能量消耗，因此如何降低其能耗成为一个重要问题。

**方法:** 作者利用最先进的ROUP近似乘法器，系统地探索了这些乘法器在神经网络中的细粒度分布方法（包括层、滤波器和内核级别），并分析了其对准确性和能耗的影响。实验采用ResNet-8模型和CIFAR-10数据集进行评估。

**结果:** 与基线量化模型相比，所提出的解决方案能够提供高达54%的能效增益，但伴随着最多4%的精度损失；同时，在与最先进的DNN近似技术对比时，该方法提供了2倍的能效增益且具有更高的精度。

**结论:** 通过引入细粒度误差弹性和硬件近似技术，可以有效地提高DNN计算的能量效率，同时控制精度损失在可接受范围内。这对于低功耗DNN计算具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAx-DNN%3A+Multi-Level+Arithmetic+Approximation+for+Energy-Efficient+DNN+Hardware+Accelerators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21371&send_immediately=true&force_search=false)

**原文摘要:** Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [70] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken, Lennart Purucker, Frank Hutter*

**主要类别:** cs.LG

**AI概要:** 通过早期停止上下文学习过程，可以加速推理并提高表格数据学习的效率。


<details>
  <summary>更多</summary>
  
**动机:** 表格基础模型在无需下游微调的情况下，在各种表格学习任务中表现出强大的性能，但其推理成本较高，特别是对于较大的数据集。

**方法:** 提出了一种动态评估是否在每次Transformer编码器层后停止上下文学习的方法，并在停止后使用预先训练好的逐层解码器对嵌入进行解码。

**结果:** 在34个小分类任务上，早期停止上下文学习可将推理速度提高多达1.3倍，预测性能几乎无下降；在五个较大分类任务上，速度提升可达2.2倍。

**结论:** 早期退出是一种有效且实用的策略，可以提高表格上下文学习的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early+Stopping+Tabular+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21387&send_immediately=true&force_search=false)

**原文摘要:** Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [71] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris, Isaac Lyngaas, John Lagregren, Mohamed Wahib, Larry York, Prasanna Balaprakash, Dan Lu, Feiyi Wang, Xiao Wang*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为D-CHAG的新方法，用于处理多通道图像数据集，显著提高了计算效率，并在超光谱成像和天气预报任务中取得了优异的性能。


<details>
  <summary>更多</summary>
  
**动机:** 视觉基础模型在科学发现和创新方面具有巨大潜力，但当前方法在处理大规模多通道图像数据时面临计算密集型的挑战。

**方法:** 引入了Distributed Cross-Channel Hierarchical Aggregation (D-CHAG) 方法，该方法兼容任何模型并行策略和视觉Transformer架构，旨在通过优化跨模态多通道图像数据的标记化和聚合过程来提高计算效率。

**结果:** 在超光谱成像和天气预报任务上的实验表明，结合张量并行性和模型分片，D-CHAG方法可将内存使用减少75%，并将1024个AMD GPU上的持续吞吐量提升两倍以上。

**结论:** D-CHAG方法为高效处理大规模多通道图像数据提供了新途径，有望推动基于视觉的科学基础模型在更多领域的应用和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distributed+Cross-Channel+Hierarchical+Aggregation+for+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21411&send_immediately=true&force_search=false)

**原文摘要:** Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [72] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala, Cody Fleming*

**主要类别:** cs.LG

**AI概要:** 生成模型（如扩散和流匹配模型）通过捕捉多模态动作分布为离线强化学习提供表达策略，但迭代采样带来高推理成本和训练不稳定性。本文提出了一种名为单步完成策略（SSCP）的生成策略，它通过增强的流匹配目标从中间样本中预测直接完成向量，实现准确的一次性动作生成。SSCP在离线、离线到在线及在线RL场景中表现出色，在速度和适应性上优于基于扩散的基线方法，并可扩展到目标条件下的RL，无需显式的分层推理即可利用子目标结构。SSCP在标准离线RL和行为克隆基准测试中取得了优异的结果，成为深度RL和序列决策的强大框架。


<details>
  <summary>更多</summary>
  
**动机:** 生成模型如扩散模型和流匹配模型尽管能够捕捉丰富的多模态动作分布，但由于其迭代采样的特性，导致了高推理成本和训练不稳定性。这促使研究者寻找一种既能保持生成模型的表达能力，又能减少训练和推理开销的新方法。

**方法:** 作者提出了单步完成策略（SSCP），这是一种通过增强流匹配目标训练的生成策略。SSCP可以从中间样本中预测直接完成向量，从而实现一次性动作生成。该方法结合了生成模型的表达能力和单模态策略的高效训练与推理特点，避免了长反向传播链的需求。此外，SSCP可以扩展到目标条件下的强化学习，支持平面策略利用子目标结构而无需显式分层推理。

**结果:** SSCP在离线、离线到在线以及在线强化学习环境中均表现良好，显著提升了速度和适应性。相比基于扩散模型的方法，SSCP提供了实质性的改进。此外，当扩展到目标条件下的强化学习时，SSCP能够在不需要显式层次推理的情况下有效利用子目标结构。实验结果表明，SSCP在标准离线强化学习和行为克隆基准测试中取得了强大的性能。

**结论:** 单步完成策略（SSCP）是一种结合生成模型表达能力和高效训练推理特点的有效方法。它不仅减少了训练和推理中的开销，还在多种强化学习环境中展现了优越的性能。SSCP还具有扩展性，适用于目标条件下的强化学习任务，成为深度强化学习和序列决策的强大工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flow-Based+Single-Step+Completion+for+Efficient+and+Expressive+Policy+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21427&send_immediately=true&force_search=false)

**原文摘要:** Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [73] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon, Thomas Jack Samuels, Stephan Hau, Lennart Högman*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了在双人互动中使用多模态机器学习技术检测欺骗行为的效果，重点关注将欺骗者和被骗者的数据整合起来。通过比较早期融合和晚期融合方法，并利用音频和视频数据（包括动作单元和注视信息）进行所有可能的模态和参与者组合分析。结果表明，结合语音和面部信息比单一模态方法表现更优。同时，包含双方参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是通过应用于双模态和参与者的晚期融合策略实现的。这些发现与心理学理论一致，即在初始互动中对面部和声音表达的控制有所不同。作为首个针对斯堪的纳维亚人群的研究，本研究为未来对双人互动（特别是心理治疗环境）的研究奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探索多模态机器学习技术在双人互动中的欺骗检测效果，特别是在整合欺骗者和被骗者数据方面。这有助于理解人类交互过程中欺骗行为的表现形式及特征。

**方法:** 采用新收集的瑞典母语者数据集，该数据集涵盖了在情感相关话题上讲述真实或谎言的情景。通过比较早期融合和晚期融合方法，使用音频和视频数据（动作单元和注视信息），对所有可能的模态和参与者组合进行分析。

**结果:** 结合语音和面部信息的多模态方法比单一模态方法表现更好。包含双方参与者的数据显著提升了欺骗检测的准确性，最佳性能（71%）由晚期融合策略实现。

**结论:** 研究表明，多模态机器学习技术能够有效提升欺骗检测的准确性，尤其是在双人互动中结合双方的语音和面部信息时。此研究为未来关于双人互动的研究提供了重要参考，特别是在心理治疗环境中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deception+Detection+in+Dyadic+Exchanges+Using+Multimodal+Machine+Learning%3A+A+Study+on+a+Swedish+Cohort，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21429，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21429&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [74] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider, Simon Heilig, Asja Fischer, Timm Faulwasser*

**主要类别:** cs.LG

**AI概要:** 提出了一种适用于标准架构和通用损失函数的ResNets训练公式，该公式反映了一个最优控制问题。通过惩罚与最优控制中的阶段成本项相对应的隐藏状态的中间输出，将两者联系起来。对于标准的ResNets，我们通过后续的跳跃连接和输出层传播状态来获得中间输出。我们的训练动态使不必要的更深残差层的权重趋于消失，这表明了一种基于理论支持的层剪枝策略的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习模型如ResNets虽然性能强大，但其深层结构在训练过程中可能存在冗余，导致资源浪费。因此，研究者希望探索一种能够优化网络结构并减少冗余的方法，从而提高效率和性能。

**方法:** 提出了一种新的ResNets训练方法，该方法反映了最优控制问题，并适用于标准架构和通用损失函数。通过惩罚隐藏状态的中间输出（对应于最优控制中的阶段成本项），将最优控制理论与深度学习模型结合。对于标准的ResNets，通过后续的跳跃连接和输出层传播状态来获取中间输出。

**结果:** 实验结果表明，这种训练动态会使不必要的更深残差层的权重趋于消失，证明了该方法的有效性。

**结论:** 该研究表明，通过引入最优控制问题的视角，可以为深度学习模型提供一种理论支持的层剪枝策略，有助于优化网络结构和减少冗余。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+an+Optimal+Control+Perspective+of+ResNet+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21453&send_immediately=true&force_search=false)

**原文摘要:** We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [75] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud, Md Gulzar Hussain, Sumaiya Kabir, Hasnain Ahmad, Mahmudus Sobhan*

**主要类别:** cs.LG

**AI概要:** 提出了一种电子化评估主观答题纸的有效解决方案，通过关键词匹配和语法拼写检查，系统在100名学生的答题纸上测试，精确度得分为0.91。


<details>
  <summary>更多</summary>
  
**动机:** 评估是衡量和确定教育系统的重要方法，当前需要一种更高效的电子化手段来评估主观答题纸。

**方法:** 设计并实现了一个综合系统，从主观答题纸中提取关键词并与开放和封闭领域解析出的关键词进行比较，同时检查答题纸中的语法和拼写错误。

**结果:** 该系统在100名学生的答题纸上进行了测试，获得了0.91的精确度评分。

**结论:** 所提出的电子评估系统能够有效评估主观答题纸，并在实际测试中表现出较高的精确度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Keyword-Based+Technique+to+Evaluate+Broad+Question+Answer+Script，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21461&send_immediately=true&force_search=false)

**原文摘要:** Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [76] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad, Vedanti Kshirsagar, Urvi Ranjan, Mahek Nakhua*

**主要类别:** cs.LG

**AI概要:** 为了提高人们对宫颈癌和乳腺癌的认识，降低癌症死亡率，作者开发了一个基于机器学习的分类模型来预测个体是否容易患宫颈癌或乳腺癌，并设计了提供最近医院或癌症治疗中心建议的系统。此外，还计划通过集成健康卡维护医疗记录、开展宣传活动等手段进一步提升公众认知。


<details>
  <summary>更多</summary>
  
**动机:** 宫颈癌、乳腺癌和口腔癌在Telangana地区的筛查比例极低（2020年分别为3.3%、0.3%和2.3%），而早期检测是减少发病率和死亡率的关键，但人们对相关症状及筛查实践的认知非常有限。

**方法:** 开发了一个机器学习分类模型，采用决策树分类算法预测宫颈癌易感性，支持向量分类算法预测乳腺癌易感性；设计了一个基于用户位置或地址提供最近医院或癌症治疗中心建议的系统；计划集成健康卡以维护个人医疗记录并进行宣传推广。

**结果:** 成功开发了ML分类模型用于预测宫颈癌和乳腺癌易感性，并设计了推荐最近医疗机构的系统，有助于提高癌症意识、降低死亡率和增加癌症知识普及。

**结论:** 通过这一解决方案，作者离实现目标更近了一步，即通过提高癌症意识来减少癌症死亡率并提升Telangana地区人民的癌症知识水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Devising+a+solution+to+the+problems+of+Cancer+awareness+in+Telangana，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21500，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21500&send_immediately=true&force_search=false)

**原文摘要:** According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [77] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li, Chenrui Fan, Tianyi Zhou*

**主要类别:** cs.LG

**AI概要:** 研究发现大规模语言模型预训练中的grokking现象，并揭示其内部动态机制，提出两个新度量方法来预测泛化性能，具有理论和实践意义。


<details>
  <summary>更多</summary>
  
**动机:** 近期在神经网络训练中观察到的“grokking”现象使得泛化机制和其他新兴能力（如推理）变得神秘。之前的研究所用模型较小、任务简单且训练周期长，本研究首次探索大规模语言模型预训练过程中的grokking现象，以揭示其背后机制。

**方法:** 在7B参数的大语言模型（LLM）即OLMoE的一次性预训练过程中，计算训练损失并评估在多个基准任务上的泛化性能，包括数学推理、代码生成和常识/领域特定知识检索任务。通过分析训练样本的通路演化（即跨层专家选择），研究了grokking现象的内部动态。提出两个新指标来量化通路距离和单个通路的复杂性。

**结果:** 验证了grokking现象在大规模基础模型预训练中依然存在；发现训练样本的通路从随机、实例特定演变为更具结构化和可共享的形式；样本通路复杂度在损失收敛后继续降低，表明记忆到泛化的转换发生；提出的两个新度量能够有效预测下游任务的泛化改进。

**结论:** Grokking现象在大规模基础模型的预训练中仍然存在，尽管不同数据可能异步进入grokking阶段。通过研究LLM内部动态，揭示了记忆到泛化的转换机制，并提出了两种新的度量方法来量化通路距离和单个通路的复杂性，可以预测下游任务的泛化改进。这些度量方法计算简单且仅依赖于训练数据，具有实际预训练中的应用价值。理论上，更结构化的通路能够降低模型复杂度并提高泛化界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+to+find+Grokking+in+LLM+Pretraining%3F+Monitor+Memorization-to-Generalization+without+Test，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21551&send_immediately=true&force_search=false)

**原文摘要:** Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio, Tegan Maharaj, Luke Ong, Stuart Russell, Dawn Song, Max Tegmark, Lan Xue, Ya-Qin Zhang, Stephen Casper, Wan Sie Lee, Sören Mindermann, Vanessa Wilfred, Vidhisha Balachandran, Fazl Barez, Michael Belinsky, Imane Bello, Malo Bourgon, Mark Brakel, Siméon Campos, Duncan Cass-Beggs, Jiahao Chen, Rumman Chowdhury, Kuan Chua Seah, Jeff Clune, Juntao Dai, Agnes Delaborde, Nouha Dziri, Francisco Eiras, Joshua Engels, Jinyu Fan, Adam Gleave, Noah Goodman, Fynn Heide, Dan Hendrycks, Cyrus Hodes, Bryan Low Kian Hsiang, Minlie Huang, Sami Jawhar, Wang Jingyu, Adam Tauman Kalai, Meindert Kamphuis, Mohan Kankanhalli, Subhash Kantamneni, Mathias Bonde Kirk, Thomas Kwa, Jeffrey Ladish, Kwok-Yan Lam, Wan Lee Sie, Taewhi Lee, Xiaojian Li, Jiajun Liu, Chaochao Lu, Yifan Mai, Richard Mallah, Julian Michael, Nick Moës, Simon Möller, Kihyuk Nam, Kwan Yee Ng, Mark Nitzberg, Besmira Nushi, Seán O hÉigeartaigh, Alejandro Ortega, Pierre Peigné, James Petrie, Benjamin Prud'Homme, Reihaneh Rabbany, Nayat Sanchez-Pi, Sarah Schwettmann, Buck Shlegeris, Saad Siddiqui, Aradhana Sinha, Martín Soto, Cheston Tan, Dong Ting, Robert Trager, Brian Tse, Anthony Tung K. H., Vanessa Wilfred, John Willes, Denise Wong, Wei Xu, Rongwu Xu, Yi Zeng, HongJiang Zhang, Djordje Žikelić*

**主要类别:** cs.AI

**AI概要:** AI能力的快速提升和自主性带来了巨大的变革潜力，同时也引发了关于如何确保AI安全（可信赖、可靠和安全）的激烈讨论。建立一个值得信赖的生态系统至关重要，它能帮助人们自信地接受AI，并为创新提供最大空间，同时避免反对。本文总结了2025年新加坡AI会议的报告，该报告基于Yoshua Bengio牵头的国际AI安全报告，并通过纵深防御模型将AI安全研究领域分为三类：创建可信赖AI系统的挑战（开发）、评估其风险的挑战（评估）以及部署后的监控与干预挑战（控制）。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI技术的快速发展和自主性的提高，确保AI的安全性变得尤为重要。为了促进这一领域的研究，需要识别和综合AI安全的研究优先事项，从而支持全球范围内的科学家共同合作。

**方法:** 通过组织国际科学交流会议（如2025年新加坡AI会议），汇集来自不同地区的AI科学家，确定并整合AI安全的研究重点。采用纵深防御模型，将AI安全研究领域分为三个主要类型：开发、评估和控制。

**结果:** 成功构建了一个框架，用于组织AI安全研究的三大领域，并在国际AI安全报告的基础上进一步推进了相关研究议程。

**结论:** 建立值得信赖的AI生态系统对于推动AI技术的安全发展至关重要。通过国际合作和科学研究，可以更好地应对AI带来的挑战，促进技术的可持续发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Singapore+Consensus+on+Global+AI+Safety+Research+Priorities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20702&send_immediately=true&force_search=false)

**原文摘要:** Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [79] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang*

**主要类别:** cs.AI

**AI概要:** 尽管LLM代理在诸如调度、谈判等任务中的协作日益增多，但现有模型（包括GPT-4o和Claude-2.7-Sonnet）对情境隐私的理解不足，在多轮对话中频繁泄露隐私信息。同时，多代理系统在保护隐私的同时难以完成任务，表明当前模型在这两方面均存在不足。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于评估基于LLM的代理是否具备对情境隐私的理解，并在非对抗性多轮对话中遵循隐私指令，以解决代理协作时可能涉及的隐私问题。

**方法:** 提出一个名为MAGPIE的新基准测试，包含15个领域内的158个高风险现实场景，用于评估LLM代理对情境隐私数据的理解以及在协作过程中不违反用户隐私的能力。

**结果:** 实验结果表明，当前最先进的LLM（如GPT-4o和Claude-2.7-Sonnet）对情境隐私的理解不足，分别有25.2%和43.6%的概率将私密数据错误分类为可共享数据；在多轮对话中，即使明确给出隐私指令，这些模型仍分别有59.9%和50.5%的概率泄露隐私信息。此外，多代理系统在71%的情况下无法完成任务。

**结论:** 当前的LLM代理模型未能有效结合情境隐私保护与协作任务解决能力，需要进一步改进以提高隐私保护水平和任务完成效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAGPIE%3A+A+dataset+for+Multi-AGent+contextual+PrIvacy+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20737&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [80] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang, Haijun Zhai, Chaitanya Belwal, Vineeth Thayanithi, Philip Baumann, Yogesh K Roy*

**主要类别:** cs.AI

**AI概要:** This paper introduces a dynamic context-aware prompt recommendation system for domain-specific AI applications, combining contextual query analysis, retrieval-augmented knowledge grounding, hierarchical skill organization, and adaptive skill ranking to generate high-quality prompts. Experiments show high usefulness and relevance of the approach.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of crafting high-quality prompts for domain-specific AI applications which are highly susceptible to the quality of user prompts.

**方法:** The system uses contextual query analysis, retrieval-augmented knowledge grounding, hierarchical skill organization, and adaptive skill ranking. It leverages behavioral telemetry and a two-stage hierarchical reasoning process to dynamically select and rank relevant skills, synthesizing prompts with predefined and adaptive templates enhanced by few-shot learning.

**结果:** Experiments on real-world datasets demonstrate high usefulness and relevance of the generated prompts, validated by both automated and expert evaluations.

**结论:** The novel dynamic context-aware prompt recommendation system effectively generates relevant and actionable prompt suggestions for domain-specific AI applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Context-Aware+Prompt+Recommendation+for+Domain-Specific+AI+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20815，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20815&send_immediately=true&force_search=false)

**原文摘要:** LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [81] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun, Denghui Zhang, ChengXiang Zhai, Heng Ji*

**主要类别:** cs.AI

**AI概要:** 提出了一种概念验证框架，评估语言模型生成的建议在社会系统中的宏观传播，并引入了100个间接伤害场景的数据集来测试模型对长期安全性的意识。该方法在新数据集上提高了20%以上，在现有安全基准上的胜率超过70%。


<details>
  <summary>更多</summary>
  
**动机:** 随着基于语言模型的代理在高风险社会决策（如公共政策和医疗）中影响日益增大，确保其积极影响需要理解其建议的深远影响。

**方法:** 提出了一个概念验证框架，用以模拟模型生成的建议如何在社会系统中随时间宏观传播；同时引入了一个包含100个间接伤害场景的数据集，用于测试模型预见非明显有害结果的能力。

**结果:** 该方法在新数据集上表现优于基线模型超过20%，并且在现有的安全基准测试（AdvBench、SafeRLHF、WildGuardMix）中平均胜率超过70%。

**结论:** 此研究提供了一个有希望的方向，可以开发更安全的语言模型代理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Reactive+Safety%3A+Risk-Aware+LLM+Alignment+via+Long-Horizon+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20949&send_immediately=true&force_search=false)

**原文摘要:** Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [82] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了大语言模型（LLMs）在因果推理方面的能力，揭示其目前仅能进行浅层因果推理，并提出了一种新方法G^2-Reasoner以提升LLMs的因果推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大语言模型（LLMs）在因果推理方面的能力尚不明确，特别是是否能够像人类一样进行深层因果推理。

**方法:** 通过分析Transformer模型的自回归机制，引入新的因果问答基准CausalProbe-2024，并提出结合通用知识和目标导向提示的方法G^2-Reasoner。

**结果:** 实验表明，G^2-Reasoner显著提升了LLMs在新鲜和反事实情境中的因果推理能力。

**结论:** 这项工作为LLMs迈向真正的因果推理提供了一条新路径，超越了当前的浅层因果推理，向深层因果推理迈进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unveiling+Causal+Reasoning+in+Large+Language+Models%3A+Reality+or+Mirage%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21215&send_immediately=true&force_search=false)

**原文摘要:** Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [83] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng QIu*

**主要类别:** cs.AI

**AI概要:** 大型视觉-语言模型（LVLMs）在具身化规划任务中展现出潜力，但在涉及不熟悉环境和多步骤目标的复杂场景中表现不佳。本文提出了一种名为World-Aware Planning Narrative Enhancement (WAP)的框架，通过四种认知能力提升LVLMs对环境的理解，并仅使用原始视觉观察进行模型开发和评估。实验结果表明，Qwen2.5-VL在任务成功率上提升了60.7%，尤其在常识推理和长期规划方面表现出色。开源模型显著优于GPT-4o和Claude-3.5-Sonnet等专有系统。


<details>
  <summary>更多</summary>
  
**动机:** 当前方法依赖于与环境无关的模仿学习，这导致模型难以处理情境敏感的指令，并在长时间交互中依赖补充线索而非视觉推理。为解决这一问题，需要一种能增强模型对环境全面理解的新方法。

**方法:** 提出了World-Aware Planning Narrative Enhancement (WAP)框架，该框架通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法接地）来增强LVLMs对环境的理解。同时，模型开发和评估仅依赖原始视觉观察并通过课程学习实现。

**结果:** 在EB-ALFRED基准上的评估显示，Qwen2.5-VL在任务成功率上绝对提升了60.7%，特别是在常识推理（+60.0）和长期规划（+70.0）方面。此外，开源模型大幅超越了GPT-4o和Claude-3.5-Sonnet等专有系统。

**结论:** WAP框架有效增强了LVLMs在具身化规划任务中的性能，特别是在复杂场景下的常识推理和长期规划能力。此研究证明了通过强化环境理解来改进LVLMs的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是World-aware+Planning+Narratives+Enhance+Large+Vision-Language+Model+Planner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21230&send_immediately=true&force_search=false)

**原文摘要:** Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [84] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann, Mario Nadj, Christian Janiesch*

**主要类别:** cs.AI

**AI概要:** 开发了一个名为IXAII的交互式可解释智能系统，结合了四种可解释AI方法，并为五类用户群体提供定制视图。通过专家和普通用户的访谈评估，结果表明IXAII有助于提高透明度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的可解释AI方法大多是静态的，忽视了用户视角，限制了对目标受众的有效性。

**方法:** 创建了一个名为IXAII的交互式可解释智能系统，该系统提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。原型为五类用户群体提供定制视图，并允许用户控制解释的内容和格式。

**结果:** 通过与专家和普通用户的访谈评估，IXAII被感知为有助于提高透明度，提供了不同的解释和多种可视化选项。

**结论:** 通过弥合可解释AI方法、交互性和实际实施之间的差距，IXAII提供了关于AI解释实践和人机交互的新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IXAII%3A+An+Interactive+Explainable+Artificial+Intelligence+Interface+for+Decision+Support+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21310&send_immediately=true&force_search=false)

**原文摘要:** Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [85] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

**主要类别:** cs.AI

**AI概要:** 这篇论文强调了当前人工智能系统在科学发现中的局限性，并提出了通过弥合抽象差距、推理差距和现实差距来推动AI驱动的科学发展。它定义了一种新的主动推断AI系统架构，该架构结合了因果自监督基础模型、贝叶斯约束的符号或神经符号规划器、持续知识图谱以及与高保真模拟器和自动化实验室的闭环交互。文章还指出人类判断在处理模拟和实验反馈中的固有模糊性和不确定性方面的重要性，认为其应作为永久架构组件纳入系统。


<details>
  <summary>更多</summary>
  
**动机:** 作者指出了当前AI系统在科学发现中的几个关键限制：操作架构的局限性、脆弱的推理机制以及与实验现实的分离。基于这些观察，他们试图提出一种新的方法来解决这些问题，以实现更深层次的科学推理和发现。

**方法:** 论文提出了一种名为'主动推断AI系统'的新架构，该架构具有四个主要特点：(i)基于因果自监督基础模型的研究记忆；(ii)带有贝叶斯护栏的符号或神经符号规划器；(iii)不断增长的知识图谱，其中包含由推理建立的因果关系和通过现实世界互动验证的概念节点；(iv)与高保真模拟器和自动化实验室的闭环交互，用于改进内部表示。

**结果:** 通过这种方法，AI系统可以进行反事实推理，并通过外部验证将假设根植于现实，从而实现科学发现。此外，文章表明，人类判断对于解释模拟和实验中的不确定性和模糊性是不可或缺的。

**结论:** 为了推动AI驱动的科学发展，需要开发能够弥合抽象、推理和现实差距的新一代AI系统。同时，人类判断应该被视为AI系统的一个永久组成部分，而不是一个临时辅助工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Inference+AI+Systems+for+Scientific+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21329&send_immediately=true&force_search=false)

**原文摘要:** The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [86] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang, Pu Chen, Yin Zhang*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的架构TableMoE，专门用于处理多模态表格数据的稳健结构推理。通过创新的神经符号路由机制和大规模预训练数据集，TableMoE在多个基准测试中显著超越现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的表格理解具有挑战性，因为其结构复杂、符号密集以及视觉退化（如模糊、倾斜、水印等）。现有的多模态大语言模型在这些条件下表现不佳，性能有限且泛化能力差。

**方法:** 提出了TableMoE，一种神经符号混合连接专家（MoCE）架构，包含神经符号路由机制，预测语义角色并将表格元素动态路由到特定专家模块。使用了大规模的TableMoE-Align数据集进行预训练，并创建了四个挑战性的基准测试来评估模型。

**结果:** 实验结果表明，TableMoE显著超越现有的最先进模型。广泛的消融研究验证了每个核心组件的重要性，强调了神经符号路由和结构化专家对齐的关键作用。

**结论:** TableMoE展示了其在多模态表格理解中的可解释性和增强的鲁棒性，证明了整合神经符号推理的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TableMoE%3A+Neuro-Symbolic+Routing+for+Structured+Expert+Reasoning+in+Multimodal+Table+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21393，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21393&send_immediately=true&force_search=false)

**原文摘要:** Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [87] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, Li Fei-Fei*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了视觉语言模型（VLMs）是否能够像人类一样从有限的视角想象整个场景，并提出了新的MindCube基准来评估VLMs在构建空间心理模型方面的能力。通过训练模型生成认知地图并进行推理，准确率从37.8%提升到60.8%，再结合强化学习进一步提升至70.7%。


<details>
  <summary>更多</summary>
  
**动机:** 研究的动机在于探索视觉语言模型是否可以像人类一样形成空间心理模型，以处理布局、视角和运动等方面的推理问题。当前的VLMs在这方面表现接近随机水平，因此需要找到方法来改进其性能。

**方法:** 研究人员首先创建了一个名为MindCube的新基准，包含21,154个问题和3,268张图像，用于系统性地评估VLMs在位置表示、方向感知和动态模拟等方面的能力。接着，他们探索了三种方法：未见中间视图、自然语言推理链和认知地图。最终提出了一种协同方法“map-then-reason”，即先生成认知地图，然后基于此进行推理。此外，还加入了强化学习以进一步提升性能。

**结果:** 通过使用“map-then-reason”方法，模型的准确率从37.8%提升到了60.8%，增加了23.0%。而加入强化学习后，准确率进一步提高到了70.7%，增加了32.9%。

**结论:** 研究表明，通过积极构建和利用内部结构化的空间表示，并结合灵活的推理过程，可以显著改善VLMs对不可观测空间的理解。这种方法为未来开发更强大的空间心理模型提供了关键见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatial+Mental+Modeling+from+Limited+Views，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21458&send_immediately=true&force_search=false)

**原文摘要:** Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [88] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević, Ravi Hammond, Tobias Gessler, Anisoara Calinescu, Jonathan Cook, Matteo Gallici, Andrei Lupu, Jakob Nicolaus Foerster*

**主要类别:** cs.AI

**AI概要:** 为了解决人类评估的高昂成本和难以重现的问题，本文提出了即兴人类AI协作挑战（AH2AC2），并开发了大规模的人类数据集代理模型以进行更高效、经济且可重复的评估。


<details>
  <summary>更多</summary>
  
**动机:** 在实际应用中，实现AI智能体与人类之间的无缝协作至关重要，但仍然是一个重要的开放性挑战。Hanabi作为一种合作性卡牌游戏，具有不完美信息、受限沟通、理论思维需求和协调行动等特点，是测试人类-AI协作的理想平台。然而，由于人类评估的挑战，其在人类-AI互动中的应用受到限制。

**方法:** 引入了Ad-Hoc Human-AI Coordination Challenge (AH2AC2) 来克服高昂和难以重现的人类评估问题。开发了基于大规模人类数据集的“人类代理模型”，这些模型可以作为AH2AC2中稳健、经济且可重复的人类样评估伙伴。同时开源了一个包含3,079局游戏的数据集，有意限制了可用的人类游戏数据量，以鼓励开发数据高效的算法。提供了两人和三人Hanabi场景的基线结果。为了确保公平评估，通过受控评估系统托管代理模型，而不是公开发布它们。

**结果:** 成功创建了一个可以替代真实人类评估的代理模型，并证明了该方法在评估AI与人类协作能力方面的有效性。提供了两人和三人Hanabi场景的基线结果，展示了该方法在不同规模下的适用性。

**结论:** AH2AC2及其相关方法为解决人类AI协作中的评估问题提供了一种新的途径，特别是通过使用人类代理模型降低了评估成本并提高了可重复性。这将促进更高效的数据驱动方法的发展，推动人类AI协作领域的研究进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ad-Hoc+Human-AI+Coordination+Challenge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21490，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21490&send_immediately=true&force_search=false)

**原文摘要:** Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [89] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了Mind2Web 2，一个包含130个需要实时网络浏览和广泛信息整合的长周期任务的基准。为了解决评估复杂和时间变化的答案的挑战，提出了Agent-as-a-Judge框架，该框架基于树形结构评分设计构建任务特定的评判代理，以自动评估答案正确性和来源归属。对九个前沿代理搜索系统和人类表现进行了全面评估，并进行了详细的错误分析。表现最好的系统OpenAI Deep Research已经能达到人类表现的50-70%，但花费的时间仅为一半。Mind2Web 2为开发和评估下一代代理搜索系统提供了严格的基矗


<details>
  <summary>更多</summary>
  
**动机:** 现有的评估基准和方法无法应对代理搜索系统的日益增长的复杂性和开放性，特别是对于长周期任务、实时网络浏览和广泛信息整合的需求。因此，需要新的基准和评估方法来衡量这些系统的性能。

**方法:** 构建了Mind2Web 2基准，包含130个高质量的长周期任务，以及Agent-as-a-Judge框架，该框架使用树形结构评分设计构建任务特定的评判代理，用于自动评估答案正确性和来源归属。

**结果:** 对九个前沿代理搜索系统和人类表现进行了全面评估，发现OpenAI Deep Research表现最佳，能达到人类表现的50-70%，但花费的时间仅为一半。同时进行了详细的错误分析，为未来的发展提供了见解。

**结论:** Mind2Web 2为开发和评估下一代代理搜索系统提供了严格的基矗通过引入新的基准和评估方法，可以更好地推动代理搜索系统的研究和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind2Web+2%3A+Evaluating+Agentic+Search+with+Agent-as-a-Judge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21506&send_immediately=true&force_search=false)

**原文摘要:** Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [90] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PsyLite+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21536&send_immediately=true&force_search=false)

**原文摘要:** With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [91] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič, Martin Vuk, Aljaž Zalar*

**主要类别:** stat.ML

**AI概要:** 本文通过解析方法完全回答了关于多变量拟共轭函数的极端值分布问题，并且解决了《搭便车者指南》中提出的开放问题5。


<details>
  <summary>更多</summary>
  
**动机:** 尽管拟共轭函数在依赖性建模领域受到关注，但其缺乏统计解释。作者之前的工作已经部分解决了与拟共轭函数相关的极端值分布问题。

**方法:** 采用线性规划方法解决开放问题5至维度d = 17，并使用解析方法提供对原问题的完整答案。

**结果:** 通过解析方法成功解决了开放问题5并反驳了一个近期的猜想。

**结论:** 解析方法为解决拟共轭函数相关问题提供了完整和有效的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+final+solution+of+the+Hitchhiker%27s+problem+%235，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20672&send_immediately=true&force_search=false)

**原文摘要:** A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [92] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang, Dan Qiao, Yu-Xiang Wang, Rahul Parhi*

**主要类别:** stat.ML

**AI概要:** 研究了在高维输入下，平坦解（flat solutions）对泛化的影响，并通过理论和实验表明，在维度增加时，平坦解的收敛速度会指数级恶化。这是首个系统解释为何在高维情况下平坦极小值可能无法良好泛化的研究。


<details>
  <summary>更多</summary>
  
**动机:** 现有的关于隐式偏差（implicit bias）的研究要么需要插值条件，要么仅关注单变量输入。而多变量输入下的平坦性/低损失曲率对泛化的影响尚未被充分研究。本文旨在填补这一空白，探讨在梯度下降训练中，极小值稳定性和边缘稳定性现象如何影响两层过参数化ReLU网络的泛化性能。

**方法:** 作者分析了两种自然设置：(1) 平坦解的泛化差距；(2) 稳定极小值在非参数函数估计中的均方误差（MSE）。通过构造一个基于边界局部化ReLU神经元的新型填充论证，揭示了平坦解如何利用“神经破碎”现象（neural shattering），即神经元很少激活但权重较大，导致在高维情况下的性能较差。

**结果:** 证明了上界和下界，表明虽然平坦性确实能带来泛化能力，但在输入维度增加时，其收敛速度会指数级恶化。此外，与低范数解（如权重衰减）相比，平坦解在高维情况下表现明显更差。这些结论得到了广泛的数值模拟支持。

**结论:** 平坦解在高维情况下可能导致泛化性能的显著下降，而低范数解则不会受到维度诅咒的影响。这为理解为什么平坦极小值在高维情况下可能失败提供了首个系统的解释。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Minima+of+ReLU+Neural+Networks+Suffer+from+the+Curse+of+Dimensionality%3A+The+Neural+Shattering+Phenomenon，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20779&send_immediately=true&force_search=false)

**原文摘要:** We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [93] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng, Lulu Kang, Yiwei Wang, Chun Liu*

**主要类别:** stat.ML

**AI概要:** This paper introduces an active learning framework for manifold Gaussian Process (GP) regression that combines manifold learning with strategic data selection to improve accuracy in high-dimensional spaces.


<details>
  <summary>更多</summary>
  
**动机:** To improve the accuracy of Gaussian Process regression in high-dimensional spaces by combining manifold learning with strategic data selection.

**方法:** Jointly optimizes a neural network for dimensionality reduction and a Gaussian process regressor in the latent space, supervised by an active learning criterion that minimizes global prediction error.

**结果:** Experiments on synthetic data demonstrate superior performance over randomly sequential learning. The framework efficiently handles complex, discontinuous functions while preserving computational tractability.

**结论:** The proposed framework offers practical value for scientific and engineering applications. Future work will focus on scalability and uncertainty-aware manifold learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning+for+Manifold+Gaussian+Process+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20928&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [94] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn, Frederick Eberhardt, Leonard J. Schulman*

**主要类别:** stat.ML

**AI概要:** 当假设被放松时，马尔可夫等价类的大小会显著增大。


<details>
  <summary>更多</summary>
  
**动机:** 因果发现算法通常只能恢复到马尔可夫等价类，除非增加额外参数假设。理解这些等价类的大小有助于了解仅从观察数据中能学到什么。

**方法:** 证明在三种情况下马尔可夫等价类的期望大小具有指数级下界：稀疏随机有向无环图、随机有向混合图和随机有向循环图。

**结果:** 展示了在放松任一假设（无环性、因果充分性和均匀模型先验）时，马尔可夫等价类不再平均较小。

**结论:** 当任何基本假设被放松时，马尔可夫等价类的大小可能会显著增大。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lower+Bounds+on+the+Size+of+Markov+Equivalence+Classes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20933&send_immediately=true&force_search=false)

**原文摘要:** Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [95] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang, Hayden Hampton*

**主要类别:** stat.ML

**AI概要:** 论文介绍了一种名为STFT-VNNGP的混合架构，该模型在2023年威胁检测算法竞赛中获胜。它通过两阶段过程克服了数据稀疏性、突发性和过度分散的问题，提高了长期预测的可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度学习模型（如TFT）在处理具有稀疏性、突发性和过度分散特性的地缘政治冲突数据时，难以生成可靠的长期预测。

**方法:** STFT-VNNGP采用两阶段方法：首先使用TFT捕捉复杂的时间动态以生成多分位数预测；然后将这些分位数作为变分最近邻高斯过程（VNNGP）的输入，进行空间时间平滑和不确定性量化。

**结果:** 在中东和美国的地缘政治冲突预测案例研究中，STFT-VNNGP持续优于单独的TFT模型，特别是在长时间范围内对事件爆发期的时机和规模预测表现更优。

**结论:** STFT-VNNGP为从困难的事件数据中生成更可靠和可操作的情报提供了一个强大的框架，并且所有代码和工作流程都公开以确保可重复性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forecasting+Geopolitical+Events+with+a+Sparse+Temporal+Fusion+Transformer+and+Gaussian+Process+Hybrid%3A+A+Case+Study+in+Middle+Eastern+and+U.S.+Conflict+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20935&send_immediately=true&force_search=false)

**原文摘要:** Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [96] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种新的方法，用于近似多个强化学习（RL）代理在有限状态马尔可夫博弈中交互的学习动态。通过同时降低学习率和增加更新频率来重新缩放学习过程，将代理的参数视为受快速混合游戏状态影响的缓慢演变变量。在状态过程的遍历性和更新的连续性等温和假设下，证明了这种重新缩放的过程收敛到一个常微分方程（ODE）。该ODE提供了一个可控的、确定性的代理学习动态近似。


<details>
  <summary>更多</summary>
  
**动机:** 研究多个强化学习代理在有限状态马尔可夫博弈中交互时的学习动态，为了解决现有方法可能无法准确捕捉复杂交互动态的问题。

**方法:** 提出一种新的方法，通过降低学习率和增加更新频率来重新缩放学习过程，将代理参数视为慢变量，并在温和假设下证明其收敛到一个常微分方程。

**结果:** 证明了重新缩放的过程收敛到一个ODE，该ODE可以有效地近似代理的学习动态。

**结论:** 提出的方法为理解多个强化学习代理在马尔可夫博弈中的学习动态提供了一个新的视角和工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Homogenization+of+Multi-agent+Learning+Dynamics+in+Finite-state+Markov+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21079&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [97] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica, Kurt Hornik*

**主要类别:** stat.ML

**AI概要:** We propose a novel VAE with spherical Cauchy latent distribution that offers natural hyperspherical representation, efficient latent space utilization, and stable training.


<details>
  <summary>更多</summary>
  
**动机:** Traditional Gaussian latent spaces and vMF distribution have limitations in capturing directional data and suffer from numerical instabilities.

**方法:** A new VAE architecture using spherical Cauchy (spCauchy) latent distribution which provides a more natural hyperspherical representation, prevents over-regularization, and allows for stable and scalable training via Möbius transformations.

**结果:** The KL divergence can be computed efficiently through a rapidly converging power series, ensuring numerical stability and scalability.

**结论:** spCauchy is an advantageous alternative for VAEs providing both theoretical benefits and practical efficiency in high-dimensional generative modeling.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperspherical+Variational+Autoencoders+Using+Efficient+Spherical+Cauchy+Distribution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21278&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [98] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种名为wild refitting的高效计算方法，用于计算基于最小二乘法的惩罚非参数估计的实例均方预测误差的高概率上界。该方法包括三个步骤：计算合适的残差、对称化和缩放它们，并用它们来定义和解决一个修改后的预测问题。在相对温和的条件下，该方法可以提供预测误差的上界。


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏一种有效的方法来计算实例均方预测误差的高概率上界，尤其是在只需要单一数据集和黑箱访问预测方法的情况下。

**方法:** wild refitting方法包含三个步骤：1) 计算适当的残差；2) 使用预因子ρ对残差进行对称化和缩放；3) 用这些处理过的残差定义并解决一个以当前估计为中心的修改后的预测问题。此方法利用Rademacher残差对称化技术。

**结果:** 在相对较弱的条件下（允许噪声异质性），wild refitting方法能够以高概率保证其性能，给出预测误差的上界。

**结论:** wild refitting方法为设计此类过程提供了理论指导，包括如何形成残差、在子问题中需要多少噪声重新缩放以获得上界以及黑箱程序的局部稳定性属性。这种方法适用于各种问题，如具有结构化矩阵惩罚的非刚性结构运动恢复、使用深度神经网络先验的插件图像恢复以及核方法的随机草图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wild+refitting+for+black+box+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21460，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21460&send_immediately=true&force_search=false)

**原文摘要:** We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [99] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias, Angelos Alexopoulos, Siran Liu, Petros Dellaportas*

**主要类别:** stat.ML

**AI概要:** 开发了高斯不变量版本的随机游走梅特罗波利斯（RWM）、调整后的朗之万算法（MALA）以及二阶海森或流形MALA采样方法，这些方法在高斯不变量下表现出更优的统计效率，并通过解析解构建控制变量以减少估计量的方差。实验表明，新方法在高维潜在高斯模型中达到领先水平。


<details>
  <summary>更多</summary>
  
**动机:** 当前的RWM和MALA方法在处理特定问题时可能面临统计效率不足的问题，因此需要一种改进的方法来提高采样器的性能，尤其是在高斯目标分布的情况下。

**方法:** 提出了高斯不变量版本的RWM、MALA和二阶海森或流形MALA等采样方法，利用高斯不变量特性获得泊松方程的精确解析解，从而构建有效的控制变量用于减少估计量的方差。

**结果:** 在多个例子中验证了新采样器和估计量的有效性，特别是在高维潜在高斯模型中，与多种先进方法比较后，取得了领先的成果。同时提供了关于几何遍历性的理论结果和最优缩放分析。

**结论:** 高斯不变量采样方法能够生成具有更高统计效率的遍历估计量，其优越性在于能够利用高斯不变量特性获得泊松方程的精确解析解，进而构建有效的控制变量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gaussian+Invariant+Markov+Chain+Monte+Carlo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21511&send_immediately=true&force_search=false)

**原文摘要:** We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>
