<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 59]
- [cs.AI](#cs.AI) [总数: 13]
- [stat.ML](#stat.ML) [总数: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin, Ipek Baris Schlicht, Ngoc Vo Hong, Sara Allievi, Jacopo Staiano, Pasquale Minervini, Andrea Passerini*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种混合人机框架MedSyn，通过医生与大型语言模型之间的多步骤互动对话来改进诊断和治疗决策，并通过模拟交互评估了开源大型语言模型作为医生助手的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 临床决策过程复杂，受认知偏差、信息不全和案例模糊性的影响。尽管大型语言模型（LLMs）在支持临床决策方面显示出潜力，但其通常的一次性或有限互动使用可能忽略了现实世界医疗实践中的复杂性。

**方法:** 研究者提出了一个名为MedSyn的混合人机框架，在该框架中医生和LLMs进行多步骤、互动式的对话以细化诊断和治疗决定。通过模拟医生-LLM的互动，研究团队评估了开源LLMs作为医生助手的潜力。

**结果:** 结果显示，开源LLMs在现实世界中作为医生助手具有相当大的潜力。

**结论:** 未来的工作将涉及真实医生的互动，以进一步验证MedSyn在提高诊断准确性和患者结果方面的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MedSyn%3A+Enhancing+Diagnostics+with+Human-AI+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14774&send_immediately=true&force_search=false)

**原文摘要:** Clinical decision-making is inherently complex, often influenced by cognitive
biases, incomplete information, and case ambiguity. Large Language Models
(LLMs) have shown promise as tools for supporting clinical decision-making, yet
their typical one-shot or limited-interaction usage may overlook the
complexities of real-world medical practice. In this work, we propose a hybrid
human-AI framework, MedSyn, where physicians and LLMs engage in multi-step,
interactive dialogues to refine diagnoses and treatment decisions. Unlike
static decision-support tools, MedSyn enables dynamic exchanges, allowing
physicians to challenge LLM suggestions while the LLM highlights alternative
perspectives. Through simulated physician-LLM interactions, we assess the
potential of open-source LLMs as physician assistants. Results show open-source
LLMs are promising as physician assistants in the real world. Future work will
involve real physician interactions to further validate MedSyn's usefulness in
diagnostic accuracy and patient outcomes.

</details>


### [2] [Two-dimensional Parallel Tempering for Constrained Optimization](https://arxiv.org/abs/2506.14781)
*Corentin Delacour, M Mahmudul Hasan Sajeeb, Joao P. Hespanha, Kerem Y. Camsari*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种二维并行回火算法(2D-PT)，该方法通过增加惩罚强度的维度来解决Ising模型中软约束导致的问题，从而改善了受约束副本的混合，并消除了对惩罚强度进行显式调优的需求。


<details>
  <summary>更多</summary>
  
**动机:** 采样玻尔兹曼概率分布是机器学习和优化中的关键环节，推动了诸如Ising机这样的硬件加速器的设计。尽管Ising模型原则上可以编码任意优化问题，但实际实现往往受到软约束的阻碍，这些约束太强时会减慢混合速度，太弱时则无法保证可行性。

**方法:** 本文引入了一个强大的并行回火算法（PT）的二维扩展，通过添加一个插值惩罚强度的第二个维度来解决这个问题。这种方案确保最终副本中的约束满足，类似于低温下的低能态。

**结果:** 在具有复制约束的图稀疏化代表性例子中，2D-PT实现了接近理想的混合，Kullback-Leibler散度随时间以O(1/t)的速度衰减。当应用于稀疏化的Wishart实例时，与相同数量副本的传统PT相比，2D-PT带来了数量级上的加速。

**结论:** 所提出的二维并行回火算法(2D-PT)广泛适用于有约束的Ising问题，并且可以在现有的Ising机器上部署，它通过改善受限副本的混合情况和消除显式调整惩罚强度的需要，提供了一种有效的方法来处理这类问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-dimensional+Parallel+Tempering+for+Constrained+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14781&send_immediately=true&force_search=false)

**原文摘要:** Sampling Boltzmann probability distributions plays a key role in machine
learning and optimization, motivating the design of hardware accelerators such
as Ising machines. While the Ising model can in principle encode arbitrary
optimization problems, practical implementations are often hindered by soft
constraints that either slow down mixing when too strong, or fail to enforce
feasibility when too weak. We introduce a two-dimensional extension of the
powerful parallel tempering algorithm (PT) that addresses this challenge by
adding a second dimension of replicas interpolating the penalty strengths. This
scheme ensures constraint satisfaction in the final replicas, analogous to
low-energy states at low temperature. The resulting two-dimensional parallel
tempering algorithm (2D-PT) improves mixing in heavily constrained replicas and
eliminates the need to explicitly tune the penalty strength. In a
representative example of graph sparsification with copy constraints, 2D-PT
achieves near-ideal mixing, with Kullback-Leibler divergence decaying as
O(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of
magnitude speedup over conventional PT with the same number of replicas. The
method applies broadly to constrained Ising problems and can be deployed on
existing Ising machines.

</details>


### [3] [Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials](https://arxiv.org/abs/2506.14782)
*Joseph Geraci, Bessi Qorri, Christian Cumbaa, Mike Tsay, Paul Leonczyk, Luca Pani*

**主要类别:** cs.LG

**AI概要:** 本文介绍了NetraAI，一种结合了动力系统、信息几何和进化算法的框架，专门用于小规模临床试验数据集的稳定性和可解释性。同时引入了大规模语言模型DeepSeek-V3作为策略层，指导发现过程。通过在精神分裂症、抑郁症和胰腺癌的研究案例中，NetraAI能够识别出具有高效应量的小群体，从而将基线模型转化为近乎完美的分类器。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在解决在处理小规模临床试验数据时所面临的挑战，通过开发一种新的框架NetraAI来提高模型的稳定性与可解释性，并且能够从少量特征中提取关键信息以改善分类效果。此外，还希望通过结合大规模语言模型的能力，进一步提升发现过程中的理论洞察力。

**方法:** NetraAI基于动力系统方法构建，利用收缩映射、信息几何以及进化算法等技术手段，实现对患者群组的有效预测。它首先将特征嵌入度量空间，并通过迭代方式朝向稳定的吸引子收缩，进而定义潜在子群。该框架还包括伪时间嵌入和长程记忆机制，用以探索更高阶特征间的交互作用。另外，内部进化循环选择紧凑且易于解释的2-4个变量组合（称为“Persona”）。为了增强发现能力，研究者引入了一个大规模语言模型作为元进化层级，负责观察Persona输出、优先考虑有前景的变量、注入领域知识并评估稳健性。

**结果:** 实验结果表明，在精神分裂症、抑郁症及胰腺癌三个案例研究中，NetraAI成功地识别出了具有显著效应值的小规模亚群，这使得原本表现不佳的基础模型（AUC约为0.50-0.68）仅依靠少数几个特征就能转变为接近完美的分类器。

**结论:** 通过结合动力系统、信息几何学与进化学习的方法，NetraAI为临床研究提供了一种新颖而强大的工具，能够在保证高度可解释性的前提下大幅提升模型性能。其与大规模语言模型相结合的方式，模拟了人类科学研究过程中实验家与理论家之间的互动模式，形成了一种自我优化的学习循环。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Dynamical+Systems+Learning+with+Foundational+Models%3A+A+Meta-Evolutionary+AI+Framework+for+Clinical+Trials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14782&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence (AI) has evolved into an ecosystem of specialized
"species," each with unique strengths. We analyze two: DeepSeek-V3, a
671-billion-parameter Mixture of Experts large language model (LLM)
exemplifying scale-driven generality, and NetraAI, a dynamical system-based
framework engineered for stability and interpretability on small clinical trial
datasets. We formalize NetraAI's foundations, combining contraction mappings,
information geometry, and evolutionary algorithms to identify predictive
patient cohorts. Features are embedded in a metric space and iteratively
contracted toward stable attractors that define latent subgroups. A
pseudo-temporal embedding and long-range memory enable exploration of
higher-order feature interactions, while an internal evolutionary loop selects
compact, explainable 2-4-variable bundles ("Personas").
  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary
layer that observes Persona outputs, prioritizes promising variables, injects
domain knowledge, and assesses robustness. This two-tier architecture mirrors
the human scientific process: NetraAI as experimentalist, the LLM as theorist,
forming a self-improving loop.
  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI
uncovered small, high-effect-size subpopulations that transformed weak baseline
models (AUC ~0.50-0.68) into near-perfect classifiers using only a few
features. We position NetraAI at the intersection of dynamical systems,
information geometry, and evolutionary learning, aligned with emerging
concept-level reasoning paradigms such as LeCun's Joint Embedding Predictive
Architecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI
offers a new generation of adaptive, self-reflective AI to accelerate clinical
discovery.

</details>


### [4] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry, Mohamed Amen, Mohamed Elzyat, Mohamed Hamed, Norhan Magdy, Maram Khaled*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合脑电图(EEG)和眼动追踪数据的ETS框架，用于开放词汇场景下的文本生成和情感分类任务。该模型在EEG到文本解码方面取得了较高的BLEU和Rouge分数，在基于EEG的情感分类上F1分数提高了10%，超越了监督基线，并且能够处理来自不同受试者和来源的数据。


<details>
  <summary>更多</summary>
  
**动机:** 使用非侵入性脑电图（EEG）从大脑活动中解码自然语言是神经科学和机器学习中的一个重大挑战，尤其是在开放词汇场景中传统方法难以应对噪声和可变性。尽管以前的研究已经在小范围封闭词汇表上达到了高精度，但在开放词汇表上的表现仍然不佳。

**方法:** 研究人员提出了ETS框架，它将EEG与同步的眼动追踪数据结合起来，以解决两个关键任务：开放词汇表的文本生成以及感知语言的情绪分类。

**结果:** 该模型在EEG到文本解码方面获得了更好的BLEU和Rouge分数，并且在基于EEG的三元情绪分类上最高提升了10%的F1分数，这显著优于监督基线。此外，研究表明提出的模型可以处理来自不同主体和源的数据。

**结论:** ETS框架显示了在高性能开放词汇表EEG到文本系统方面的巨大潜力，能够在文本生成和情绪分类任务中取得优异表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ETS%3A+Open+Vocabulary+Electroencephalography-To-Text+Decoding+and+Sentiment+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14783&send_immediately=true&force_search=false)

**原文摘要:** Decoding natural language from brain activity using non-invasive
electroencephalography (EEG) remains a significant challenge in neuroscience
and machine learning, particularly for open-vocabulary scenarios where
traditional methods struggle with noise and variability. Previous studies have
achieved high accuracy on small-closed vocabularies, but it still struggles on
open vocabularies. In this study, we propose ETS, a framework that integrates
EEG with synchronized eye-tracking data to address two critical tasks: (1)
open-vocabulary text generation and (2) sentiment classification of perceived
language. Our model achieves a superior performance on BLEU and Rouge score for
EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment
classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for high performance open
vocabulary eeg-to-text system.

</details>


### [5] [Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation](https://arxiv.org/abs/2506.14784)
*Emre Yilmaz, Philipp Bekemeyer*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于迁移学习的方法来预测迎流参数，如攻角和迎流速度。该方法首先离线训练一个卷积神经网络模型用于核心预测任务，然后冻结除输出节点前选定层之外的所有权重，最后通过重新训练这些层来进行迁移学习。实验表明，此方法在适应数据分布变化、领域扩展和任务更新方面具有潜力，但在处理噪声数据时效果不佳。


<details>
  <summary>更多</summary>
  
**动机:** 传统的迎流参数预测依赖于直接测量，这在传感器故障的情况下可能会遇到挑战。因此，需要一种基于表面压力数据的数据驱动预测模型，并且这种预测器需要接近实时的学习能力以满足实际应用需求，例如监控风洞操作或学习航空航天和风能系统的空气动力学性能变化。

**方法:** 提出了一种迁移学习方法论，首先离线训练一个卷积神经网络(ConvNet)模型完成主要预测任务；接着除了输出节点前选定的几层以外，冻结这个模型的其他权重；最后通过对这几层进行再训练来执行迁移学习。

**结果:** 结果成功展示了该方法对于应对数据分布变化、领域拓展以及任务更新方面的潜力，但对于噪声数据的应用则被认为不那么有效。

**结论:** 所提出的迁移学习方法为迎流参数预测提供了一个有效的解决方案，特别是在面临数据分布变化、领域扩展和任务更新的情况下。然而，对于包含大量噪声的数据集，该方法的表现并不理想。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Onflow+Parameters+Using+Transfer+Learning+for+Domain+and+Task+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14784&send_immediately=true&force_search=false)

**原文摘要:** Determining onflow parameters is crucial from the perspectives of wind tunnel
testing and regular flight and wind turbine operations. These parameters have
traditionally been predicted via direct measurements which might lead to
challenges in case of sensor faults. Alternatively, a data-driven prediction
model based on surface pressure data can be used to determine these parameters.
It is essential that such predictors achieve close to real-time learning as
dictated by practical applications such as monitoring wind tunnel operations or
learning the variations in aerodynamic performance of aerospace and wind energy
systems. To overcome the challenges caused by changes in the data distribution
as well as in adapting to a new prediction task, we propose a transfer learning
methodology to predict the onflow parameters, specifically angle of attack and
onflow speed. It requires first training a convolutional neural network
(ConvNet) model offline for the core prediction task, then freezing the weights
of this model except the selected layers preceding the output node, and finally
executing transfer learning by retraining these layers. A demonstration of this
approach is provided using steady CFD analysis data for an airfoil for i)
domain adaptation where transfer learning is performed with data from a target
domain having different data distribution than the source domain and ii) task
adaptation where the prediction task is changed. Further exploration on the
influence of noisy data, performance on an extended domain, and trade studies
varying sampling sizes and architectures are provided. Results successfully
demonstrate the potential of the approach for adaptation to changing data
distribution, domain extension, and task update while the application for noisy
data is concluded to be not as effective.

</details>


### [6] [PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series](https://arxiv.org/abs/2506.14786)
*Haobo Li, Eunseo Jung, Zixin Chen, Zhaowei Wang, Yueya Wang, Huamin Qu, Alexis Kai Hon Lau*

**主要类别:** cs.LG

**AI概要:** 提出了一种轻量级的方法——物理信息位置编码（PIPE），该方法将物理信息嵌入到视觉语言模型中，以改进多模态时间序列预测，特别是在气候科学中的台风预测。通过在最大的开源卫星图像数据集上的实验，PIPE 在深度学习预测和气候领域方法上都取得了最先进的性能，比之前的工作提高了12%的台风强度预测准确率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态方法主要集中在利用文本数据来辅助时间序列预测，而忽略了现有时间序列数据集中的视觉数据。此外，模型难以有效捕捉视觉数据中嵌入的物理信息，如卫星图像的时间和地理空间背景。

**方法:** 提出了物理信息位置编码（PIPE），它包括两个关键创新：(1) 一种物理信息位置索引方案，用于将物理特性映射到位置ID；(2) 一个变频位置编码机制，在嵌入空间内对物理变量的频率信息及令牌顺序进行编码。

**结果:** 通过代表性最强且规模最大的开源卫星图像数据集上的实验，PIPE 在深度学习预测和气候领域方法上都达到了最先进的性能，并且在台风强度预测方面比先前的工作提升了12%。

**结论:** 通过保持物理信息和顺序信息，PIPE 显著提高了多模态对齐和预测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PIPE%3A+Physics-Informed+Position+Encoding+for+Alignment+of+Satellite+Images+and+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14786&send_immediately=true&force_search=false)

**原文摘要:** Multimodal time series forecasting is foundational in various fields, such as
utilizing satellite imagery and numerical data for predicting typhoons in
climate science. However, existing multimodal approaches primarily focus on
utilizing text data to help time series forecasting, leaving the visual data in
existing time series datasets untouched. Furthermore, it is challenging for
models to effectively capture the physical information embedded in visual data,
such as satellite imagery's temporal and geospatial context, which extends
beyond images themselves. To address this gap, we propose physics-informed
positional encoding (PIPE), a lightweight method that embeds physical
information into vision language models (VLMs). PIPE introduces two key
innovations: (1) a physics-informed positional indexing scheme for mapping
physics to positional IDs, and (2) a variant-frequency positional encoding
mechanism for encoding frequency information of physical variables and
sequential order of tokens within the embedding space. By preserving both the
physical information and sequential order information, PIPE significantly
improves multimodal alignment and forecasting accuracy. Through the experiments
on the most representative and the largest open-sourced satellite image
dataset, PIPE achieves state-of-the-art performance in both deep learning
forecasting and climate domain methods, demonstrating superiority across
benchmarks, including a 12% improvement in typhoon intensity forecasting over
prior works. Our code is provided in the supplementary material.

</details>


### [7] [Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems](https://arxiv.org/abs/2506.14787)
*Funing Li, Yuan Tian, Ruben Noortwyck, Jifeng Zhou, Liming Kuang, Robert Schulz*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于深度强化学习的框架，用于解决多深存储系统中异构项目配置下的检索问题。通过图神经网络和Transformer模型相结合的新神经网络架构来处理系统的拓扑结构，并通过广泛的数值实验展示了该方法在优化检索延迟方面的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决多深自主车辆存储与检索系统（AVS/RS）中的检索操作由于通道堵塞而遇到的重大挑战，同时提高存储密度。传统的解决方案限制了多深存储系统的灵活性和适应性。

**方法:** 引入了一种基于图的状态表示，结合物品属性和多深仓库的局部拓扑结构。设计了一个新的神经网络架构，它将图神经网络（GNN）与变压器模型相结合。GNN 将拓扑和特定于项目的相关信息编码为所有直接可访问项目的嵌入，而变压器则将这些嵌入映射到全局优先级分配。

**结果:** 通过大量的数值实验验证了所提出的神经网络架构的有效性，并且与启发式方法相比，训练后的代理在优化检索延迟方面表现出了优越性。

**结论:** 基于深度强化学习的方法可以有效地减少多深存储系统中异构物品配置下的总延迟，同时保持对不同布局存储系统的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topology-Aware+and+Highly+Generalizable+Deep+Reinforcement+Learning+for+Efficient+Retrieval+in+Multi-Deep+Storage+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14787&send_immediately=true&force_search=false)

**原文摘要:** In modern industrial and logistics environments, the rapid expansion of fast
delivery services has heightened the demand for storage systems that combine
high efficiency with increased density. Multi-deep autonomous vehicle storage
and retrieval systems (AVS/RS) present a viable solution for achieving greater
storage density. However, these systems encounter significant challenges during
retrieval operations due to lane blockages. A conventional approach to mitigate
this issue involves storing items with homogeneous characteristics in a single
lane, but this strategy restricts the flexibility and adaptability of
multi-deep storage systems.
  In this study, we propose a deep reinforcement learning-based framework to
address the retrieval problem in multi-deep storage systems with heterogeneous
item configurations. Each item is associated with a specific due date, and the
objective is to minimize total tardiness. To effectively capture the system's
topology, we introduce a graph-based state representation that integrates both
item attributes and the local topological structure of the multi-deep
warehouse. To process this representation, we design a novel neural network
architecture that combines a Graph Neural Network (GNN) with a Transformer
model. The GNN encodes topological and item-specific information into
embeddings for all directly accessible items, while the Transformer maps these
embeddings into global priority assignments. The Transformer's strong
generalization capability further allows our approach to be applied to storage
systems with diverse layouts. Extensive numerical experiments, including
comparisons with heuristic methods, demonstrate the superiority of the proposed
neural network architecture and the effectiveness of the trained agent in
optimizing retrieval tardiness.

</details>


### [8] [AZT1D: A Real-World Dataset for Type 1 Diabetes](https://arxiv.org/abs/2506.14789)
*Saman Khamesian, Asiful Arefeen, Bithika M. Thompson, Maria Adela Grando, Hassan Ghasemzadeh*

**主要类别:** cs.LG

**AI概要:** 介绍了AZT1D数据集，该数据集包含来自25名使用自动胰岛素输送系统的一型糖尿病患者的数据，提供了连续血糖监测、胰岛素泵和给药数据、碳水化合物摄入量及设备模式等详细信息。


<details>
  <summary>更多</summary>
  
**动机:** 由于公开可用的提供详细且全面的患者数据集稀缺，限制了在T1D管理中数据驱动方法的进步，包括个性化治疗设计、数字孪生系统和血糖预测模型的发展。

**方法:** 收集了25个使用自动胰岛素输送系统的T1D个体的数据，这些数据包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入以及每个患者的6到8周内不同设备模式（常规、睡眠和运动）。

**结果:** 创建了一个名为AZT1D的数据集，它提供了详细的胰岛素推注给药特征，如总剂量、推注类型和校正特定量，这些特征在现有数据集中很少见。

**结论:** AZT1D数据集通过提供丰富的自然主义数据支持广泛的人工智能和机器学习应用，旨在改善T1D中的临床决策和个人化护理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AZT1D%3A+A+Real-World+Dataset+for+Type+1+Diabetes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14789&send_immediately=true&force_search=false)

**原文摘要:** High quality real world datasets are essential for advancing data driven
approaches in type 1 diabetes (T1D) management, including personalized therapy
design, digital twin systems, and glucose prediction models. However, progress
in this area has been limited by the scarcity of publicly available datasets
that offer detailed and comprehensive patient data. To address this gap, we
present AZT1D, a dataset containing data collected from 25 individuals with T1D
on automated insulin delivery (AID) systems. AZT1D includes continuous glucose
monitoring (CGM) data, insulin pump and insulin administration data,
carbohydrate intake, and device mode (regular, sleep, and exercise) obtained
over 6 to 8 weeks for each patient. Notably, the dataset provides granular
details on bolus insulin delivery (i.e., total dose, bolus type, correction
specific amounts) features that are rarely found in existing datasets. By
offering rich, naturalistic data, AZT1D supports a wide range of artificial
intelligence and machine learning applications aimed at improving clinical
decision making and individualized care in T1D.

</details>


### [9] [Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting](https://arxiv.org/abs/2506.14790)
*Tianxiang Zhan, Ming Jin, Yuanpeng He, Yuxuan Liang, Yong Deng, Shirui Pan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为连续进化池(CEP)的方法，用于处理时间序列中常见的重复概念漂移问题。通过存储不同概念的预测器实例，并在概念重现时充分利用先前学习的知识，从而提高在线预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决方案虽然采用参数更新技术来延缓遗忘，但可能会导致之前学到的部分知识丢失，同时忽略了对知识保留机制的探索。为了解决这一问题，研究者提出了新的方法来保持所有概念知识并在概念重现时充分利用这些知识。

**方法:** 研究者设计了连续进化池(CEP)，一种能够储存针对不同概念的不同预测器实例的汇集机制。当遇到测试样本时，该方法会选择最近的预测器并通过其邻近样本来学习特征（检索过程）。如果邻近样本不足，则表明出现了新概念，此时将从当前最接近的样本演化出一个新模型加入到池中以保存新概念的知识。同时，过时的知识会被清除机制定期清理掉，确保预测器的效果。

**结果:** 实验结果表明，在不同的架构模型和八个真实数据集上，CEP能够有效地保留不同类型的概念知识。特别是在具有重复出现概念的在线预测场景中，CEP显著提高了预测效果。

**结论:** CEP作为一种新颖的方法，成功解决了重复概念漂移条件下如何有效保留并利用已有知识的问题，为在线预测领域提供了一个强有力的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Evolution+Pool%3A+Taming+Recurring+Concept+Drift+in+Online+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14790&send_immediately=true&force_search=false)

**原文摘要:** Recurring concept drift, a type of concept drift in which previously observed
data patterns reappear after some time, is one of the most prevalent types of
concept drift in time series. As time progresses, concept drift occurs and
previously encountered concepts are forgotten, thereby leading to a decline in
the accuracy of online predictions. Existing solutions employ parameter
updating techniques to delay forgetting; however, this may result in the loss
of some previously learned knowledge while neglecting the exploration of
knowledge retention mechanisms. To retain all conceptual knowledge and fully
utilize it when the concepts recur, we propose the Continuous Evolution Pool
(CEP), a pooling mechanism that stores different instances of forecasters for
different concepts. Our method first selects the forecaster nearest to the test
sample and then learns the features from its neighboring samples - a process we
refer to as the retrieval. If there are insufficient neighboring samples, it
indicates that a new concept has emerged, and a new model will evolve from the
current nearest sample to the pool to store the knowledge of the concept.
Simultaneously, the elimination mechanism will enable outdated knowledge to be
cleared to ensure the prediction effect of the forecasters. Experiments on
different architectural models and eight real datasets demonstrate that CEP
effectively retains the knowledge of different concepts. In the scenario of
online forecasting with recurring concepts, CEP significantly enhances the
prediction results.

</details>


### [10] [Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout](https://arxiv.org/abs/2506.14793)
*Aditya Ravuri, Neil D. Lawrence*

**主要类别:** cs.LG

**AI概要:** 研究发现，在蛋白质语言模型的特征化/嵌入层和其转换器之间于推理时注入一个dropout层，并以类似于蒙特卡洛dropout的方式平均输出，可以提高对ProteinGym数据集子集的零样本预测性能。即使模型最初没有使用dropout进行训练，也不需要重新训练或微调PLM。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高蛋白质语言模型（如ESM2）对于蛋白质关键标量属性（适应性）的零样本预测能力，研究人员探索了一种新的方法，即在推理过程中引入dropout层来增强模型的表现。

**方法:** 在蛋白质语言模型的特征化/嵌入层和其转换器之间添加一个dropout层，并且在推理时激活该层，然后通过对多次前向传播的结果取平均值来模拟蒙特卡洛dropout的效果。

**结果:** 实验结果表明，这种方法能够在不重新训练或微调原始模型的情况下，提升对ProteinGym数据集中特定子集的零样本预测性能；并且发现0.1的dropout率对于所有测试模型都表现良好。

**结论:** 通过在推理时加入dropout层并采取类似蒙特卡洛的方法平均输出，能够有效提高蛋白质语言模型的零样本学习能力，这一方法为改善生物信息学领域内相关模型的实用性提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Protein+Language+Model+Zero-Shot+Fitness+Predictions+are+Improved+by+Inference-only+Dropout，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14793&send_immediately=true&force_search=false)

**原文摘要:** Protein Language Models (PLMs) such as ESM2 have been shown to be capable of
zero-shot prediction of critical scalar properties of proteins (fitness). In
this work, we show that injecting a dropout layer at inference time between a
PLM's featurizer/embedding layer and its transformer, and averaging its output
akin to Monte-Carlo dropout increases zero-shot performance on a subset of the
ProteinGym dataset. This is the case even when the model was not trained with
dropouts to begin with, and does not require retraining or finetuning of the
PLM. A dropout of 0.1 seems performant across all models.

</details>


### [11] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales, Arturo Jaramillo, Heli Ricalde Guerrero*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基于粒子的变分推理方法——分支斯坦变分梯度下降（BSVGD），通过引入随机分支机制来探索状态空间，适用于多模态分布。提供了理论保证和数值实验验证了算法的有效性，并通过Wasserstein距离和计算时间与SVGD进行了性能比较。


<details>
  <summary>更多</summary>
  
**动机:** 为了处理多模态分布的问题，需要一种能有效探索状态空间并准确估计复杂分布的新方法。

**方法:** 发展了Branched Stein Variatorial Gradient Descent (BSVGD) 方法，该方法在经典SVGD算法基础上增加了随机分支机制，以促进对状态空间更广泛的探索。

**结果:** 研究表明BSVGD方法具有理论上的收敛性保证，并且通过数值实验证明了其适用性。相对于SVGD，在样本间的Wasserstein距离以及所需计算时间上，BSVGD展示了更好的性能。

**结论:** BSVGD为多模态分布下的变分推断提供了一个有效的解决方案，相比传统方法如SVGD，它在探索状态空间和效率方面表现更优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Branching+Stein+Variational+Gradient+Descent+for+sampling+multimodal+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13916&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel particle-based variational inference method designed to
work with multimodal distributions. Our approach, referred to as Branched Stein
Variational Gradient Descent (BSVGD), extends the classical Stein Variational
Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism
that encourages the exploration of the state space. In this work, a theoretical
guarantee for the convergence in distribution is presented, as well as
numerical experiments to validate the suitability of our algorithm. Performance
comparisons between the BSVGD and the SVGD are presented using the Wasserstein
distance between samples and the corresponding computational times.

</details>


### [12] [Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors](https://arxiv.org/abs/2506.14794)
*Henrik Klagges, Robert Dahlke, Fabian Klemm, Benjamin Merkel, Daniel Klingmann, David A. Reiss, Dan Zecha*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的"Assembly-of-Experts"(AoE)构造方法，该方法能够以线性时间创建现有Mixture-of-Experts父模型的能力子变体。通过插值模型权重张量来增强或抑制父模型的语义特征。使用此方法构建了DeepSeek R1T "Chimera"混合模型，该模型结合了DeepSeek的V3-0324和R1模型版本的特点，在没有微调或蒸馏的情况下展示了紧凑且有序的推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地利用对预训练模型的巨大投资，并提高计算效率，作者开发了新的“Assembly-of-Experts”（AoE）构造方法。

**方法:** 采用“Assembly-of-Experts”（AoE）的方法，个别地插值模型权重张量，以增强或抑制父模型的语义特性。在不同比例的从父模型中提取的权重下观察AoE子模型特性的变化。

**结果:** 几乎每个生成的模型都是功能性和有能力的，这简化了模型空间的搜索。构建了DeepSeek R1T 'Chimera' 671B开放权重混合模型，它仅继承了R1的路由专家张量，但达到了接近R1级别的智能水平，同时使用的输出令牌减少了约40%，速度接近V3。

**结论:** AoE方法能够有效生成功能完备的子模型，这些子模型在某些方面甚至优于其父模型，而且无需进行额外的微调或知识蒸馏过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assembly+of+Experts%3A+Linear-time+construction+of+the+Chimera+LLM+variants+with+emergent+and+adaptable+behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14794&send_immediately=true&force_search=false)

**原文摘要:** Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM
during pretraining is extremely expensive and seems inefficient. To better
leverage the huge investments made into pretrained models, we develop the new
"Assembly-of-Experts" (AoE) construction method to create capable child
variants of existing Mixture-of-Experts parent models in linear time. Model
weight tensors get interpolated individually, allowing to enhance or suppress
semantic features of the parents.
  Varying the proportion of weights taken from the parent models, we observe
some properties of the AoE child model changing gradually, while other
behavioral traits emerge with a sharp transition. Surprisingly, nearly every
generated model is functional and capable, which makes searching the model
space straightforward.
  We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model
combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the
routed expert tensors of R1, but still achieves about R1-level intelligence. At
the same time, it uses about 40\% fewer output tokens, close to V3 speed.
Constructed without any fine-tuning or distillation, the Chimera exhibits
surprisingly compact, orderly reasoning compared to its parent models.

</details>


### [13] [Bound by semanticity: universal laws governing the generalization-identification tradeoff](https://arxiv.org/abs/2506.14797)
*Marco Nurisso, Jesseba Fernando, Raj Deshpande, Alan Perotti, Raja Marjieh, Steven M. Frankland, Richard L. Lewis, Taylor W. Webb, Declan Campbell, Francesco Vaccarino, Jonathan D. Cohen, Giovanni Petri*

**主要类别:** cs.LG

**AI概要:** 本文探讨了智能系统在表示输入时需要平衡广义化能力和保持输入身份之间的关系，并揭示了一个基本限制。通过数学推导，文章给出了与输入空间几何无关的普适帕累托前沿来固定模型正确泛化的概率和识别的概率。进一步分析表明多输入处理能力随着输入数量增加而急剧下降，并且存在一个非单调最优值。实验验证了这些规律不仅适用于简单的ReLU网络，也适用于更复杂的卷积神经网络和最新的视觉-语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于理解智能系统如何在结构化（支持广义化）和选择性（保持输入身份）之间取得平衡，以及这种权衡存在的根本限制。

**方法:** 采用理论分析方法，对具有有限语义分辨率ε的模型进行了表示相似度衰减的研究，并推导出封闭形式的表达式。接着扩展到有噪声、异构的空间及超过两个输入的情况，并使用简单的ReLU网络、卷积神经网络以及先进的视觉-语言模型进行实证验证。

**结果:** 得出了泛化概率p_S和识别概率p_I被限定在一个与输入空间几何无关的普适帕累托前沿上；发现当输入数量n增加时，多输入处理能力会出现剧烈的1/n下降趋势，并且对于p_S存在一个非单调最优值；通过不同复杂程度的模型验证了这些理论预测的有效性。

**结论:** 研究表明，有限分辨率相似性是信息表示的基本约束，它影响着深层网络乃至大脑的表征能力。此外，论文提供了关于泛化-识别权衡的精确理论，并阐明了语义分辨率是如何塑造深度网络和大脑的表征容量的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bound+by+semanticity%3A+universal+laws+governing+the+generalization-identification+tradeoff，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14797&send_immediately=true&force_search=false)

**原文摘要:** Intelligent systems must deploy internal representations that are
simultaneously structured -- to support broad generalization -- and selective
-- to preserve input identity. We expose a fundamental limit on this tradeoff.
For any model whose representational similarity between inputs decays with
finite semantic resolution $\varepsilon$, we derive closed-form expressions
that pin its probability of correct generalization $p_S$ and identification
$p_I$ to a universal Pareto front independent of input space geometry.
Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs
predicts a sharp $1/n$ collapse of multi-input processing capacity and a
non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end
reproduces these laws: during learning a resolution boundary self-organizes and
empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for
linearly decaying similarity. Finally, we demonstrate that the same limits
persist in two markedly more complex settings -- a convolutional neural network
and state-of-the-art vision-language models -- confirming that
finite-resolution similarity is a fundamental emergent informational
constraint, not merely a toy-model artifact. Together, these results provide an
exact theory of the generalization-identification trade-off and clarify how
semantic resolution shapes the representational capacity of deep networks and
brains alike.

</details>


### [14] [Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size](https://arxiv.org/abs/2506.15025)
*Soufiane Hayou, Liyuan Liu*

**主要类别:** cs.LG

**AI概要:** 本文研究了词汇量大小对大规模语言模型训练动态的影响，发现随着词汇量的增加，训练动态介于μP机制和大型词汇（LV）机制之间。在LV机制中，最佳嵌入学习率与隐藏层学习率的比例应大致按Θ(√宽度)缩放，这与先前文献中的实证结果非常接近，而不同于μP预测的Θ(宽度)比例。通过实验验证理论，并从头开始预训练了一个10亿参数的模型来展示建议的学习率缩放规则的好处。


<details>
  <summary>更多</summary>
  
**动机:** 由于预训练大规模语言模型的成本高昂，人们提出了多种方法来优化模型架构/参数化和硬件使用。其中，μP（最大更新参数化）允许超参数在不同宽度的模型间转移，但其理论在处理实际较大的词汇量时存在局限性。本文旨在分析当词汇量增大时，训练动态的变化以及提供更优的嵌入学习率调整规则。

**方法:** 本文提供了关于词汇量大小对训练动态影响的理论分析，并指出随着词汇量的增加，训练动态会介于μP机制和一个称为大型词汇（LV）机制的新机制之间。此外，文章还进行了多组实验来验证该理论，并且从零开始预训练了一个10亿参数的模型以展示推荐的嵌入学习率缩放规则的实际好处。

**结果:** 研究表明，在LV机制下，最佳的嵌入学习率与隐藏层学习率之比应该大约按照Θ(√宽度)进行缩放，这一发现与之前文献报道的经验观察结果相近，而不同于由μP所预测的Θ(宽度)比率。实验结果支持了这个理论，并表明采用新的学习率缩放规则能够提高预训练效率。

**结论:** 论文得出结论，对于具有大词汇量的语言模型来说，采用基于Θ(√宽度)的嵌入学习率与隐藏层学习率之比的缩放规则，可以改善模型的训练过程。此结论得到了实验的支持，并且为未来的大规模语言模型预训练提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Embedding+Learning+Rate+in+LLMs%3A+The+Effect+of+Vocabulary+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15025&send_immediately=true&force_search=false)

**原文摘要:** Pretraining large language models is a costly process. To make this process
more efficient, several methods have been proposed to optimize model
architecture/parametrization and hardware use. On the parametrization side,
$\mu P$ (Maximal Update Parametrization) parametrizes model weights and
learning rate (LR) in a way that makes hyperparameters (HPs) transferable with
width (embedding dimension): HPs can be tuned for a small model and used for
larger models without additional tuning. While $\mu$P showed impressive results
in practice, recent empirical studies have reported conflicting observations
when applied to LLMs. One limitation of the theory behind $\mu$P is the fact
that input dimension (vocabulary size in LLMs) is considered fixed when taking
the width to infinity. This is unrealistic since vocabulary size is generally
much larger than width in practice. In this work, we provide a theoretical
analysis of the effect of vocabulary size on training dynamics, and
subsequently show that as vocabulary size increases, the training dynamics
\emph{interpolate between the $\mu$P regime and another regime that we call
Large Vocab (LV) Regime}, where optimal scaling rules are different from those
predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal
embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$,
surprisingly close to the empirical findings previously reported in the
literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P.
We conduct several experiments to validate our theory, and pretrain a 1B model
from scratch to show the benefit of our suggested scaling rule for the
embedding LR.

</details>


### [15] [ss-Mamba: Semantic-Spline Selective State-Space Model](https://arxiv.org/abs/2506.14802)
*Zuochen Ye*

**主要类别:** cs.LG

**AI概要:** 提出了ss-Mamba，一种新的基础模型，通过在选择性状态空间建模框架中整合语义感知嵌入和自适应样条基时间编码来增强时间序列预测。它基于Mamba选择性状态空间模型，相比Transformer架构，具有线性时间复杂度的优势。实验表明ss-Mamba在准确性、鲁棒性和可解释性方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决现有时间序列预测方法的局限性，尤其是计算复杂度过高和对未见数据泛化能力不足的问题，同时保持或提高预测性能。

**方法:** 开发了ss-Mamba模型，该模型结合了语义索引嵌入（从预训练语言模型初始化）和基于样条的Kolmogorov-Arnold网络（KAN），以动态且可解释地捕捉复杂的季节性和非平稳时间效应，并采用Mamba选择性状态空间模型作为高效替代方案，将时间复杂度从二次降低到线性。

**结果:** 广泛的实验证明，ss-Mamba提供了卓越的准确性、鲁棒性和可解释性，显示出其作为传统基于Transformer模型的时间序列预测中的多功能且计算效率高的替代品的能力。

**结论:** ss-Mamba作为一个新颖的基础模型，在时间序列预测任务上表现出色，不仅减少了计算成本，还提高了对新数据的泛化能力，以及增强了模型的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ss-Mamba%3A+Semantic-Spline+Selective+State-Space+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14802&send_immediately=true&force_search=false)

**原文摘要:** We propose ss-Mamba, a novel foundation model that enhances time series
forecasting by integrating semantic-aware embeddings and adaptive spline-based
temporal encoding within a selective state-space modeling framework. Building
upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba
selective state space model as an efficient alternative that achieves
comparable performance while significantly reducing computational complexity
from quadratic to linear time. Semantic index embeddings, initialized from
pretrained language models, allow effective generalization to previously unseen
series through meaningful semantic priors. Additionally, spline-based
Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex
seasonalities and non-stationary temporal effects, providing a powerful
enhancement over conventional temporal feature encodings. Extensive
experimental evaluations confirm that ss-Mamba delivers superior accuracy,
robustness, and interpretability, demonstrating its capability as a versatile
and computationally efficient alternative to traditional Transformer-based
models in time-series forecasting.

</details>


### [16] [Muon Optimizes Under Spectral Norm Constraints](https://arxiv.org/abs/2506.15054)
*Lizhang Chen, Jonathan Li, Qiang Liu*

**主要类别:** cs.LG

**AI概要:** 本文通过将Muon优化器归类到Lion-$\mathcal{K}$优化器家族中，提供了Muon的理论分析，并揭示了其隐式正则化效应。


<details>
  <summary>更多</summary>
  
**动机:** 虽然Muon优化器在深度学习中表现出色，但它的理论基础尚未被充分理解。研究旨在填补这一空白，为Muon提供坚实的理论支撑。

**方法:** 通过将Muon与Lion-$\mathcal{K}$优化器相关联，特别是当使用核范数时，证明了Muon等同于Lion-$\mathcal{K}$。利用Lion-$\mathcal{K}$的理论成果来建立Muon（带有解耦权重衰减）隐式解决了一个对权重矩阵谱范数施加约束的优化问题。

**结果:** 研究表明Muon优化器实际上是在解决一个隐含约束条件下的优化问题，这有助于解释它为什么能够有良好的性能。此外，通过改变凸映射$\mathcal{K}$的选择，可以自然地推广Muon，探索更广泛的隐式正则化和约束优化算法。

**结论:** 本研究不仅阐明了Muon优化器的隐式正则化效果，还为进一步发展新的优化算法奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Muon+Optimizes+Under+Spectral+Norm+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15054&send_immediately=true&force_search=false)

**原文摘要:** The pursuit of faster optimization algorithms remains an active and important
research direction in deep learning. Recently, the Muon optimizer [JJB+24] has
demonstrated promising empirical performance, but its theoretical foundation
remains less understood. In this paper, we bridge this gap and provide a
theoretical analysis of Muon by placing it within the Lion-$\mathcal{K}$ family
of optimizers [CLLL24]. Specifically, we show that Muon corresponds to
Lion-$\mathcal{K}$ when equipped with the nuclear norm, and we leverage the
theoretical results of Lion-$\mathcal{K}$ to establish that Muon (with
decoupled weight decay) implicitly solves an optimization problem that enforces
a constraint on the spectral norm of weight matrices. This perspective not only
demystifies the implicit regularization effects of Muon but also leads to
natural generalizations through varying the choice of convex map $\mathcal{K}$,
allowing for the exploration of a broader class of implicitly regularized and
constrained optimization algorithms.

</details>


### [17] [Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis](https://arxiv.org/abs/2506.14806)
*Bochen Lyu, Xiaojing Zhang, Fangyi Zheng, He Wang, Zheng Wang, Zhanxing Zhu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个连续时间近似模型，即分段连续微分方程，用于离散的Heavy-Ball (HB) 动量方法，并明确考虑了离散化误差。该研究为动量方法提供了额外的理论工具，可以控制任意阶步长的离散化误差，并被应用于发现方向平滑性的新隐式正则化以及对角线性网络的隐式偏置，从而有助于深度学习领域。


<details>
  <summary>更多</summary>
  
**动机:** 尽管动量在基于梯度的优化方法中起着关键作用，但原始离散动力学与由于离散化误差造成的连续时间近似之间的差距尚未得到全面解决。为了弥补这一差距并提供更多的理论工具，作者们专注于离散化误差，以期更好地理解HB动量方法。

**方法:** 设计了一阶分段连续微分方程，其中添加了多个反向项来显式地考虑离散化误差，从而为HB动量方法提供了一个允许控制任意阶步长离散化误差的连续时间模型。

**结果:** 通过所提出的连续时间模型，研究人员能够发现方向平滑性的一种新的隐式正则化，并探讨了HB对于对角线性网络的隐式偏置，这些结果可能对深度学习的应用有帮助。

**结论:** 本研究工作为HB动量方法提供了一个改进的连续时间近似，这有助于更精确地分析和理解动量方法的行为。此外，它还展示了如何将这种改进的方法应用到深度学习中去，例如探索隐式的正则化效应和动量方法的偏置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heavy-Ball+Momentum+Method+in+Continuous+Time+and+Discretization+Error+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14806&send_immediately=true&force_search=false)

**原文摘要:** This paper establishes a continuous time approximation, a piece-wise
continuous differential equation, for the discrete Heavy-Ball (HB) momentum
method with explicit discretization error. Investigating continuous
differential equations has been a promising approach for studying the discrete
optimization methods. Despite the crucial role of momentum in gradient-based
optimization methods, the gap between the original discrete dynamics and the
continuous time approximations due to the discretization error has not been
comprehensively bridged yet. In this work, we study the HB momentum method in
continuous time while putting more focus on the discretization error to provide
additional theoretical tools to this area. In particular, we design a
first-order piece-wise continuous differential equation, where we add a number
of counter terms to account for the discretization error explicitly. As a
result, we provide a continuous time model for the HB momentum method that
allows the control of discretization error to arbitrary order of the step size.
As an application, we leverage it to find a new implicit regularization of the
directional smoothness and investigate the implicit bias of HB for diagonal
linear networks, indicating how our results can be used in deep learning. Our
theoretical findings are further supported by numerical experiments.

</details>


### [18] [Neural Canonical Polyadic Factorization for Traffic Analysis](https://arxiv.org/abs/2506.15079)
*Yikai Hou, Peng Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种神经典型多路分解(NCPF)模型，该模型结合了低秩张量代数和深度表示学习，以实现鲁棒的交通数据填补。通过在六个城市交通数据集上的广泛评估，NCPF相对于六种最先进基线方法表现出优越性，并为下一代交通数字孪生和自适应交通控制系统提供了关键支持。


<details>
  <summary>更多</summary>
  
**动机:** 现代智能交通系统依赖于准确的时空交通分析来优化城市流动性和基础设施韧性，然而由于传感器故障和异构感知差距导致的数据缺失严重阻碍了可靠的交通建模。

**方法:** 提出了一种神经典型多路分解（NCPF）模型，该模型将CP分解嵌入到神经网络结构中，通过可学习的嵌入投影将稀疏交通张量编码成密集潜在因子，跨越道路段、时间间隔和移动度量。层次特征融合机制利用哈达玛积显式地建模多线性交互作用，而堆叠的多层感知器层非线性地细化这些表征，以捕捉复杂的时空耦合关系。

**结果:** 在六个城市交通数据集上进行的广泛评估表明，NCPF优于六种最先进的基线方法。

**结论:** 通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，NCPF为高维交通数据填补提供了一种有原则且灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了重要支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Canonical+Polyadic+Factorization+for+Traffic+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15079&send_immediately=true&force_search=false)

**原文摘要:** Modern intelligent transportation systems rely on accurate spatiotemporal
traffic analysis to optimize urban mobility and infrastructure resilience.
However, pervasive missing data caused by sensor failures and heterogeneous
sensing gaps fundamentally hinders reliable traffic modeling. This paper
proposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes
low-rank tensor algebra with deep representation learning for robust traffic
data imputation. The model innovatively embeds CP decomposition into neural
architecture through learnable embedding projections, where sparse traffic
tensors are encoded into dense latent factors across road segments, time
intervals, and mobility metrics. A hierarchical feature fusion mechanism
employs Hadamard products to explicitly model multilinear interactions, while
stacked multilayer perceptron layers nonlinearly refine these representations
to capture complex spatiotemporal couplings. Extensive evaluations on six urban
traffic datasets demonstrate NCPF's superiority over six state-of-the-art
baselines. By unifying CP decomposition's interpretable factor analysis with
neural network's nonlinear expressive power, NCPF provides a principled yet
flexible approaches for high-dimensional traffic data imputation, offering
critical support for next-generation transportation digital twins and adaptive
traffic control systems.

</details>


### [19] [PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models](https://arxiv.org/abs/2506.14808)
*Jenny Schmalfuss, Nadine Chang, Vibashan VS, Maying Shen, Andres Bruhn, Jose M. Alvarez*

**主要类别:** cs.LG

**AI概要:** 本文研究了视觉语言模型(VLMs)对不同提示的敏感性，通过提出PARC框架来分析VLMs在面对多样化提示时的表现。结果显示VLMs在视觉领域中表现出与大型语言模型相似的提示敏感性，并且来自InternVL2系列的模型显示出较高的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 为了确定视觉语言模型（VLMs）是否继承了大型语言模型（LLMs）对于变化提示的不稳定性，需要研究VLMs对哪些提示变化最为敏感以及哪些VLMs对提示变化具有较强的适应能力。

**方法:** 引入了名为PARC（通过可靠性和校准进行提示分析）的VLM提示敏感性分析框架，该框架基于三个支柱：1）在语言和视觉领域中合理的提示变化；2）带有内置保证的新颖模型可靠性评分；3）允许跨数据集和提示范围进行提示变化分析的校准步骤。

**结果:** PARC评估表明，VLMs在视觉领域的提示敏感性反映了LLM的语言提示敏感性，并且最具破坏性的变化会改变预期答案。在所评估的22个模型中，InternVL2家族的模型表现出了出色的鲁棒性。此外，有迹象表明提示敏感性与训练数据有关。

**结论:** 视觉语言模型确实继承了大型语言模型对于变化提示的敏感性，但特定的VLMs如InternVL2系列展示了较高的抗干扰能力。这表明通过适当的训练数据和技术手段可以提高VLMs的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PARC%3A+A+Quantitative+Framework+Uncovering+the+Symmetries+within+Vision+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14808&send_immediately=true&force_search=false)

**原文摘要:** Vision language models (VLMs) respond to user-crafted text prompts and visual
inputs, and are applied to numerous real-world problems. VLMs integrate visual
modalities with large language models (LLMs), which are well known to be
prompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this
instability to varying prompts. We therefore investigate which prompt
variations VLMs are most sensitive to and which VLMs are most agnostic to
prompt variations. To this end, we introduce PARC (Prompt Analysis via
Reliability and Calibration), a VLM prompt sensitivity analysis framework built
on three pillars: (1) plausible prompt variations in both the language and
vision domain, (2) a novel model reliability score with built-in guarantees,
and (3) a calibration step that enables dataset- and prompt-spanning prompt
variation analysis. Regarding prompt variations, PARC's evaluation shows that
VLMs mirror LLM language prompt sensitivity in the vision domain, and most
destructive variations change the expected answer. Regarding models,
outstandingly robust VLMs among 22 evaluated models come from the InternVL2
family. We further find indications that prompt sensitivity is linked to
training data. The code will be at https://github.com/NVlabs/PARC.

</details>


### [20] [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)
*Alejandro Francisco Queiruga, Theo Gutman-Solo, Shuai Jiang*

**主要类别:** cs.LG

**AI概要:** 本文通过将数值分析的严谨性应用于机器学习，特别是对1D泊松微分方程使用不同ML技术时的准确性进行了量化，并证明了在有限数据离散化和受限训练数据子空间下的泛化边界和收敛率。研究发现，即使对于深度线性模型、浅层神经网络以及特定物理的DeepONets和神经算子等，泛化到真实的物理方程也并非总是保证。此外，还提出了一种新的机制可解释性视角来提取黑盒模型中的格林函数表示，并建议一种新的交叉验证技术以评估物理系统中的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管机器学习在科学问题上的应用看起来很有前景，但视觉可能具有欺骗性；对于科学应用而言，实际的定量准确性至关重要。

**方法:** 采用数值分析的方法严格地对机器学习进行研究，特别关注于当不同的机器学习技术应用于基础的一维泊松微分方程时其准确性的量化。通过对训练动态的分析并推导出白盒微分方程发现方法和黑盒线性模型的最佳参数，识别出数据的功能空间对于模型的泛化至关重要。

**结果:** 理论和实证都表明，在探索的所有情况下，不能保证对真实物理方程的泛化。不同类别的模型可能会表现出相反的泛化行为。基于理论分析，还可以从黑盒模型的权重中提取格林函数表示。这些结果为测量物理系统中的泛化提供了一种新的交叉验证技术。

**结论:** 本文的工作强调了在科学建模中考虑泛化能力的重要性，并且提出了一个新的交叉验证技术作为未来方法的评估基准，以帮助改善机器学习模型在解决物理问题时的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretability+and+Generalization+Bounds+for+Learning+Spatial+Physics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15199，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15199&send_immediately=true&force_search=false)

**原文摘要:** While there are many applications of ML to scientific problems that look
promising, visuals can be deceiving. For scientific applications, actual
quantitative accuracy is crucial. This work applies the rigor of numerical
analysis for differential equations to machine learning by specifically
quantifying the accuracy of applying different ML techniques to the elementary
1D Poisson differential equation. Beyond the quantity and discretization of
data, we identify that the function space of the data is critical to the
generalization of the model. We prove generalization bounds and convergence
rates under finite data discretizations and restricted training data subspaces
by analyzing the training dynamics and deriving optimal parameters for both a
white-box differential equation discovery method and a black-box linear model.
The analytically derived generalization bounds are replicated empirically.
Similar lack of generalization is empirically demonstrated for deep linear
models, shallow neural networks, and physics-specific DeepONets and Neural
Operators. We theoretically and empirically demonstrate that generalization to
the true physical equation is not guaranteed in each explored case.
Surprisingly, we find that different classes of models can exhibit opposing
generalization behaviors. Based on our theoretical analysis, we also
demonstrate a new mechanistic interpretability lens on scientific models
whereby Green's function representations can be extracted from the weights of
black-box models. Our results inform a new cross-validation technique for
measuring generalization in physical systems. We propose applying it to the
Poisson equation as an evaluation benchmark of future methods.

</details>


### [21] [Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies](https://arxiv.org/abs/2506.14810)
*Qiwen Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出并评估了一种Model-Router框架，它能够基于每种产品的独特需求模式动态选择最合适的预测模型（包括经典、机器学习和深度学习方法）。实验表明，与强大的单一模型基准相比，该框架使用InceptionTime作为路由器时，可将预测准确性提高至多11.8%（NWRMSLE），并且推理速度快了4.67倍。


<details>
  <summary>更多</summary>
  
**动机:** 供应链中稀疏且间歇性的需求预测提出了一个关键挑战，频繁的零需求期阻碍了传统模型的准确性并对库存管理产生影响。

**方法:** 作者们提出并评估了一个Model-Router框架，这个框架可以根据每个产品独特的需求数字模式动态地从经典方法、机器学习(ML)以及深度学习(DL)方法中选择最为合适的预测模型。通过对基于规则的、LightGBM以及InceptionTime路由器进行比较，研究者的方法学会了分配适当的预测策略，有效地区分平滑、块状或间歇性需求状态以优化预测结果。

**结果:** 在Favorita大型数据集上的实验表明，相较于强大的单一模型基准，使用深度学习(Inception Time)路由器的方法可以将预测精度提高最多11.8%（NWRMSLE），同时拥有4.67倍更快的推理时间。

**结论:** 这些在预测精度上的改进将显著减少缺货情况和浪费过多的库存，强调了智能适应性AI在优化现代供应链运作中的关键作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intelligent+Routing+for+Sparse+Demand+Forecasting%3A+A+Comparative+Evaluation+of+Selection+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14810&send_immediately=true&force_search=false)

**原文摘要:** Sparse and intermittent demand forecasting in supply chains presents a
critical challenge, as frequent zero-demand periods hinder traditional model
accuracy and impact inventory management. We propose and evaluate a
Model-Router framework that dynamically selects the most suitable forecasting
model-spanning classical, ML, and DL methods for each product based on its
unique demand pattern. By comparing rule-based, LightGBM, and InceptionTime
routers, our approach learns to assign appropriate forecasting strategies,
effectively differentiating between smooth, lumpy, or intermittent demand
regimes to optimize predictions. Experiments on the large-scale Favorita
dataset show our deep learning (Inception Time) router improves forecasting
accuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with
4.67x faster inference time. Ultimately, these gains in forecasting precision
will drive substantial reductions in both stockouts and wasteful excess
inventory, underscoring the critical role of intelligent, adaptive Al in
optimizing contemporary supply chain operations.

</details>


### [22] [Warping and Matching Subsequences Between Time Series](https://arxiv.org/abs/2506.15452)
*Simiao Lin, Wannes Meert, Pieter Robberechts, Hendrik Blockeel*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的技术，该技术简化了扭曲路径，以突出显示、量化并可视化时间序列比较中的关键转换（如位移、压缩和幅度差异），从而增强了子序列匹配的可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 在时间序列的对比中，虽然弹性距离度量提供了鲁棒性的定量比较，但缺乏定性的比较；传统的可视化只关注点对点的对齐，并不能传达子序列层面更广泛的结构关系，这使得难以理解一个时间系列相对于另一个是如何以及在哪里发生偏移、加速或减速的。

**方法:** 作者提出了一种新技术，它能够简化用于比较的时间序列之间的扭曲路径，以便更好地强调、量化和可视化重要的转变，例如位移、压缩及振幅变化。

**结果:** 通过提供时间序列之间子序列如何匹配的更清晰表示，所提方法提高了时间序列比较中的可解释性。

**结论:** 这项研究为时间序列分析引入了一种改进的方法，该方法不仅允许进行稳健的定量对比，而且通过可视化关键变换来增强定性理解，从而有助于用户更容易地识别出不同时间序列间的动态变化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Warping+and+Matching+Subsequences+Between+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15452&send_immediately=true&force_search=false)

**原文摘要:** Comparing time series is essential in various tasks such as clustering and
classification. While elastic distance measures that allow warping provide a
robust quantitative comparison, a qualitative comparison on top of them is
missing. Traditional visualizations focus on point-to-point alignment and do
not convey the broader structural relationships at the level of subsequences.
This limitation makes it difficult to understand how and where one time series
shifts, speeds up or slows down with respect to another. To address this, we
propose a novel technique that simplifies the warping path to highlight,
quantify and visualize key transformations (shift, compression, difference in
amplitude). By offering a clearer representation of how subsequences match
between time series, our method enhances interpretability in time series
comparison.

</details>


### [23] [Self-Composing Policies for Scalable Continual Reinforcement Learning](https://arxiv.org/abs/2506.14811)
*Mikel Malagón, Josu Ceberio, Jose A. Lozano*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种可增长且模块化的神经网络架构，可以自然地避免连续强化学习中的灾难性遗忘和干扰。实验表明该方法比其他方法实现了更大的知识迁移和性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决在连续强化学习中出现的灾难性遗忘以及干扰问题，并加速当前任务的学习过程。

**方法:** 提出了一种新的、可扩展的神经网络架构，这种架构由多个模块组成，每个模块能够选择性地结合先前策略及其内部策略，同时参数数量随着任务数量线性增长而不牺牲可塑性。

**结果:** 通过在基准连续控制和视觉问题上的实验，展示了所提方法相较于其他方法具有更好的知识迁移效果和更高的性能表现。

**结论:** 该研究提出的可增长及模块化神经网络结构有效地解决了连续学习环境下的灾难性遗忘问题，并促进了跨任务的知识转移。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Composing+Policies+for+Scalable+Continual+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14811&send_immediately=true&force_search=false)

**原文摘要:** This work introduces a growable and modular neural network architecture that
naturally avoids catastrophic forgetting and interference in continual
reinforcement learning. The structure of each module allows the selective
combination of previous policies along with its internal policy, accelerating
the learning process on the current task. Unlike previous growing neural
network approaches, we show that the number of parameters of the proposed
approach grows linearly with respect to the number of tasks, and does not
sacrifice plasticity to scale. Experiments conducted in benchmark continuous
control and visual problems reveal that the proposed approach achieves greater
knowledge transfer and performance than alternative methods.

</details>


### [24] [LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models](https://arxiv.org/abs/2506.15492)
*Mohammadreza Nemati, Zhipeng Huang, Kevin S. Xu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为LIT-LVM的方法，它通过使用低维空间中的潜在向量表示特征来估计线性预测器中交互项的系数。该方法在高维度设置下比传统的正则化方法如套索和弹性网络更有效地防止过拟合，并且在多种模拟数据和真实数据上表现出优越的预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们注意到，在统计学和机器学习中，加权线性组合特征是最简单但最常用的预测因子之一。为了建模特征之间的非线性关系，通常会添加所有特征对的乘积作为交互项。然而，准确估计这些交互项的系数是一项挑战，尤其是在样本数量相对较少而交互项数目较多的情况下。因此，本文提出了一个假设，即不同的交互项系数具有近似的低维结构，并基于此开发了一种新的方法。

**方法:** 研究团队提出了一种称为LIT-LVM的新方法，该方法将每个特征表示为低维空间中的一个潜在向量。这种方法被视为一种结构化的正则化途径，能够在高维场景下进一步减少过拟合现象，这超越了诸如套索和弹性网络等标准正则化技术的效果。

**结果:** 实验结果表明，LIT-LVM方法在各种模拟和实际数据集上都优于弹性网络和因子分解机，特别是在交互项的数量远多于样本数的情况下。此外，LIT-LVM还提供了有用的特征低维潜在表示，有助于可视化和分析特征间的关系。

**结论:** LIT-LVM提供了一个有效的框架来估计线性预测器中交互项的系数，并在高维数据环境下表现出了卓越的性能。它不仅提高了预测精度，还促进了对特征之间复杂关系的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LIT-LVM%3A+Structured+Regularization+for+Interaction+Terms+in+Linear+Predictors+using+Latent+Variable+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15492&send_immediately=true&force_search=false)

**原文摘要:** Some of the simplest, yet most frequently used predictors in statistics and
machine learning use weighted linear combinations of features. Such linear
predictors can model non-linear relationships between features by adding
interaction terms corresponding to the products of all pairs of features. We
consider the problem of accurately estimating coefficients for interaction
terms in linear predictors. We hypothesize that the coefficients for different
interaction terms have an approximate low-dimensional structure and represent
each feature by a latent vector in a low-dimensional space. This
low-dimensional representation can be viewed as a structured regularization
approach that further mitigates overfitting in high-dimensional settings beyond
standard regularizers such as the lasso and elastic net. We demonstrate that
our approach, called LIT-LVM, achieves superior prediction accuracy compared to
elastic net and factorization machines on a wide variety of simulated and real
data, particularly when the number of interaction terms is high compared to the
number of samples. LIT-LVM also provides low-dimensional latent representations
for features that are useful for visualizing and analyzing their relationships.

</details>


### [25] [Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks](https://arxiv.org/abs/2506.14813)
*Yuxuan Jiang, Ziming Zhou, Boyu Xu, Beijie Liu, Runhui Xu, Peng Huang*

**主要类别:** cs.LG

**AI概要:** 本文提出了TRAINCHECK框架，它能够自动推断出针对深度学习训练的不变量，并利用这些不变量主动检测训练过程中的静默错误，同时提供调试帮助。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习模型训练过程复杂，容易出现难以发现和诊断的静默错误。

**方法:** TRAINCHECK框架通过自动推断深度学习训练的不变量来主动检测静默错误并提供调试支持。

**结果:** 在重现20个具有不同根本原因的真实世界静默训练错误时，TRAINCHECK成功地在一个训练迭代中检测到18个错误，并且发现了流行训练库中导致静默错误的6个未知漏洞。

**结论:** TRAINCHECK被证明是有效检测深度学习训练过程中静默错误的工具，并且能够帮助开发者定位问题所在。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+with+Confidence%3A+Catching+Silent+Errors+in+Deep+Learning+Training+with+Automated+Proactive+Checks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14813&send_immediately=true&force_search=false)

**原文摘要:** Training deep learning (DL) models is a complex process, making it prone to
silent errors that are challenging to detect and diagnose. This paper presents
TRAINCHECK, a framework that takes a proactive checking approach to address
silent training errors. TRAINCHECK automatically infers invariants tailored for
DL training. It uses these invariants to proactively detect silent errors
during the training process while providing debugging help. To evaluate
TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root
causes. TRAINCHECK successfully detects 18 errors within a single training
iteration. It also uncovers 6 unknown bugs in popular training libraries that
lead to silent errors.

</details>


### [26] [A Simplified Analysis of SGD for Linear Regression with Weight Averaging](https://arxiv.org/abs/2506.15535)
*Alexandru Meterez, Depen Morwani, Costin-Andrei Oncescu, Jingfeng Wu, Cengiz Pehlevan, Sham Kakade*

**主要类别:** cs.LG

**AI概要:** 本文通过使用简单的线性代数工具简化了对带常学习率的随机梯度下降（SGD）在线性回归中优化的分析，重现了之前工作中的偏差和方差界，使得SGD在处理线性回归时的分析更加易于理解。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在简化先前关于带常学习率的随机梯度下降（SGD）在线性回归中优化过程的理论分析，使其更易被理解和应用。

**方法:** 采用简单的线性代数方法来分析SGD，绕过了对正半定(PSD)矩阵上操作符的处理要求。

**结果:** 成功地提供了与之前研究相同偏差和方差边界，但使用了更简化的手段。

**结论:** 该研究提供了一种更简单的方法来分析SGD在线性回归中的表现，这可能有助于进一步研究小批量处理和学习率调度策略，从而改进实际模型训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Simplified+Analysis+of+SGD+for+Linear+Regression+with+Weight+Averaging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15535&send_immediately=true&force_search=false)

**原文摘要:** Theoretically understanding stochastic gradient descent (SGD) in
overparameterized models has led to the development of several optimization
algorithms that are widely used in practice today. Recent work
by~\citet{zou2021benign} provides sharp rates for SGD optimization in linear
regression using constant learning rate, both with and without tail iterate
averaging, based on a bias-variance decomposition of the risk. In our work, we
provide a simplified analysis recovering the same bias and variance bounds
provided in~\citep{zou2021benign} based on simple linear algebra tools,
bypassing the requirement to manipulate operators on positive semi-definite
(PSD) matrices. We believe our work makes the analysis of SGD on linear
regression very accessible and will be helpful in further analyzing
mini-batching and learning rate scheduling, leading to improvements in the
training of realistic models.

</details>


### [27] [Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning](https://arxiv.org/abs/2506.14815)
*Gyaneshwar Agrahari, Kiran Bist, Monika Pandey, Jacob Kapita, Zachary James, Jackson Knox, Steven Heymsfield, Sophia Ramirez, Peter Wolenski, Nadejda Drenska*

**主要类别:** cs.LG

**AI概要:** 研究提出了一种基于3D光学图像生物标志物的半监督$p$-Laplacian回归模型，用于预测人体成分变量如ALM、BFP和BMD。该模型在数据受限的情况下表现出良好的性能，误差率分别为约13%（ALM）、10%（BMD）和20%（BFP）。


<details>
  <summary>更多</summary>
  
**动机:** 准确预测人体成分变量对于早期诊断多种慢性疾病至关重要。然而，目前使用的DXA扫描技术成本高昂且耗时。因此，需要一种更经济高效的方法来预测这些关键指标。

**方法:** 研究采用从3D光学图像中获得的生物标志物（如身高、体积、左小腿围等），并应用统计和机器学习模型来进行预测。为了克服医疗保健领域数据获取的技术挑战与法律限制，并解决监督学习算法对大量训练数据的需求，研究人员实现了一个半监督模型——$p$-Laplacian回归模型。

**结果:** $p$-Laplacian模型在仅使用10%的数据进行训练时，对于ALM、BMD和BFP的预测误差分别约为13%、10%和20%。此外，在监督算法中，支持向量回归(SVR) 对于ALM和BMD表现最佳，误差约为8%，而最小二乘SVR对于BFP的表现最好，误差约为11%，当使用80%的数据进行训练时。

**结论:** 研究表明，$p$-Laplacian模型作为一种新的方法，在数据有限的情况下能够有效预测人体成分变量，显示出其作为医疗健康领域工具的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Anthropometric+Body+Composition+Variables+Using+3D+Optical+Imaging+and+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14815，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14815&send_immediately=true&force_search=false)

**原文摘要:** Accurate prediction of anthropometric body composition variables, such as
Appendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral
Density (BMD), is essential for early diagnosis of several chronic diseases.
Currently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to
measure these metrics; however, DXA scans are costly and time-consuming. This
work proposes an alternative to DXA scans by applying statistical and machine
learning models on biomarkers (height, volume, left calf circumference, etc)
obtained from 3D optical images. The dataset consists of 847 patients and was
sourced from Pennington Biomedical Research Center. Extracting patients' data
in healthcare faces many technical challenges and legal restrictions. However,
most supervised machine learning algorithms are inherently data-intensive,
requiring a large amount of training data. To overcome these limitations, we
implemented a semi-supervised model, the $p$-Laplacian regression model. This
paper is the first to demonstrate the application of a $p$-Laplacian model for
regression. Our $p$-Laplacian model yielded errors of $\sim13\%$ for ALM,
$\sim10\%$ for BMD, and $\sim20\%$ for BFP when the training data accounted for
10 percent of all data. Among the supervised algorithms we implemented, Support
Vector Regression (SVR) performed the best for ALM and BMD, yielding errors of
$\sim 8\%$ for both, while Least Squares SVR performed the best for BFP with
$\sim 11\%$ error when trained on 80 percent of the data. Our findings position
the $p$-Laplacian model as a promising tool for healthcare applications,
particularly in a data-constrained environment.

</details>


### [28] [Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints](https://arxiv.org/abs/2506.14821)
*Sunil Kumar, Bowen Zhao, Leo Dirac, Paulina Varshavskaya*

**主要类别:** cs.LG

**AI概要:** 本文通过借鉴Deepseek-r1等方法，采用GRPO训练小型视觉-语言模型使用如缩放工具等外部工具，以提高在有限计算资源下的详细视觉推理能力。通过简化奖励结构、工具调用接口，并为工具调出结果分配额外标记，以及增加视觉上较难示例的训练数据比例，所提出的方法相比类似规模的基线模型，在某些VQA任务中表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型模型的推理能力取得了巨大进步，但视觉-语言模型（VLMs）在处理详细的视觉推理时仍然面临挑战，尤其是在计算资源受限的情况下。为了应对这一挑战，作者们受到了像Deepseek-r1这样的方法启发。

**方法:** 研究者采用了Group Relative Policy Optimization (GRPO)来训练较小规模的模型，使其能够利用外部工具，比如放大功能。该方法结合了GRPO学习、简化的奖励机制、简化的工具调用界面，并将额外的token分配给工具调用的结果，同时训练数据集包含较多视觉难度较高的例子。

**结果:** 与相同规模的基础模型相比，本研究的方法在一些视觉问答(VQA)任务中达到了更好的性能，这得益于从外部工具收集到的更详细的视觉信息。

**结论:** 研究表明，通过采用GRPO和一系列优化措施，即使是在计算资源有限的情况下，也能显著提升视觉-语言模型的详细视觉推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcing+VLMs+to+Use+Tools+for+Detailed+Visual+Reasoning+Under+Resource+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14821&send_immediately=true&force_search=false)

**原文摘要:** Despite tremendous recent advances in large model reasoning ability,
vision-language models (VLMs) still struggle with detailed visual reasoning,
especially when compute resources are limited. To address this challenge, we
draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale
models with Group Relative Policy Optimization (GRPO) to use external tools
such as zoom. The greatest benefit is obtained with a combination of GRPO
learning, a simple reward structure, a simplified tool-calling interface,
allocating additional tokens to the result of the tool call, and a training
data mix that over-represents visually difficult examples. Compared to
similarly-sized baseline models, our method achieves better performance on some
visual question-answering (VQA) tasks, thanks to the detailed visual
information gathered from the external tool.

</details>


### [29] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang, Hewei Gao, Haokun Chen, Weiguo Li, Yunpu Ma, Volker Tresp*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为FedNano的联邦学习框架，该框架通过在服务器上集中大型语言模型（LLM）并在客户端引入轻量级模块NanoEdge来解决多模态大语言模型在现实世界部署中的挑战。这种方法显著减少了客户端存储需求和通信开销，并且能够处理异构客户端数据和资源限制，同时保持隐私性。实验表明FedNano优于先前的联邦学习基线，为可扩展的、去中心化的多模态AI系统铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型（MLLMs）虽然在多模态推理和跨模态检索等任务中表现出色，但在实际场景中由于分布式多模态数据和严格的隐私要求而面临部署难题。联邦学习（FL）提供了一个解决方案，允许不集中数据的情况下进行协作模型训练。然而，对于MLLMs实现FL存在重大挑战，包括高计算需求、有限的客户端容量、巨大的通信成本以及客户端数据的异质性。现有的FL方法假设在客户端部署完整模型，这对于大规模MLLMs来说不可行，因为它们的规模庞大且通信需求高。

**方法:** 提出了FedNano，这是第一个将LLM集中在服务器上的FL框架，同时引入了NanoEdge——一个用于客户端特定适应的轻量级模块。NanoEdge采用了模态特定编码器、连接器和低秩适应的可训练NanoAdapters。这种设计消除了在客户端部署LLM的需求，将客户端存储减少了95%，并将通信开销限制到了模型参数的0.01%。通过仅传输紧凑的NanoAdapter更新，FedNano能够在保持隐私的同时处理异构客户端数据和资源约束。

**结果:** 实验结果表明，FedNano超越了先前的FL基准，在性能上弥补了MLLM规模与FL可行性之间的差距。它使得开发可扩展的、去中心化的多模态AI系统成为可能。

**结论:** FedNano作为一个创新的联邦学习框架，成功地解决了多模态大语言模型在实际应用中的部署难题，通过减少客户端负担和通信成本，实现了对异构数据的有效处理及隐私保护。这标志着向构建更加灵活、高效并且尊重用户隐私的AI系统的迈进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedNano%3A+Toward+Lightweight+Federated+Tuning+for+Pretrained+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14824&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) excel in tasks like multimodal
reasoning and cross-modal retrieval but face deployment challenges in
real-world scenarios due to distributed multimodal data and strict privacy
requirements. Federated Learning (FL) offers a solution by enabling
collaborative model training without centralizing data. However, realizing FL
for MLLMs presents significant challenges, including high computational
demands, limited client capacity, substantial communication costs, and
heterogeneous client data. Existing FL methods assume client-side deployment of
full models, an assumption that breaks down for large-scale MLLMs due to their
massive size and communication demands. To address these limitations, we
propose FedNano, the first FL framework that centralizes the LLM on the server
while introducing NanoEdge, a lightweight module for client-specific
adaptation. NanoEdge employs modality-specific encoders, connectors, and
trainable NanoAdapters with low-rank adaptation. This design eliminates the
need to deploy LLM on clients, reducing client-side storage by 95%, and
limiting communication overhead to only 0.01% of the model parameters. By
transmitting only compact NanoAdapter updates, FedNano handles heterogeneous
client data and resource constraints while preserving privacy. Experiments
demonstrate that FedNano outperforms prior FL baselines, bridging the gap
between MLLM scale and FL feasibility, and enabling scalable, decentralized
multimodal AI systems.

</details>


### [30] [Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes](https://arxiv.org/abs/2506.14828)
*Sk Md Ahnaf Akif Alvi, Mrinalini Mulukutla, Nicolas Flores, Danial Khatamsaz, Jan Janssen, Danny Perez, Douglas Allaire, Vahid Attari, Raymundo Arroyave*

**主要类别:** cs.LG

**AI概要:** 本文系统地评估了四种主要的代理模型（cGP、DGP、用于多输出回归的编码器-解码器神经网络和XGBoost）在高熵合金AlCoCrCuFeMnNiV系统中的拟合性能，特别是在处理异方差、异位和不完整数据方面。研究发现基于机器学习先验的DGP在捕捉属性间相关性和输入依赖性不确定性方面优于其他代理模型，从而提高了预测精度。


<details>
  <summary>更多</summary>
  
**动机:** 加速高熵合金的发现与优化，并且结合计算预测和稀疏实验观察数据，需要有效的代理建模技术来处理材料信息学中常见的复杂数据类型。

**方法:** 采用四种代理模型：传统高斯过程(cGP)、深度高斯过程(DGP)、用于多输出回归的编码器-解码器神经网络以及XGBoost，对包含实验和计算性质的混合数据集进行拟合性能评估。特别关注这些模型在预测相关材料特性上的能力。

**结果:** 结果表明，集成机器学习基础先验的DGP在捕捉属性间关联及输入依赖性不确定性方面表现突出，超越了其他几种代理模型。

**结论:** 高级代理模型，特别是基于机器学习先验的DGP，在提高预测准确性上表现出色，为实现鲁棒且高效的数据驱动材料设计提供了强有力的支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accurate+and+Uncertainty-Aware+Multi-Task+Prediction+of+HEA+Properties+Using+Prior-Guided+Deep+Gaussian+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14828&send_immediately=true&force_search=false)

**原文摘要:** Surrogate modeling techniques have become indispensable in accelerating the
discovery and optimization of high-entropy alloys(HEAs), especially when
integrating computational predictions with sparse experimental observations.
This study systematically evaluates the fitting performance of four prominent
surrogate models conventional Gaussian Processes(cGP), Deep Gaussian
Processes(DGP), encoder-decoder neural networks for multi-output regression and
XGBoost applied to a hybrid dataset of experimental and computational
properties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their
capabilities in predicting correlated material properties, including yield
strength, hardness, modulus, ultimate tensile strength, elongation, and average
hardness under dynamic and quasi-static conditions, alongside auxiliary
computational properties. The comparison highlights the strengths of
hierarchical and deep modeling approaches in handling heteroscedastic,
heterotopic, and incomplete data commonly encountered in materials informatics.
Our findings illustrate that DGP infused with machine learning-based prior
outperform other surrogates by effectively capturing inter-property
correlations and input-dependent uncertainty. This enhanced predictive accuracy
positions advanced surrogate models as powerful tools for robust and
data-efficient materials design.

</details>


### [31] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen, Ruoxin Zhang, Chao Wang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，以提高存储设备健康状态分类的准确性和稳定性。实验结果表明该模型具有很高的分类准确率和良好的泛化能力，并且能够为工业级存储系统的预防性维护提供实用价值。


<details>
  <summary>更多</summary>
  
**动机:** 固态硬盘（SSD）健康状态预测对于数据可靠性保障起着关键作用，而传统模型在泛化能力上存在瓶颈。

**方法:** 提出了一种新的混合BiGRU-MHA模型，该模型利用了BiGRU网络双向时序建模的优势来捕捉SSD退化特征的前后依赖关系，同时使用多头注意力机制动态分配特征权重，增强了对关键健康指标的敏感度。

**结果:** 所提出的模型在训练集上的分类准确率为92.70%，测试集上的分类准确率为92.44%，性能差距仅为0.26%，展示了优秀的泛化能力。此外，ROC曲线下的面积（AUC）为0.94，证实了模型稳健的二元分类性能。

**结论:** 这项工作不仅为SSD健康预测提供了新的技术方法，还解决了传统模型的泛化瓶颈问题，为工业级存储系统的预防性维护提供了一个可验证且具有实际价值的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimization+of+bi-directional+gated+loop+cell+based+on+multi-head+attention+mechanism+for+SSD+health+state+classification+model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14830，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14830&send_immediately=true&force_search=false)

**原文摘要:** Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [32] [CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration](https://arxiv.org/abs/2506.14843)
*Luca Gherardini, Imre Lengyel, Tunde Peto, Caroline C. W. Klaverd, Magda A. Meester-Smoord, Johanna Maria Colijnd, EYE-RISK Consortium, E3 Consortium, Jose Sousa*

**主要类别:** cs.LG

**AI概要:** 本文介绍了CACTUS，一种用于提高年龄相关性黄斑变性(AMD)分期分类的综合抽象和分类工具。该工具提供了可解释性和灵活性，优于标准机器学习模型，并通过识别关键因素来增强决策制定。


<details>
  <summary>更多</summary>
  
**动机:** 在医疗保健领域，特别是对于影响数百万老年人的年龄相关性黄斑变性（AMD），早期诊断至关重要，因为缺乏有效的治疗方法来逆转疾病进展。当前的机器学习模型往往受限于数据量不足或不完整的问题，且某些模型缺乏透明度，这使得它们难以被理解和使用。此外，不同的数据集还会影响解决方案的可信度。因此需要一个能够考虑遗传、饮食、临床和人口统计学因素的分类方法。

**方法:** 研究者们引入了CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures)，这是一个旨在改善AMD阶段分类的工具。CACTUS设计为提供更好的可解释性和适应性，它能帮助识别重要因素并对其结果提供置信度。

**结果:** CACTUS表现优于传统的机器学习模型，通过确定重要的特征并与现有的医学知识进行比较，从而提高了决策的质量。此外，通过排除不太相关或有偏见的数据，研究人员创建了一个让临床医生可以提供反馈和解决偏见的临床场景。

**结论:** 通过引入CACTUS，研究人员不仅提升了AMD阶段分类的准确性，还增强了模型的透明度和可信度，同时允许与现有医学知识进行有意义的对比。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CACTUS+as+a+Reliable+Tool+for+Early+Classification+of+Age-related+Macular+Degeneration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14843&send_immediately=true&force_search=false)

**原文摘要:** Machine Learning (ML) is used to tackle various tasks, such as disease
classification and prediction. The effectiveness of ML models relies heavily on
having large amounts of complete data. However, healthcare data is often
limited or incomplete, which can hinder model performance. Additionally, issues
like the trustworthiness of solutions vary with the datasets used. The lack of
transparency in some ML models further complicates their understanding and use.
In healthcare, particularly in the case of Age-related Macular Degeneration
(AMD), which affects millions of older adults, early diagnosis is crucial due
to the absence of effective treatments for reversing progression. Diagnosing
AMD involves assessing retinal images along with patients' symptom reports.
There is a need for classification approaches that consider genetic, dietary,
clinical, and demographic factors. Recently, we introduced the -Comprehensive
Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed
at improving AMD stage classification. CACTUS offers explainability and
flexibility, outperforming standard ML models. It enhances decision-making by
identifying key factors and providing confidence in its results. The important
features identified by CACTUS allow us to compare with existing medical
knowledge. By eliminating less relevant or biased data, we created a clinical
scenario for clinicians to offer feedback and address biases.

</details>


### [33] [Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification](https://arxiv.org/abs/2506.14895)
*Jenni Raitoharju*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用负样本改进的广义参考核（GRKneg）方法，用于单类支持向量机。实验表明，该方法在负样本数量较少时优于标准OC-SVM和二分类SVM。


<details>
  <summary>更多</summary>
  
**动机:** 研究者旨在解决小规模单类分类问题，并且在有少量负样本可用的情况下改善分类性能。

**方法:** 提出了广义参考核与负样本（GRKneg），并探讨了选择/生成参考向量的不同方式。此方法不使用任何标签进行模型优化，而是通过使用负数据来改进原始OC-SVM实现中使用的核。

**结果:** 提出的GRKneg方法始终优于使用径向基函数核的标准OC-SVM。当负样本充足时，二分类SVM表现最佳；但对于最少数量的负样本，所提方法明显优于二分类SVM。

**结论:** 新提出的GRKneg方法为小规模单类分类提供了一个有效的解决方案，特别是在负样本有限的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Reference+Kernel+With+Negative+Samples+For+Support+Vector+One-class+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14895&send_immediately=true&force_search=false)

**原文摘要:** This paper focuses on small-scale one-class classification with some negative
samples available. We propose Generalized Reference Kernel with Negative
Samples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study
different ways to select/generate the reference vectors and recommend an
approach for the problem at hand. It is worth noting that the proposed method
does not use any labels in the model optimization but uses the original OC-SVM
implementation. Only the kernel used in the process is improved using the
negative data. We compare our method with the standard OC-SVM and with the
binary Support Vector Machine (SVM) using different amounts of negative
samples. Our approach consistently outperforms the standard OC-SVM using Radial
Basis Function kernel. When there are plenty of negative samples, the binary
SVM outperforms the one-class approaches as expected, but we show that for the
lowest numbers of negative samples the proposed approach clearly outperforms
the binary SVM.

</details>


### [34] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang, Boyu Wang, Bin Gu, Charles Ling*

**主要类别:** cs.LG

**AI概要:** 本文首次识别了在线垂直联邦学习中的挑战，提出了一个事件驱动的在线VFL框架，并引入动态局部遗憾来解决非凸模型在非稳态环境下的在线学习问题。实验表明该框架比现有方法更稳定，同时大大减少了通信和计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 在线垂直联邦学习（VFL）相比离线学习更能适应现实世界场景。然而，由于VFL的独特性质，即客户端对于同一样本拥有不相交的特征集，将在线学习整合到VFL中存在挑战。在实际情况中，客户可能不会同步接收到同一实体的不同特征的数据流。数据通常由只与部分客户相关的'事件'生成。这些挑战此前未被研究充分认识。

**方法:** 为了解决上述挑战，作者们提出了一种事件驱动的在线VFL框架，在这个框架中每个事件期间仅有部分客户被激活，而其余客户则被动地参与学习过程。此外，为了应对非凸模型在非稳态环境下在线学习的问题，他们还将'动态局部遗憾(DLR)'的概念融入到了VFL当中。

**结果:** 通过详尽的遗憾分析，特别是对带有事件驱动在线VFL的DLR在非凸条件下的分析，以及广泛的实验验证，结果表明所提出的框架在非稳态数据条件下比现有的在线VFL框架更加稳定，同时也显著降低了通信和计算成本。

**结论:** 这项工作提供了一个新的视角来看待在线VFL，并且通过提出的解决方案，它不仅解决了现存的挑战，还提高了系统的稳定性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Event-Driven+Online+Vertical+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14911&send_immediately=true&force_search=false)

**原文摘要:** Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


### [35] [FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning](https://arxiv.org/abs/2506.14929)
*Ganyu Wang, Jinjie Fang, Maxwell J. Ying, Bin Gu, Xi Chen, Boyu Wang, Charles Ling*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为FedOne的框架，该框架是一种联邦黑盒离散提示学习方法，旨在与基于云的大语言模型交互时最大化查询效率。通过理论分析和数值实验表明，FedOne框架在查询效率方面有显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦黑盒提示调优研究忽视了与基于云的大型语言模型服务相关的大量查询成本。为了填补这一空白，作者对联邦黑盒提示调优中的查询效率进行了理论分析。

**方法:** 作者提出了一个称为FedOne的框架，该框架通过每轮仅激活一个客户端来实现最优查询效率，并且设计用于与基于云的大型语言模型交互时提高查询效率。

**结果:** 数值实验证明了FedOne框架在多个方面的性能，并显示了查询效率的显著提升，这与理论结果一致。

**结论:** FedOne框架为联邦黑盒离散提示学习提供了一个新的方法，它能够有效地减少与基于云的大型语言模型交互时的查询成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedOne%3A+Query-Efficient+Federated+Learning+for+Black-box+Discrete+Prompt+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14929，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14929&send_immediately=true&force_search=false)

**原文摘要:** Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes
discrete prompts without accessing model parameters or gradients, making the
prompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting
federated learning to BDPL could further enhance prompt tuning performance by
leveraging data from diverse sources. However, all previous research on
federated black-box prompt tuning had neglected the substantial query cost
associated with the cloud-based LLM service. To address this gap, we conducted
a theoretical analysis of query efficiency within the context of federated
black-box prompt tuning. Our findings revealed that degrading FedAvg to
activate only one client per round, a strategy we called \textit{FedOne},
enabled optimal query efficiency in federated black-box prompt learning.
Building on this insight, we proposed the FedOne framework, a federated
black-box discrete prompt learning method designed to maximize query efficiency
when interacting with cloud-based LLMs. We conducted numerical experiments on
various aspects of our framework, demonstrating a significant improvement in
query efficiency, which aligns with our theoretical results.

</details>


### [36] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda, Pedro Ivo da Cruz, Murilo Bellezoni Loiola*

**主要类别:** cs.LG

**AI概要:** 本文提出使用机器学习算法（K-近邻、K-均值和支持向量机）自动定义自编码器在异常检测系统中的阈值，以解决数据不平衡问题并改进检测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于自编码器的异常检测系统虽然能够处理数据不平衡的问题，但因为其使用的非标准分离阈值会影响检测过程的性能，因此需要一种方法来自动定义这个阈值。

**方法:** 研究者评估了三种不同的机器学习算法：K-近邻(K-Nearest Neighbors)、K-均值(K-Means) 和支持向量机(Support Vector Machine)，用来自动确定自编码器中用于分类重建误差的阈值。

**结果:** 通过采用这三种机器学习算法，研究者们能够自动地定义一个阈值，该阈值有助于提高自编码器在异常检测系统中的表现。

**结论:** 这项工作表明，利用机器学习算法自动设定自编码器的分离阈值是可行的，并且可能改善异常检测系统的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Determina%C3%A7%C3%A3o+Autom%C3%A1tica+de+Limiar+de+Detec%C3%A7%C3%A3o+de+Ataques+em+Redes+de+Computadores+Utilizando+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14937，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14937&send_immediately=true&force_search=false)

**原文摘要:** Currently, digital security mechanisms like Anomaly Detection Systems using
Autoencoders (AE) show great potential for bypassing problems intrinsic to the
data, such as data imbalance. Because AE use a non-trivial and nonstandardized
separation threshold to classify the extracted reconstruction error, the
definition of this threshold directly impacts the performance of the detection
process. Thus, this work proposes the automatic definition of this threshold
using some machine learning algorithms. For this, three algorithms were
evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [37] [Flat Channels to Infinity in Neural Loss Landscapes](https://arxiv.org/abs/2506.14951)
*Flavio Martinelli, Alexander Van Meegen, Berfin Şimşek, Wulfram Gerstner, Johanni Brea*

**主要类别:** cs.LG

**AI概要:** 论文研究了神经网络损失景观中的特殊结构，即当两个神经元的输出权重发散至正负无穷大而它们的输入权重向量变得相等时，沿着这些通道损失会极其缓慢地减少。最终这两个神经元实现了门控线性单元的功能。


<details>
  <summary>更多</summary>
  
**动机:** 作者们想要理解神经网络损失景观中一种特殊结构，在这种结构下两个神经元的权重行为异常，并且优化算法容易误认为是平坦局部极小值的情况。

**方法:** 通过理论分析和几何描述来表征损失景观中特定的通道结构，其中两个神经元的权重出现极端情况并最终实现门控线性单元功能。

**结果:** 发现沿着某些通道，尽管参数趋向于无穷大，但优化方法如SGD或ADAM却高概率地达到这些区域，而且这些通道与对称引起的临界点线渐近平行。

**结论:** 这项工作揭示了全连接层计算能力的一个意外方面，即在看似平坦的局部极小值区域实际上形成了门控线性单元，为理解梯度动态、几何以及功能解释提供了全面的观点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flat+Channels+to+Infinity+in+Neural+Loss+Landscapes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14951&send_immediately=true&force_search=false)

**原文摘要:** The loss landscapes of neural networks contain minima and saddle points that
may be connected in flat regions or appear in isolation. We identify and
characterize a special structure in the loss landscape: channels along which
the loss decreases extremely slowly, while the output weights of at least two
neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight
vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At
convergence, the two neurons implement a gated linear unit:
$a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot
\mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot
\mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these
channels to infinity are asymptotically parallel to symmetry-induced lines of
critical points. Gradient flow solvers, and related optimization methods like
SGD or ADAM, reach the channels with high probability in diverse regression
settings, but without careful inspection they look like flat local minima with
finite parameter values. Our characterization provides a comprehensive picture
of these quasi-flat regions in terms of gradient dynamics, geometry, and
functional interpretation. The emergence of gated linear units at the end of
the channels highlights a surprising aspect of the computational capabilities
of fully connected layers.

</details>


### [38] [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective](https://arxiv.org/abs/2506.14965)
*Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为Guru的强化学习推理语料库，该语料库包含了跨越六个推理领域的92K个可验证示例，并基于此研究了不同领域中RL训练的效果。此外，还提出了两个模型Guru-7B和Guru-32B，在公开数据上进行RL训练后表现超越现有基准。


<details>
  <summary>更多</summary>
  
**动机:** 当前大多数开放的努力集中在数学和代码上，限制了我们对强化学习在一般推理中的广泛适用性的理解。关键挑战在于缺乏可靠的、可扩展的跨多种推理领域的强化学习奖励信号。

**方法:** 构建了一个名为Guru的强化学习（RL）推理语料库，包含92K个经过策划的、可验证的例子，涵盖了六个推理领域：数学、代码、科学、逻辑、模拟和表格。每个领域都是通过特定领域的奖励设计、去重和过滤来构建的，以确保RL训练的可靠性和有效性。

**结果:** 研究发现，预训练时常见领域（如数学、代码、科学）容易从跨领域RL训练中受益，而预训练暴露较少的领域（如逻辑、模拟和表格）则需要在领域内训练才能取得有意义的性能提升。提出的Guru-7B和Guru-32B模型在17项任务评估套件上的表现超过了最佳基线，分别提高了7.9%和6.7%。

**结论:** 研究表明，对于预训练期间经常遇到的领域，跨领域的RL训练可以带来好处；而对于预训练过程中接触较少的领域，则需要专门的领域内训练以获得显著的性能改进。这表明RL可能有助于真正技能的获取。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Reinforcement+Learning+for+LLM+Reasoning+from+A+Cross-Domain+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14965&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has emerged as a promising approach to improve
large language model (LLM) reasoning, yet most open efforts focus narrowly on
math and code, limiting our understanding of its broader applicability to
general reasoning. A key challenge lies in the lack of reliable, scalable RL
reward signals across diverse reasoning domains. We introduce Guru, a curated
RL reasoning corpus of 92K verifiable examples spanning six reasoning
domains--Math, Code, Science, Logic, Simulation, and Tabular--each built
through domain-specific reward design, deduplication, and filtering to ensure
reliability and effectiveness for RL training. Based on Guru, we systematically
revisit established findings in RL for LLM reasoning and observe significant
variation across domains. For example, while prior work suggests that RL
primarily elicits existing knowledge from pretrained models, our results reveal
a more nuanced pattern: domains frequently seen during pretraining (Math, Code,
Science) easily benefit from cross-domain RL training, while domains with
limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain
training to achieve meaningful performance gains, suggesting that RL is likely
to facilitate genuine skill acquisition. Finally, we present Guru-7B and
Guru-32B, two models that achieve state-of-the-art performance among open
models RL-trained with publicly available data, outperforming best baselines by
7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We
also show that our models effectively improve the Pass@k performance of their
base models, particularly on complex tasks less likely to appear in pretraining
data. We release data, models, training and evaluation code to facilitate
general-purpose reasoning at: https://github.com/LLM360/Reasoning360

</details>


### [39] [Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits](https://arxiv.org/abs/2506.14988)
*Tianyi Xu, Jiaxin Liu, Zizhan Zheng*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种多智能体多臂老虎机框架，旨在确保各智能体间结果公平的同时最大化整个系统的性能。为了解决在有限信息下进行决策的挑战，引入了一种新的探测机制来收集选定摇臂的信息。离线场景中，设计了一个具有可证明性能界限的贪婪探测算法；在线场景中，则开发了一套既能保持公平又能实现次线性遗憾的算法。实验表明该方法比基线方法表现更好，在公平性和效率方面均有提升。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决多智能体环境下如何在保证公平性的前提下优化系统整体性能的问题。特别地，针对的是在关于摇臂奖励信息有限的情况下做出有效决策这一关键挑战。

**方法:** 对于已知奖励分布的离线环境，利用子模性质设计了带理论保障的贪婪探测算法；针对更复杂的在线环境，则提出了一个能够在维持公平性的同时达到次线性后悔界的算法。

**结果:** 通过合成数据集和真实世界数据集上的广泛实验验证了所提方法的有效性，相比基准方法，在公平性和效率上均取得了更好的表现。

**结论:** 本研究成功地将一种新颖的探测框架应用于多智能体多臂老虎机问题中，不仅提高了系统的总体性能还保证了跨智能体之间的公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+Algorithms+with+Probing+for+Multi-Agent+Multi-Armed+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14988&send_immediately=true&force_search=false)

**原文摘要:** We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at
ensuring fair outcomes across agents while maximizing overall system
performance. A key challenge in this setting is decision-making under limited
information about arm rewards. To address this, we introduce a novel probing
framework that strategically gathers information about selected arms before
allocation. In the offline setting, where reward distributions are known, we
leverage submodular properties to design a greedy probing algorithm with a
provable performance bound. For the more complex online setting, we develop an
algorithm that achieves sublinear regret while maintaining fairness. Extensive
experiments on synthetic and real-world datasets show that our approach
outperforms baseline methods, achieving better fairness and efficiency.

</details>


### [40] [ODD: Overlap-aware Estimation of Model Performance under Distribution Shift](https://arxiv.org/abs/2506.14978)
*Aayush Mishra, Anqi Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，即重叠感知分歧差异(ODD)，用于在未见测试域中更准确地估计机器学习模型的误差。相较于之前的分歧差异(DIS^2)方法，ODD通过仅在非重叠的目标域内最大化分歧来消除竞争，从而更好地预测目标性能，并且在一系列基准测试中证明了其改进的整体性能估计误差的有效性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 先前的工作使用分歧差异（DIS^2）来推导在分布偏移下的实用误差边界。尽管这种方法提供了一个可靠且相对准确的目标误差估计，但研究者发现该方法存在一个问题，即在源域和目标域之间的重叠区域中，分歧差异目标会相互竞争。基于这样的背景，作者提出了一个直观假设，即由于足够高的支持，在重叠区域中目标分歧不应超过源分歧。

**方法:** 为了解决这个问题，作者设计了重叠感知分歧差异(ODD)。ODD的最大化只需要在非重叠目标域内的分歧，从而消除了在重叠区域的竞争。ODD基于边界利用领域分类器来估计领域重叠，并且比DIS^2更好预测目标表现。

**结果:** 通过在广泛的基准测试上进行实验，作者展示了他们的方法能够改善整体性能估计误差，同时保持有效性和可靠性。

**结论:** 新提出的ODD方法在不同数据集上表现出色，能更准确地估计模型在未见测试域中的误差，同时保证了估计的稳定性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ODD%3A+Overlap-aware+Estimation+of+Model+Performance+under+Distribution+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14978&send_immediately=true&force_search=false)

**原文摘要:** Reliable and accurate estimation of the error of an ML model in unseen test
domains is an important problem for safe intelligent systems. Prior work uses
disagreement discrepancy (DIS^2) to derive practical error bounds under
distribution shifts. It optimizes for a maximally disagreeing classifier on the
target domain to bound the error of a given source classifier. Although this
approach offers a reliable and competitively accurate estimate of the target
error, we identify a problem in this approach which causes the disagreement
discrepancy objective to compete in the overlapping region between source and
target domains. With an intuitive assumption that the target disagreement
should be no more than the source disagreement in the overlapping region due to
high enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).
Maximizing ODD only requires disagreement in the non-overlapping target domain,
removing the competition. Our ODD-based bound uses domain-classifiers to
estimate domain-overlap and better predicts target performance than DIS^2. We
conduct experiments on a wide array of benchmarks to show that our method
improves the overall performance-estimation error while remaining valid and
reliable. Our code and results are available on GitHub.

</details>


### [41] [Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment](https://arxiv.org/abs/2506.15019)
*Yue Gao*

**主要类别:** cs.LG

**AI概要:** 本文研究了在脓毒症治疗中使用受控微分方程（CDE）状态表示法进行强化学习，通过确保训练稳定性及采用与临床评分相关联的锐度感知表示法，实现了强效的RL策略。实验结果表明，稳定的CDE自编码器能够产生与病情严重程度评分高度相关的表示，并支持更优性能的RL策略。


<details>
  <summary>更多</summary>
  
**动机:** 有效的脓毒症治疗强化学习依赖于从不规则ICU时间序列中学习稳定且具有临床意义的状态表示。虽然先前的工作探索了这种任务的表示学习，但尚未充分解决顺序表示中的训练不稳定性和其对策略表现的负面影响问题。

**方法:** 本研究采用了受控微分方程(CDE)作为状态表示方法，同时保证了训练稳定性（通过早期停止或稳定化方法）以及通过与临床评分（SOFA, SAPS-II, OASIS）的相关性正则化来强制实现锐度感知表示。

**结果:** 实验显示，当满足两个关键因素时，即训练稳定性和锐度感知表示，CDE状态表示可以实现强大的RL策略。稳定的CDE自编码器生成的表示与急性评分有很强的相关性，并允许RL策略表现出优越的性能（WIS回报>0.9）。而不稳定的CDE表示导致表示质量下降和策略失败（WIS回报~0）。

**结论:** 这些发现强调了在临床强化学习中使用CDE编码不规则医疗时间序列的实际指导原则，并突出了序列表示学习中训练稳定性的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+CDE+Autoencoders+with+Acuity+Regularization+for+Offline+Reinforcement+Learning+in+Sepsis+Treatment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15019，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15019&send_immediately=true&force_search=false)

**原文摘要:** Effective reinforcement learning (RL) for sepsis treatment depends on
learning stable, clinically meaningful state representations from irregular ICU
time series. While previous works have explored representation learning for
this task, the critical challenge of training instability in sequential
representations and its detrimental impact on policy performance has been
overlooked. This work demonstrates that Controlled Differential Equations (CDE)
state representation can achieve strong RL policies when two key factors are
met: (1) ensuring training stability through early stopping or stabilization
methods, and (2) enforcing acuity-aware representations by correlation
regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the
MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces
representations strongly correlated with acuity scores and enables RL policies
with superior performance (WIS return $> 0.9$). In contrast, unstable CDE
representation leads to degraded representations and policy failure (WIS return
$\sim$ 0). Visualizations of the latent space show that stable CDEs not only
separate survivor and non-survivor trajectories but also reveal clear acuity
score gradients, whereas unstable training fails to capture either pattern.
These findings highlight practical guidelines for using CDEs to encode
irregular medical time series in clinical RL, emphasizing the need for training
stability in sequential representation learning.

</details>


### [42] [Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks](https://arxiv.org/abs/2506.14986)
*Maxime Usdin, Lito Kriara, Licinio Craveiro*

**主要类别:** cs.LG

**AI概要:** 研究使用先进的基础模型和机器学习方法，结合临床数据和数字生物标记物来预测多发性硬化症患者的残疾进展。结果显示，整合数字数据的模型比仅使用临床数据的模型表现更好，尤其是当采用多模态转换器时。


<details>
  <summary>更多</summary>
  
**动机:** 早期预测多发性硬化症(MS)患者的残疾进展具有挑战性，因为疾病异质性导致难以准确预测。研究旨在通过结合稀疏基线临床数据与12周的每日数字化Floodlight数据来提高早期预测准确性。

**方法:** 采用了最新的表格和时间序列基础模型（FMs）、定制的多模态注意力机制变压器以及机器学习方法。特别地，一个基于Moment FM的单模态嵌入的变压器模型取得了最佳结果，但研究中的多模态变压器在综合性能上始终优于单模态同类产品。

**结果:** 尽管早期预测难度大(AUROC 0.63)，但通过高级模型整合数字数据后，相比单独使用临床数据，性能有所提升。

**结论:** 研究发现表明，利用基础模型(FMs)和多模态方法从复杂且多样化的临床及数字生命科学数据中提取预测信号是可行的，这为MS及其他复杂疾病的更准确预后提供了希望。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early+Prediction+of+Multiple+Sclerosis+Disability+Progression+via+Multimodal+Foundation+Model+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14986&send_immediately=true&force_search=false)

**原文摘要:** Early multiple sclerosis (MS) disability progression prediction is
challenging due to disease heterogeneity. This work predicts 48- and 72-week
disability using sparse baseline clinical data and 12 weeks of daily digital
Floodlight data from the CONSONANCE clinical trial. We employed
state-of-the-art tabular and time-series foundation models (FMs), a custom
multimodal attention-based transformer, and machine learning methods. Despite
the difficulty of early prediction (AUROC 0.63), integrating digital data via
advanced models improved performance over clinical data alone. A transformer
model using unimodal embeddings from the Moment FM yielded the best result, but
our multimodal transformer consistently outperformed its unimodal counterpart,
confirming the advantages of combining clinical with digital data. Our findings
demonstrate the promise of FMs and multimodal approaches to extract predictive
signals from complex and diverse clinical and digital life sciences data (e.g.,
imaging, omics), enabling more accurate prognostics for MS and potentially
other complex diseases.

</details>


### [43] [SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models](https://arxiv.org/abs/2506.15021)
*Gyuhak Kim, Sumiran Singh Thakur, Su Min Park, Wei Wei, Yujia Bao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的监督微调方法SFT-GO，该方法通过基于重要性对token进行分组并优化最差组损失和标准交叉熵损失的加权组合来改善大型语言模型的学习动态。实验结果表明SFT-GO在不同数据集和基准测试中均优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的监督微调(SFT)方法通常将每个训练实例视为均匀序列，对所有tokens给予同等的重要性，忽视了只有部分tokens可能包含关键任务信息的事实。为了克服这一局限性，研究者提出了SFT-GO方法，旨在更好地处理不同组分布，并提高整体学习效果。

**方法:** SFT-GO根据样本中各token的重要性值对其进行分组，并利用最差组损失与标准交叉熵损失的加权组合来优化大语言模型。此机制自适应地强调最具挑战性的token组，并引导模型更好地应对不同的组分布。

**结果:** 理论分析显示SFT-GO具有较高的收敛效率；实证研究表明，采用三种不同的token分组策略时，使用SFT-GO训练的模型在多个流行的LLM基准上一致优于基线方法。这些改进在各种数据集和基础模型上都得到了验证。

**结论:** SFT-GO是一种有效的监督微调方法，能够通过自适应地调整不同重要性级别的token组权重来提升大型语言模型的表现。其在不同场景下的鲁棒性和有效性得到了证实。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SFT-GO%3A+Supervised+Fine-Tuning+with+Group+Optimization+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15021&send_immediately=true&force_search=false)

**原文摘要:** Supervised fine-tuning (SFT) has become an essential step in tailoring large
language models (LLMs) to align with human expectations and specific downstream
tasks. However, existing SFT methods typically treat each training instance as
a uniform sequence, giving equal importance to all tokens regardless of their
relevance. This overlooks the fact that only a subset of tokens often contains
critical, task-specific information. To address this limitation, we introduce
Supervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that
treats groups of tokens differently based on their importance.SFT-GO groups
tokens in each sample based on their importance values and optimizes the LLM
using a weighted combination of the worst-group loss and the standard
cross-entropy loss. This mechanism adaptively emphasizes the most challenging
token groups and guides the model to better handle different group
distributions, thereby improving overall learning dynamics. We provide a
theoretical analysis of SFT-GO's convergence rate, demonstrating its
efficiency. Empirically, we apply SFT-GO with three different token grouping
strategies and show that models trained with SFT-GO consistently outperform
baseline approaches across popular LLM benchmarks. These improvements hold
across various datasets and base models, demonstrating the robustness and the
effectiveness of our method.

</details>


### [44] [Sequential Policy Gradient for Adaptive Hyperparameter Optimization](https://arxiv.org/abs/2506.15051)
*Zheng Li, Jerry Cheng, Huanying Helen Gu*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sequential+Policy+Gradient+for+Adaptive+Hyperparameter+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15051，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15051&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning is essential for neural architecture search and
hyperparameter optimization, but the conventional approaches impede widespread
use due to prohibitive time and computational costs. Inspired by DeepSeek-V3
multi-token prediction architecture, we propose Sequential Policy Gradient
modeling (SPG), a novel trajectory generation paradigm for lightweight online
hyperparameter optimization. In contrast to conventional policy gradient
methods, SPG extends the base model with temporary modules, enabling it to
generate state-action (padded) trajectories in a single forward pass. Our
experiments demonstrate that models gain performance when retrained with SPG on
their original datasets and also outperform standard transfer fine-tuning. We
evaluate on five datasets spanning computer vision (ImageNet, COCO), natural
language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial
applicability of SPG. The proposed method demonstrates consistent improvements
across widely adopted models, achieving performance gains of $+0.2\sim7\%$,
with significantly low computational costs. Fully reproducible code and
pre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.

</details>


### [45] [Singular Value Decomposition on Kronecker Adaptation for Large Language Model](https://arxiv.org/abs/2506.15251)
*Yee Hin Chong, Peng Qu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的参数高效微调策略SoKA，结合了Kronecker-乘积张量分解与SVD驱动初始化和频谱感知动态秩选择。实验结果表明，SoKA在算术推理、形式数学和代码生成任务上仅需0.99M可训练参数，比LoRA/PiSSA少25%，同时匹配或超越基线性能，并且收敛更快，梯度更稳定。


<details>
  <summary>更多</summary>
  
**动机:** 大型预训练Transformer模型虽然在多种语言和推理任务中取得了最先进的成果，但完全微调会导致大量的存储、内存和计算开销。现有的参数高效微调(PEFT)方法存在推断时延（适配器模块）、次优收敛性（随机初始化低秩更新）或者依赖固定秩选择可能不适应任务复杂度等问题。

**方法:** 提出了SoKA (SVD on Kronecker Adaptation)，一种新颖的PEFT策略，它结合了Kronecker-乘积张量分解与SVD驱动初始化以及基于频谱意识的动态秩选择。通过Kronecker-Product SVD(KPSVD)过程将完整的权重更新的主要成分提取到紧凑的Kronecker因子中，而自适应秩选择算法利用能量阈值和拐点标准来剔除可以忽略的成分。

**结果:** 对LLaMA2-7B在算术推理(GSM8K)、形式数学(MATH)和代码生成(MBPP)上的实证评估显示，SoKA只需要0.99M个可训练参数，比LoRA/PiSSA少25%，同时匹配或超过基线性能。此外，SoKA表现出更快的收敛性和更稳定的梯度。

**结论:** SoKA提供了一种更加高效和鲁棒的方法来进行大规模模型的适应，不仅减少了所需参数数量，还提高了收敛速度和稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Singular+Value+Decomposition+on+Kronecker+Adaptation+for+Large+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15251&send_immediately=true&force_search=false)

**原文摘要:** Large pre-trained Transformer models achieve state-of-the-art results across
diverse language and reasoning tasks, but full fine-tuning incurs substantial
storage, memory, and computational overhead. Parameter-efficient fine-tuning
(PEFT) methods mitigate these costs by learning only a small subset of
task-specific parameters, yet existing approaches either introduce
inference-time latency (adapter modules), suffer from suboptimal convergence
(randomly initialized low-rank updates), or rely on fixed rank choices that may
not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that
combines Kronecker-product tensor factorization with SVD-driven initialization
and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)
procedure extracts principal components of the full weight update into compact
Kronecker factors, while an adaptive rank selection algorithm uses
energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal
mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires
only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or
exceeding baseline performance. Moreover, SoKA exhibits faster convergence and
more stable gradients, highlighting its robustness and efficiency for
large-scale model adaptation.

</details>


### [46] [Unlocking Post-hoc Dataset Inference with Synthetic Data](https://arxiv.org/abs/2506.15271)
*Bihe Zhao, Pratyush Maini, Franziska Boenisch, Adam Dziedzic*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种通过合成生成所需保留数据集的方法来解决数据集推理(DI)在实践中难以应用的问题。该方法能够创建高质量、多样化的合成数据，并通过后处理校准缩小真实与合成数据之间的可能性差距，使得DI能够高置信度地检测原始训练集且保持较低的误报率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据集推理（DI）方法需要一个已知未参与训练的私有数据集，其分布要与被泄露的数据集紧密匹配。然而，在实际中很难获得这种同分布但未参与训练的数据，这严重限制了DI的应用。为了解决这个问题，研究者们提出了一种新的方法，旨在通过合成方式生成所需的保留数据集。

**方法:** 研究人员开发了一种方法来生成高质量和多样化的人工数据，这些数据能够准确反映原始数据分布。他们利用一种经过精心设计的基于后缀完成任务训练的数据生成器来实现这一点。此外，还采用了后处理校准技术来弥合真实数据和人工生成数据之间存在的似然性差异。

**结果:** 广泛的实验结果表明，使用所生成的数据作为保留数据集，可以使DI以高可信度识别出原训练数据集，同时保持较低的假阳性率。这一成果使版权持有者能够对数据使用情况提出合法要求，并展示了该方法在现实世界诉讼中的可靠性。

**结论:** 这项工作提供了一种新的解决方案，即通过合成生成所需的数据集，来克服现有数据集推理方法的局限性。这种方法不仅提高了DI的实际适用性，也为版权拥有者提供了强有力的工具来监测和管理他们的数据资产。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlocking+Post-hoc+Dataset+Inference+with+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15271&send_immediately=true&force_search=false)

**原文摘要:** The remarkable capabilities of Large Language Models (LLMs) can be mainly
attributed to their massive training datasets, which are often scraped from the
internet without respecting data owners' intellectual property rights. Dataset
Inference (DI) offers a potential remedy by identifying whether a suspect
dataset was used in training, thereby enabling data owners to verify
unauthorized use. However, existing DI methods require a private set-known to
be absent from training-that closely matches the compromised dataset's
distribution. Such in-distribution, held-out data is rarely available in
practice, severely limiting the applicability of DI. In this work, we address
this challenge by synthetically generating the required held-out set. Our
approach tackles two key obstacles: (1) creating high-quality, diverse
synthetic data that accurately reflects the original distribution, which we
achieve via a data generator trained on a carefully designed suffix-based
completion task, and (2) bridging likelihood gaps between real and synthetic
data, which is realized through post-hoc calibration. Extensive experiments on
diverse text datasets show that using our generated data as a held-out set
enables DI to detect the original training sets with high confidence, while
maintaining a low false positive rate. This result empowers copyright owners to
make legitimate claims on data usage and demonstrates our method's reliability
for real-world litigations. Our code is available at
https://github.com/sprintml/PostHocDatasetInference.

</details>


### [47] [Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation](https://arxiv.org/abs/2506.15309)
*Júlia Vilalta-Mor, Alexis Molina, Laura Ortega Varga, Isaac Filella-Merce, Victor Guallar*

**主要类别:** cs.LG

**AI概要:** 提出了一种结构化的主动学习方法，该方法结合了序列到序列变分自编码器，并在迭代循环中设计以平衡化学多样性、分子质量和多目标亲和力。通过针对三种相关冠状病毒主要蛋白酶的原理验证研究，该方法高效地生成了一组结构多样化的广谱抑制剂候选物。


<details>
  <summary>更多</summary>
  
**动机:** 同时优化针对多个治疗靶点的分子仍然是药物发现中的一个重大挑战，特别是在奖励稀疏和设计约束冲突的情况下。

**方法:** 使用一种集成序列到序列变分自编码器（Seq2Seq VAE）的结构化主动学习（AL）范式，在迭代循环中旨在平衡化学多样性、分子质量和对多个靶点的亲和力。

**结果:** 这种方法能够有效地生成一组针对SARS-CoV-2、SARS-CoV和MERS-CoV主蛋白酶的结构多样化广谱抑制剂候选物。

**结论:** 通过在主动学习流程中精心安排化学过滤器的时间和位置，可以显著增强有益化学空间的探索，将稀疏奖励的多目标药物设计问题转变为可计算的任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning-Guided+Seq2Seq+Variational+Autoencoder+for+Multi-target+Inhibitor+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15309&send_immediately=true&force_search=false)

**原文摘要:** Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to sparse
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the sparse-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.

</details>


### [48] [When and How Unlabeled Data Provably Improve In-Context Learning](https://arxiv.org/abs/2506.15329)
*Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak*

**主要类别:** cs.LG

**AI概要:** 该研究探讨了在上下文学习中，即使示例具有缺失或错误标签时，多层或循环变换器也能有效利用未标记数据，并通过实验表明所提方法可以显著提高半监督表格学习的性能。


<details>
  <summary>更多</summary>
  
**动机:** 近期研究表明，即使是在有缺失或不正确标签的情况下，在上下文学习（ICL）仍然可以是有效的。为了阐明这一能力，本文研究了一个典型场景：演示是从二元高斯混合模型（GMM）中抽取的，并且一定比例的演示缺少标签。

**方法:** 研究人员提供了一个全面的理论研究，证明单层线性注意力模型无法利用未标记的数据，而多层或循环变换器能够通过构建特定形式的估计量来有效地利用未标记数据。此外，他们还描述了深度和多项式之间的关系，并与常用的半监督学习算法——期望最大化法（EM算法）建立了联系。

**结果:** 理论分析指出，多层或循环变换器能够隐式地构造出一种特定形式的估计量，这使得它们能很好地利用未标记的数据。实际应用上，通过对现成的表格基础模型进行循环处理，可以增强其半监督学习能力。

**结论:** 大量的真实数据集评估显示，该方法相较于标准的一次性推理，在半监督表格学习的表现上有显著提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+and+How+Unlabeled+Data+Provably+Improve+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15329&send_immediately=true&force_search=false)

**原文摘要:** Recent research shows that in-context learning (ICL) can be effective even
when demonstrations have missing or incorrect labels. To shed light on this
capability, we examine a canonical setting where the demonstrations are drawn
according to a binary Gaussian mixture model (GMM) and a certain fraction of
the demonstrations have missing labels. We provide a comprehensive theoretical
study to show that: (1) The loss landscape of one-layer linear attention models
recover the optimal fully-supervised estimator but completely fail to exploit
unlabeled data; (2) In contrast, multilayer or looped transformers can
effectively leverage unlabeled data by implicitly constructing estimators of
the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting
features and partially-observed labels (with missing entries set to zero). We
characterize the class of polynomials that can be expressed as a function of
depth and draw connections to Expectation Maximization, an iterative
pseudo-labeling algorithm commonly used in semi-supervised learning.
Importantly, the leading polynomial power is exponential in depth, so mild
amount of depth/looping suffices. As an application of theory, we propose
looping off-the-shelf tabular foundation models to enhance their
semi-supervision capabilities. Extensive evaluations on real-world datasets
show that our method significantly improves the semisupervised tabular learning
performance over the standard single pass inference.

</details>


### [49] [HiPreNets: High-Precision Neural Networks through Progressive Training](https://arxiv.org/abs/2506.15064)
*Ethan Mulle, Wei Kang, Qi Gong*

**主要类别:** cs.LG

**AI概要:** 提出了一种渐进式框架，用于训练和调优高精度神经网络（HiPreNets），通过顺序学习预测残差来提高完全连接的神经网络的准确性，并利用残差结构指导损失函数的选择、参数数量以及自适应数据采样技术。


<details>
  <summary>更多</summary>
  
**动机:** 随着问题复杂性的增加，训练高度准确的深度神经网络模型变得具有挑战性，非凸优化和众多需要调整的超参数使得性能提升困难，且传统方法往往重视最小化均方误差(MSE)，而忽视了在许多应用中至关重要的$L^{\infty}$误差。

**方法:** 采用一种改进过的阶段训练技术，逐步学习现有全连接神经网络的预测残差，使用额外的网络来改进整体准确性；同时探讨如何利用残差结构来指导损失函数的选择、使用的参数数量及引入自适应数据采样技术的方法。

**结果:** 通过几个基准问题验证了该框架的有效性。

**结论:** 提出的渐进式框架能够有效地训练和调优高精度神经网络，解决了非凸优化和超参数调整带来的挑战，同时考虑到了$L^{\infty}$误差的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HiPreNets%3A+High-Precision+Neural+Networks+through+Progressive+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15064&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks are powerful tools for solving nonlinear problems in
science and engineering, but training highly accurate models becomes
challenging as problem complexity increases. Non-convex optimization and
numerous hyperparameters to tune make performance improvement difficult, and
traditional approaches often prioritize minimizing mean squared error (MSE)
while overlooking $L^{\infty}$ error, which is the critical focus in many
applications. To address these challenges, we present a progressive framework
for training and tuning high-precision neural networks (HiPreNets). Our
approach refines a previously explored staged training technique for neural
networks that improves an existing fully connected neural network by
sequentially learning its prediction residuals using additional networks,
leading to improved overall accuracy. We discuss how to take advantage of the
structure of the residuals to guide the choice of loss function, number of
parameters to use, and ways to introduce adaptive data sampling techniques. We
validate our framework's effectiveness through several benchmark problems.

</details>


### [50] [Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI](https://arxiv.org/abs/2506.15408)
*David Dembinsky, Adriano Lucieri, Stanislav Frolov, Hiba Najjar, Ko Watanabe, Andreas Dengel*

**主要类别:** cs.LG

**AI概要:** 本文通过系统性文献回顾提出了一个统一的框架VXAI，用于评估可解释的人工智能（XAI）方法，并提出了一种涵盖解释类型、评估情境性和解释质量期望的三维分类方案。


<details>
  <summary>更多</summary>
  
**动机:** 由于现代AI系统依赖于复杂的黑盒模型，如深度神经网络，其缺乏透明度导致了信任问题。尽管有越来越多的XAI方法来提供人类可理解的模型行为解释，但该领域缺少标准化的评估协议和适当的度量标准共识。

**方法:** 采用PRISMA指南进行系统的文献综述，收集了362篇相关出版物，并将它们的贡献归纳为41个功能相似的度量组。此外，还提出了一种三维分类方案。

**结果:** 创建了一个名为VXAI的框架，它提供了迄今为止最全面且结构化的XAI评估概览。

**结论:** VXAI框架支持系统性的度量选择，促进了不同方法之间的比较，并为未来扩展提供了灵活的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unifying+VXAI%3A+A+Systematic+Review+and+Framework+for+the+Evaluation+of+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15408&send_immediately=true&force_search=false)

**原文摘要:** Modern AI systems frequently rely on opaque black-box models, most notably
Deep Neural Networks, whose performance stems from complex architectures with
millions of learned parameters. While powerful, their complexity poses a major
challenge to trustworthiness, particularly due to a lack of transparency.
Explainable AI (XAI) addresses this issue by providing human-understandable
explanations of model behavior. However, to ensure their usefulness and
trustworthiness, such explanations must be rigorously evaluated. Despite the
growing number of XAI methods, the field lacks standardized evaluation
protocols and consensus on appropriate metrics. To address this gap, we conduct
a systematic literature review following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a
unified framework for the eValuation of XAI (VXAI). We identify 362 relevant
publications and aggregate their contributions into 41 functionally similar
metric groups. In addition, we propose a three-dimensional categorization
scheme spanning explanation type, evaluation contextuality, and explanation
quality desiderata. Our framework provides the most comprehensive and
structured overview of VXAI to date. It supports systematic metric selection,
promotes comparability across methods, and offers a flexible foundation for
future extensions.

</details>


### [51] [HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models](https://arxiv.org/abs/2506.15065)
*Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, Chengyu Song*

**主要类别:** cs.LG

**AI概要:** 本文首次系统地研究了基于大语言模型（LLMs）的具身代理在场景-任务不一致情况下执行长期任务时出现的幻觉现象。通过构建一个专门用于诱导高幻觉率的数据集，研究发现模型虽然能够进行推理，但无法解决场景-任务间的矛盾，这暴露了处理不可行任务时的基本局限性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型越来越多地被用作具身代理的认知核心，由用户指令与物理环境观察之间不匹配导致的幻觉问题变得越来越严重，可能导致导航错误等问题。因此，有必要理解这些问题发生的程度、触发它们的不一致性类型以及当前模型对此类情况的反应方式。

**方法:** 作者们建立了一个基于现有基准测试的幻觉探测数据集，该数据集能够诱导出比基础提示高出40倍的幻觉率。然后，在两个模拟环境中对12个不同模型进行了评估。

**结果:** 研究表明，尽管这些模型展示了某种程度上的推理能力，但在面对场景-任务不一致的情况下，它们无法妥善解决这类问题，揭示了现有模型在处理不可行任务时存在的根本限制。

**结论:** 这项工作不仅突出了当前基于LLM的具身代理面临的重要挑战，还为开发更强大可靠的规划策略提供了宝贵的见解和指导方针。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HEAL%3A+An+Empirical+Study+on+Hallucinations+in+Embodied+Agents+Driven+by+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15065&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly being adopted as the cognitive
core of embodied agents. However, inherited hallucinations, which stem from
failures to ground user instructions in the observed physical environment, can
lead to navigation errors, such as searching for a refrigerator that does not
exist. In this paper, we present the first systematic study of hallucinations
in LLM-based embodied agents performing long-horizon tasks under scene-task
inconsistencies. Our goal is to understand to what extent hallucinations occur,
what types of inconsistencies trigger them, and how current models respond. To
achieve these goals, we construct a hallucination probing set by building on an
existing benchmark, capable of inducing hallucination rates up to 40x higher
than base prompts. Evaluating 12 models across two simulation environments, we
find that while models exhibit reasoning, they fail to resolve scene-task
inconsistencies-highlighting fundamental limitations in handling infeasible
tasks. We also provide actionable insights on ideal model behavior for each
scenario, offering guidance for developing more robust and reliable planning
strategies.

</details>


### [52] [Reward Models in Deep Reinforcement Learning: A Survey](https://arxiv.org/abs/2506.15421)
*Rui Yu, Shenghua Wan, Yucen Wang, Chen-Xiao Gao, Le Gan, Zongzhang Zhang, De-Chuan Zhan*

**主要类别:** cs.LG

**AI概要:** 本综述论文对深度强化学习文献中的奖励建模技术进行了全面回顾，包括背景、方法分类、应用实例以及评估方法，并指出了未来研究的前景。


<details>
  <summary>更多</summary>
  
**动机:** 为了引导策略优化，在强化学习中引入了奖励模型作为期望目标的代理，本文旨在提供一个系统性的奖励模型综述，以填补现有文献中这一领域的空白。

**方法:** 本文首先概述了奖励建模的背景和预备知识，然后介绍了最近的奖励建模方法，并根据来源、机制和学习范式对其进行分类。

**结果:** 讨论了这些奖励建模技术的各种应用，并回顾了奖励模型的评估方法。

**结论:** 文章最后强调了奖励建模中有前景的研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reward+Models+in+Deep+Reinforcement+Learning%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15421&send_immediately=true&force_search=false)

**原文摘要:** In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.

</details>


### [53] [Zero-Shot Reinforcement Learning Under Partial Observability](https://arxiv.org/abs/2506.15446)
*Scott Jeen, Tom Bewley, Jonathan M. Cullen*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在部分可观测环境下标准零样本强化学习方法的性能下降问题，并展示了基于记忆的架构能够有效解决这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们发现，当马尔可夫状态只能部分观测到时，这会影响零样本强化学习方法的表现。因此，研究旨在寻找一种有效的方法来改善在这些条件下的性能。

**方法:** 通过评估基于记忆的零样本RL方法，在状态、奖励以及动态变化都只能部分观测的情况下，与不使用记忆机制的基础方法进行对比。

**结果:** 结果显示，在部分可观测性条件下，基于记忆的方法相较于没有记忆机制的方法表现更好。

**结论:** 对于部分可观测环境中的零样本强化学习任务来说，采用基于记忆的架构是一种有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Reinforcement+Learning+Under+Partial+Observability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15446&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that, under certain assumptions, zero-shot
reinforcement learning (RL) methods can generalise to any unseen task in an
environment after reward-free pre-training. Access to Markov states is one such
assumption, yet, in many real-world applications, the Markov state is only
partially observable. Here, we explore how the performance of standard
zero-shot RL methods degrades when subjected to partially observability, and
show that, as in single-task RL, memory-based architectures are an effective
remedy. We evaluate our memory-based zero-shot RL methods in domains where the
states, rewards and a change in dynamics are partially observed, and show
improved performance over memory-free baselines. Our code is open-sourced via:
https://enjeeneer.io/projects/bfms-with-memory/.

</details>


### [54] [Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions](https://arxiv.org/abs/2506.15115)
*Lulu Xue, Shengshan Hu, Wei Lu, Yan Shen, Dongxu Li, Peijin Guo, Ziqi Zhou, Minghui Li, Yanjun Zhang, Leo Yu Zhang*

**主要类别:** cs.LG

**AI概要:** 本文首次对机器遗忘验证方法进行了系统性综述，提出了基于行为验证和参数验证两大类别的分类体系，并分析了各类方法的假设、优缺点以及潜在漏洞，最后指出了当前研究中的开放问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着对隐私保护、安全性和法律合规性的需求不断增长（如GDPR），机器遗忘作为一种确保机器学习模型可控性和符合法规的技术变得至关重要。然而，该领域的一个基本挑战在于如何有效地验证遗忘操作是否成功且彻底地执行。尽管关于遗忘技术的研究越来越多，但验证方法学仍然相对较少被探索，并且常常是零散的。现有方法缺乏统一的分类法和系统的评估框架。

**方法:** 文章提出了一种分类体系，将现有的技术分为两大主要类别——基于用于评估遗忘保真度的证据类型的行为验证和参数验证。对于每个类别，都考察了代表性方法，分析了它们的基本假设、优势和局限性，并识别了实际部署中的潜在漏洞。

**结果:** 通过这项工作，作者们为开发更加稳健、高效且理论基础更坚实的遗忘验证机制奠定了基础。此外，论文还明确了一系列当前验证研究中的开放问题。

**结论:** 本文提供了首个结构化的机器遗忘验证方法调查，旨在为未来的研究提供一个坚实的基础，并推动在这一重要领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Reliable+Forgetting%3A+A+Survey+on+Machine+Unlearning+Verification%2C+Challenges%2C+and+Future+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15115，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15115&send_immediately=true&force_search=false)

**原文摘要:** With growing demands for privacy protection, security, and legal compliance
(e.g., GDPR), machine unlearning has emerged as a critical technique for
ensuring the controllability and regulatory alignment of machine learning
models. However, a fundamental challenge in this field lies in effectively
verifying whether unlearning operations have been successfully and thoroughly
executed. Despite a growing body of work on unlearning techniques, verification
methodologies remain comparatively underexplored and often fragmented. Existing
approaches lack a unified taxonomy and a systematic framework for evaluation.
To bridge this gap, this paper presents the first structured survey of machine
unlearning verification methods. We propose a taxonomy that organizes current
techniques into two principal categories -- behavioral verification and
parametric verification -- based on the type of evidence used to assess
unlearning fidelity. We examine representative methods within each category,
analyze their underlying assumptions, strengths, and limitations, and identify
potential vulnerabilities in practical deployment. In closing, we articulate a
set of open problems in current verification research, aiming to provide a
foundation for developing more robust, efficient, and theoretically grounded
unlearning verification mechanisms.

</details>


### [55] [ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning](https://arxiv.org/abs/2506.15181)
*Bing Liu, Chengcheng Zhao, Li Chai, Peng Cheng, Yaonan Wang*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为ImprovDML的去中心化分布式机器学习框架，该框架能够在保证隐私保护和抵御拜占庭攻击的同时实现高模型准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前，在分布式机器学习中同时解决拜占庭攻击和隐私泄露问题变得非常重要，但结合这些技术往往会导致模型准确性的显著下降。

**方法:** 提出了一个去中心化的分布式机器学习框架ImprovDML，利用一种鲁棒向量共识算法来计算正常代理节点凸包内的点，并在每次迭代时引入多变量高斯噪声到梯度以保护隐私。

**结果:** 提供了非凸设置下的收敛性保证并推导了渐近学习误差界限，这些界限比现有工作中的更紧致；对于隐私分析采用了集中地理隐私的概念，证明了它能够比差分隐私提供更好的隐私保护与模型准确性之间的权衡。

**结论:** 通过数值模拟验证了理论结果，表明提出的ImprovDML框架在保持隐私性和抵抗拜占庭攻击的同时可以达到较高的模型准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImprovDML%3A+Improved+Trade-off+in+Private+Byzantine-Resilient+Distributed+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15181&send_immediately=true&force_search=false)

**原文摘要:** Jointly addressing Byzantine attacks and privacy leakage in distributed
machine learning (DML) has become an important issue. A common strategy
involves integrating Byzantine-resilient aggregation rules with differential
privacy mechanisms. However, the incorporation of these techniques often
results in a significant degradation in model accuracy. To address this issue,
we propose a decentralized DML framework, named ImprovDML, that achieves high
model accuracy while simultaneously ensuring privacy preservation and
resilience to Byzantine attacks. The framework leverages a kind of resilient
vector consensus algorithms that can compute a point within the normal
(non-Byzantine) agents' convex hull for resilient aggregation at each
iteration. Then, multivariate Gaussian noises are introduced to the gradients
for privacy preservation. We provide convergence guarantees and derive
asymptotic learning error bounds under non-convex settings, which are tighter
than those reported in existing works. For the privacy analysis, we adopt the
notion of concentrated geo-privacy, which quantifies privacy preservation based
on the Euclidean distance between inputs. We demonstrate that it enables an
improved trade-off between privacy preservation and model accuracy compared to
differential privacy. Finally, numerical simulations validate our theoretical
results.

</details>


### [56] [Pixel-level Certified Explanations via Randomized Smoothing](https://arxiv.org/abs/2506.15499)
*Alaa Anani, Tobias Lorenz, Mario Fritz, Bernt Schiele*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pixel-level+Certified+Explanations+via+Randomized+Smoothing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15499&send_immediately=true&force_search=false)

**原文摘要:** Post-hoc attribution methods aim to explain deep learning predictions by
highlighting influential input pixels. However, these explanations are highly
non-robust: small, imperceptible input perturbations can drastically alter the
attribution map while maintaining the same prediction. This vulnerability
undermines their trustworthiness and calls for rigorous robustness guarantees
of pixel-level attribution scores. We introduce the first certification
framework that guarantees pixel-level robustness for any black-box attribution
method using randomized smoothing. By sparsifying and smoothing attribution
maps, we reformulate the task as a segmentation problem and certify each
pixel's importance against $\ell_2$-bounded perturbations. We further propose
three evaluation metrics to assess certified robustness, localization, and
faithfulness. An extensive evaluation of 12 attribution methods across 5
ImageNet models shows that our certified attributions are robust,
interpretable, and faithful, enabling reliable use in downstream tasks. Our
code is at https://github.com/AlaaAnani/certified-attributions.

</details>


### [57] [Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors](https://arxiv.org/abs/2506.15190)
*Jiyi Wang, Jingyang Ke, Bo Dai, Anqi Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于技能的模仿学习框架(SKIL)，用于理解动物行为。该框架通过表示学习来推断可解释的技能集，并将策略参数化为这些技能的动态混合体，从而更准确地反映动物行为生成过程。


<details>
  <summary>更多</summary>
  
**动机:** 现有的行为分割方法在处理动物如何灵活重组有限的核心运动原语以满足多样任务需求时过于简化，通常在限制性生成假设下施加离散音节。为了更好地反映动物行为生成的过程，作者提出了新的方法。

**方法:** 研究者引入了基于技能的模仿学习（SKIL），这是一种基于强化学习的模仿框架，它利用转换概率上的表示学习来推断出可解释的技能集（即行为的潜在基础函数），并将策略作为这些技能的动态混合进行参数化。

**结果:** 经过简单的网格世界、离散迷宫以及自由移动动物的无约束视频等不同任务的验证，该方法能够识别可重复使用的技能组件，学习连续演化的组合策略，并生成超越传统离散模型能力的真实轨迹。

**结论:** 通过利用具有组合表示的生成行为建模，该方法提供了一个简洁且有原则的说明，解释了复杂动物行为是如何从基本运动原语的动态组合中产生的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Task-Agnostic+Skill+Bases+to+Uncover+Motor+Primitives+in+Animal+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15190&send_immediately=true&force_search=false)

**原文摘要:** Animals flexibly recombine a finite set of core motor primitives to meet
diverse task demands, but existing behavior-segmentation methods oversimplify
this process by imposing discrete syllables under restrictive generative
assumptions. To reflect the animal behavior generation procedure, we introduce
skill-based imitation learning (SKIL) for behavior understanding, a
reinforcement learning-based imitation framework that (1) infers interpretable
skill sets, i.e., latent basis functions of behavior, by leveraging
representation learning on transition probabilities, and (2) parameterizes
policies as dynamic mixtures of these skills. We validate our approach on a
simple grid world, a discrete labyrinth, and unconstrained videos of freely
moving animals. Across tasks, it identifies reusable skill components, learns
continuously evolving compositional policies, and generates realistic
trajectories beyond the capabilities of traditional discrete models. By
exploiting generative behavior modeling with compositional representations, our
method offers a concise, principled account of how complex animal behaviors
emerge from dynamic combinations of fundamental motor primitives.

</details>


### [58] [Over-squashing in Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2506.15507)
*Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein*

**主要类别:** cs.LG

**AI概要:** 本文研究了时空图神经网络（STGNNs）中的信息传播问题，特别是过压缩现象，并且揭示了与静态图神经网络相比其特有的特性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管图神经网络在许多领域取得了显著的成功，但最近的理论进展发现了它们在信息传播能力上的基本限制，例如过压缩现象，其中远离的节点无法有效地交换信息。这种问题在处理与图节点相关的序列的时空图神经网络中尚未被探索。

**方法:** 本文对时空过压缩问题进行了形式化定义，并展示了它与静态情况相比的独特特征。通过分析表明，出乎意料的是，卷积STGNNs更倾向于从时间上距离较远而非较近的点传播信息。此外，证明了采用时间-空间或先时间后空间处理范式的架构都受到这种现象的影响。

**结果:** 研究结果在合成和真实世界数据集上得到了验证，提供了对其操作动态的更深入理解以及为更有效的设计提供原则性指导。

**结论:** 文章提出了关于时空图神经网络中过压缩问题的新见解，为未来该领域的有效模型设计提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Over-squashing+in+Spatiotemporal+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15507&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have achieved remarkable success across various
domains. However, recent theoretical advances have identified fundamental
limitations in their information propagation capabilities, such as
over-squashing, where distant nodes fail to effectively exchange information.
While extensively studied in static contexts, this issue remains unexplored in
Spatiotemporal GNNs (STGNNs), which process sequences associated with graph
nodes. Nonetheless, the temporal dimension amplifies this challenge by
increasing the information that must be propagated. In this work, we formalize
the spatiotemporal over-squashing problem and demonstrate its distinct
characteristics compared to the static case. Our analysis reveals that
counterintuitively, convolutional STGNNs favor information propagation from
points temporally distant rather than close in time. Moreover, we prove that
architectures that follow either time-and-space or time-then-space processing
paradigms are equally affected by this phenomenon, providing theoretical
justification for computationally efficient implementations. We validate our
findings on synthetic and real-world datasets, providing deeper insights into
their operational dynamics and principled guidance for more effective designs.

</details>


### [59] [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)
*Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark*

**主要类别:** cs.LG

**AI概要:** 研究发现稀疏自编码器(SAEs)中频繁激活的密集潜在变量不仅持续存在，而且往往反映了模型中有意义的表示。这些密集潜在变量在语言模型计算中扮演着功能性角色，不应被视为训练噪声。


<details>
  <summary>更多</summary>
  
**动机:** 研究人员希望探讨稀疏自编码器（SAEs）中出现的密集潜在变量是否为训练过程中的不理想产物，并试图理解它们的几何结构、功能以及来源。

**方法:** 研究者们系统地研究了密集潜在变量的几何特性、功能及起源，展示了它们不仅是持久存在的，而且通常反映了有意义的模型表征。他们首先证明了密集潜在变量倾向于形成反向对，重构残差流中的特定方向；接着引入了一个密集潜在变量的分类法，识别出与位置跟踪、上下文绑定、熵调节等相关的类别；最后分析了这些特征如何随层演变。

**结果:** 研究结果表明，密集潜在变量在语言模型计算中承担着功能性作用，从早期层的结构性特征到中间层的语义性特征，再到最后一层面向输出的信号，揭示了它们在不同层级上的转变。

**结论:** 结论是密集潜在变量不是训练噪音，而是在语言模型中发挥重要作用的内在属性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dense+SAE+Latents+Are+Features%2C+Not+Bugs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15679&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders (SAEs) are designed to extract interpretable features
from language models by enforcing a sparsity constraint. Ideally, training an
SAE would yield latents that are both sparse and semantically meaningful.
However, many SAE latents activate frequently (i.e., are \emph{dense}), raising
concerns that they may be undesirable artifacts of the training procedure. In
this work, we systematically investigate the geometry, function, and origin of
dense latents and show that they are not only persistent but often reflect
meaningful model representations. We first demonstrate that dense latents tend
to form antipodal pairs that reconstruct specific directions in the residual
stream, and that ablating their subspace suppresses the emergence of new dense
features in retrained SAEs -- suggesting that high density features are an
intrinsic property of the residual space. We then introduce a taxonomy of dense
latents, identifying classes tied to position tracking, context binding,
entropy regulation, letter-specific output signals, part-of-speech, and
principal component reconstruction. Finally, we analyze how these features
evolve across layers, revealing a shift from structural features in early
layers, to semantic features in mid layers, and finally to output-oriented
signals in the last layers of the model. Our findings indicate that dense
latents serve functional roles in language model computation and should not be
dismissed as training noise.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [CALM: Contextual Analog Logic with Multimodality](https://arxiv.org/abs/2506.14936)
*Maxwell J. Jacobson, Corey J. Maley, Yexiang Xue*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种结合符号推理与神经生成的上下文模拟逻辑多模态(CALM)方法，该方法在多模态输入上实现了92.2%的填空物体放置任务准确率，并且能够根据逻辑约束和细致的人类偏好生成空间热图。


<details>
  <summary>更多</summary>
  
**动机:** 经典二值逻辑系统不能捕捉人类决策中的细微差别，并且在多模态环境中需要人为设定基础，这可能是临时、僵化且脆弱的。而神经网络虽然擅长从多模态数据中提取丰富的上下文信息，却缺乏可解释的推理结构。CALM旨在弥合逻辑与神经感知之间的差距，创造一种能够在多模态输入上进行推理的模拟逻辑。

**方法:** CALM通过域树来表示每个谓词，当实体的上下文基础确定时，会迭代地细化其模拟真值。这种迭代细化由能够捕捉多模态信息的神经网络预测，并通过一个符号推理模块过滤以确保满足约束条件。

**结果:** 在填空物体放置任务中，CALM达成了92.2%的准确率，超过了经典的逻辑（86.3%）和大型语言模型（LLM, 59.4%）基线。此外，它还展示了符合逻辑约束及细腻人类偏好的空间热图生成能力。

**结论:** CALM显示了在多模态环境中与偏好保持一致的同时使用逻辑结构进行推理的潜力，为下一代AI系统奠定了基础，这些系统需要逻辑的精确性和解释性以及神经网络的多模态信息处理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CALM%3A+Contextual+Analog+Logic+with+Multimodality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14936，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14936&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce Contextual Analog Logic with Multimodality (CALM).
CALM unites symbolic reasoning with neural generation, enabling systems to make
context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human
decision-making. They also require human grounding in multi-modal environments,
which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting
rich contextual information from multi-modal data, but lack interpretable
structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception,
creating an analog logic that can reason over multi-modal inputs. Without this
integration, AI systems remain either brittle or unstructured, unable to
generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate
to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which
iteratively refines its analog truth value when the contextual groundings of
its entities are determined. The iterative refinement is predicted by neural
networks capable of capturing multi-modal information and is filtered through a
symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%
accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It
also demonstrated spatial heatmap generation aligned with logical constraints
and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure
while aligning with preferences in multi-modal environments. It lays the
foundation for next-gen AI systems that require the precision and
interpretation of logic and the multimodal information processing of neural
networks.

</details>


### [61] [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.14990)
*Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Mykola Pechenizkiy*

**主要类别:** cs.AI

**AI概要:** 本文提出了MEAL，这是一个专为持续多智能体强化学习(CMARL)设计的基准测试。通过使用JAX进行GPU加速，MEAL能够支持在标准台式PC上几个小时内完成100个任务序列的持续学习。研究发现，单纯结合流行的持续学习(CL)和多智能体强化学习(MARL)方法虽然在简单环境中表现出色，但在需要持续协调和适应的复杂环境中表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 目前针对合作多智能体设置中的持续学习(CCL)领域探索较少，并且现有CL基准测试通常运行在CPU上，导致计算瓶颈并限制了任务序列的长度。因此，有必要创建一个专门针对持续多智能体强化学习(CMARL)的基准测试。

**方法:** 开发了一个名为MEAL（Multi-agent Environments for Adaptive Learning）的新基准测试，它利用JAX来实现GPU加速，从而允许在几小时内处理包含100个任务的序列。此外，还进行了消融研究以识别对于CMARL来说重要的架构和算法特征。

**结果:** 结果表明，将流行的CL和MARL方法简单组合可以在简单环境中取得良好的性能，但无法很好地扩展到更复杂的场景，这些场景需要长期的协作与适应。

**结论:** MEAL作为首个针对持续多智能体强化学习设计的基准，解决了现有计算瓶颈的问题，并揭示了当前方法在面对复杂环境时的不足之处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEAL%3A+A+Benchmark+for+Continual+Multi-Agent+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14990&send_immediately=true&force_search=false)

**原文摘要:** Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms, with environment availability strongly
impacting research. One particularly underexplored intersection is continual
learning (CL) in cooperative multi-agent settings. To remedy this, we introduce
MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark
tailored for continual multi-agent reinforcement learning (CMARL). Existing CL
benchmarks run environments on the CPU, leading to computational bottlenecks
and limiting the length of task sequences. MEAL leverages JAX for GPU
acceleration, enabling continual learning across sequences of 100 tasks on a
standard desktop PC in a few hours. We show that naively combining popular CL
and MARL methods yields strong performance on simple environments, but fails to
scale to more complex settings requiring sustained coordination and adaptation.
Our ablation study identifies architectural and algorithmic features critical
for CMARL on MEAL.

</details>


### [62] [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
*Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Truncated Proximal Policy Optimization (T-PPO)的新方法，旨在通过优化策略更新和限制响应生成长度来提高大型语言模型在测试时扩展的训练效率。T-PPO通过引入Extended Generalized Advantage Estimation(EGAE)进行优势估计，并设计了一个计算优化机制来独立优化策略和价值模型，从而减少了冗余计算并加速了训练过程。实验结果表明，T-PPO相比现有方法能够将推理LLM的训练效率提高至2.5倍。


<details>
  <summary>更多</summary>
  
**动机:** 由于现有的近端策略优化（PPO）及其变体在训练大型语言模型时存在时间消耗过长的问题，特别是在生成较长响应时更加明显，因此需要一种更高效的方法来改进训练效率。此外，传统方法中硬件利用率低下的问题也亟待解决。

**方法:** 作者提出了Truncated Proximal Policy Optimization (T-PPO)，它基于PPO进行了拓展，通过简化策略更新流程以及控制响应长度来提高训练效率。为了解决不完整响应带来的优势估计问题，他们引入了Extended Generalized Advantage Estimation (EGAE)。同时，为了进一步提升计算效率，开发了一种新的机制，该机制允许策略与价值模型分开优化，并通过筛选提示词和截断标记减少不必要的计算。

**结果:** 研究者们在AIME 2024上用一个32B的基础模型展示了T-PPO的有效性和效率。实验结果显示，T-PPO能将推理型大语言模型的训练效率提升高达2.5倍，并且性能优于当前存在的竞争方法。

**结论:** T-PPO作为一种新的强化学习方法，显著提高了大型语言模型特别是那些需要长时间生成回答的模型的训练效率。其关键在于有效利用了硬件资源、优化了策略与价值函数的学习过程，并且保持了良好的收敛性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Truncated+Proximal+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15050&send_immediately=true&force_search=false)

**原文摘要:** Recently, test-time scaling Large Language Models (LLMs) have demonstrated
exceptional reasoning capabilities across scientific and professional tasks by
generating long chains-of-thought (CoT). As a crucial component for developing
these reasoning models, reinforcement learning (RL), exemplified by Proximal
Policy Optimization (PPO) and its variants, allows models to learn through
trial and error. However, PPO can be time-consuming due to its inherent
on-policy nature, which is further exacerbated by increasing response lengths.
In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a
novel extension to PPO that improves training efficiency by streamlining policy
update and length-restricted response generation. T-PPO mitigates the issue of
low hardware utilization, an inherent drawback of fully synchronized
long-generation procedures, where resources often sit idle during the waiting
periods for complete rollouts. Our contributions are two-folds. First, we
propose Extended Generalized Advantage Estimation (EGAE) for advantage
estimation derived from incomplete responses while maintaining the integrity of
policy learning. Second, we devise a computationally optimized mechanism that
allows for the independent optimization of the policy and value models. By
selectively filtering prompt and truncated tokens, this mechanism reduces
redundant computations and accelerates the training process without sacrificing
convergence performance. We demonstrate the effectiveness and efficacy of T-PPO
on AIME 2024 with a 32B base model. The experimental results show that T-PPO
improves the training efficiency of reasoning LLMs by up to 2.5x and
outperforms its existing competitors.

</details>


### [63] [HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges](https://arxiv.org/abs/2506.15196)
*Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian*

**主要类别:** cs.AI

**AI概要:** 提出了HeurAgenix，一个基于大型语言模型的两阶段超启发式框架，用于解决组合优化问题。该框架首先通过比较种子启发式解和高质量解来进化启发式策略，然后在解决问题时自动选择最合适的启发式方法。实验表明HeurAgenix超越了现有的基于LLM的超启发式算法，并且可以与专业求解器相媲美或超过它们。


<details>
  <summary>更多</summary>
  
**动机:** 传统的启发式算法设计依赖于人工专业知识并且难以泛化到不同的实例中。为了解决这个问题并提高对组合优化问题的适应性，研究者们开发了HeurAgenix框架。

**方法:** HeurAgenix框架利用大型语言模型(LLMs)进行两阶段处理：首先，在启发式进化阶段，使用LLM对比种子启发式解和更高质量的解，提取可重用的进化策略；其次，在问题解决过程中，依靠LLM的感知能力动态选择当前问题状态下最有潜力的启发式方法。此外，为了降低成本，可以选择微调过的轻量级模型作为选择器，并通过双奖励机制来改进它以应对噪声标注。

**结果:** 广泛的实验结果表明，HeurAgenix不仅优于现有的基于LLM的超启发式算法，而且能够达到甚至超过专门求解器的表现。

**结论:** HeurAgenix提供了一种新颖的方法来自动进化和选择启发式算法，以有效解决组合优化问题。其表现超过了现有基于LLM的技术，并且能够在多种场景下保持竞争力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HeurAgenix%3A+Leveraging+LLMs+for+Solving+Complex+Combinatorial+Optimization+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15196&send_immediately=true&force_search=false)

**原文摘要:** Heuristic algorithms play a vital role in solving combinatorial optimization
(CO) problems, yet traditional designs depend heavily on manual expertise and
struggle to generalize across diverse instances. We introduce
\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large
language models (LLMs) that first evolves heuristics and then selects among
them automatically. In the heuristic evolution phase, HeurAgenix leverages an
LLM to compare seed heuristic solutions with higher-quality solutions and
extract reusable evolution strategies. During problem solving, it dynamically
picks the most promising heuristic for each problem state, guided by the LLM's
perception ability. For flexibility, this selector can be either a
state-of-the-art LLM or a fine-tuned lightweight model with lower inference
cost. To mitigate the scarcity of reliable supervision caused by CO complexity,
we fine-tune the lightweight heuristic selector with a dual-reward mechanism
that jointly exploits singals from selection preferences and state perception,
enabling robust selection under noisy annotations. Extensive experiments on
canonical benchmarks show that HeurAgenix not only outperforms existing
LLM-based hyper-heuristics but also matches or exceeds specialized solvers.
Code is available at https://github.com/microsoft/HeurAgenix.

</details>


### [64] [Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study](https://arxiv.org/abs/2506.15207)
*Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk*

**主要类别:** cs.AI

**AI概要:** 本文探讨了基于强化学习(RL)的自主地球观测任务规划，并通过多智能体强化学习(MARL)框架扩展到多卫星星座。研究解决了能量和数据存储限制、卫星观测不确定性以及在部分可观测性下的去中心化协调复杂性等问题。通过近似真实的卫星仿真环境，评估了包括PPO, IPPO, MAPPO和HAPPO在内的最新MARL算法的训练稳定性和性能。结果显示MARL能够有效平衡成像与资源管理，同时解决多卫星协调中的非平稳性和奖励互依性问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着低地球轨道(LEO)卫星数量的快速增长，对气候监测、灾害管理等领域提出了新的挑战。然而，在多卫星系统中实现自主协调仍然是一个基本难题。传统的优化方法难以满足动态地球观测任务实时决策的需求，因此需要采用强化学习(RL)和多智能体强化学习(MARL)技术来应对这些挑战。

**方法:** 研究者们首先为单个卫星操作建模，然后使用多智能体强化学习(MARL)框架扩展至多卫星星座。面对诸如能量与数据存储限制、卫星观察中的不确定性以及在信息不完全情况下的分散协调等关键挑战时，研究利用了一个接近现实情况的卫星模拟环境，以测试当前最先进的MARL算法（如PPO, IPPO, MAPPO及HAPPO）的训练稳定性和表现。

**结果:** 研究结果表明，MARL能够在处理多卫星协作过程中遇到的非静态特性与奖惩相互依赖性的同时，有效地达到成像需求与资源管理之间的平衡。

**结论:** 本研究为自主卫星运行提供了基础，并为改善去中心化地球观测任务中的策略学习提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Reinforcement+Learning+for+Autonomous+Multi-Satellite+Earth+Observation%3A+A+Realistic+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15207&send_immediately=true&force_search=false)

**原文摘要:** The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised
Earth Observation (EO) missions, addressing challenges in climate monitoring,
disaster management, and more. However, autonomous coordination in
multi-satellite systems remains a fundamental challenge. Traditional
optimisation approaches struggle to handle the real-time decision-making
demands of dynamic EO missions, necessitating the use of Reinforcement Learning
(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we
investigate RL-based autonomous EO mission planning by modelling
single-satellite operations and extending to multi-satellite constellations
using MARL frameworks. We address key challenges, including energy and data
storage limitations, uncertainties in satellite observations, and the
complexities of decentralised coordination under partial observability. By
leveraging a near-realistic satellite simulation environment, we evaluate the
training stability and performance of state-of-the-art MARL algorithms,
including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can
effectively balance imaging and resource management while addressing
non-stationarity and reward interdependency in multi-satellite coordination.
The insights gained from this study provide a foundation for autonomous
satellite operations, offering practical guidelines for improving policy
learning in decentralised EO missions.

</details>


### [65] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han*

**主要类别:** cs.AI

**AI概要:** 本文针对海上物联网（MIoT）任务的不确定性，提出了一种UAVs和船只合作的多接入边缘计算框架，并通过Lyapunov优化和异构代理软演员-评论家方法来解决计算卸载和资源分配问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着海上物联网（MIoT）对计算需求的快速增长，基于无人机（UAVs）和船只的多接入边缘计算（MEC）能够满足这些需求。但是，不确定的海上任务给计算卸载和资源分配带来了低效的挑战。

**方法:** 提出了一个合作的MEC框架，包括MIoT设备、无人机和船只之间的协作。利用Lyapunov优化处理不可预测的任务到达和变化的计算资源可用性。将长期约束转化为短期约束，得到一系列小规模优化问题。考虑到无人机和船只行为与资源的异质性，将小规模优化问题重新表述为马尔可夫博弈（MG）。提出了一种异构代理软演员-评论家方法，以顺序更新各种神经网络并有效解决MG问题。

**结果:** 仿真结果验证了所提方法在解决计算卸载和资源分配问题上的有效性。

**结论:** 该研究提供了一个有效的框架，利用无人机和船只的合作来优化海上物联网中的计算卸载和资源分配，特别适用于处理不确定性的任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Joint+Computation+Offloading+and+Resource+Allocation+for+Uncertain+Maritime+MEC+via+Cooperation+of+UAVs+and+Vessels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15225&send_immediately=true&force_search=false)

**原文摘要:** The computation demands from the maritime Internet of Things (MIoT) increase
rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels
based multi-access edge computing (MEC) can fulfill these MIoT requirements.
However, the uncertain maritime tasks present significant challenges of
inefficient computation offloading and resource allocation. In this paper, we
focus on the maritime computation offloading and resource allocation through
the cooperation of UAVs and vessels, with consideration of uncertain tasks.
Specifically, we propose a cooperative MEC framework for computation offloading
and resource allocation, including MIoT devices, UAVs and vessels. Then, we
formulate the optimization problem to minimize the total execution time. As for
the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the
unpredictable task arrivals and varying computational resource availability. By
converting the long-term constraints into short-term constraints, we obtain a
set of small-scale optimization problems. Further, considering the
heterogeneity of actions and resources of UAVs and vessels, we reformulate the
small-scale optimization problem into a Markov game (MG). Moreover, a
heterogeneous-agent soft actor-critic is proposed to sequentially update
various neural networks and effectively solve the MG problem. Finally,
simulations are conducted to verify the effectiveness in addressing
computational offloading and resource allocation.

</details>


### [66] [Efficient and Generalizable Environmental Understanding for Visual Navigation](https://arxiv.org/abs/2506.15377)
*Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao*

**主要类别:** cs.AI

**AI概要:** 提出了一种因果感知导航（CAN）方法，通过引入因果理解模块来增强代理对环境的理解能力，在各种任务和模拟环境中表现优于基线。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在处理历史观测时忽略了数据内部的关联结构，这可能限制了任务性能的进一步提高。

**方法:** 从因果关系的角度审视导航任务的独特特征，并提出了一个因果框架，以突出传统序列方法的局限性。基于此洞察，研究者们提出了Causality-Aware Navigation (CAN)，该方法包含了一个因果理解模块来提升代理对环境的认知能力。

**结果:** 实验评估表明，所提方法在不同任务和仿真环境中始终优于基准。消融研究表明这些改进归因于因果理解模块，该模块在强化学习和监督学习设置下均能有效泛化且不增加计算开销。

**结论:** 通过引入因果理解模块，新提出的CAN方法能够改善代理对环境的理解，并在多种导航任务上实现性能上的显著提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Generalizable+Environmental+Understanding+for+Visual+Navigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15377&send_immediately=true&force_search=false)

**原文摘要:** Visual Navigation is a core task in Embodied AI, enabling agents to navigate
complex environments toward given objectives. Across diverse settings within
Navigation tasks, many necessitate the modelling of sequential data accumulated
from preceding time steps. While existing methods perform well, they typically
process all historical observations simultaneously, overlooking the internal
association structure within the data, which may limit the potential for
further improvements in task performance. We address this by examining the
unique characteristics of Navigation tasks through the lens of causality,
introducing a causal framework to highlight the limitations of conventional
sequential methods. Leveraging this insight, we propose Causality-Aware
Navigation (CAN), which incorporates a Causal Understanding Module to enhance
the agent's environmental understanding capability. Empirical evaluations show
that our approach consistently outperforms baselines across various tasks and
simulation environments. Extensive ablations studies attribute these gains to
the Causal Understanding Module, which generalizes effectively in both
Reinforcement and Supervised Learning settings without computational overhead.

</details>


### [67] [Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents](https://arxiv.org/abs/2506.15567)
*Aline Dobrovsky, Konstantin Schekotihin, Christian Burmer*

**主要类别:** cs.AI

**AI概要:** 本文探讨了基于大型语言模型的规划代理（LPA）的设计与实现，以帮助故障分析工程师处理他们的案例。通过整合大模型、高级规划能力和外部工具使用，LPA能够自主处理复杂查询，从外部系统检索相关数据，并生成可读性强的回答。评估结果显示该代理在支持故障分析任务中具有操作有效性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能组件在故障分析实验室计算基础设施中的集成日益增多，尽管这些AI组件能够自动执行多种任务，但如何将这些组件组织成一个连贯且高效的工作流程成为了一个挑战。为了应对这一挑战并提高故障分析过程的效率，需要开发一种新的解决方案来辅助故障分析工程师。

**方法:** 研究设计并实现了一种基于大型语言模型的规划代理（LPA），它结合了大型语言模型的能力、先进的规划功能以及对外部工具的利用。LPA旨在通过自主处理复杂的请求、从外部系统获取相关信息以及产生易于理解的响应来协助故障分析工程师。

**结果:** 评估结果表明，所提出的基于大型语言模型的规划代理（LPA）在支持故障分析任务方面表现出良好的操作效果和可靠性。

**结论:** 基于大型语言模型的规划代理为故障分析提供了一种有效的自动化辅助手段，能够显著提升故障分析工作的效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Managing+Complex+Failure+Analysis+Workflows+with+LLM-based+Reasoning+and+Acting+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15567，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15567&send_immediately=true&force_search=false)

**原文摘要:** Failure Analysis (FA) is a highly intricate and knowledge-intensive process.
The integration of AI components within the computational infrastructure of FA
labs has the potential to automate a variety of tasks, including the detection
of non-conformities in images, the retrieval of analogous cases from diverse
data sources, and the generation of reports from annotated images. However, as
the number of deployed AI models increases, the challenge lies in orchestrating
these components into cohesive and efficient workflows that seamlessly
integrate with the FA process.
  This paper investigates the design and implementation of a Large Language
Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their
analysis cases. The LPA integrates LLMs with advanced planning capabilities and
external tool utilization, enabling autonomous processing of complex queries,
retrieval of relevant data from external systems, and generation of
human-readable responses. Evaluation results demonstrate the agent's
operational effectiveness and reliability in supporting FA tasks.

</details>


### [68] [The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games](https://arxiv.org/abs/2506.15624)
*Lyle Goodyear, Rachel Guo, Ramesh Johari*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个统一的框架，用于为多轮多人游戏中的大语言模型代理构建自然语言"状态"表示。通过在动态自私路由游戏中应用该框架，研究发现提供给代理的自然语言历史摘要、遗憾信息而非原始收益以及他人行动有限的信息能够使代理的行为更接近于博弈论均衡预测，并且游戏玩法更加稳定。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）作为动态环境下的决策者展现出潜力，但其无状态特性需要创建历史的自然语言表示。以往针对带有LLM代理的游戏的研究采用了临时的方法来编码游戏历史，这不仅掩盖了状态表示对代理行为的影响，也限制了研究之间的可比性。

**方法:** 作者提出了一种系统地构建自然语言“状态”表示的统一框架，用于提示重复多代理游戏中的LLM代理。该框架沿着三个轴来表征状态表示方法：动作信息量（即状态表示捕捉到所玩动作的程度）、奖励信息量（即状态表示描述所得奖励的程度）和提示风格（或自然语言压缩，即全文历史被概括的程度）。

**结果:** 当将这个框架应用于一个动态自私路由游戏时，尽管该游戏相对简单，研究人员发现LLM代理的行为高度依赖于自然语言的状态表示。特别是，他们观察到如果提供给代理的是过去历史的摘要而不是完整的自然语言表示、是关于遗憾的信息而不是原始收益、以及对他人的行动有限的信息，那么代理的行为会更贴近于博弈论平衡预测，并且游戏过程更加稳定。

**结论:** 研究结果表明，特定类型的自然语言状态表示可以显著影响LLM代理的行为，使其更加符合理论预期。此外，这些发现揭示了不同状态表示方法对于维持游戏稳定性的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Effect+of+State+Representation+on+LLM+Agent+Behavior+in+Dynamic+Routing+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15624&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown promise as decision-makers in dynamic
settings, but their stateless nature necessitates creating a natural language
representation of history. We present a unifying framework for systematically
constructing natural language "state" representations for prompting LLM agents
in repeated multi-agent games. Previous work on games with LLM agents has taken
an ad hoc approach to encoding game history, which not only obscures the impact
of state representation on agents' behavior, but also limits comparability
between studies. Our framework addresses these gaps by characterizing methods
of state representation along three axes: action informativeness (i.e., the
extent to which the state representation captures actions played); reward
informativeness (i.e., the extent to which the state representation describes
rewards obtained); and prompting style (or natural language compression, i.e.,
the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it
admits a simple equilibrium both in theory and in human subject experiments
\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find
that there are key dependencies of LLM agent behavior on the natural language
state representation. In particular, we observe that representations which
provide agents with (1) summarized, rather than complete, natural language
representations of past history; (2) information about regrets, rather than raw
payoffs; and (3) limited information about others' actions lead to behavior
that more closely matches game theoretic equilibrium predictions, and with more
stable game play by the agents. By contrast, other representations can exhibit
either large deviations from equilibrium, higher variation in dynamic game play
over time, or both.

</details>


### [69] [The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy](https://arxiv.org/abs/2506.15639)
*James Weichert, Daniel Dunlap, Mohammed Farghally, Hoda Eldardiry*

**主要类别:** cs.AI

**AI概要:** 本文针对当前计算机科学课程在人工智能伦理和政策教育方面的不足，开发了一个AI政策模块，并通过试点项目验证了该模块对学生态度的积极影响。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能技术在个人和专业场景中的深入应用，对AI伦理、治理和监管的关注日益增加。然而，现有的高等教育计算课程未能充分准备未来的AI从业者将抽象的道德原则和规范性政策偏好融入AI系统的设计与开发中。

**方法:** 研究者们开发了一个AI政策模块，并将其引入计算机科学课程中，以帮助学生了解AI政策领域，并能够将伦理原则转化为实践。基于2024年秋季的成功试点，他们更新并扩展了该模块，包括一个关于'AI监管'的技术作业。此外，还通过模块前后的调查问卷来评估学生对于AI伦理和政策的态度变化。

**结果:** 经过模块学习后，学生们对AI技术的伦理影响表示出更多的担忧，同时也表达了对自己参与AI监管讨论能力的信心增强。

**结论:** AI政策模块被证明是一个有效的工具，它不仅提高了学生对AI伦理问题的认识，而且增强了他们在面对AI伦理挑战时运用政策的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+AI+Policy+Module%3A+Developing+Computer+Science+Student+Competency+in+AI+Ethics+and+Policy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15639&send_immediately=true&force_search=false)

**原文摘要:** As artificial intelligence (AI) further embeds itself into many settings
across personal and professional contexts, increasing attention must be paid
not only to AI ethics, but also to the governance and regulation of AI
technologies through AI policy. However, the prevailing post-secondary
computing curriculum is currently ill-equipped to prepare future AI
practitioners to confront increasing demands to implement abstract ethical
principles and normative policy preferences into the design and development of
AI systems. We believe that familiarity with the 'AI policy landscape' and the
ability to translate ethical principles to practices will in the future
constitute an important responsibility for even the most technically-focused AI
engineers.
  Toward preparing current computer science (CS) students for these new
expectations, we developed an AI Policy Module to introduce discussions of AI
policy into the CS curriculum. Building on a successful pilot in fall 2024, in
this innovative practice full paper we present an updated and expanded version
of the module, including a technical assignment on "AI regulation". We present
the findings from our pilot of the AI Policy Module 2.0, evaluating student
attitudes towards AI ethics and policy through pre- and post-module surveys.
Following the module, students reported increased concern about the ethical
impacts of AI technologies while also expressing greater confidence in their
abilities to engage in discussions about AI regulation. Finally, we highlight
the AI Regulation Assignment as an effective and engaging tool for exploring
the limits of AI alignment and emphasizing the role of 'policy' in addressing
ethical challenges.

</details>


### [70] [Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement](https://arxiv.org/abs/2506.15647)
*Weixiang Zhao, Jiahe Guo, Yang Deng, Xingyu Sui, Yulin Hu, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu*

**主要类别:** cs.AI

**AI概要:** 本文探讨了大型推理模型（LRMs）产生冗余内容的问题，并提出了两种轻量级方法来提高这些模型的效率，同时保持或提升任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型在解决复杂问题上取得了显著进展，但它们往往表现出过度思考的行为，导致效率降低和推理成本增加。研究旨在揭示这种低效的表现和行为根源，并发现模型具有更简洁推理的能力。

**方法:** 1. 提出了Efficiency Steering，一种无需训练的激活导向技术，通过模型表示空间中的单个方向来调节推理行为。
2. 开发了Self-Rewarded Efficiency RL，一个强化学习框架，能够通过奖励简洁正确的解决方案来动态平衡任务准确性和简洁性。

**结果:** 实验表明所提出的方法能够在多个数学推理基准测试中显著减少推理长度，同时保持或改善任务表现。

**结论:** 研究表明，通过利用并引导现有模型内在能力的方式，可以提高推理效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+and+Exploiting+the+Inherent+Efficiency+within+Large+Reasoning+Models+for+Self-Guided+Efficiency+Enhancement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15647&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large reasoning models (LRMs) have significantly
enhanced language models' capabilities in complex problem-solving by emulating
human-like deliberative thinking. However, these models often exhibit
overthinking (i.e., the generation of unnecessarily verbose and redundant
content), which hinders efficiency and inflates inference cost. In this work,
we explore the representational and behavioral origins of this inefficiency,
revealing that LRMs inherently possess the capacity for more concise reasoning.
Empirical analyses show that correct reasoning paths vary significantly in
length, and the shortest correct responses often suffice, indicating untapped
efficiency potential. Exploiting these findings, we propose two lightweight
methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a
training-free activation steering technique that modulates reasoning behavior
via a single direction in the model's representation space. Second, we develop
Self-Rewarded Efficiency RL, a reinforcement learning framework that
dynamically balances task accuracy and brevity by rewarding concise correct
solutions. Extensive experiments on seven LRM backbones across multiple
mathematical reasoning benchmarks demonstrate that our methods significantly
reduce reasoning length while preserving or improving task performance. Our
results highlight that reasoning efficiency can be improved by leveraging and
guiding the intrinsic capabilities of existing models in a self-guided manner.

</details>


### [71] [SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence](https://arxiv.org/abs/2506.15672)
*Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为SwarmAgentic的完全自动化代理系统生成框架，该框架能够从零开始构建代理系统，并通过语言驱动探索共同优化代理功能和协作。在六个真实世界任务上进行了评估，结果显示相比其他基准方法，SwarmAgentic显著提高了性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的代理系统生成框架缺乏完全自主性，缺少从零开始的代理生成、自我优化的代理功能以及协作能力，这限制了其适应性和可扩展性。

**方法:** SwarmAgentic框架采用群体智能与全自动化多代理生成相结合的方法，维持候选系统的群体并通过反馈引导更新来进化这些系统，借鉴了粒子群优化(PSO)的思想。

**结果:** 在涉及高层次规划、系统级协调和创造性推理的六项开放式现实任务中，仅给出任务描述和目标函数，SwarmAgentic就超过了所有基线，在TravelPlanner基准测试中相对于ADAS实现了+261.8%的相对改进。

**结论:** SwarmAgentic框架代表了朝向可扩展和自主代理系统设计迈出的重要一步，它将群体智能与全自动化系统多代理生成联系起来。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwarmAgentic%3A+Towards+Fully+Automated+Agentic+System+Generation+via+Swarm+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15672&send_immediately=true&force_search=false)

**原文摘要:** The rapid progress of Large Language Models has advanced agentic systems in
decision-making, coordination, and task execution. Yet, existing agentic system
generation frameworks lack full autonomy, missing from-scratch agent
generation, self-optimizing agent functionality, and collaboration, limiting
adaptability and scalability. We propose SwarmAgentic, a framework for fully
automated agentic system generation that constructs agentic systems from
scratch and jointly optimizes agent functionality and collaboration as
interdependent components through language-driven exploration. To enable
efficient search over system-level structures, SwarmAgentic maintains a
population of candidate systems and evolves them via feedback-guided updates,
drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our
method on six real-world, open-ended, and exploratory tasks involving
high-level planning, system-level coordination, and creative reasoning. Given
only a task description and an objective function, SwarmAgentic outperforms all
baselines, achieving a +261.8% relative improvement over ADAS on the
TravelPlanner benchmark, highlighting the effectiveness of full automation in
structurally unconstrained tasks. This framework marks a significant step
toward scalable and autonomous agentic system design, bridging swarm
intelligence with fully automated system multi-agent generation. Our code is
publicly released at https://yaoz720.github.io/SwarmAgentic/.

</details>


### [72] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang*

**主要类别:** cs.AI

**AI概要:** 本文提出了具身网络智能体，一种新型的人工智能范式，旨在融合物理交互和大规模网络推理。为此，作者们开发了一个统一的模拟平台，集成了现实的3D环境与功能性的网页界面，并基于此构建了具身网络智能体基准测试，涵盖烹饪、导航、购物、旅游和地理定位等任务。实验结果揭示了当前AI系统与人类能力之间的显著差距。


<details>
  <summary>更多</summary>
  
**动机:** 目前的人工智能代理大多被局限在单独处理大量数字信息或通过实体感知、规划和行动与物理世界互动，很少能够同时做到两者。这种分离限制了它们解决需要整合物理和数字智能的任务的能力。

**方法:** 为了实现这一概念，研究人员首先开发了一种名为Embodied Web Agents task environments的统一仿真平台，该平台紧密地将逼真的3D室内和室外环境与功能性网络接口结合在一起。基于这个平台，他们构建并发布了Embodied Web Agents Benchmark，包括一系列多样的任务如烹饪、导航、购物、旅游以及地理位置识别——所有这些都需要跨物理和数字领域进行协调推理。

**结果:** 实验结果显示，现有的最先进AI系统与人类能力之间存在明显的表现差距，这为具身认知与大规模知识访问交叉领域的研究提供了挑战与机遇。

**结论:** Embodied Web Agents提供了一种新的方法来连接物理体现和网络规模的推理，通过创建一个综合的模拟平台和基准测试，它展示了现有AI技术与人类表现之间的鸿沟，指出了未来研究的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embodied+Web+Agents%3A+Bridging+Physical-Digital+Realms+for+Integrated+Agent+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15677&send_immediately=true&force_search=false)

**原文摘要:** AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [73] [Optimal Convergence Rates of Deep Neural Network Classifiers](https://arxiv.org/abs/2506.14899)
*Zihan Zhang, Lei Shi, Ding-Xuan Zhou*

**主要类别:** stat.ML

**AI概要:** 本文研究了在Tsybakov噪声条件和组合假设下的二分类问题，证明了分类器的最优收敛率，并展示了ReLU深度神经网络可以达到接近最优的收敛率。


<details>
  <summary>更多</summary>
  
**动机:** 研究者想要探索在Tsybakov噪声条件下，当数据分布满足组合假设时，二分类问题中分类器的最优收敛速度，并验证ReLU深度神经网络是否能够实现这一最优收敛速度。

**方法:** 研究采用理论分析的方法，定义了组合假设并据此推导出在给定条件下分类器的最优收敛速度。同时，通过理论证明表明，使用铰链损失训练的ReLU深层神经网络能达到接近于所推导出的最优收敛速度。

**结果:** 研究表明，在特定条件下，分类器的最优收敛率为(1/n)^(β·(1∧β)^q/(d_*/(s+1)+(1+1/(s+1))·β·(1∧β)^q))，并且ReLU DNNs训练的模型可以达到这个最优收敛率（忽略对数因子）。

**结论:** 本研究为ReLU DNNs在实际分类任务中的出色表现提供了理论依据，尤其是在高维设置下。此外，用于建立这些结果的技术扩展了之前工作中的oracle不等式，该方法本身具有独立的研究价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Convergence+Rates+of+Deep+Neural+Network+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14899&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we study the binary classification problem on $[0,1]^d$ under
the Tsybakov noise condition (with exponent $s \in [0,\infty]$) and the
compositional assumption. This assumption requires the conditional class
probability function of the data distribution to be the composition of $q+1$
vector-valued multivariate functions, where each component function is either a
maximum value function or a H\"{o}lder-$\beta$ smooth function that depends
only on $d_*$ of its input variables. Notably, $d_*$ can be significantly
smaller than the input dimension $d$. We prove that, under these conditions,
the optimal convergence rate for the excess 0-1 risk of classifiers is $$
\left( \frac{1}{n}
\right)^{\frac{\beta\cdot(1\wedge\beta)^q}{{\frac{d_*}{s+1}+(1+\frac{1}{s+1})\cdot\beta\cdot(1\wedge\beta)^q}}}\;\;\;,
$$ which is independent of the input dimension $d$. Additionally, we
demonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can
achieve this optimal convergence rate up to a logarithmic factor. This result
provides theoretical justification for the excellent performance of ReLU DNNs
in practical classification tasks, particularly in high-dimensional settings.
The technique used to establish these results extends the oracle inequality
presented in our previous work. The generalized approach is of independent
interest.

</details>


### [74] [Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond](https://arxiv.org/abs/2506.14950)
*Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska*

**主要类别:** stat.ML

**AI概要:** 提出了一种新的两阶段条件矩限制(CMR)估计器DML-CMR，该估计器在使用深度神经网络(DNN)进行因果推断时能够提供无偏估计，并具有快速收敛速度。


<details>
  <summary>更多</summary>
  
**动机:** 解决条件矩限制(CMRs)是统计学、因果推理和计量经济学中的一个关键问题，许多因果推理技术，如工具变量(IV)回归和近端因果学习(PCL)，都是CMR问题。现有的CMR估计器通常采用两阶段方法，但直接将第一阶段的估计结果代入第二阶段会导致严重的偏差，尤其是在两个阶段都使用深度神经网络(DNN)估计器的情况下，正则化和过拟合偏差尤为明显。

**方法:** 基于双/去偏差机器学习(DML)框架，提出了DML-CMR算法。设计了一个新颖的学习目标来减少偏差，并保证了快速的收敛率。

**结果:** 证明了DML-CMR估计器在参数化及温和的正则条件下可以达到最小最大最优收敛率$O(N^{-1/2})$。通过实际数据集上的IV回归和近端因果学习等应用，展示了DML-CMR相对于现有CMR估计器和针对这些问题定制的算法达到了最先进性能。

**结论:** DML-CMR是一种有效的两阶段CMR估计方法，它解决了传统CMR估计器中因直接代入第一阶段估计而导致的偏差问题，尤其适用于使用深度神经网络的情形，并且在真实世界的数据集中表现出了优秀的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Double+Machine+Learning+for+Conditional+Moment+Restrictions%3A+IV+regression%2C+Proximal+Causal+Learning+and+Beyond，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14950&send_immediately=true&force_search=false)

**原文摘要:** Solving conditional moment restrictions (CMRs) is a key problem considered in
statistics, causal inference, and econometrics, where the aim is to solve for a
function of interest that satisfies some conditional moment equalities.
Specifically, many techniques for causal inference, such as instrumental
variable (IV) regression and proximal causal learning (PCL), are CMR problems.
Most CMR estimators use a two-stage approach, where the first-stage estimation
is directly plugged into the second stage to estimate the function of interest.
However, naively plugging in the first-stage estimator can cause heavy bias in
the second stage. This is particularly the case for recently proposed CMR
estimators that use deep neural network (DNN) estimators for both stages, where
regularisation and overfitting bias is present. We propose DML-CMR, a two-stage
CMR estimator that provides an unbiased estimate with fast convergence rate
guarantees. We derive a novel learning objective to reduce bias and develop the
DML-CMR algorithm following the double/debiased machine learning (DML)
framework. We show that our DML-CMR estimator can achieve the minimax optimal
convergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity
conditions, where $N$ is the sample size. We apply DML-CMR to a range of
problems using DNN estimators, including IV regression and proximal causal
learning on real-world datasets, demonstrating state-of-the-art performance
against existing CMR estimators and algorithms tailored to those problems.

</details>


### [75] [An Observation on Lloyd's k-Means Algorithm in High Dimensions](https://arxiv.org/abs/2506.14952)
*David Silva-Sánchez, Roy R. Lederman*

**主要类别:** stat.ML

**AI概要:** 本文研究了k-means算法在高维、高噪声和小样本量情况下的表现，通过简单的高斯混合模型（GMM）解释了k-means失败的理论原因，并识别出数据几乎每个分区都成为k-means算法固定点的情况。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决更复杂情形下的分析难题，如掩蔽GMMs，以及冷冻电镜应用中出现的问题。

**方法:** 使用简单的高斯混合模型（GMM）来模拟数据，并且通过理论分析确定了k-means算法在特定条件下可能失效的概率性条件。

**结果:** 研究结果表明，在某些条件下，k-means算法有很高的概率将几乎所有数据分区视为一个稳定点，这揭示了k-means在高维度、高噪声及有限样本大小的情况下为何会表现不佳。

**结论:** 结论是k-means算法在高维设置下对于高噪声和小样本量的数据集容易陷入局部最优解，从而导致聚类效果差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Observation+on+Lloyd%27s+k-Means+Algorithm+in+High+Dimensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14952&send_immediately=true&force_search=false)

**原文摘要:** Clustering and estimating cluster means are core problems in statistics and
machine learning, with k-means and Expectation Maximization (EM) being two
widely used algorithms. In this work, we provide a theoretical explanation for
the failure of k-means in high-dimensional settings with high noise and limited
sample sizes, using a simple Gaussian Mixture Model (GMM). We identify regimes
where, with high probability, almost every partition of the data becomes a
fixed point of the k-means algorithm. This study is motivated by challenges in
the analysis of more complex cases, such as masked GMMs, and those arising from
applications in Cryo-Electron Microscopy.

</details>


### [76] [Performative Validity of Recourse Explanations](https://arxiv.org/abs/2506.15366)
*Gunnar König, Hidde Fokkema, Timo Freiesleben, Celestine Mendler-Dünner, Ulrike on Luxburg*

**主要类别:** stat.ML

**AI概要:** 当许多申请者根据算法决策系统的补救解释采取行动时，他们的集体行为可能会改变数据中的统计规律，并且在模型重新拟合后也会改变决策边界。这可能导致补救建议失效。研究指出了在何种条件下补救解释能够保持有效性，并警告不要使用标准反事实解释和因果补救方法，而是提倡仅基于因果变量提出行动建议的补救方法。


<details>
  <summary>更多</summary>
  
**动机:** 补救解释为被算法决策系统拒绝的申请者提供了可操作的建议，以改变他们的输入特征从而获得正面评价。然而，一个关键但被忽视的现象是，当大量申请者根据这些建议采取行动时，他们的集体行为可能改变数据中的统计规律，进而改变决策边界。因此，补救算法可能会使其自身的建议失效。

**方法:** 本文正式描述了在表现性（performativity）下补救解释保持有效的条件。作者分析了补救措施可能因受到非因果变量的影响或干预而非因果变量变得无效的情况。

**结果:** 研究发现，在某些情况下，如果补救措施受非因果变量影响或干预非因果变量，则这些补救措施可能变得无效。

**结论:** 基于上述分析，作者反对使用标准的反事实解释和因果补救方法，转而提倡只针对因果变量推荐行动的补救方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performative+Validity+of+Recourse+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15366，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15366&send_immediately=true&force_search=false)

**原文摘要:** When applicants get rejected by an algorithmic decision system, recourse
explanations provide actionable suggestions for how to change their input
features to get a positive evaluation. A crucial yet overlooked phenomenon is
that recourse explanations are performative: When many applicants act according
to their recommendations, their collective behavior may change statistical
regularities in the data and, once the model is refitted, also the decision
boundary. Consequently, the recourse algorithm may render its own
recommendations invalid, such that applicants who make the effort of
implementing their recommendations may be rejected again when they reapply. In
this work, we formally characterize the conditions under which recourse
explanations remain valid under performativity. A key finding is that recourse
actions may become invalid if they are influenced by or if they intervene on
non-causal variables. Based on our analysis, we caution against the use of
standard counterfactual explanations and causal recourse methods, and instead
advocate for recourse methods that recommend actions exclusively on causal
variables.

</details>


### [77] [Time-dependent density estimation using binary classifiers](https://arxiv.org/abs/2506.15505)
*Agnimitra Dasgupta, Javier Murgoitio-Esandi, Ali Fardisi, Assad A Oberai*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种数据驱动的方法，通过训练一个基于对比估计的时间依赖二分类器来学习多变量随机过程的时间依赖概率密度。该方法可以显式建模时间依赖的概率分布，并且能够生成复杂、时变的多模态和接近退化的密度函数。此外，它还能够在无监督异常检测中有效应用，并可靠地检测出真实世界数据中的罕见事件。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于开发一种能够从样本路径中学习多变量随机过程随时间变化的概率密度的方法，同时假定初始概率密度是已知并可评估的。

**方法:** 提出的方法使用了新颖的时间依赖二分类器，该分类器通过基于对比估计的目标进行训练，以区分随机过程在两个相邻时间点的实现。该方法明确建模了时间依赖的概率分布，允许获取感兴趣时间段内的概率密度值。对于需要训练的系统，利用随机插值法生成样本路径；然后采用基于梯度的马尔科夫链蒙特卡洛方法生成新的样本。

**结果:** 所提方法能够准确重构复杂的、时变的多模态及接近退化的密度函数，有效地扩展到中等高维问题上，并且可靠地识别出现实世界数据中的罕见事件。

**结论:** 本研究成功地开发了一种用于学习多变量随机过程时间依赖概率密度的数据驱动方法，该方法不仅能够生成样本，还能在无监督异常检测等应用中发挥作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-dependent+density+estimation+using+binary+classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15505&send_immediately=true&force_search=false)

**原文摘要:** We propose a data-driven method to learn the time-dependent probability
density of a multivariate stochastic process from sample paths, assuming that
the initial probability density is known and can be evaluated. Our method uses
a novel time-dependent binary classifier trained using a contrastive
estimation-based objective that trains the classifier to discriminate between
realizations of the stochastic process at two nearby time instants.
Significantly, the proposed method explicitly models the time-dependent
probability distribution, which means that it is possible to obtain the value
of the probability density within the time horizon of interest. Additionally,
the input before the final activation in the time-dependent classifier is a
second-order approximation to the partial derivative, with respect to time, of
the logarithm of the density. We apply the proposed approach to approximate the
time-dependent probability density functions for systems driven by stochastic
excitations. We also use the proposed approach to synthesize new samples of a
random vector from a given set of its realizations. In such applications, we
generate sample paths necessary for training using stochastic interpolants.
Subsequently, new samples are generated using gradient-based Markov chain Monte
Carlo methods because automatic differentiation can efficiently provide the
necessary gradient. Further, we demonstrate the utility of an explicit
approximation to the time-dependent probability density function through
applications in unsupervised outlier detection. Through several numerical
experiments, we show that the proposed method accurately reconstructs complex
time-dependent, multi-modal, and near-degenerate densities, scales effectively
to moderately high-dimensional problems, and reliably detects rare events among
real-world data.

</details>


### [78] [Revisiting Randomization in Greedy Model Search](https://arxiv.org/abs/2506.15643)
*Xin Chen, Jason M. Klusowski, Yan Shuo Tan, Chang Yu*

**主要类别:** stat.ML

**AI概要:** 本文提出并分析了一种基于特征子采样的贪婪前向选择估计器的集成方法，该方法在稀疏线性回归中使用。通过动态规划的新颖实现提高了计算效率，并且通过数值实验表明该方法在多种设置下优于lasso和弹性网络等流行方法。此外，研究发现随机集成不仅可以减少训练误差还可以降低自由度，从而改变了基础估计器的偏差-方差权衡曲线。在正交特征的情况下，集成估计器用两参数逻辑权重重新调整普通最小二乘系数，从而扩大了模型搜索空间。


<details>
  <summary>更多</summary>
  
**动机:** 将随机估计器组合成集成（如随机森林）已成为现代数据科学中的基本技术，但可能会非常耗费计算资源。此外，这种技术如何提高预测性能的机制尚未得到充分理解。

**方法:** 提出了一个集成贪婪前向选择估计器的方法，在每一步迭代中从随机特征子集中选择最佳特征。设计了一种基于动态规划的新颖实现以大幅提高其计算效率。

**结果:** 通过细致的数值实验表明，所提出的方法可以在广泛的情景下超越诸如lasso和弹性网络之类的流行方法。此外，与普遍认为随机集成类似于收缩的观点相反，研究表明它能够同时减少训练误差和自由度，从而移动基础估计器的整体偏差-方差权衡曲线。

**结论:** 这些结果增强了我们对随机森林的理解，并暗示一般而言隐式正则化可能比显式正则化具有更复杂的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Randomization+in+Greedy+Model+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15643&send_immediately=true&force_search=false)

**原文摘要:** Combining randomized estimators in an ensemble, such as via random forests,
has become a fundamental technique in modern data science, but can be
computationally expensive. Furthermore, the mechanism by which this improves
predictive performance is poorly understood. We address these issues in the
context of sparse linear regression by proposing and analyzing an ensemble of
greedy forward selection estimators that are randomized by feature subsampling
-- at each iteration, the best feature is selected from within a random subset.
We design a novel implementation based on dynamic programming that greatly
improves its computational efficiency. Furthermore, we show via careful
numerical experiments that our method can outperform popular methods such as
lasso and elastic net across a wide range of settings. Next, contrary to
prevailing belief that randomized ensembling is analogous to shrinkage, we show
via numerical experiments that it can simultaneously reduce training error and
degrees of freedom, thereby shifting the entire bias-variance trade-off curve
of the base estimator. We prove this fact rigorously in the setting of
orthogonal features, in which case, the ensemble estimator rescales the
ordinary least squares coefficients with a two-parameter family of logistic
weights, thereby enlarging the model search space. These results enhance our
understanding of random forests and suggest that implicit regularization in
general may have more complicated effects than explicit regularization.

</details>
