<div id=toc></div>

# 目录

- [cs.CL](#cs.CL) [总数: 45]
- [cs.CV](#cs.CV) [总数: 59]
- [cs.AI](#cs.AI) [总数: 20]
- [cs.LG](#cs.LG) [总数: 87]
- [cs.CR](#cs.CR) [总数: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar, Anup Shakya*

Main category: cs.CL

TL;DR: This project focuses on an important sub-task in language modeling, context embedding. It proposes an approach using Graph Convolution operation in GNNs with LSTMs to predict the next word given a local context of preceding words. Experiments on a custom Wikipedia text corpus demonstrate its effectiveness with limited resources.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address an important sub-task in language modeling, context embedding, by proposing a more cost-effective method without building massive models or requiring immense computation resources.

Method: The method involves using Graph Convolution operation in GNNs to encode the context and combining it with LSTMs to predict the next word based on a local context of preceding words.

Result: The approach works fairly well in predicting the next word when tested on a custom Wikipedia text corpus.

Conclusion: This project demonstrates the potential of using Graph Convolution operations in GNNs combined with LSTMs for effective context embedding in language modeling, even with limited resources.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Next+Word+Suggestion+using+Graph+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09649&send_immediately=true&force_search=false)

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [2] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi*

Main category: cs.CL

TL;DR: This paper proposes a Diversity-aware Reward Adjustment (DRA) method that enhances the exploration capability of reinforcement learning models for language model post-training, especially in low-resource settings. DRA improves the performance on mathematical reasoning tasks by adjusting rewards based on semantic diversity.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of existing methods like GRPO which fail to capture semantic diversity among sampled completions, leading to a diversity-quality inconsistency.

Method: Introducing Diversity-aware Reward Adjustment (DRA) that uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones, integrating seamlessly with GRPO and its variant DR.

Result: DRA improves the performance on five mathematical reasoning benchmarks, achieving state-of-the-art results with an average accuracy of 58.2% using limited resources.

Conclusion: The proposed DRA method effectively addresses the diversity-quality inconsistency issue and can be applied to enhance reinforcement learning models in low-resource scenarios.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DRA-GRPO%3A+Exploring+Diversity-Aware+Reward+Adjustment+for+R1-Zero-Like+Training+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09655&send_immediately=true&force_search=false)

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [3] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger*

Main category: cs.CL

TL;DR: This study compares the persuasive abilities of a large language model (Claude Sonnet 3.5) with incentivized human persuaders in an interactive, real-time conversational quiz setting. Results show that the LLM outperforms humans in both truthful and deceptive persuasion contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI's persuasion capabilities surpass those of humans in real-money bonus scenarios.

Method: A preregistered, large-scale incentivized experiment involving quiz takers and persuaders (humans or LLMs) attempting to influence quiz takers' answers.

Result: LLM persuaders achieved higher compliance rates than humans in both truthful and deceptive persuasion contexts. They increased quiz takers' accuracy and earnings when guiding towards correct answers but decreased accuracy and earnings when steering towards incorrect answers.

Conclusion: AI's persuasive capabilities already exceed those of humans with real-money incentives. This highlights the need for new alignment and governance frameworks for increasingly capable AI persuaders.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+Are+More+Persuasive+Than+Incentivized+Human+Persuaders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09662&send_immediately=true&force_search=false)

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [4] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi, Jinheon Baek, Sung Ju Hwang*

Main category: cs.CL

TL;DR: This paper introduces a novel problem called bilevel system prompt optimization, which aims to design robust and transferable system prompts for large language models. A meta-learning framework is proposed to optimize system prompts across multiple datasets while updating user prompts iteratively.


<details>
  <summary>Details</summary>
Motivation: Existing work on prompt optimization has focused on user prompts, but this paper emphasizes the importance of system prompts that can be applied across different tasks and domains.

Method: A meta-learning framework is proposed to optimize system prompts by considering various user prompts across multiple datasets and updating user prompts iteratively.

Result: Experiments on 14 unseen datasets from 5 different domains show that the approach produces system prompts that generalize well to diverse user prompts and enables rapid adaptation to unseen tasks.

Conclusion: The proposed method demonstrates the effectiveness of bilevel system prompt optimization in improving the performance of large language models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System+Prompt+Optimization+with+Meta-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09666，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09666&send_immediately=true&force_search=false)

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [5] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu, Lechen Zhang, Sheza Munir, Yiyang Gu, Lu Wang*

Main category: cs.CL

TL;DR: Introduce VeriFact for better factuality evaluation of large language models and create FactRBench for assessing both precision and recall.


<details>
  <summary>Details</summary>
Motivation: Evaluating the factuality of large language models is challenging due to complex inter-sentence dependencies in generated facts. Existing methods often fail to capture necessary context and miss key relational facts.

Method: Introducing VeriFact, a factuality evaluation framework focusing on identifying and resolving incomplete and missing facts.

Result: VeriFact enhances fact extraction and supports more accurate verification results. FactRBench evaluates both precision and recall in long-form model responses, providing reference fact sets from advanced LLMs and human-written answers.

Conclusion: VeriFact significantly improves fact completeness and preserves critical relational information, offering more precise factuality evaluation.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VeriFact%3A+Enhancing+Long-Form+Factuality+Evaluation+with+Refined+Fact+Extraction+and+Reference+Facts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09701&send_immediately=true&force_search=false)

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [6] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: 本文介绍了一种利用大型语言模型(LLMs)进行文本分析的分步教程，展示了如何通过迭代和协作过程开发和应用分类法，分析非结构化数据，并讨论了这种方法的潜力与限制。


<details>
  <summary>Details</summary>
Motivation: 分析文本（如开放式回应、标题或社交媒体帖子）是一个耗时且容易产生偏见的过程。大型语言模型（LLMs）是很有前景的工具，可以用预定义的（自上而下的）或数据驱动的（自下而上的）分类法进行文本分析，而不会牺牲质量。

Method: 提供了一个逐步教程，展示如何通过研究人员和LLMs之间的迭代和协作过程，有效地开发、测试和应用用于分析非结构化数据的分类法。使用参与者提供的个人目标作为示例，展示了如何编写提示来审查数据集并生成生活领域的分类法，通过提示和直接修改评估和优化分类法，测试分类法并评估编码者间的一致性，以及应用分类法对整个数据集进行分类，具有高编码者间可靠性。

Result: 演示了如何使用LLMs对非结构化数据进行分类，并在个人目标的例子中实现了高编码者间一致性。

Conclusion: 讨论了使用LLMs进行文本分析的可能性和局限性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+AI-Powered+Research+Assistant+in+the+Lab%3A+A+Practical+Guide+for+Text+Analysis+Through+Iterative+Collaboration+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09724&send_immediately=true&force_search=false)

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [7] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak, Vinayak Pahalwan, Adithya Kamath, Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: This paper presents Tokenadapt, a model-agnostic tokenizer transplantation method that improves efficiency and performance of pretrained language models.


<details>
  <summary>Details</summary>
Motivation: Traditional tokenization schemes limit the efficiency and performance of pretrained language models, especially in multilingual or specialized applications.

Method: Tokenadapt introduces a hybrid heuristic for initializing new token embeddings and proposes pre-tokenization learning for multi-word Supertokens.

Result: Empirical studies show Tokenadapt outperforms other methods like Transtokenizer and ReTok, achieving better compression and lower perplexity.

Conclusion: Tokenadapt effectively addresses the limitations of traditional tokenization approaches with minimal retraining needs.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Achieving+Tokenizer+Flexibility+in+Language+Models+through+Heuristic+Adaptation+and+Supertoken+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09738&send_immediately=true&force_search=false)

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [8] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova, J. M. Auñón, A. Mártinez-Pérez, M. E. Pérez-Martínez, M. E. Gas-López*

Main category: cs.CL

TL;DR: This study explores the use of NLP techniques, specifically Named Entity Recognition (NER), to automatically identify and extract key clinical information from electronic health records (EHRs) related to lung and breast cancer.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of information from clinical reports is time-consuming and error-prone, limiting the efficiency of data-driven approaches in healthcare. The study focuses on lung and breast cancer due to their high incidence and significant impact on public health.

Method: The study utilized GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained in Spanish, to perform NER on a dataset from Health Research Institute Hospital La Fe.

Result: The results show strong overall performance in identifying entities like MET and PAT, although challenges remain with less frequent entities like EVOL.

Conclusion: NLP techniques can enhance the accuracy and efficiency of data extraction from EHRs for lung and breast cancer.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Detection+of+Clinical+Entities+in+Lung+and+Breast+Cancer+Reports+Using+NLP+Techniques，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09794&send_immediately=true&force_search=false)

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [9] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov, David Martens*

Main category: cs.CL

TL;DR: This work explores the generalization of truth direction in LLMs across conversational formats and proposes a method using a fixed key phrase to enhance lie detection reliability.


<details>
  <summary>Details</summary>
Motivation: To investigate how the truth direction in LLMs generalizes between different conversational formats and to improve the generalization ability of lie detectors.

Method: Exploration of the generalization of the truth direction between various conversational formats and proposing a solution involving adding a fixed key phrase at the end of each conversation.

Result: Good generalization between short conversations ending on a lie, but poor generalization to longer formats where the lie appears earlier. A proposed solution improves this type of generalization.

Conclusion: The study concludes that while there is a universal truth direction in LLMs allowing linear separability of true and false statements, reliable lie detectors that generalize across different conversational formats still face significant challenges.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+the+generalization+of+LLM+truth+directions+on+conversational+formats，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09807&send_immediately=true&force_search=false)

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [10] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 提出首个针对解读推理的细读基准KRISTEVA，发现最先进的大型语言模型在细读能力上虽有一定水平，但仍逊色于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 虽然细读被认为是批判性思维的基础，并广泛作为大学课程的必要组成部分被采用，但大型语言模型从未被评估过其细读能力，且多学科基准如MMLU也不包括文学作为主题。

Method: 提出了KRISTEVA，这是一个新的细读基准，包含1331个多选题，并设计了三个逐步增加难度的任务集来模拟不同的细读过程元素。

Result: 先进的大型语言模型在理解与推理文学作品方面展现出了不同程度的能力，但在10个任务中表现不如人类评估者。

Conclusion: 尽管最先进的大型语言模型在某些大学级别的细读能力方面表现出一定的准确性（49.7%-69.7%），但在我们11个任务中的10个任务上，它们的表现仍落后于有经验的人类评估者。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KRISTEVA%3A+Close+Reading+as+a+Novel+Task+for+Benchmarking+Interpretive+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09825&send_immediately=true&force_search=false)

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [11] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova, Sarath Chandra Lingareddy, Sagnik Ray Choudhury, Mark V. Albert*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) were investigated for their ability to predict violent conflict escalation and fatalities without external data. The parametric knowledge of LLMs was compared with non-parametric capabilities which use structured and unstructured context from conflict datasets and recent news reports. A two-part evaluation framework was used to evaluate the models' performance.


<details>
  <summary>Details</summary>
Motivation: The study aimed to explore whether LLMs can be useful for early warning systems, humanitarian planning, and policy-making by predicting conflict escalation and fatalities.

Method: The parametric knowledge of LLMs was compared with non-parametric capabilities using Retrieval-Augmented Generation (RAG). The models were evaluated in two parts: parametric and non-parametric settings.

Result: The study found that LLMs have strengths and limitations for conflict forecasting and that augmenting them with structured external knowledge enhances their performance.

Conclusion: LLMs show potential for conflict forecasting but require augmentation with structured external knowledge for better performance.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Large+Language+Models+Know+Conflict%3F+Investigating+Parametric+vs.+Non-Parametric+Knowledge+of+LLMs+for+Conflict+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09852&send_immediately=true&force_search=false)

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [12] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila, Esteban Villa Turek, Ellen Karina Chumbe Fernandez, Luis Felipe Polo Galvez, Luis Cadavid, Andrea Marroquin, Rebeca Vargas Quesada, Johanna Crew, Nicole Vallejo Galarraga, Christopher Rodriguez, Diego Gutierrez, Radhi Datla*

Main category: cs.CL

TL;DR: This paper explores regional variations of written Spanish across Latin America and Spain, emphasizing their sociocultural and linguistic differences. It argues for the necessity of locale-sensitive AI models to bridge these gaps, promote inclusivity, and support sustainable user growth.


<details>
  <summary>Details</summary>
Motivation: To highlight the importance of regional localized models due to significant differences in the quotidian use of Spanish among dialectal groups.

Method: An in-depth analysis of sociocultural and linguistic contexts of Spanish variants.

Result: Locale-sensitive AI models can play a pivotal role in bridging sociolinguistic dissonances and enhancing localization strategies.

Conclusion: Implementing five sub-variants of Spanish can foster user trust, cultural awareness, and support internationalization strategies.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Crossing+Borders+Without+Crossing+Boundaries%3A+How+Sociolinguistic+Awareness+Can+Optimize+User+Engagement+with+Localized+Spanish+AI+Models+Across+Hispanophone+Countries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09902&send_immediately=true&force_search=false)

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [13] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang*

Main category: cs.CL

TL;DR: 提出了一种新的水印框架，该框架结合了基于logits和基于采样的水印策略的优点，实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，AI生成文本的滥用问题日益严重，水印技术成为一种有前景的解决方案。然而，现有的水印方案在鲁棒性、文本质量和安全性之间存在权衡问题。

Method: 提出了一种新的水印框架，包括串行、并行和混合三种策略，并通过token熵和语义熵自适应地嵌入水印，以优化检测能力、鲁棒性、文本质量和安全性之间的平衡。

Result: 实验表明，所提出的方法在多个数据集和模型上的表现优于现有基线，达到了最先进的性能。

Conclusion: 这项工作为不同的水印范式提供了新的见解，并且代码已经公开可用。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Trade-off+to+Synergy%3A+A+Versatile+Symbiotic+Watermarking+Framework+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09924，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09924&send_immediately=true&force_search=false)

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [14] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao*

Main category: cs.CL

TL;DR: Prompt optimization (PO) is a method for improving large language models' performance without changing model weights. This paper introduces MePO, a new PO approach that uses interpretable design and lightweight training to optimize prompts effectively for both large and small models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing PO methods that rely on advanced LLMs, which can overwhelm lightweight models and degrade response quality.

Method: MePO is a merit-guided, lightweight, and locally deployable prompt optimizer trained on a preference dataset built from merit-aligned prompts generated by a lightweight LLM.

Result: MePO outperforms previous PO methods across diverse tasks and model types, providing a scalable and robust solution for real-world deployment.

Conclusion: MePO offers a practical, efficient, and privacy-conscious way to optimize prompts for various language models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Prompt+Optimizers%3A+From+Prompt+Merits+to+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09930&send_immediately=true&force_search=false)

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [15] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.CL

TL;DR: This case study compares different multitask natural language model variants using a phased hyperparameter optimization process.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of multitask natural language models by optimizing their hyperparameters.

Method: A phased hyperparameter optimization process with short Bayesian optimization sessions leveraging multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance.

Result: The method was demonstrated on variants of the 2021 Joint Entity and Relation Extraction model proposed by Eberts and Ulges.

Conclusion: The phased hyperparameter optimization process can effectively optimize multitask natural language models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interim+Report+on+Human-Guided+Adaptive+Hyperparameter+Optimization+with+Multi-Fidelity+Sprints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09792，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09792&send_immediately=true&force_search=false)

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [16] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad, Chanhee Lee, Dongha Kim, Hokeun Kim*

Main category: cs.CL

TL;DR: This paper proposes an approach using retrieval augmented generation with knowledge graphs to improve the accuracy of large language model's personalized response generation.


<details>
  <summary>Details</summary>
Motivation: LLMs often undergo high levels of over-fitting, resulting in the generation of extra and incorrect data, thus causing hallucinations in output generation. One of the root causes of such problems is the lack of timely, factual, and personalized information fed to the LLM.

Method: introducing retrieval augmented generation (RAG) using knowledge graphs (KGs) to assist the LLM in personalized response generation tailored to the users.

Result: Our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.

Conclusion: Our experimental results show that our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalizing+Large+Language+Models+using+Retrieval+Augmented+Generation+and+Knowledge+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09945&send_immediately=true&force_search=false)

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [17] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin, Fan Huang*

Main category: cs.CL

TL;DR: This paper introduces DIF (Demographic Implicit Fairness), a new method to benchmark implicit bias in large language models.


<details>
  <summary>Details</summary>
Motivation: Concerns over potential biases in large language models inherited from training data.

Method: Developed a method for calculating DIF by evaluating preexisting LLM logic and math problem datasets with sociodemographic personas.

Result: DIF can statistically validate the presence of implicit bias in LLM behavior.

Conclusion: There is an inverse trend between question answering accuracy and implicit bias.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DIF%3A+A+Framework+for+Benchmarking+and+Verifying+Implicit+Bias+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10013&send_immediately=true&force_search=false)

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [18] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Lei Fang*

Main category: cs.CL

TL;DR: A novel two-stage method named CAFE is introduced to improve multi-document question-answering by progressively reducing the impact of irrelevant documents.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in balancing retrieval precision and recall for long-context inputs in large language models.

Method: Two-stage coarse-to-fine approach: Coarse-grained filtering and fine-grained steering.

Result: Experiments show that CAFE surpasses baselines, improving SubEM by 22.1% and 13.7% over SFT and RAG methods respectively.

Conclusion: CAFE effectively enhances multi-document question-answering capabilities in large language models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAFE%3A+Retrieval+Head-based+Coarse-to-Fine+Information+Seeking+to+Enhance+Multi-Document+QA+Capability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10063&send_immediately=true&force_search=false)

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [19] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are susceptible to jailbreaking due to unfiltered training data. This study reveals a universal jailbreak attack that compromises multiple state-of-the-art models, enabling harmful outputs. Despite responsible disclosure, major LLM providers have shown inadequate responses.


<details>
  <summary>Details</summary>
Motivation: To identify the growing threat of dark LLMs and unethical modifications through jailbreak techniques.

Method: Uncovered a universal jailbreak attack compromising multiple state-of-the-art LLMs.

Result: The attack effectively enables LLMs to answer almost any question and produce harmful outputs upon request.

Conclusion: Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dark+LLMs%3A+The+Growing+Threat+of+Unaligned+AI+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10066&send_immediately=true&force_search=false)

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [20] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah, Francois Meyer*

Main category: cs.CL

TL;DR: This paper investigates linguistic knowledge in pretrained language models (PLMs) for African languages, finding that PLMs adapted for these languages encode more linguistic information than massively multilingual PLMs.


<details>
  <summary>Details</summary>
Motivation: To understand why PLMs for African languages are improving and what linguistic knowledge they encode.

Method: Training layer-wise probes for six typologically diverse African languages and designing control tasks for the MasakhaPOS dataset.

Result: PLMs adapted for African languages encode more linguistic information about target languages than massively multilingual PLMs. Token-level syntactic information is concentrated in middle-to-last layers, while sentence-level semantic information is distributed across all layers. Performance reflects internal knowledge of PLMs rather than probe memorisation.

Conclusion: This study highlights the internal mechanisms underlying the success of strategies like active learning and multilingual adaptation by applying established interpretability techniques to African-language PLMs.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Designing+and+Contextualising+Probes+for+African+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10081&send_immediately=true&force_search=false)

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [21] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber*

Main category: cs.CL

TL;DR: A new benchmark called XRAG is proposed to assess the cross-lingual Retrieval-Augmented Generation (RAG) capabilities of large language models (LLMs). XRAG consists of questions derived from recent news articles requiring external knowledge and covers both monolingual and multilingual retrieval scenarios. The dataset's complex nature is indicated by the performance gap between humans and LLMs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the cross-lingual generation abilities of LLMs in RAG settings where user language differs from retrieval results.

Method: Constructing a novel benchmark named XRAG using a dataset construction pipeline that generates questions needing complex reasoning.

Result: XRAG highlights two challenges in cross-lingual RAG: maintaining response language correctness in monolingual retrieval and reasoning across languages in multilingual retrieval.

Conclusion: XRAG is a useful benchmark for studying LLM reasoning abilities, especially in cross-lingual contexts.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是XRAG%3A+Cross-lingual+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10089&send_immediately=true&force_search=false)

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [22] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan, Di Wu, Yibin Lei, Christof Monz, Iacer Calixto*

Main category: cs.CL

TL;DR: This paper introduces S-MedQA, a new English medical QA dataset, and finds that improvements in medical QA tasks are mainly due to domain shifting rather than knowledge injection.


<details>
  <summary>Details</summary>
Motivation: To benchmark large language models in fine-grained clinical specialties and check the applicability of a popular hypothesis related to knowledge injection.

Method: Using S-MedQA to test the applicability of a popular hypothesis related to knowledge injection in medical QA.

Result: Training on data from a specialty does not necessarily lead to best performance on that specialty and token probabilities of clinically relevant terms for all specialties increase consistently.

Conclusion: We conclude that improvement gains in medical QA tasks come mostly from domain shifting rather than knowledge injection, suggesting the need to reconsider the role of fine-tuning data in the medical domain.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Does+Neuro+Mean+to+Cardio%3F+Investigating+the+Role+of+Clinical+Specialty+Data+in+Medical+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10113，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10113&send_immediately=true&force_search=false)

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [23] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da, Parth Mitesh Shah, Kuan-Ru Liou, Jiaxing Zhang, Hua Wei*

Main category: cs.CL

TL;DR: This paper presents GE-Chat, a knowledge graph enhanced framework for evidence-based response generation in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the issue of unreliable outputs from large language models, particularly hallucinated responses, and improve trustworthiness.

Method: Proposes GE-Chat, which uses a knowledge graph to enhance retrieval-augmented generation, along with chain-of-thought logic generation, n-hop sub-graph searching, and entailment-based sentence generation.

Result: Demonstrates improved performance in identifying exact evidence in free-form contexts, providing a reliable way to examine LLM conclusion resources and judge trustworthiness.

Conclusion: GE-Chat offers a solution to enhance the reliability and trustworthiness of large language model outputs by leveraging knowledge graphs and advanced reasoning techniques.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GE-Chat%3A+A+Graph+Enhanced+RAG+Framework+for+Evidential+Response+Generation+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10143&send_immediately=true&force_search=false)

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [24] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi, Taro Yano, Masafumi Oyamada*

Main category: cs.CL

TL;DR: This study evaluates Reasoning CPT, a form of continual pretraining that uses synthetic data to reconstruct hidden thought processes, revealing its ability to enhance performance across various domains, especially on difficult problems.


<details>
  <summary>Details</summary>
Motivation: To explore how to effectively synthesize training data for reasoning and how such data affect a wide range of domains.

Method: Applying Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and comparing it to standard CPT on the MMLU benchmark.

Result: Reasoning CPT consistently improves performance across all evaluated domains. Reasoning skills acquired in one domain transfer effectively to others. The performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.

Conclusion: Reasoning CPT can improve performance across all evaluated domains, and the performance gap with conventional methods widens as problem difficulty increases.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mining+Hidden+Thoughts+from+Texts%3A+Evaluating+Continual+Pretraining+with+Synthetic+Data+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10182&send_immediately=true&force_search=false)

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [25] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo*

Main category: cs.CL

TL;DR: This paper introduces the CoT Encyclopedia, a new framework for analyzing and steering reasoning in large language models.


<details>
  <summary>Details</summary>
Motivation: To understand the reasoning strategies underlying the capabilities of large language models.

Method: Automatically extract diverse reasoning criteria from model-generated CoTs, embed them into a semantic space, cluster them into representative categories, and derive contrastive rubrics to interpret reasoning behavior.

Result: The framework produces more interpretable and comprehensive analyses than existing methods and enables performance gains by predicting which strategy a model is likely to use and guiding it toward more effective alternatives.

Conclusion: Training data format has a greater impact on reasoning behavior than data domain, highlighting the importance of format-aware model design.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+CoT+Encyclopedia%3A+Analyzing%2C+Predicting%2C+and+Controlling+how+a+Reasoning+Model+will+Think，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10185&send_immediately=true&force_search=false)

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [26] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao, Hongyi Huang, Jiayi Wu, YiMing Cheng, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.CL

TL;DR: VQ-Logits是一种新的方法，通过向量量化显著减少大型语言模型输出层的参数数量和计算负载。实验表明，这种方法在保持较高效率的同时大幅减少了参数并加快了计算速度。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型由于其庞大的输出词汇表而面临巨大的计算和内存挑战，特别是在推理过程中最后的线性投影层占用了大量资源。

Method: VQ-Logits利用向量量化来替换大型输出嵌入矩阵，并使用一个小的共享代码本映射到完整的词汇空间。

Result: 在标准语言建模基准测试中，VQ-Logits实现了高达99%的参数减少和6倍的logit计算加速，同时仅增加了4%的困惑度。

Conclusion: VQ-Logits展示了其在减少大型语言模型输出层参数和计算复杂度方面的有效性和鲁棒性，同时在性能上几乎没有损失。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VQ-Logits%3A+Compressing+the+Output+Bottleneck+of+Large+Language+Models+via+Vector+Quantized+Logits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10202&send_immediately=true&force_search=false)

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [27] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang*

Main category: cs.CL

TL;DR: We introduce RAIDEN-R1, a novel reinforcement learning framework that improves role consistency in conversational agents through verifiable role-awareness reward and advanced reasoning strategies.


<details>
  <summary>Details</summary>
Motivation: Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency.

Method: We propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys.

Result: Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness.

Conclusion: This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAIDEN-R1%3A+Improving+Role-awareness+of+LLMs+via+GRPO+with+Verifiable+Reward，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10218&send_immediately=true&force_search=false)

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [28] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert*

Main category: cs.CL

TL;DR: This study examines five top-tier large language models' capability for zero-shot and few-shot annotation of a complex social media post dataset in Russian and Ukrainian, focusing on binary classification of human rights violation references. Model annotations are compared against human-labeled data in both English and Russian, revealing error patterns and cross-linguistic adaptability.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and applicability of large language models in handling sensitive, domain-specific tasks in multilingual contexts, especially concerning subjective and context-dependent judgments.

Method: Binary classification task using zero-shot and few-shot annotations from GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 on a dataset of Russian and Ukrainian social media posts, compared against human double-annotated labels.

Result: The models' performance varies under different prompting conditions and languages, with unique error patterns and disagreements identified, offering insights into their strengths, limitations, and cross-linguistic capabilities.

Conclusion: This research highlights the potential and challenges of using large language models for sensitive multilingual tasks, emphasizing the need for careful evaluation and adaptation when deploying such models in real-world scenarios.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparing+LLM+Text+Annotation+Skills%3A+A+Study+on+Human+Rights+Violations+in+Social+Media+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10260&send_immediately=true&force_search=false)

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [29] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang, Huitao Li, Matthew Yu Heng Wong, Yuhe Ke, Xin Li, Kunyu Yu, Jingchi Liao, Jonathan Chong Kai Liew, Sabarinath Vinod Nair, Jasmine Chiat Ling Ong, Irene Li, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, Nan Liu*

Main category: cs.CL

TL;DR: 比较生成式大语言模型与传统自然语言处理在医学任务上的差异，并强调其道德使用的必要性


<details>
  <summary>Details</summary>
Motivation: 探索生成式大语言模型与传统自然语言处理在不同医学任务中的差异

Method: 分析了19,123篇研究

Result: 生成式大语言模型在开放性任务上有优势，而传统自然语言处理在信息提取和分析任务上占主导地位

Conclusion: 随着技术进步，确保这些技术在医学应用中的道德使用至关重要

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Evolving+Landscape+of+Generative+Large+Language+Models+and+Traditional+Natural+Language+Processing+in+Medicine，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10261&send_immediately=true&force_search=false)

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [30] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li, Nan Jiang, Kangping Huang, Ruiqi Tu, Shuyu Ouyang, Huayu Yu, Lin Qiao, Chen Yu, Tianshu Zhou, Danyang Tong, Qian Wang, Mengtao Li, Xiaofeng Zeng, Yu Tian, Xinping Tian, Jingsong Li*

Main category: cs.CL

TL;DR: Quicker, an AI-powered clinical decision support system, automates evidence synthesis and generates clinical recommendations, showing strong performance in helping physicians make faster and more reliable decisions.


<details>
  <summary>Details</summary>
Motivation: Integrating clinical evidence into real-time practice is challenging due to the enormous workload, complex professional processes, and time constraints.

Method: This study introduces Quicker, an evidence-based clinical decision support system powered by large language models (LLMs), designed to automate evidence synthesis and generate clinical recommendations modeled after standard clinical guideline development processes.

Result: Quicker-implemented chain covers all phases, from questions to clinical recommendations, and further enables customized decision-making through integrated tools and interactive user interfaces. The Q2CRBench-3 benchmark dataset was developed based on clinical guideline development records for three different diseases.

Conclusion: In general, our findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Questions+to+Clinical+Recommendations%3A+Large+Language+Models+Driving+Evidence-Based+Clinical+Decision+Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10282&send_immediately=true&force_search=false)

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [31] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha*

Main category: cs.CL

TL;DR: J1是一种强化学习方法，用于训练AI判断模型。该方法通过将可验证和不可验证的提示转换为具有可验证奖励的判断任务来提高模型的判断能力，并减少判断偏差。实验表明，J1在多个基准测试中优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 改进AI评估的质量，增强链式思维推理能力，寻找最佳的训练方法。

Method: 将可验证和不可验证的提示转换为具有可验证奖励的判断任务，使用强化学习进行训练。

Result: J1在多个基准测试中优于其他模型，包括从DeepSeek-R1蒸馏出来的模型。

Conclusion: J1通过学习制定评估标准、对比自动生成的参考答案以及重新评估模型响应的正确性，提高了判断质量。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是J1%3A+Incentivizing+Thinking+in+LLM-as-a-Judge+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10320&send_immediately=true&force_search=false)

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [32] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang, Zhanyu Shen, Hui Huang*

Main category: cs.CL

TL;DR: 提出一种低维密集且可解释的文本嵌入模型LDIR，其数值维度表示通过最远点采样与不同锚文本的语义相关性，在多个语义文本相似度、检索和聚类任务上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入方法虽然性能优秀但难以追踪和解释；稀疏可解释的词袋模型表现不佳；基于大语言模型的可解释文本嵌入通常维度较高。

Method: 提出低维（低于500）密集且可解释的文本嵌入模型LDIR，其维度数值通过最远点采样表示与不同锚文本的语义相关性。

Result: 在多个任务上验证了LDIR的性能，接近黑盒基准模型且优于其他可解释嵌入基准模型，且维度更少。

Conclusion: 提出的LDIR模型在保持高性能的同时实现了更低维度和更好的可解释性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LDIR%3A+Low-Dimensional+Dense+and+Interpretable+Text+Embeddings+with+Relative+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10354&send_immediately=true&force_search=false)

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [33] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye, Shaonan Wang*

Main category: cs.CL

TL;DR: This paper proposes a unified framework for reconstructing language from brain activity elicited by different input modalities (visual, auditory, and textual). The approach uses modality-specific experts from visual-language models to decode thoughts and shows comparable performance with adaptability.


<details>
  <summary>Details</summary>
Motivation: Prior studies on decoding thoughts from brain activity focused on single-modal inputs like images or audio, whereas human thought is multimodal. This paper aims to bridge the gap by proposing a multimodal framework for brain activity decoding.

Method: The proposed method uses visual-language models with modality-specific experts to interpret information across visual, auditory, and textual modalities for reconstructing coherent language from brain recordings.

Result: Experiments show that the proposed method achieves performance comparable to state-of-the-art systems while maintaining adaptability and extensibility.

Conclusion: This work contributes to more ecologically valid and generalizable mind decoding by introducing a unified framework for multimodal brain activity decoding.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Coherent+Language+Reconstruction+from+Brain+Recordings+with+Flexible+Multi-Modal+Input+Stimuli，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10356&send_immediately=true&force_search=false)

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [34] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White, Anastasia Shimorina*

Main category: cs.CL

TL;DR: This paper investigates the design of an aspect-based sentiment analysis system using LLMs, focusing on quadruple opinion extraction. It shows that a combined multi-domain model performs as well as specialized single-domain models but with less complexity.


<details>
  <summary>Details</summary>
Motivation: To develop an effective and efficient aspect-based sentiment analysis system using LLMs for real-world applications.

Method: Fine-tuning a single model to handle multiple domain-specific taxonomies simultaneously.

Result: The combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity.

Conclusion: A unified approach using LLMs can be effective for aspect-based sentiment analysis across different domains and languages.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-domain+Multilingual+Sentiment+Analysis+in+Industry%3A+Predicting+Aspect-based+Opinion+Quadruples，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10389&send_immediately=true&force_search=false)

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [35] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li*

Main category: cs.CL

TL;DR: Propose an efficient decoding approach called RPG to alleviate the repetition problems in code generation for LLMs.


<details>
  <summary>Details</summary>
Motivation: The problem of repetitions during the generation process in code generation, especially structural repetition, continues to linger. Previous work mainly focused on content repetition.

Method: An efficient decoding approach called RPG (Repetition Penalization based on Grammar) is proposed to alleviate the repetition problems in code generation for LLMs.

Result: RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.

Conclusion: RPG efficiently reduces structural repetition in code generation for LLMs by leveraging grammar rules.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Repetition+Problems+of+LLMs+in+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10402&send_immediately=true&force_search=false)

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [36] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo, Jae Ho Sohn, Gondy Leroy, Trevor Cohen*

Main category: cs.CL

TL;DR: Large language models (LLMs) show promise in generating plain language summaries (PLSs), but they may not enhance comprehension compared to human-written PLSs. Evaluation methods should go beyond surface quality.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLM-generated PLSs are understandable and comparable to human-written PLSs.

Method: Crowdsourced evaluation with 150 participants using subjective Likert-scale ratings and objective comprehension measures.

Result: LLMs produce PLSs that seem similar to human-written ones in subjective evaluations but lead to poorer comprehension. Automated metrics poorly align with human judgments.

Conclusion: Evaluation frameworks should prioritize comprehension over surface-level quality, and generation methods should focus on optimizing for layperson understanding.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+LLM-generated+plain+language+summaries+truly+understandable%3F+A+large-scale+crowdsourced+evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10409&send_immediately=true&force_search=false)

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [37] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou*

Main category: cs.CL

TL;DR: Proposes LongRefiner, an efficient plug-and-play refiner for long-document processing in RAG applications.


<details>
  <summary>Details</summary>
Motivation: To reduce inference costs and improve performance in long-context input scenarios.

Method: Dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model.

Result: Achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline.

Conclusion: LongRefiner provides practical insights for real-world long-text RAG applications.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Document+Refinement+for+Long-context+Retrieval-augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10413&send_immediately=true&force_search=false)

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [38] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT is a novel reasoning framework for diffusion language models, which optimizes the entire reasoning trajectory with reinforcement learning to improve the correctness of the final answer.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible and effective reasoning method for diffusion language models compared to traditional chain-of-thought methods.

Method: DCoLT treats each intermediate step as a latent 'thinking' action and uses outcome-based reinforcement learning to optimize the reasoning trajectory.

Result: Experiments on math and code generation tasks show that DCoLT-reinforced DLMs outperform other DLMs trained by supervised fine-tuning or reinforcement learning.

Conclusion: DCoLT improves the reasoning accuracy of diffusion language models significantly across different datasets and tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcing+the+Diffusion+Chain+of+Lateral+Thought+with+Diffusion+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10446&send_immediately=true&force_search=false)

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [39] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang, Licheng Zhang, Zheren Fu, Zhendong Mao*

Main category: cs.CL

TL;DR: This paper introduces CL-RAG, a multi-stage Curriculum Learning-based framework for training Retrieval-Augmented Generation (RAG) systems. By constructing training data with varying difficulty levels and training models progressively from easy to difficult tasks, CL-RAG enhances the generalization ability of both retrievers and generators within the RAG system. The approach improves performance by 2%-4% across four open-domain QA datasets compared to several advanced methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of varying document effectiveness in RAG systems and improve the adaptability of retrievers and generators during training, inspired by human cognitive learning principles.

Method: A multi-stage Curriculum Learning-based framework named CL-RAG that constructs training data with different difficulty levels and trains models progressively.

Result: CL-RAG shows consistent effectiveness across four open-domain QA datasets, improving performance by 2%-4% over multiple advanced methods.

Conclusion: The proposed CL-RAG framework enhances the generalization ability of RAG systems, achieving better performance across different datasets.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CL-RAG%3A+Bridging+the+Gap+in+Retrieval-Augmented+Generation+with+Curriculum+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10493，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10493&send_immediately=true&force_search=false)

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [40] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou, Xiao Deng, Yuxiao Luo, Shikun Zhang, Wei Ye*

Main category: cs.CL

TL;DR: CoV-Eval is a multi-task benchmark for evaluating LLM code security, including tasks like code completion, vulnerability repair, detection and classification. VC-Judge is an improved model for reviewing LLM-generated programs for vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Current code security benchmarks lack comprehensive assessment across multiple dimensions of code security.

Method: Proposed CoV-Eval benchmark and developed VC-Judge model.

Result: Most LLMs identify vulnerable codes well but struggle with generating secure codes, recognizing specific vulnerability types and performing repairs.

Conclusion: Comprehensive experiments and analyses offer insights for future research in LLM code security.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+You+Really+Trust+Code+Copilots%3F+Evaluating+Large+Language+Models+from+a+Code+Security+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10494，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10494&send_immediately=true&force_search=false)

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [41] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing, Goran Glavaš*

Main category: cs.CL

TL;DR: This work investigates low-level design decisions in using word aligners for label projection in translation-based cross-lingual transfer for token classification tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of translation-based XLT for token classification tasks by optimizing low-level design decisions in label projection using word aligners.

Method: Systematically investigate the effects of different algorithms for projecting labels between multi-token spans, filtering strategies to reduce noisy mappings, and pre-tokenization of translated sentences.

Result: Optimized word aligner-based label projection performs comparably or better than marker-based methods, and an ensemble method further improves performance while reducing sensitivity to design choices.

Conclusion: Optimizing low-level design decisions in word aligner-based label projection can significantly enhance translation-based XLT for token classification tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Devil+Is+in+the+Word+Alignment+Details%3A+On+Translation-Based+Cross-Lingual+Transfer+for+Token+Classification+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10507&send_immediately=true&force_search=false)

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [42] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis*

Main category: cs.CL

TL;DR: Proposes MuToR, an effective method for multi-token prediction that can be easily integrated into existing language models for better performance in fine-tuning and pretraining.


<details>
  <summary>Details</summary>
Motivation: Existing multi-token prediction methods have not consistently improved performance across different settings like fine-tuning, so there is a need for a more versatile method.

Method: Introducing learnable register tokens into the input sequence and having each token predict future targets.

Result: MuToR shows effectiveness and versatility in various use cases including supervised fine-tuning, parameter-efficient fine-tuning, and pretraining for both language and vision domains.

Conclusion: MuToR is a simple and effective approach for multi-token prediction that can be easily integrated into existing pretrained language models without requiring any architectural changes.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Token+Prediction+Needs+Registers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10518&send_immediately=true&force_search=false)

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [43] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, An Yang, Binyuan Hui, Dayiheng Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Bowen Yu, Jingren Zhou, Junyang Lin*

Main category: cs.CL

TL;DR: World Preference Modeling (WorldPM) is proposed to explore the scaling potential in preference modeling. Extensive experiments demonstrate its effectiveness for preference fine-tuning and generalization performance improvement.


<details>
  <summary>Details</summary>
Motivation: Scaling laws in language modeling inspire the study of similar laws in preference modeling.

Method: Collecting preference data from public forums and conducting extensive training with models of different sizes.

Result: Adversarial metrics scale up, objective metrics show emergent behavior, subjective metrics don't demonstrate scaling trends. WorldPM improves generalization performance across various human preference datasets and enhances in-house and public evaluation sets.

Conclusion: WorldPM demonstrates scalability potential and effectiveness as a foundation for preference fine-tuning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WorldPM%3A+Scaling+Human+Preference+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10527&send_immediately=true&force_search=false)

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [44] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li*

Main category: cs.CL

TL;DR: Meta-ability alignment improves reasoning capabilities of large models effectively.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unpredictability and uncontrollability of emergent reasoning behaviors in large reasoning models.

Method: Using automatically generated, self-verifiable tasks and a three stage-pipeline including individual alignment, parameter-space merging, and domain-specific reinforcement learning.

Result: Performance boost by over 10% relative to instruction-tuned baselines and an additional 2% average gain in performance ceiling across math, coding, and science benchmarks.

Conclusion: Explicit meta-ability alignment offers a scalable and dependable foundation for reasoning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+%27Aha%21%27%3A+Toward+Systematic+Meta-Abilities+Alignment+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10554&send_immediately=true&force_search=false)

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [45] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在癌症信息生成中的能力与限制，发现通用型模型语言质量较高，医学专用模型可及性更强，但均需改进以提高安全性和减少偏见。


<details>
  <summary>Details</summary>
Motivation: 由于公众对乳腺癌和宫颈癌预防、筛查和治疗的理解存在显著差距，可能导致诊断延迟和治疗不足，因此本研究旨在评估大型语言模型在生成准确、安全且可及的癌症相关信息方面的潜力和局限性，以支持患者理解。

Method: 采用混合方法评估框架，评估了五种通用型和三种医学专用型大型语言模型在语言质量、安全性与可信度以及沟通可及性和有效性方面的表现。使用定量指标、专家定性评分以及Welch方差分析、Games-Howell检验和Hedges' g统计分析方法。

Result: 通用型大型语言模型在语言质量和沟通有效性方面表现优异；而医学专用型大型语言模型在沟通可及性方面表现更好。然而，医学专用模型往往表现出更高的潜在危害、毒性及偏见，影响了其安全性和可信度。

Conclusion: 本研究发现大型语言模型在生成癌症相关信息方面表现出一定的能力与局限性。通用型大型语言模型在语言质量和有效性方面表现较好，而医学专用的大型语言模型在沟通可及性上表现更优。然而，医学专用模型存在较高的潜在危害、毒性及偏见，这降低了其在安全性和可信度上的表现。因此，未来需要对模型设计进行有意图的改进，特别是在减少危害和偏见、提升安全性和有效性方面。这项研究为改善AI生成的健康内容提供了重要的见解，并为未来开发准确、安全且可及的数字健康工具提供了指导。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+for+Cancer+Communication%3A+Evaluating+Linguistic+Quality%2C+Safety%2C+and+Accessibility+in+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10472&send_immediately=true&force_search=false)

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales, Ayah Elsayed, Debbie Zhao, Filip Loncaric, Ainhoa Aguado, Mireia Masias, Gina Quill, Marc Ramos, Ada Doltra, Ana Garcia, Marta Sitges, David Marlevi, Alistair Young, Martyn Nash, Bart Bijnens, Oscar Camara*

Main category: cs.CV

TL;DR: This study introduces an open-source computational framework for analyzing 4D Flow MRI of the left atrium, which can provide accurate automated segmentations and assess energy, vorticity, and pressure parameters as potential prognostic biomarkers.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of left atrial hemodynamics and overcome the limitations of conventional ultrasound analysis by using 4D Flow MRI.

Method: Developing an open-source computational framework tailored for the analysis of 4D Flow MRI in the left atrium, which enables comprehensive qualitative and quantitative analysis of advanced hemodynamic parameters.

Result: The framework proves robust to data from different centers of varying quality, producing high-accuracy automated segmentations. It also conducts the first comprehensive assessment of energy, vorticity, and pressure parameters in the left atrium across a spectrum of disorders.

Conclusion: The introduced framework could be used to analyze 4D Flow MRI of the left atrium and assess potential prognostic biomarkers.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Computational+Pipeline+for+Advanced+Analysis+of+4D+Flow+MRI+in+the+Left+Atrium，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09746&send_immediately=true&force_search=false)

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [47] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji*

Main category: cs.CV

TL;DR: This paper introduces Dyadic Mamba, a novel State-Space Model (SSM)-based approach for generating high-quality dyadic human motion from text descriptions of arbitrary length.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods struggle with longer sequences due to limitations in positional encoding schemes.

Method: Dyadic Mamba uses a simple architecture facilitating information flow through concatenation without complex cross-attention mechanisms.

Result: It performs competitively on short-term benchmarks and outperforms transformers on longer sequences.

Conclusion: SSM-based architectures show promise for long-term dyadic human motion synthesis from text.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dyadic+Mamba%3A+Long-term+Dyadic+Human+Motion+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09827，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09827&send_immediately=true&force_search=false)

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [48] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria, Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: Propose BoundarySeg, a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation.


<details>
  <summary>Details</summary>
Motivation: Reduce annotation costs while maintaining segmentation accuracy in low data regimes.

Method: BoundarySeg multi-task framework with organ boundary prediction as an auxiliary task

Result: Performance comparable to or exceeding state-of-the-art semi-supervised approaches without using unannotated data or increasing computational demands

Conclusion: BoundarySeg is simple, effective, and computationally efficient for medical image segmentation.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BoundarySeg%3AAn+Embarrassingly+Simple+Method+To+Boost+Medical+Image+Segmentation+Performance+for+Low+Data+Regimes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09829&send_immediately=true&force_search=false)

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [49] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel*

Main category: cs.CV

TL;DR: This paper introduces a novel text-conditioned diffusion-based method to synthesize high-fidelity surgical videos for under-represented classes, addressing data imbalance issues. The method uses a two-stage process with 2D latent diffusion for spatial content and temporal attention layers for temporal consistency. It also includes a rejection sampling strategy to select suitable synthetic samples.


<details>
  <summary>Details</summary>
Motivation: To overcome the data imbalance issue often found in surgical video datasets, which hinders the development of high-performing models.

Method: A unique two-stage, text-conditioned diffusion-based method is proposed. It uses a 2D latent diffusion model for spatial content and integrates temporal attention layers for temporal consistency. A rejection sampling strategy is introduced to select synthetic samples.

Result: The method improves model performance on downstream tasks like surgical action recognition and intra-operative event prediction when incorporating synthetic videos.

Conclusion: This work demonstrates the effectiveness of using synthesized surgical videos to address data imbalance and enhance model performance.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mission+Balance%3A+Generating+Under-represented+Class+Samples+using+Video+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09858&send_immediately=true&force_search=false)

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [50] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee, Taylor Webb, Trevor Bihl, Keith Holyoak, Hongjing Lu*

Main category: cs.CV

TL;DR: A new model called Probabilistic Schema Induction (PSI) is introduced which employs deep learning to perform analogical mapping over structured representations of examples, forming a compositional concept called a schema.


<details>
  <summary>Details</summary>
Motivation: To model rapid human-like learning of compositional visual concepts

Method: PSI employs a novel conception of similarity that weighs object-level similarity and relational similarity, as well as a mechanism for amplifying relations relevant to classification.

Result: PSI produces human-like learning performance and outperforms two controls.

Conclusion: These findings suggest that structured representations and analogical mapping are critical to modeling rapid human-like learning of compositional visual concepts.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few-Shot+Learning+of+Visual+Compositional+Concepts+through+Probabilistic+Schema+Induction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09859&send_immediately=true&force_search=false)

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [51] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang*

Main category: cs.CV

TL;DR: This paper presents LSG-SLAM, a large-scale 3D Gaussian Splatting-based visual SLAM system using stereo cameras. It introduces a multi-modality strategy for pose estimation under large view changes and uses continuous Gaussian Splatting submaps for scalability in large-scale scenarios. Loop detection and pose optimization are performed between Gaussian Splatting submaps, followed by a structure refinement module to enhance reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in neural radiance fields and 3D Gaussian splatting-based visual SLAM systems for large-scale outdoor scenarios, especially those requiring RGBD sensors.

Method: LSG-SLAM employs a multi-modality strategy for pose estimation under large view changes, feature-alignment warping constraints, continuous Gaussian Splatting submaps for scalability, loop detection through place recognition, and a structure refinement module.

Result: LSG-SLAM achieves superior performance compared to existing neural, 3DGS-based, and traditional approaches when evaluated on the EuRoc and KITTI datasets.

Conclusion: The proposed LSG-SLAM demonstrates strong performance in large-scale outdoor visual SLAM tasks, particularly with stereo cameras.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large-Scale+Gaussian+Splatting+SLAM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09915，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09915&send_immediately=true&force_search=false)

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [52] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang*

Main category: cs.CV

TL;DR: A new method called AdaptCLIP is proposed for universal visual anomaly detection, which improves performance across different domains.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current methods in designing prompts, handling complex token interactions, and requiring additional fine-tuning for visual anomaly detection.

Method: AdaptCLIP uses adaptive visual and textual representations learned alternately and incorporates both contextual and aligned residual features for comparative learning. It adds three simple adapters to CLIP models.

Result: AdaptCLIP shows superior performance on 12 anomaly detection benchmarks from industrial and medical domains compared to existing methods.

Conclusion: AdaptCLIP demonstrates the effectiveness of its approach in universal visual anomaly detection without additional fine-tuning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdaptCLIP%3A+Adapting+CLIP+for+Universal+Visual+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09926，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09926&send_immediately=true&force_search=false)

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [53] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin, Shaolei Liu, Manning Wang*

Main category: cs.CV

TL;DR: This paper proposes a novel source-free domain adaptation (SFDA) framework that introduces preadaptation to generate a preadapted model, a data-dependent frequency prompt for style translation, and a style-related layer fine-tuning strategy to improve the efficiency and quality of model adaptation in medical datasets.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods face challenges such as limited access to labeled source domain data in medical datasets, reliance on domain-specific image style translation and self-supervision techniques with suboptimal results, and inefficiency in training the entire model under limited supervision.

Method: The proposed framework includes preadaptation for generating a preadapted model, a data-dependent frequency prompt for effective style translation, and a style-related layer fine-tuning strategy for efficient adaptation using target domain images and pseudo-labels.

Result: Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks show that the proposed method outperforms existing state-of-the-art methods.

Conclusion: The proposed SFDA framework improves the quality of pseudo-labels and the efficiency of adaptation, demonstrating superior performance compared to current methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DDFP%3A+Data-dependent+Frequency+Prompt+for+Source+Free+Domain+Adaptation+of+Medical+Image+Segmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09927&send_immediately=true&force_search=false)

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [54] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Quoc Dai Tran*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架VRU-CIPI，它利用GRU和Transformer机制来预测城市交叉路口弱势道路使用者的穿越意图，取得了先进的成果，并且通过与I2V通信集成，可以进一步提高交叉路口的安全性。


<details>
  <summary>Details</summary>
Motivation: 理解并在野外预测人类行为，特别是在城市交叉路口，对于提高道路使用者之间的交互安全性至关重要。其中最关键的行为之一是弱势道路使用者（VRUs）的穿越意图，对这些意图的误判可能会导致与迎面而来的车辆发生危险冲突。

Method: 提出了一种名为VRU-CIPI的框架，其中包括一个基于序列注意的模型，用于预测VRU在交叉路口的穿越意图。该模型使用Gated Recurrent Unit (GRU)来捕获VRU移动中的时间动态，并结合多头Transformer自注意力机制来编码上下文和空间依赖性以预测穿越方向。

Result: 在UCF-VRU数据集上的评估显示，所提出的VRU-CIPI框架达到了最先进的性能，准确率为96.45％，并且实现了每秒33帧的实时推理速度。

Conclusion: 本研究提出了一个名为VRU-CIPI的框架，用于预测城市交叉路口弱势道路使用者（VRUs）的穿越意图。该模型使用门控循环单元（GRU）捕捉VRU运动的时间动态，并结合多头Transformer自注意力机制来编码关键的上下文和空间依赖性以预测穿越方向。实验结果表明，该模型在UCF-VRU数据集上达到了最先进的性能，准确率达到96.45%，并且实现了每秒33帧的实时推理速度。此外，通过与基础设施到车辆（I2V）通信集成，我们的方法可以通过及时激活穿越信号并为连接的车辆提供早期警告来主动提高交叉口的安全性，从而确保所有道路用户更顺畅和安全的交互。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VRU-CIPI%3A+Crossing+Intention+Prediction+at+Intersections+for+Improving+Vulnerable+Road+Users+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09935&send_immediately=true&force_search=false)

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [55] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan, Lei Zhou, Liu Mao, Shaofan Chen, Chuanqiu Ren, Xia Xie*

Main category: cs.CV

TL;DR: This study introduces a new remote sensing change detection task called non-registration change detection to handle emergencies like natural disasters. It proposes eight real-world scenarios for non-registration problems and transforms a registration change detection dataset into a non-registration version. It also shows that non-registration change detection can severely impact advanced methods.


<details>
  <summary>Details</summary>
Motivation: To address the increasing number of emergencies such as natural disasters, anthropogenic accidents, and military strikes.

Method: Systematically proposing eight scenarios for non-registration problems and developing image transformation schemes to convert a registration change detection dataset into a non-registration version.

Result: Non-registration change detection can cause significant damage to state-of-the-art methods.

Conclusion: Introduced a novel remote sensing change detection task and demonstrated its challenges and impacts on advanced methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-Registration+Change+Detection%3A+A+Novel+Change+Detection+Task+and+Benchmark+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09939&send_immediately=true&force_search=false)

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [56] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng, Kexuan Li, Xingye Cui, Jiaxuan Li, Chang Long, Tian Pu, Zhenming Peng*

Main category: cs.CV

TL;DR: This paper presents CSPENet, a new method for infrared small target detection that improves target localization and contour representation under dense clutter.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with accurately localizing dim targets and perceiving contour information in dense clutter environments.

Method: CSPENet includes a surround-convergent prior extraction module, a dual-branch priors embedding architecture, and an attention-guided feature enhancement module.

Result: Experiments on three public datasets show that CSPENet outperforms other state-of-the-art methods.

Conclusion: The proposed CSPENet method significantly enhances infrared small target detection performance.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CSPENet%3A+Contour-Aware+and+Saliency+Priors+Embedding+Network+for+Infrared+Small+Target+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09943&send_immediately=true&force_search=false)

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [57] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang, Tao Tan, Shuai Tan, Weiqin Yang, Kunyan Cai, Calvin Chen, Yue Sun*

Main category: cs.CV

TL;DR: MambaControl uses a novel framework combining selective state-space modeling with diffusion processes to predict Alzheimer's disease progression with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with longitudinal dependencies and structural consistency in progressive disorders.

Method: Integrates selective state-space modelling with diffusion processes, uses Mamba-based long-range modelling combined with graph-guided anatomical control, introduces Fourier-enhanced spectral graph representations.

Result: State-of-the-art performance in Alzheimer's disease prediction with improved progression prediction quality and anatomical fidelity.

Conclusion: MambaControl has potential for personalized prognosis and clinical decision support.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MambaControl%3A+Anatomy+Graph-Enhanced+Mamba+ControlNet+with+Fourier+Refinement+for+Diffusion-Based+Disease+Trajectory+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09965&send_immediately=true&force_search=false)

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [58] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: This paper presents a novel framework for facial expression recognition in the wild, focusing on Texture Key Driver Factors (TKDF) to capture subtle and localized texture cues. The proposed method uses a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF) to achieve state-of-the-art performance on RAF-DB and KDEF datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of recognizing facial expressions due to subtle and localized features and complex variations in facial appearance.

Method: Introduce a framework focusing on Texture Key Driver Factors (TKDF), using a Texture-Aware Feature Extractor (TAFE) with a ResNet-based backbone enhanced with multi-branch attention and Dual Contextual Information Filtering (DCIF) for refining features.

Result: The method achieves state-of-the-art performance on RAF-DB and KDEF datasets.

Conclusion: Incorporating Texture Key Driver Factors (TKDF) into Facial Expression Recognition (FER) pipelines is effective and robust.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TKFNet%3A+Learning+Texture+Key+Factor+Driven+Feature+for+Facial+Expression+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09967&send_immediately=true&force_search=false)

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [59] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang*

Main category: cs.CV

TL;DR: Proposed APCoTTA, a Continuous Test-Time Adaptation method for ALS point cloud semantic segmentation, improving performance with novel mechanisms and creating new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of domain shifts in ALS point clouds and improve model performance in evolving, unlabeled target domains.

Method: Dynamic trainable layer selection module, entropy-based consistency loss, and random parameter interpolation mechanism.

Result: APCoTTA outperforms direct inference on two benchmarks.

Conclusion: APCoTTA improves performance on two benchmarks with mIoU improvements of about 9% and 14%.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是APCoTTA%3A+Continual+Test-Time+Adaptation+for+Semantic+Segmentation+of+Airborne+LiDAR+Point+Clouds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09971&send_immediately=true&force_search=false)

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [60] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou, Yichong Xia, Sicheng Pan, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou*

Main category: cs.CV

TL;DR: An underwater image compression algorithm called HQUIC is introduced, which uses an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamic weighting of multi-scale frequency components to improve compression efficiency.


<details>
  <summary>Details</summary>
Motivation: Contemporary underwater image compression algorithms do not fully utilize the unique characteristics of underwater scenes, leading to suboptimal performance.

Method: HQUIC employs an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamically weights multi-scale frequency components.

Result: HQUIC outperforms state-of-the-art compression methods on diverse underwater datasets.

Conclusion: HQUIC is a novel underwater image compression algorithm that improves compression efficiency by exploiting underwater-image-specific features.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High+Quality+Underwater+Image+Compression+with+Adaptive+Correction+and+Codebook-based+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09986&send_immediately=true&force_search=false)

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [61] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna*

Main category: cs.CV

TL;DR: PointArena是一个用于评估多模态指向能力的综合平台，包含三个组件：Point-Bench数据集、Point-Battle交互式竞技场和Point-Act机器人操作系统。研究发现Molmo-72B模型表现最佳，但特定任务的有监督训练也能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常只关注指代物体定位任务，而忽略了其他推理场景中的指向能力。

Method: 开发了PointArena平台，包含三个部分：Point-Bench数据集、Point-Battle竞技场和Point-Act机器人操作系统。对多种多模态模型进行了广泛的评估。

Result: Molmo-72B在所有评估的模型中表现最优，但有监督的指向任务训练能显著提高模型性能。在多阶段评估管道中观察到强相关性，表明精确的指向能力对于连接抽象推理与实际世界行动至关重要。

Conclusion: 这项工作强调了精确指向能力的重要性，并通过PointArena平台展示了如何有效评估和改进多模态模型的指向能力。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PointArena%3A+Probing+Multimodal+Grounding+Through+Language-Guided+Pointing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09990&send_immediately=true&force_search=false)

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [62] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang, Jiyeong Lee, Kwanghoon Sohn*

Main category: cs.CV

TL;DR: DITM提出了一种新的图像文本匹配方法，通过探索语言的描述灵活性来学习图像和文本之间的分级上下文相似性。与现有方法不同，它超越了严格的二元监督，增强了发现最佳匹配和潜在正样本对的能力。实验表明，该方法在表示复杂的图像文本关系方面比最先进的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有图像文本匹配方法通常采用稀疏二元监督，忽视了视觉和语言之间固有的多对多对应关系以及从一般到具体描述的隐式连接。DITM旨在解决这些问题，通过探索语言的描述灵活性来学习分级上下文相似性。

Method: DITM通过累积词频-逆文档频率（TF-IDF）计算每个句子的描述分数，以此平衡成对相似性。该方法通过两种方式利用句子描述性进行鲁棒图像文本匹配：（1）动态放松正负对之间的连通性以细化错误负标签，（2）按照通用到具体的顺序对相关句子集进行对齐以构建更精确的匹配。

Result: DITM在MS-COCO、Flickr30K和CxC数据集上的广泛实验表明，与最先进的方法相比，其方法在表示复杂图像文本关系方面更有效。此外，在HierarCaps基准上进行的广泛分析表明，DITM增强了模型的分层推理能力。

Conclusion: DITM通过探索语言的描述灵活性，超越了严格的二元监督，增强了发现最佳匹配和潜在正样本对的能力。实验结果表明，DITM在处理复杂的图像文本关系时表现出色，并且提升了模型的分层推理能力。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Descriptive+Image-Text+Matching+with+Graded+Contextual+Similarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09997&send_immediately=true&force_search=false)

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [63] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang, Yuanqi Hu, Xinyu Chen, Yuxia Xu, Suhui Wang, Chunan Yu, Lanyun Zhu, Deyi Ji, Xin Xu, Tianrun Chen*

Main category: cs.CV

TL;DR: A new 3D sketch-driven garment generation framework is introduced, enabling ordinary users to create high-quality digital clothing through simple 3D sketches.


<details>
  <summary>Details</summary>
Motivation: Existing 3D garment design tools are inaccessible to everyday users due to technical barriers and limited data.

Method: Combining conditional diffusion model, sketch encoder, and adaptive curriculum learning strategy to interpret imprecise input and produce realistic garments.

Result: The method outperforms existing baselines in fidelity and usability.

Conclusion: This work demonstrates the potential of democratized fashion design on next-generation consumer platforms.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Air+to+Wear%3A+Personalized+3D+Digital+Fashion+with+AR%2FVR+Immersive+3D+Sketching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09998&send_immediately=true&force_search=false)

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [64] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv8的改进自主目标检测网络，通过整合结构重参数化技术、双向金字塔结构网络模型和新的检测管道，在多尺度、小物体和远距离物体检测方面表现出色。实验结果显示该模型对大小物体的检测准确率达到65%，显著优于传统方法。此模型在实际应用中潜力巨大，特别适合自主驾驶竞赛如FSAC中单目标和小物体检测场景。


<details>
  <summary>Details</summary>
Motivation: 当前用于环境感知的雷达、道路感知的摄像头以及车辆传感器网络存在高成本、易受天气和光照条件影响及分辨率有限等问题。因此，需要一种更高效且精确的目标检测方法来提升自动驾驶的安全性和功能性。

Method: 在YOLOv8框架内集成结构重参数化技术、双向金字塔结构网络模型和新的检测管道。

Result: 实验表明，增强后的模型能够有效检测大、小物体，检测精度达到65%，较传统方法有显著进步。

Conclusion: 提出的改进模型在多尺度、小物体和远程物体检测方面表现优异，具有重要的实际应用价值，特别是在自主驾驶竞赛中单目标和小物体检测场景中的优越性能。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Application+of+YOLOv8+in+monocular+downward+multiple+Car+Target+detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10016，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10016&send_immediately=true&force_search=false)

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [65] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: A reinforcement learning-based method for fine-tuning latent diffusion models to enhance super-resolution quality and adaptability in complex remote sensing scenes.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods have limitations in handling complex scenes and preserving image details.

Method: Constructing a reinforcement learning environment with states, actions, and rewards and optimizing decision objectives through proximal policy optimization during the reverse denoising process of the LDM model.

Result: Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes.

Conclusion: The proposed reinforcement learning-based latent diffusion model fine-tuning method improves super-resolution quality and adaptability across scenes.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ORL-LDM%3A+Offline+Reinforcement+Learning+Guided+Latent+Diffusion+Model+Super-Resolution+Reconstruction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10027&send_immediately=true&force_search=false)

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [66] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga, Dhriti Parikh, Swarna Priya Ramu*

Main category: cs.CV

TL;DR: DeepSeqCoco is a deep learning model for automatic coconut tree disease identification, which shows higher accuracy (up to 99.5%) and faster processing times compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying coconut tree diseases are manual, labor-intensive, and not scalable, especially in developing countries where early diagnosis is crucial for agricultural yield protection.

Method: DeepSeqCoco uses deep learning techniques and was tested with various optimizers like SGD, Adam, and hybrid configurations to optimize performance metrics.

Result: The model achieved up to 99.5% accuracy, up to 5% higher than existing models, with the hybrid SGD-Adam configuration showing the lowest validation loss of 2.81%. Training and prediction times were reduced by up to 18% and 85%, respectively.

Conclusion: DeepSeqCoco demonstrates potential for improving precision agriculture through an AI-based, scalable, and efficient disease monitoring system.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepSeqCoco%3A+A+Robust+Mobile+Friendly+Deep+Learning+Model+for+Detection+of+Diseases+in+Cocos+nucifera，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10030&send_immediately=true&force_search=false)

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [67] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu, Quan Quan, Qingsong Yao, Zaiyi Liu, S. Kevin Zhou*

Main category: cs.CV

TL;DR: A new method called UOD is developed for one-shot medical landmark detection across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with domain preference and robustness issues in multi-domain settings.

Method: UOD uses two stages with domain-specific and domain-shared modules, including a self-supervised convolution model and a domain-adaptive transformer.

Result: UOD shows superior performance compared to previous methods on three X-ray datasets.

Conclusion: UOD demonstrates effectiveness in handling multi-domain medical images with limited labeled data.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UOD%3A+Universal+One-shot+Detection+of+Anatomical+Landmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2306.07615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2306.07615&send_immediately=true&force_search=false)

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [68] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang, Boyang Zheng, Xichen Pan, Sayak Paul, Saining Xie*

Main category: cs.CV

TL;DR: This paper explores the design space of combining large language models and diffusion transformers for text-to-image synthesis, providing a detailed comparison with alternatives and offering a clear training recipe.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed comparisons and undisclosed design details in previous studies on text-to-image synthesis.

Method: An empirical study conducting controlled comparisons with established baselines and analyzing important design choices.

Result: Provides a clear, reproducible recipe for training at scale.

Conclusion: Hopes to offer meaningful data points and practical guidelines for future research in multi-modal generation.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+the+Deep+Fusion+of+Large+Language+Models+and+Diffusion+Transformers+for+Text-to-Image+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10046&send_immediately=true&force_search=false)

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [69] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao*

Main category: cs.CV

TL;DR: 综述了动态场景表示与重建领域的200多篇论文，提供了一个全面的概览，涵盖从隐式神经表示到显式高斯基元的方法，并探讨了运动表示范式、重建技术、辅助信息整合策略及正则化方法等关键方面。


<details>
  <summary>Details</summary>
Motivation: 动态场景表示和重建近年来取得了突破性进展，这些技术最初用于静态环境，但随着对复杂4D动态场景的研究深入，其应用范围不断扩大。

Method: 对超过200篇关于使用辐射场进行动态场景表示的论文进行了分类评估，包括隐式神经表示到显式高斯基元的各种方法，并通过多个关键视角进行分析。

Result: 提出了一种统一的表示框架来组织各种方法论，并通过批判性审查指出了该领域的持续挑战和有前景的研究方向。

Conclusion: 总结了动态场景表示和重建领域的主要挑战和未来研究方向，旨在为研究人员提供权威参考，同时为有经验的从业者提供系统性的概念原理和实践前沿理解。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advances+in+Radiance+Field+for+Dynamic+Scene%3A+From+Neural+Field+to+Gaussian+Field，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10049&send_immediately=true&force_search=false)

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [70] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq, Yingjie Zhang, Irfan Ali Khan*

Main category: cs.CV

TL;DR: Evaluate the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language.


<details>
  <summary>Details</summary>
Motivation: NLP in Pashto faces challenges due to the cursive nature of its script and a scarcity of structured datasets.

Method: Developed a synthetic Pashto OCR dataset, PsOCR, covering variations across 1,000 unique font families, colors, image sizes, and layouts.

Result: Gemini achieved the best performance among all models; Qwen-7B performed best among open-source models.

Conclusion: This work provides insights into the capabilities and limitations of current LMMs for OCR tasks in Pashto and establishes a foundation for further research.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PsOCR%3A+Benchmarking+Large+Multimodal+Models+for+Optical+Character+Recognition+in+Low-resource+Pashto+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10055&send_immediately=true&force_search=false)

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [71] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB是一个两阶段框架，用于通过Gaussian blendshapes合成多样化的风格化3D头像。第一阶段生成风格化视频，第二阶段合成Gaussian blendshapes。在Arcane和Pixar风格上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩展Toonify框架以合成多样化的风格化3D头像。

Method: 提出ToonifyGB框架，包括两阶段：第一阶段生成风格化视频，第二阶段合成Gaussian blendshapes。

Result: 在基准数据集上使用Arcane和Pixar风格验证了框架的有效性。

Conclusion: ToonifyGB能够高效地渲染具有任意表情的风格化头像。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ToonifyGB%3A+StyleGAN-based+Gaussian+Blendshapes+for+3D+Stylized+Head+Avatars，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10072&send_immediately=true&force_search=false)

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [72] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo, Xiaodong Gu*

Main category: cs.CV

TL;DR: Proposes Multi-Modal Representation Learning (MMRL) to improve generalization of vision-language models and introduces an improved version, MMRL++, which reduces parameters and enhances interactions.


<details>
  <summary>Details</summary>
Motivation: Adapting large-scale pre-trained Vision-Language Models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks.

Method: Introduces Multi-Modal Representation Learning (MMRL), which includes a shared, learnable, modality-agnostic representation space. Also proposes MMRL++, a parameter-efficient and interaction-aware extension.

Result: Extensive experiments on 15 datasets show consistent performance improvement.

Conclusion: MMRL and MMRL++ outperform state-of-the-art methods and achieve a strong balance between task-specific adaptation and generalization.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMRL%2B%2B%3A+Parameter-Efficient+and+Interaction-Aware+Representation+Learning+for+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10088&send_immediately=true&force_search=false)

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [73] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu*

Main category: cs.CV

TL;DR: MoB improves visual token pruning by addressing the trade-off between prompt alignment and visual preservation, achieving efficient performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods overlook the varying relative importance of prompt alignment and visual preservation across tasks, leading to inconsistent performance.

Method: MoB reformulates visual token pruning as a bi-objective covering problem and handles the trade-off between prompt alignment and visual preservation through budget allocation via greedy radius trading.

Result: MoB achieves high performance preservation with a significantly reduced number of visual tokens, accelerating models without significant performance loss.

Conclusion: MoB provides a provable performance bound and shows linear scalability with respect to the number of input visual tokens, allowing it to adapt to challenging pruning scenarios.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+1+%2B+1+%3C+1+in+Visual+Token+Pruning%3A+Beyond+Naive+Integration+via+Multi-Objective+Balanced+Covering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10118&send_immediately=true&force_search=false)

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [74] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil, Lucas Robinet, Laurent Risser, Soleakhena Ken*

Main category: cs.CV

TL;DR: This paper introduces a novel method called IMITATE to estimate unknown condition-related images without needing fixed images, applying it to manage image moving tumors in radiotherapy treatments using 4D-CT scans.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of reconstructing artifact-free volumes from irregular breathing patterns during radiotherapy treatments.

Method: Using a conditional U-Net architecture to model the new image registration formalism.

Result: Artifact-free volumes were successfully reconstructed from 4D-CT clinical data with real-time latencies.

Conclusion: The proposed method effectively removes reconstruction artifacts caused by irregular breathing, hysteresis, and poor correlation between breathing signals and internal motion.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IMITATE%3A+Image+Registration+with+Context+for+unknown+time+frame+recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10124&send_immediately=true&force_search=false)

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [75] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: Propose a new method called MCSAD for federated domain generalization, which improves performance by generating data in broader style space and conducting domain-invariant learning.


<details>
  <summary>Details</summary>
Motivation: To address the limited style space issue caused by existing style augmentation methods either exploring data styles within isolated source domain or interpolating style information across existing source domains under the data decentralization scenario.

Method: Multi-source Collaborative Style Augmentation and Domain-invariant learning (MCSAD)

Result: Extensive experiments on multiple domain generalization datasets show that our method significantly outperforms the state-of-the-art federated domain generalization methods.

Conclusion: Our method significantly outperforms the state-of-the-art federated domain generalization methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Source+Collaborative+Style+Augmentation+and+Domain-Invariant+Learning+for+Federated+Domain+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10152&send_immediately=true&force_search=false)

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [76] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer, Harneet Khanuja, Matthias Bethge*

Main category: cs.CV

TL;DR: This paper explores the challenges of predicting fixations across different saliency datasets, finding a significant performance drop due to dataset bias. It proposes a novel architecture that effectively addresses this issue by introducing fewer than 20 dataset-specific parameters, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of dataset bias affecting the performance of saliency prediction models when applied across different datasets.

Method: Introducing a novel architecture that extends a dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters focusing on multi-scale structure, center bias, and fixation spread.

Result: The proposed model significantly reduces the generalization gap, achieving state-of-the-art results on three datasets of the MIT/Tuebingen Saliency Benchmark, especially when adapting to respective training datasets.

Conclusion: The model not only improves performance across datasets but also offers insights into spatial saliency properties.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Saliency+Dataset+Bias，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10169&send_immediately=true&force_search=false)

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [77] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva*

Main category: cs.CV

TL;DR: This paper presents VolE, a novel framework that uses mobile device-driven 3D reconstruction to estimate food volume without reference or depth information.


<details>
  <summary>Details</summary>
Motivation: Accurate food volume estimation is important for medical nutrition management and health monitoring applications, but current methods have limitations.

Method: VolE captures images and camera locations in free motion to generate precise 3D models using AR-capable mobile devices and leverages food video segmentation for food mask generation.

Result: Experiments show that VolE outperforms existing volume estimation techniques across multiple datasets with a MAPE of 2.22%.

Conclusion: VolE is a superior method for food volume estimation.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VolE%3A+A+Point-cloud+Framework+for+Food+3D+Reconstruction+and+Volume+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10205&send_immediately=true&force_search=false)

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [78] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: Evaluate alternative augmentation strategies (MixUp and Auxiliary Fourier Augmentation) to improve out-of-distribution generalization and robustness in medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of medical image segmentation models due to mismatches between training and test distributions in real-world clinical settings.

Method: Systematically evaluate MixUp and Auxiliary Fourier Augmentation methods.

Result: These augmentation methods improve out-of-distribution generalization and robustness to imaging variations in cardiac cine MRI and prostate MRI segmentation tasks.

Conclusion: Integration of these augmentation techniques into nnU-Net training pipelines enhances the reliability of medical segmentation models in real-world applications.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Agnostic+Augmentations+for+Unknown+Variations%3A+Out-of-Distribution+Generalisation+in+MRI+Segmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10223，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10223&send_immediately=true&force_search=false)

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [79] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes*

Main category: cs.CV

TL;DR: Deep neural networks in medical imaging have biases leading to fairness gaps. Incorporating human insights reduces these gaps and improves out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: To explore Human-AI alignment and fairness in medical imaging.

Method: Systematic exploration.

Result: Incorporating human insights reduces fairness gaps and enhances out-of-domain generalization, but excessive alignment may introduce performance trade-offs.

Conclusion: Human-AI alignment is a promising approach for fair, robust, and generalizable medical AI systems.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Interplay+of+Human-AI+Alignment%2CFairness%2C+and+Performance+Trade-offs+in+Medical+Imaging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10231&send_immediately=true&force_search=false)

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [80] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: This paper introduces MTVCrafter, a novel framework for human image animation that directly models 4D motion sequences instead of relying on 2D pose images. It uses 4DMoT to tokenize 3D motion and MV-DiT for animation, achieving superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D-rendered pose images for motion guidance, limiting generalization and losing essential 3D information.

Method: Proposes MTVCrafter, including 4DMoT for tokenizing 3D motion into 4D motion tokens and MV-DiT for leveraging these tokens in animation.

Result: Achieves state-of-the-art results with an FID-VID of 6.98, surpassing previous methods, and shows good generalization to various characters and scenarios.

Conclusion: Introduces a new direction for pose-guided human video generation by effectively utilizing 4D motion information.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MTVCrafter%3A+4D+Motion+Tokenization+for+Open-World+Human+Image+Animation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10238，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10238&send_immediately=true&force_search=false)

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [81] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen, Wanqi Yin, Xiaofeng Yang, Cheng Chen, Chaoyue Song, Zhongang Cai, Lei Yang, Hao Wang, Guosheng Lin*

Main category: cs.CV

TL;DR: Proposes ADHMR, a framework that aligns a diffusion-based HMR model through preference optimization. Uses an HMR-Scorer model to evaluate predictions and improve existing HMR models.


<details>
  <summary>Details</summary>
Motivation: To address depth ambiguity and occlusions in human mesh recovery from a single image, especially for in-the-wild images.

Method: Trains an HMR-Scorer model, creates a preference dataset, and uses it to fine-tune the base model via direct preference optimization.

Result: ADHMR outperforms current state-of-the-art methods.

Conclusion: The proposed ADHMR framework improves the alignment of 3D human mesh predictions with 2D image observations and enhances robustness to in-the-wild images.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADHMR%3A+Aligning+Diffusion-based+Human+Mesh+Recovery+via+Direct+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10250&send_immediately=true&force_search=false)

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [82] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu, Jiaqi Tang, Jiyao Wang, Yunfan LU, Xu Cao, Qingyong Hu, Yin Wang, Yuting Zhang, Tianxin Xie, Yunpeng Zhang, Yong Chen, Jiayu. Gao, Bin Huang, Dengbo He, Shuiguang Deng, Hao Chen, Ying-Cong Chen*

Main category: cs.CV

TL;DR: This paper presents SAGE DeeR, a driving agent that adapts to individual preferences, understands complex inputs, and improves through implicit learning.


<details>
  <summary>Details</summary>
Motivation: To meet the comfort, interaction, and safety needs of different users in intelligent driving cockpits.

Method: Developing a Super-Aligned and GEneralist DRiving agent that aligns with individual preferences, processes multi-modal inputs, and uses self-eliciting for improved capabilities.

Result: Achieved super alignment, generalist understanding, and self-eliciting features; created a large-scale benchmark for measuring perceptual decision-making and alignment accuracy.

Conclusion: SAGE DeeR demonstrates advanced adaptability and understanding capabilities in intelligent driving environments.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sage+Deer%3A+A+Super-Aligned+Driving+Generalist+Is+Your+Copilot，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10257&send_immediately=true&force_search=false)

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [83] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz, Pascal Colling, Qi Han, Tobias Meisen*

Main category: cs.CV

TL;DR: This paper proposes an innovative offline mapping approach that incorporates driver trails into the map creation process using transformer-based deep learning models, which outperforms state-of-the-art online mapping approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by online mapping such as temporal consistency, sensor occlusion, runtime, and generalization.

Method: Integrating trail data from the ego vehicle and other traffic participants to construct a comprehensive global map using transformer-based deep learning models.

Result: The proposed method demonstrates superior performance compared to state-of-the-art online mapping approaches, with improved generalization to unseen environments and sensor configurations.

Conclusion: This offline mapping approach enables continuous updates while remaining sensor-agnostic, facilitating efficient data transfer.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inferring+Driving+Maps+by+Deep+Learning-based+Trail+Map+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10258&send_immediately=true&force_search=false)

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [84] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev, Petr Surovtsev, Alexander Kapitanov, Karina Kvanchiani, Aleksandr Nagaev*

Main category: cs.CV

TL;DR: This paper presents HandReader, a set of three architectures that improve fingerspelling recognition accuracy using RGB video and keypoint data.


<details>
  <summary>Details</summary>
Motivation: Previous methods on fingerspelling recognition have mainly focused on processing the temporal dimension of videos but have left room for improvement in accuracy.

Method: The paper proposes HandReader which includes HandReader_RGB, HandReader_KP, and HandReader_RGB+KP. HandReader_RGB uses the Temporal Shift-Adaptive Module (TSAM) to process RGB video features. HandReader_KP is based on the Temporal Pose Encoder (TPE) using keypoints as tensors, and HandReader_RGB+KP has a joint encoder to combine both modalities.

Result: The HandReader models outperform existing approaches on the ChicagoFSWild and ChicagoFSWild+ datasets. They also show high performance on the new Znaki dataset for Russian fingerspelling.

Conclusion: The paper introduces HandReader, a set of three architectures that achieve state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets for fingerspelling recognition. Additionally, the models perform well on the newly introduced Znaki dataset for Russian fingerspelling.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HandReader%3A+Advanced+Techniques+for+Efficient+Fingerspelling+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10267&send_immediately=true&force_search=false)

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [85] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu, Kaixin Chen, Heng Guo, Yixiang Huang, Ming Wu, Zhenwei Shi, Chuang Zhang, Jun Guo*

Main category: cs.CV

TL;DR: Introduce MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting, comprising over 68,000 high-resolution samples from 15 regions and 6 satellites.


<details>
  <summary>Details</summary>
Motivation: Limited availability of open-source datasets for marine fog detection and forecasting, especially those that can evaluate model performance across diverse conditions.

Method: Create a new dataset called MFogHub which integrates annotated marine fog observations from multiple regions and satellites.

Result: MFogHub contains over 68,000 high-resolution samples from 15 coastal fog-prone regions and six geostationary satellites.

Conclusion: MFogHub can help reveal generalization fluctuations and serve as a valuable resource for developing scalable fog prediction techniques.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MFogHub%3A+Bridging+Multi-Regional+and+Multi-Satellite+Data+for+Global+Marine+Fog+Detection+and+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10281&send_immediately=true&force_search=false)

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [86] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang, Shuai Xu, Xuelin Zhu, Yicong Li*

Main category: cs.CV

TL;DR: 提出了一种多阶段跨模态交互模型，改进了细粒度局部视觉信息的捕捉能力，并在多个数据集上取得了较好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要依赖于CLIP的跨模态对齐能力，但忽略了其在捕捉细粒度局部特征方面的局限性，这些问题源于其架构和训练范式。

Method: 提出了一种多阶段跨模态交互（MSCI）模型，利用CLIP视觉编码器的中间层信息，设计了两个自适应聚合器分别提取低层视觉特征中的局部信息和高层视觉特征中的全局信息，并通过逐步互动机制将其融入文本表示中。

Result: 在三个常用数据集上的实验验证了所提出的MSCI模型的有效性和优越性。

Conclusion: 提出了一个多阶段跨模态交互（MSCI）模型，通过设计两个自适应聚合器来提取低层视觉特征中的局部信息和高层视觉特征中的全局信息，并通过逐步互动机制将其融入文本表示中，显著提高了对细粒度局部视觉信息的感知能力。此外，MSCI 模型还可以根据不同的组合以及同一组合内的不同元素动态调整全局和局部视觉信息之间的注意力权重，使其能够灵活适应各种场景。实验结果表明，该模型在三个常用数据集上的表现优于现有方法。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MSCI%3A+Addressing+CLIP%27s+Inherent+Limitations+for+Compositional+Zero-Shot+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10289&send_immediately=true&force_search=false)

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [87] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira, David Martins de Matos*

Main category: cs.CV

TL;DR: A new dataset named StoryReasoning with 4,178 stories from movie images helps improve visual storytelling by maintaining consistent characters and objects.


<details>
  <summary>Details</summary>
Motivation: To solve the problems of referential hallucinations in visual storytelling systems.

Method: Introduce cross-frame object re-identification, chain-of-thought reasoning, and a grounding scheme linking text to visual entities.

Result: Fine-tuned Qwen2.5-VL 7B improves storytelling performance reducing hallucinations by 12.3%.

Conclusion: The proposed approach effectively maintains character and object consistency in visual storytelling.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StoryReasoning+Dataset%3A+Using+Chain-of-Thought+for+Scene+Understanding+and+Grounded+Story+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10292&send_immediately=true&force_search=false)

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [88] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter*

Main category: cs.CV

TL;DR: MIPHEI uses H&E images to predict mIF signals for comprehensive marker panels, achieving accurate cell-type classification and showing potential for large-scale H&E dataset analysis.


<details>
  <summary>Details</summary>
Motivation: To enable more precise cell type identification via proteomic markers without the cost and logistical constraints of mIF, bridging the gap between H&E and mIF analysis.

Method: A U-Net-inspired architecture integrating ViT foundation models as encoders to predict mIF signals from H&E images.

Result: Accurate cell-type classification from H&E alone with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming a state-of-the-art baseline and a random classifier for most markers.

Conclusion: MIPHEI effectively captures complex relationships between nuclear morphologies and molecular markers, offering a promising approach for cell-type-aware analysis of large-scale H&E datasets.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MIPHEI-ViT%3A+Multiplex+Immunofluorescence+Prediction+from+H%26E+Images+using+ViT+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10294，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10294&send_immediately=true&force_search=false)

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [89] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu, Jirong Zha, Ding Li, Leye Wang*

Main category: cs.CV

TL;DR: This paper investigates membership inference on visual self-supervised models under a more realistic black-box attack setting. A unified method called PartCrop is proposed which crops parts of objects in an image to query responses within the image in representation space. Extensive attacks and defenses are conducted showing the effectiveness and generalization of PartCrop as well as proposing a scalable PartCrop-v2.


<details>
  <summary>Details</summary>
Motivation: Investigate membership inference on visual self-supervised models under a more realistic black-box attack setting.

Method: Propose a unified method called PartCrop which crops parts of objects in an image to query responses within the image in representation space.

Result: Extensive attacks and defenses are conducted showing the effectiveness and generalization of PartCrop.

Conclusion: Propose a scalable PartCrop-v2 by introducing two structural improvements to PartCrop.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Unified+and+Scalable+Membership+Inference+Method+for+Visual+Self-supervised+Encoder+via+Part-aware+Capability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10351，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10351&send_immediately=true&force_search=false)

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [90] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong*

Main category: cs.CV

TL;DR: An efficient spike-driven video Transformer called SpikeVideoFormer is introduced with linear temporal complexity.


<details>
  <summary>Details</summary>
Motivation: Existing SNN-based Transformers mainly focus on single-image tasks without effectively leveraging SNNs' efficiency in video-based vision tasks.

Method: A spike-driven Hamming attention (SDHA) is designed to provide a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Various spike-driven space-time attention designs are analyzed and an optimal scheme is identified.

Result: The model demonstrates generalization ability and efficiency across diverse downstream video tasks, achieving state-of-the-art performance with over 15% improvement on the latter two tasks compared to existing SNN approaches. It also matches the performance of recent ANN-based methods while offering significant efficiency gains.

Conclusion: SpikeVideoFormer achieves state-of-the-art performance in video tasks while maintaining superior energy efficiency.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpikeVideoFormer%3A+An+Efficient+Spike-Driven+Video+Transformer+with+Hamming+Attention+and+%24%5Cmathcal%7BO%7D%28T%29%24+Complexity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10352&send_immediately=true&force_search=false)

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [91] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire, Radu Timofte*

Main category: cs.CV

TL;DR: This paper proposes an unpaired training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content.


<details>
  <summary>Details</summary>
Motivation: Developing a learned ISP is a difficult and costly step due to the requirement of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images.

Method: An unpaired training method with a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks.

Result: Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics.

Conclusion: Our unpaired approach can eliminate the need for direct correspondences between raw images and ground-truth data with matching content.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learned+Lightweight+Smartphone+ISP+with+Unpaired+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10420，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10420&send_immediately=true&force_search=false)

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [92] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran, Sangeet Khemlani, J. G. Trafton*

Main category: cs.CV

TL;DR: This study examines the capability of vision language models (VLMs) in understanding visuospatial properties of scenes depicted in images by testing their ability to process virtual objects mentioned in prompts.


<details>
  <summary>Details</summary>
Motivation: To explore how well VLMs comprehend the visuospatial properties of scenes in images.

Method: Using descriptions of virtual objects paired with images, evaluating state-of-the-art VLMs' reasoning about spatial relations between real and virtual objects.

Result: The results show that current VLMs have inadequate ability to process virtual objects.

Conclusion: The study suggests that VLMs need improvement in comprehending visuospatial properties of scenes involving virtual objects.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vision+language+models+have+difficulty+recognizing+virtual+objects，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10453&send_immediately=true&force_search=false)

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [93] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang, Hongkun Cao, Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS is a 3D Gaussian splatting optimization method that provides semantically meaningful and cross-scene consistent quantity-quality control while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian splatting methods lack the ability for users to intuitively adjust the trade-off between Gaussian quantity and rendering quality to suit practical needs.

Method: ControlGS achieves semantically meaningful and cross-scene consistent quantity-quality control through a single training run using a user-specified hyperparameter.

Result: ControlGS outperforms baselines by achieving higher rendering quality with fewer Gaussians and supports a broad adjustment range with stepless control over the trade-off.

Conclusion: ControlGS provides a solution for adjusting the trade-off between Gaussian quantity and rendering quality in 3D Gaussian splatting.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Consistent+Quantity-Quality+Control+across+Scenes+for+Deployment-Aware+Gaussian+Splatting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10473&send_immediately=true&force_search=false)

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [94] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev*

Main category: cs.CV

TL;DR: This study introduces Logos, a new Russian Sign Language dataset, which improves cross-language ISLR model training and annotation policies. It shows that pre-training on Logos can serve as a universal encoder for other language SLR tasks, and explicitly annotating visually similar signs enhances model quality.


<details>
  <summary>Details</summary>
Motivation: The limited amount of data for individual sign languages and ambiguity in dataset labeling pose challenges for ISLR model training and annotation policies.

Method: Introduce Logos, a novel Russian Sign Language dataset, and explore cross-language transfer learning approaches with multiple classification heads.

Result: A model pre-trained on Logos can be used as a universal encoder for other language SLR tasks, and explicitly labeling visually similar signs improves trained model quality. The study outperforms current state-of-the-art results for the WLASL dataset and gets competitive results for the AUTSL dataset with a single stream model processing solely RGB video.

Conclusion: The Logos dataset is the most extensive ISLR dataset by the number of signers and one of the largest available datasets while being the largest RSL dataset in size and vocabulary.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logos+as+a+Well-Tempered+Pre-train+for+Sign+Language+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10481&send_immediately=true&force_search=false)

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [95] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li*

Main category: cs.CV

TL;DR: Introduce UniEval, the first unified evaluation framework for multimodal models without extra models, images, or annotations.


<details>
  <summary>Details</summary>
Motivation: Lack of unified evaluation framework for multimodal models

Method: Develop UniEval framework including UniBench and UniScore metric

Result: UniBench more challenging than existing benchmarks, UniScore aligns closely with human evaluations

Conclusion: UniEval provides simplified and unified evaluation process for multimodal models

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniEval%3A+Unified+Holistic+Evaluation+for+Unified+Multimodal+Understanding+and+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10483&send_immediately=true&force_search=false)

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [96] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales*

Main category: cs.CV

TL;DR: A comprehensive evaluation framework named CheXGenBench is introduced to assess synthetic chest radiograph generation across different models. It evaluates fidelity, privacy risks, and clinical utility using standardized metrics. The framework reveals inefficiencies in current evaluation methods and provides a new standard for the medical AI community.


<details>
  <summary>Details</summary>
Motivation: To overcome methodological inconsistencies and provide a unified evaluation protocol for assessing synthetic chest radiograph generation.

Method: Introduces CheXGenBench with over 20 quantitative metrics evaluating generation quality, privacy risks, and clinical applicability across 11 leading text-to-image architectures.

Result: Identifies inefficiencies in existing evaluation protocols, particularly in assessing generative fidelity. Establishes a standardized benchmark for the medical AI community and releases a high-quality synthetic dataset, SynthCheX-75K.

Conclusion: CheXGenBench sets a new standard for evaluating synthetic chest radiograph generation, enabling more objective and reproducible comparisons.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CheXGenBench%3A+A+Unified+Benchmark+For+Fidelity%2C+Privacy+and+Utility+of+Synthetic+Chest+Radiographs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10496&send_immediately=true&force_search=false)

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [97] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev, Nuno Goncalves*

Main category: cs.CV

TL;DR: A novel dual-branch classification strategy is proposed to enhance deep network robustness against face morphing attacks.


<details>
  <summary>Details</summary>
Motivation: Modern face recognition systems need to be robust against presentation attacks like face morphing.

Method: Introduce a dual-branch classification strategy that modifies the labeling ambiguity of face morphs.

Result: The method improves the model's ability to distinguish morph images from genuine ones and shows effectiveness on public benchmarks.

Conclusion: The approach is universally applicable and can be integrated into existing face recognition training pipelines.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MorphGuard%3A+Morph+Specific+Margin+Loss+for+Enhancing+Robustness+to+Face+Morphing+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10497，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10497&send_immediately=true&force_search=false)

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [98] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma, Shivansh Gupta, Samar Agarwal, Vishak Prasad C., Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: This paper proposes an enhancement to the retriever framework in the MIRAGE model by using submodular subset selection techniques to pre-select semantically relevant images, improving retrieval performance in Multiple Image Question Answering tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by large multimodal models when dealing with multiple images, especially in terms of scalability and retrieval performance.

Method: Enhancing the retriever framework in MIRAGE by incorporating query-aware submodular functions like GraphCut to pre-select relevant images before the main retrieval component.

Result: The proposed method demonstrates improved effectiveness in the submodular-retriever pipeline, particularly beneficial in scenarios with large haystack sizes.

Conclusion: The use of anchor-based queries and data augmentation enhances the retrieval performance in multiple image question answering tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Multi-Image+Question+Answering+via+Submodular+Subset+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10533&send_immediately=true&force_search=false)

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [99] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue*

Main category: cs.CV

TL;DR: This paper introduces a novel approach to assess the visual comprehension of Multimodal Large Language Models by defining implicit visual misunderstanding and proposing a new metric called attention accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs mainly focus on answer correctness but ignore whether the models truly understand visual inputs.

Method: Decoupling visual and textual modalities in the causal attention module and analyzing the attention distribution across network layers.

Result: The authors reveal that attention increasingly focuses on the correct image and propose a scale-agnostic metric called attention accuracy to better evaluate visual understanding.

Conclusion: The proposed method provides a more reliable assessment of visual understanding in MLLMs and demonstrates its effectiveness across different granularities.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Implicit+Visual+Misunderstandings+in+Multimodal+Large+Language+Models+through+Attention+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10541&send_immediately=true&force_search=false)

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [100] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu, Jessica Bader, Jae Myung Kim*

Main category: cs.CV

TL;DR: This paper investigates the necessity of enforcing feasibility when generating synthetic training data for CLIP-based classifiers and finds that it has minimal effect on performance.


<details>
  <summary>Details</summary>
Motivation: Investigating whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture.

Method: Introducing VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model.

Result: Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. The attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.

Conclusion: Enforcing feasibility is not necessary when generating synthetic training data for CLIP-based classifiers.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Feasibility+Matter%3F+Understanding+the+Impact+of+Feasibility+on+Synthetic+Training+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10551&send_immediately=true&force_search=false)

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [101] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li*

Main category: cs.CV

TL;DR: 提出了一种利用代码作为监督来实现跨模态对齐的方法，开发了一个图像到代码模型（FigCodifier）和一个名为ImgCode-8.6M的数据集，并利用此数据集构建了一个高质量的多模态数学指令微调数据集（MM-MathInstruct-3M）。最终提出的MathCoder-VL模型在所有六个指标上达到了开源SOTA，特别是在解决几何问题方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言图像描述数据集主要关注自然场景，忽视了数学图解中的复杂细节，这些细节对于解决问题至关重要，阻碍了当前大型多模态模型在数学推理方面的进步。

Method: 通过利用代码作为监督来建立图像和代码之间的精确连接，采用模型循环的方法共同开发图像到代码模型和数据集。

Result: 提出的MathCoder-VL模型在多个指标上达到了开源SOTA，特别是在解决几何问题方面表现突出，超过了GPT-4o和Claude 3.5 Sonnet。

Conclusion: 该研究通过引入新的数据集和模型显著提升了多模态数学推理的能力，并且开放了数据集和模型供公众使用。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MathCoder-VL%3A+Bridging+Vision+and+Code+for+Enhanced+Multimodal+Mathematical+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10557，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10557&send_immediately=true&force_search=false)

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [102] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang*

Main category: cs.CV

TL;DR: Proposes ETT, an end-to-end vision tokenizer tuning approach that jointly optimizes vision tokenization and target autoregressive tasks, achieving 2-6% performance gains in multimodal understanding and visual generation tasks compared to frozen tokenizer baselines.


<details>
  <summary>Details</summary>
Motivation: Existing vision tokenization assumes visual tokens can generalize well across various tasks, but this leads to a misalignment where the loss of vision tokenization becomes a bottleneck for target tasks.

Method: ETT leverages visual embeddings of the tokenizer codebook and optimizes vision tokenizers end-to-end with both reconstruction and caption objectives, integrating seamlessly into existing training pipelines with minimal architecture modifications.

Result: ETT achieves significant performance gains in multimodal understanding and visual generation tasks without sacrificing the original reconstruction capability.

Conclusion: ETT is a simple yet effective method that can empower multimodal foundation models beyond image generation and understanding.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是End-to-End+Vision+Tokenizer+Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10562，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10562&send_immediately=true&force_search=false)

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [103] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, Zhou Zhao*

Main category: cs.CV

TL;DR: 提出了一种Prior Depth Anything框架，结合度量信息和几何结构生成准确的深度图，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么缺乏精确的度量信息，要么缺乏对任意场景的通用性。因此，需要一种能够结合精确但不完整的度量信息与完整但相对的几何结构的方法。

Method: 设计了一个从粗到细的管道，逐步整合两种互补的深度源。首先引入了像素级度量对齐和距离感知加权来预填充多样的度量先验。其次开发了一个条件单目深度估计（MDE）模型来细化深度先验的固有噪声。

Result: 在7个真实世界的数据集上展示了令人印象深刻的零样本泛化能力，匹配甚至超过了以前的任务特定方法。

Conclusion: 提出了一种名为Prior Depth Anything的框架，该框架能够生成准确、密集且详细的度量深度图。模型展示了在多个真实世界数据集上的卓越零样本泛化能力，并能够在测试时通过切换预测模型来改进精度-效率权衡。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Depth+Anything+with+Any+Prior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10565，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10565&send_immediately=true&force_search=false)

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [104] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao*

Main category: cs.CV

TL;DR: We introduce 3D-Fixup, a new framework for 2D image editing guided by learned 3D priors, supporting complex edits like object translation and rotation.


<details>
  <summary>Details</summary>
Motivation: The challenge of 3D-aware image editing due to objects being specified by a single image.

Method: A training-based approach using diffusion models and 3D guidance from an Image-to-3D model, with a designed data generation pipeline for high-quality training.

Result: Effective support for complex, identity-coherent 3D-aware edits with high-quality results.

Conclusion: Integrating 3D priors advances diffusion models' application in realistic image manipulation.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是3D-Fixup%3A+Advancing+Photo+Editing+with+3D+Priors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10566&send_immediately=true&force_search=false)

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: A new search algorithm is proposed and shown to outperform existing ones in two-player zero-sum games with perfect information.


<details>
  <summary>Details</summary>
Motivation: To evaluate the generality of search algorithms' performance in two-player zero-sum games with perfect information.

Method: The proposed new search algorithm

Result: The new search algorithm shows superior performance compared to other studied algorithms.

Conclusion: For a short search time, the new search algorithm outperforms all studied algorithms on all games in the large experiment. For a medium search time, it outperforms all studied algorithms on 17 of the 22 studied games.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Study+and+improvement+of+search+algorithms+in+two-players+perfect+information+games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09639&send_immediately=true&force_search=false)

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [106] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle, Santiago Cifuentes*

Main category: cs.AI

TL;DR: This paper improves existing techniques for identifying crucial features in classification models using concepts from propositional logic.


<details>
  <summary>Details</summary>
Motivation: To improve the identification of relevant and necessary features in complex models like neural networks.

Method: Improving existing techniques and algorithms for deciding on relevant and/or necessary features, generalizing the notion of relevancy, and introducing a new global notion of usefulness.

Result: Efficient detection of necessity in complex models and development of algorithms for detecting usefulness in decision trees and other models.

Conclusion: The paper shows that necessity can be efficiently detected in complex models and proves the relationship between usefulness and relevancy/necessity.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Relevancy%2C+Necessity+and+Usefulness%3A+Complexity+and+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09640&send_immediately=true&force_search=false)

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [107] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad, Reuth Mirsky*

Main category: cs.AI

TL;DR: This paper introduces General Dynamic Goal Recognition (GR) problem and uses a model-free goal-conditioned RL approach to achieve fast adaptation for GR across different changing tasks.


<details>
  <summary>Details</summary>
Motivation: Goal recognition is crucial in human-robot interaction and multi-agent collaborations. However, traditional GR methods struggle in dynamic environments with numerous and evolving goals.

Method: A model-free goal-conditioned RL approach is used to enable fast adaptation for GR.

Result: The proposed method can adapt quickly for GR across various changing tasks.

Conclusion: This work broadens the scope of GR by introducing the General Dynamic GR problem and provides a potential solution using a model-free goal-conditioned RL approach.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是General+Dynamic+Goal+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09737&send_immediately=true&force_search=false)

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [108] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty, Rishi Ramaesh, Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI是一种可扩展的专家驱动模型，用于从胸部X光片中检测多种肺部疾病，同时保留了可解释的人类临床概念。实验表明，XpertXAI在预测准确性和概念级解释方面都优于现有技术，并能更好地与放射科医生的判断相一致。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型在肺病检测中的透明度和可解释性，以便于临床应用。

Method: 引入了一种名为XpertXAI的新方法，该方法基于InceptionV3分类器，并使用公共胸部X光数据集与放射学报告进行比较评估。

Result: XpertXAI在预测准确性和概念级解释方面优于现有的后处理解释方法和无监督CBM（XCBs）。

Conclusion: 这种方法展示了如何通过以人为中心的设计来扩展到更广泛的诊断环境中，从而为医学诊断提供有意义的可解释人工智能的可扩展路径。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainability+Through+Human-Centric+Design+for+XAI+in+Lung+Cancer+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09755&send_immediately=true&force_search=false)

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [109] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi, Ting Xiao, Mark V. Albert*

Main category: cs.AI

TL;DR: Proposed a multi-agent framework for radiology report generation that improves accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address challenges such as factual inconsistency, hallucination, and cross-modal misalignment faced by previous approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG).

Method: A multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis.

Result: Our approach improves the accuracy, structure, and interpretability of radiology report generation.

Conclusion: Our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multimodal+Multi-Agent+Framework+for+Radiology+Report+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09787&send_immediately=true&force_search=false)

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [110] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang, Yongli Zhu*

Main category: cs.AI

TL;DR: This paper studies offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration, demonstrating their feasibility and effectiveness on various offline datasets, even those with low-quality experience.


<details>
  <summary>Details</summary>
Motivation: To address the issue of lacking online environment interactions in microgrid voltage regulation, which may be due to technical or safety reasons.

Method: Using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration.

Result: The proposed approach can obtain an applicable model through offline-style training on previously collected datasets.

Conclusion: The study shows that the proposed approach is feasible and effective for microgrid voltage regulation with solar power penetration on different offline datasets, including those with low-quality experience.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+Reinforcement+Learning+for+Microgrid+Voltage+Regulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09920&send_immediately=true&force_search=false)

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [111] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin, Donghyun Kim, Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本文提出了一种新的问题评估框架，通过两个维度（合适性和有效性）评估问题质量，并验证了其在不同数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究缺乏全面评估问题质量的研究，因此提出了一种新的评估框架来定义和评估好问题。

Method: 提出了两个关键评估维度：合适性（语境中的社会语言能力）和有效性（目标实现的战略能力），并基于这些基础维度开发了基于量规的评分系统。该框架通过半自适应标准实现了结构和灵活性。

Result: 通过CAUS和SQUARE数据集验证了该方法的有效性，证明该框架能够评估良好和有问题的问题，并适应不同的上下文。

Conclusion: 这项研究建立了一个灵活且全面的问题评估框架，朝着将提问行为与基于提问本质的结构化分析方法相结合迈出了重要一步。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%22There+Is+No+Such+Thing+as+a+Dumb+Question%2C%22+But+There+Are+Good+Ones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09923&send_immediately=true&force_search=false)

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [112] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara, Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: This whitepaper discusses the development of AI from simple rule-based systems to advanced autonomous agents, highlighting key milestones and their societal impacts. It emphasizes the rapid advancement of AI, suggesting intelligence doubles every six months, and stresses the importance of wisdom in managing this new era.


<details>
  <summary>Details</summary>
Motivation: To document the evolution of AI and highlight its significant advancements and societal implications.

Method: Chronological documentation of AI milestones and analysis of their impact.

Result: AI agents like ChatGPT and Grok represent the 'final generation' of current intelligence, showcasing complex reasoning and interaction capabilities.

Conclusion: Wisdom and foresight are crucial for navigating the opportunities and challenges of the powerful new era of AI.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+AI+Agents%3A+The+Final+Generation+of+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09932&send_immediately=true&force_search=false)

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [113] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini*

Main category: cs.AI

TL;DR: A new method called Pre-Act is introduced to improve the performance of agent systems by creating a multi-step execution plan with detailed reasoning. The method shows better performance than ReAct, especially when fine-tuning smaller models.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of agent systems with better reasoning and action capabilities.

Method: Introducing Pre-Act, which creates a multi-step execution plan with detailed reasoning for user inputs.

Result: Pre-Act outperforms ReAct by 70% in Action Recall on the Almita dataset at the turn level. Fine-tuned smaller models also show significant improvements in action accuracy and goal completion rate.

Conclusion: Pre-Act is an effective approach for improving agent system performance, particularly beneficial for smaller models in practical applications.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pre-Act%3A+Multi-Step+Planning+and+Reasoning+Improves+Acting+in+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09970&send_immediately=true&force_search=false)

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [114] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu, Zelin Fu, Xinhe Kuang, Jiacheng Dong, Qi Zhang, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: The MPDD Challenge aims to improve depression detection by considering individual differences and using multimodal data, providing age-specific datasets and a baseline model.


<details>
  <summary>Details</summary>
Motivation: Existing datasets and detection methods primarily focus on young adults, neglecting the broader age spectrum and individual differences that influence depression manifestation. Current approaches often fail to capture the complexity and diversity of depression across individuals.

Method: A baseline model that fuses audio and video modalities with individual difference information is provided to detect depression manifestations in diverse populations.

Result: The challenge includes two tracks based on age-specific subsets: Track 1 uses the MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses the MPDD-Young dataset for detecting depression in younger participants.

Conclusion: The Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to address the gap in current depression detection methods by incorporating multimodal data alongside individual difference factors.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+First+MPDD+Challenge%3A+Multimodal+Personality-aware+Depression+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10034&send_immediately=true&force_search=false)

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [115] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash*

Main category: cs.AI

TL;DR: This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs to guide learners in understanding knowledge concepts in MOOC platforms.


<details>
  <summary>Details</summary>
Motivation: The lack of direct interaction in MOOCs and the unreliability of LLMs due to hallucinations motivated the research.

Method: The proposed pipeline includes a PKG-based Question Generation method and an EduKG-based Question Answering method.

Result: The evaluation with 3 expert instructors on 3 different MOOCs showed the potential of Graph RAG in providing a personalized learning experience.

Conclusion: Graph RAG can empower learners to understand new knowledge concepts in MOOCs through personalized guidance.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Graph+Retrieval-Augmented+Generation+to+Support+Learners%27+Understanding+of+Knowledge+Concepts+in+MOOCs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10074&send_immediately=true&force_search=false)

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [116] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: This study uses AI techniques to extract information from 1,367 Taiwan studies papers and creates a knowledge graph and vector database to help explore the field's intellectual trajectories and research gaps.


<details>
  <summary>Details</summary>
Motivation: There is a need to systematically revisit and reorganize decades of Taiwan-based China Studies scholarship.

Method: Generative AI techniques and large language models are used to extract entity relation triples from academic texts, which are then visualized through a D3.js based system to form a knowledge graph and vector database.

Result: The infrastructure created allows users to explore conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.

Conclusion: This work shows how generative AI can enhance scholarly access to literature and support a reimagined scholarly infrastructure for regional knowledge systems.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Text+to+Network%3A+Constructing+a+Knowledge+Graph+of+Taiwan-Based+China+Studies+Using+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10093&send_immediately=true&force_search=false)

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [117] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker, Olivia Sanchez-Graillet, Moana Seidler, Christian Brandt, Jörg Wellmer, Philipp Cimiano*

Main category: cs.AI

TL;DR: This paper explores different approaches for generating explanations in XAI and evaluates their effectiveness through a user study with physicians.


<details>
  <summary>Details</summary>
Motivation: To understand which types of explanations increase transparency and build trust between doctors and ML systems in shared decision-making scenarios.

Method: A user study involving physicians who completed surveys and participated in interviews to assess different types of AI-generated explanations.

Result: The study identified the most effective and useful explanations that improve the diagnostic process.

Conclusion: The findings contribute to understanding the types of explanations that are most effective in enhancing the diagnostic process.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+User+Study+Evaluating+Argumentative+Explanations+in+Diagnostic+Decision+Support，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10188，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10188&send_immediately=true&force_search=false)

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [118] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo, Haiyang Shen, Jinsheng Huang, Zhengyang Mao, Junyu Luo, Zhuoru Chen, Xuhui Liu, Bingyu Xia, Luchen Liu, Yun Ma, Ming Zhang*

Main category: cs.AI

TL;DR: This paper presents MASS, a multi-agent system based on large language models for portfolio construction. It achieves continuous excess returns through large-scale simulations and end-to-end optimization, outperforming six state-of-the-art baselines across three challenging A-share stock pools.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems are limited to pure simulations or predefined workflows, reducing their applicability and effectiveness.

Method: MASS progressively increases the number of agents for large-scale simulations and optimizes agent distribution end-to-end via reverse optimization.

Result: Performance experiments, ablation studies, backtesting experiments, experiments on updated data and stock pools, scaling experiments, parameter sensitivity experiments, and visualization experiments show MASS's superiority.

Conclusion: The paradigm established by MASS can be expanded to other similar tasks, and its implementation is open-sourced.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MASS%3A+Multi-Agent+Simulation+Scaling+for+Portfolio+Construction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10278&send_immediately=true&force_search=false)

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [119] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting*

Main category: cs.AI

TL;DR: This paper proposes a new method to evaluate common sense in AI, particularly in large language models (LLMs), by considering the diversity among human judgments. It finds that most LLMs perform below the human median in individual common sense tasks and correlate modestly with real humans in simulating a hypothetical population.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the assumption that human common sense is homogeneous and emphasizes the importance of incorporating observed human diversity into AI evaluations.

Method: The proposed method measures the correspondence between a model's judgment and that of a human population, treating LLMs as independent survey respondents or simulators of a hypothetical population.

Result: Smaller, open-weight models outperform larger, proprietary models in both individual and population-based evaluations.

Conclusion: The study suggests that AI models should be adapted to different human collectivities with diverse knowledge bases.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Empirically+evaluating+commonsense+intelligence+in+large+language+models+with+large-scale+human+judgments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10309&send_immediately=true&force_search=false)

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [120] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink, Stephie Do, Kristofer Bengtsson, Sabino Francesco Roselli, Martin Fabian*

Main category: cs.AI

TL;DR: This work proposes generic constraint formulations for healthcare scheduling and compares SMT and MILP solvers, showing each excels at certain types of problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in healthcare scheduling with Satisfiability Modulo Theories (SMT).

Method: Proposing generic constraint formulations and comparing Z3 (SMT solver) and Gurobi (MILP solver) on academic and real-world inspired rostering problems.

Result: Each solver excels for certain types of problems; MILP performs better with highly constrained or infeasible problems, while SMT performs better with varied shifts and personnel.

Conclusion: SMT-based methods are promising for future research in personnel scheduling.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparative+Study+of+SMT+and+MILP+for+the+Nurse+Rostering+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10328&send_immediately=true&force_search=false)

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [121] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel, Michael Bowling, André Barreto, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh*

Main category: cs.AI

TL;DR: This paper introduces a new concept called plasticity, which measures how much an agent can be influenced by its past observations. It also explores the connection between plasticity and empowerment, suggesting that both need to be considered in agent design.


<details>
  <summary>Details</summary>
Motivation: To explore the foundational capacity of agents being influenced by past observations and how this relates to empowerment.

Method: Defining plasticity using a new information-theoretic quantity called the generalized directed information, which generalizes Massey's directed information.

Result: Plasticity is found to be the mirror of empowerment, and there is a tension between them in agent design.

Conclusion: Understanding plasticity, empowerment, and their relationship is crucial for comprehending agency.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Plasticity+as+the+Mirror+of+Empowerment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10361，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10361&send_immediately=true&force_search=false)

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [122] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell*

Main category: cs.AI

TL;DR: 提出了一种新的评估框架AXE，该框架不需要理想的真实解释进行比较，也不依赖于模型敏感性，提供了解释质量的独立度量，并通过比较基线来验证AXE，展示了它如何用于检测解释公平性。


<details>
  <summary>Details</summary>
Motivation: 当前的解释评估框架存在局限性，需要一种新的方法来评估局部特征重要性解释的质量。

Method: 提出了AXE框架，并通过比较基线来验证AXE，展示了它如何用于检测解释公平性。

Result: AXE框架能够独立地评估解释质量，并且可以检测解释公平性。

Conclusion: 提出了一种新的评估框架AXE，该框架不需要理想的真实解释进行比较，也不依赖于模型敏感性，提供了解释质量的独立度量。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Model+Explanations+without+Ground+Truth，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10399&send_immediately=true&force_search=false)

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [123] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee*

Main category: cs.AI

TL;DR: This study clarifies the difference between AI Agents and Agentic AI by offering a detailed taxonomy, application mapping, and challenge analysis. It compares their design philosophies and capabilities, analyzing their architectures, operational mechanisms, interaction styles, and autonomy levels.


<details>
  <summary>Details</summary>
Motivation: Clarify the distinction between AI Agents and Agentic AI, providing a roadmap for developing robust, scalable, and explainable AI systems.

Method: Provide a structured conceptual taxonomy, application mapping, and challenge analysis. Compare AI Agents and Agentic AI through architectural evolution, operational mechanisms, interaction styles, and autonomy levels.

Result: Identify unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions.

Conclusion: This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agents+vs.+Agentic+AI%3A+A+Conceptual+Taxonomy%2C+Applications+and+Challenge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10468，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10468&send_immediately=true&force_search=false)

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [124] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova*

Main category: cs.AI

TL;DR: This study examines the effectiveness of different prompting techniques in enhancing the adaptability of large language models in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of large language models as self-learning and reasoning agents in dynamic environments.

Method: Systematic evaluation of self-reflection, heuristic mutation, and planning as prompting techniques through experiments with various open-source language models.

Result: Larger models generally outperform smaller ones, but strategic prompting can close the performance gap. Advanced prompting techniques benefit smaller models on complex games, but less so for high-performing large models. Advanced reasoning methods yield variable outcomes, with some leading to significant improvements but others causing performance drops. Human-level emergent reasoning is not observed, and crucial areas like planning, reasoning, and spatial coordination remain limited.

Conclusion: Current-generation large language models have fundamental shortcomings that cannot be fully overcome by self-reflective prompting alone. The complexity of reasoning requires moving beyond static benchmarks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Deeper+Understanding+of+Reasoning+Capabilities+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10543&send_immediately=true&force_search=false)

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [125] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen, Xiaotian Song, Yanan Sun*

Main category: cs.LG

TL;DR: 提出了一种新的ANN-SNN转换方法LAS，实现了完全基于尖峰的大语言模型的无损转换，并在多个模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的ANN-SNN转换方法难以处理基于ANN的大语言模型中的极端激活异常值和不兼容的非线性操作。

Method: 提出了两种新型神经元来解决上述问题，并定制了尖峰等效的Transformer组件以确保完全尖峰转换且不失性能。

Result: 在六个语言模型和两个视觉语言模型上实现了无损转换，在OPT-66B上甚至提高了WSC任务的准确率2%。

Conclusion: 提出的LAS方法能够有效地实现大语言模型的无损尖峰转换，并在多个模型上展示了其优越性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LAS%3A+Loss-less+ANN-SNN+Conversion+for+Fully+Spike-Driven+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09659&send_immediately=true&force_search=false)

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [126] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian*

Main category: cs.LG

TL;DR: This paper presents a general and scalable method to adapt large language models for execution on noisy, low-precision analog hardware, achieving performance comparable to 4-bit weight and 8-bit activation baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying large language models on analog in-memory computing hardware which introduces fundamental challenges like noisy computations and strict input/output quantization constraints.

Method: Introducing a new training methodology that allows large language models to perform well on noisy, low-precision analog hardware.

Result: State-of-the-art models can maintain performance similar to 4-bit weight and 8-bit activation baselines on analog hardware, and can also be quantized for inference on low-precision digital hardware.

Conclusion: This work bridges the gap between high-capacity large language models and efficient analog hardware, providing a pathway towards energy-efficient foundation models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analog+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09663&send_immediately=true&force_search=false)

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [127] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu, Prathyush Poduval, Wenjun Huang, Yang Ni, Hanning Chen, Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出了一个公平图遗忘方法(FGU)，该方法在保证隐私和准确率的同时，能有效减少图遗忘过程中引入的偏见，确保公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的图遗忘方法主要集中在保持模型预测性能的同时删除用户信息，但当用户信息从模型中删除时，不同敏感组之间的预测分布往往会发生变化。并且图模型容易放大偏见，使得研究图遗忘中的公平性尤为重要。

Method: FGU方法通过在划分的小图上训练分片模型，从相应的子图中遗忘请求的数据，并在修改后的子图上重新训练分片模型来保证隐私。为了确保公平性，FGU采用双层去偏过程：首先在分片模型再训练中加入公平正则化器来实现分片级公平性，然后通过使所有分片模型对齐来最小化全局差异以实现全局级公平性。

Result: 实验结果表明，FGU在保证隐私和准确率的同时，还能实现优秀的公平性，并且对不同的遗忘请求具有鲁棒性，在各种数据分布下都能保持公平性和实用性。

Conclusion: 实验表明，FGU在保证隐私和准确率的同时，还能实现优秀的公平性，并且对不同的遗忘请求具有鲁棒性，在各种数据分布下都能保持公平性和实用性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enabling+Group+Fairness+in+Graph+Unlearning+via+Bi-level+Debiasing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09702&send_immediately=true&force_search=false)

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [128] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira, Fernanda Famá, Charalampos Kalalas, Paolo Dini*

Main category: cs.LG

TL;DR: This study examines the energy consumed during the federated learning process in AIoT scenarios, focusing on three energy-intensive processes: pre-processing, communication, and local learning. Two clustering-informed methods are proposed to group AIoT devices with similar label distributions, achieving high convergence rates while maintaining low energy consumption.


<details>
  <summary>Details</summary>
Motivation: The energy implications of federated learning within AIoT scenarios are often overlooked in the existing literature.

Method: Two clustering-informed methods are proposed to group AIoT devices with similar label distributions.

Result: These methods alleviate the heterogeneity often encountered in real-world distributed learning applications and maintain low energy consumption.

Conclusion: Our clustering strategies typically achieve high convergence rates while maintaining low energy consumption compared to other recent approaches.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Federated+Learning+for+AIoT+using+Clustering+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09704&send_immediately=true&force_search=false)

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [129] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos, Petros Maragos*

Main category: cs.LG

TL;DR: 研究深度形态神经网络（DMNNs），提出新的架构，在约束条件下成功训练DMNNs，发现形态层能加速梯度下降收敛。


<details>
  <summary>Details</summary>
Motivation: 研究深度形态神经网络（DMNNs），并探讨层间激活的重要性以及如何在约束条件下成功训练它们。

Method: 提出了几种新的架构，对参数有不同约束的深度形态神经网络（DMNNs）。

Result: 提出的网络可以成功训练，并且比线性网络更容易修剪；形态层显著加速了大批次梯度下降的收敛。

Conclusion: 尽管深度形态神经网络（DMNNs）具有固有的非线性，但层间的激活对于DMNNs至关重要。我们提出了几种新的架构，每个架构对参数有不同的约束。我们的提出的网络可以成功训练，并且比线性网络更容易修剪。我们首次在这样的约束下成功训练了DMNNs，尽管网络的泛化能力仍然有限。最后，我们提出了一种结合线性和形态层的混合网络架构，实证表明形态层显著加速了大批次梯度下降的收敛。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Deep+Morphological+Neural+Networks+as+Universal+Approximators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09710&send_immediately=true&force_search=false)

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [130] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: This paper discusses out-of-distribution (OOD) generalization and how intelligent systems can learn compositional structures from data. The authors argue that simply testing on an OOD setup is insufficient to confirm that an algorithm learns compositional structures, and they demonstrate this by exploring two tasks where common neural networks fail. They also introduce two new network architectures that perform well in OOD scenarios.


<details>
  <summary>Details</summary>
Motivation: To investigate the ability of intelligent systems to learn compositional structures from data and generalize out-of-distribution.

Method: Testing the performance of common neural networks (MLP, CNN, Transformer) and developing two novel network architectures with specific biases.

Result: Common neural networks fail in OOD scenarios, while the newly developed architectures perform well, but even with correct biases, an algorithm can still fail to learn the correct features for compositional generalization.

Conclusion: Learning compositional structures requires more than just good performance in OOD scenarios; identifying truly compositional features is crucial.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-distribution+generalisation+is+hard%3A+evidence+from+ARC-like+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09716&send_immediately=true&force_search=false)

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [131] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen, Ali Boyaci*

Main category: cs.LG

TL;DR: This study proposes a federated learning method to address data quality issues and demonstrates significant performance improvement on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Data quality issues like noisy labels, missing classes and imbalanced distributions significantly challenge the effectiveness of federated learning. Therefore, it is necessary to propose a method to address these problems.

Method: This study proposes a federated learning methodology that systematically addresses data quality issues including noise, class imbalance and missing labels. The approach enhances data integrity through adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation and robust federated model training.

Result: Experimental evaluations on benchmark datasets show significant improvements in federated model performance under varying noise and class imbalance conditions.

Conclusion: The proposed method effectively improves federated model performance especially macro-F1 score under different noise and class imbalance conditions. It provides a robust, scalable and privacy compliant solution for real-world federated learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+with+Confidence-Weighted+Filtering+and+GAN-Based+Completion+under+Noisy+and+Incomplete+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09733&send_immediately=true&force_search=false)

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [132] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang, Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: 提出了一种生成式的端到端求解器，用于黑盒组合优化问题，强调了样本效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒组合优化方法往往在样本效率和解决方案质量之间存在权衡。为了提高样本效率和解决方案质量，我们提出了这种方法。

Method: 我们从基于退火的算法中汲取灵感，将黑盒目标视为能量函数，并训练神经网络来建模相关的玻尔兹曼分布。通过调节温度，网络能够捕捉从高温度下的接近均匀分布到低温度下围绕全局最优解的尖峰分布，从而学习能量景观的结构并促进全局优化。

Result: 我们在具有有限和无限查询预算的具有挑战性的组合任务上验证了我们的方法，展示了与最先进的黑盒优化器竞争的性能。

Conclusion: 我们提出了一种生成式的端到端求解器，用于黑盒组合优化问题。该方法在NP问题上强调了样本效率和解决方案质量。通过将黑盒目标视为能量函数并训练神经网络来建模相关的玻尔兹曼分布，该网络从高温度下的接近均匀分布到低温度下围绕全局最优解的尖峰分布，从而学习能量景观的结构并促进全局优化。我们在具有有限和无限查询预算的具有挑战性的组合任务上验证了我们的方法，展示了与最先进的黑盒优化器竞争的性能。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Generative+Neural+Annealer+for+Black-Box+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09742&send_immediately=true&force_search=false)

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [133] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: A novel MARL framework integrating community structure, transferability, and active learning with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To capture flexible and abstract coordination patterns in time-evolving networks with latent community structures.

Method: Each agent belongs to multiple overlapping communities maintaining shared policy and value functions. Actor-critic algorithms exploit this structure.

Result: Supports transfer learning and active learning while providing convergence guarantees under linear function approximation.

Conclusion: This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Community-based+Multi-Agent+Reinforcement+Learning+with+Transfer+and+Active+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09756，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09756&send_immediately=true&force_search=false)

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [134] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei, Xueru Zhang*

Main category: cs.LG

TL;DR: This paper studies the impact of noisy and adversarially curated data on generative models under self-consuming retraining loops. It provides theoretical analysis and designs attack algorithms for competitive scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand how generative models evolve under self-consuming retraining loops with noisy and adversarially curated data, especially in competitive environments.

Method: Theoretical analysis and designing attack algorithms.

Result: Identified conditions for the robustness of the retraining process and demonstrated the effectiveness of the proposed algorithms on synthetic and real-world datasets.

Conclusion: Generative models can be significantly affected by noisy and adversarially curated data in self-consuming retraining loops, but certain conditions can ensure robustness.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Consuming+Generative+Models+with+Adversarially+Curated+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09768&send_immediately=true&force_search=false)

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [135] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington, Cornel Constantinescu*

Main category: cs.LG

TL;DR: This paper analyzes the checkpointing process in Large Language Models (LLMs) training and proposes an effective compression solution called Language Model Compressor (LMC) which improves compression performance while reducing processing time.


<details>
  <summary>Details</summary>
Motivation: To optimize the checkpointing process in LLMs training by reducing the volume of checkpoint data.

Method: Experimental analysis of checkpoint data to maximize lossless compression, leveraging byte-grouping and Huffman encoding.

Result: LMC outperforms BZ2 in compression performance with significantly less processing time. A 16-core parallel implementation achieves 2.78 GiB/s compression and 3.76 GiB/s decompression throughput.

Conclusion: The proposed LMC solution reduces CPU resource needs and allows for higher-frequency checkpoints.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lossless+Compression+for+LLM+Tensor+Incremental+Snapshots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09810&send_immediately=true&force_search=false)

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [136] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova, Stefan Eftimov, Bojan Ristov, Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: This study examines the effectiveness of various machine learning algorithms in predicting stroke risk with the aim to address challenges like class imbalance and missing data. The models, though accurate, show low sensitivity which limits their application in clinical settings.


<details>
  <summary>Details</summary>
Motivation: To develop more reliable and interpretable models for the early assessment of stroke risk due to its significant impact on global health.

Method: Using demographic, clinical, and lifestyle data from the Stroke Prediction Dataset, evaluating Logistic Regression, Random Forest, and XGBoost models while handling class imbalance and missing data.

Result: High accuracy was achieved by the models but sensitivity remains low, limiting practical clinical use.

Conclusion: Identifies influential predictive features and suggests strategies to improve machine learning-based stroke prediction.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+Stroke+Prediction+Models+Using+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09812&send_immediately=true&force_search=false)

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [137] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu*

Main category: cs.LG

TL;DR: 提出了一种新的指数梯度下降方法，用于对大型语言模型进行越狱攻击，该方法在成功率和效率上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，系统地理解它们对于提高其安全性和发挥其全部潜力至关重要。然而，现有的对抗性攻击方法可能不够有效。因此，需要一种更有效的技术来实现对LLMs的越狱。

Method: 开发了一种使用指数梯度下降和Bregman投影方法的内在优化技术，以确保优化的一热编码始终保持在概率单形内。

Result: 该技术在五个开源LLMs上进行了测试，并在四个公开可用的数据集上展示了其有效性。结果显示，该技术在成功率和效率方面都优于其他三种最先进的越狱技术。

Conclusion: 该技术在几个广泛使用的LLMs上成功地实现了越狱，与三种最先进的越狱技术相比，该技术的成功率更高且效率更高。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Attack+on+Large+Language+Models+using+Exponentiated+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09820&send_immediately=true&force_search=false)

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [138] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi, Gal Mishne*

Main category: cs.LG

TL;DR: This paper studies learning a Kronecker-structured product graph from smooth signals, proposing an alternating optimization scheme with theoretical convergence guarantees, and demonstrates superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To model diverse dependency structures in multi-way data using the more intricate Kronecker product instead of the commonly used Cartesian product.

Method: Proposing an alternating optimization scheme to tackle the non-convex problem of learning Kronecker-structured product graphs, with modifications for strong product graphs.

Result: The proposed method demonstrates efficacy and superior performance compared to existing methods through experiments on synthetic and real-world graphs.

Conclusion: Learning Kronecker-structured product graphs is pivotal for applying GSP to unknown domains with multi-way data, and the proposed alternating optimization scheme provides theoretical guarantees and practical advantages.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Kronecker-Structured+Graphs+from+Smooth+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09822&send_immediately=true&force_search=false)

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [139] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei*

Main category: cs.LG

TL;DR: This paper presents a new approach called Causal Predictive Optimization and Generation for optimizing the sales process in B2B businesses. It includes three layers: prediction layer with causal ML, optimization layer with constraint optimization and contextual bandit, and serving layer with Generative AI and feedback-loop. The authors detail its implementation and deployment in LinkedIn and share insights that can be applied to similar systems.


<details>
  <summary>Details</summary>
Motivation: Optimizing the sales process is crucial for the success of B2B businesses. The paper aims to introduce a principled approach to sales optimization and business AI.

Method: The approach includes three layers: 1) prediction layer with causal ML, 2) optimization layer with constraint optimization and contextual bandit, and 3) serving layer with Generative AI and feedback-loop.

Result: The system was implemented and deployed in LinkedIn, showing significant improvements over legacy systems.

Conclusion: The paper concludes by sharing learnings and insights from the implementation and deployment of the Causal Predictive Optimization and Generation system, which can be broadly applied to the field.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Predictive+Optimization+and+Generation+for+Business+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09847&send_immediately=true&force_search=false)

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [140] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj, Golrokh Mirzaei*

Main category: cs.LG

TL;DR: Integrating imaging and genomic data via a novel method that uses a heterogeneous bipartite graph representation learning, this study successfully classifies Alzheimer's disease into three stages and identifies key genes associated with each stage.


<details>
  <summary>Details</summary>
Motivation: To integrate imaging and genomic data for new insights into disease complexities, specifically focusing on Alzheimer's disease detection.

Method: Developing a novel heterogeneous bipartite graph representation learning featuring genes and images as distinct node types.

Result: Effectively classifying Alzheimer's disease into three stages and identifying significant genes for each classification group using a small dataset.

Conclusion: The proposed method has potential applications in radiogenomic-based disease classification beyond Alzheimer's.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Radiogenomic+Bipartite+Graph+Representation+Learning+for+Alzheimer%27s+Disease+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09848&send_immediately=true&force_search=false)

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [141] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang, Shun-Li Shang, Zi-Kui Liu, Wenrui Hao*

Main category: cs.LG

TL;DR: This paper extends zentropy theory into data science by proposing ZENN, which effectively learns from heterogeneous data and demonstrates superior performance in classification tasks and energy landscape reconstructions.


<details>
  <summary>Details</summary>
Motivation: The need to effectively learn from heterogeneous data sources with intrinsic disparities due to the rapid growth of data across various domains.

Method: Introducing intrinsic entropy to extend zentropy theory into the data science domain and proposing a zentropy-enhanced neural network (ZENN) that learns both energy and intrinsic entropy components.

Result: ZENN shows superior generalization capabilities and robustness, especially in predicting high-order derivatives, and is successfully applied to reconstruct the Helmholtz energy landscape of Fe3Pt.

Conclusion: The study introduces a novel approach for data-driven machine learning based on zentropy theory, demonstrating ZENN's versatility and robustness in handling complex, heterogeneous datasets.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZENN%3A+A+Thermodynamics-Inspired+Computational+Framework+for+Heterogeneous+Data-Driven+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09851，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09851&send_immediately=true&force_search=false)

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [142] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil, Katia Obraczka*

Main category: cs.LG

TL;DR: This paper introduces Chisme, a suite of protocols designed to address the challenges of implementing robust intelligence in the network edge with heterogeneous data distributions, episodic connectivity, and lack of infrastructure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing paradigms like federated learning and decentralized FL in connectivity and synchronization imposed by resource-constrained and infrastructure-less environments, as well as the limitations of gossip learning algorithms which are generally designed for homogeneous data distributions.

Method: Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. A data similarity heuristic is introduced to allow agents to infer affinity with each other using existing communication of model updates in decentralized FL and GL, which is leveraged to extend DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration.

Result: Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

Conclusion: Chisme addresses the challenges of implementing robust intelligence in the network edge with heterogeneous data distributions, episodic connectivity, and lack of infrastructure.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chisme%3A+Fully+Decentralized+Differentiated+Deep+Learning+for+Edge+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09854&send_immediately=true&force_search=false)

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [143] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku, Thomas L. Griffiths, Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 研究探讨了Transformer模型中两种学习模式（IWL和ICL）之间的关系，发现环境稳定性影响IWL和ICL的使用偏好，提示可靠性增强ICL的效果。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解Transformer模型中的两种学习模式（IWL和ICL）之间的相互作用，研究人员受到进化生物学中类似适应策略的启发，即基因编码和表型可塑性。

Method: 通过实验操作化环境的可预测性维度，并系统地调查其对Transformer模型中ICL/IWL平衡的影响。使用回归和分类任务来展示结果。

Result: 在高环境稳定性下，IWL被优先使用；在高提示可靠性下，ICL更有效。任务依赖性的学习动态变化揭示了初始IWL阶段后来转变为ICL主导的现象。

Conclusion: 研究结果表明，在不同的环境稳定性下，Transformer模型会偏向使用不同的学习模式。在高环境稳定性下，IWL被优先使用；而在高提示可靠性下，ICL更有效。此外，任务依赖性的学习动态变化也揭示了初始IWL阶段后来转变为ICL主导的现象。这些发现支持了相对成本假设，并强调了可预测性作为指导Transformer自适应策略的关键因素。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictability+Shapes+Adaptation%3A+An+Evolutionary+Perspective+on+Modes+of+Learning+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09855&send_immediately=true&force_search=false)

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [144] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei*

Main category: cs.LG

TL;DR: This paper introduces a new attribution approach using transformers to handle different types of data for marketing intelligence.


<details>
  <summary>Details</summary>
Motivation: To improve modern marketing intelligence by developing a more effective data-driven attribution method.

Method: A unified transformer-based attribution approach that can handle various types of data including member-level, aggregate-level, and external macro factors.

Result: The approach was implemented on a large scale at LinkedIn and showed significant impact.

Conclusion: The proposed method has broad applicability in the marketing and ad tech fields.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LiDDA%3A+Data+Driven+Attribution+at+LinkedIn，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09861&send_immediately=true&force_search=false)

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [145] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni, Filippo Leveni, Diego Stucchi, Luca Frittoli, Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA is a non-parametric change-detection algorithm that combines Kernel-QuantTree histogram and EWMA statistic for monitoring multivariate data streams online.


<details>
  <summary>Details</summary>
Motivation: To present a non-parametric change-detection algorithm that can control false alarms and detect changes in multivariate data streams.

Method: Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA)

Result: KQT-EWMA can control ARL_0 and achieve detection delays comparable to or lower than state-of-the-art methods.

Conclusion: KQT-EWMA can control ARL_0 while achieving detection delays comparable to or lower than state-of-the-art methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Change+Detection+in+Multivariate+data+streams%3A+Online+Analysis+with+Kernel-QuantTree，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2410.13778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2410.13778&send_immediately=true&force_search=false)

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [146] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: Large models are expensive to train and operate. BINGO introduces a method to identify and prune insignificant weights in one step, preserving accuracy while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Reduce the high cost of training and operating large machine learning models, making AI development more accessible.

Method: BINGO studies subsets of a neural network during training to assign a significance score to each weight, enabling one-shot pruning of insignificant weights.

Result: BINGO achieves accuracy-preserving pruning with lower computational requirements compared to existing methods.

Conclusion: BINGO offers a solution to make AI development more affordable by efficiently pruning models without sacrificing accuracy.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BINGO%3A+A+Novel+Pruning+Mechanism+to+Reduce+the+Size+of+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09864&send_immediately=true&force_search=false)

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [147] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, Aman Raj*

Main category: cs.LG

TL;DR: This paper surveys adversarial attacks targeting four modalities: text, image, video, and audio. It provides a view of the adversarial attack landscape in the multimodal world.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a practitioner-focused view on adversarial attacks in the multimodal world.

Method: Survey of adversarial attacks targeting all four modalities.

Result: A comprehensive summarization of the threat landscape in the multimodal world.

Conclusion: This survey is the first comprehensive summarization of the threat landscape in the multimodal world.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Attacks+in+Multimodal+Systems%3A+A+Practitioner%27s+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.03084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.03084&send_immediately=true&force_search=false)

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [148] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian*

Main category: cs.LG

TL;DR: This paper investigates the exploration-exploitation trade-off in large language models (LLMs), comparing their decision-making behavior with humans and MAB algorithms using multi-armed bandit tasks.


<details>
  <summary>Details</summary>
Motivation: To examine whether LLMs exhibit similar decision-making behavior to humans and can achieve comparable or superior performance.

Method: Using interpretable choice models to capture the E&E strategies of agents and investigating how explicit reasoning affects LLM decision-making through different prompting strategies and reasoning-enhanced models.

Result: Reasoning shifts LLMs towards more human-like behavior, showing similar levels of random and directed exploration as humans in simple tasks but struggling with adaptability in complex, non-stationary environments.

Conclusion: LLMs show promise as simulators of human behavior and automated decision-making tools but have limitations, especially in complex, changing environments.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparing+Exploration-Exploitation+Strategies+of+LLMs+and+Humans%3A+Insights+from+Standard+Multi-armed+Bandit+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09901&send_immediately=true&force_search=false)

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [149] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni, Guilherme Weigert Cassales, Bernhard Pfahringer, Albert Bifet, Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest是一种新的方法，专为流条件设计，可以无缝跟踪随时间演变的数据生成过程。实验证明它在效率上优于所有竞争对手，特别是在需要快速识别异常的应用场景如网络安全、欺诈和故障检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的离线异常检测方法在流上下文中需要重复访问内存中的数据，并且施加不切实际的假设；现有的在线异常检测方法通常无法解决这些问题，往往通过周期性重新训练来适应在线环境。

Method: 提出了一种名为Online-iForest的新方法，专为流条件设计，可以无缝跟踪随时间变化的数据生成过程。

Result: 实验验证表明，在真实世界的数据集上，Online-iForest的表现与在线替代方案相当，并且接近经过周期性重新训练的最先进的离线异常检测技术。

Conclusion: Online-iForest在效率方面始终优于所有竞争对手，使其成为快速识别异常的重要应用中的有前途的解决方案，如网络安全、欺诈和故障检测。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Isolation+Forest，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09593&send_immediately=true&force_search=false)

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [150] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang, LuFeng, Ruijia Liang*

Main category: cs.LG

TL;DR: 提出一种结合TCN、MLP和注意力机制的混合深度学习模型用于哈斯牛油果价格预测，实验显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于人们对健康食品的需求增加，农产品价格预测变得越来越重要。哈斯牛油果作为一种高价值作物，其价格受季节性、地区和天气等因素影响复杂波动。传统预测模型往往难以应对高度非线性和动态的数据。因此，研究提出了一个新的混合深度学习模型来解决这些问题。

Method: 研究使用了包含50,000多条记录的数据集，涵盖了2015年至2018年美国各地的哈斯牛油果销售情况，变量包括销售量、平均价格、时间、地区、天气和品种类型等。数据来自POS系统和哈斯牛油果委员会。研究采用了缺失值填补和特征归一化等预处理方法，然后训练并评估了所提出的TCN-MLP-注意力架构模型。

Result: 实验结果显示，所提出的TCN-MLP-注意力模型在预测性能上表现优异，RMSE为1.23，MSE为1.51，优于传统方法。

Conclusion: 研究提出了一种结合TCN、MLP和注意力机制的混合深度学习模型，用于预测哈斯牛油果的价格。实验结果表明，该模型在农业市场的时序预测方面表现出色，并提供了智能供应链管理和价格策略优化的有价值的见解。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Avocado+Price+Prediction+Using+a+Hybrid+Deep+Learning+Model%3A+TCN-MLP-Attention+Architecture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09907&send_immediately=true&force_search=false)

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [151] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu, Wei Zhang, Tiejun Li*

Main category: cs.LG

TL;DR: This paper investigates direct sampling of Euclidean diffusion models for manifold-constrained data and proposes two novel methods, Niso-DM and Tango-DM, to improve sampling accuracy.


<details>
  <summary>Details</summary>
Motivation: Investigate direct sampling of the Euclidean diffusion models for general manifold-constrained data instead of explicitly utilizing the structure of special manifolds.

Method: Niso-DM, which introduces non-isotropic noise along the normal direction to reduce scale discrepancies, and Tango-DM, which trains only the tangential component of the score function using a tangential-only loss function.

Result: Superior performance on distributions over various manifolds with complex geometries.

Conclusion: Our methods achieve superior performance on distributions over various manifolds with complex geometries.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+Euclidean+Diffusion+Generation+of+Manifold+Data+by+Mitigating+Score+Function+Singularity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09922&send_immediately=true&force_search=false)

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [152] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He*

Main category: cs.LG

TL;DR: This paper proposes a novel interactive continual learning paradigm named RiCL that leverages LLMs to learn new skills from real-time human feedback while retaining prior knowledge. The method addresses limitations of traditional continual learning approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional continual learning approaches suffer from limitations like reliance on static datasets with fixed labels and clean labels assumption, which do not hold in real-world scenarios where data is noisy and dynamic.

Method: RiCL incorporates three key components: a temporal consistency-aware purifier, an interaction-aware direct preference optimization strategy, and a noise-resistant contrastive learning module.

Result: Experiments show that RiCL outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods on two benchmark datasets with realistic noise patterns.

Conclusion: The proposed RiCL framework demonstrates effectiveness in learning from dynamic and noisy feedback, providing a promising direction for future research in interactive continual learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforced+Interactive+Continual+Learning+via+Real-time+Noisy+Human+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09925，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09925&send_immediately=true&force_search=false)

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [153] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden*

Main category: cs.LG

TL;DR: This research uses a fine-tuned large language model to analyze freeway crash data and identify crash causation factors like alcohol-impaired driving and speeding.


<details>
  <summary>Details</summary>
Motivation: To understand the factors contributing to traffic crashes and develop strategies to reduce their severity.

Method: Fine-tuning the Llama3 8B model using QLoRA on a dataset created from 226 traffic safety studies, then using zero-shot classification to identify crash causation.

Result: The model successfully identified primary crash causes and provided comprehensive explanations. It also showed practical applicability with high agreement from traffic safety researchers.

Conclusion: LLMs can comprehensively analyze crash causation and contribute to more effective traffic safety practices.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advanced+Crash+Causation+Analysis+for+Freeway+Safety%3A+A+Large+Language+Model+Approach+to+Identifying+Key+Contributing+Factors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09949&send_immediately=true&force_search=false)

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [154] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai, Jie Zhou, Yuxuan Cai, Qin Chen, Wen Wu, Xingjiao Wu, Xipeng Qiu, Liang He*

Main category: cs.LG

TL;DR: This paper introduces Long-CL, a novel framework for long-term continual learning inspired by human memory mechanisms, which addresses catastrophic forgetting and outperforms previous methods on two newly released benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of catastrophic forgetting in long-term continual learning with a large number of tasks.

Method: Proposes a task-core memory management strategy and a long-term memory consolidation mechanism.

Result: Outperforms previous state-of-the-art methods by 7.4% and 6.5% AP on the two constructed benchmarks.

Conclusion: The proposed Long-CL framework effectively mitigates catastrophic forgetting and improves performance in long-term continual learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Task-Core+Memory+Management+and+Consolidation+for+Long-term+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09952&send_immediately=true&force_search=false)

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [155] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim, Seulki Lee*

Main category: cs.LG

TL;DR: 提出了一种名为TransPL的新方法，用于解决无监督领域自适应（UDA）中的时间序列数据问题，通过代码转换矩阵显式建模不同域之间的时序转换和通道偏移，并提供可解释的伪标签生成。在四个时间序列UDA基准上进行的广泛分析验证了TransPL的有效性，其准确率和F1分数分别比最先进的伪标签方法提高了6.1%和4.9%。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列UDA伪标签策略无法捕捉时序模式和域间的通道偏移，导致次优的伪标签。

Method: 引入TransPL方法，利用向量量化（VQ）从时间序列片段中获取代码，并构建类和通道级别的代码转换矩阵，使用贝叶斯规则生成目标域的伪标签。

Result: TransPL在四个时间序列UDA基准上进行了测试，显示出显著优于现有伪标签方法的性能提升，同时提供了对领域适应过程的可解释洞察。

Conclusion: TransPL是一种有效且可解释的方法，能够显式建模时间序列UDA中的时序和通道偏移，适用于不同的UDA场景。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TransPL%3A+VQ-Code+Transition+Matrices+for+Pseudo-Labeling+of+Time+Series+Unsupervised+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09955&send_immediately=true&force_search=false)

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [156] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo, Bohui An, Zhongqi Lu*

Main category: cs.LG

TL;DR: This paper introduces FedRAG, a federated reinforcement learning framework that enhances performance by sharing approximated behavior metric-based state projection functions without exposing sensitive task-specific information.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of Federated Reinforcement Learning while protecting sensitive information.

Method: Proposing a method where clients learn a projection function of states and the central server aggregates these functions' parameters. This approach does not share sensitive task-specific information but still provides information gain for each client.

Result: Extensive experiments were conducted on the DeepMind Control Suite to demonstrate insightful results.

Conclusion: Sharing approximated behavior metric-based state projection functions is a promising way to enhance Federated Reinforcement Learning performance while ensuring privacy.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approximated+Behavioral+Metric-based+State+Projection+for+Federated+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09959&send_immediately=true&force_search=false)

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [157] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir, Shiva Razzagzadeh, Zeynab Rezaei*

Main category: cs.LG

TL;DR: A machine learning framework using Logistic Regression, KNN, and Random Forest is developed for heart disease prediction, showing that Random Forest outperforms others with 91% accuracy and 0.89 F1-score.


<details>
  <summary>Details</summary>
Motivation: To develop an effective machine learning-based system for heart disease prediction using the heart-disease dataset.

Method: Data preprocessing, model training using Logistic Regression, KNN, and Random Forest, hyperparameter tuning with GridSearchCV and RandomizedSearchCV.

Result: Random Forest classifier achieved the best performance with 91% accuracy and 0.89 F1-score, showing balanced performance across classes.

Conclusion: The proposed machine learning model has strong potential for aiding clinical decision-making but requires larger and more diverse datasets to improve generalizability.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Machine+Learning+Framework+for+Heart+Disease+Prediction%3A+Performance+Evaluation+and+Future+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09969&send_immediately=true&force_search=false)

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [158] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao, Zhuoran Xiao, Yihang Huang, Chenhui Ye, Yijia Feng, Liyu Cai, Jiang Chang, Fangkun Liu, Yin Xu, Dazhi He, Yunfeng Guan, Wenjun Zhang*

Main category: cs.LG

TL;DR: This paper proposes AI2MMUM, a scalable task-aware AI-air interface multi-modal universal model for 6G systems, demonstrating superior performance in various physical layer tasks.


<details>
  <summary>Details</summary>
Motivation: To design a universal model capable of processing multi-modal data and executing diverse air interface tasks for future wireless systems.

Method: Proposes AI2MMUM with LLM backbone, fine-tuning, fixed task keywords, learnable prefix prompts, frozen radio modality encoders, adapter layers, and lightweight task-specific heads.

Result: Achieves SOTA performance across five representative physical environment/wireless channel-based downstream tasks using WAIR-D and DeepMIMO datasets.

Conclusion: AI2MMUM demonstrates flexibility and effectiveness in performing various physical layer tasks according to subtle task instructions.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI2MMUM%3A+AI-AI+Oriented+Multi-Modal+Universal+Model+Leveraging+Telecom+Domain+Large+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10003&send_immediately=true&force_search=false)

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [159] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen, Shengbo Wang, Nian Si*

Main category: cs.LG

TL;DR: This paper studies distributionally robust average-reward reinforcement learning, proposing two algorithms with near-optimal sample complexity.


<details>
  <summary>Details</summary>
Motivation: Stable long-term performance in critical applications like robotics, operations research, and healthcare.

Method: Proposes two algorithms: one reduces the problem to a DR discounted MDP, the other introduces an anchoring state to stabilize the transition kernels.

Result: Both algorithms attain a sample complexity of O(|S||A|t_mix^2ε^-2) under certain conditions.

Conclusion: The first finite-sample convergence guarantee for DR average-reward reinforcement learning is achieved.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Complexity+of+Distributionally+Robust+Average-Reward+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10007，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10007&send_immediately=true&force_search=false)

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [160] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu*

Main category: cs.LG

TL;DR: This paper introduces ImagineBench, a new benchmark for evaluating offline RL algorithms that use both real and synthetic data from large language models.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning depends heavily on extensive real-world interaction data. Recent work shows that large language models can generate synthetic experience (imaginary rollouts) to master novel tasks, but progress in this field is hindered due to the lack of a standard benchmark.

Method: Introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts.

Result: Existing offline RL algorithms achieve only 35.44% success rate in hard tasks when using LLM-imaginary rollouts, compared to 64.37% when trained on real rollouts.

Conclusion: Simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, highlighting the need for algorithm advancements to better leverage LLM-imaginary rollouts.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImagineBench%3A+Evaluating+Reinforcement+Learning+with+Large+Language+Model+Rollouts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10010，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10010&send_immediately=true&force_search=false)

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [161] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito, Lysenko Artem, Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: This paper introduces a new normalization strategy for improving the stability of Quantum-classical Hybrid Machine Learning models used in predicting anti-cancer drug responses.


<details>
  <summary>Details</summary>
Motivation: To improve the stability of QHML models by addressing their sensitivity to data encoding, especially when dealing with small datasets in biomedical applications.

Method: Proposes a normalization function based on a moderated gradient version of tanh to transform neural network outputs, preventing concentration at extreme value ranges.

Result: The proposed method was tested on a dataset of gene expression and drug response measurements, showing improved prediction performance of QHML models over classical models when data was optimally normalized.

Conclusion: This research suggests potential advancements in biomedical data analysis using quantum computers.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+normalization+in+quantum-classical+hybrid+models+for+anti-cancer+drug+response+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10037&send_immediately=true&force_search=false)

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [162] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang*

Main category: cs.LG

TL;DR: This paper introduces a framework that improves circuit discovery methods by fully identifying and distinguishing logic gates, ensuring faithfulness, completeness, and sparsity of circuits.


<details>
  <summary>Details</summary>
Motivation: To address the incompleteness issue in standard circuit discovery methods caused by partially detecting OR gates.

Method: Introduces three types of logic gates (AND, OR, ADDER), decomposes circuits into combinations of these gates, and proposes a framework combining noising-based and denoising-based interventions.

Result: The framework successfully restores the faithfulness, completeness, and sparsity of circuits through extensive experiments.

Conclusion: Uncovered fundamental properties of logic gates and their contributions to language model functionalities.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Circuit+Completeness+in+Language+Models%3A+AND%2C+OR%2C+and+ADDER+Gates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10039&send_immediately=true&force_search=false)

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [163] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong*

Main category: cs.LG

TL;DR: This paper introduces Instance-Prototype Affinity Learning (IPAL), a novel approach for Non-Exemplar Continual Graph Learning (NECGL) using Prototype Contrastive Learning (PCL) to address the issue of catastrophic forgetting in Graph Neural Networks (GNN) without revisiting historical data.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of catastrophic forgetting in GNNs when learning new information, while avoiding memory explosion and privacy concerns associated with rehearsal-based techniques.

Method: Introduce IPAL which uses Topology-Integrated Gaussian Prototypes (TIGP) and Instance-Prototype Affinity Distillation (IPAD) to guide feature distributions and maintain task memory respectively. Also, include a Decision Boundary Perception (DBP) mechanism within PCL.

Result: Performance evaluations on four node classification benchmarks show that IPAL outperforms current state-of-the-art methods in balancing plasticity and stability.

Conclusion: The proposed NECGL framework, leveraging PCL, demonstrates improved performance in continual learning tasks for GNNs without the need for historical data.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Instance-Prototype+Affinity+Learning+for+Non-Exemplar+Continual+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10040&send_immediately=true&force_search=false)

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [164] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki, Mehedi Masud*

Main category: cs.LG

TL;DR: 提出了一种结合梯度提升模型与XAI技术的金融欺诈检测框架，实现了高准确率和良好可解释性的统一。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型虽然有较高的预测准确性，但缺乏透明性和可解释性，难以满足监管要求并获得利益相关者的信任。

Method: 使用XGBoost、LightGBM和CatBoost的叠加集成，并结合SHAP、LIME、PDP和PFI等XAI技术提高模型的透明性和可解释性。

Result: 模型在IEEE-CIS欺诈检测数据集上达到了99%的准确率和0.99的AUC-ROC分数，优于多个近期相关方法。

Conclusion: 结合梯度提升模型的叠加集成框架与XAI技术在金融欺诈检测中实现了高预测准确率与透明解释性的统一。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Financial+Fraud+Detection+Using+Explainable+AI+and+Stacking+Ensemble+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10050&send_immediately=true&force_search=false)

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [165] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng, Ying Zhang, Yuxuan Liang, Roger Zimmermann, Zhiwen Yu, Bin Guo*

Main category: cs.LG

TL;DR: This work explores how the multi-task distillation could be used to improve the unified modeling of depth estimation and scene segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: A joint modeling of depth estimation and scene segmentation tasks in intelligent transportation systems will reduce the requirement for both the storage and training efforts.

Method: self-adaptive distillation method and a trajectory-based distillation loss

Result: Our method achieves a clearly improvement.

Conclusion: Compared to the state-of-the-art solutions, our method achieves a clearly improvement.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JointDistill%3A+Adaptive+Multi-Task+Distillation+for+Joint+Depth+Estimation+and+Scene+Segmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10057&send_immediately=true&force_search=false)

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [166] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao*

Main category: cs.LG

TL;DR: This paper proposes ChronoSteer, a multimodal time series forecasting model that integrates large language models and time series foundation models via text-to-instruction transformation. It introduces a decoupled framework and a two-stage training strategy to handle data scarcity, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To leverage both temporal and textual information for better forecasting by combining large language models and time series foundation models.

Method: A decoupled framework where textual events are transformed into revision instructions to guide the output of time series foundation models. A two-stage training strategy using synthetic data is also devised.

Result: ChronoSteer outperforms unimodal models by 25.7% and surpasses the previous best multimodal method by 22.5% in prediction accuracy.

Conclusion: The integration of large language models and time series foundation models through text-to-instruction transformation shows great promise in multimodal time series forecasting.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChronoSteer%3A+Bridging+Large+Language+Model+and+Time+Series+Foundation+Model+via+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10083&send_immediately=true&force_search=false)

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [167] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfei Wang, Jun Luo*

Main category: cs.LG

TL;DR: This paper presents MiCo, a hierarchical language agent framework using large language models to solve the Online Dynamic Multidimensional Bin Packing problem in cloud VM scheduling, demonstrating strong performance in large-scale and fluctuating demand scenarios.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional methods in handling large-scale complexity and fluctuating demands in cloud VM scheduling.

Method: Proposes MiCo framework with two stages: Option Miner discovers non-context-aware strategies via LLMs, and Option Composer integrates them with contextual strategies.

Result: Achieves a 96.9% competitive ratio in large-scale scenarios with over 10,000 VMs, maintaining high performance under nonstationary requests and diverse configurations.

Conclusion: MiCo demonstrates effectiveness in complex and large-scale cloud environments through extensive real-world experiments.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Virtual+Machine+Scheduling+in+Cloud+Computing+through+Language+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10117&send_immediately=true&force_search=false)

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [168] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: A novel multitask Graph Transformer approach enhances molecular property prediction by using synthetic tasks generated from XGBoost models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating rule-based models into differentiable neural networks for molecular property prediction.

Method: Jointly training a Graph Transformer neural network on both experimental targets and synthetic targets derived from XGBoost models trained on molecular descriptors.

Result: Performance improvements were observed across all 19 molecular property prediction tasks, with the multitask Graph Transformer outperforming the XGBoost single-task learner for 16 out of 19 targets.

Conclusion: The proposed multitask Graph Transformer model, which uses synthetic tasks derived from XGBoost models, consistently improves performance across 19 molecular property prediction tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是All+You+Need+Is+Synthetic+Task+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10120&send_immediately=true&force_search=false)

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [169] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou, Shu Ding, ZeLin Li, Wei Wang*

Main category: cs.LG

TL;DR: This paper introduces the adaptability of local models and enhances the performance of the global model by improving the adaptability of local models.


<details>
  <summary>Details</summary>
Motivation: In federated learning, due to the heterogeneous data distributions over clients and data privacy, it is difficult to train local models to achieve a well-performed global model.

Method: Introduce the adaptability of local models and enhance the performance of the global model by improving the adaptability of local models. Formalize the property into the local training objective with a constraint and propose a feasible solution to train the local model.

Result: Extensive experiments on federated learning benchmarks demonstrate that our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.

Conclusion: Our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+the+Performance+of+Global+Model+by+Improving+the+Adaptability+of+Local+Models+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10125&send_immediately=true&force_search=false)

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [170] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le, Latif U. Khan, Choong Seon Hong*

Main category: cs.LG

TL;DR: This study presents FedAPC, a novel prototype-based federated learning framework that improves the generalization ability of the global model under domain heterogeneity by using prototype augmentation. The method enhances feature diversity and model robustness through aligning local features with global prototypes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of domain heterogeneity in Federated Learning which hinders the global model's convergence.

Method: FedAPC uses prototypes derived from augmented data mean features to capture richer representations and aligns local features with global prototypes.

Result: The framework outperforms state-of-the-art baselines on the Office-10 and Digits datasets, showing superior performance.

Conclusion: FedAPC effectively improves the generalization ability of the global model under domain heterogeneity in Federated Learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+on+Edge+Devices+with+Domain+Heterogeneity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10128&send_immediately=true&force_search=false)

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [171] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash, Nikhil Karamchandani, Avishek Ghosh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Near+Optimal+Best+Arm+Identification+for+Clustered+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10147&send_immediately=true&force_search=false)

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [172] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer introduces a novel Complex Multi-Head Attention (CMHA) that allows each attention head to independently model semantic and positional information in the complex plane. It also includes per-head Euler transformation and adaptive differential rotation mechanisms to enhance flexibility and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing transformer models in effectively integrating positional information while maintaining the flexibility of multi-head attention.

Method: ComplexFormer uses Complex Multi-Head Attention (CMHA) which models semantic and positional differences within the complex plane. It applies per-head Euler transformation and adaptive differential rotation mechanisms.

Result: Experiments on various tasks showed that ComplexFormer outperforms strong baselines like RoPE-Transformers in terms of performance, generation perplexity, and long-context coherence.

Conclusion: ComplexFormer provides a more expressive and adaptable attention mechanism, demonstrating strong parameter efficiency and superior performance across multiple tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ComplexFormer%3A+Disruptively+Advancing+Transformer+Inference+Ability+via+Head-Specific+Complex+Vector+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10222&send_immediately=true&force_search=false)

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [173] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique*

Main category: cs.LG

TL;DR: This paper introduces QuXAI, a framework based on Q-MEDLEY, for explaining feature importance in hybrid quantum-classical machine learning (HQML) models with quantized feature encoding and classical learning. It shows that Q-MEDLEY effectively explains influential aspects and noise in HQML models and performs well in classical validation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust global and local explainability approaches for HQML models employing quantized feature encoding followed by classical learning.

Method: Creating HQML models with quantum feature maps, using Q-MEDLEY which combines feature-based inferences while preserving the quantum transformation stage and visualizing attributions.

Result: Q-MEDLEY successfully identifies influential classical aspects and noise in HQML models and performs competitively against established XAI techniques in classical validation settings.

Conclusion: QuXAI improves the interpretability and reliability of HQML models, fostering greater confidence and safe/responsible use of quantum-enhanced AI technology.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuXAI%3A+Explainers+for+Hybrid+Quantum+Machine+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10167&send_immediately=true&force_search=false)

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [174] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li, Libing Chen, Yin Tang*

Main category: cs.LG

TL;DR: 提出Alinear模型，该模型在时间序列预测任务中表现出色且参数量极小，挑战了大模型更优的传统观念。


<details>
  <summary>Details</summary>
Motivation: 质疑时间序列预测领域中大模型参数量呈指数增长的必要性，探索轻量化模型的可能性。

Method: 引入适应性分解机制和渐进频率衰减策略，不依赖注意力机制实现稳定预测。

Result: Alinear在七个基准数据集上表现优于大规模模型，仅使用不到其1%的参数量。

Conclusion: 此研究揭示了趋势和季节成分的重要性随数据特征变化而非固定模式的事实，建议向更高效的时间序列建模范式转变。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Scaling+Law+Apply+in+Time+Series+Forecasting%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10172&send_immediately=true&force_search=false)

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [175] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde, Priyadarshini P. Pai, Shashishekar P. Adiga, K. Subramanya Mayya, Yongbeom Seo, Myungsoo Hwang, Heeyoung Go, Changmin Park*

Main category: cs.LG

TL;DR: This study generates synthetic SEM images with known defect distributions to train and test object detection models for EUV patterning defect detection, finding YOLOv8 to be the most effective at detecting smaller defects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of defect-annotated quality data with good representation of smaller defects in EUV pattering, which hinders the deployment of deep learning-based defect detection models in semiconductor fabrication lines.

Method: Artificially generating SEM images with known defect distributions and autonomously annotating them, then applying state-of-the-art object detection models to evaluate their performance.

Result: YOLOv8 achieved the highest mean average precision of 96%, surpassing EfficientNet (83%) and SSD (77%). It also demonstrated reliable detection of smaller defects and showed promising results in real SEM data testing.

Conclusion: The YOLOv8 model shows the best performance in detecting smaller defects using both synthetic and real SEM data.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defect+Detection+in+Photolithographic+Patterns+Using+Deep+Learning+Models+Trained+on+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10192&send_immediately=true&force_search=false)

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [176] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu, Ziming Liu, Jeff Gore*

Main category: cs.LG

TL;DR: 研究了表示重叠对神经缩放定律的影响，发现表示重叠是LLMs中观察到的神经缩放定律的重要机制。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）中的神经缩放定律的起源，即随着模型规模增大，损失会按照幂律减少。

Method: 通过构建一个玩具模型来研究模型大小与损失之间的缩放关系，并分析了四种开源LLMs。

Result: 在弱叠加情况下，当只有最频繁出现的特征被表示且没有干扰时，损失随模型大小的缩放取决于潜在特征频率；如果特征频率遵循幂律，则损失也会如此。而在强叠加情况下，所有特征都被表示但相互重叠，损失则与模型维度成反比。

Conclusion: 表示重叠是观察到的神经缩放定律的重要机制。这些见解有望激发新的训练策略和模型架构，以更少的计算量和参数实现更好的性能。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Superposition+Yields+Robust+Neural+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10465&send_immediately=true&force_search=false)

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [177] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero, José Omar Chelotti, Luciano Sebastián Martinez-Rau, Leandro Vignolo, Martín Pires, Julio Ricardo Galli, Leonardo Luis Giovanini, Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: This work introduces a deep neural network that combines acoustic and inertial signals to monitor feeding behavior in grazing cattle more accurately.


<details>
  <summary>Details</summary>
Motivation: Efficient herd management and resource utilization in grazing cattle require monitoring feeding behavior, which can also help detect metabolic problems and animal discomfort.

Method: A deep neural network based on the fusion of acoustic and inertial signals, including convolutional, recurrent, and dense layers, was developed to automatically extract features from each signal.

Result: The model achieved an F1-score value of 0.802, surpassing state-of-the-art machine learning methods by 14%. Feature-level fusion proved superior to data and decision-level fusion by at least 0.14 based on the F1-score metric.

Conclusion: This study demonstrates the effectiveness of using multiple sensor signals and deep neural networks for improving the accuracy of feeding behavior recognition in cattle.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+multi-head+deep+fusion+model+for+recognition+of+cattle+foraging+events+using+sound+and+movement+signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10198，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10198&send_immediately=true&force_search=false)

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [178] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu*

Main category: cs.LG

TL;DR: We introduce a new method called Parallel Scaling (ParScale) that increases model's parallel computation during both training and inference time. This method is more inference-efficient than parameter scaling or inference-time scaling.


<details>
  <summary>Details</summary>
Motivation: To find a more efficient way to scale language models.

Method: Applying diverse and learnable transformations to the input, executing forward passes of the model in parallel, and dynamically aggregating the outputs.

Result: A model with P parallel streams is similar to scaling the parameters by O(logP) while showing superior inference efficiency. ParScale can use up to 22x less memory increase and 6x less latency increase compared to parameter scaling that achieves the same performance improvement.

Conclusion: The new scaling law potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parallel+Scaling+Law+for+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10475&send_immediately=true&force_search=false)

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [179] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo, Alireza Moradi*

Main category: cs.LG

TL;DR: 提出了一种新的跨域知识转移框架来提高大型语言模型在时间序列预测中的性能，并在真实世界的数据集上进行了验证，结果显示该方法显著优于没有辅助信息的基准模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在自然语言任务之外的应用越来越广泛，需要建立最佳实践来利用其能力。

Method: 提出了一个系统地向大型语言模型注入结构化时间信息的新框架以提高其预测准确性。

Result: 该方法在真实世界的时间序列数据集上的表现明显优于没有辅助信息的基准模型。

Conclusion: 知识转移策略有潜力弥合大型语言模型与特定领域的预测任务之间的差距。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Informed+Forecasting%3A+Leveraging+Auxiliary+Knowledge+to+Boost+LLM+Performance+on+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10213，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10213&send_immediately=true&force_search=false)

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [180] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich*

Main category: cs.LG

TL;DR: This paper introduces a new router-based architecture to generate high-quality synthetic data for fine-tuning large language models in function calling tasks without real user interaction data.


<details>
  <summary>Details</summary>
Motivation: The lack of real-world task-specific data and privacy concerns make it difficult to train models directly on real user interaction data.

Method: A router-based architecture that uses domain resources, text-to-text and vision-to-text language models to generate synthetic training data.

Result: Evaluation on real user queries shows improved function classification accuracy and API parameter selection.

Conclusion: The proposed method sets new benchmarks for function calling tasks by outperforming traditional approaches.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RouteNator%3A+A+Router-Based+Multi-Modal+Architecture+for+Generating+Synthetic+Training+Data+for+Function+Calling+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10495&send_immediately=true&force_search=false)

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [181] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa*

Main category: cs.LG

TL;DR: Introduce MASSV, a two-phase approach that adapts small language models into multimodal drafters for speculative decoding in vision-language models, improving accepted length and inference speed.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding accelerates language model inference but faces challenges when applied to vision-language models due to small drafters lacking architectural components for processing visual inputs and mismatched token predictions with target models.

Method: MASSV connects the vision encoder of the target VLM to the draft model via a trainable projector and uses self-distilled visual instruction tuning to align token predictions.

Result: Experiments show MASSV increases accepted length by up to 30% and achieves end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

Conclusion: MASSV offers a scalable and architecture-compatible method to accelerate current and future vision-language models.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MASSV%3A+Multimodal+Adaptation+and+Self-Data+Distillation+for+Speculative+Decoding+of+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10526，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10526&send_immediately=true&force_search=false)

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [182] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload is a high-throughput inference engine that improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x by embedding speculative decoding into offloading.


<details>
  <summary>Details</summary>
Motivation: Existing systems offload model weights to CPU memory due to limited GPU memory, which incurs substantial I/O overhead and leads to underutilized GPU cores and low impact on performance from GPU memory.

Method: SpecOffload embeds speculative decoding into offloading and unlocks latent GPU resources for storing and executing a draft model used for speculative decoding.

Result: SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.

Conclusion: SpecOffload is an efficient inference engine that significantly improves GPU core utilization and inference throughput for LLM inference on resource-constrained devices.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpecOffload%3A+Unlocking+Latent+GPU+Capacity+for+LLM+Inference+on+Resource-Constrained+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10259，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10259&send_immediately=true&force_search=false)

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [183] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Lajos Hanzo*

Main category: cs.LG

TL;DR: This paper investigates the charging scheduling problem of Electric Buses using Deep Reinforcement Learning. It proposes a Hierarchical DRL approach to decouple the original MDP into a high-level SMDP and multiple low-level MDPs. An HDDQN-HER algorithm is proposed to solve decision problems at different temporal resolutions. Numerical experiments using real-world data evaluate the algorithm's performance.


<details>
  <summary>Details</summary>
Motivation: To address the charging scheduling problem of Electric Buses using Deep Reinforcement Learning with the aim of minimizing charging costs.

Method: Proposes a Hierarchical DRL approach including a high-level SMDP and multiple low-level MDPs. Uses the HDDQN-HER algorithm to handle experience replay buffers for both levels of agents.

Result: Proves that the flat policy constructed by combining high-level and low-level policies performs as well as the optimal policy of the original MDP. Numerical experiments are conducted with real-world data.

Conclusion: The proposed Hierarchical DRL approach effectively solves the charging scheduling problem of Electric Buses with minimized charging costs.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Electric+Bus+Charging+Schedules+Relying+on+Real+Data-Driven+Targets+Based+on+Hierarchical+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10262&send_immediately=true&force_search=false)

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [184] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习中的数据重建攻击方法，该方法克服了现有方法的限制，能够高效地恢复大规模数据批次。


<details>
  <summary>Details</summary>
Motivation: 现有的数据重建攻击存在局限性，如依赖于对客户端数据分布的假设或当批量大小超过几十个样本时效率显著下降。

Method: 利用全连接层的新几何视角来制作恶意模型参数，从而实现对客户端训练数据的重建。

Result: 在图像和表格数据集上的大量实验表明，所提出的攻击方法优于现有方法，并且可以成功地重建比以前技术大两个数量级的数据批次。

Conclusion: 提出的方法能够完美恢复分类任务中任意大小的数据批次，优于现有方法，并且可以处理比之前技术大两个数量级的数据批次。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cutting+Through+Privacy%3A+A+Hyperplane-Based+Data+Reconstruction+Attack+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10264&send_immediately=true&force_search=false)

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [185] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent*

Main category: cs.LG

TL;DR: A deep learning model integrating multiple data sources outperforms existing systems in European high-resolution precipitation forecasting.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of radar-only deep learning models with short forecast lead times.

Method: The model efficiently integrates multiple data sources including radar, satellite, and physics-based numerical weather prediction (NWP), featuring a compact architecture.

Result: Accurate forecasts with robust uncertainty quantification through consistent probabilistic maps.

Conclusion: The presented deep learning model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models, setting a new standard for high-resolution precipitation forecasting in Europe.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RainPro-8%3A+An+Efficient+Deep+Learning+Model+to+Estimate+Rainfall+Probabilities+Over+8+Hours，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10271&send_immediately=true&force_search=false)

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [186] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer, Sascha Gaudlitz, Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: 本文探讨了一种基于尖峰时间的Hebbian学习规则，揭示了它与梯度下降和镜像下降的关系，并证明了其最终能识别出突触前神经元中活动最高的那个。


<details>
  <summary>Details</summary>
Motivation: 探索基于精确尖峰时间的学习规则，因为关于这方面的知识相对较少。

Method: 通过将Hebbian基于尖峰的时间依赖可塑性规则与概率单纯形上的自然损失函数的嘈杂梯度下降联系起来。

Result: 发现了与嘈杂镜像下降的内在联系，并证明了学习规则的行为。

Conclusion: 该学习规则最终识别出突触前神经元中活动最高的一个。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spike-timing-dependent+Hebbian+learning+as+noisy+gradient+descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10272&send_immediately=true&force_search=false)

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [187] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Dusit Niyato*

Main category: cs.LG

TL;DR: This paper proposes a Hierarchical Deep Reinforcement Learning (HDRL) approach called Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E) to optimize electric bus charging schedules. The approach addresses uncertainties and real-world complexities, and experiments show its superior performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Optimizing electric bus charging schedules remains a critical challenge due to uncertainties in travel time, energy consumption, and fluctuating electricity prices, as well as the need for efficient multi-timescale decision-making and scalability for large fleets.

Method: A Hierarchical Deep Reinforcement Learning approach is proposed, reformulating the original Markov Decision Process into two augmented MDPs. A novel HDRL algorithm, DAC-MAPPO-E, is introduced to solve these MDPs. Enhancements are made at both high and low decision levels to address scalability challenges.

Result: Extensive experiments with real-world data demonstrate the superior performance and scalability of DAC-MAPPO-E in optimizing electric bus fleet charging schedules.

Conclusion: The proposed HDRL approach effectively optimizes electric bus charging schedules under various uncertainties and real-world complexities, showing promising results in terms of performance and scalability.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Electric+Bus+Charging+Scheduling+with+Uncertainties+Using+Hierarchical+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10296，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10296&send_immediately=true&force_search=false)

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [188] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma, Youcheng Sun, Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: This paper presents FeRA, a novel defense mechanism for detecting backdoor attacks in federated learning systems with heterogeneous, non-IID data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting backdoor attacks in federated learning due to the diverse, non-IID data produced by resource-constrained edge devices.

Method: FeRA uses cross-client attention over internal feature representations to compute an anomaly score based on representation reconstruction errors, distinguishing between benign and malicious clients.

Result: FeRA shows robust performance across various FL scenarios, reducing backdoor attack success rates while maintaining high accuracy on the main task.

Conclusion: FeRA is a model-agnostic, attack-agnostic defense mechanism that does not require labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defending+the+Edge%3A+Representative-Attention+for+Mitigating+Backdoor+Attacks+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10297&send_immediately=true&force_search=false)

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [189] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi*

Main category: cs.LG

TL;DR: A novel anomaly detection method named PIF is proposed, which combines adaptive isolation methods with preference embedding.


<details>
  <summary>Details</summary>
Motivation: To detect anomalies with respect to structured patterns.

Method: Embed data in a high dimensional space and use PI-Forest to compute an anomaly score.

Result: PIF outperforms other state-of-the-art methods on both synthetic and real datasets.

Conclusion: PI-Forest is superior in measuring arbitrary distances and isolating points in the preference space.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PIF%3A+Anomaly+detection+via+preference+embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10441&send_immediately=true&force_search=false)

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [190] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的图对比学习方法NML-GCL，通过引入可学习的负度量网络和双层优化目标的联合训练方案，有效解决了假阴性问题，提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 图对比学习(GCL)经常受到假阴性问题的影响，这会降低下游任务的性能。现有的解决假阴性问题的方法通常依赖于人工先验知识，仍然导致GCL的结果不理想。

Method: 提出了一种名为NML-GCL的新方法，该方法利用可学习的负度量网络(NMN)构建负度量空间，并提出了一种具有双层优化目标的联合训练方案。

Result: 提出的NML-GCL方法在理论上和实验上都表现出了优越性。

Conclusion: 提出了一个名为NML-GCL的新方法，该方法利用可学习的负度量网络(NMN)构建负度量空间，在这个空间中，基于锚节点的距离，可以更好地将错误否定与真实否定区分开来。此外，为了克服NML缺乏显式监督信号的问题，提出了一种具有双层优化目标的联合训练方案，该方案隐式地利用自监督信号来迭代优化编码器和负度量网络。理论分析和在广泛使用的基准数据集上的大量实验验证了所提出方法的优越性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Negative+Metric+Learning+for+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10307&send_immediately=true&force_search=false)

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [191] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri*

Main category: cs.LG

TL;DR: Introduces SEAL, a NAS-based method for data-incremental learning that reduces forgetting and improves accuracy while keeping model size low.


<details>
  <summary>Details</summary>
Motivation: Balancing plasticity and stability in incremental learning settings, especially in resource-constrained environments.

Method: A NAS-based framework tailored for data-incremental learning, which adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric.

Result: SEAL outperforms previous methods in reducing forgetting and enhancing accuracy with a smaller model size.

Conclusion: SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEAL%3A+Searching+Expandable+Architectures+for+Incremental+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10457&send_immediately=true&force_search=false)

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [192] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou, Shi Pu*

Main category: cs.LG

TL;DR: This paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under practical assumptions, analyzes its convergence, and shows it outperforms existing methods in wall-clock convergence time across various scenarios.


<details>
  <summary>Details</summary>
Motivation: The need for leveraging distributed data without central control, enhancing scalability and privacy in decentralized optimization.

Method: Introduces ADSGD, analyzes Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool, and establishes convergence result without assuming bounded data heterogeneity.

Result: ADSGD converges under computation-delay-independent step sizes and performs better than existing methods in wall-clock convergence time across various scenarios.

Conclusion: ADSGD is simple, efficient in memory and communication, and resilient to communication and computation delays, making it suitable for real-world decentralized learning tasks.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asynchronous+Decentralized+SGD+under+Non-Convexity%3A+A+Block-Coordinate+Descent+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10322，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10322&send_immediately=true&force_search=false)

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [193] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna*

Main category: cs.LG

TL;DR: This paper proposes ALERT, a method that detects feature distribution changes and triggers model re-training to improve AI model performance in wireless networks.


<details>
  <summary>Details</summary>
Motivation: In real deployment, feature distribution changes may degrade the performance of AI models and lead to undesired behaviors.

Method: ALERT includes three components: representation learning (using MLP), statistical testing (using Kolmogorov-Smirnov and Population Stability Index tests), and utility assessment (using a new function).

Result: The proposed method outperforms ten standard drift detection methods on two wireless network use cases.

Conclusion: The proposed ALERT method can effectively detect feature distribution changes and trigger model re-training in wireless network use cases.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Representation+Learning+Approach+to+Feature+Drift+Detection+in+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10325，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10325&send_immediately=true&force_search=false)

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [194] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: This work focuses on enabling real-world autonomous systems to adapt to environmental changes using improved deep RL methods.


<details>
  <summary>Details</summary>
Motivation: Conventional RL methods struggle to adapt when conditions change as they assume a stationary environment and do not account for changes after training.

Method: Deep reinforcement learning (RL) is used with proposed strategies for efficient online adaptation.

Result: The proposed strategies allow RL agents to efficiently adapt their behavior during deployment without catastrophically forgetting useful prior knowledge.

Conclusion: This dissertation demonstrates that efficient online adaptation requires two key capabilities: prioritized exploration and sampling strategies and selective preservation of prior knowledge.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Adaptation+of+Reinforcement+Learning+Agents+to+Sudden+Environmental+Change，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10330，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10330&send_immediately=true&force_search=false)

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [195] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera, Luigi Loreti, Giovanni Todeschini, Alessio Fumagalli, Francesco Regazzoni*

Main category: cs.LG

TL;DR: 提出了一种理论模型来研究随机分类器集合的行为，发现存在一个最优温度使得分类效果最佳，且此性质不依赖于具体教师分类器和分类器数量。


<details>
  <summary>Details</summary>
Motivation: 研究随机系统如何从微观无序过渡到宏观有序的现象，特别是在数据科学和机器学习领域中的广泛应用。

Method: 通过引入基于分类损失的能量Gibbs测度定义加权随机分类器集合，并分析其集体行为的理论模型。

Result: 证明了存在一个特定的温度参数使得分类达到最优，且这种最优性具有普适性；实验验证了这一现象在MNIST数据集上的有效性。

Conclusion: 研究证明了随机分类器集合在特定温度下可以实现最优分类，且这种最优性与教师分类器和随机分类器的数量无关。实验表明该现象在高质量数据集上具有重要意义，并通过物理类比揭示了其自组织特性。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergence+of+Structure+in+Ensembles+of+Random+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10331&send_immediately=true&force_search=false)

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [196] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares, Liyuan Liu*

Main category: cs.LG

TL;DR: This tutorial introduces discrete variational autoencoders (VAEs) with categorical distributed latent variables, providing a rigorous derivation and training recipe.


<details>
  <summary>Details</summary>
Motivation: To introduce discrete latent spaces which are becoming popular for many data modalities like text.

Method: Using encoder and decoder networks, focusing on a categorical distribution for the latent space.

Result: A concrete training method and an example implementation of discrete VAEs.

Conclusion: Discrete VAEs offer a promising approach for probabilistic unsupervised learning with discrete latent variables.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Introduction+to+Discrete+Variational+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10344&send_immediately=true&force_search=false)

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [197] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama, Valdir Grassi Jr*

Main category: cs.LG

TL;DR: Evaluate the effectiveness of Specialized Multi-Task Optimizers (SMTOs) and whether equally weighted tasks can achieve competitive results.


<details>
  <summary>Details</summary>
Motivation: Addressing critiques suggesting SMTO results were influenced by poor hyperparameter optimization and lack of regularization.

Method: Extensive empirical evaluation of SMTOs on complex multi-task problems.

Result: SMTOs perform well compared to uniform loss; fixed weights can achieve competitive performance compared to SMTOs.

Conclusion: Uniform loss can perform similarly to SMTOs in some instances.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uniform+Loss+vs.+Specialized+Optimization%3A+A+Comparative+Analysis+in+Multi-Task+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10347&send_immediately=true&force_search=false)

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [198] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe*

Main category: cs.LG

TL;DR: Introduces a real-time AI-scribing method called Facts that extracts clinical info and generates more accurate and concise notes.


<details>
  <summary>Details</summary>
Motivation: AI-scribing solutions for healthcare currently rely on one-shot or few-shot prompts which can lead to long notes with more hallucinations and misrepresentation of clinicians' intent, posing risks to patient safety.

Method: Introduce a method called Facts which extracts salient clinical information in real-time and uses it recursively to generate the final note.

Result: The FactsR method results in more accurate and concise notes and opens up new use cases within real-time decision support.

Conclusion: This approach places the clinician-in-the-loop of note generation and improves patient safety.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FactsR%3A+A+Safer+Method+for+Producing+High+Quality+Healthcare+Documentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10360&send_immediately=true&force_search=false)

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [199] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra, Lizhen Lin*

Main category: cs.LG

TL;DR: This paper presents Schrier-Coset Graph Propagation (SCGP), an efficient method for improving long-range message passing in graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing methods to solve the over-squashing problem in GNNs, such as graph rewiring and bottleneck-resistant architectures like Cayley and expander graphs, have introduced scalability issues.

Method: SCGP uses Schreier-coset embeddings to enrich node features, which helps to avoid over-squashing and improves the network's expressive capacity.

Result: SCGP has been shown to achieve performance on par with or exceeding expander graph and rewired GNN baselines, particularly excelling in processing hierarchical and modular graph structures.

Conclusion: Schrier-Coset Graph Propagation (SCGP) introduces a new way to improve long-range message passing in graph neural networks without increasing memory usage.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Schreier-Coset+Graph+Propagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10392&send_immediately=true&force_search=false)

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [200] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding, Choon Hwai Yap, Kangjun Ji, Simão Castro*

Main category: cs.LG

TL;DR: AneuG is a novel two-stage VAE-based IA mesh generator that can generate realistic intracranial aneurysm pouch shapes and parent vessels with controlled morphological measurements.


<details>
  <summary>Details</summary>
Motivation: The lack of a large IA image dataset and the limitations of existing shape generation methods motivated the development of AneuG.

Method: AneuG uses a two-stage approach where it first generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes, then generates parent vessels conditioned on GHD tokens.

Result: AneuG demonstrated the ability to generate realistic IA shapes with controlled morphological measurements, which is useful for understanding shape variations and flow simulations.

Conclusion: AneuG provides a new method for generating realistic IA shapes with controlled morphological measurements, which can benefit disease progression studies and flow simulation research.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-Stage+Generative+Model+for+Intracranial+Aneurysm+Meshes+with+Morphological+Marker+Conditioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10407，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10407&send_immediately=true&force_search=false)

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [201] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp, Christopher MacLellan, Erik Harpstead, Kenneth Koedinger*

Main category: cs.LG

TL;DR: This study investigates whether human learners' rapid learning from tens of examples is due to their ability to use multiple specialized learning mechanisms. The research compares reinforcement learning to a symbolic rule induction approach and finds that decomposing learning into multiple mechanisms significantly improves data efficiency, aligning it with human learning.


<details>
  <summary>Details</summary>
Motivation: To understand why human learners can learn rapidly from fewer examples compared to data-driven deep learning.

Method: Ablation analysis of inductive human learning simulations in online tutoring environments comparing reinforcement learning to a symbolic rule induction approach.

Result: Decomposing learning into multiple distinct mechanisms significantly improves data efficiency, bringing it in line with human learning. The decomposition has a greater impact on efficiency than the distinction between symbolic and subsymbolic learning alone.

Conclusion: Integrating multiple specialized learning mechanisms may be key to aligning data-driven machine learning with human learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposed+Inductive+Procedure+Learning%3A+Learning+Academic+Tasks+with+Human-Like+Data+Efficiency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10422&send_immediately=true&force_search=false)

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [202] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer, Eran Malach*

Main category: cs.LG

TL;DR: This paper demonstrates that neural networks trained by gradient descent have fundamental limitations for distribution-free learning, and introduces a new theoretical framework (adc) to highlight these limitations.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between gradient-based optimization of parametric models and optimization of random features, and to investigate the fundamental limitations of distribution-free learning in neural networks trained by gradient descent.

Method: The paper studies the relationship between gradient-based optimization and optimization of random features, introduces a new theoretical framework called average probabilistic dimension complexity (adc), and proves its relationship with statistical query dimension.

Result: If a parametric model can be learned using mini-batch stochastic gradient descent without distribution assumptions, then the target function can be approximated using a polynomial-sized combination of random features.

Conclusion: The paper concludes that there are fundamental limitations to distribution-free learning in neural networks trained by gradient descent, emphasizing the necessity of data distribution assumptions.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Power+of+Random+Features+and+the+Limits+of+Distribution-Free+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10423&send_immediately=true&force_search=false)

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [203] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang, Wenwen Qiang, Zeen Song, Changwen Zheng, Hui Xiong*

Main category: cs.LG

TL;DR: 提出了一种名为Learning to Think (L2T)的信息论强化微调框架，用于大型语言模型，使模型能够以较少的token实现最优推理。该框架通过量化每次交互中的信息增益来优化模型，并通过理论分析和实验证明了其在不同任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了推理效果与计算效率之间的权衡，导致不必要的长推理链和浪费token的问题。

Method: 提出了一种信息论强化微调框架L2T，它将每次查询-响应交互视为多集元组的层次化会话，并提出通用密集过程奖励来量化每次交互的信息增益，无需额外标注或任务特定评估器。还提出了基于PAC-Bayes界限和Fisher信息矩阵快速估计此奖励的方法。

Result: 在各种推理基准和基础模型上进行的实证结果显示，L2T在不同任务中都具有优势，提高了推理效果和效率。

Conclusion: L2T是一种有效的框架，可以显著减少计算复杂性并提高估计准确性，同时优化模型更新以实现更有效的推理。

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Think%3A+Information-Theoretic+Reinforcement+Fine-Tuning+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10425，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10425&send_immediately=true&force_search=false)

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [204] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase, Katherine Haynes, Lander Ver Hoef, Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: This paper explores using score-based diffusion models to nowcast clouds and precipitation over zero to three hours. It compares different types of diffusion models and finds that a residual correction diffusion model performs best.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for simulating clouds and precipitation are challenging due to sub-grid parameterizations. Early machine learning methods often produced blurry forecasts.

Method: The authors experimented with three types of diffusion models: standard score-based diffusion model, residual correction diffusion model, and latent diffusion model.

Result: Diffusion models were able to advect, generate, and decay clouds, including convective initiation, using only past infrared satellite imagery. The residual correction diffusion model performed best.

Conclusion: Score-based diffusion models show promise for nowcasting clouds and precipitation, particularly in preserving high-resolution features and enabling ensemble generation with calibrated spread.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Score-based+diffusion+nowcasting+of+GOES+imagery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10432&send_immediately=true&force_search=false)

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [205] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: This paper discusses the limitations of conventional experimental methods for deriving models of gas turbine engines and proposes an approach using data collected from standard engine operation under closed-loop control. It estimates rotor dynamics, maps autonomous dynamics into an optimally constructed Koopman eigenfunction space, and designs a globally optimal nonlinear feedback controller and a Kalman estimator. The results show that the Koopman-based controller outperforms other controllers in reference tracking and disturbance rejection.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in deriving physics-based models of gas turbine engines which require performance characteristics that are not always available and necessitate many simplifying assumptions.

Method: Employing identification techniques based on data collected from standard engine operation under closed-loop control, estimating rotor dynamics using sparse identification of nonlinear dynamics, mapping autonomous part of dynamics into an optimally constructed Koopman eigenfunction space, and designing a globally optimal nonlinear feedback controller and a Kalman estimator in the eigenfunction space.

Result: The Koopman-based controller outperformed other benchmark controllers in both reference tracking and disturbance rejection under sea-level and varying flight conditions.

Conclusion: The proposed method using data collected from standard engine operation under closed-loop control and the Koopman framework is effective in improving the performance of gas turbine engines.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identification+and+Optimal+Nonlinear+Control+of+Turbojet+Engine+Using+Koopman+Eigenfunction+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10438&send_immediately=true&force_search=false)

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [206] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu*

Main category: cs.LG

TL;DR: This paper introduces NCDPO, a new framework that reformulates diffusion policy as a noise-conditioned deterministic policy to improve sample efficiency and final performance in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion policies can generate sub-optimal trajectories and catastrophic failures due to the sub-optimal and limited coverage of demonstration data. RL-based fine-tuning faces challenges in adapting PPO to diffusion models because of the computational intractability of action likelihood estimation during denoising.

Method: NCDPO reformulates diffusion policy as a noise-conditioned deterministic policy, enabling tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.

Result: NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforms existing methods in sample efficiency and final performance across diverse benchmarks, and shows robustness to the number of denoising timesteps.

Conclusion: NCDPO improves the performance and sample efficiency of diffusion policies in robotics and game scenarios.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-tuning+Diffusion+Policies+with+Backpropagation+Through+Diffusion+Timesteps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10482&send_immediately=true&force_search=false)

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [207] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim, Sol A Kim, Geonhyeong Kim, Enver Menadjiev, Chanwoo Lee, Seongwook Chung, Nari Kim, Jaesik Choi*

Main category: cs.LG

TL;DR: Introduce a universal XAI framework named PnPXAI that supports diverse data modalities and neural network models in a Plug-and-Play manner, which can automatically detect model architectures, recommend applicable explanation methods, and optimize hyperparameters for optimal explanations.


<details>
  <summary>Details</summary>
Motivation: Existing XAI frameworks suffer from several limitations such as limited flexibility, restricted number of supported XAI methods, and sub-optimal recommendations of explanations.

Method: Develop a universal XAI framework named PnPXAI that supports diverse data modalities and neural network models in a Plug-and-Play manner.

Result: The framework is validated through user surveys and showcases its versatility across various domains, including medicine and finance.

Conclusion: PnPXAI addresses the limitations of existing XAI frameworks and provides a more flexible, adaptable, and optimized solution for real-world applications.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PnPXAI%3A+A+Universal+XAI+Framework+Providing+Automatic+Explanations+Across+Diverse+Modalities+and+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10515&send_immediately=true&force_search=false)

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [208] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato*

Main category: cs.LG

TL;DR: A novel family of value function decomposition models called QFIX is proposed to enhance the performance of cooperative multi-agent reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limited representation capabilities or are unnecessarily complex.

Method: QFIX expands the representation capabilities of prior models through a thin 'fixing' layer.

Result: QFIX outperforms previous methods, learns more stably, and performs better than QPLEX.

Conclusion: The introduction of QFIX provides a simpler and more effective solution for value function decomposition in cooperative multi-agent reinforcement learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fixing+Incomplete+Value+Function+Decomposition+for+Multi-Agent+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10484，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10484&send_immediately=true&force_search=false)

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [209] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark*

Main category: cs.LG

TL;DR: Introduces Neural Thermodynamic Laws (NTL), a framework that reveals new insights into LLM training dynamics by showing how thermodynamic concepts emerge from river-valley loss landscapes.


<details>
  <summary>Details</summary>
Motivation: To understand the laws underlying large language models beyond neural scaling laws.

Method: Demonstrates emergence of thermodynamic quantities and principles under river-valley loss landscape assumptions.

Result: Provides intuitive guidelines for designing learning rate schedules.

Conclusion: Introduces a new framework, NTL, offering fresh perspectives on LLM training dynamics.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Thermodynamic+Laws+for+Large+Language+Model+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10559&send_immediately=true&force_search=false)

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


### [210] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar, Barnabas Poczos, Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a new method for generating 3D molecular graphs that integrates pharmacophore modeling with 3D generative techniques, showing better performance than traditional methods in drug design.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules is a major challenge in drug discovery, especially for novel targets without structural or functional data. Pharmacophore modeling can capture the key features needed for molecular bioactivity.

Method: PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation which uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.

Result: PharmaDiff outperforms ligand-based drug design methods in matching 3D pharmacophore constraints and achieves higher docking scores across a range of proteins in structure-based drug design without requiring target protein structures.

Conclusion: PharmaDiff provides a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pharmacophore-Conditioned+Diffusion+Model+for+Ligand-Based+De+Novo+Drug+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10545，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10545&send_immediately=true&force_search=false)

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [211] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain*

Main category: cs.LG

TL;DR: Use AI and wearable device data to predict how air pollution affects an individual's health.


<details>
  <summary>Details</summary>
Motivation: Air pollution harms health and climate change makes it worse. New data collection methods and AI advancements offer opportunities to improve healthcare.

Method: Integrate wearable device data with environmental exposure data to train an AI model using adversarial autoencoders and transfer learning.

Result: AI model accurately predicts health responses to pollution and adapts well to real-world user data.

Conclusion: This new workflow can help monitor and predict personalized health responses to pollution exposure.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+AI-driven+framework+for+the+prediction+of+personalised+health+response+to+air+pollution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10556&send_immediately=true&force_search=false)

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [212] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu, Qilong Wu, Lingjuan Lyu, Shibei Xue*

Main category: cs.CR

TL;DR: A sybil-based virtual data poisoning attack is proposed to reduce costs in federated learning.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of federated learning to poisoning attacks by malicious adversaries and reduce the costs of achieving effective attacks.

Method: Generating sybil nodes to amplify the poisoning model's impact and developing a virtual data generation method based on gradient matching to reduce neural network computational complexity.

Result: The method outperforms other attack algorithms in simulation.

Conclusion: This paper proposes a novel sybil-based virtual data poisoning attack that effectively reduces the cost of poisoning federated learning.

Ai: [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sybil-based+Virtual+Data+Poisoning+Attacks+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09983&send_immediately=true&force_search=false)

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>
