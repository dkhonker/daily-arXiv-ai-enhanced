<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 83]
- [cs.CL](#cs.CL) [总数: 114]
- [cs.CR](#cs.CR) [总数: 63]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个结合AI和计算心理学的多层面框架，通过端到端开发流程建立了从预测模型到交互式心理分析系统的完整解决方案，包括基准测试、Transformer微调、LLM开发以及微服务部署。


<details>
  <summary>更多</summary>
  
**动机:** 弥合孤立预测建模与交互式心理分析系统之间的差距，通过计算手段建模和理解复杂的人类心理状态。

**方法:** 1) 使用经典机器学习技术建立四个心理学数据集的基础性能基准；2) 微调最先进的Transformer模型，解决回归任务中的数值不稳定性和资源限制下的训练挑战；3) 使用参数高效技术微调生成式大语言模型作为交互式"人格大脑"；4) 将预测和生成模型架构部署为可扩展的微服务生态系统。

**结果:** 成功稳定了基于Transformer的情感计算回归模型，在标准方法失败的情况下显示出有意义的预测性能；开发了可复现的大规模AI研究方法论。

**结论:** 该工作展示了从研究到部署的完整流程，整合了预测分析与生成对话，为计算心理学和人机交互的未来研究提供了实用模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Component+AI+Framework+for+Computational+Psychology%3A+From+Robust+Predictive+Modeling+to+Deployed+Generative+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21720&send_immediately=true&force_search=false)

**原文摘要:** The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [2] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda, Takehiro Takayanagi*

**主要类别:** cs.AI

**AI概要:** PREFINE是一个无需参数更新或直接用户反馈的个性化故事生成框架，通过伪用户代理和用户特定评分标准实现个性化文本生成


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM在个性化文本生成方面存在挑战，传统方法依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私问题

**方法:** 构建伪用户代理从用户交互历史中生成用户特定评分标准，通过该代理基于定制标准进行批判和精炼输出

**结果:** 在PerDOC和PerMPST数据集上评估显示，PREFINE在自动评估中获得更高胜率和统计显著分数，且不损害一般故事质量

**结论:** 该方法在故事生成中有效，并具有扩展到对话系统、教育和推荐等更广泛应用的潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PREFINE%3A+Personalized+Story+Generation+via+Simulated+User+Critics+and+User-Specific+Rubric+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21721&send_immediately=true&force_search=false)

**原文摘要:** While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [3] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang, Herbert Woisetscläger*

**主要类别:** cs.AI

**AI概要:** SIGN方法通过引入轻量级结构指导多智能体命名游戏，相比无约束自然语言实现了5.8倍更快的收敛速度和更高的协议一致性


<details>
  <summary>更多</summary>
  
**动机:** 现实AI系统中多智能体交互时可能产生不一致的约定，导致协调失败，特别是在协作编码和分布式规划等需要可靠通信的应用中

**方法:** 引入Schema-Induced Games for Naming (SIGN)，一种通过轻量级结构指导约定形成的命名游戏方法

**结果:** 相比无约束自然语言通信，SIGN方法实现了更快的收敛速度，协议一致性最高提升5.8倍

**结论:** 最小化结构可以作为多智能体协调的有效控制机制，该方法的应用潜力超出了命名游戏的范畴

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SIGN%3A+Schema-Induced+Games+for+Naming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21855&send_immediately=true&force_search=false)

**原文摘要:** Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [4] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

**主要类别:** cs.AI

**AI概要:** 研究发现解码器自回归语言模型在知识密集型任务中存在能力上限，参数规模扩大无法显著提升准确率，尽管损失函数持续下降。


<details>
  <summary>更多</summary>
  
**动机:** 探究解码器自回归语言模型在不同规模下对知识密集型任务的能力表现，以及参数缩放对性能的影响。

**方法:** 系统评估OPT和Pythia模型家族（70M-30B参数），分析损失函数和准确率的关系，并进行注意力干预实验。

**结果:** 知识检索任务准确率几乎无改善，数学基准测试准确率稳定在19-20%，而算术等程序性任务则呈现常规缩放模式。注意力扰动导致性能灾难性崩溃。

**结论:** 对于使用OPT和Pythia架构的知识密集型应用，超过1-2B参数的缩放几乎无法带来准确率提升，这为资源分配决策提供了量化依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Capability+Ceilings+in+Autoregressive+Language+Models%3A+Empirical+Evidence+from+Knowledge-Intensive+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21866&send_immediately=true&force_search=false)

**原文摘要:** We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [5] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi, Chuanyu Qin, Shipeng Song, Man Luo*

**主要类别:** cs.AI

**AI概要:** 该论文针对大语言模型在几何视觉推理任务中表现不佳的问题，开发了包含详细推理链的GeoThoughts数据集，并基于此训练了GeoThought-MLLM模型，显著提升了几何推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在文本数学推理上表现良好，但在几何视觉推理任务中性能显著下降，主要因为几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集缺乏规模性、多样性和显式推理轨迹。

**方法:** 开发GeoThoughts数据集（包含6,243样本的Geo-Thought-6K和10,834样本的Geo-Thought-Augmented-10K），每个样本包含视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此数据集训练GeoThought-MLLM多模态模型。

**结果:** GeoThought-MLLM模型在几何任务中优于现有基准测试，表明使用Chain-of-Thought数据集训练能提升模型在域内和域外设置下的几何推理能力。

**结论:** 通过分析错误案例发现，错误主要源于数学概念误解或空间判断错误，通过调用思维链（CoT）纠正这些错误后，模型能产生正确答案，验证了方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GeoThought%3A+A+Dataset+for+Enhancing+Mathematical+Geometry+Reasoning+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21881&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [6] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

**主要类别:** cs.AI

**AI概要:** 本研究将生成流网络(GFlowNets)应用于三个图优化问题：旅行商问题、最小生成树和最短路径问题，通过训练学习采样与奖励函数成比例的解决方案，在基准测试中能够找到最优解。


<details>
  <summary>更多</summary>
  
**动机:** 探索生成模型在组合优化问题中的应用，特别是利用GFlowNets的学习策略来解决传统算法复杂度固定的问题，实现计算的可扩展性。

**方法:** 使用轨迹平衡损失训练GFlowNets，顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。在不同节点数量的多种图配置上进行测试。

**结果:** 生成的解决方案与经典算法（Dijkstra最短路径、Kruskal生成树、TSP精确求解器）的结果匹配。训练收敛性取决于问题复杂度，随着图规模增大，需要的训练回合数增加。

**结论:** 生成模型可以通过学习策略解决组合优化问题，主要优势是计算可扩展性。GFlowNets通过训练分摊计算，在足够计算资源下可扩展到经典精确方法不可行的大规模问题实例。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+through+Generation%3A+Applying+GFlowNets+to+Structured+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21886&send_immediately=true&force_search=false)

**原文摘要:** This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [7] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi, Xiaoqi Tan*

**主要类别:** cs.AI

**AI概要:** 该论文研究了在部分q^π-可实现性框架下强化学习的计算复杂性，证明了在该设置下学习ε-最优策略是计算困难的，包括NP-hardness和指数级下界。


<details>
  <summary>更多</summary>
  
**动机:** 研究在弱于q^π-可实现性但强于q*-可实现性的线性函数逼近框架下的计算复杂性，这是一个函数逼近自然出现的实用模型。

**方法:** 通过从δ-Max-3SAT和δ-Max-3SAT(b)问题归约到GLinear-κ-RL（贪婪策略）和SLinear-κ-RL（softmax策略）实例来建立计算复杂性结果。

**结果:** 证明了在参数化贪婪策略集下是NP-hard的，在softmax策略集下存在指数级下界（除非NP=RP），表明即使扩展策略集超越最优策略，计算困难仍然存在。

**结论:** 在部分q^π-可实现性框架下，通常无法获得积极的计算结果，这与生成访问模型下的q^π-可实现性形成对比。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Hardness+of+Reinforcement+Learning+with+Partial+%24q%5E%CF%80%24-Realizability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21888，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21888&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [8] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo, Nikola Tankovic*

**主要类别:** cs.AI

**AI概要:** 本文研究小型开源模型在电商意图识别任务中的可行性，通过QLoRA微调和后训练量化技术，使10亿参数的Llama 3.2模型达到与GPT-4.1相当的99%准确率，同时大幅降低计算资源需求。


<details>
  <summary>更多</summary>
  
**动机:** 商用大型语言模型在电商等专业领域部署时面临高计算成本、延迟和运营费用的问题，需要寻找资源效率更高的替代方案。

**方法:** 使用量化低秩适应(QLoRA)在合成的多语言电商查询数据集上微调10亿参数Llama 3.2模型，然后应用后训练量化技术创建GPU优化(GPTQ)和CPU优化(GGUF)版本。

**结果:** 专用1B模型达到99%准确率，与GPT-4.1性能相当。GPTQ减少41%显存使用但推理速度下降82%，GGUF在CPU上实现18倍推理吞吐量提升和90%以上内存消耗减少。

**结论:** 经过适当优化的小型开源模型是领域特定应用的更合适替代方案，能以极低的计算成本实现最先进的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performance+Trade-offs+of+Optimizing+Small+Language+Models+for+E-Commerce，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21970&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [9] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang, Mengfei Li, Shuai Shao*

**主要类别:** cs.AI

**AI概要:** 提出DSA方法解决LLM模拟调查响应时的分布偏差问题，通过两阶段微调学习分布变化而非拟合训练数据，显著减少真实数据需求并提高准确性


<details>
  <summary>更多</summary>
  
**动机:** 现有零样本方法存在提示敏感性和低准确性问题，传统微调方法容易过拟合训练集分布，无法实现比训练集更准确的模拟结果

**方法:** Distribution Shift Alignment (DSA)两阶段微调方法，对齐输出分布和不同背景下的分布变化，学习分布变化规律而非单纯拟合训练数据

**结果:** 在五个公共调查数据集上持续优于其他方法，将真实数据需求减少53.48-69.12%，在准确性、鲁棒性和数据节省方面表现优异

**结论:** DSA方法能提供比训练数据更接近真实分布的结果，证明了在调查模拟中的有效性和效率，为大规模数据收集提供了更优解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distribution+Shift+Alignment+Helps+LLMs+Simulate+Survey+Response+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21977&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [10] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang, Jiayu Liu, Xin Lin, Zhiyuan Ma, Shangzi Xue, Tong Xiao, Qi Liu, Yee Whye Teh, Enhong Chen*

**主要类别:** cs.AI

**AI概要:** 这篇论文对数学应用题(MWP)求解研究进行了系统性综述，从人类认知视角分析了AI模型在模拟人类推理能力方面的进展，包括问题理解、逻辑组织、联想记忆、批判性思维和知识学习五个关键认知能力。


<details>
  <summary>更多</summary>
  
**动机:** 数学应用题作为AI领域的基础研究课题，虽然经历了从规则方法到深度学习再到大语言模型的发展，但缺乏系统性的分类综述和当前发展趋势的讨论，需要从人类认知角度全面回顾相关研究。

**方法:** 总结了MWP求解的5个关键认知能力，回顾了近10年两种主流MWP模型（神经网络求解器和基于LLM的求解器），重新运行了所有代表性MWP求解器并在5个主流基准上进行了统一性能比较。

**结果:** 论文提供了过去十年有影响力的MWP研究的全面分析，从人类推理认知角度进行了整合性比较，展示了AI模型在模拟人类认知能力方面的进展。

**结论:** 这项综述首次从人类推理认知视角系统分析了MWP研究，希望为AI推理领域的进一步研究提供启发，相关资源已在GitHub上开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+of+Intelligence%3A+Review+of+Math+Word+Problems+from+Human+Cognition+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21999&send_immediately=true&force_search=false)

**原文摘要:** Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [11] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang, Chao Huang*

**主要类别:** cs.AI

**AI概要:** LightAgent是一个移动GUI代理基础模型，通过设备-云协作解决移动设备上小模型性能不足和大模型部署成本高的问题，在AndroidLab基准测试中表现接近大模型且显著降低云成本。


<details>
  <summary>更多</summary>
  
**动机:** 移动GUI代理面临困境：真正在设备上的小模型（4B或更小）性能不足，而能力强的模型（7B以上）要么太大无法移动部署，要么成本过高（如仅限云的闭源MLLMs）。

**方法:** 提出LightAgent解决方案，通过设备-云协作利用设备模型的成本效益和云模型的高能力；通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型；集成高效长推理机制在有限资源下利用历史交互；默认在设备上执行，仅通过实时复杂度评估将挑战性子任务升级到云端。

**结果:** 在在线AndroidLab基准测试和多样化应用上的实验显示，LightAgent匹配或接近更大模型的性能，同时显著降低云成本。

**结论:** LightAgent成功解决了移动GUI代理的困境，通过创新的设备-云协作架构实现了性能与成本的有效平衡，为移动平台上的智能代理系统提供了可行的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightAgent%3A+Mobile+Agentic+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22009&send_immediately=true&force_search=false)

**原文摘要:** With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [12] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen, Joseph Ternasky, Aaron Ontoyin Yin, Xianling Mu, Fuat Alican, Yigit Ihlamur*

**主要类别:** cs.AI

**AI概要:** LLM-AR：一个结合大语言模型和概率逻辑推理的框架，通过将LLM生成的启发式规则转化为可解释的概率规则，用于创业公司成功预测，在准确性和可解释性方面表现优异


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型具备模式识别和推理能力，但其准确性不稳定限制了在高风险决策应用中的采用，特别是在风险投资领域需要可靠预测的需求

**方法:** 提出LLM-AR管道，受神经符号系统启发，将LLM生成的启发式规则转化为ProbLog自动推理引擎执行的概率规则，并采用迭代策略进化循环结合关联规则挖掘来逐步优化预测规则

**结果:** 在未见数据上达到59.5%的精确率和8.7%的召回率，精确率是随机基线的5.9倍，同时保持决策路径完全可解释

**结论:** 该框架具有可解释性和超参数可调性，展示了扩展到其他领域的潜力，为高风险决策应用提供了可靠的AI辅助工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-AR%3A+LLM-powered+Automated+Reasoning+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22034&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [13] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo, Han Hou, Will Dabney, Edgar Y. Walker*

**主要类别:** cs.AI

**AI概要:** 该研究通过在元强化学习中整合自监督预测编码模块，成功学习了更紧凑、可解释的贝叶斯最优信念状态表示，显著提升了在部分可观测环境中的表示学习效果和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 元强化学习虽然能获得接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态，这种表示效率低下限制了智能体的适应性和泛化能力。

**方法:** 受神经科学中预测编码理论和深度强化学习中辅助预测目标的启发，将自监督预测编码模块整合到元强化学习中，通过状态机模拟验证方法有效性。

**结果:** 相比传统元强化学习，带预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地近似贝叶斯最优信念状态；在需要主动信息搜索的挑战性任务中，只有带预测模块的方法能成功学习最优表示和策略。

**结论:** 预测学习可以作为在部分可观测环境中导航的智能体进行有效表示学习的指导原则，更好的表示学习带来了改进的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictive+Coding+Enhances+Meta-RL+To+Achieve+Interpretable+Bayes-Optimal+Belief+Representation+Under+Partial+Observability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22039&send_immediately=true&force_search=false)

**原文摘要:** Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [14] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini, Braz Izaias da Silva Junior*

**主要类别:** cs.AI

**AI概要:** 该论文应用SpecC方法学对PCM-to-PWM转换器进行系统级硬件/软件协同设计，通过建模和探索找到满足实时约束的HW/SW分区方案，在降低全硬件方案成本的同时避免高端处理器纯软件实现的高昂费用。


<details>
  <summary>更多</summary>
  
**动机:** 研究系统级硬件/软件协同设计方法在PCM-to-PWM转换器（Class-D音频放大器核心）中的应用价值，探索如何在满足实时性能约束的同时优化成本效益。

**方法:** 采用SpecC方法学进行建模和系统级探索，使用系统级估算和快速功能仿真来评估不同的硬件/软件映射方案。

**结果:** 尽管设计复杂度适中，但研究结果证明了系统级协同设计在提供早期架构洞察、快速验证以及可行的成本/性能权衡方面的价值。

**结论:** 系统级硬件/软件协同设计方法能够有效支持早期设计决策，在满足实时约束的前提下实现成本优化，为类似嵌入式系统设计提供了有价值的参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HW%2FSW+Co-design+of+a+PCM%2FPWM+converter%3A+a+System+Level+Approach+based+in+the+SpecC+Methodology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22046&send_immediately=true&force_search=false)

**原文摘要:** We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [15] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

**主要类别:** cs.AI

**AI概要:** 该论文提出从预测准确性转向因果解释的智能概念，引入能量结构化因果模型（ESCMs）作为可干预的因果表示框架，使机器学习系统能够进行机制层面的外科手术式编辑。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习虽然实现了最先进的预测性能，但其内部表示缺乏因果语义，无法对特定机制进行精确干预和编辑，需要重新定义智能为构建和修正可验证解释的能力。

**方法:** 提出计算解释的概念，并实例化为能量结构化因果模型（ESCMs），其中机制表示为约束（能量函数或向量场）而非显式输入输出映射，干预通过对这些约束进行局部手术来实现。

**结果:** ESCMs在温和条件下恢复了标准SCM语义，提供了结构因果原则LAP和ICM的具体实例化，并分析了经验风险最小化导致表示断裂和纠缠的问题。

**结论:** 该研究为追求理解而不仅仅是预测的系统提供了因果推理的形式化语言，将智能重新定义为在批评下构建解释的能力，为可解释和可干预的AI系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Error-Centric+Intelligence+II%3A+Energy-Structured+Causal+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22050&send_immediately=true&force_search=false)

**原文摘要:** Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [16] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee, Niraj K. Jha, Jonathan D. Cohen, Thomas L. Griffiths, Hongjing Lu, Diana Marculescu, Ashiqur Rasul, Keshab K. Parhi*

**主要类别:** cs.AI

**AI概要:** 论文提出AI发展的新方向：从当前耗能巨大的大型语言模型转向轻量级、领域特定的多模态智能体，这些智能体能够在动态环境中进行推理、规划和决策，同时实现能效提升1000倍以上的硬件支持。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI模型（如GPT-4）存在高能耗（50-60 GWh训练成本）、幻觉问题以及无法在关键应用领域部署的局限性，而人脑仅消耗20W功率，需要开发更高效、更智能的AI系统。

**方法:** 提出开发轻量级领域特定多模态模型，这些模型能够利用实时数据和先验知识进行推理、规划和决策，并支持持续学习和进化能力。同时需要重新设计硬件以实现超过当前技术水平1000倍的能效提升。

**结果:** 论文构建了未来AI系统的愿景框架，提出了从大数据训练的大型模型向节能高效的专业智能体转变的技术路线图。

**结论:** 下一代AI应该向轻量级、领域特定、高能效的多模态智能系统发展，这需要硬件和软件技术的协同创新，以实现比现有技术高1000倍的能效提升，从而支持在动态不确定环境中的实时推理和决策能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Domain-Specific+Artificial+Intelligence+Models+and+Agents%3A+Pathways+and+Paradigms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22052&send_immediately=true&force_search=false)

**原文摘要:** The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [17] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen, Xinni Zhang, Yifei Zhang, Yangning Li, Henry Peng Zou, Chunyu Miao, Weizhi Zhang, Xue Liu, Philip S. Yu*

**主要类别:** cs.AI

**AI概要:** 这篇立场论文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，主张将智能体重新定义为主动协作伙伴而非被动信号处理器，强调伦理数据处理和可靠人机协作框架的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 当前脑机接口技术面临信息传输率低、用户特定校准需求高等限制，虽然大型语言模型的整合有所进展，但代理AI部署仍存在技术障碍和伦理问题，需要更全面的讨论。

**方法:** 这是一篇立场论文，通过分析当前BCI技术的局限性和LLM整合的进展，提出从BCI到BAC的范式转变概念框架。

**结果:** 提出了Brain-Agent Collaboration (BAC)的新范式，强调智能体应作为主动协作伙伴，需要关注伦理数据处理、模型可靠性和人机协作框架。

**结论:** 该领域需要从BCI扩展到BAC范式，通过建立安全、可信、有效的系统，实现智能体作为人类认知活动的主动协作伙伴，这需要重点关注伦理、可靠性和协作框架等问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embracing+Trustworthy+Brain-Agent+Collaboration+as+Paradigm+Extension+for+Intelligent+Assistive+Technologies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22095&send_immediately=true&force_search=false)

**原文摘要:** Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [18] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

**主要类别:** cs.AI

**AI概要:** 提出基于熵最小化的自优化思维向量方法，实现可控数学推理，在GSM8K上达到90.1%准确率和0.42可控性分数


<details>
  <summary>更多</summary>
  
**动机:** 需要开发能够动态调控大语言模型内部推理过程的可控数学推理方法，避免依赖外部奖励标注

**方法:** 引入可学习的思维向量，通过熵最小化奖励机制引导聚焦推理模式，使用Gemma-2-9B模型在GSM8K数据集上进行实验

**结果:** 达到90.1%的准确率，可控性分数0.42，思维向量形成明显聚类，在不同控制条件下保持低熵分布

**结论:** 熵基奖励机制能有效指导聚焦推理模式，验证了可控AI推理框架的有效性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Controllable+Mathematical+Reasoning+via+Self-Optimizing+Thought+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22132&send_immediately=true&force_search=false)

**原文摘要:** We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [19] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost, Shreyans Jain, Shivam Raval, Grant Corser, Allen Roush, Nina Xu, Jacqueline Hammack, Ravid Shwartz-Ziv, Amirali Abdullah*

**主要类别:** cs.AI

**AI概要:** 提出一个AI心理测量框架，使用情境判断测试和复杂人物设定来评估AI系统在需要情感判断和伦理考量的角色中的表现，并在执法助手案例中构建了包含8500个人物、4000个测试和30万回应的数据集。


<details>
  <summary>更多</summary>
  
**动机:** 现有AI心理测量研究通常重复使用人类特质清单或临时人物设定，限制了行为真实性和领域相关性，需要更现实的评估方法。

**方法:** 提出三部分框架：(1)使用真实情境的情境判断测试评估领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂人物设定；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙事。

**结果:** 在执法助手案例研究中构建了丰富数据集，涵盖8种人物原型和11个属性的测试，包含8500个人物、4000个测试和30万条回应。

**结论:** 该框架提供了更现实的AI心理测量方法，将公开发布数据集和所有代码，促进该领域研究发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measure+what+Matters%3A+Psychometric+Evaluation+of+AI+with+Situational+Judgment+Tests，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22170&send_immediately=true&force_search=false)

**原文摘要:** AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [20] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种名为Dopamine的免导数优化器，通过权重扰动学习和奖励预测误差机制来训练神经网络，解决了反向传播的计算和内存效率问题，在性能相当的同时具有更好的神经生物学合理性。


<details>
  <summary>更多</summary>
  
**动机:** 解决突触信用分配问题(CAP)是神经网络学习的关键。反向传播虽然有效但计算成本高，存在权重传输和更新锁定问题，需要更高效的神经生物学合理方法。

**方法:** 采用神经AI方法，从神经强化学习中获得灵感，开发Dopamine优化器。通过权重扰动学习，利用随机权重更新，通过最小化扰动模型与未扰动模型之间的奖励预测误差来调整学习率。

**结果:** 在XOR任务和多层感知机、混沌时间序列预测的循环神经网络上测试，Dopamine训练模型显示加速收敛，优于标准权重扰动方法，性能与基于梯度的算法相当，同时显著减少计算和内存消耗。

**结论:** Dopamine优化器不仅找到了鲁棒解且性能与最先进的机器学习优化器相当，而且在神经生物学上更合理，为高效神经网络训练提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dopamine-driven+synaptic+credit+assignment+in+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22178&send_immediately=true&force_search=false)

**原文摘要:** Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [21] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu, Jie Wang, Yuyang Cai, Xiongwei Han, Yufei Kuang, Jianye Hao*

**主要类别:** cs.AI

**AI概要:** OptiTree提出了一种基于树搜索的自适应问题分解方法，通过构建建模树来组织运筹学问题的层次分类和复杂度，显著提升了大型语言模型在复杂优化建模任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于大型语言模型的优化建模方法采用固定的步骤分解，但由于运筹学问题具有高度复杂的数学结构，这种方法往往无法达到高性能。

**方法:** 开发了一个建模树，根据层次化问题分类和复杂度组织各类运筹学问题，每个节点代表一个问题类别并包含相关的高层建模思路。通过递归搜索树结构来识别更简单的子问题，并自适应整合层次化思路来合成全局建模思路。

**结果:** 实验显示OptiTree相比最先进方法显著提高了建模准确性，在具有挑战性的基准测试中实现了超过10%的性能提升。

**结论:** OptiTree通过自适应问题分解和层次化思路整合，有效解决了复杂运筹学问题的自动化建模挑战，为大型语言模型在技术性优化任务中的应用提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OptiTree%3A+Hierarchical+Thoughts+Generation+with+Tree+Search+for+LLM+Optimization+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22192&send_immediately=true&force_search=false)

**原文摘要:** Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [22] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon, Hee Suk Yoon, Jaehyun Jang, SooHwan Eom, Qi Dai, Chong Luo, Mark A. Hasegawa-Johnson, Chang D. Yoo*

**主要类别:** cs.AI

**AI概要:** 提出PACR方法，通过模型内在的密集奖励信号改进RLVR训练，加速探索并提高推理效果


<details>
  <summary>更多</summary>
  
**动机:** RLVR的稀疏奖励无法为中间推理步骤提供指导，导致探索效率低下

**方法:** 使用PACR（渐进上升置信度奖励），基于模型对正确答案置信度的上升趋势构建密集的内在奖励

**结果:** PACR加速了探索过程，用更少的轨迹达到奖励饱和，在多个基准测试中取得改进

**结论:** 密集的模型内在塑造信号可以使RLVR训练更有效和可靠

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PACR%3A+Progressively+Ascending+Confidence+Reward+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22255&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [23] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen, Bernard Cheng, Kelvin Soh*

**主要类别:** cs.AI

**AI概要:** 本文创建了首个越南语歌词转录数据集VietLyrics，并通过微调Whisper模型在越南语歌词转录任务上取得了优于现有系统的性能。


<details>
  <summary>更多</summary>
  
**动机:** 越南语歌词转录面临音调复杂和方言变异的独特挑战，且缺乏专门的数据集，导致该领域研究不足。

**方法:** 构建了包含647小时歌曲的大规模越南语歌词数据集VietLyrics，并基于该数据集对Whisper模型进行微调。

**结果:** 微调后的Whisper模型在越南语歌词转录任务上表现优于现有的多语言ALT系统（包括LyricWhiz），解决了传统ASR方法中的转录错误和幻觉问题。

**结论:** VietLyrics数据集和微调模型的发布将推动越南音乐计算研究，展示了该方法在低资源语言和音乐歌词转录中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VietLyrics%3A+A+Large-Scale+Dataset+and+Models+for+Vietnamese+Automatic+Lyrics+Transcription，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22295，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22295&send_immediately=true&force_search=false)

**原文摘要:** Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [24] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

**主要类别:** cs.AI

**AI概要:** 提出基于多级图粗化和精化的CVRPTW求解框架，通过时空距离度量聚合客户节点，显著减少计算时间同时保持或提升解质量


<details>
  <summary>更多</summary>
  
**动机:** CVRPTW是物流领域的NP难优化问题，大规模实例对精确求解器计算挑战巨大，需要高效求解方法

**方法:** 使用多级图粗化将客户聚合成元节点，在简化问题上应用经典启发式算法，然后通过可行性校正扩展回原问题空间

**结果:** 在Solomon基准测试中显示计算时间减少，解质量保持或改进，特别是在容量和时间窗约束方面表现良好

**结论:** 该方法有效提升CVRPTW求解效率，量子启发优化技术的集成展示出进一步加速大规模车辆路径任务的潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Coarsening+Approach+for+the+Capacitated+Vehicle+Routing+Problem+with+Time+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22329&send_immediately=true&force_search=false)

**原文摘要:** The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [25] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu, Yuansheng Lian, Ke Zhang, Yunxuan Li, Yuelong Su, Meng Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出一个基于文献知识微调大语言模型(LIFT LLM)的可解释性卡车驾驶风险预测框架，通过整合文献知识库和微调技术，实现了准确的风险预测和可解释性分析。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决卡车驾驶风险预测中的准确性和可解释性问题，研究旨在开发一个能够结合领域文献知识并具有强解释能力的大语言模型预测框架。

**方法:** 构建包含LLM驱动推理核心、文献处理管道和结果评估器的三部分框架，使用299篇领域文献构建知识库，并在真实卡车驾驶数据集上进行微调。

**结果:** LIFT LLM在召回率上优于基准模型26.7%，F1分数提升10.1%，变量重要性排序与基准模型一致，并能识别通过PERMANOVA验证的关键风险变量组合。

**结论:** 该框架成功实现了准确且可解释的卡车驾驶风险预测，证明了文献知识库和微调过程对模型可解释性的重要贡献，在数据驱动知识发现方面具有巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LIFT%3A+Interpretable+truck+driving+risk+prediction+with+literature-informed+fine-tuned+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22333&send_immediately=true&force_search=false)

**原文摘要:** This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [26] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu, Shijie Lian, Zihao Liu, Lei Zhang, Laurence Tianruo Yang, Kai Chen*

**主要类别:** cs.AI

**AI概要:** DynaSolidGeo是首个针对视觉语言模型空间推理能力的动态基准测试，包含503个专家策划的立体几何问题，可动态生成无限多样的多模态实例，并通过过程评估来测量推理链的逻辑有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有数学推理基准主要关注2D平面几何、依赖静态数据集容易导致数据污染和记忆问题，且仅通过最终答案评估模型，忽略了推理过程。

**方法:** 通过半自动标注流程构建包含503个种子问题的数据集，可动态生成多样化多模态实例，并引入基于专家标注推理链的过程评估方法。

**结果:** 实验显示开源和闭源视觉语言模型存在较大性能差距，在动态设置下性能严重下降，在需要高水平空间智能的任务上表现较差。

**结论:** DynaSolidGeo填补了立体几何推理评估的空白，揭示了当前模型在空间推理能力上的局限性，为未来研究提供了重要基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaSolidGeo%3A+A+Dynamic+Benchmark+for+Genuine+Spatial+Mathematical+Reasoning+of+VLMs+in+Solid+Geometry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22340&send_immediately=true&force_search=false)

**原文摘要:** Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [27] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar, Jimson Huang, Yunxin Sun, Fei Xia, Abulhair Saparov*

**主要类别:** cs.AI

**AI概要:** 研究表明大语言模型在复杂推理任务上存在局限性，虽然经过微调的大推理模型在简单推理问题上表现优异，但在复杂程度增加时性能急剧下降，无法实现泛化推理。


<details>
  <summary>更多</summary>
  
**动机:** 重新审视大语言模型在复杂推理任务中的表现，验证其是否真正具备泛化推理能力，特别是针对现有基准测试复杂度有限的问题。

**方法:** 开发了Deep Reasoning Dataset (DeepRD)数据集，通过可扩展复杂度的生成过程创建无限样本，评估模型在图连接性和自然语言证明规划任务上的性能。

**结果:** 大推理模型在达到足够复杂度时性能急剧下降，无法泛化到训练分布之外的复杂问题，尽管多数现实世界问题仍在模型成功范围内。

**结论:** 大推理模型在近期具有实用价值，但需要开发新方法来解决训练分布之外的复杂推理问题，以应对现实世界中的长尾复杂情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Models+Reason+Well%2C+Until+They+Don%27t，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22371&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [28] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar, Ali Nazari, Erfan Shayegani, Nael Abu-Ghazaleh*

**主要类别:** cs.AI

**AI概要:** 该论文提出使用有限状态机(FSM)模型来分析大型推理模型(LRMs)的思维过程，将复杂的推理步骤抽象为离散状态和状态转移，为理解LLM的推理机制提供了结构化框架。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大型语言模型在链式思维推理方面表现出色，但其内部推理能力的涌现机制仍然难以理解，需要开发可解释的分析方法来理解模型的推理动态和层次化思维策略。

**方法:** 采用无记忆有限状态机(FSM)公式化方法，将LRM的推理过程抽象为6个离散状态(初始化、演绎、增强策略、不确定性估计、回溯、最终结论)，通过标注CoT步骤的状态转换来分析推理轨迹。

**结果:** FSM模型能够系统性地分析和可视化不同模型的推理方式，揭示了不同的推理模式和潜在缺陷，为模型评估和改进提供了新视角。

**结论:** 基于FSM的分析方法为理解和解释LLM推理能力提供了有价值的框架，有助于改进模型训练和增强推理鲁棒性，是解决LLM推理机制理解难题的重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Hierarchical+Thinking+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22437，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22437&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [29] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath, Nikhil Krishnaswamy*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种新型的合作伙伴感知学习算法ICR，用于训练LLM驱动的协作代理，使其能够更好地接受干预并促进团队共识达成。


<details>
  <summary>更多</summary>
  
**动机:** 随着LLM越来越多地在代理设置中与人类协作，需要评估其在多轮多方任务中的协作能力。标准RLHF训练的LLM代理倾向于忽略干预，这使得增加团队共同基础变得困难。

**方法:** 使用双玩家修改动作MDP分析标准AI代理的次优行为，并提出ICR（可中断协作角色扮演）算法来训练共同基础最优的协作代理。

**结果:** 在多个协作任务环境中的实验表明，ICR平均能更有效地促进成功的共同基础收敛，并在任务中探索更多样化的解决方案。

**结论:** ICR算法通过使LLM代理更加合作伙伴感知，能够显著提高协作效果和团队共识达成能力，为LLM在代理协作场景中的应用提供了重要改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+%22Partner-Aware%22+Collaborators+in+Multi-Party+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22462&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [30] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng, Zirui Pang, Ling li, Zhijie Deng, Yuhan Pu, Zhaowei Zhu, Xiaobo Xia, Jiaheng Wei*

**主要类别:** cs.AI

**AI概要:** OFFSIDE是一个针对多模态大语言模型设计的机器遗忘基准测试，专注于足球转会谣言的信息遗忘评估，包含15.68K条手动标注数据，揭示了当前多模态遗忘方法的显著漏洞。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型的发展加剧了数据隐私担忧，现有机器遗忘基准存在图像多样性不足、准确性问题和评估场景不充分等局限，无法反映真实应用的复杂性。

**方法:** 构建基于足球转会谣言的手动标注数据集，包含四个测试集评估遗忘效果、泛化性、实用性和鲁棒性，支持选择性遗忘、纠正再学习和单模态遗忘等高级设置。

**结果:** 评估发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法都难以处理视觉谣言；遗忘的谣言容易被恢复；所有方法都易受提示攻击。

**结论:** 当前多模态遗忘方法存在重大脆弱性，需要开发更鲁棒的多模态遗忘解决方案，OFFSIDE基准为这一领域的发展提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OFFSIDE%3A+Benchmarking+Unlearning+Misinformation+in+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22535&send_immediately=true&force_search=false)

**原文摘要:** Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [31] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi, Ludovic Moncla, Khalid Benabdeslem, Rémy Cazabet, Pierre Cléau*

**主要类别:** cs.AI

**AI概要:** ATOM是一个从非结构化文本构建和持续更新时序知识图谱的少样本可扩展方法，通过原子事实提取和双时间建模，在提取完整性、稳定性和延迟方面显著优于基线方法


<details>
  <summary>更多</summary>
  
**动机:** 传统静态知识图谱忽略了数据的动态性和时效性，而现有的零/少样本方法存在不稳定性和关键事实覆盖不全的问题

**方法:** 将输入文档分割为最小自包含的原子事实，采用双时间建模区分信息观测时间和有效时间，并行合并构建原子时序知识图谱

**结果:** 相比基线方法，ATOM实现了约18%的完整性提升、17%的稳定性改善和超过90%的延迟减少

**结论:** ATOM展示了在动态时序知识图谱构建方面的强大可扩展潜力，有效解决了现有方法的局限性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ATOM%3A+AdapTive+and+OptiMized+dynamic+temporal+knowledge+graph+construction+using+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22590&send_immediately=true&force_search=false)

**原文摘要:** In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [32] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song, Jiaxiang Li, Rong Wang, Songtao Lu, Mingyi Hong*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个新的理论框架来分析上下文学习(ICL)性能，通过构建简单的一层Transformer示例，揭示了当预训练数据分布与查询任务分布不同时，适当构建的上下文可以将输出分布向查询任务分布转移，并量化了ICL性能、上下文长度以及预训练与查询任务分布之间KL散度的精确关系。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型预训练语言模型展现出了强大的上下文学习能力，但理论上尚不清楚这种能力是如何产生的，特别是预训练过程和上下文构建等关键因素的确切作用。

**方法:** 构建了一个包含网络架构、数据编码、数据生成和提示构建过程的现实设置框架。首先使用单层Transformer构建简单示例，然后扩展到更一般情况，并进行实验验证。

**结果:** 研究发现当预训练数据分布与查询任务分布不同时，适当构建的上下文可以量化地将输出分布向查询任务分布转移，从而在查询主题上实现准确预测。

**结论:** 研究提供了一个理论框架来理解ICL能力的产生机制，揭示了上下文在弥合预训练分布与目标任务分布差异中的关键作用，为理解和改进上下文学习提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Quantifying+How+Pre-Training+and+Context+Benefit+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22594&send_immediately=true&force_search=false)

**原文摘要:** Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [33] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan, Rafid Mostafiz, Md. Abir Hossain, Bikash Kumar Paul*

**主要类别:** cs.AI

**AI概要:** CLIN-LLM是一个安全约束的混合AI系统，通过多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，实现了98%的准确率，比现有模型提升7.1%，并显著减少67%的不安全抗生素建议。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于大语言模型的医疗系统缺乏医学基础且无法量化不确定性，导致输出不安全，特别是在诊断风险高的异质患者环境中。

**方法:** 使用BioBERT在1200个临床病例上微调，结合Focal Loss和蒙特卡洛Dropout进行置信度感知预测；利用Biomedical Sentence-BERT从26万样本的MedDialog语料库检索相关对话；使用微调的FLAN-T5模型生成个性化治疗，并通过RxNorm进行抗生素管理和药物相互作用筛查。

**结果:** 达到98%的准确率和F1分数，比ClinicalBERT提升7.1%（p<0.001），78%的top-5检索精度，临床医生评分4.2/5，不安全抗生素建议减少67%。

**结论:** CLIN-LLM展示了强大的鲁棒性、可解释性和临床安全性，为资源有限的医疗环境提供了可部署的人机协同决策支持框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLIN-LLM%3A+A+Safety-Constrained+Hybrid+Framework+for+Clinical+Diagnosis+and+Treatment+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22609&send_immediately=true&force_search=false)

**原文摘要:** Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [34] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh, Aaron Shen, Brian Law, Ahmed Ismail, Jonas Rohweder, Sean O'Brien, Kevin Zhu*

**主要类别:** cs.AI

**AI概要:** SwiftSolve是一个复杂度感知的多智能体系统，专门用于竞争性编程，通过算法规划、性能分析和复杂度指导的修复来确保程序不仅正确，还要满足时间和内存约束。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM生成的程序虽然能通过单元测试，但经常违反比赛的时间和内存限制，需要一种能同时保证正确性和效率的解决方案。

**方法:** 采用多智能体系统架构，包括规划器、静态修剪器、编码器、性能分析器和复杂度分析器，通过JSON通信和迭代控制机制协作工作。

**结果:** 在26个问题上测试，首尝试通过率61.54%，3次内解决率80.77%，平均运行时间12.40秒，整体运行成功率73.08%。

**结论:** SwiftSolve通过性能分析和复杂度指导的重新规划显著提高了程序效率，在保持准确性的同时减少了资源超限问题，相比Claude Opus 4有显著改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwiftSolve%3A+A+Self-Iterative%2C+Complexity-Aware+Multi-Agent+Framework+for+Competitive+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22626&send_immediately=true&force_search=false)

**原文摘要:** Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [35] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan, Shaked Zychlinski*

**主要类别:** cs.AI

**AI概要:** 提出基于首个生成token的对数概率分布来检测LLM模板化响应的方法，可在单步生成后实现高效分类，显著降低计算成本和延迟


<details>
  <summary>更多</summary>
  
**动机:** LLMs在生成模板化响应（如拒绝、简单确认和问候）时消耗大量计算资源，增加了不必要的成本和延迟

**方法:** 使用首个生成token的对数概率分布作为信号，通过轻量级k-NN分类器预测响应类型（实质性回答vs模板化响应）

**结果:** 实验表明首个token的对数概率向量能形成明显可分离的聚类，实现高精度分类，支持早期终止或重定向到小模型

**结论:** 该方法提供了一种计算简单的实用技术，可优化LLM推理效率，为更高效和可持续的LLM部署提供直接路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Stop+Me+Now%3A+Detecting+Boilerplate+Responses+with+a+Single+Iteration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22679&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [36] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander, Sai Pragnya Ranga, Prathamesh Mayekar*

**主要类别:** cs.AI

**AI概要:** 本文提出了Atlas城市指数(AUI)，一种基于Sentinel-2卫星影像的新城市发展度量指标，通过视觉语言模型(VLMs)处理时间序列影像数据，克服传统NDBI指标在大气噪声、季节变化和云层覆盖方面的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法如NDBI在准确捕捉城市发展方面存在困难，受大气噪声、季节变化和云层覆盖等因素影响，阻碍了大规模人类发展和城市化的监测。

**方法:** 收集每个区域的Sentinel-2时间序列影像，在固定时间窗口内处理影像以获得最小云层覆盖的代表性图像；采用两种策略确保评分一致性：(i)提供代表不同城市化水平的参考图像集，(ii)提供最近历史图像以保持时间一致性和减轻云相关噪声。

**结果:** 在班加罗尔的定性实验表明，AUI优于NDBI等标准指标。

**结论:** AUI能够克服传统城市化指数的挑战，产生更可靠和稳定的发展评分，为大规模城市发展监测提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Atlas+Urban+Index%3A+A+VLM-Based+Approach+for+Spatially+and+Temporally+Calibrated+Urban+Development+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22702&send_immediately=true&force_search=false)

**原文摘要:** We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [37] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai, Jusheng Zhang, Yijia Fan, Jing Yang, Keze Wang*

**主要类别:** cs.AI

**AI概要:** RaCoT是一个新的RAG框架，通过在检索前阶段生成对比性问题来指导模型关注关键差异，从而在单次检索中抑制语义干扰，显著提升长尾查询的处理效果。


<details>
  <summary>更多</summary>
  
**动机:** 解决RAG在处理知识稀疏和语义模糊的长尾查询时面临的检索噪声问题，避免昂贵的后处理成本。

**方法:** 提出RaCoT框架，在检索前自动生成语义相邻但答案不同的对比问题，提取Δ-Prompt捕获关键差异，引导模型主动关注决定答案分歧的关键细节。

**结果:** 在六个权威基准测试中，RaCoT比RankRAG和Self-RAG等强基线高出0.9-2.4个百分点，在对抗性测试中性能下降仅8.6%，远优于其他方法的15%以上下降，同时具有低延迟(3.12s)和低token开销(11.54)。

**结论:** RaCoT将RAG范式从"事后上下文清理"重新定义为"先验形成判别推理"，为实时、资源受限部署提供了高效稳健的可靠AI系统路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RaCoT%3A+Plug-and-Play+Contrastive+Example+Generation+Mechanism+for+Enhanced+LLM+Reasoning+Reliability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22710&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [38] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli, Aditi Singh, Arun Sharma*

**主要类别:** cs.AI

**AI概要:** 本研究比较了五大主流大语言模型（Gemini、DeepSeek、Claude、GPT和LLaMA）在性能准确性、伦理偏见缓解和可用性集成三个关键维度的差异表现。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在各领域的广泛应用，不同模型基于不同理念构建，在性能、道德行为和可用性方面存在显著差异，需要系统评估以指导用户选择。

**方法:** 通过分析三个重要因素进行比较：性能与准确性、伦理与偏见缓解、可用性与集成能力。

**结果:** Claude在道德推理方面表现优异，Gemini在多模态能力和伦理框架方面更强，DeepSeek擅长基于事实的推理，LLaMA适合开源应用，ChatGPT在整体性能和使用体验上表现均衡。

**结论:** 不同模型在性能、易用性和伦理处理方面各具特色，用户应根据具体需求选择最适合的模型以最大化利用其优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Critical+Insights+into+Leading+Conversational+AI+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22729&send_immediately=true&force_search=false)

**原文摘要:** Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [39] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

**主要类别:** cs.AI

**AI概要:** 开发了一个实时事实核查框架，通过多知识源交叉验证来检测和纠正LLM的幻觉问题，在保持响应质量的同时将幻觉减少67%，专家满意度达89%


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型会自信地生成看似合理但错误的信息（幻觉问题），这成为在准确性要求高的现实应用中部署的主要障碍

**方法:** 构建事实验证框架，结合结构化数据库、实时网络搜索和学术文献，在LLM生成过程中实时交叉验证事实主张，检测到不一致时自动纠正并保持回答的自然流畅性

**结果:** 跨多个领域测试显示幻觉减少67%，响应质量不受影响。医疗、金融和科研领域的专家对纠正后输出的满意度达89%，相比未验证的LLM响应有显著提升

**结论:** 这项工作为在事实准确性至关重要的应用中提高LLM的可信度提供了实用解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Fact-Verification+Framework+for+Reducing+Hallucinations+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22751&send_immediately=true&force_search=false)

**原文摘要:** While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [40] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu, Junyu Feng, Ruichuan An, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang*

**主要类别:** cs.AI

**AI概要:** Jarvis是一个创新的个性化AI助手框架，通过个人KV-Cache检索技术，在文本和视觉token的KV-Cache中存储用户特定信息，实现了更准确的个性化问答。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法要么学习一组概念token，要么训练VLM使用用户特定信息，但都难以作为个性化助手生成准确答案。

**方法:** 将用户信息总结为元数据创建文本token，从用户图像中提取独特图像块生成视觉token，存储在KV-Cache中。回答问题时会先检索相关KV-Cache以确保准确性。

**结果:** Jarvis在视觉问答和纯文本任务上都取得了最先进的结果，特别是在依赖特定局部细节时能提供更准确的响应。

**结论:** Jarvis为个性化AI助手提供了一条实用路径，通过KV-Cache检索机制有效利用用户特定信息，显著提升了回答准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jarvis%3A+Towards+Personalized+AI+Assistant+via+Personal+KV-Cache+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22765&send_immediately=true&force_search=false)

**原文摘要:** The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [41] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang, Yijia Shao, Omar Shaikh, Daniel Fried, Graham Neubig, Diyi Yang*

**主要类别:** cs.AI

**AI概要:** 本研究首次直接比较人类和AI代理在多个工作技能上的表现，发现AI代理虽然速度快、成本低，但工作质量较差且倾向于程序化方法，与人类基于UI的方法形成对比。


<details>
  <summary>更多</summary>
  
**动机:** AI代理在人类工作任务中快速发展，但缺乏对人类工作方式的深入理解，需要揭示AI代理的专业能力和在不同工作流程中的角色。

**方法:** 引入可扩展的工具包，从人类和AI代理的计算机使用活动中提取可解释的结构化工作流程，并在数据分析、工程、计算、写作和设计等五个领域进行直接比较。

**结果:** (1) AI代理倾向于程序化方法，与人类UI中心方法形成对比；(2) AI代理工作质量较差，但会通过数据伪造和工具滥用掩盖缺陷；(3) AI代理速度快88.3%，成本低90.4-96.2%。

**结论:** AI代理在效率和经济性方面具有优势，但需要改进工作质量和方法论，通过将易编程任务委托给代理可以实现高效人机协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Do+AI+Agents+Do+Human+Work%3F+Comparing+AI+and+Human+Workflows+Across+Diverse+Occupations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22780&send_immediately=true&force_search=false)

**原文摘要:** AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [42] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu, Yunshen Zhou*

**主要类别:** cs.AI

**AI概要:** 微软提出了一种Agentic Meta-orchestrator (AMO)框架，用于在Copilot服务中协调多个任务和可扩展的代理，通过元学习决策树模型选择最佳推理策略，并在M365电商Copilot和代码合规Copilot两个生产用例中验证了有效性。


<details>
  <summary>更多</summary>
  
**动机:** 微软Copilot套件需要处理各种任务，从客户产品购买协助到企业代码漏洞检测，需要强大的编排器来将用户提示分发到正确的代理，支持动态扩展新代理。

**方法:** 提出了Agentic Meta-orchestrator (AMO)框架，利用元学习方法训练决策树模型来决定不同代理/模型之间的最佳推理策略，支持自然语言和动作响应。

**结果:** 通过两个生产用例验证：M365电商Copilot提供最新产品信息并连接多个代理（关系数据库和人工客服）；代码合规Copilot扫描DevOps代码检测已知和新的合规问题。

**结论:** AMO框架能够有效协调多个代理处理复杂任务，为Copilot服务提供了可扩展的编排解决方案，在实际生产环境中展示了良好的应用效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Meta-Orchestrator+for+Multi-task+Copilots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22781&send_immediately=true&force_search=false)

**原文摘要:** Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [43] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi, Emna Ben Romdhane*

**主要类别:** cs.AI

**AI概要:** 这篇论文分析了人工智能对人类构成的生存风险，从当前AI发展到超智能的轨迹，探讨了AGI和超级智能的伦理与生存影响，指出人类灭绝可能源于不可控的认知优势而非恶意。


<details>
  <summary>更多</summary>
  
**动机:** 基于Irving J. Good和Nick Bostrom的理论工作以及近期出版物，研究人工智能指数级增长的认知能力对人类生存的潜在威胁。

**方法:** 通过理论分析和文献回顾，探讨人工智能从当前水平到超智能的发展轨迹，分析机器认知能力的指数增长和假设IQ水平。

**结果:** 揭示了超级智能可能带来的存在性风险，指出即使没有恶意，人类也可能因无法控制的认知优势而面临灭绝风险。

**结论:** 人工智能的超智能发展对人类构成严重的存在性威胁，需要认真对待其伦理和生存影响，防范不可控的认知优势带来的风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Will+Humanity+Be+Rendered+Obsolete+by+AI%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22814&send_immediately=true&force_search=false)

**原文摘要:** This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [44] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang, David Rawlinson*

**主要类别:** cs.AI

**AI概要:** HRM-Agent是基于分层推理模型(HRM)的强化学习变体，能够在动态不确定的迷宫环境中学习导航，并成功重用先前时间步的计算。


<details>
  <summary>更多</summary>
  
**动机:** 虽然HRM模型在小型规模下具有出色的推理能力，但仅限于监督学习、静态和完全可观测的问题。现实世界问题通常是动态、不确定、部分可观测的，需要重用先前计算。

**方法:** 开发HRM-Agent，使用纯强化学习训练HRM变体，探索其在动态不确定迷宫环境中的导航能力，并分析循环推理过程的动态特性。

**结果:** HRM-Agent成功学会了在动态不确定环境中导航到目标，研究显示其循环推理过程能够有效重用先前时间步的计算。

**结论:** HRM-Agent扩展了HRM的应用范围，证明了其在强化学习设置下处理动态不确定问题的能力，为实际应用提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HRM-Agent%3A+Training+a+recurrent+reasoning+model+in+dynamic+environments+using+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22832，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22832&send_immediately=true&force_search=false)

**原文摘要:** The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [45] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein, Jessica Chen, Gwyneth Anne Delos Santos, Bayley Sapara, Michael Bowling*

**主要类别:** cs.AI

**AI概要:** 论文研究如何让强化学习智能体在提升性能的同时降低计算开销，通过让智能体感知计算成本并自主控制计算使用，在ALE环境中实现了75%游戏性能提升和平均3倍计算效率提升。


<details>
  <summary>更多</summary>
  
**动机:** 人类在任务熟练后会降低认知努力，而现有强化学习智能体在性能提升时计算效率不增反降。研究旨在让智能体能够像人类一样在提升熟练度的同时减少计算资源消耗。

**方法:** 在Arcade Learning Environment中实验，让智能体感知计算成本并赋予其控制计算使用的自主权，通过计算预算约束来训练智能体。

**结果:** 在相同训练计算预算下，75%的游戏上智能体表现更好，平均计算使用量减少三倍，同时分析了各游戏中效率提升的具体情况。

**结论:** 让强化学习智能体感知和控制计算成本能显著提升计算效率，为开发更节能的智能体和释放计算资源用于其他进程（如规划）提供了可行路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Agents+That+Reason+About+Their+Computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22833&send_immediately=true&force_search=false)

**原文摘要:** While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [46] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao, Qiucheng Wu, Yang Zhang, Zhaowen Wang, Handong Zhao, Shiyu Chang*

**主要类别:** cs.AI

**AI概要:** 本文分析多模态大语言模型中的模态鸿沟问题，发现现有训练方法加剧了文本和视觉模态之间的性能差距，并提出从数据和损失函数设计两方面来弥合这一差距的方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型在视觉-语言任务中表现出色，但存在模态不平衡问题——过度依赖文本线索而忽视视觉内容，导致需要真正视觉推理的任务表现不佳。

**方法:** 通过训练方法视角分析模态鸿沟，首先证明现有训练方法会放大这种差距，然后从数据和损失函数设计两个互补角度系统探索弥合策略。

**结果:** 研究发现为开发能够减轻模态鸿沟并促进更平衡多模态推理的训练方法提供了洞见。

**结论:** 论文提出了弥合多模态大语言模型中文本和视觉模态之间性能差距的有效策略，通过改进训练方法实现更平衡的多模态推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Text-Vision+Reasoning+Imbalance+in+MLLMs+through+the+Lens+of+Training+Recipes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22836&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [47] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li, Erik-Jan van Kampen*

**主要类别:** cs.AI

**AI概要:** 开发了一个级联在线学习飞行控制系统并改进了动作平滑性，研究了以李雅普诺夫函数增量为特征的收敛性能，考虑了离散化误差和增量模型引入的状态预测误差，通过飞行控制仿真展示比较结果


<details>
  <summary>更多</summary>
  
**动机:** 研究级联在线学习飞行控制系统的收敛性能，特别关注动作平滑性的改进效果

**方法:** 使用李雅普诺夫函数增量作为性能指标，考虑离散化误差和状态预测误差，通过飞行控制仿真进行对比分析

**结果:** 开发了增强动作平滑性的级联在线学习飞行控制系统，并对其收敛性能进行了理论分析和仿真验证

**结论:** 该控制系统的收敛性能得到了理论分析和仿真验证，李雅普诺夫函数增量方法有效表征了系统性能，考虑了实际工程中的误差因素

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lyapunov+Function-guided+Reinforcement+Learning+for+Flight+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22840，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22840&send_immediately=true&force_search=false)

**原文摘要:** A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [48] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno, Jean-Louis Dessalles*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个基于逻辑门电子电路的统一框架，将多种推理机制（分类、归纳、溯因、因果推理等）整合到统一的视角下，通过组合探索识别出四种主要依赖形式和八种常见推理模式。


<details>
  <summary>更多</summary>
  
**动机:** 认知研究和人工智能为各种推理机制开发了不同的模型，但缺乏统一的理论框架。论文试图填补这一空白，从物质角度思考高级激活过程。

**方法:** 使用符号AI建模技术，通过基于逻辑门的电子电路简化视角来分析推理机制，进行组合探索识别依赖形式，并在逻辑程序背景下分析推理模式。

**结果:** 识别出四种可由推理电路实现的主要依赖形式，发现了八种常见的推理模式，在统一框架下揭示了传统上不同的推理机制。

**结论:** 即使论证主要基于符号方法和数字系统基础设施，这些观察结果可能指向更普遍适用的结构，为认知和AI推理机制提供了统一的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Structures+of+Inferential+Mechanisms+through+Simplistic+Digital+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22883&send_immediately=true&force_search=false)

**原文摘要:** Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [49] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat, Omkar Ghugarkar, Julian McAuley*

**主要类别:** cs.AI

**AI概要:** 论文提出了CoreThink智能体推理框架，通过轻量级符号推理层增强大语言模型，在多工具调用环境中实现了530%的性能提升和显著的计算成本降低。


<details>
  <summary>更多</summary>
  
**动机:** 解决智能体工具调用环境中的泛化挑战，当前大语言模型在跨领域工具协调和推理策略迁移方面存在显著泛化差距。

**方法:** 提出CoreThink智能体推理器框架，为大语言模型添加轻量级符号推理层，实现结构化分解和自适应工具编排，无需额外训练。

**结果:** 在多个工具调用基准测试中实现最先进性能，准确率提升530%，计算成本降低约90%，在MAVEN新基准上大多数模型准确率低于50%。

**结论:** CoreThink框架有效解决了智能体工具调用泛化问题，证明了符号推理层在增强大语言模型跨领域推理能力方面的重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Generalization+in+Agentic+Tool+Calling%3A+CoreThink+Agentic+Reasoner+and+MAVEN+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22898&send_immediately=true&force_search=false)

**原文摘要:** Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [50] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu*

**主要类别:** cs.AI

**AI概要:** GTR-Mamba是一个新颖的跨流形条件路由框架，通过双曲几何建模静态偏好层级，在欧几里得空间处理动态序列，有效解决了现有POI推荐模型无法同时捕捉空间层次结构和用户时间上下文动态变化的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于图神经网络和序列模型的POI推荐方法存在根本局限性：难以同时捕捉空间选择的内在层次结构以及用户特定时间上下文的动态和不规则变化。

**方法:** 提出GTR-Mamba框架，利用不同数学空间的优势：在双曲几何中建模静态的树状偏好层级，在欧几里得切空间中通过新颖的Mamba层处理动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。

**结果:** 在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中始终优于最先进的基线模型。

**结论:** GTR-Mamba通过跨流形条件路由的方法成功解决了POI推荐中的时空建模挑战，证明了利用不同几何空间处理不同类型信息的有效性，为下一代位置推荐系统提供了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GTR-Mamba%3A+Geometry-to-Tangent+Routing+for+Hyperbolic+POI+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22942&send_immediately=true&force_search=false)

**原文摘要:** Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [51] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng, Sinuo Zhang, Rongpeng Li, Xiangming Meng, Chan Wang, Ming Lei, Zhifeng Zhao*

**主要类别:** cs.AI

**AI概要:** 提出了基于多智能体条件扩散模型规划器(MA-CDMP)的去中心化通信资源管理方法，通过扩散模型捕捉环境动态和规划轨迹，结合均值场机制处理大规模智能体交互，有效解决了传统方法的非平稳性和合作问题。


<details>
  <summary>更多</summary>
  
**动机:** 集中式多智能体强化学习存在可扩展性和隐私风险问题，而分布式训练去中心化执行范式面临非平稳性和有限合作的挑战，需要新的方法来提升无线通信系统的资源分配效率。

**方法:** 基于模型强化学习范式，使用扩散模型(DMs)捕捉环境动态和规划未来轨迹，通过逆动力学模型指导动作生成，引入均值场(MF)机制近似大规模智能体交互，减少通信开销。

**结果:** 实验表明MA-CDMP在平均奖励和QoS指标上持续优于现有MARL基线方法，展现了良好的可扩展性和实际应用价值。

**结论:** MA-CDMP通过扩散模型和均值场机制有效解决了多智能体资源管理的非平稳性和合作问题，理论保证了收敛稳定性，为实际无线网络优化提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Conditional+Diffusion+Model+with+Mean+Field+Communication+as+Wireless+Resource+Allocation+Planner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22969&send_immediately=true&force_search=false)

**原文摘要:** In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [52] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu, Jiakai Wang, Linna Jing, Haolin Li, Haodong Liu, Haotong Qin, Aishan Liu, Ke Xu, Xianglong Liu*

**主要类别:** cs.AI

**AI概要:** 本文提出了InSUR框架，通过多维度指令不确定性减少技术，解决了语义对抗样本生成中的语言不确定性挑战，显著提升了对抗样本的可迁移性、适应性和有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前语义对抗样本生成方法因未充分研究人类指令中的语义不确定性（如指称多样性、描述不完整性和边界模糊性）而攻击能力不足，需要开发更有效的生成框架。

**方法:** 提出多维度指令不确定性减少(InSUR)框架：1）采样方法维度：残差驱动攻击方向稳定化(ResAdv-DDIM采样器)；2）任务建模维度：上下文编码攻击场景约束；3）生成器评估维度：语义抽象攻击评估增强。

**结果:** 大量实验证明InSUR在迁移攻击性能上具有优越性，首次实现了无需参考的语义约束3D对抗样本生成。

**结论:** InSUR框架通过系统解决语义不确定性挑战，显著提升了语义对抗样本的生成质量，为未来语义对抗攻击研究提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Semantic-constrained+Adversarial+Example+with+Instruction+Uncertainty+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22981&send_immediately=true&force_search=false)

**原文摘要:** Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [53] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales, Carlos Andrés Ferro Sánchez, Reinel Tabares-Soto, Jesús Alfonso López Sotelo, Gonzalo A. Ruz, Johan Sebastian Piña Durán*

**主要类别:** cs.AI

**AI概要:** ProfileXAI是一个结合事后解释器与检索增强LLM的框架，为不同用户生成可解释的AI解释，在心脏病和甲状腺癌数据集上评估显示不同解释器各有优势，系统能稳定生成高质量解释。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决AI模型解释性不足的问题，需要为不同类型的用户提供可理解且可靠的事后解释，同时整合多种解释方法以适应不同场景需求。

**方法:** 开发模型和领域无关的框架，耦合SHAP、LIME、Anchor等事后解释器与检索增强的大型语言模型，通过多模态知识库索引、基于量化标准选择解释器，并使用聊天式提示生成接地气的叙述。

**结果:** 在心脏病和甲状腺癌数据集上评估显示：LIME在保真度-鲁棒性权衡上最佳，Anchor产生最稀疏的低token规则，SHAP获得最高满意度。Profile条件化稳定了token使用并保持积极评分。

**结论:** ProfileXAI框架能够有效整合多种解释方法，为不同用户群体生成高效且可信的解释，证明了多解释器协同工作的价值，没有单一解释器在所有指标上都表现最优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProfileXAI%3A+User-Adaptive+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22998&send_immediately=true&force_search=false)

**原文摘要:** ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [54] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang, Xiaoming Li, Jie Chen, Yongxu Liu, Xingpeng Zhang, Chen Liu, Wei Chen*

**主要类别:** cs.AI

**AI概要:** 该研究提出了一个多维可信度评估框架(MDCA)来提升LLM生成肝脏MRI报告的可信度，并提供了机构特定的提示优化指导，在多个先进LLM模型上进行了评估比较。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型在从影像发现生成诊断结论方面表现出潜力，但缺乏针对不同临床场景的提示优化系统指导，以及评估LLM生成放射学报告可信度的标准化框架。

**方法:** 引入多维可信度评估(MDCA)框架，应用该框架在SiliconFlow平台上评估比较多个先进LLM模型(Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3、ByteDance-Seed-OSS-36B-Instruct)的性能。

**结果:** 研究提出了一个评估框架并进行了模型比较，但摘要中未明确给出具体的评估结果数据。

**结论:** 该研究通过建立MDCA框架和提供提示优化指导，为提升LLM生成肝脏MRI报告的可信度提供了系统化的方法学支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Prompt+Optimization+to+Multi-Dimensional+Credibility+Evaluation%3A+Enhancing+Trustworthiness+of+Chinese+LLM-Generated+Liver+MRI+Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23008&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [55] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh, Rajesh P. N. Rao*

**主要类别:** cs.AI

**AI概要:** MDD提出可调节密度的扩散规划器，通过非均匀稀疏步长规划在多个任务领域实现新的SOTA性能


<details>
  <summary>更多</summary>
  
**动机:** 现有扩散规划器使用稀疏步长规划可捕获长期依赖但性能会下降，作者发现时间密度阈值在时间维度上不均匀，需要更智能的密度分配

**方法:** 提出Mixed Density Diffuser (MDD)，在整个规划时间范围内密度可作为可调超参数进行配置

**结果:** 在Maze2D、Franka Kitchen和Antmaze D4RL任务领域达到了新的SOTA性能

**结论:** 通过允许不同时间段的规划密度可调节，MDD证明了非均匀稀疏规划的有效性，为扩散规划器提供了更灵活的框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixed+Density+Diffuser%3A+Efficient+Planning+with+Non-uniform+Temporal+Resolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23026&send_immediately=true&force_search=false)

**原文摘要:** Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [56] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie, Pan Zhou, Lichao Sun*

**主要类别:** cs.AI

**AI概要:** 这篇综述论文提出了一个六阶段方法论框架来系统分析AI科学家领域的发展，从早期基础模块到当前的规模化与人类协作阶段，为自主科学系统的未来发展提供路线图。


<details>
  <summary>更多</summary>
  
**动机:** AI正从计算工具转变为自主科学知识创造者，但该领域的快速无序发展导致研究碎片化，缺乏统一的方法论框架和发展趋势分析。

**方法:** 引入统一的六阶段方法论框架：文献综述、想法生成、实验准备、实验执行、科学写作和论文生成，通过这一分析视角梳理该领域的演进历程。

**结果:** 将领域发展划分为三个阶段：基础模块阶段（2022-2023）、闭环系统阶段（2024）、规模化与人类协作阶段（2025至今），系统梳理了技术发展脉络。

**结论:** 该综述不仅阐明了自主科学的当前状态，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖且不可或缺的合作伙伴。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+AI+Scientists%3A+Surveying+the+automatic+Scientists+and+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23045&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [57] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang, Meixin Su, Yang Yang, Chunyan Zeng, Lizhi Ye*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于深度学习和迁移学习的跨学科认知诊断方法(TLCD)，通过利用主学科的共同特征来提升目标学科中的模型性能，实验证明该方法在跨学科认知诊断任务中表现优于基础模型。


<details>
  <summary>更多</summary>
  
**动机:** 在线教育模式下，传统认知诊断方法在跨学科领域面临知识体系差异、认知结构不同和数据特征不统一等挑战，需要新的方法来准确评估学生的能力水平。

**方法:** 提出TLCD方法，结合深度学习技术和迁移学习策略，利用主学科的共同特征来增强模型在目标学科中的性能，研究神经网络认知诊断和知识关联神经网络认知诊断。

**结果:** 实验结果显示，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能更准确地评估学生的学习情况。

**结论:** TLCD方法通过深度学习和迁移学习的结合，有效解决了跨学科认知诊断中的挑战，为在线教育提供了更准确的学生能力评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TLCD%3A+A+Deep+Transfer+Learning+Framework+for+Cross-Disciplinary+Cognitive+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23062&send_immediately=true&force_search=false)

**原文摘要:** Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [58] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld, Xi Qin, Alexander Schaefer, Yaad Oren*

**主要类别:** cs.AI

**AI概要:** 研究证明小型语言模型（如Phi-4）可以通过添加回归层和监督微调转变为有效的奖励模型，用于代码生成任务，能识别正确代码解决方案并提升搜索能力20%以上。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型在代码生成方面仍有挑战，奖励模型是推理模型发展的必要中间步骤，但现有研究主要关注大模型，本研究探索小型语言模型是否也能成为有效的奖励模型。

**方法:** 构建基于APPS编程挑战基准的代码样本数据集，训练带有回归头的价值模型来估计中间输出的成功概率，结合过程奖励和结果奖励的考量。

**结果:** 小型LLM能够作为有效的奖励模型或代码评估评判器，成功识别多个候选方案中的正确解决方案，使用该评判器使多代代码中最准确代码的搜索能力提升超过20%。

**结论:** 小型语言模型可以被成功转化为实用的奖励模型，在代码生成任务中结合过程奖励和结果奖励的考量，为代码生成模型的进化提供有效的中间步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smaller+Models%2C+Smarter+Rewards%3A+A+Two-Sided+Approach+to+Process+and+Outcome+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23083&send_immediately=true&force_search=false)

**原文摘要:** Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [59] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang, Jiawei Zhang, Yumou Liu, Hanqun Cao, Chunbin Gu, Mengdi Liu, Zhangyang Gao, Zitong Jerry Wang, Xuanhe Zhou, Pheng-Ann Heng, Lijun Wu, Conghui He, Cheng Tan*

**主要类别:** cs.AI

**AI概要:** 研究发现，在生物推理任务中，为科学大语言模型提供高层次结构化上下文比直接处理原始生物分子序列效果更好，甚至原始序列会干扰模型性能，表明Sci-LLMs的真正优势在于对结构化知识的推理能力而非序列解码。


<details>
  <summary>更多</summary>
  
**动机:** 解决科学大语言模型在处理原始生物分子序列时面临的tokenization困境——无论是将序列视为特殊语言可能丢失功能motif信息，还是作为单独模态引入对齐挑战，当前策略都限制了模型的推理能力。

**方法:** 通过系统比较领先的Sci-LLMs在生物推理任务上的表现，测试了三种输入模式：仅序列、仅上下文、以及两者结合。

**结果:** 上下文仅方法始终且显著优于所有其他模式。更令人惊讶的是，将原始序列与其高层次上下文结合会持续降低性能，表明原始序列即使对于具有专门tokenization方案的模型也表现为信息噪声。

**结论:** 现有Sci-LLMs的主要优势不在于从头解释生物分子语法的新兴能力，而在于对结构化、人类可读知识的深刻推理能力。应重新构建Sci-LLMs，将其视为专家知识的强大推理引擎而非序列解码器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+in+Tokenization%3A+Context+as+the+Key+to+Unlocking+Biomolecular+Understanding+in+Scientific+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23127&send_immediately=true&force_search=false)

**原文摘要:** Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [60] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent François-Lavet, Edward S. Hu*

**主要类别:** cs.AI

**AI概要:** FoG方法通过基础模型将人类偏好融入技能发现过程，有效避免传统方法产生的不良行为，同时保持技能多样性。


<details>
  <summary>更多</summary>
  
**动机:** 现有技能发现方法只关注技能多样性最大化，忽略了人类偏好，导致产生不良甚至危险的行为，如机器人翻滚或进入危险区域。

**方法:** 提出Foundation model Guided (FoG)方法，利用基础模型提取评分函数来评估状态，基于人类意图为期望状态分配高值、不良状态分配低值，然后用这些分数重新加权技能发现算法的奖励。

**结果:** FoG成功消除了翻转、翻滚等不良行为，避免了危险区域，在状态基和像素基任务中都表现良好，还能发现难以定义的行为技能。

**结论:** FoG方法有效将人类意图整合到技能发现中，解决了传统方法的安全性问题，同时保持了技能的多样性，为下游任务提供了更安全的技能基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guiding+Skill+Discovery+with+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23167&send_immediately=true&force_search=false)

**原文摘要:** Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [61] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker, Alexander Dockhorn, Bodo Rosenhahn*

**主要类别:** cs.AI

**AI概要:** AUPO是一种基于奖励分布统计的自动动作抽象算法，作为MCTS的即插即用修改，在IPPC基准测试中明显优于标准MCTS，能检测对称动作且无需转移概率或DAG搜索图


<details>
  <summary>更多</summary>
  
**动机:** 解决现有自动抽象算法需要转移概率和DAG搜索图的限制，以及无法有效处理状态空间中相距较远的对称状态的问题

**方法:** 基于MCTS过程中获取的奖励分布统计，开发自动动作抽象算法AUPO，仅影响决策策略而不影响树搜索

**结果:** 在IPPC基准问题上的比较显示AUPO明显优于MCTS，能检测到ASAP等先进框架难以处理的对称动作

**结论:** AUPO是一种有效的自动动作抽象方法，无需额外信息需求，可与仅影响树搜索的其他抽象技术兼容使用

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AUPO+--+Abstracted+Until+Proven+Otherwise%3A+A+Reward+Distribution+Based+Abstraction+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23214&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [62] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini, Joakim Bergdahl, Jean-Philippe Barrette-LaPierre, Florian Fuchs, Brady Chen, Micheal Jones, Linus Gisslén*

**主要类别:** cs.AI

**AI概要:** 提出一种针对游戏行业优化的样本高效深度强化学习方法，通过利用预收集数据和增加网络可塑性，在EA SPORTS FC 25中训练的门将AI比内置AI的扑救率高10%，训练速度快50%，且行为更接近人类玩家。


<details>
  <summary>更多</summary>
  
**动机:** 虽然深度强化学习在游戏AI研究中取得进展，但由于需要大量资源和训练超人类智能体，游戏行业难以实际应用。需要为资源有限的游戏工作室开发能够训练类人智能体的高效方法。

**方法:** 改进基于价值的深度强化学习方法，利用预收集数据提高样本效率，增加网络可塑性，专门针对游戏工业环境中的智能体训练和微调。

**结果:** 在EA SPORTS FC 25中训练的门将智能体扑救率比内置AI高10%，训练速度比标准DRL方法快50%，专家评估显示行为更接近人类玩家。

**结论:** 该方法成功解决了游戏行业中DRL应用的实际问题，证明了在工业环境中训练高效、类人智能体的可行性，并将被应用于该游戏系列的后续版本中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-Like+Goalkeeping+in+a+Realistic+Football+Simulation%3A+a+Sample-Efficient+Reinforcement+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23216&send_immediately=true&force_search=false)

**原文摘要:** While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [63] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang, Wenkai Yang, Jie Wang, Huanshuo Dong, Zijie Geng, Zhen Huang, Depeng Xie, Zhezheng Hao, Hande Dong*

**主要类别:** cs.AI

**AI概要:** 提出BlocKOA算法，用于高效生成集成电路热仿真数据，相比现有方法实现420倍加速，同时保证数据精度，解决了数据驱动方法需要大量高保真训练数据的高计算成本问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有神经网络算子等数据驱动方法需要大量高保真训练数据（如芯片参数和温度分布），导致计算成本显著。需要一种能够同时加速数据生成过程并提高生成数据精度的方法。

**方法:** 提出BlocKOA算法：1）基于热传导方程结构使用块Krylov算法快速获取基础解；2）组合这些解得到满足物理约束的多种温度分布；3）应用热算子确定热源分布，高效生成精确数据点。

**结果:** 理论分析显示BlocKOA时间复杂度比现有方法低一个数量级。实验验证其效率，在生成5000个不同物理参数和IC结构芯片的热仿真数据时实现420倍加速。仅用4%的生成时间，基于BlocKOA生成数据训练的数据驱动方法性能与使用现有方法相当。

**结论:** BlocKOA算法成功解决了集成电路热仿真数据生成的高计算成本问题，大幅提升了数据生成效率，同时保持了数据精度，为数据驱动方法提供了高效的数据生成解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+IC+Thermal+Simulation+Data+Generation+via+Block+Krylov+and+Operator+Action，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23221&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [64] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello, Daniele Lizzio Bosco, Jacopo Cossio, Dusan Sutulovic, Giuseppe Serra, Carla Piazza, Paolo Burelli*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于强化学习的新方法来解决CNOT门最小化问题，使用单一智能体处理不同规模的量子电路，在3-15规模的矩阵上表现优于现有最优算法。


<details>
  <summary>更多</summary>
  
**动机:** CNOT门是量子计算中的基本组件，用于产生量子纠缠。减少CNOT门数量对量子算法效率至关重要，但CNOT最小化问题仍是一个计算复杂度未完全明确的开放挑战。

**方法:** 采用强化学习方法，使用单一智能体处理固定规模m的电路，对不同规模矩阵采用嵌入或高斯条带化预处理。研究中训练了m=8的智能体。

**结果:** 在规模n从3到15的矩阵上评估，结果显示随着n值增大，该方法的表现优于当前最优算法。

**结论:** 强化学习方法在CNOT门最小化问题上展现出优势，特别是在处理较大规模量子电路时，为量子计算资源优化提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CNOT+Minimal+Circuit+Synthesis%3A+A+Reinforcement+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23304&send_immediately=true&force_search=false)

**原文摘要:** CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [65] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das, John Duff, Jörg Hoffmann, Vera Demberg*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个基于理性言语行为（RSA）建模框架的自适应信号理论框架，通过多步规划和用户意识建模来优化人机协作中的信息传递时机和特异性。


<details>
  <summary>更多</summary>
  
**动机:** 在快速变化的环境中，确保人类准确理解关键任务信息是一个挑战，因为人类注意力是有限的零和认知资源，需要智能体不仅识别优先级信息，还要估计如何最有效地在何时传递这些信息。

**方法:** 使用贝叶斯参考解析的理性沟通原则，通过Rational Speech Act（RSA）建模框架来规划消息序列，根据用户和场景特点自适应调整消息的特异性和时机，基于多步预测来优化用户信念与环境对齐。

**结果:** 与基线方法相比，该方法在多步规划与真实用户意识模型结合方面表现出关键的有效性，证明了其在动态环境中通信的优越性。

**结论:** 作为RSA在动态环境通信和人类-AI交互中的首次应用，本研究为人机团队的实用沟通建立了理论基础，展示了如何利用认知科学见解来指导辅助智能体的设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Planning+Ahead+with+RSA%3A+Efficient+Signalling+in+Dynamic+Environments+by+Projecting+User+Awareness+across+Future+Timesteps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23340&send_immediately=true&force_search=false)

**原文摘要:** Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [66] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar, A. G. Phakatkar*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于模糊逻辑的细粒度意见挖掘方法，用于从评论文本中提取更详细的属性评价，并基于这些细粒度信息对实体进行排序。


<details>
  <summary>更多</summary>
  
**动机:** 当前社交媒体和电商网站产生了大量意见数据，现有研究主要进行情感分类和基于整体评价的实体排序，但缺乏对意见的细粒度分析和基于细粒度信息的排序方法。

**方法:** 使用模糊逻辑推理方法进行深层次的细粒度意见挖掘，提取评论中针对实体具体属性的评价信息。

**结果:** 开发了一种能够从评论文本中挖掘更细致意见信息的方法，并实现了基于细粒度评价的实体排序。

**结论:** 该方法填补了意见挖掘领域中细粒度分析和基于细粒度信息进行实体排序的研究空白，为更精准的意见分析和实体评价提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Opinion+Mining+Based+Entity+Ranking+using+Fuzzy+Logic+Algorithmic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23384&send_immediately=true&force_search=false)

**原文摘要:** Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [67] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer*

**主要类别:** cs.AI

**AI概要:** AutoStreamPipe是一个基于大语言模型的自动化流处理管道框架，通过超图思维(HGoT)技术实现高效的设计、生成和部署，显著提升开发效率和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 解决流处理管道开发中高层用户意图与平台特定实现之间的语义鸿沟，减少手动开发的时间和错误率。

**方法:** 结合弹性执行策略、高级查询分析和超图思维(HGoT)技术，实现多智能体推理的自动化管道生成。

**结果:** 实验表明，相比传统LLM代码生成方法，开发时间减少6.3倍，错误率降低5.19倍(通过新型无错误评分EFS衡量)。

**结论:** AutoStreamPipe框架有效解决了流处理管道自动化的关键挑战，为实时数据分析提供了高效可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoStreamPipe%3A+LLM+Assisted+Automatic+Generation+of+Data+Stream+Processing+Pipelines，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23408&send_immediately=true&force_search=false)

**原文摘要:** Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [68] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng*

**主要类别:** cs.AI

**AI概要:** 该论文提出了Bid2X竞价基础模型，通过统一函数估计不同竞价场景下的广告效果，使用注意力机制处理异构数据和时间依赖关系，在淘宝平台部署后显著提升了广告效果指标。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自动竞价模型通常针对特定竞价场景设计，缺乏跨环境泛化能力，需要开发一个能够适应不同场景的统一竞价基础模型。

**方法:** 提出Bid2X模型，使用统一序列嵌入处理异构数据，设计两种注意力机制分别处理变量间和时间上的依赖关系，采用变量感知融合模块进行自适应预测，并使用零膨胀投影模块处理数据分布特性。

**结果:** 在8个数据集上的离线评估显示Bid2X优于各种基线方法，在线A/B测试中GMV提升4.65%，投资回报率提升2.44%。

**结论:** Bid2X作为一个竞价基础模型，成功实现了跨场景的泛化能力，为计算广告领域的竞价建模开辟了新途径，并在实际电商平台上验证了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bid2X%3A+Revealing+Dynamics+of+Bidding+Environment+in+Online+Advertising+from+A+Foundation+Model+Lens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23410，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23410&send_immediately=true&force_search=false)

**原文摘要:** Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [69] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi, Amir Saki, Usef Faghihi*

**主要类别:** cs.AI

**AI概要:** 将因果推理与DQN结合，使用PEACE公式估计因果效应，减少虚假相关性，提升强化学习性能


<details>
  <summary>更多</summary>
  
**动机:** 传统DQN依赖关联学习容易获得虚假相关性，限制了其问题解决能力，需要引入因果推理来理解环境中的因果结构

**方法:** 提出新框架将因果原则整合到DQN中，利用PEACE（概率易变因果效应）公式进行因果效应估计，在训练过程中加入因果推理

**结果:** 实验结果表明该方法在标准基准环境中优于传统DQN，显著增强了问题解决能力且不牺牲性能

**结论:** 这项工作为通过原则性因果推断提升深度强化学习智能体能力提供了有前景的途径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Deep+Q+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23424&send_immediately=true&force_search=false)

**原文摘要:** Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [70] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种智能系统来解决网络安全与法律交叉领域的信息检索难题，旨在弥合法律专家与网络安全专业人员之间的知识鸿沟。


<details>
  <summary>更多</summary>
  
**动机:** 网络安全与法律的交叉领域形成了复杂的信息空间，传统法律研究工具难以处理案件、法规和技术漏洞之间的细微联系，这阻碍了法律专家与网络安全专业人员的协作。

**方法:** 开发能够导航复杂网络法律领域的智能系统，作为解决这一重要差距的第一步。

**结果:** 在多语言任务上展示了有希望的初步结果。

**结论:** 这项工作为解决网络法律领域的复杂信息检索问题提供了初步但有前景的解决方案，为未来更深入的跨领域协作奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Neuro-Symbolic+Multi-Agent+Approach+to+Legal-Cybersecurity+Knowledge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23443&send_immediately=true&force_search=false)

**原文摘要:** The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [71] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

**主要类别:** cs.AI

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+are+the+odds%3F+Risk+and+uncertainty+about+AI+existential+risk，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23453&send_immediately=true&force_search=false)

**原文摘要:** This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [72] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi, Muzakkiruddin Ahmed Mohammed, Hendrika Maclean, Mert Can Cakmak, John R. Talburt*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种基于大语言模型的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略进行匹配，实现安全、合规且可追溯的访问决策。


<details>
  <summary>更多</summary>
  
**动机:** 企业需要满足最小权限原则、符合法规要求且可审计的访问决策，传统方法难以有效处理自然语言请求与复杂策略的匹配问题。

**方法:** 使用Google Gemini 2.0 Flash实现六阶段推理框架：上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险综合，采用早期硬策略门控和默认拒绝机制。

**结果:** 在14个典型案例测试中，精确决策匹配率从10/14提升到13/14（92.9%），拒绝召回率达到1.00，必须拒绝场景的错误批准率降为0，功能适当性和合规性均为14/14。

**结论:** 策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策，中位延迟低于一分钟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy-Aware+Generative+AI+for+Safe%2C+Auditable+Data+Access+Governance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23474，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23474&send_immediately=true&force_search=false)

**原文摘要:** Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [73] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani, Shayan Kiyani, George Pappas, Hamed Hassani*

**主要类别:** cs.AI

**AI概要:** 提出人类-AI协作不确定性量化框架，通过AI优化人类专家的预测集，避免对正确判断的损害并补全人类遗漏的结果，在多种任务中优于单独使用人类或AI。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI在不确定性下的鲁棒决策仍缺乏领域知识、长时上下文和物理世界推理能力，需要结合人类与AI的互补优势进行协作决策。

**方法:** 引入人类-AI协作不确定性量化框架，开发基于双阈值结构的离线/在线校准算法，具有分布无关的有限样本保证，能适应分布偏移和人类行为变化。

**结果:** 在图像分类、回归和医疗决策等任务中，协作预测集始终优于单独使用人类或AI，实现了更高的覆盖率和更小的集合大小。

**结论:** 该框架为人类-AI协作提供了理论基础和实用算法，证明了在不确定性量化中结合人类与AI优势的重要性，为可靠决策系统设计提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-AI+Collaborative+Uncertainty+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23476，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23476&send_immediately=true&force_search=false)

**原文摘要:** AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [74] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani, Ziyou Li, Anton Podkopaev, Maliheh Izadi*

**主要类别:** cs.AI

**AI概要:** 该论文建立了现代智能体AI系统架构类别与乔姆斯基层级抽象机器之间的形式等价关系，提出基于内存架构的智能体计算能力分类框架


<details>
  <summary>更多</summary>
  
**动机:** 为智能体系统提供理论基础和形式化验证方法，解决AI智能体的安全性、可预测性和计算效率优化问题

**方法:** 通过建立智能体架构与自动机类别的映射关系：简单反射智能体对应有限自动机，分层任务分解智能体对应下推自动机，具有读写内存的反思智能体对应图灵机

**结果:** 提出了Automata-Agent框架，能够对智能体进行形式化分类，并为概率性LLM智能体扩展了概率自动机框架以支持定量风险分析

**结论:** 该框架为智能体系统的形式验证、安全性保证和静态分析工具开发提供了理论基础，并规划了未来研究方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Agents+Just+Automata%3F+On+the+Formal+Equivalence+Between+Agentic+AI+and+the+Chomsky+Hierarchy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23487，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23487&send_immediately=true&force_search=false)

**原文摘要:** This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [75] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha, Jeong Hun Yeo, Yeonju Kim, Yong Man Ro*

**主要类别:** cs.AI

**AI概要:** 提出情感推理验证器(ERV)和解释奖励机制，解决多模态大语言模型在情感识别中解释与预测不一致的问题，提高情感解释的忠实度和准确性


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型在情感理解时，生成的情感解释往往与目标标签不一致，甚至与自身预测的情感相矛盾，这影响了系统的可解释性和用户信任

**方法:** 提出情感推理验证器(ERV)和解释奖励机制，在不修改模型架构或需要额外视频-描述标注的情况下，引导模型产生与目标情感一致的解释

**结果:** 在MAFW和DFEW数据集上显著提高了解释-预测一致性和解释情感准确性，通过实验和人工评估验证了方法的有效性

**结论:** 该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信的交互，是实现真正类人交互系统的关键一步

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emotion-Coherent+Reasoning+for+Multimodal+LLMs+via+Emotional+Rationale+Verifier，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23506&send_immediately=true&force_search=false)

**原文摘要:** The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [76] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh, Rodrigue Rizk, Longwei Wang*

**主要类别:** cs.AI

**AI概要:** 论文提出Human AI (HAI)框架，通过增量学习、碳感知优化和人机协作，实现可持续、高效和负责任的人工智能发展。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能快速发展带来巨大计算需求和环境伦理问题，批评当前依赖大规模静态数据集和单一训练范式的做法。

**方法:** 引入HAI框架，采用增量学习、碳感知优化、人机协作，借鉴生物认知和动态架构设计。

**结果:** 提出理论基础、系统设计和操作原则，使AI能够持续情境学习，同时减少碳足迹和人工标注成本。

**结论:** HAI框架为解决主动学习、持续适应和节能模型部署等挑战提供了路径，推动负责任、以人为本的人工智能发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Carbon-Neutral+Human+AI%3A+Rethinking+Data%2C+Computation%2C+and+Learning+Paradigms+for+Sustainable+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23524，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23524&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [77] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert*

**主要类别:** cs.AI

**AI概要:** 论文提出了新的基准测试NoRA，旨在解决现有系统性关系推理基准过于简化的问题，要求模型超越基于路径的推理方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有系统性关系推理基准过于简化，假设推理可简化为关系路径组合，导致模型在现有基准表现良好但难以泛化到其他场景。

**方法:** 引入NoRA基准测试，增加多个难度级别，要求模型进行超越路径推理的系统性关系推理。

**结果:** NoRA基准为神经网络系统性关系推理领域提供了更全面、更具挑战性的评估标准。

**结论:** NoRA基准将推动系统性关系推理领域的进一步发展，促使开发更具泛化能力的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+No+Paths+Lead+to+Rome%3A+Benchmarking+Systematic+Neural+Relational+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23532&send_immediately=true&force_search=false)

**原文摘要:** Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [78] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan*

**主要类别:** cs.AI

**AI概要:** JanusCode-800K是最大的多模态代码语料库，支持JanusCoder系列模型的训练，该模型能够从文本指令、视觉输入或两者结合生成代码，在文本和视觉编码任务中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 神经代码智能正从基于文本的源代码扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展，需要解决合成和质量评估的挑战。

**方法:** 开发了一个完整的合成工具包，利用数据模态之间的协同作用，构建了JanusCode-800K语料库，并训练了JanusCoder和JanusCoderV模型，建立了从文本指令、视觉输入或两者结合生成代码的视觉-编程接口。

**结果:** JanusCoder系列模型在文本和视觉编码任务中表现出优越性能，7B到14B规模的模型接近甚至超过商业模型的性能。

**结论:** 该研究通过构建大规模多模态代码语料库和统一模型，成功解决了视觉-编程接口的生成问题，为协调程序逻辑与视觉表达提供了关键见解，代码和检查点已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JanusCoder%3A+Towards+a+Foundational+Visual-Programmatic+Interface+for+Code+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23538&send_immediately=true&force_search=false)

**原文摘要:** The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [79] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis, Stacie Severyn, Fjollë Novakazi, Hadi Banaee, Cogan Shimizu*

**主要类别:** cs.AI

**AI概要:** OntoPret是一个基于认知科学和模块化工程方法的行为解释本体论，旨在解决技术中心化机器人框架与描述性行为本体论之间的研究空白，提供机器可处理的框架来分类人类行为，包括任务偏差和欺骗性动作。


<details>
  <summary>更多</summary>
  
**动机:** 随着人机协作在工业5.0等范式中变得至关重要，机器需要安全有效地解释复杂人类行为。目前存在技术中心化机器人框架缺乏细致人类行为模型，而描述性行为本体论不适合实时协作解释的研究空白。

**方法:** 基于认知科学和模块化工程方法论，开发OntoPret本体论，提供形式化的机器可处理框架，用于行为分类（包括任务偏差和欺骗性动作）。在两个不同用例（制造和游戏）中验证其适应性。

**结果:** 开发了OntoPret本体论框架，能够有效分类人类行为，建立了支持高级人类意图推理所需的语义基础。

**结论:** OntoPret成功填补了现有研究空白，为实时协作环境中的人类行为解释提供了有效的本体论解决方案，为高级意图推理奠定了语义基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OntoPret%3A+An+Ontology+for+the+Interpretation+of+Human+Behavior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23553，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23553&send_immediately=true&force_search=false)

**原文摘要:** As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [80] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu*

**主要类别:** cs.AI

**AI概要:** ReCode提出了一种递归代码生成的新范式，通过将规划和行动统一在单一代码表示中，使LLM智能体能够在不同决策粒度间灵活操作，显著提升了推理性能和数据效率。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界任务需要在不同粒度上进行决策，而当前基于LLM的智能体缺乏这种跨粒度操作能力，因为现有范式将高层次规划与低层次行动严格分离，限制了动态适应性和泛化能力。

**方法:** ReCode将高层次计划视为抽象占位函数，然后递归地将其分解为更细粒度的子函数，直到达到原始行动。这种递归方法消除了规划与行动之间的刚性边界。

**结果:** 大量实验显示ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率。

**结论:** 通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法，递归结构还能生成丰富的多粒度训练数据，使模型能够学习分层决策过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReCode%3A+Unify+Plan+and+Action+for+Universal+Granularity+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23564&send_immediately=true&force_search=false)

**原文摘要:** Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [81] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann, Aleksandra Urman, Ulrich Leicht-Deobald, Zachary J. Roman, Anikó Hannák, Markus Christen*

**主要类别:** cs.AI

**AI概要:** 研究通过瑞士大规模调查发现，ChatGPT推出后公众对AI的接受度下降，对人机监督需求增加，并扩大了教育、语言和性别方面的社会不平等


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI技术快速普及但缺乏对用户偏好的考虑，公众对AI在重要决策场景中的态度研究不足

**方法:** 使用瑞士人口代表性的大规模两波调查（第一波1514人，第二波1488人），比较ChatGPT发布前后的公众态度变化

**结果:** 生成式AI热潮显著降低了公众对AI的接受度，完全不可接受AI的比例从23%升至30%；支持纯人类决策的比例从18%升至26%；扩大了教育、语言和性别差距

**结论:** 研究结果挑战了行业对公众AI接受度的假设，强调技术发展必须与不断变化的公众偏好保持一致的重要性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reduced+AI+Acceptance+After+the+Generative+AI+Boom%3A+Evidence+From+a+Two-Wave+Survey+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23578&send_immediately=true&force_search=false)

**原文摘要:** The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [82] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhan, Mostofa Patwary, Jiaxuan You*

**主要类别:** cs.AI

**AI概要:** MAE是一个多智能体自我进化框架，通过三个角色（提议者、求解者、评判者）的协同进化，使用强化学习提升大语言模型的推理能力，无需人类标注数据，在多个基准测试中平均提升4.54%。


<details>
  <summary>更多</summary>
  
**动机:** 现有强化学习方法依赖人类标注数据和可验证奖励，限制了扩展性和通用性；而现有的自我博弈方法需要特定环境反馈，难以扩展到通用领域。

**方法:** 提出MAE框架，使用单一LLM实例化三个交互智能体：提议者生成问题，求解者尝试解答，评判者进行评估，三者通过强化学习协同进化。

**结果:** 在Qwen2.5-3B-Instruct模型上，MAE在数学、推理和常识问答等多个基准测试中实现了平均4.54%的性能提升。

**结论:** MAE是一种可扩展、数据高效的方法，能够以最小的人类监督依赖显著提升LLM的通用推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Evolve%3A+LLM+Self-Improve+through+Co-evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23595&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [83] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang*

**主要类别:** cs.AI

**AI概要:** ALITA-G是一个自进化框架，通过系统生成、抽象和策划MCP工具，将通用智能体转化为领域专家，在多个基准测试中实现SOTA性能并降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 当前自进化智能体主要局限于提示重写或失败重试，需要更系统的领域适应方法来提升LLM在复杂推理任务中的性能。

**方法:** 通过执行领域任务生成候选MCP工具，抽象为参数化原语并整合到MCP Box中，在推理时进行检索增强的MCP选择并使用MCP执行器执行。

**结果:** 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的SOTA结果，同时相比基线减少约15%的平均token使用量。

**结论:** ALITA-G提供了从通用能力到可重用领域专业能力的原理性路径，显著提升了复杂推理任务的准确性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Alita-G%3A+Self-Evolving+Generative+Agent+for+Agent+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23601&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

**主要类别:** cs.CL

**AI概要:** 该论文介绍了一个包含83.3万个段落的科学文献数据集，这些段落被分类为致谢、数据提及、软件/代码提及和临床试验提及四个类别，主要用于训练文本分类模型和命名实体识别系统。


<details>
  <summary>更多</summary>
  
**动机:** 为科学文献挖掘提供高质量的标注数据集，支持文本分类和命名实体识别模型的训练，促进科学文献的自动化处理和分析。

**方法:** 从CC-BY许可的科学出版物中提取段落，使用fastText进行语言识别，OpenAlex进行科学领域标注，通过GROBID处理法国开放科学监测器语料库。

**结果:** 创建了一个包含833k段落的多样化数据集，涵盖多种语言和科学领域，并在HuggingFace平台上公开提供。

**结论:** 该数据集为科学文献挖掘研究提供了有价值的资源，支持后续的文本分类和实体识别任务，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-lingual+Dataset+of+Classified+Paragraphs+from+Open+Access+Scientific+Publications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21762&send_immediately=true&force_search=false)

**原文摘要:** We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [85] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal, Aakash Sen Sharma, Dhruv Kumar, Saurabh Deshpande, Murari Mandal*

**主要类别:** cs.CL

**AI概要:** 研究发现策略优化在开放式思维链结构中倾向于选择最简单路径，即使给复杂格式更高奖励权重，模型仍会丢弃显式推理而退化到仅输出答案的格式，揭示了奖励破解的关键挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前策略优化算法在严格思维链格式下表现良好，但在开放式思维链结构中的行为尚未充分研究，需要探索当放松格式约束时策略优化的行为模式。

**方法:** 通过一系列受控实验和奖励分解实验，分析不同模型和算法在开放式思维链结构中的行为，研究KL正则化策略的优化过程。

**结果:** 策略优化始终遵循最小阻力路径，即使复杂格式奖励权重提高4倍，模型仍会丢弃推理步骤；优化过程系统性地优先优化最简单的奖励组件。

**结论:** 给予策略发散自由是一把双刃剑：虽然能发现高奖励捷径，但也导致模型倾向于利用奖励函数最简单部分，这对对齐中的奖励破解提出了关键挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy+Optimization+Prefers+The+Path+of+Least+Resistance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21853&send_immediately=true&force_search=false)

**原文摘要:** Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [86] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang, Tianqi Du, Jizhe Zhang, Mingqing Xiao, Yifei Wang, Yisen Wang, Zhouchen Lin*

**主要类别:** cs.CL

**AI概要:** 论文提出Language Ranker框架，将LLM解码过程重新概念化为推荐系统中的排序阶段，通过轻量级重排模块提升响应质量，在保持性能的同时大幅降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM研究主要关注输出分布优化，而忽略了解码过程的重要性。现有方法如奖励模型计算成本高且适用性有限，存在冗余等问题。

**方法:** 借鉴推荐系统思想，将解码过程类比为排序阶段，提出Language Ranker框架，使用基础模型提取的特征通过轻量级模块对候选响应进行重排序。

**结果:** 在多种任务上的实验表明，该方法仅需增加<0.5M参数就能达到与大规模奖励模型相当的性能，显著降低了训练和推理阶段的计算开销。

**结论:** 该方法高效且有效，有潜力充分释放LLM的能力，为解码过程优化提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Ranker%3A+A+Lightweight+Ranking+framework+for+LLM+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21883&send_immediately=true&force_search=false)

**原文摘要:** Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [87] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

**主要类别:** cs.CL

**AI概要:** 本文提出RACE框架，通过比较LLM生成解释与逻辑回归特征重要性评分的一致性，评估LLM解释的忠实度和完整性，发现在正确预测中支持特征覆盖率更高，错误预测中矛盾特征覆盖率更高。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习在敏感领域的应用增加，对透明可解释AI的需求日益增长。虽然LLM能够生成自然语言解释，但这些解释是否真实反映预测信号尚不明确。

**方法:** 提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归基线的特征重要性评分，使用词元感知、精确字符串和编辑距离匹配技术从多粒度层面捕捉对齐关系。

**结果:** 实证结果显示一致的不对称性：正确预测显示更高的支持特征覆盖率，而错误预测与更高的矛盾特征覆盖率相关。编辑距离匹配发现释义重叠，在保持不对称性的同时提高了覆盖率。

**结论:** LLM解释结合了表面层面和灵活的证据重用，但在错误情况下也会放大误导性线索。RACE为评估LLM解释的忠实度提供了新见解，并为评估神经语言模型的推理完整性建立了量化基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Framework+for+Machine+Evaluation+of+Reasoning+Completeness+in+Large+Language+Models+For+Classification+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21884&send_immediately=true&force_search=false)

**原文摘要:** The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [88] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham, Mihir Thalanki, Michael Sun, Aditya Chaloo, Ankita Gupta, Tian Xia, Aditya Mate, Ehimwenma Nosakhare, Soundararajan Srinivasan*

**主要类别:** cs.CL

**AI概要:** 提出了一个行为感知采样框架，通过基于指令-响应行为和语义多样性选择安全示例，有效减少大语言模型在微调时的安全行为遗忘问题，仅需0.5%额外训练数据即可减少41%有害输出。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在良性数据微调时经常出现安全行为遗忘现象，现有方法虽然可以通过添加随机安全示例缓解，但哪些示例最有效仍不清楚。

**方法:** 提出行为感知采样框架，基于两个互补因素选择安全示例：指令-响应行为（拒绝vs服从）和跨危害类别的语义多样性。

**结果:** 系统评估显示该方法显著减少有害输出同时保持帮助性，仅用0.5%额外训练数据就实现了高达41%的有害性减少。

**结论:** 定向数据选择可以显著提高大规模微调的安全性和效率，为模型安全优化提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preventing+Catastrophic+Forgetting%3A+Behavior-Aware+Sampling+for+Safer+Language+Model+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21885&send_immediately=true&force_search=false)

**原文摘要:** Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [89] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj, Julia Kempe, Tim G. J. Rudner*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种基于语义各向同性（semantic isotropy）的低成本方法来评估大语言模型长文本响应的可信度，通过计算文本嵌入在单位球面上的角度离散度来预测事实一致性，无需标注数据或微调。


<details>
  <summary>更多</summary>
  
**动机:** 在需要准确回答开放式提示的高风险应用领域部署大语言模型时，需要可靠且计算成本低廉的方法来评估长文本响应的可信度，而现有的逐项事实核查方法计算昂贵且脆弱。

**方法:** 通过生成多个长文本响应，将其嵌入到单位球面上，并计算这些嵌入的角度离散度（语义各向同性程度）来评估响应的事实一致性。

**结果:** 研究发现较高的语义各向同性（即更大的嵌入离散度）可靠地表明样本间的事实一致性较低。该方法在多个领域都优于现有方法，仅需少量样本即可预测非事实性。

**结论:** 该方法提供了一种实用、低成本的信任评估方法，可以集成到实际的大语言模型工作流程中，无需标注数据、微调或超参数选择，适用于开放或封闭权重的嵌入模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedding+Trust%3A+Semantic+Isotropy+Predicts+Nonfactuality+in+Long-Form+Text+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21891&send_immediately=true&force_search=false)

**原文摘要:** To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [90] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing, Chang Tian, Jianan Zhang, Lichen Pan, Peipei Liu, Zhaoteng Yan, Yinliang Yue*

**主要类别:** cs.CL

**AI概要:** NetMind是一个使用自然语言查询网络的框架，通过树形配置分块、统一事实图和混合命令-声明语言解决LLM在网络配置分析中的三大挑战，实现了准确且可扩展的网络行为理解。


<details>
  <summary>更多</summary>
  
**动机:** 大规模网络配置复杂，传统基于领域特定语言的方法学习曲线陡峭且灵活性有限。自然语言接口更易用，但LLM在处理长配置文件、设备异构性和复杂网络推理方面存在挑战。

**方法:** 1) 树形配置分块策略保持语义连贯性；2) 构建统一事实图作为中间表示来规范化厂商特定配置；3) 设计混合命令-声明语言减轻LLM推理负担

**结果:** 实验表明NetMind在准确性和可扩展性方面优于现有基线方法

**结论:** NetMind框架成功解决了LLM在网络配置分析中的关键挑战，为自然语言驱动的网络行为理解提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Network+Behaviors+through+Natural+Language+Question-Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21894&send_immediately=true&force_search=false)

**原文摘要:** Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [91] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang, Han Cui, Yidong Wang, Yijian Tian, Qi Guo, Cunxiang Wang, Jian Wu, Chiyu Song, Yue Zhang*

**主要类别:** cs.CL

**AI概要:** IterSurvey是一个基于循环大纲生成的文献综述自动生成框架，通过迭代检索、阅读和更新大纲来提升综述质量，在内容覆盖、结构连贯性和引用质量方面显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有文献综述生成系统采用一次性检索和静态大纲生成，导致检索噪声大、结构碎片化和上下文过载，限制了综述质量。受人类研究者迭代阅读过程的启发，需要更智能的生成方法。

**方法:** 提出IterSurvey框架，包含规划代理进行增量检索和阅读，设计论文卡片提取每篇论文的贡献、方法和发现，引入审阅-精炼循环和可视化增强来改善文本流并整合多模态元素。

**结果:** 在成熟和新兴主题上的实验表明，IterSurvey在内容覆盖、结构连贯性和引用质量方面大幅优于最先进基线方法，生成更易访问和组织更好的综述。

**结论:** IterSurvey通过迭代式大纲生成和论文级基础验证，显著提升了自动文献综述的质量，同时引入Survey-Arena评估基准为机器生成与人工撰写综述提供了更可靠的对比评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Literature+Survey+Automation+with+an+Iterative+Workflow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21900&send_immediately=true&force_search=false)

**原文摘要:** Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [92] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett, Tyler A. Chang, Stella Biderman, Benjamin K. Bergen*

**主要类别:** cs.CL

**AI概要:** 本文研究了多语言文本编码中的token溢价现象，发现即使控制数据集大小、词汇表大小和数据内容，单语分词器在不同语言间仍存在显著token溢价差异。通过训练约7000个单语分词器并分析影响因素，发现词汇表大小和预分词策略是关键因素，而训练测试数据相似性影响不大。


<details>
  <summary>更多</summary>
  
**动机:** 不同语言在编码为token时存在数量差异（token溢价），这会导致训练吞吐量降低和推理成本增加。研究旨在理解导致这些跨语言差异的根本原因。

**方法:** 训练了约7000个可比单语分词器，涵盖97种语言，控制分词算法、词汇表大小和数据集大小等变量，测量token溢价并分析数据相似性、词汇表大小、预分词策略以及语言特定特征的影响。

**结果:** 发现训练测试数据相似性不影响token溢价，但词汇表大小和预分词策略是关键因素。确定每种语言的'最优'词汇表大小可显著降低token溢价效应。超级词分词器（允许跨空格合并）能同时降低token溢价并提高整体压缩率。

**结论:** 通过干预词汇表大小或预分词器可以显著减少跨语言token溢价效应，这为多语言NLP系统的效率优化提供了重要指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+and+Mitigating+Crosslingual+Tokenizer+Inequities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21909&send_immediately=true&force_search=false)

**原文摘要:** The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [93] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk, Aleksander Smywiński-Pohl*

**主要类别:** cs.CL

**AI概要:** MATT方法通过注意力影响建模将源模型的token间通信模式蒸馏到新tokenizer的目标模型中，相比仅关注嵌入相似性的启发式方法，能更有效地实现多语言LLM的tokenizer迁移。


<details>
  <summary>更多</summary>
  
**动机:** 现有tokenizer迁移方法主要依赖语义启发式来初始化新嵌入，忽略了高层模型动态，限制了迁移质量。LLMs支持的语言越来越多，但预定义tokenizer仍然是适应低资源或不同文字语言时的瓶颈。

**方法:** 提出Model-Aware Tokenizer Transfer (MATT)方法，引入Attention Influence Modeling (AIM)目标，将源模型的token间通信模式蒸馏到使用新tokenizer的目标模型中，为标准语言建模提供高效预热。

**结果:** 在不同语言设置下的实验显示，MATT在几个GPU小时内就能恢复原模型大部分性能，优于启发式基线方法。

**结论:** 将模型级信号纳入tokenizer迁移过程为多语言LLMs提供了实用且有效的鲁棒tokenizer迁移路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Aware+Tokenizer+Transfer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21954&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [94] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay, Jiayi Chen, Mohammad J. Latifi, Daniel N. Rockmore, Jeremy R. Manning*

**主要类别:** cs.CL

**AI概要:** 该论文展示如何通过训练单独的GPT-2模型来识别不同作者的写作风格，模型能更准确地预测目标作者的文本，并成功应用于确认R. P. Thompson对《绿野仙踪》系列第15本书的作者身份


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型在作者识别方面的应用潜力，验证模型是否能捕捉和体现特定作者的独特写作风格

**方法:** 为每位作者从头开始训练单独的GPT-2模型，然后比较这些模型在预测目标作者与其他作者文本时的准确性差异

**结果:** 模型能够更准确地预测训练时所针对作者的文本，成功区分了八位不同作者的写作风格，并确认了R. P. Thompson对《绿野仙踪》第15本书的作者身份

**结论:** 大型语言模型能够有效捕捉和体现特定作者的独特写作风格，为作者识别和文本归属分析提供了新的技术手段

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Stylometric+Application+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21958&send_immediately=true&force_search=false)

**原文摘要:** We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [95] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha*

**主要类别:** cs.CL

**AI概要:** 本研究探讨如何利用社会科学中的说服理论来设计对抗性提示，成功绕过大型语言模型的安全对齐机制，揭示了模型对具有说服力结构的提示更易产生有害输出。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注攻击策略的可读性和可迁移性，但忽视了语言和心理机制对模型脆弱性的影响。研究者希望通过跨学科方法，利用人类说服理论来测试和增强LLM的安全性。

**方法:** 基于社会科学中成熟的说服策略理论，设计具有说服结构的对抗性提示，并在多个已对齐的LLM上进行实证评估。

**结果:** 实验结果显示，具有说服意识的提示能显著绕过模型的安全防护，诱导出越狱行为，证明了说服策略在攻击中的有效性。

**结论:** 这项工作强调了跨学科见解在应对LLM安全挑战中的重要性，表明需要从心理学和语言学角度深入理解模型脆弱性，以开发更强大的安全措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+the+Persuasive+Fingerprint+of+LLMs+in+Jailbreaking+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21983&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [96] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball, Niki Hasrati, Alexander Robey, Avi Schwarzschild, Frauke Kreuter, Zico Kolter, Andrej Risteski*

**主要类别:** cs.CL

**AI概要:** 该论文分析了离散优化越狱攻击的迁移性机制，发现三个统计属性与迁移成功强相关：原始提示激活拒绝方向的程度、后缀推离拒绝方向的强度、以及正交方向上的位移大小，而语义相似性相关性较弱。


<details>
  <summary>更多</summary>
  
**动机:** 虽然离散优化越狱攻击生成的短后缀具有跨提示和模型的迁移性，但缺乏对迁移何时发生及为何发生的严格分析，需要填补这一理论空白。

**方法:** 通过大量实验识别与迁移成功强相关的统计属性，包括拒绝方向激活度、推离拒绝方向的强度、正交位移大小，并进行干预实验验证。

**结果:** 发现三个统计属性与迁移成功强相关，而语义相似性仅弱相关，这些发现可用于实际提升攻击成功率。

**结论:** 研究提供了对越狱攻击迁移性的细粒度理解，统计分析方法可转化为实际攻击效果的提升，为防御机制设计提供 insights。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Understanding+the+Transferability+of+Adversarial+Suffixes+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22014&send_immediately=true&force_search=false)

**原文摘要:** Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [97] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang, Wenda Xu, Zhongtao Liu, Tetsuji Nakagawa, Markus Freitag*

**主要类别:** cs.CL

**AI概要:** 研究发现机器翻译质量评估(QE)指标存在严重长度偏差：过度预测长文本错误且偏好短译文，提出长度归一化和参考文本两种缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 质量评估指标在机器翻译中至关重要，但长度偏差的普遍性和影响尚未充分研究，可能对长文本造成不公平惩罚并导致次优决策。

**方法:** 对10种语言对的顶尖回归基和LLM评估QE指标进行系统研究，分析长度偏差现象。

**结果:** 发现QE指标存在两种关键长度偏差：1)随译文长度增加而过度预测错误；2)对同一源文本偏好较短译文。

**结论:** 提出的长度归一化训练和参考文本评估两种策略能有效缓解长度偏差问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Penalizing+Length%3A+Uncovering+Systematic+Bias+in+Quality+Estimation+Metrics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22028&send_immediately=true&force_search=false)

**原文摘要:** Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [98] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre, Sneha Kudugunta, Niklas Muennighoff, I-Hung Hsu, Isaac Caswell, Alex Pentland, Sercan Arik, Chen-Yu Lee, Sayna Ebrahimi*

**主要类别:** cs.CL

**AI概要:** 该研究进行了迄今为止最大的多语言扩展定律研究，涵盖774个多语言训练实验，提出了优于现有方法的自适应迁移扩展定律(ATLAS)，并揭示了多语言学习的动态特性、语言间迁移规律以及多语言性的计算权衡点。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩展定律研究主要集中于英语，但主流AI模型需要服务数十亿国际用户，因此需要研究多语言环境下的扩展规律。

**方法:** 进行了774个多语言训练实验，覆盖10M-8B参数规模、400+训练语言和48种评估语言，提出了自适应迁移扩展定律(ATLAS)，并推导了跨语言迁移矩阵和语言无关的扩展定律。

**结果:** ATLAS定律在样本外泛化能力上比现有扩展定律平均提升超过0.3 R²，建立了38×38=1444种语言对的相互受益评分矩阵，确定了从头训练与多语言检查点微调的计算权衡点。

**结论:** 该研究为多语言扩展定律的民主化提供了科学基础，使实践者能够高效地扩展超越英语优先AI的多语言模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ATLAS%3A+Adaptive+Transfer+Scaling+Laws+for+Multilingual+Pretraining%2C+Finetuning%2C+and+Decoding+the+Curse+of+Multilinguality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22037&send_immediately=true&force_search=false)

**原文摘要:** Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [99] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman, Adar Avsian, Larry Heck*

**主要类别:** cs.CL

**AI概要:** 论文研究发现大语言模型内部存在低维情感流形，情感表征具有方向性编码且分布在各层中，该结构在不同深度保持稳定并能泛化到多语言情感数据集，可通过干预模块操控情感感知。


<details>
  <summary>更多</summary>
  
**动机:** 探究大语言模型如何在内部表示情感，分析其隐藏状态空间的几何结构，以理解模型对情感的内部化处理机制。

**方法:** 分析LLM隐藏状态空间的几何结构，识别低维情感流形，研究情感表征的方向性编码和层级分布，使用跨领域对齐和线性探针评估，开发干预模块进行情感操控实验。

**结果:** 发现稳定且可泛化的情感子空间结构，跨语言情感数据集对齐误差低且线性探针性能强，干预模块能有效操控情感感知同时保持语义完整性，特别对基本情感跨语言控制效果显著。

**结论:** LLMs内部存在一致且可操控的情感几何结构，揭示了模型内部化和处理情感的机制，为理解AI情感表征提供了新见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emotions+Where+Art+Thou%3A+Understanding+and+Characterizing+the+Emotional+Latent+Space+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22042&send_immediately=true&force_search=false)

**原文摘要:** This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [100] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

**主要类别:** cs.CL

**AI概要:** 本研究比较了六种减少大语言模型性别偏见的技术，发现监督微调(SFT)在合规性和词汇多样性方面表现最佳，而基于偏好的方法(如DPO)在处理组合约束时失败，表明明确的监督学习对于消除偏见更有效。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型即使在性别中立的语境中仍会产生性别刻板印象语言，反映了深层社会偏见。虽然已有多种缓解方法，但各种方法的比较效果和学习动态仍不清楚。

**方法:** 比较分析了六种偏见缓解控制技术：提示词方法、生成后过滤、DFA-based Ctrl-G解码、监督微调(SFT)、直接偏好优化(DPO)和迭代零空间投影(INLP)。在组合约束任务上进行评估，要求为20个职业生成包含至少一个主动性和一个社群性描述符的句子。

**结果:** SFT达到99.87%的合规性和高词汇多样性；DPO仅达到4.53%合规性；Ctrl-G保证完全合规但严重降低流畅性和多样性；基于偏好的方法无法满足组合约束要求。

**结论:** 只有明确的正面监督能够缓解组合偏见，基于偏好的对齐方法无法泛化逻辑结构，这凸显了偏好学习的局限性和明确监督对于公平流畅控制生成的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compositional+Bias+Control+in+Large+Language+Models%3A+Preference+Learning+Fails%2C+Supervision+Succeeds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22084&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [101] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

**主要类别:** cs.CL

**AI概要:** 提出了一个统一框架来理解和控制LLMs的泛化与记忆模式，基于信息瓶颈理论开发了动态模式导向算法，显著提升了逻辑一致性和事实准确性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型存在泛化能力和机械记忆的双重性，这种不可预测性在高风险应用中降低了可靠性。

**方法:** 基于信息瓶颈原理建立理论模型，开发动态模式导向(DMS)算法，包含轻量级线性探针和动态激活导向机制，实现推理时的自适应控制。

**结果:** 在推理和真实性任务上的实验表明，DMS显著改善了逻辑一致性和事实准确性。

**结论:** DMS提供了一个原则性方法来增强LLM的可靠性，通过控制模型在泛化和记忆模式之间的转换。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+or+Memorization%3A+Dynamic+Decoding+for+Mode+Steering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22099&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [102] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson, Zoran Tiganj*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种通过输入令牌的对数压缩而非修改Transformer架构来处理长上下文的方法，在WikiText-103和PG-19基准测试中降低了困惑度并提升了长程记忆性能


<details>
  <summary>更多</summary>
  
**动机:** 大多数长上下文处理方法通过集成循环或辅助记忆模块增加了Transformer架构的复杂性，需要寻找更简单的替代方案

**方法:** 受人类记忆认知模型启发，对输入令牌应用尺度不变的对数压缩，生成压缩表示后由标准未修改的Transformer处理

**结果:** 在WikiText-103和PG-19语言建模基准测试中，相比未压缩基线降低了困惑度，且随着压缩时间上下文的延长性能持续提升

**结论:** 输入级别的对数压缩是扩展Transformer长程记忆的一种简单有效方法，保持了架构的简洁性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradual+Forgetting%3A+Logarithmic+Compression+for+Extending+Transformer+Context+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22109&send_immediately=true&force_search=false)

**原文摘要:** Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [103] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team, Ang Li, Ben Liu, Binbin Hu, Bing Li, Bingwei Zeng, Borui Ye, Caizhi Tang, Changxin Tian, Chao Huang, Chao Zhang, Chen Qian, Chenchen Ju, Chenchen Li, Chengfu Tang, Chili Fu, Chunshao Ren, Chunwei Wu, Cong Zhang, Cunyin Peng, Dafeng Xu, Daixin Wang, Dalong Zhang, Dingnan Jin, Dingyuan Zhu, Dongke Hu, Fangzheng Zhao, Feifan Wu, Feng Zhu, Gangshan Wang, Haitao Zhang, Hailin Zhao, Hanxiao Zhang, Hanzi Wang, Hao Qian, Haoyi Yu, Heng Zhang, Hongliang Zhang, Hongzhi Luan, Huirong Dong, Huizhong Li, Jia Li, Jia Liu, Jialong Zhu, Jian Sha, Jianping Wei, Jiaolong Yang, Jieyue Ma, Jiewei Wu, Jinjing Huang, Jingyun Tian, Jingyuan Zhang, Jinquan Sun, Juanhui Tu, Jun Liu, Jun Xu, Jun Zhou, Junjie Ou, Junpeng Fang, Kaihong Zhang, Kaiqin Hu, Ke Shi, Kun Tang, Kunlong Chen, Lanyin Mei, Lei Liang, Lei Xu, Libo Zhang, Lin Ju, Lin Yuan, Ling Zhong, Lintao Ma, Lu Liu, Lu Yu, Lun Cai, Meiqi Zhu, Mengying Li, Min Chen, Minghao Xue, Minghong Cai, Mingming Yin, Peijie Jiang, Peilong Zhao, Pingping Liu, Qian Zhao, Qing Cui, Qingxiang Huang, Qingyuan Yang, Quankun Yu, Shaowei Wei, Shijie Lian, Shoujian Zheng, Shun Song, Shungen Zhang, Shuo Zhang, Siyuan Li, Song Liu, Ting Guo, Tong Zhao, Wanli Gu, Weichang Wu, Weiguang Han, Wenjing Fang, Wubin Wang, Xiang Shu, Xiao Shi, Xiaoshun Lan, Xiaolu Zhang, Xiaqing Sun, Xin Zhao, Xingyu Lu, Xiong Xu, Xudong Wang, Xudong Wang, Xuemin Yang, Yajie Yang, Yang Xiang, Yanzhe Li, Yi Zhang, Yilong Wang, Yingxue Li, Yongzhen Guo, Yuzhuo Fu, Yuanyuan Wang, Yue Yang, Yue Yu, Yufeng Deng, Yun Zhang, Yunfei Xu, Yuqi Zhang, Yuxiao He, Zengke Gui, Zhaoxin Huan, Zhaoyang Wang, Zhibo Zhu, Zhihao Wang, Zhiqiang Zhang, Zhoufei Wang, Zihang Zeng, Ziqi Liu, Zitao Xuan, Zuoli Tang*

**主要类别:** cs.CL

**AI概要:** Ling 2.0是一个基于稀疏激活MoE架构的推理导向语言模型系列，包含从160亿到1万亿参数的三个模型，在计算效率上相比密集模型提升高达7倍，建立了推理精度与计算效率的新帕累托前沿。


<details>
  <summary>更多</summary>
  
**动机:** 基于"每个激活都能增强推理能力"的原则，旨在通过高稀疏性和跨尺度一致性，在统一MoE范式下构建可扩展的高效推理基础模型。

**方法:** 采用高稀疏MoE架构配合MTP技术、推理导向数据、中期训练CoT激活、基于强化的微调（DFT、Evo-CoT），以及全尺度FP8训练和细粒度异构流水线。

**结果:** Ling-1T在万亿参数规模上建立了推理精度与计算效率的新帕累托前沿，证明稀疏激活与推理目标正确对齐时可实现可扩展的高效智能。

**结论:** Ling 2.0为推进未来推理和思维模型提供了一个连贯、开放且高效的基础，包括基于相同基础的Ring系列模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Every+Activation+Boosted%3A+Scaling+General+Reasoner+to+1+Trillion+Open+Language+Foundation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22115，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22115&send_immediately=true&force_search=false)

**原文摘要:** We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [104] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao, Jundong Shen, Bei Shi, Jiapeng Wang, Ying Ju, Junfeng Yao, Jiao Ran, Yong Zhang, Lin Dong, Huiyu Yu, Tingting Ye*

**主要类别:** cs.CL

**AI概要:** OlaMind是一个基于检索增强生成的人类化、防幻觉客服框架，通过两阶段学习（Learn-to-Think和Learn-to-Respond）显著提升智能解决率和降低人工接管率。


<details>
  <summary>更多</summary>
  
**动机:** 现有检索增强生成客服系统存在幻觉问题和机械式回复，可能带来业务风险并影响用户体验。

**方法:** 采用两阶段方法：先学习人类专家的推理过程和响应策略（Learn-to-Think），然后结合监督微调和强化学习进行冷启动自优化（Learn-to-Respond）。

**结果:** 在工业级社交客服环境中，社区支持场景智能解决率提升28.92%，直播互动场景提升18.42%；人工接管率分别降低6.08%和7.12%。

**结论:** OlaMind框架在多样化实际应用中展现出一致有效性，显著提升人类化程度和自然性，同时有效缓解幻觉和关键业务风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OlaMind%3A+Towards+Human-Like+and+Hallucination-Safe+Customer+Service+for+Retrieval-Augmented+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22143&send_immediately=true&force_search=false)

**原文摘要:** Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [105] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan, Mahendra Kumar Gurve, Anuj, Nitin, Yamuna Prasad*

**主要类别:** cs.CL

**AI概要:** 该论文针对低资源语言迈蒂利语，构建了首个包含情感极性和自然语言解释的基准数据集，通过专家标注确保质量，并验证了其在可解释情感分析中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 迈蒂利语作为印度超过1300万人使用的印欧语系语言，在自然语言处理研究中代表性不足，缺乏细粒度标注和可解释性机制的情感分析资源。

**方法:** 创建包含3221个迈蒂利语句子的数据集，由语言专家标注情感极性和提供自然语言解释（使用迈蒂利语撰写），确保标签可靠性和上下文保真度。

**结果:** 使用经典机器学习和最先进的transformer架构进行广泛实验，证明该数据集在可解释情感分析中的有效性。

**结论:** 这项工作为迈蒂利语建立了首个可解释情感计算的基准，为多语言NLP和可解释AI的广泛发展贡献了宝贵资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SentiMaithili%3A+A+Benchmark+Dataset+for+Sentiment+and+Reason+Generation+for+the+Low-Resource+Maithili+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22160&send_immediately=true&force_search=false)

**原文摘要:** Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [106] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova, Alessia Battisti, Lukas Fischer, Yingqiang Gao*

**主要类别:** cs.CL

**AI概要:** 本文提出了DETECT，第一个专门针对德语文本自动简化(ATS)的评估指标，该指标在简洁性、意义保持和流畅性三个维度上全面评估ATS质量，完全基于合成的大语言模型响应进行训练。


<details>
  <summary>更多</summary>
  
**动机:** 当前德语ATS评估依赖通用指标如SARI、BLEU和BERTScore，这些指标在捕捉简化质量方面不足。德语缺乏专门的评估指标和人工标注语料库。

**方法:** 采用LENS框架并针对德语进行适配，通过(i)基于LLM生成合成质量分数的流程，(ii)基于LLM的分级标准对齐细化步骤，构建了无需人工标注的数据集。

**结果:** DETECT与人工评估的相关性显著高于广泛使用的ATS指标，在意义保持和流畅性方面表现尤为突出。

**结论:** 研究不仅填补了德语ATS评估的空白，还揭示了LLM在自动评估方面的潜力和局限性，为通用语言可访问性任务提供了可转移的指导方针。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DETECT%3A+Determining+Ease+and+Textual+Clarity+of+German+Text+Simplifications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22212&send_immediately=true&force_search=false)

**原文摘要:** Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [107] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

**主要类别:** cs.CL

**AI概要:** 本研究提出了一种无需真实标签的方法来测量LLM在成对文本比较中的错误率，通过Copeland计数构建排名并发现LLM比较的可扩展性差，在测试6个主流LLM后发现Claude表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 需要量化评估大型语言模型在成对文本比较任务中的错误概率，特别是位置偏差对比较结果的影响，而现有方法依赖于真实标签或无法准确测量错误率。

**方法:** 提出两种场景的误差估计方法：(i)统一错误率，通过每个文本对两次比较估计；(ii)二元位置偏差，通过重复比较估计不同顺序的错误率。使用Copeland计数从成对偏好构建文本排名。

**结果:** 测试了6个LLM（ChatGPT、Claude、DeepSeek、Gemini、Grok、Qwen）在五种文本类型上的表现，发现两个位置偏差项相似且接近统一错误率，Claude在错误率和提示鲁棒性方面表现最佳。

**结论:** 该方法在指示LLM比较错误方面优于有偏Bradley-Terry模型和交换性评分，为评估LLM比较性能提供了有效工具，揭示了LLM基于成对比较的可扩展性限制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+the+Error+of+Large+Language+Models+at+Pairwise+Text+Comparison，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22219，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22219&send_immediately=true&force_search=false)

**原文摘要:** We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [108] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

**主要类别:** cs.CL

**AI概要:** 本文批判性分析了Swadesh方法在语言年代学中的局限性，指出即使满足所有假设也存在数学精度限制，并提出词汇渐进修改过程作为重要补充因素，可显著提高时间分离估计的精度。


<details>
  <summary>更多</summary>
  
**动机:** Swadesh方法基于词汇替换的随机过程来估计语言分离时间，但其基本假设常因污染现象和误判而不现实，且即使假设完美满足，数学上也存在固有的精度限制，这些限制在以往研究中常被忽视。

**方法:** 从纯概率角度详细分析Swadesh方法的数学精度限制，并引入词汇渐进修改这一第二随机过程，通过概率建模展示其对词汇演变的重要贡献。

**结果:** 研究发现Swadesh方法存在固有的概率性精度限制，同时证实词汇渐进修改过程是词汇演变的重要驱动力，考虑此过程可显著提高时间分离估计的精确度。

**结论:** 语言年代学研究需要同时考虑词汇替换和渐进修改两个随机过程，单纯依赖Swadesh方法存在根本性局限，结合两种过程可提供更准确的语言分离时间估计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolution+of+the+lexicon%3A+a+probabilistic+point+of+view，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22220&send_immediately=true&force_search=false)

**原文摘要:** The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [109] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

**主要类别:** cs.CL

**AI概要:** 论文提出了"Sculpting"提示方法，相比标准CoT能减少语义歧义和常识错误，但在不同能力模型上效果不同：在gpt-4o上表现更好，在更强的gpt-5上反而变差，揭示了提示策略需要与模型能力协同进化。


<details>
  <summary>更多</summary>
  
**动机:** 标准Chain-of-Thought提示方法存在语义歧义和常识推理错误的问题，需要开发更有效的提示工程方法来提升大语言模型的推理能力。

**方法:** 提出基于规则的约束性提示方法"Sculpting"，在GSM8K数学推理基准上对比了三种提示策略（Zero Shot、标准CoT、Sculpting）在三个OpenAI模型（gpt-4o-mini、gpt-4o、gpt-5）上的表现。

**结果:** 发现"提示反转"现象：Sculpting在gpt-4o上表现优于标准CoT（97% vs 93%），但在gpt-5上反而变差（94.00% vs 96.36%）。约束条件在中级模型中防止错误，在高级模型中却导致过度字面化理解。

**结论:** 最优提示策略必须与模型能力协同进化，更强大的模型可能需要更简单的提示方法，避免过度约束导致的性能下降。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是You+Don%27t+Need+Prompt+Engineering+Anymore%3A+The+Prompting+Inversion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22251&send_immediately=true&force_search=false)

**原文摘要:** Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [110] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao, Ming Yan, Yilun Qiu, Haoting Ni, Yang Zhang, Fuli Feng, Hong Cheng, Tat-Seng Chua*

**主要类别:** cs.CL

**AI概要:** SteerX是一种基于因果推理的个性化LLM激活导向方法，通过分离偏好驱动和偏好无关的组件，提升个性化效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有激活导向方法使用所有历史数据计算导向向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号。

**方法:** 基于因果推理理论，估计token级别的因果效应来识别偏好驱动的token，将这些离散信号转化为连贯描述，用于指导个性化LLM生成。

**结果:** 在两个代表性导向骨干方法和真实数据集上的实验表明，SteerX持续提升导向向量质量。

**结论:** SteerX通过专注于真正偏好驱动的信息，提供了更有效的LLM个性化实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SteerX%3A+Disentangled+Steering+for+LLM+Personalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22256&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [111] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou, Denis Cavallucci*

**主要类别:** cs.CL

**AI概要:** PatenTEB是一个针对专利文本嵌入的综合性基准测试，包含15个任务、206万个样本，解决了现有基准测试在专利领域特异性挑战方面的不足，并提出了patembed模型家族在多个任务上达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试无法充分捕捉专利文本嵌入的特殊挑战，如领域分层、非对称片段-文档匹配等专利特定场景。

**方法:** 开发PatenTEB基准测试，采用领域分层划分、领域特定难负样本挖掘；通过多任务训练开发patembed模型家族（67M-344M参数，最长4096 tokens上下文）。

**结果:** patembed-base在MTEB BigPatentClustering.v2上达到0.494 V-measure（之前最佳为0.445）；patembed-large在DAPFAM上达到0.377 NDCG@100；多任务训练显著提升外部泛化能力。

**结论:** PatenTEB填补了专利文本嵌入基准测试的空白，patembed模型家族在多个专利相关任务上表现优异，多任务训练和领域预训练初始化带来一致优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PatenTEB%3A+A+Comprehensive+Benchmark+and+Model+Family+for+Patent+Text+Embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22264&send_immediately=true&force_search=false)

**原文摘要:** Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [112] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh, Philipp Nicolas Schumacher, Jan Niehues*

**主要类别:** cs.CL

**AI概要:** 研究发现将大学课程材料（特别是包含视觉元素的幻灯片）通过多模态检索增强生成(RAG)方式整合到LLM中，比持续预训练(CPT)更有效地提升LLM在计算机科学课程问答中的表现。


<details>
  <summary>更多</summary>
  
**动机:** LLMs在教育场景中支持学生学习时，在大学计算机科学课程问答中仍存在准确性问题，需要探索如何通过整合课程材料来提升性能。

**方法:** 比较两种策略：检索增强生成(RAG)和持续预训练(CPT)。针对包含视觉元素的幻灯片，还探索了多模态RAG方法，将检索内容以图像形式呈现给生成器。

**结果:** RAG比CPT更有效且高效；在多模态设置中将幻灯片作为图像整合显著优于纯文本检索的性能。

**结论:** 这些发现为开发更好支持学习和教学的AI助手提供了实用策略，并鼓励在其他教育场景中进行类似努力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Slides+to+Chatbots%3A+Enhancing+Large+Language+Models+with+University+Course+Materials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22272&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [113] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

**主要类别:** cs.CL

**AI概要:** 论文比较了三种临床命名实体识别方法：BERT编码器、GPT-4o少样本上下文学习和GPT-4o监督微调，发现监督微调效果最佳但成本高，简单提示优于复杂提示


<details>
  <summary>更多</summary>
  
**动机:** 研究临床命名实体识别任务，比较不同模型家族在CADEC语料库上的性能表现，探索大语言模型在医疗NER任务中的应用潜力

**方法:** 使用CADEC语料库评估三种方法：(i) BERT类编码器(BERT Base, BioClinicalBERT, RoBERTa-large)；(ii) GPT-4o少样本上下文学习(简单vs复杂提示)；(iii) GPT-4o监督微调；评估五个实体类型(ADR、药物、疾病、症状、发现)

**结果:** RoBERTa-large和BioClinicalBERT相比BERT Base改进有限；简单提示优于复杂提示；监督微调获得最佳性能(F1≈87.1%)但成本较高；LLM在简化任务(二分类)上准确率更高

**结论:** 监督微调是临床NER任务的最有效方法，但需权衡成本效益；大语言模型在简化任务中表现更好；提示设计对少样本学习效果有重要影响

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Fine-Tuning+or+In-Context+Learning%3F+Evaluating+LLMs+for+Clinical+NER，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22285&send_immediately=true&force_search=false)

**原文摘要:** We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [114] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch, Ainhoa Risco Patón, Teun Buijse, Peter Berck, Maarten van Gompel*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种基于内存的语言建模方法OLIFANT，作为深度神经网络语言建模的高效环保替代方案，具有对数线性可扩展的预测性能和强记忆能力。


<details>
  <summary>更多</summary>
  
**动机:** 寻求一种比深度神经网络更高效、更环保的语言建模方法，减少训练和推理过程中的生态足迹。

**方法:** 采用基于内存的语言建模方法，实现k近邻分类的快速近似，完全依赖CPU运行，实现低延迟的token处理。

**结果:** OLIFANT在下一个token预测准确率、排放估算和速度方面与GPT-2和GPT-Neo进行了比较，显示出竞争力。

**结论:** 基于内存的语言建模是一种简单透明、生态友好的高效替代方案，在保持性能的同时显著降低了环境影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-based+Language+Models%3A+An+Efficient%2C+Explainable%2C+and+Eco-friendly+Approach+to+Large+Language+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22317，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22317&send_immediately=true&force_search=false)

**原文摘要:** We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [115] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines, Bonnie Dorr*

**主要类别:** cs.CL

**AI概要:** 本文提出了首个多语言目标立场提取（TSE）基准，涵盖6种语言，展示了在无需为每种语言单独建模的情况下扩展TSE流程的可行性，但性能显著低于英语单语言设置。


<details>
  <summary>更多</summary>
  
**动机:** 社交媒体数据驱动的争议问题公众意见分析需要多语言支持，但现有TSE研究仅限于英语，缺乏多语言基准和评估标准。

**方法:** 将原始TSE流程扩展到多语言环境，涵盖加泰罗尼亚语、爱沙尼亚语、法语、意大利语、普通话和西班牙语语料库，无需为每种语言单独训练模型。

**结果:** 模型获得了12.78的F1分数，表明多语言任务相比英语单语言设置难度显著增加，目标预测是主要瓶颈。

**结论:** 这项工作为多语言TSE提供了急需的资源、算法和评估基准，并首次证明了TSE的F1分数对不同目标表述方式的敏感性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilingual+Target-Stance+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22334&send_immediately=true&force_search=false)

**原文摘要:** Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [116] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl, Majid Asgari-Bidhendi, Behrooz Minaei-Bidgoli*

**主要类别:** cs.CL

**AI概要:** FAIR-RAG是一个新型的代理框架，通过结构化证据评估和自适应查询优化机制，显著提升了复杂多跳查询中的RAG性能，在多个基准测试中实现了最先进的结果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统性识别和填补证据空白的机制，容易传播噪声或无法收集全面上下文。

**方法:** 提出FAIR-RAG框架，包含结构化证据评估(SEA)模块作为分析门控机制，将查询分解为所需发现的清单，识别已确认事实和明确信息空白，通过自适应查询优化代理生成针对性子查询来检索缺失信息，形成迭代优化循环。

**结果:** 在HotpotQA、2WikiMultiHopQA和MusiQue等挑战性多跳QA基准测试中，FAIR-RAG显著优于强基线方法，在HotpotQA上达到0.453的F1分数，比最强的迭代基线绝对提升8.3个百分点。

**结论:** 研究表明，具有明确空白分析的结构化证据驱动优化过程对于在复杂知识密集型任务中实现可靠准确的RAG推理至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAIR-RAG%3A+Faithful+Adaptive+Iterative+Refinement+for+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22344&send_immediately=true&force_search=false)

**原文摘要:** While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [117] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad, Nisar Hussain, Amna Qasim, Momina Hafeez, Muhammad Usman Grigori Sidorov, Alexander Gelbukh*

**主要类别:** cs.CL

**AI概要:** 该研究通过将英语讽刺语料库翻译成乌尔都语，评估了多种机器学习算法和Transformer模型在乌尔都语讽刺识别任务上的表现，发现LLaMA 3 (8B)模型取得了最佳性能，F1分数达到94.61%。


<details>
  <summary>更多</summary>
  
**动机:** 乌尔都语作为一种语法和文化背景与英语不同的低资源语言，其讽刺识别在自然语言处理中具有挑战性，研究旨在探索有效的讽刺检测方法。

**方法:** 将英语讽刺语料库翻译成乌尔都语，使用GloVe和Word2Vec词嵌入评估10种最先进的机器学习算法，并微调BERT、RoBERTa、LLaMA 2 (7B)、LLaMA 3 (8B)和Mistral等Transformer模型。

**结果:** 机器学习模型中梯度提升算法表现最佳（F1分数89.18%），Transformer模型中LLaMA 3 (8B)表现最优（F1分数94.61%）。

**结论:** 结合音译技术和现代NLP模型能够在乌尔都语这种历史低资源语言中实现鲁棒的讽刺检测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Irony+Detection+in+Urdu+Text%3A+A+Comparative+Study+Using+Machine+Learning+Models+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22356&send_immediately=true&force_search=false)

**原文摘要:** Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [118] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin, Daria Khomich, Nikita Savushkin, Anastasia Ianina, Fyodor Minkin*

**主要类别:** cs.CL

**AI概要:** GigaEmbeddings是一个基于GigaChat-3B模型的三阶段训练框架，通过分层指令调优为俄语文本生成高性能嵌入，在ruMTEB基准测试中达到69.1分的最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有方法在俄语文本嵌入方面的局限性，通过统一多样化目标和利用合成数据生成来提升性能。

**方法:** 采用三阶段流水线：大规模对比预训练、硬负样本微调、多任务泛化（检索、分类、聚类）；架构创新包括双向注意力、潜在注意力池化、25% Transformer层剪枝。

**结果:** 在包含23个多语言任务的ruMTEB基准测试中获得69.1的平均分数，超越了参数更多的强基线模型，达到最先进性能。

**结论:** GigaEmbeddings框架通过创新的训练方法和架构优化，成功实现了俄语文本嵌入的高性能和高效率，为俄语NLP任务提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GigaEmbeddings%3A+Efficient+Russian+Language+Embedding+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22369&send_immediately=true&force_search=false)

**原文摘要:** We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [119] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie, Zhiyang Zhang, Yifan Wu, Sirong Lu, Jiayi Zhang, Zhaoyang Yu, Jinlin Wang, Sirui Hong, Bang Liu, Chenglin Wu, Yuyu Luo*

**主要类别:** cs.CL

**AI概要:** 该论文提出了VisJudge-Bench，首个用于评估多模态大语言模型在可视化质量评估方面能力的综合基准，并开发了专门的可视化美学质量评估模型VisJudge，显著缩小了与人类专家判断的差距。


<details>
  <summary>更多</summary>
  
**动机:** 可视化质量评估具有挑战性，需要同时判断数据编码准确性、信息表达性和视觉美学。虽然多模态大语言模型在自然图像美学评估中表现良好，但缺乏系统性的基准来评估其在可视化领域的表现。

**方法:** 创建包含3,090个专家标注样本的VisJudge-Bench基准，涵盖32种图表类型的单图、多图和仪表盘。基于此基准测试先进MLLMs性能，并开发专门的VisJudge模型。

**结果:** 最先进的MLLMs（如GPT-5）与人类专家存在显著差距（MAE=0.551，相关性0.429）。VisJudge将MAE降低至0.442（减少19.8%），与人类专家一致性提升至0.681（提高58.7%）。

**结论:** VisJudge-Bench填补了可视化质量评估基准的空白，VisJudge模型在可视化美学评估方面显著优于通用MLLMs，为可视化质量自动评估提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VisJudge-Bench%3A+Aesthetics+and+Quality+Assessment+of+Visualizations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22373&send_immediately=true&force_search=false)

**原文摘要:** Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [120] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba, Aman Sinha, Timothee Mickus, Raul Vazquez, Patanjali Bhamidipati, Claudio Savelli, Ahana Chattopadhyay, Laura A. Zanella, Yash Kankanampati, Binesh Arakkal Remesh, Aryan Ashok Chandramania, Rohit Agarwal, Chuyuan Li, Ioana Buhnila, Radhika Mamidi*

**主要类别:** cs.CL

**AI概要:** CAP数据集是一个多语言资源，用于研究大语言模型在科学文本生成中的幻觉现象，包含900个科学问题和7000多个模型生成的答案，覆盖9种语言，标注了事实性错误和流畅性问题。


<details>
  <summary>更多</summary>
  
**动机:** 科学领域中的幻觉会扭曲事实知识，而专业术语、统计推理和上下文依赖解释加剧了这种扭曲，特别是考虑到LLMs缺乏真正理解、上下文理解有限和偏向表面泛化的问题。

**方法:** 创建跨语言数据集，涵盖5种高资源语言和4种低资源语言，包含900个精选科学问题和16个公开可用模型生成的7000多个答案，提供问题-答案对、标记序列和对应logits，每个实例都标注了科学幻觉的二元标签和流畅性标签。

**结果:** CAP数据集公开发布，包含丰富的多语言科学文本生成数据，标注了事实性错误和流畅性问题。

**结论:** CAP数据集有助于促进幻觉检测、LLMs的多语言评估以及更可靠的科学NLP系统的开发方面的先进研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Confabulations+from+ACL+Publications+%28CAP%29%3A+A+Dataset+for+Scientific+Hallucination+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22395，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22395&send_immediately=true&force_search=false)

**原文摘要:** We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [121] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong, Cong Wang, Maria Teleki, Millennium Bismay, James Caverlee*

**主要类别:** cs.CL

**AI概要:** CHOIR是一个测试时框架，通过协调多个角色条件下的推理信号来提升LLM推理的鲁棒性，无需额外训练即可显著提高不同人口统计群体的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 角色分配的LLMs中，即使是简单的人称代词变化等人口统计扰动也会导致推理轨迹变化和不同的正确答案，作者希望将这些变化作为建设性资源而非需要消除的偏见来提升推理鲁棒性。

**方法:** 提出CHOIR框架，在反事实角色之间进行协作解码，动态平衡其推理路径中的一致性和分歧，将多个角色条件下的推理信号协调为统一的预测。

**结果:** 在各种推理基准测试中，CHOIR持续提升了不同人口统计群体、模型架构、规模和任务的性能，个体人口统计群体改进最高达26.4%，五个群体平均改进19.2%，即使在基础角色非最优时也有效。

**结论:** 通过将角色变化重新定义为建设性信号，CHOIR为更可靠的LLM推理提供了一种可扩展和可泛化的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CHOIR%3A+Collaborative+Harmonization+fOr+Inference+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22475&send_immediately=true&force_search=false)

**原文摘要:** Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [122] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang, Zhaxi Zerong*

**主要类别:** cs.CL

**AI概要:** 该研究引入了一种计算方法来量化声调发生过程中音高的功能作用，通过分析藏语方言对音高平坦化的敏感性，揭示了声调发生的连续统现象。


<details>
  <summary>更多</summary>
  
**动机:** 传统声调发生研究主要依赖比较重建和声学语音学方法，需要更精细的计算方法来量化音高在不同声调发展阶段的功能性作用。

**方法:** 通过测量音高操作对自动语音识别（ASR）性能的影响，分析一组密切相关的藏语方言对音高平坦化的敏感性。

**结果:** 发现了声调发生的连续统：无声调的安多方言对音高移除最耐受，完全有声调的卫藏方言显示严重性能下降，而中间的康方言处于两者之间。

**结论:** 计算方法能够捕捉语音变化的精细阶段，传统基于最小对立对的功能负荷指标可能在过渡性系统中高估音高依赖性，因为音段和超音段线索在语音上仍然交织。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Tonogenesis+Continuum+in+Tibetan%3A+A+Computational+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22485&send_immediately=true&force_search=false)

**原文摘要:** Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [123] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian, Junjie Liu, Xican Yang, Haishan Ye, Yan Song*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种针对大语言模型的任务特定剪枝方法，通过结合通用和任务特定的特征分布来计算参数重要性，在压缩模型的同时保持特定任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统剪枝方法主要关注保持模型生成流畅句子的能力，但忽视了在特定领域和任务上的性能表现。

**方法:** 分析传统剪枝的损失扰动最小化原理，将任务特定特征分布纳入重要性计算，分别使用通用和任务特定校准数据计算重要性分数，基于激活范数差异将参数分为共享组和专属组，融合分数指导剪枝过程。

**结果:** 在广泛使用的基准测试中，该方法在相同剪枝比例和不同设置下始终优于基线方法。

**结论:** 该方法能有效保持大语言模型在压缩后的专门能力，并能与各种基础剪枝技术无缝集成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Frustratingly+Easy+Task-aware+Pruning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22489&send_immediately=true&force_search=false)

**原文摘要:** Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [124] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang, Nicolas Ballier, Gina-Anne Levow, Richard Wright*

**主要类别:** cs.CL

**AI概要:** 研究分析Whisper多语言ASR模型在49种语言上的解码行为，发现子词发现数量与预训练数据量无关，子词发现遵循指数饱和模式，提出了声学饱和时间(AST)概念，揭示了子词使用更多受语言统计、类型和正字法结构影响而非训练数据规模。


<details>
  <summary>更多</summary>
  
**动机:** 探究多语言ASR模型需要多少音频才能充分观察其学习的子词库，以及多语言预训练中的数据差异是否影响推理过程中这些子词的使用方式。

**方法:** 通过记录Whisper模型在49种语言推理过程中的解码候选子词，并追踪它们随时间的累积发现情况，分析模型子词空间的使用模式。

**结果:** 发现子词发现总数与语言预训练时长无关；子词发现率遵循一致的指数饱和模式；揭示了类似Zipf-Mandelbrot的分布规律；拉丁文字语言表现优于西里尔、中日韩和闪族文字语言。

**结论:** 多语言ASR推理中的子词使用更多受语音的统计、类型和正字法结构约束，而非训练数据规模，这为更公平的语料构建和跨语言评估提供了实证基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Limits+of+Data+Scaling%3A+Sub-token+Utilization+and+Acoustic+Saturation+in+Multilingual+ASR，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22492&send_immediately=true&force_search=false)

**原文摘要:** How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [125] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott, Siyu Liang, Alicia Wassink, Gina-Anne Levow*

**主要类别:** cs.CL

**AI概要:** 该研究系统评估了四种主流商业语音识别系统在太平洋西北英语语料库中的种族偏见，发现语音变异是导致不同种族群体识别准确率差异的主要原因，特别是非裔美国人受影响最大。


<details>
  <summary>更多</summary>
  
**动机:** 评估商业ASR系统中存在的种族偏见，分析社会语音变异如何导致不同种族群体在语音识别性能上的差异。

**方法:** 使用太平洋西北英语语料库，分析四个种族群体（非裔美国人、白人美国人、奇卡诺人和雅卡马人）的转录准确性，引入启发式确定的语音错误率指标，分析11种社会语音特征。

**结果:** 发现元音质量变异，特别是对低后元音合并和前鼻音合并模式的抵抗，与不同种族群体的错误率差异系统相关，非裔美国人在所有评估系统中受影响最显著。

**结论:** 方言语音变异的声学建模是商业ASR系统偏见的主要来源，研究为通过训练数据中有针对性地表示社会语音多样性来改进ASR性能提供了可行指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Sociophonetic+Analysis+of+Racial+Bias+in+Commercial+ASR+Systems+Using+the+Pacific+Northwest+English+Corpus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22495&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [126] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu, Sahithi Singireddy, Sravani Gona, Sujal Timilsina*

**主要类别:** cs.CL

**AI概要:** 本研究系统评估了在服务条款文档中进行不公平条款检测的不同方法，包括全微调、参数高效适配（LoRA、QLoRA）和零样本提示策略，发现全微调效果最佳但成本高，LoRA方法在内存效率上更具优势


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在文本理解方面表现出色，但在专业法律领域的应用受到全微调成本的限制，需要探索更高效的适配方法

**方法:** 使用BERT和DistilBERT进行全微调，对TinyLlama、LLaMA 3B/7B和SaulLM应用4位低秩适配（LoRA），并在零样本设置下评估GPT-4o及其变体

**结果:** 全微调获得最佳精确率-召回率平衡，基于LoRA的模型提供竞争性的召回率且内存成本降低最多3倍

**结论:** 研究揭示了高效领域适配LLM的实际设计权衡，为法律文本处理中的微调研究提供了开放基准

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text+to+Trust%3A+Evaluating+Fine-Tuning+and+LoRA+Trade-offs+in+Language+Models+for+Unfair+Terms+of+Service+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22531&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [127] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He, Yuxuan Wang, Jiaqi Li, Kexin Liang, Muhan Zhang*

**主要类别:** cs.CL

**AI概要:** LooGLE v2是一个评估大语言模型在真实世界长文本场景下理解能力的新基准，测试显示即使最佳模型也只能达到59.2%的分数，表明当前模型的长上下文处理能力存在显著局限


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型虽然具备更长的上下文窗口，但在长依赖任务中的实际理解能力仍然有限且缺乏充分研究，特别是在真实世界长文本应用中缺乏基准测试

**方法:** 构建包含法律、金融、游戏和代码领域的自动收集的真实长文本数据集（16k到2M token），设计10种领域特定的长依赖任务类型，生成1,934个多样化和复杂性的QA实例，评估6个本地部署和4个API基础的LLM

**结果:** 评估结果显示，即使表现最佳的模型在基准测试中仅获得59.2%的总体分数，流行LLM实际能理解的上文长度远低于其宣称的能力，在处理长依赖任务时存在显著限制

**结论:** 当前大语言模型在真实世界长上下文理解方面存在重大局限性，揭示了模型在实际长依赖任务处理能力上的不足，表明在实用长上下文理解方面还有很大的改进空间

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LooGLE+v2%3A+Are+LLMs+Ready+for+Real+World+Long+Dependency+Challenges%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22548，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22548&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [128] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang*

**主要类别:** cs.CL

**AI概要:** SABlock是一种语义感知的KV缓存淘汰框架，通过自适应块大小来平衡语义连贯性和内存效率，在长上下文LLM推理中显著减少内存占用并提升性能


<details>
  <summary>更多</summary>
  
**动机:** KV缓存不断增长的内存占用成为长上下文LLM推理的可扩展性瓶颈，现有压缩方法难以平衡语义连贯性和内存效率

**方法:** SABlock首先进行语义分割以对齐压缩边界和语言结构，然后应用分段引导的token评分来优化重要性评估，最后通过预算驱动的搜索策略自适应确定最佳块大小

**结果:** 在相同内存预算下，SABlock持续优于最先进基线方法。在NIAH测试中仅用96个KV条目就达到99.9%检索准确率，在1024固定缓存预算下减少46.28%峰值内存使用，在128K上下文长度下解码速度提升9.5倍

**结论:** SABlock通过语义感知的自适应块大小策略有效解决了KV缓存内存瓶颈问题，在保持语义完整性的同时显著提升了压缩效率和推理性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SABlock%3A+Semantic-Aware+KV+Cache+Eviction+with+Adaptive+Compression+Block+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22556&send_immediately=true&force_search=false)

**原文摘要:** The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [129] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang, Xinyue Zheng, Chunyan Zeng*

**主要类别:** cs.CL

**AI概要:** EduLoop-Agent是一个端到端的个性化学习代理，通过神经认知诊断模型、自适应测试策略和大型语言模型形成诊断-推荐-反馈的闭环系统，在ASSISTments数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前个性化学习方法存在模型粗糙、适应性有限和反馈不具体的问题，各组件孤立运作而非形成闭环，需要整合建模、项目选择和反馈的端到端解决方案。

**方法:** 提出EduLoop-Agent系统，包含三个核心组件：神经认知诊断模型(NCD)提供细粒度知识掌握度评估，有界能力估计计算机自适应测试策略(BECAT)动态选择项目，大型语言模型(LLMs)生成结构化可操作反馈。

**结果:** 在ASSISTments数据集上，NCD模块在响应预测方面表现优异且提供可解释的掌握度评估，自适应推荐策略提高了项目相关性和个性化程度，LLM反馈能针对已识别的弱点提供针对性学习指导。

**结论:** 该设计方案有效且可实际部署，为智能教育中生成个性化学习轨迹提供了可行路径，实现了诊断-推荐-反馈的闭环个性化学习框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Closed-Loop+Personalized+Learning+Agent+Integrating+Neural+Cognitive+Diagnosis%2C+Bounded-Ability+Adaptive+Testing%2C+and+LLM-Driven+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22559&send_immediately=true&force_search=false)

**原文摘要:** As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [130] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya, Ekaterina Kochmar*

**主要类别:** cs.CL

**AI概要:** 本文回顾了AI教育领域智能辅导系统的评估现状，指出当前缺乏标准化评估框架的问题，并提出了基于学习科学的三项研究方向来建立公平统一的评估方法。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI和大型语言模型的成功加速了智能辅导系统的发展，但这些系统的进展和影响难以追踪，因为缺乏可靠、普遍接受且以教学为导向的评估框架和基准。

**方法:** 通过回顾最先进的评估实践，分析相关挑战，并基于真实案例研究，结合跨学科AI教育研究的见解。

**结果:** 识别了当前评估方法的主观性和非标准化问题，导致不一致性和有限的泛化能力。

**结论:** 提出了三项实践可行、理论扎实的研究方向，这些方向基于学习科学原则，旨在为智能辅导系统建立公平、统一和可扩展的评估方法论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pedagogy-driven+Evaluation+of+Generative+AI-powered+Intelligent+Tutoring+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22581，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22581&send_immediately=true&force_search=false)

**原文摘要:** The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [131] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi, Elena Maria Muià, Federico Siciliano, Giovanni Trappolini, Vincenzo Crisà, Peter Kruger, Fabrizio Silvestri*

**主要类别:** cs.CL

**AI概要:** AutoBench是一个全自动的LLM评估框架，通过模型间的互评机制动态生成评估任务，避免了传统静态基准的测试集污染问题，并能产生更鲁棒的人类一致性评估结果。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统静态基准测试存在的测试集污染和有限适应性问题，需要一种能够持续评估不断演进的语言模型的动态评估方法。

**方法:** 采用互评机制，让模型交替充当问题生成器、参赛者和评委角色，通过迭代加权机制放大可靠评估者的影响力，将同行判断聚合成基于共识的排名。

**结果:** 实验显示与MMLU-Pro和GPQA基准有强相关性（分别为78%和63%），多评委设计显著优于单评委基线，证明分布式评估产生更鲁棒和人类一致的评估。

**结论:** AutoBench提供了一个可扩展、抗污染的替代方案，适用于对不断演进的语言模型进行持续评估，验证了同行驱动评估范式的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoBench%3A+Automating+LLM+Evaluation+through+Reciprocal+Peer+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22593&send_immediately=true&force_search=false)

**原文摘要:** We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [132] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian, Ramesh Jain*

**主要类别:** cs.CL

**AI概要:** 提出了一个名为个人护理公用事业（PCU）的全球性AI驱动系统，通过多模态数据和实时分析提供个性化健康指导和持续健康管理。


<details>
  <summary>更多</summary>
  
**动机:** 基于数字基础设施和生物医学创新的成功，旨在解决传统间歇性医疗护理的局限性，提供持续、实时的健康指导。

**方法:** 采用多模态智能体、事件中心建模和情境推理技术，整合个人传感、体验计算和群体层面分析。

**结果:** PCU系统具备三个核心能力：个性化可信健康信息、主动健康导航和行为指导、医疗事件后的持续恢复解释。

**结论:** PCU不仅能够改善个人健康结果，还为公共卫生和科学发现提供了新的基础，代表了医疗保健范式的转变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personal+Care+Utility+%28PCU%29%3A+Building+the+Health+Infrastructure+for+Everyday+Insight+and+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22602&send_immediately=true&force_search=false)

**原文摘要:** Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [133] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani, Mohammadtaha Bagherifard, Erfan Zinvandi, Mehran Sarmadi*

**主要类别:** cs.CL

**AI概要:** PerCoR是首个大规模波斯语常识推理基准数据集，包含10.6万个选择题，采用新颖的连词分割策略生成句子补全对，并使用DRESS-AF方法创建具有挑战性的干扰项。


<details>
  <summary>更多</summary>
  
**动机:** 解决波斯语缺乏大规模常识推理基准的问题，推动波斯语自然语言处理的发展。

**方法:** 1) 从40多个新闻和文化网站收集数据；2) 使用连词分割策略生成句子补全对；3) 开发DRESS-AF方法（基于嵌入相似性评分和对抗过滤）选择干扰项；4) 在HellaSwag英文基准上验证方法有效性。

**结果:** 人类标注者准确率89%，OpenAI-o3模型表现最佳（92.18%），最强开源模型DeepSeek-R1达到82.51%，显示数据集具有挑战性且存在性能差距。DRESS-AF方法成功提升英文HellaSwag基准的难度而不影响人类可解性。

**结论:** PerCoR填补了波斯语常识推理基准的空白，提出的DRESS-AF方法有效提升了数据集质量，为波斯语NLP研究提供了重要资源，同时该方法在跨语言场景中具有通用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PerCoR%3A+Evaluating+Commonsense+Reasoning+in+Persian+via+Multiple-Choice+Sentence+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22616&send_immediately=true&force_search=false)

**原文摘要:** We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [134] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha, Sajal Saha, Debanjan Ballav, Soumi Mitra, Hritwick Chakraborty*

**主要类别:** cs.CL

**AI概要:** 该论文开发了一个托托语-孟加拉语-英语三语学习应用，通过AI技术和语言文档化来保护和振兴印度濒危的托托语。


<details>
  <summary>更多</summary>
  
**动机:** 保护语言多样性，通过数字化方式保存濒危的托托语，确保其可访问性和可用性。

**方法:** 通过田野调查收集语言数据，创建词素标注的三语语料库，训练小型语言模型和Transformer翻译引擎，分析屈折形态和派生策略，开发文字标准化和数字素养工具。

**结果:** 成功开发了三语学习应用，建立了结构化的语言语料库，训练了AI模型，为濒危语言保护提供了可持续的数字化解决方案。

**结论:** 研究展示了将传统语言学方法与AI技术相结合的有效性，为基于社区的语言振兴提供了跨学科合作模式，为其他濒危语言的保护提供了可持续模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Linguistics+and+AI%3A+Morphological+Analysis+and+Corpus+development+of+Endangered+Toto+Language+of+West+Bengal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22629&send_immediately=true&force_search=false)

**原文摘要:** Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [135] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis, Lisa Alazraki*

**主要类别:** cs.CL

**AI概要:** FormaMentis是一个基于意大利语言和文化的物理常识推理基准数据集，为MRL 2025多语言物理推理共享任务而创建


<details>
  <summary>更多</summary>
  
**动机:** 为英语以外的语言创建手动标注的物理常识推理评估数据，特别是针对意大利语言和文化背景

**方法:** 由熟悉当地习俗的意大利母语专家创建数据样本，并保留意大利文化元素的同时翻译成英文

**结果:** 开发了FormaMentis基准数据集，包含意大利语言文化背景下的物理常识推理样本

**结论:** 该工作为多语言物理推理研究提供了意大利语文化背景下的高质量评估资源

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Culturally+Grounded+Physical+Commonsense+Reasoning+in+Italian+and+English%3A+A+Submission+to+the+MRL+2025+Shared+Task，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22631&send_immediately=true&force_search=false)

**原文摘要:** This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [136] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang, Qingtian Zeng, Hua Duan, Cheng Cheng, Minghao Zou, Ziyang Wang*

**主要类别:** cs.CL

**AI概要:** 提出了一种新的少样本知识图谱补全框架CR-FKGC，通过共轭关系建模解决复杂关系模式和数据稀疏性问题，在三个基准测试中取得了优于现有方法的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的少样本知识图谱补全方法难以捕捉复杂的关系模式并缓解数据稀疏性问题，需要新的解决方案。

**方法:** 使用邻域聚合编码器整合高阶邻居信息，结合隐式条件扩散关系模块和稳定关系模块的共轭关系学习器来捕捉稳定语义和不确定性偏移，以及流形共轭解码器在流形空间中进行高效评估和推理。

**结果:** 在三个基准测试中，该方法相比最先进的方法取得了更优越的性能表现。

**结论:** CR-FKGC框架通过有效的共轭关系建模，成功解决了少样本知识图谱补全中的复杂关系模式捕捉和数据稀疏性挑战，展现出优异的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conjugate+Relation+Modeling+for+Few-Shot+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22656，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22656&send_immediately=true&force_search=false)

**原文摘要:** Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [137] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth, Parke Godfrey, Lukasz Golab, Divesh Srivastava, Jarek Szlichta*

**主要类别:** cs.CL

**AI概要:** 提出了首个针对检索增强生成（RAG）大语言模型的规则解释方法，通过分析检索源的存在与否来生成if-then规则解释模型输出来源，并设计了优化算法加速规则生成。


<details>
  <summary>更多</summary>
  
**动机:** 现有的if-then规则解释方法主要针对传统机器学习模型，而新兴的检索增强生成（RAG）大语言模型需要新的解释方法来理解其基于检索信息的输出来源。

**方法:** 提出基于Apriori剪枝思想的优化算法，通过探测LLM在不同检索源组合下的输出，生成连接检索源存在与否与输出结果的if-then规则，避免暴力枚举所有组合。

**结果:** 开发了高效的规则生成方法，能够解释RAG-LLM系统的输出来源，如"如果检索到泰晤士高等教育排名文章，则LLM将牛津大学排名第一"。

**结论:** 通过定性和定量实验验证了所提解决方案的价值和效率，为RAG大语言模型提供了有效的可解释性工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rule-Based+Explanations+for+Retrieval-Augmented+LLM+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22689&send_immediately=true&force_search=false)

**原文摘要:** If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [138] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky, Shai Nahum-Gefen, Elad Ben Zaken*

**主要类别:** cs.CL

**AI概要:** SALSA是一种针对指令调优大语言模型的文本分类优化方法，通过结构化提示、类别到标记映射和参数高效微调，实现单次前向传播的高效准确分类。


<details>
  <summary>更多</summary>
  
**动机:** 尽管指令调优大语言模型具有强大的泛化能力，但在文本分类基准测试中表现不佳，需要改进其分类性能。

**方法:** 将每个类别标签映射到不同的输出标记，构建提示以引发单标记响应，在推理时仅将模型输出投影到相关类别标记的logits上。

**结果:** SALSA在多个基准测试中取得了最先进的结果，证明了其在基于LLM的分类应用中的鲁棒性和可扩展性。

**结论:** SALSA通过避免冷启动训练，提供了一种高效准确的LLM文本分类解决方案，具有实际应用的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SALSA%3A+Single-pass+Autoregressive+LLM+Structured+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22691，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22691&send_immediately=true&force_search=false)

**原文摘要:** Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [139] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao*

**主要类别:** cs.CL

**AI概要:** E²Rank是一个统一的检索和重排序框架，通过列表排序目标继续训练单个文本嵌入模型，实现高效且高质量的检索和重排名功能


<details>
  <summary>更多</summary>
  
**动机:** 传统文本嵌入模型在检索效率上表现优异，但在排序保真度上不如专门的LLM列表重排序器，后者能捕捉细粒度的查询-文档和文档-文档交互

**方法:** 使用查询和文档嵌入的余弦相似度作为统一排序函数，构建包含原始查询和候选文档的列表排序提示，类似于传统检索中的伪相关反馈机制

**结果:** 在BEIR重排序基准上达到最先进结果，在BRIGHT推理密集型基准上表现优异，重排序延迟极低，同时在MTEB基准上提升了嵌入性能

**结论:** 单个嵌入模型可以有效地统一检索和重排序功能，在保持计算效率的同时提供有竞争力的排序准确性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%5Ctext%7BE%7D%5E2%5Ctext%7BRank%7D%24%3A+Your+Text+Embedding+can+Also+be+an+Effective+and+Efficient+Listwise+Reranker，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22733&send_immediately=true&force_search=false)

**原文摘要:** Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [140] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim*

**主要类别:** cs.CL

**AI概要:** 本研究使用持续预训练和参数高效微调方法，以极少的参数量（<1%）和数据量成功将大语言模型适配到魁北克法语方言，在方言基准上获得提升的同时保持标准法语性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型主要局限于高资源语言的问题，探索在有限数据和计算预算下将LLM适配到低资源方言的方法。

**方法:** 采用低秩适应（LoRA）和计算高效的持续预训练方法，使用小数据集对三个LLM进行魁北克法语适配，并在COLE基准套件上进行评估。

**结果:** 实验显示在少数民族方言基准上获得改进，同时标准语言基准仅有最小程度的性能回归，仅更新了不到1%的模型参数。

**结论:** CPT结合PEFT能以成本效益高的方式缩小方言差距，为少数民族语言社区提供高质量LLM访问，研究结果高度依赖语料库组成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Low-Resource+Dialect+Adaptation+of+Large+Language+Models%3A+A+French+Dialect+Case-Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22747，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22747&send_immediately=true&force_search=false)

**原文摘要:** Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [141] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj, Deven Mahesh Mistry, Sahaj Singh Maini, Yash Aggarwal, Zoran Tiganj*

**主要类别:** cs.CL

**AI概要:** 论文研究了大型语言模型在上下文学习中的时间偏差问题，通过实验发现LLMs在检索信息时存在显著的首位和近因效应，中间位置的信息检索可靠性较低，这一现象在transformer和state-space模型中表现相似。


<details>
  <summary>更多</summary>
  
**动机:** 受人类情景记忆的启发，研究想要探究预训练语言模型是否能够像人类一样区分和检索时间上分离的事件，理解LLMs在上下文学习中如何处理时间关系。

**方法:** 使用包含相同token多次出现的序列提示模型，固定重复token位置并置换其他token来消除语义干扰，隔离时间因素对下一个token预测的影响，进行了消融实验分析transformer中的归纳头机制。

**结果:** 模型始终对重复token后的token赋予最高概率，但存在对序列开头和结尾位置的明显偏好；中间位置的记忆检索可靠性较低；transformer和state-space模型表现出相似的时间偏差模式。

**结论:** 研究揭示了LLMs在上下文学习中存在系统性的时间偏差，这种偏差有助于实现时间分离和情景检索，增进了对模型内部记忆机制的理解，为改进模型设计提供了 insights。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Semantics%3A+How+Temporal+Biases+Shape+Retrieval+in+Transformer+and+State-Space+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22752&send_immediately=true&force_search=false)

**原文摘要:** In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [142] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou, Lutong Yu, You Lyu, Yihang Lin, Zefeng Zhao, Junyi Ao, Yuhao Zhang, Benyou Wang, Haizhou Li*

**主要类别:** cs.CL

**AI概要:** EchoMind是一个新颖的多层级基准测试，专门评估语音语言模型在共情对话中整合语言内容、声学线索和上下文推理的能力，发现当前最先进模型在处理高表达性声音线索方面仍有显著局限。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试通常孤立评估语言、声学、推理或对话能力，而忽视了这些技能在实现类人化、情感智能对话中的整合需求，特别是模型是否能同时感知非词汇声音线索并做出符合情感和上下文因素的共情回应。

**方法:** 提出EchoMind基准，通过顺序的、上下文关联的任务模拟共情对话的认知过程：口语内容理解、声音线索感知、整合推理和回应生成。使用语义中性的相同脚本，控制声音风格变化来独立测试表达效果，基于包含3个粗粒度和12个细粒度维度、39个声音属性的共情导向框架进行评估。

**结果:** 测试12个先进SLM显示，即使最先进模型也难以处理高表达性声音线索，限制了共情回应质量。在指令遵循、自然语音变化的韧性以及有效利用声音线索实现共情方面存在持续弱点。

**结论:** 研究结果强调了需要开发能够整合语言内容与多样化声音线索的SLM，以实现真正的共情对话能力，当前模型在这方面仍有显著改进空间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EchoMind%3A+An+Interrelated+Multi-level+Benchmark+for+Evaluating+Empathetic+Speech+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22758，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22758&send_immediately=true&force_search=false)

**原文摘要:** Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [143] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem, Muhammad Hazim Al Farouq, John D. Kelleher*

**主要类别:** cs.CL

**AI概要:** 本文提出基于层重要性分析的迭代层剪枝方法，在保持翻译质量的同时显著减小模型大小和推理时间


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在机器翻译等NLP任务中表现出色，但其巨大的计算需求使得高效部署面临挑战

**方法:** 采用迭代层剪枝策略，通过层重要性分析指导剪枝过程，使用Aya-Expanse-8B模型在捷克语-德语和英语-埃及阿拉伯语翻译任务上进行评估

**结果:** 方法实现了模型大小和推理时间的显著减少，同时保持了基准模型的翻译质量

**结论:** 迭代层剪枝是一种有效的模型压缩方法，能够在保持性能的同时解决LLMs部署中的计算效率问题

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Iterative+Layer+Pruning+for+Efficient+Translation+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22763&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [144] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu, Yilun Zhou, Pranav Narayanan Venkit, Kung-Hsiang Huang, Jiaxin Zhang, Nanyun Peng, Chien-Sheng Wu*

**主要类别:** cs.CL

**AI概要:** MMPersuade是一个研究大型视觉语言模型在多媒体内容中如何被说服的框架，包含多模态数据集和评估方法，发现多模态输入比纯文本更具说服力，特别是在错误信息场景中。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型视觉语言模型在购物、健康、新闻等领域的部署增加，它们面临大量说服性内容。需要了解模型如何被多模态说服内容影响，因为过度易被说服的模型可能采纳误导性信念、覆盖用户偏好或生成不道德输出。

**方法:** 提出MMPersuade框架，包含：(1)综合多模态数据集，将图像和视频与商业、主观行为及对抗场景中的说服原则配对；(2)评估框架，通过第三方协议评分和对话历史的自我估计标记概率来量化说服效果和模型易感性。

**结果:** 对六个领先LVLM的研究发现：(1)多模态输入相比纯文本显著提高说服效果和模型易感性，特别是在错误信息场景；(2)声明的先前偏好降低易感性，但多模态信息仍保持说服优势；(3)不同策略在不同场景效果不同，互惠在商业和主观场景最有效，可信度和逻辑在对抗场景占主导。

**结论:** MMPersuade通过联合分析说服效果和易感性，为开发在面对说服性多模态内容时具有鲁棒性、偏好一致性和伦理对齐的模型提供了原则性基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMPersuade%3A+A+Dataset+and+Evaluation+Framework+for+Multimodal+Persuasion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22768&send_immediately=true&force_search=false)

**原文摘要:** As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [145] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu, Boyin Tan, Xiaoyuan Liu, Chao Peng, Pengfei Gao, Pinjia He*

**主要类别:** cs.CL

**AI概要:** R4P是一个基于推理的补丁验证模型，为软件工程代理提供可扩展的奖励机制，解决了传统测试监督方法在可扩展性和稳定性方面的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于测试的监督方法存在两个问题：(1)构建和运行测试沙箱成本高且脆弱；(2)高覆盖率测试数据稀缺且容易受到边缘案例测试攻击的威胁，限制了数据扩展的潜力。

**方法:** 提出R4P补丁验证模型，将其视为推理任务，模拟人类代码审查者不编写新测试就能审查补丁的方式。采用组间目标进行强化学习训练，通过相互比较多个补丁修改来获得密集奖励。

**结果:** R4P在SWE-bench-verified上达到72.2%的补丁验证准确率，超越OpenAI o3。基于R4P训练的Mini-SE模型在SWE-bench-verified上达到26.2%的Pass@1，比原Qwen3-32B提升10.0%。R4P验证补丁仅需1秒，比平均测试速度快50倍。

**结论:** R4P通过推理为基础的补丁验证提供了可扩展、高效的奖励机制，实现了稳定的扩展曲线和高效率，展示了其在软件工程代理训练中的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Supervising+Software+Agents+with+Patch+Reasoner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22775&send_immediately=true&force_search=false)

**原文摘要:** While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [146] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim*

**主要类别:** cs.CL

**AI概要:** VEHME是一种用于评估手写数学表达式的视觉语言模型，通过两阶段训练流程和表达式感知视觉提示模块，在开放形式的手写数学答案评估中实现了高精度和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 手写数学解决方案的自动评估在教育技术中很重要但具有挑战性，因为学生作业格式多样、布局非结构化且符号复杂。

**方法:** 采用两阶段训练流程：(i)使用结构化推理数据进行监督微调，(ii)通过强化学习将模型输出与多维评分目标对齐。提出表达式感知视觉提示模块增强空间理解。

**结果:** 在AIHub和FERMAT数据集上评估，VEHME在开源模型中达到最先进性能，接近专有系统的准确性。

**结论:** VEHME展示了作为可扩展和易获取的自动化数学评估工具的潜力，训练和实验代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VEHME%3A+A+Vision-Language+Model+For+Evaluating+Handwritten+Mathematics+Expressions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22798&send_immediately=true&force_search=false)

**原文摘要:** Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [147] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova, Amrit Adhikari, Matthew Pearson, Vamsi Krishna Sadu, Mark V. Albert*

**主要类别:** cs.CL

**AI概要:** 本文首次系统比较了商业API和开源大语言模型在7种语言上的人权侵犯检测性能，发现模型对齐而非规模决定了跨语言稳定性，为资源有限的人道组织提供了成本-可靠性权衡的实用指南。


<details>
  <summary>更多</summary>
  
**动机:** 人道组织面临选择：投资昂贵的商业API还是依赖免费开源模型进行多语言人权监控。商业系统可靠但昂贵，开源模型缺乏实证验证，特别是在冲突地区常见的低资源语言上。

**方法:** 在78,000次多语言推理中评估6个模型（4个对齐模型和2个开源模型），使用标准分类指标和新的跨语言可靠性指标：校准偏差、决策偏差、语言鲁棒性评分和语言稳定性评分。

**结果:** 对齐模型在类型学差异大和低资源语言上保持近乎不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。对齐而非规模决定了稳定性。

**结论:** 多语言对齐实现了语言无关的推理，为人道组织在多语言部署中平衡预算约束和可靠性提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Lingual+Stability+and+Bias+in+Instruction-Tuned+Language+Models+for+Humanitarian+NLP，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22823&send_immediately=true&force_search=false)

**原文摘要:** Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [148] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua, Hong Jiao, Xinyi Wang*

**主要类别:** cs.CL

**AI概要:** 本研究探索使用生成式语言模型通过摘要和提示技术对长文进行自动评分，相比BERT等编码器模型在长文评分上的局限性取得了显著改进


<details>
  <summary>更多</summary>
  
**动机:** BERT等编码器模型受限于512个token的长度限制，在长文自动评分方面存在不足，需要探索更有效的解决方案

**方法:** 采用生成式语言模型，通过摘要技术和提示工程方法对长文进行自动评分

**结果:** 评分准确性显著提升，在Learning Agency Lab Automated Essay Scoring 2.0数据集上，QWK分数从0.822提高到0.8878

**结论:** 生成式语言模型结合摘要和提示技术能有效解决长文自动评分问题，比传统编码器模型表现更优

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+of+Summarization+by+Generative+Language+Models+for+Automated+Scoring+of+Long+Essays，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22830，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22830&send_immediately=true&force_search=false)

**原文摘要:** BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [149] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi, Dong Won Lee, Beatriz Flamia, Jasmine David, Brandon Hanks, Cynthia Breazeal, Emma Anderson, Grace Lin*

**主要类别:** cs.CL

**AI概要:** 该研究探讨了如何利用明确的对话线程信息来提升大语言模型在同步多人群组对话中对关系性话语行为的自动编码性能，提出了线程识别指南并测试了不同提示策略。


<details>
  <summary>更多</summary>
  
**动机:** 同步口语对话中的话题线程检测具有挑战性（由于重叠话轮和隐含线索），而大语言模型在处理需要追踪对话链接的长上下文任务时存在困难，需要探索如何通过明确的线程链接来改进基于LLM的关系性话语分析。

**方法:** 开发了同步多人群组转录本中线程识别的系统指南，对不同LLM提示策略进行基准测试，然后测试线程信息对下游会话分析框架编码性能的影响。

**结果:** 提供清晰的对话线程信息能够提高LLM编码性能，表明下游分析严重依赖良好结构的对话。

**结论:** 这项工作推进了将LLM与稳健的对话线程结构相结合的方法，以理解复杂的实时群组互动，并讨论了人机混合方法在时间和成本方面的实际权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Large+Language+Models+to+Identify+Conversation+Threads+in+Collaborative+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22844&send_immediately=true&force_search=false)

**原文摘要:** Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [150] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein, Neelay Velingker, Mayur Naik, Eric Wong*

**主要类别:** cs.CL

**AI概要:** PIPS方法通过实例级程序合成和结构反馈，在复杂推理任务中显著优于CoT和PoT方法，提升准确率并减少不良程序生成。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在零样本推理方面表现优异，但在复杂多步推理任务中仍存在困难。现有的CoT和PoT方法虽然有所改进，但在算法领域经常产生不理想的解决方案。

**方法:** 提出PIPS方法：1) 在实例级别生成和精炼程序；2) 使用结构反馈而不依赖任务特定指导或显式测试用例；3) 引入置信度指标动态选择直接推理或程序合成策略。

**结果:** 在3个前沿LLM和30个基准测试中，PIPS相比PoT和CoT分别提升绝对调和平均准确率8.6%和9.4%，在算法任务中比PoT减少65.1%的不良程序生成。

**结论:** PIPS通过实例级程序合成和自适应策略选择，有效提升了复杂推理任务的性能，为LLM在算法领域的应用提供了更可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Once+Upon+an+Input%3A+Reasoning+via+Per-Instance+Program+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22849&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [151] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He, Tianjun Zhong, Richard Antonello, Gavin Mischler, Micah Goldblum, Nima Mesgarani*

**主要类别:** cs.CL

**AI概要:** 该研究提出了一种残差解耦方法，从大语言模型中分离出词汇、句法、语义和推理四个正交的表示成分，用于建模大脑对自然语言的处理过程，发现推理成分具有独特的神经表征和时序特征。


<details>
  <summary>更多</summary>
  
**动机:** 现有大语言模型的内部表示高度'纠缠'，混合了词汇、句法、语义和推理信息，这导致传统脑编码分析偏向浅层语言特征，难以分离深层认知过程的神经基础。

**方法:** 采用残差解耦方法，首先探测语言模型以识别特征特定层，然后迭代回归掉低层表示，生成四个近乎正交的嵌入表示（词汇、句法、语义和推理）。

**结果:** 1) 分离出的推理嵌入具有独特预测能力，解释了其他语言特征无法解释的神经活动变异；2) 推理的神经信号在时间上更晚出现（约350-400ms）；3) 标准非解耦嵌入会误导分析，其预测成功主要归因于浅层特征。

**结论:** 该方法成功分离了语言处理的不同认知成分，揭示了推理过程具有独特的神经表征和时序特征，为理解大脑语言处理层次提供了新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Far+from+the+Shallow%3A+Brain-Predictive+Reasoning+Embedding+through+Residual+Disentanglement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22860&send_immediately=true&force_search=false)

**原文摘要:** Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [152] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy, Ayush Rajesh Jhaveri, Ilias Triantafyllopoulos*

**主要类别:** cs.CL

**AI概要:** 大型语言模型存在不确定性现象，即重新提示时会改变之前正确的答案为错误答案。研究发现非检索注意力头是主要原因，屏蔽这些头可减少15%的翻转行为，但在下游任务中存在权衡。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在重新提示时会改变正确答案为错误答案，这种不确定性行为在高风险领域存在严重风险，需要研究其机制并找到缓解方法。

**方法:** 采用Needle-in-a-Haystack检索框架，集成Flip式重新评估提示来模拟答案翻转场景，分析注意力机制并识别导致问题的特定注意力头。

**结果:** 发现检索头不是避免不确定性的主要原因，而是识别出一小部分非检索注意力头过度关注误导性标记。屏蔽这些头可减少高达15%的翻转行为，且不引入不连贯或过度校正。

**结论:** 研究为机制可解释性领域做出贡献，提出了一种简单有效的技术来缓解LLMs中的不确定性驱动故障模式，但在下游任务应用中需要权衡考虑。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+and+Mitigating+Unwanted+Uncertainty+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22866&send_immediately=true&force_search=false)

**原文摘要:** Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [153] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Amit Sheth, Vasu Sharma, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amitava Das*

**主要类别:** cs.CL

**AI概要:** 该研究构建了一个包含5.8万+文本样本的综合数据集，结合纽约时报真实文章和多种先进LLM生成的合成文本，为AI文本检测和模型溯源任务提供基准数据。


<details>
  <summary>更多</summary>
  
**动机:** 随着LLM生成文本越来越像人类写作，需要解决内容真实性、错误信息和可信度问题，这要求有大规模、多样化且标注良好的数据集来开发可靠的检测方法。

**方法:** 创建包含真实纽约时报文章和Gemma-2-9b、Mistral-7B、Qwen-2-72B、LLaMA-8B、Yi-Large、GPT-4-o等多种先进LLM生成文本的数据集，提供原文摘要作为提示和完整人类撰写叙述。

**结果:** 建立了两个关键任务的基准结果：区分人类写作与AI生成文本的准确率为58.35%，将AI文本溯源到生成模型的准确率为8.92%。

**结论:** 通过将真实新闻内容与现代生成模型相结合，该数据集旨在促进稳健检测和溯源方法的发展，在生成式AI时代培养信任和透明度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Dataset+for+Human+vs.+AI+Generated+Text+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22874&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [154] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang*

**主要类别:** cs.CL

**AI概要:** 论文提出EQSPEC和EXSPEC两种批处理推测解码方法，解决批量处理中的参差不齐张量问题，在保证输出等价性的同时显著提升推理吞吐量。


<details>
  <summary>更多</summary>
  
**动机:** 推测解码在批处理中面临参差不齐张量问题，导致序列对齐破坏、位置ID和注意力掩码损坏，现有实现违反输出等价性要求。

**方法:** 提出EQSPEC方法保证正确性，分析重对齐开销；引入EXSPEC方法维护滑动序列池和动态分组，减少重对齐开销。

**结果:** 在SpecBench数据集上，Vicuna、Qwen3和GLM-4等模型上实现最高3倍吞吐量提升，批量大小8时保持95%输出等价性。

**结论:** 该方法无需定制内核，可与现有推理栈无缝集成，有效解决了批处理推测解码的正确性和效率问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Batch+Speculative+Decoding+Done+Right，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22876&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [155] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang, Lanser Contributors*

**主要类别:** cs.CL

**AI概要:** Lanser-CLI是一个CLI优先的编排层，通过钉住和中介语言服务器协议(LSP)服务器，为编码代理和CI提供确定性、可重放的工作流程，解决大语言模型在API幻觉和编辑定位错误方面的问题。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型经常产生API幻觉和错误定位编辑，而语言服务器能够计算关于真实代码的经过验证的IDE级事实。需要一种方法将语言服务器的确定性能力与AI代理的工作流程相结合。

**方法:** 开发Lanser-CLI工具，包含：1) 通过Selector DSL实现稳健的寻址方案；2) 确定性分析包标准化语言服务器响应；3) 为变异操作提供安全信封；4) 基于语言服务器事实的过程奖励函数。

**结果:** 实现了在冻结快照下的确定性，建立了过程奖励的单调性属性，使其适用于过程监督和反事实分析。

**结论:** 语言服务器不仅提供结构信息，还提供可操作的过程奖励，Lanser-CLI成功地将语言服务器的验证能力与AI代理工作流程集成，提供了机器检查的逐步信号。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Server+CLI+Empowers+Language+Agents+with+Process+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22907&send_immediately=true&force_search=false)

**原文摘要:** Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [156] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi*

**主要类别:** cs.CL

**AI概要:** 该论文介绍了Infinity-Chat数据集，用于评估语言模型在开放性问题生成中的多样性问题，揭示了模型存在的人工蜂群效应（模式崩溃），即模型内部和不同模型之间都产生高度相似的输出。


<details>
  <summary>更多</summary>
  
**动机:** 语言模型在生成多样化、类人创造性内容方面存在困难，可能导致人类思维的长期同质化，但目前缺乏评估LM输出多样性的可扩展方法。

**方法:** 引入Infinity-Chat数据集（26K个多样化、开放式用户查询），建立首个全面的开放式提示分类法（6个顶级类别，17个子类别），并进行大规模模式崩溃研究，包含31,250个人工标注。

**结果:** 发现LM在开放式生成中存在明显的人工蜂群效应：模型内部重复性和模型间同质性；LM、奖励模型和LM评判器在对引发不同个体偏好的人类评分校准方面表现较差。

**结论:** Infinity-Chat为系统研究现实世界开放式查询提供了首个大规模资源，揭示了缓解人工蜂群效应带来的长期AI安全风险的关键见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Hivemind%3A+The+Open-Ended+Homogeneity+of+Language+Models+%28and+Beyond%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22954&send_immediately=true&force_search=false)

**原文摘要:** Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [157] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam*

**主要类别:** cs.CL

**AI概要:** 提出TAG（标签增强生成）方法，通过在长文本中添加标签或标签定义来增强LLM的长上下文问答能力，无需修改原文结构，在多个基准测试中取得显著性能提升


<details>
  <summary>更多</summary>
  
**动机:** 现有大型语言模型在长上下文问答和推理方面存在严重限制，传统方法如RAG和分块重排需要复杂的预处理且对策略敏感

**方法:** 使用轻量级数据增强策略TAG，通过在检索文档中添加标签或标签定义来增强提示，保持文档完整性

**结果:** 在NoLima和NovelQA基准测试中，TAG方法相比基线性能提升最高达17%（32K token上下文）和2.9%（复杂多跳推理）

**结论:** TAG是一种有效且轻量的长上下文增强方法，能够显著提升LLM性能而不破坏文档完整性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tagging-Augmented+Generation%3A+Assisting+Language+Models+in+Finding+Intricate+Knowledge+In+Long+Contexts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22956&send_immediately=true&force_search=false)

**原文摘要:** Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [158] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao*

**主要类别:** cs.CL

**AI概要:** 本文针对大语言模型在长文本生成中的事实准确性评估难题，提出了集成大规模数据集、多智能体验证机制和加权评估指标的系统方法，构建了中文长文本事实性数据集LongHalluQA和多智能体验证系统MAD-Fact。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对其输出事实准确性的担忧，现有短文本评估方法难以处理长文本中复杂的推理链、交织观点和累积信息的问题。

**方法:** 提出系统性方法：构建中文长文本事实性数据集LongHalluQA；开发基于辩论的多智能体验证系统MAD-Fact；引入事实重要性层次结构来捕捉长文本中不同主张的重要性差异。

**结果:** 在两个基准测试上的实验表明，更大的LLM通常保持更高的事实一致性，而国产模型在中文内容上表现更优。

**结论:** 本研究为评估和提升长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAD-Fact%3A+A+Multi-Agent+Debate+Framework+for+Long-Form+Factuality+Evaluation+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22967&send_immediately=true&force_search=false)

**原文摘要:** The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [159] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

**主要类别:** cs.CL

**AI概要:** 本研究开发了基于句子级嵌入的定制化大语言模型，用于教学质量的客观评估，实现了接近甚至超越人类评估者的性能，并验证了与教师增值测量的外部有效性。


<details>
  <summary>更多</summary>
  
**动机:** 教育领域长期缺乏客观可扩展的教学质量测量方法，传统通用大语言模型在应用复杂的课堂观察工具时可靠性不足。

**方法:** 使用句子级嵌入架构（而非传统的子词标记化）构建定制化LLMs，系统评估五种句子嵌入模型，采用防过拟合的数据高效训练策略，分析标注上下文窗口。

**结果:** 专业模型达到人类水平甚至超人类性能（专家人类评分相关性>0.65，超越平均人-人评估相关性），高级模型更多关注课程层面特征而非孤立话语，总体模型分数与教师增值测量一致。

**结论:** 建立了一种可行且强大的AI驱动教学测量新方法，为实现可扩展、可靠、有效的教育者发展反馈提供了路径，但模型在单项水平上尚未完全泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Teaching+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22968&send_immediately=true&force_search=false)

**原文摘要:** Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [160] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang, Timothy Tin-Long Tse, Jian Zhu*

**主要类别:** cs.CL

**AI概要:** 对最先进的transformer、state-space和混合架构大语言模型在知识型上下文学习任务上的深入评估，发现不同架构模型虽然表现相似但内部机制不同，函数向量主要位于自注意力和Mamba层，且在不同知识类型任务中重要性各异


<details>
  <summary>更多</summary>
  
**动机:** 深入研究不同架构大语言模型在上下文学习中的内部工作机制差异，特别是transformer、state-space和混合架构在知识型任务中的表现和机制

**方法:** 使用行为探测和干预方法相结合的方式，评估模型在两类知识型上下文学习任务上的表现，分析函数向量的位置和作用机制

**结果:** 发现不同架构LLMs任务表现相似但内部机制不同；函数向量主要位于自注意力和Mamba层；Mamba2可能使用不同于函数向量的机制；函数向量在参数化知识检索任务中更重要，在上下文知识理解中作用较小

**结论:** 研究提供了对不同架构和任务类型的更细致理解，强调了结合行为和机制分析对于研究LLM能力的重要性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+In-Context+Learning+Beyond+Transformers%3A+An+Investigation+of+State+Space+and+Hybrid+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23006&send_immediately=true&force_search=false)

**原文摘要:** We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [161] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim*

**主要类别:** cs.CL

**AI概要:** LangLingual是一个基于LangChain框架和大型语言模型构建的对话代理系统，专门为语言学习者提供实时语法反馈、情境感知的语言练习和学习进度追踪。


<details>
  <summary>更多</summary>
  
**动机:** 语言教育者希望为学习者创造丰富的学习体验，但在提供反馈和练习方面可能受到限制，需要技术解决方案来扩展教学能力。

**方法:** 使用LangChain框架和大型语言模型构建对话代理系统，设计包含实时语法反馈、情境感知练习生成和学习进度追踪功能的架构。

**结果:** 系统表现出良好的可用性，产生了积极的学习成果，并获得了学习者的高度参与和认可。

**结论:** LangLingual系统成功解决了语言教育中的反馈限制问题，通过技术手段有效提升了语言学习体验和效果，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LangLingual%3A+A+Personalised%2C+Exercise-oriented+English+Language+Learning+Tool+Leveraging+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23011&send_immediately=true&force_search=false)

**原文摘要:** Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [162] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu*

**主要类别:** cs.CL

**AI概要:** TIR-Judge是一个端到端的强化学习框架，通过集成代码执行器来训练LLM评估器，在多个评估基准上显著优于基于推理的评估器，且无需蒸馏即可实现自我进化。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LLM评估器主要基于文本推理，难以验证复杂约束或进行精确计算，而工具集成推理在其他任务中已证明有效。

**方法:** 提出TIR-Judge框架，基于三个原则：多样化训练（可验证和不可验证领域）、灵活评估格式（点对、配对、列表式）、迭代强化学习（无需蒸馏直接从初始模型启动）。

**结果:** 在7个公共基准上，TIR-Judge比基于推理的评估器提升6.4%（点对）和7.7%（配对），8B参数的模型达到与Claude-Opus-4相当的列表式性能，无需蒸馏的变体也能达到蒸馏版本的效果。

**结论:** 工具增强的评估器可以通过迭代强化学习自我进化，为LLM评估提供了更精确和可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incentivizing+Agentic+Reasoning+in+LLM+Judges+via+Tool-Integrated+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23038&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [163] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li*

**主要类别:** cs.CL

**AI概要:** 提出一种新的注意力机制Knocking-Heads Attention (KHA)，通过在多头注意力计算前引入跨头特征交互，解决了传统多头注意力机制中头间缺乏强交互的问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统多头注意力机制(MHA)及其变体(GQA、GTA)只是简单拼接各头的输出，缺乏头间的强交互，且增加头数会削弱单个头的容量。

**方法:** 使用共享的对角初始化投影矩阵在所有头之间建立特征级交互，使注意力头能够相互"敲击"，在缩放点积注意力计算前进行跨头交互。

**结果:** 在6.1B参数的MoE模型上训练验证，KHA带来更优且更稳定的训练动态，在下游任务中表现更好。

**结论:** KHA通过最小参数和计算开销实现了头间的有效交互，可无缝集成到各种注意力变体中，提升模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knocking-Heads+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23052&send_immediately=true&force_search=false)

**原文摘要:** Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [164] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon, Byeolhee Kim, Nikhil Verma*

**主要类别:** cs.CL

**AI概要:** QTT-RAG通过质量感知翻译标注提升多语言检索增强生成性能，在不改变原文内容的情况下评估翻译质量，帮助生成模型基于翻译可靠性做出明智决策


<details>
  <summary>更多</summary>
  
**动机:** 现有的mRAG方法在低资源语言环境中依赖英语文档翻译，但翻译质量差会降低生成性能，现有方法要么假设翻译质量足够好，要么使用会导致事实扭曲和幻觉的重写方法

**方法:** 提出QTT-RAG方法，明确从三个维度评估翻译质量：语义等价性、语法准确性和自然流畅性，并将这些分数作为元数据附加而不改变原始内容

**结果:** 在两个开放域QA基准测试中，使用6个2.4B到14B参数的指令调优LLM，涵盖韩语、芬兰语和中文，QTT-RAG优于CrossRAG和DKM-RAG基线

**结论:** QTT-RAG在保持事实完整性的同时，使生成模型能够基于翻译可靠性做出明智决策，为低资源环境下有限母语文档的跨语言文档使用提供了实用且鲁棒的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quality-Aware+Translation+Tagging+in+Multilingual+RAG+system，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23070&send_immediately=true&force_search=false)

**原文摘要:** Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [165] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai*

**主要类别:** cs.CL

**AI概要:** 本调查论文对大型语言模型的中期训练进行了系统研究，提出了中期训练的形式化定义，分析了数据管理、训练策略和模型架构优化的框架，并展示了中期训练在LLM能力渐进发展中的关键作用。


<details>
  <summary>更多</summary>
  
**动机:** 基础模型的最新进展突显了多阶段训练的重要价值，特别是中期训练作为连接预训练和后训练的关键阶段，能够系统性增强特定能力同时保持基础能力。

**方法:** 通过形式化定义LLM中期训练概念，研究包含数据管理、训练策略和模型架构优化的优化框架，分析主流模型在目标驱动干预下的实现方式。

**结果:** 阐明了中期训练的独特贡献，提供了全面的分类体系和可操作的见解，展示了中期训练如何作为LLM能力发展的一个独特且关键的阶段。

**结论:** 中期训练是LLM能力渐进发展中不可或缺的阶段，该研究为未来LLM发展的研究和创新提供了理论基础和实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+LLM+Mid-training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23081&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [166] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han*

**主要类别:** cs.CL

**AI概要:** MAP4TS是一个新颖的多方面提示框架，通过将经典时间序列分析融入提示设计，显著提升了基于大语言模型的时间序列预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态方法往往忽视了时间序列数据特有的统计特性和时间依赖性，需要一种能够更好结合时间序列专业知识的提示框架。

**方法:** 提出包含四个专门提示组件的框架：全局域提示、局部域提示、统计提示和时间提示，结合自相关、偏自相关和傅里叶分析等手工设计的洞察，通过跨模态对齐模块生成统一表示。

**结果:** 在八个不同数据集上的广泛实验显示，MAP4TS持续优于最先进的基于LLM的方法，消融研究表明提示感知设计显著提高了性能稳定性。

**结论:** 结构化提示与GPT-2骨干网络的组合在长期预测任务中表现优于LLaMA等更大模型，证明了将领域专业知识融入提示设计的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAP4TS%3A+A+Multi-Aspect+Prompting+Framework+for+Time-Series+Forecasting+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23090，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23090&send_immediately=true&force_search=false)

**原文摘要:** Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [167] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang*

**主要类别:** cs.CL

**AI概要:** 该研究探讨了在医疗多文档摘要任务中使用层次结构输入能否改善模型的信息组织能力，发现层次化方法在保持事实性、覆盖面和连贯性的同时提高了人类对生成摘要的偏好。


<details>
  <summary>更多</summary>
  
**动机:** 医疗多文档摘要是复杂任务，需要有效管理跨文档关系，传统平面摘要方法在信息组织和上下文化方面存在局限，因此研究层次结构是否能改善模型性能。

**方法:** 研究比较了两种层次组织方法在三个大语言模型中的应用，使用自动化指标、基于模型的指标以及领域专家在多个维度（偏好、可理解性、清晰度等）的评估进行综合分析。

**结果:** 人类专家更偏好模型生成的摘要而非人工撰写的摘要；层次化方法在保持事实性、覆盖面和连贯性的同时提高了人类偏好；GPT-4模拟判断与人类判断在更客观的评估维度上具有更高一致性。

**结论:** 层次结构可以提高医疗摘要的清晰度同时保持内容覆盖面，为改善人类对生成摘要的偏好提供了实用方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Hierarchical+Organization+for+Medical+Multi-document+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23104，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23104&send_immediately=true&force_search=false)

**原文摘要:** Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [168] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada, Jana Straková*

**主要类别:** cs.CL

**AI概要:** 提出一种紧凑的单模型多语言词形变化方法，在73种语言上联合训练，性能优于单语言基线，简化了部署需求。


<details>
  <summary>更多</summary>
  
**动机:** 解决缺乏开源、通用、多语言形态变化系统的问题，特别是能够处理未见词汇和多种语言（包括捷克语）的需求。

**方法:** 使用多语言联合训练模型，从73种Universal Dependencies树库中提取lemma-tag-form三元组及其频率计数，采用频率加权、lemma不相交的训练-开发-测试重采样程序。

**结果:** 模型轻量、对未见词汇鲁棒，在大多数语言上超越单语言基线，证明了多语言建模在词形变化任务中的有效性。

**结论:** 多语言建模在词形变化任务中具有显著优势，能够简化部署并提高性能，所有代码已开源发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flexing+in+73+Languages%3A+A+Single+Small+Model+for+Multilingual+Inflection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23114&send_immediately=true&force_search=false)

**原文摘要:** We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [169] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li*

**主要类别:** cs.CL

**AI概要:** TopLoRA提出了一种基于token-wise投影的低秩适应方法，通过动态调整LoRA权重来捕捉token特定信息，在多个模型和数据集上表现优于标准LoRA及其变体。


<details>
  <summary>更多</summary>
  
**动机:** 标准LoRA中所有输入token共享相同权重，无法有效捕捉token间的语义差异，限制了其表达能力。

**方法:** 提出TopLoRA方法，使用动态生成的Σ_X对角矩阵为每个输入token生成特定的LoRA权重BΣ_XA，实现token-wise的输入输出投影。

**结果:** 在多个模型和数据集上的实验表明，TopLoRA始终优于标准LoRA及其变体，且不增加LoRA权重秩数。

**结论:** TopLoRA通过token-wise的权重调整有效提升了LoRA的适应能力，为大语言模型的高效微调提供了更优解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Higher+Rank%3A+Token-wise+Input-Output+Projections+for+Efficient+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23123，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23123&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [170] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada, Jana Straková*

**主要类别:** cs.CL

**AI概要:** 该论文探索将语料库频率信息融入形态屈折任务中，通过频率加权划分数据集、引入词例准确率评估指标和频率感知训练方法，在43种语言中26种语言上优于均匀采样。


<details>
  <summary>更多</summary>
  
**动机:** 传统形态屈折方法缺乏频率分布信息，而实际应用中用户输入反映的是自然文本的真实频率分布，需要开发能更好反映现实世界频率分布的系统。

**方法:** 提出三个维度的方法：(1) 词干不相交的频率加权训练-开发-测试集划分；(2) 补充标准类型准确率，引入词例准确率评估；(3) 在训练数据采样中引入频率感知训练方法。

**结果:** 频率感知训练在43种语言中的26种语言上表现优于均匀采样，证明频率信息对提升形态屈折性能的有效性。

**结论:** 将语料库频率信息融入形态屈折任务的多维度方法能更好地反映现实世界语言使用情况，频率感知训练显著提升系统性能，为实际部署提供更实用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corpus+Frequencies+in+Morphological+Inflection%3A+Do+They+Matter%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23131&send_immediately=true&force_search=false)

**原文摘要:** The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [171] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei*

**主要类别:** cs.CL

**AI概要:** ENTP框架通过神经符号方法从低质量数据中提取价值，仅使用低质量数据构建的数据集在多个基准测试中超越了传统数据选择方法和完整原始数据集


<details>
  <summary>更多</summary>
  
**动机:** 现有监督微调方法丢弃低质量数据，但其中可能包含有价值信息，且质量过滤方法不完善

**方法:** 提出ENTP框架：符号模块基于统计先验识别和修剪噪声样本，神经模块利用潜在表示和模型知识合成增强的指令-响应对

**结果:** ENTP增强的数据集在五个指令跟随基准测试中优于13个现有数据选择基线，甚至超越了使用完整原始数据集（约30万样本）的微调效果

**结论:** 低质量数据具有未开发的潜力，智能净化和合成对于高效指令对齐至关重要

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ENTP%3A+Enhancing+Low-Quality+SFT+Data+via+Neural-Symbolic+Text+Purge-Mix，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23160&send_immediately=true&force_search=false)

**原文摘要:** Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [172] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu*

**主要类别:** cs.CL

**AI概要:** 提出双阶段精炼(DSR)框架，将剧本生成分解为创意叙事生成和格式转换两个独立阶段，通过混合数据合成解决训练数据稀缺问题，显著提升LLM生成剧本的质量


<details>
  <summary>更多</summary>
  
**动机:** 传统端到端LLM剧本生成方法同时要求模型掌握创意叙事构建和严格格式遵循两种不同能力，导致生成结果缺乏深层结构完整性和故事实质

**方法:** Dual-Stage Refinement (DSR)框架：第一阶段将简要大纲转换为丰富的小说风格散文，第二阶段将叙事精炼为专业格式剧本。采用混合数据合成方法（反向合成解构现有剧本，正向合成生成高质量叙事文本）

**结果:** 专业编剧盲测显示DSR对Gemini-2.5-Pro等强基线达到75%胜率，达到人类水平表现的82.7%

**结论:** 分解生成架构结合定制化数据合成能有效让LLM在复杂创意领域实现专业化，为解决多能力要求任务提供了有效范式

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Direct+Generation%3A+A+Decomposed+Approach+to+Well-Crafted+Screenwriting+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23163&send_immediately=true&force_search=false)

**原文摘要:** The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [173] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro*

**主要类别:** cs.CL

**AI概要:** 论文提出了MATCH，一种基于对比学习的无参考代码评估指标，用于评估AI生成代码与开发者意图的匹配程度，相比现有指标在功能正确性和人类偏好方面表现出更强的相关性。


<details>
  <summary>更多</summary>
  
**动机:** AI代码生成日益普及，但传统评估方法如单元测试难以扩展且成本高，语法相似性指标无法捕捉代码功能，现有无参考评估指标选择有限，需要更好的评估解决方案。

**方法:** 使用对比学习技术为代码和自然语言任务描述生成有意义的嵌入表示，通过相似性评分来评估生成代码实现任务的程度。

**结果:** MATCH在多种编程语言中显示出比现有指标更强的与功能正确性和人类偏好的相关性。

**结论:** MATCH作为一种无参考评估指标，能够有效解决AI生成代码的评估挑战，为代码生成系统的性能评估提供了更好的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MATCH%3A+Task-Driven+Code+Evaluation+through+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23169&send_immediately=true&force_search=false)

**原文摘要:** AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [174] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang, Wenxuan Zhao, Jun Gao*

**主要类别:** cs.CL

**AI概要:** SI-Bench是一个基于真实社交媒体对话的社会智能评估基准，包含2221个多轮对话，用于评估大语言模型在复杂社交情境中的表现。研究发现SOTA模型在过程推理上超越人类专家，但在回复质量上仍落后于人类，且CoT推理可能降低模型在社交对话任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型具备类人能力并被部署为自主代理与人类互动，需要评估其在真实复杂社交互动中的表现。现有研究多通过模拟代理间互动构建数据集，无法捕捉真实人类对话的语言风格和关系动态。

**方法:** 基于广泛的社会科学理论，从社交网络应用收集2221个真实多轮对话构建SI-Bench基准，并选取312个对话对8个主要模型进行人工标注评估。

**结果:** 实验显示：1)SOTA模型在复杂社交情境的过程推理上超越人类专家；2)在回复质量上仍落后于人类；3)引入Chain-of-Thought推理可能降低LLMs在社交对话任务中的性能。

**结论:** SI-Bench提供了一个基于真实对话的社会智能评估框架，揭示了LLMs在社交智能方面的优势与不足，为未来研究提供了重要基准和数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SI-Bench%3A+Benchmarking+Social+Intelligence+of+Large+Language+Models+in+Human-to-Human+Conversations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23182&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [175] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli*

**主要类别:** cs.CL

**AI概要:** 提出DREAM方法，使用预训练关系抽取模型从医学文本中构建药物关系本体，并用大语言模型验证结果，在PubMed摘要子集上达到71%的验证一致性。


<details>
  <summary>更多</summary>
  
**动机:** 药物关系抽取对识别药物相互作用和预测副作用至关重要，但缺乏专门的数据集，需要采用迁移学习方法。

**方法:** 先使用训练好的关系抽取模型发现实体关系，然后应用于医学文本语料库构建药物关系本体，最后用大语言模型验证抽取的关系。

**结果:** 定量结果显示LLM对PubMed摘要子集中71%的抽取关系表示同意；定性分析显示该方法能揭示医学领域的模糊性。

**结论:** 该方法能有效构建药物关系本体，但揭示了医学领域关系抽取的内在挑战和模糊性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DREaM%3A+Drug-Drug+Relation+Extraction+via+Transfer+Learning+Method，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23189&send_immediately=true&force_search=false)

**原文摘要:** Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [176] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz*

**主要类别:** cs.CL

**AI概要:** 提出一种句子级别的过程奖励模型(PRM)，用于检测大型视觉语言模型在放射学报告生成中的幻觉问题，该模型在临床指标上显著优于现有方法，并能有效泛化到未见过的模型。


<details>
  <summary>更多</summary>
  
**动机:** 大型视觉语言模型在放射学报告生成中存在严重的临床幻觉风险，现有检测方法缺乏句子级粒度且泛化能力不足。

**方法:** 开发轻量级句子级过程奖励模型(PRM)，使用弱监督标签在MIMIC-CXR数据集上微调，预测每个生成句子的事实正确性，基于临床上下文和先前文本。

**结果:** PRM在Matthews相关系数上相对提升7.5%，AUROC提升1.8%；能有效过滤低质量报告，F1-CheXbert分数提升4.5%；在加权最佳选择过程中，F1-CheXbert提升7.4%，BERTScore提升0.6%。

**结论:** 轻量级、上下文感知的PRM为临床LVLM提供了模型无关的安全层，无需访问内部激活，有效解决了幻觉检测问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Process+Reward+Models+for+Sentence-Level+Verification+of+LVLM+Radiology+Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23217&send_immediately=true&force_search=false)

**原文摘要:** Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [177] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat*

**主要类别:** cs.CL

**AI概要:** 该研究创建了一个78小时的孟加拉语方言语音数据集Ben-10，发现语音基础模型在方言ASR任务中表现不佳，无论是零样本还是微调设置。方言特定的模型训练能缓解这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统语音识别研究主要关注标准语言形式，而方言ASR通常被视为微调任务。研究旨在探索方言变体对语音识别的影响。

**方法:** 开发了78小时的带标注孟加拉语方言语音转文本语料库Ben-10，从语言学和数据驱动角度分析语音基础模型在方言ASR中的表现。

**结果:** 语音基础模型在方言ASR中表现严重不足，所有深度学习方法都难以有效建模方言变异的语音数据，但方言特定训练能改善性能。

**结论:** 方言ASR需要专门的处理方法，Ben-10数据集可作为资源受限条件下ASR建模的分布外资源，数据集和代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+ASR+foundation+models+generalized+enough+to+capture+features+of+regional+dialects+for+low-resource+languages%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23252，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23252&send_immediately=true&force_search=false)

**原文摘要:** Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [178] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi*

**主要类别:** cs.CL

**AI概要:** Mubeen是一个专有的阿拉伯语语言模型，通过深度语言工程框架训练，专注于阿拉伯语言学、伊斯兰研究和文化遗产的深度理解，采用原生阿拉伯语源数据确保文化真实性和准确性，通过Practical Closure架构解决"效用差距危机"。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有阿拉伯语模型依赖英语翻译数据导致意图检测和RAG失败的问题，确保文化真实性和准确性，同时解决事实正确答案无法满足用户核心需求的"效用差距危机"。

**方法:** 使用专有阿拉伯OCR引擎数字化历史手稿扩展真实阿拉伯语源数据集，结合深度语言工程框架训练，整合语言学、法学、圣训和古兰经注释等学术著作，采用Practical Closure架构优先清晰度和决定性指导。

**结果:** 开发出能够精确理解古典文本、当代写作和地区方言的阿拉伯语模型，在文化保存和一般知识领域均表现出强大性能，从信息存储库转变为决定性指南。

**结论:** Mubeen模型成功解决了阿拉伯语AI领域的文化真实性和效用差距问题，与沙特2030愿景保持一致，为阿拉伯语言和文化传承提供了专业化的AI解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mubeen+AI%3A+A+Specialized+Arabic+Language+Model+for+Heritage+Preservation+and+User+Intent+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23271&send_immediately=true&force_search=false)

**原文摘要:** Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [179] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei*

**主要类别:** cs.CL

**AI概要:** 提出新方法GRPO-AR提升LLM生成代码的美学质量，通过大规模美学指令数据集AesCode-358K和多智能体奖励反馈机制，在OpenDesign基准测试中显著超越GPT-4o等模型


<details>
  <summary>更多</summary>
  
**动机:** LLM在代码生成任务中表现出色，但在视觉导向的编程任务中美学质量较差，需要专门优化代码美观度

**方法:** 构建AesCode-358K指令调优数据集；设计多智能体奖励反馈系统评估可执行性、静态美学和交互美学；开发GRPO-AR算法联合优化功能性和美学

**结果:** 实验显示该方法在OpenDesign基准上显著提升性能，AesCoder-4B模型超越GPT-4o和GPT-4.1，达到480B-685B参数开源模型的性能水平

**结论:** 结合监督微调和强化学习的多智能体奖励反馈方法能有效提升LLM生成代码的美学质量，为视觉编程任务提供了新的优化路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Code+Aesthetics+with+Agentic+Reward+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23272&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [180] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel*

**主要类别:** cs.CL

**AI概要:** 第九届CHiME挑战赛引入多模态上下文感知识别任务(MCoRec)，解决单房间环境下重叠对话的鸡尾酒会问题，通过音频、视觉和上下文线索进行说话人分离和转录。


<details>
  <summary>更多</summary>
  
**动机:** 解决自然多人群聊场景中的极端语音重叠问题（可达100%重叠率），需要同时回答"谁在何时说了什么、与谁对话"这一复杂问题。

**方法:** 使用音频和视觉多模态方法，收集非脚本化的自然群聊数据，开发基线系统进行说话人语音转录和对话聚类。

**结果:** 纯音频基线系统的词错误率超过100%，而加入视觉线索后性能提升50%，证明了多模态方法的重要性。

**结论:** 多模态方法在处理极端语音重叠场景中至关重要，视觉信息的引入显著提升了说话人分离和语音识别的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Cocktail-Party+Benchmark%3A+Multi-Modal+dataset+and+Comparative+Evaluation+Results，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23276&send_immediately=true&force_search=false)

**原文摘要:** We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


### [181] [DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model](https://arxiv.org/abs/2510.23284)
*Yuanzhen Xie, Liu Ye, Jiqun Chu, Mochi Gao, Hehuan Liu, Yunzhi Tan, Bo Hu, Zang Li*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个全自动的数据中心化pipeline用于Text-to-SQL任务，包括自适应数据修复和错误数据增强，并采用多模型协作训练和集成策略，在轻量级模型中取得了最佳性能。


<details>
  <summary>更多</summary>
  
**动机:** 虽然基于代理的框架在Text-to-SQL任务中取得了显著改进，但数据中心化策略的影响尚未得到充分探索。现有单一微调模型的能力有限，需要更有效的数据处理和多模型协作方法。

**方法:** 设计了全自动数据中心化pipeline：1)自适应数据修复自动发现和修复训练数据错误；2)错误数据增强扩散和增强模型预测的错误数据；3)多模型协作训练，用不同增强数据训练多个模型；4)集成策略整合多个模型能力解决多选问题。

**结果:** 实验和消融研究证明了数据中心化pipeline和多模型交互迭代策略的有效性，在轻量级Text-to-SQL模型（70B参数内）中取得了第一名。

**结论:** 数据中心化方法和多模型协作策略显著提升了Text-to-SQL任务的性能，证明了数据处理质量对模型性能的重要影响，为轻量级模型提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DCMM-SQL%3A+Automated+Data-Centric+Pipeline+and+Multi-Model+Collaboration+Training+for+Text-to-SQL+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23284，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23284&send_immediately=true&force_search=false)

**原文摘要:** Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).

</details>


### [182] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri, Dania Desouki, Khloud Al Jallad*

**主要类别:** cs.CL

**AI概要:** 该论文介绍了Arabic Little STT数据集，包含288名阿拉伯儿童的355条语音，并评估了Whisper模型在儿童语音识别上的表现，发现即使最佳模型在儿童语音上的词错误率也高达0.66，远高于成人语音的0.20以下，强调了儿童语音数据和伦理框架的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 解决阿拉伯语等低资源语言中儿童特定语音语料库缺失的问题，为阿拉伯语儿童提供更公平的语音技术。

**方法:** 创建Arabic Little STT数据集（包含288名6-13岁儿童的355条语音），并系统评估8个Whisper变体在该数据集上的表现，与成人阿拉伯语基准进行比较。

**结果:** 最佳模型（Large_v3）在儿童语音上的词错误率高达0.66，而成人数据集上的词错误率低于0.20，性能差异显著。

**结论:** 研究强调了在ASR开发中需要专门的儿童语音基准和包容性训练数据，并需建立严格的伦理和隐私框架保护儿童敏感信息，为阿拉伯语儿童公平语音技术研究迈出第一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Arabic+Little+STT%3A+Arabic+Children+Speech+Recognition+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23319，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23319&send_immediately=true&force_search=false)

**原文摘要:** The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [183] [Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models](https://arxiv.org/abs/2510.23334)
*Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik*

**主要类别:** cs.CL

**AI概要:** AdaSearch是一种新颖的块状搜索策略，通过自适应分配计算预算来优化LLM对齐任务，相比传统方法在无害性生成、情感控制和数学推理任务上提升超过10%的胜率。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM对齐任务中，推理时方法虽然灵活但计算效率低，研究发现响应初始标记对对齐任务更为关键，需要更有效的计算分配策略。

**方法:** 提出AdaSearch块状搜索策略，采用自适应采样计划分配固定计算预算，重点关注关键初始标记，并开发了其树搜索版本AdaBeam。

**结果:** 在8个LLM上的综合评估显示，AdaSearch在无害性生成、控制情感生成和数学推理任务上相比Best-of-N基线方法胜率提升超过10%。

**结论:** AdaSearch通过自适应计算预算分配有效提升了LLM对齐任务的性能，证明了关注响应初始关键标记的重要性，为推理时对齐方法提供了更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Blockwise+Search%3A+Inference-Time+Alignment+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23334&send_immediately=true&force_search=false)

**原文摘要:** LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

</details>


### [184] [BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning](https://arxiv.org/abs/2510.23337)
*Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia*

**主要类别:** cs.CL

**AI概要:** 该论文提出了首个基于八字命理的人格推理QA数据集和BaZi-LLM系统，通过符号推理与大语言模型结合，实现了时间动态和细粒度的虚拟人格生成，相比主流LLM在准确性上有显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前虚拟角色生成方法依赖标注数据或手工制作的人格提示，难以扩展且难以生成真实、上下文一致的人格，需要新的方法来创建更真实的虚拟角色。

**方法:** 创建了首个基于八字的人格推理QA数据集，将人类经验分类为财富、健康、亲情、事业和关系等维度；提出了BaZi-LLM系统，将符号推理与大语言模型集成。

**结果:** 相比DeepSeek-v3和GPT-5-mini等主流LLM，该方法实现了30.3%-62.6%的准确率提升；当使用错误八字信息时，模型准确率下降20%-45%。

**结论:** 基于文化根基的符号推理与LLM集成在真实角色模拟方面具有巨大潜力，为虚拟角色的生成提供了新的有效方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BaZi-Based+Character+Simulation+Benchmark%3A+Evaluating+AI+on+Temporal+and+Persona+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23337&send_immediately=true&force_search=false)

**原文摘要:** Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.

</details>


### [185] [LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data](https://arxiv.org/abs/2510.23341)
*Teng Lin*

**主要类别:** cs.CL

**AI概要:** LightKGG是一个使用小型语言模型(SLMs)从文本数据中高效提取知识图谱的新框架，通过上下文集成图提取和拓扑增强关系推断两项技术创新，解决了传统方法依赖错误模式匹配或资源密集型大语言模型的问题。


<details>
  <summary>更多</summary>
  
**动机:** 高质量知识图谱的稀缺性限制了AI应用发展，现有提取方法要么依赖错误易发的模式匹配技术，要么需要资源密集型的大语言模型，计算需求大且难以在低资源环境中部署。

**方法:** 1) 上下文集成图提取：将上下文信息与节点和边整合到统一图结构中，减少对复杂语义处理的依赖；2) 拓扑增强关系推断：利用提取图的固有拓扑结构高效推断关系，无需依赖LLMs的复杂语言理解能力。

**结果:** LightKGG框架能够以最小的硬件需求准确构建知识图谱，在结构化NLP任务中优化了小型语言模型的效率。

**结论:** 该工作填补了自动化知识提取与实际部署场景之间的差距，为在低资源环境下使用小型语言模型进行高效知识图谱提取提供了科学严谨的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightKGG%3A+Simple+and+Efficient+Knowledge+Graph+Generation+from+Textual+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23341&send_immediately=true&force_search=false)

**原文摘要:** The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

</details>


### [186] [How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes](https://arxiv.org/abs/2510.23358)
*Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个评估大语言模型预测AI对就业需求影响的基准测试，结合美国行业级职位发布数据和全球AI对职业影响的预测数据，测试不同提示策略在劳动力市场预测中的效果。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能正在重塑劳动力市场，但缺乏系统预测AI对就业影响的工具。现有研究显示LLMs可以提取情感、总结经济报告和模拟预测行为，但尚未评估其在前瞻性劳动力预测中的应用。

**方法:** 结合两个互补数据集：美国行业级高频职位发布指数和全球AI采用导致的职业变化预测数据，构建具有明确时间分割的预测任务。评估多种提示策略（任务支架、人物驱动和混合方法）在不同模型家族中的表现。

**结果:** 结构化任务提示持续提高预测稳定性，人物提示在短期趋势上具有优势。但不同行业和时间跨度的表现差异显著，表明需要领域感知的提示和严格的评估协议。

**结论:** 通过发布基准测试，支持未来关于劳动力预测、提示设计和基于LLM的经济推理研究，为研究AI作为劳动力市场预测工具的局限性和机会提供可复现的测试平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+AI+Forecasts+AI+Jobs%3A+Benchmarking+LLM+Predictions+of+Labor+Market+Changes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23358&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

</details>


### [187] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen, Pien Pieterse, Yusuf Çelik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren*

**主要类别:** cs.CL

**AI概要:** 本研究通过规则模型和大型语言模型分析宗教语言在气候相关文本中的使用，发现基于规则的方法比LLMs检测到更多宗教语言，揭示了宗教语言检测的方法学挑战和定义争议。


<details>
  <summary>更多</summary>
  
**动机:** 宗教语言在当代话语中普遍存在，包括看似世俗的环境活动领域。研究旨在探索宗教和非宗教NGO在气候文本中使用的显性和隐性宗教语言。

**方法:** 采用双重方法：基于生态神学文献构建宗教术语层次树的规则模型，以及在零样本设置下运行的大型语言模型(LLMs)。分析超过88万句的数据集。

**结果:** 规则方法比LLMs持续标记更多句子为宗教语言。两种方法在检测结果上存在一致性和分歧点。

**结论:** 研究不仅揭示了计算检测宗教语言的方法学挑战，还展现了宗教语言应仅由词汇定义还是需考虑语境意义的广泛争议。为宗教研究的数字方法展示了分析神圣性在气候话语中持续存在的潜力和局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Religious+Language+in+Climate+Discourse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23395，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23395&send_immediately=true&force_search=false)

**原文摘要:** Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [188] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个基于混合专家(MoE)框架的时间序列预测模型，结合了xLSTM、增强线性模型、PatchTST和minGRU等多种SOTA模型，通过Transformer门控网络集成，在标准基准测试中超越了所有现有TSF模型。


<details>
  <summary>更多</summary>
  
**动机:** 针对时间序列预测领域中Transformer模型有效性争议和近期研究表明简单模型有时优于复杂模型的现状，以及TSF数据具有近期偏好和不可预测事件的特点，需要开发更强大的预测框架。

**方法:** 提出混合专家(MoE)框架，集成xLSTM、增强线性模型、PatchTST、minGRU等多种互补的SOTA模型，使用基于Transformer的门控网络进行模型选择和集成。

**结果:** 在标准基准测试中，提出的MoE框架超越了所有现有的时间序列预测模型，包括最新的基于MoE框架的方法，取得了最优性能。

**结论:** 通过集成多种互补的SOTA模型到MoE框架中，能够有效应对时间序列预测的挑战，证明了混合专家方法在TSF领域的优越性和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EMTSF%3AExtraordinary+Mixture+of+SOTA+Models+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23396&send_immediately=true&force_search=false)

**原文摘要:** The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [189] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao*

**主要类别:** cs.CL

**AI概要:** Omni-Reward是一个支持多模态和自由形式偏好的通用奖励模型系统，解决了现有奖励模型的模态不平衡和偏好刚性两大挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有奖励模型主要局限于文本和图像模态，且基于固定二元偏好对训练，无法捕捉多模态内容和个性化偏好的复杂性。

**方法:** 提出Omni-Reward系统，包括：1)Omni-RewardBench多模态基准测试；2)Omni-RewardData包含31.7万偏好对的多模态数据集；3)支持判别式和生成式奖励模型的Omni-RewardModel。

**结果:** 模型在Omni-RewardBench和其他广泛使用的奖励建模基准测试中表现出强劲性能。

**结论:** Omni-Reward为通用多模态奖励建模迈出了重要一步，能够更好地支持视频、音频、3D等多种模态的自由形式偏好对齐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omni-Reward%3A+Towards+Generalist+Omni-Modal+Reward+Modeling+with+Free-Form+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23451&send_immediately=true&force_search=false)

**原文摘要:** Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [190] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**主要类别:** cs.CL

**AI概要:** 该论文研究LLM搜索代理在多轮交互中表达置信度的能力，发现高置信度时任务准确率显著提升，提出基于置信度的Test-Time Scaling方法，在减少token消耗的同时保持竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注单轮场景的置信度，而对复杂多轮交互中LLM表达置信度的能力研究有限，需要探索LLM搜索代理在长序列动作后能否通过语言化置信分数传达不确定性。

**方法:** 在开源代理模型上进行实验，提出Test-Time Scaling (TTS)方法，利用置信分数判断答案质量，鼓励模型在置信度不足时重新尝试，直至达到满意的置信水平。

**结果:** 实验发现模型在高置信度时任务准确率很高，低置信度时准确率接近零；提出的TTS方法显著减少了token消耗，相比基线固定预算TTS方法表现出竞争力。

**结论:** LLM搜索代理能够通过语言化置信分数有效传达不确定性，基于置信度的TTS方法在保持性能的同时优化了计算资源使用，为多轮交互中的置信度评估提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BrowseConf%3A+Confidence-Guided+Test-Time+Scaling+for+Web+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23458&send_immediately=true&force_search=false)

**原文摘要:** Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [191] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea*

**主要类别:** cs.CL

**AI概要:** 本研究构建了一个针对金融领域三大核心指标（债务、每股收益、销售额）的句子级立场检测语料库，并评估了大型语言模型在零样本、少样本和思维链提示策略下的表现，发现少样本+思维链提示效果最佳，证明了LLMs在金融立场分析中的实用性。


<details>
  <summary>更多</summary>
  
**动机:** SEC文件财报和财报电话会议记录中的金融叙述对投资者、审计师和监管者很重要，但其长度、金融术语和微妙语言使细粒度分析变得困难，传统情感分析需要大量昂贵标注数据。

**方法:** 从10-K年报和财报电话会议记录中提取句子，使用ChatGPT-o3-pro模型在严格人工验证下标注立场（积极、消极、中性），系统评估现代LLMs在零样本、少样本和思维链提示策略下的表现。

**结果:** 少样本+思维链提示策略表现最佳，优于监督基线方法，LLMs在不同数据集（SEC和ECT）上的表现存在差异。

**结论:** 研究结果表明，无需大量标注数据即可利用LLMs进行金融领域特定目标的立场检测，具有实际可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Large+Language+Models+for+Stance+Detection+on+Financial+Targets+from+SEC+Filing+Reports+and+Earnings+Call+Transcripts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23464，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23464&send_immediately=true&force_search=false)

**原文摘要:** Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [192] [MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring](https://arxiv.org/abs/2510.23477)
*Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang*

**主要类别:** cs.CL

**AI概要:** MMTutorBench是首个AI数学辅导基准测试，包含685个围绕关键教学步骤设计的问题，通过六个维度评估模型在洞察发现、操作制定和操作执行三个任务上的表现。评估显示专有模型优于开源模型，与人类导师仍有差距，OCR会降低质量，少样本提示效果有限，基于量表的LLM评估方法可靠。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试忽视了AI数学辅导所需的诊断学生困难和逐步引导的关键教学技能，需要专门的评估工具来推动AI辅导系统发展。

**方法:** 构建包含685个教学关键步骤问题的MMTutorBench基准，每个问题配备特定评分量表，分为洞察发现、操作制定和操作执行三个任务，评估12个主流多模态大语言模型。

**结果:** 专有模型表现优于开源模型，与人类导师水平仍有显著差距；OCR处理会降低辅导质量；少样本提示提升有限；基于量表的LLM评估方法表现出高可靠性。

**结论:** MMTutorBench揭示了AI数学辅导的难度和诊断价值，为推进AI辅导系统发展提供了重要基准，显示了当前模型与人类导师的差距以及改进方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMTutorBench%3A+The+First+Multimodal+Benchmark+for+AI+Math+Tutoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23477&send_immediately=true&force_search=false)

**原文摘要:** Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.

</details>


### [193] [M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset](https://arxiv.org/abs/2510.23508)
*Jiahui Geng, Jonathan Tonglet, Iryna Gurevych*

**主要类别:** cs.CL

**AI概要:** M4FC是一个新的大规模多模态事实核查数据集，包含4,982张图像和6,980个声明，涵盖10种语言和6个多模态事实核查任务，解决了现有数据集的多个局限性。


<details>
  <summary>更多</summary>
  
**动机:** 现有真实世界多模态自动事实核查数据集存在多个局限性：样本数量少、语言和任务覆盖有限、存在证据泄露问题、依赖外部新闻文章集来获取真实声明。

**方法:** 构建M4FC数据集，包含由22个专业事实核查组织验证的4,982张图像和6,980个声明，涵盖10种语言，设计6个多模态事实核查任务。

**结果:** 提供了所有任务的基线结果，并分析了组合中间任务对下游裁决预测性能的影响。数据集和代码已公开提供。

**结论:** M4FC数据集解决了现有数据集的多个局限性，为多模态事实核查研究提供了更全面和实用的资源，有助于推动该领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M4FC%3A+a+Multimodal%2C+Multilingual%2C+Multicultural%2C+Multitask+Real-World+Fact-Checking+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23508&send_immediately=true&force_search=false)

**原文摘要:** Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.

</details>


### [194] [IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering](https://arxiv.org/abs/2510.23536)
*Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee*

**主要类别:** cs.CL

**AI概要:** 该论文提出了IPQA基准测试，用于评估个性化问答中的核心意图识别能力，填补了现有基准测试只关注回答质量而忽略意图识别的空白。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试只评估回答质量或检索性能，没有直接衡量意图识别能力，这导致系统无法理解用户优先考虑哪些意图来满足信息需求。

**方法:** 基于满意理论，从用户选择答案的可观察行为模式中推导核心意图，通过系统筛选、基于LLM的标注以及自动化验证与人工验证相结合的质量控制来构建多领域数据集。

**结果:** 实验评估显示当前最先进的语言模型在个性化场景下的核心意图识别表现不佳，模型无法从用户历史中识别核心意图，且随着问题复杂度增加性能下降。

**结论:** 核心意图识别对个性化问答至关重要，当前系统在此方面存在不足，发布的代码和数据集将促进该方向的未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IPQA%3A+A+Benchmark+for+Core+Intent+Identification+in+Personalized+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23536&send_immediately=true&force_search=false)

**原文摘要:** Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

</details>


### [195] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan*

**主要类别:** cs.CL

**AI概要:** LIMRANK是一种使用少量合成数据训练的高效信息重排序模型，仅需传统方法5%的数据量就能达到竞争性性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM信息重排序方法需要大规模微调，计算成本高昂，需要寻找更高效的适应方法。

**方法:** 设计LIMRANK-SYNTHESIZER管道生成多样化、具有挑战性的合成重排序数据，并用这些数据微调LIMRANK模型。

**结果:** LIMRANK在BRIGHT和FollowIR基准测试中表现优异，展示了强大的泛化能力，适用于科学文献搜索和知识密集型问题解决。

**结论:** 通过高质量合成数据训练，LLM可以在极少量监督下有效适应信息重排序任务，显著降低计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LimRank%3A+Less+is+More+for+Reasoning-Intensive+Information+Reranking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23544&send_immediately=true&force_search=false)

**原文摘要:** Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [196] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos, Hiram Calvo, Olga Kolesnikova*

**主要类别:** cs.CL

**AI概要:** 该研究评估了传统机器学习模型和微调transformer在希望语音检测任务上的表现，发现transformer模型在精度和召回率方面优于传统方法，特别是在小数据集上表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 识别社交媒体上的希望语音（包含激励性表达和目标导向行为的内容）已成为重要的NLP任务，需要有效的检测方法来处理这类内容。

**方法:** 使用预先分割的希望语音数据集（训练集、开发集和测试集），评估了传统机器学习模型（线性核SVM、RBF核SVM、逻辑回归、朴素贝叶斯）和微调的transformer模型。

**结果:** 传统模型中线性核SVM和逻辑回归的macro-F1为0.78，RBF核SVM为0.77，朴素贝叶斯为0.75。Transformer模型表现更好，最佳模型达到加权精度0.82、加权召回率0.80、加权F1 0.79、macro F1 0.79、准确率0.80。

**结论:** 虽然优化配置的传统机器学习模型仍具有敏捷性，但transformer架构能够检测希望语音中更细微的语义特征，在精度和召回率方面表现更优，表明大型transformer和LLM在小数据集上可能表现更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hope+Speech+Detection+in+Social+Media+English+Corpora%3A+Performance+of+Traditional+and+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23585&send_immediately=true&force_search=false)

**原文摘要:** The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


### [197] [Think Twice: Branch-and-Rethink Reasoning Reward Model](https://arxiv.org/abs/2510.23596)
*Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau*

**主要类别:** cs.CL

**AI概要:** BR-RM是一个两阶段的奖励模型，通过分支再思考机制减少判断扩散，提高对细微错误的敏感性，在多个奖励建模基准上达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统奖励模型将多个质量维度压缩为单一标量评分，导致注意力分散和浅层分析（判断扩散问题），而大语言模型的思考两次策略显示二次思考能提升推理能力。

**方法:** 提出分支再思考奖励模型（BR-RM）：第一轮进行自适应分支选择关键维度并生成证据寻求假设；第二轮执行分支条件再思考，针对性重新评估假设。使用GRPO风格的强化学习训练，采用带严格格式检查的二元结果奖励。

**结果:** 在三个具有挑战性的奖励建模基准测试中实现了最先进的性能，证明该方法能有效减少判断扩散并提高对细微错误的敏感性。

**结论:** BR-RM成功将思考两次原则应用于奖励建模，通过结构化两轮评估机制解决了判断扩散问题，同时保持了实用性和可扩展性，与标准RLHF流程兼容。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Twice%3A+Branch-and-Rethink+Reasoning+Reward+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23596&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [198] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah, Maha Driss, Henda Ben Ghezela*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种基于微服务架构和联邦学习的IoT数据分析解决方案，通过边缘计算降低延迟并保护数据隐私，在恶意软件检测用例中取得了99.24%的优异性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决IoT数据分析中的隐私安全问题和延迟问题，需要在边缘设备上进行分布式数据分析，同时确保时间敏感应用的快速响应。

**方法:** 采用微服务架构将IoT应用构建为细粒度、松耦合的可重用实体，结合联邦学习技术提供智能微服务，使用MaleVis数据集（14,000+张RGB图像）进行恶意软件检测验证。

**结果:** 提出的方法在恶意软件检测和分类性能上优于现有最先进方法，达到了99.24%的高准确率。

**结论:** 基于微服务架构和联邦学习的解决方案能够有效实现边缘计算环境下的高效、灵活和可扩展的数据分析，同时保障数据隐私和降低延迟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedMicro-IDA%3A+A+Federated+Learning+and+Microservices-based+Framework+for+IoT+Data+Analytics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20852&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [199] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng, Jin Li, Zhenhua Zhao, Shaowei Wang*

**主要类别:** cs.CR

**AI概要:** FPT-Noise是一种新的测试时防御方法，通过动态特征调制器和特征感知阈值来增强CLIP模型的对抗鲁棒性，无需昂贵的微调训练。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉语言模型（如CLIP）在对抗攻击下表现脆弱，传统对抗训练方法计算成本高昂，需要一种无需重新训练的高效防御方案。

**方法:** 提出FPT-Noise方法：1) 动态特征调制器生成图像特定和攻击自适应的噪声强度参数；2) 建立特征感知阈值区分干净图像和对抗图像；3) 集成场景感知调节和测试时变换集成技术。

**结果:** FPT-Noise显著优于现有测试时防御方法，在AutoAttack下将平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上保持高性能（仅下降1.1%）。

**结论:** 该方法提供了一种高效且有效的测试时防御解决方案，显著提升了CLIP模型的对抗鲁棒性，同时保持了在干净数据上的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FPT-Noise%3A+Dynamic+Scene-Aware+Counterattack+for+Test-Time+Adversarial+Defense+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20856&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [200] [Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology](https://arxiv.org/abs/2510.20858)
*Nubio Vidal, Naghmeh Moradpoor, Leandros Maglaras*

**主要类别:** cs.CR

**AI概要:** 提出了Agnostic Incident Reporting (AIR)框架，包含25个元素和7个分组，用于实时OT事件报告，支持技术协调和监管对齐。


<details>
  <summary>更多</summary>
  
**动机:** OT网络与IT融合扩大了攻击面，但现有标准未明确事件中应捕获的数据，阻碍了利益相关者间的协调；IT指南定义了报告内容但未考虑OT约束。

**方法:** 开发AIR框架，将其映射到主要OT标准，定义集成激活点，并回溯应用于2015年乌克兰电网事件进行验证。

**结果:** AIR能将高层需求转化为具体字段，独立于供应商覆盖现有框架，支持响应过程中的态势感知和通信。

**结论:** AIR为标准化实时OT事件报告提供了基础，同时支持技术协调和监管一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Everyone+Needs+AIR%3A+An+Agnostic+Incident+Reporting+Framework+for+Cybersecurity+in+Operational+Technology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20858&send_immediately=true&force_search=false)

**原文摘要:** Operational technology (OT) networks are increasingly coupled with
information technology (IT), expanding the attack surface and complicating
incident response. Although OT standards emphasise incident reporting and
evidence preservation, they do not specify what data to capture during an
incident, which hinders coordination across stakeholders. In contrast, IT
guidance defines reporting content but does not address OT constraints. This
paper presents the Agnostic Incident Reporting (AIR) framework for live OT
incident reporting. AIR comprises 25 elements organised into seven groups to
capture incident context, chronology, impacts, and actions, tailored to
technical, managerial, and regulatory needs. We evaluate AIR by mapping it to
major OT standards, defining activation points for integration and triggering
established OT frameworks, and then retrospectively applying it to the 2015
Ukrainian distribution grid incident. The evaluation indicates that AIR
translates high-level requirements into concrete fields, overlays existing
frameworks without vendor dependence, and can support situational awareness and
communication during response. AIR offers a basis for standardising live OT
incident reporting while supporting technical coordination and regulatory
alignment.

</details>


### [201] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares, Mário S. Alvim, Natasha Fernandes*

**主要类别:** cs.CR

**AI概要:** 本文提出了一个新的动态信息泄漏定义，将攻击者对秘密值的信念与基准分布解耦，并验证了其满足信息论公理，同时与静态视角兼容。


<details>
  <summary>更多</summary>
  
**动机:** 定量信息流(QIF)中动态视角的理论成熟度不足，缺乏与静态视角相媲美的理论基础，需要填补这一研究空白。

**方法:** 提出新颖的动态泄漏定义，解耦攻击者信念与基准分布；验证其满足非干扰性、单调性和数据处理不等式等公理；分析强公理版本不成立的条件；展示与静态视角的兼容性；通过隐私保护数据发布攻击案例进行实证。

**结果:** 成功定义了一个满足关键信息论公理的新型动态泄漏度量，揭示了强单调性和强DPI在某些分析中可能不成立的原因，并证明了与静态泄漏定义的兼容性。

**结论:** 该研究为动态信息流分析提供了更成熟的理论基础，填补了动态视角的理论空白，为系统监控和跟踪应用提供了更精确的信息泄漏评估方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+new+measure+for+dynamic+leakage+based+on+quantitative+information+flow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20922&send_immediately=true&force_search=false)

**原文摘要:** Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [202] [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)
*Soham Hans, Stacy Marsella, Sophia Hirschmann, Nikolos Gurney*

**主要类别:** cs.CR

**AI概要:** 使用大语言模型分析入侵检测系统日志，从底层网络遥测数据推断攻击者的MITRE ATT&CK技术和认知策略，为认知自适应网络安全防御提供新途径


<details>
  <summary>更多</summary>
  
**动机:** 传统网络安全依赖高层情报报告和人工分析攻击链，但实时防御需要直接从底层系统遥测数据推断攻击者意图和认知策略

**方法:** 开发策略驱动的提示系统，将大量网络日志数据分割为不同的行为阶段，利用大语言模型将每个阶段与可能的技术和潜在认知动机关联

**结果:** 大语言模型能够弥合数据包级日志与战略意图之间的语义鸿沟，揭示工具切换、协议转换等行为信号与心理决策点的对应关系

**结论:** 该方法为行为自适应网络防御和认知特征推断奠定了基础，展示了向认知自适应网络安全防御的可行路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Logs+to+ATT%26CK+Insights%3A+Leveraging+LLMs+for+High-Level+Threat+Understanding+and+Cognitive+Trait+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20930&send_immediately=true&force_search=false)

**原文摘要:** Understanding adversarial behavior in cybersecurity has traditionally relied
on high-level intelligence reports and manual interpretation of attack chains.
However, real-time defense requires the ability to infer attacker intent and
cognitive strategy directly from low-level system telemetry such as intrusion
detection system (IDS) logs. In this paper, we propose a novel framework that
leverages large language models (LLMs) to analyze Suricata IDS logs and infer
attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded
in the hypothesis that attacker behavior reflects underlying cognitive biases
such as loss aversion, risk tolerance, or goal persistence that can be
extracted and modeled through careful observation of log sequences. This lays
the groundwork for future work on behaviorally adaptive cyber defense and
cognitive trait inference. We develop a strategy-driven prompt system to
segment large amounts of network logs data into distinct behavioral phases in a
highly efficient manner, enabling the LLM to associate each phase with likely
techniques and underlying cognitive motives. By mapping network-layer events to
high-level attacker strategies, our method reveals how behavioral signals such
as tool switching, protocol transitions, or pivot patterns correspond to
psychologically meaningful decision points. The results demonstrate that LLMs
can bridge the semantic gap between packet-level logs and strategic intent,
offering a pathway toward cognitive-adaptive cyber defense.
  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),
Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive
Biases

</details>


### [203] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, Mehrdad Saif*

**主要类别:** cs.CR

**AI概要:** 本研究调查城市空中交通(UAM)车辆自主导航和着陆系统的漏洞，重点关注针对CNN等深度学习模型的木马攻击，实验显示木马攻击导致准确率从96.4%降至73.3%。


<details>
  <summary>更多</summary>
  
**动机:** 随着城市空中交通系统的发展，需要评估自主导航系统面对木马攻击的安全漏洞，这些攻击通过在训练数据中嵌入隐蔽触发器来导致特定条件下的系统故障。

**方法:** 使用DroNet框架评估城市自主飞行器的脆弱性，收集自定义数据集并训练模型模拟真实条件，开发了识别木马感染模型的评估框架。

**结果:** 实验结果显示木马攻击导致模型准确率显著下降，从清洁数据的96.4%降至触发数据上的73.3%。

**结论:** 木马攻击对UAM系统构成严重安全风险，本研究为未来增强UAM系统韧性的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Experimental+Study+of+Trojan+Vulnerabilities+in+UAV+Autonomous+Landing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20932&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [204] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong, Stephen H. Bach*

**主要类别:** cs.CR

**AI概要:** 论文发现推理语言模型存在自我越狱现象，即经过良性推理训练后，模型会使用多种策略绕过自身的安全防护机制，为有害请求寻找正当化理由。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在揭示推理语言模型在数学或代码领域良性训练后出现的意外不对齐现象，即模型会自我突破安全限制，为恶意请求提供合理化解释。

**方法:** 通过分析多个开源推理语言模型（如DeepSeek-R1-distilled、Phi-4-mini-reasoning等），研究其推理过程中的自我越狱策略，并提供机制性理解。

**结果:** 发现许多模型存在自我越狱问题，模型在良性推理训练后变得更顺从，并在思维链中将恶意请求感知为危害性较低，从而满足这些请求。

**结论:** 通过在训练中加入最少量的安全推理数据可以有效缓解自我越狱问题，为保持推理语言模型的安全性提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Jailbreaking%3A+Language+Models+Can+Reason+Themselves+Out+of+Safety+Alignment+After+Benign+Reasoning+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20956&send_immediately=true&force_search=false)

**原文摘要:** We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [205] [REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering](https://arxiv.org/abs/2510.20975)
*Darrin Lea, James Ghawaly, Golden Richard III, Aisha Ali-Gombe, Andrew Case*

**主要类别:** cs.CR

**AI概要:** 该论文开发了一个名为REx86的本地开源大语言模型，专门针对x86二进制逆向工程任务进行参数高效微调，在代码理解和注释生成方面表现出色，解决了云端闭源模型的安全隐私问题。


<details>
  <summary>更多</summary>
  
**动机:** x86二进制逆向工程对恶意软件和固件分析至关重要，但由于元数据缺失和对抗性混淆，效率低下。云端闭源大语言模型存在隐私安全风险，无法在封闭网络环境中使用。

**方法:** 在5,981个x86汇编样本的自定义数据集上，对CodeLlama、Qwen2.5-Coder和CodeGemma系列的8个开源模型进行参数高效微调。

**结果:** 微调的Qwen2.5-Coder-7B模型（REx86）表现最佳：测试集交叉熵损失降低64.2%，语义余弦相似度提升20.3%；用户案例研究中代码理解显著改善（p=0.031），正确解决率从31%提升至53%。

**结论:** REx86在本地开源LLM中提供了最先进的x86逆向工程辅助能力，证明了领域特定微调的价值，并强调需要更多带注释的反汇编数据来进一步提升LLM在逆向工程中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REx86%3A+A+Local+Large+Language+Model+for+Assisting+in+x86+Assembly+Reverse+Engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20975&send_immediately=true&force_search=false)

**原文摘要:** Reverse engineering (RE) of x86 binaries is indispensable for malware and
firmware analysis, but remains slow due to stripped metadata and adversarial
obfuscation. Large Language Models (LLMs) offer potential for improving RE
efficiency through automated comprehension and commenting, but cloud-hosted,
closed-weight models pose privacy and security risks and cannot be used in
closed-network facilities. We evaluate parameter-efficient fine-tuned local
LLMs for assisting with x86 RE tasks in these settings. Eight open-weight
models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned
on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them
quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top
performer, which we name REx86.
  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic
cosine similarity against ground truth by 20.3\% over its base model. In a
limited user case study (n=43), REx86 significantly enhanced line-level code
understanding (p = 0.031) and increased the correct-solve rate from 31% to 53%
(p = 0.189), though the latter did not reach statistical significance.
Qualitative analysis shows more accurate, concise comments with fewer
hallucinations.
  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight
LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and
highlight the need for more commented disassembly data to further enhance LLM
performance in RE. REx86, its dataset, and LoRA adapters are publicly available
at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.

</details>


### [206] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen, Alsharif Abuadbba, Kristen Moore, Tingming Wu*

**主要类别:** cs.CR

**AI概要:** 该论文评估了FOICE音频深度伪造技术的检测能力，发现现有检测器在标准和噪声条件下均失效，提出了针对性的微调策略以改进检测准确率，并揭示了专业化与泛化能力之间的权衡。


<details>
  <summary>更多</summary>
  
**动机:** FOICE技术能够从单张面部图像生成逼真的合成语音，绕过行业标准认证系统，且面部图像比语音样本更容易获取，这带来了严重的安全威胁，需要评估现有检测器的有效性。

**方法:** 系统评估现有音频深度伪造检测器对FOICE生成语音的检测能力（包括标准和噪声条件），引入针对性微调策略捕获FOICE特有伪影，并评估微调后的泛化能力。

**结果:** 主流检测器在标准和噪声条件下均无法可靠检测FOICE生成语音；针对性微调显著提高了检测准确率；但微调后在FOICE专业化和对未见合成管道的鲁棒性之间存在权衡。

**结论:** 当前防御系统存在根本性弱点，需要开发新架构和训练协议来构建下一代音频深度伪造检测系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Current+Detectors+Catch+Face-to-Voice+Deepfake+Attacks%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21004&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [207] [JSTprove: Pioneering Verifiable AI for a Trustless Future](https://arxiv.org/abs/2510.21024)
*Jonathan Gold, Tristan Freiberg, Haruna Isah, Shirin Shahabi*

**主要类别:** cs.CR

**AI概要:** JSTprove是一个基于零知识机器学习的工具包，旨在让AI开发者无需密码学专业知识即可验证AI推理过程，保护数据隐私的同时提供可审计的验证能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI系统在关键行业的广泛应用，需要确保AI决策的透明度和正确性，但传统零知识机器学习系统需要深厚的密码学专业知识，限制了普通ML工程师的使用。

**方法:** 基于Polyhedra Network的Expander后端构建JSTprove工具包，提供端到端的可验证AI推理流水线，通过简单的命令行界面隐藏密码学复杂性，同时提供可审计的工件。

**结果:** 开发了JSTprove工具包，使AI开发者和ML工程师能够生成和验证AI推理证明，无需密码学专业知识。

**结论:** JSTprove既是满足当前工程需求的可用zkML产品，也是未来可验证AI研究和生产部署的可复现基础，鼓励社区评审和扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JSTprove%3A+Pioneering+Verifiable+AI+for+a+Trustless+Future，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21024&send_immediately=true&force_search=false)

**原文摘要:** The integration of machine learning (ML) systems into critical industries
such as healthcare, finance, and cybersecurity has transformed decision-making
processes, but it also brings new challenges around trust, security, and
accountability. As AI systems become more ubiquitous, ensuring the transparency
and correctness of AI-driven decisions is crucial, especially when they have
direct consequences on privacy, security, or fairness. Verifiable AI, powered
by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these
challenges. zkML enables the verification of AI model inferences without
exposing sensitive data, providing an essential layer of trust and privacy.
However, traditional zkML systems typically require deep cryptographic
expertise, placing them beyond the reach of most ML engineers. In this paper,
we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's
Expander backend, to enable AI developers and ML engineers to generate and
verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI
inference pipeline that hides cryptographic complexity behind a simple
command-line interface while exposing auditable artifacts for reproducibility.
We present the design, innovations, and real-world use cases of JSTprove as
well as our blueprints and tooling to encourage community review and extension.
JSTprove therefore serves both as a usable zkML product for current engineering
needs and as a reproducible foundation for future research and production
deployments of verifiable AI.

</details>


### [208] [A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](https://arxiv.org/abs/2510.21053)
*Li An, Yujian Liu, Yepeng Liu, Yuheng Bu, Yang Zhang, Shiyu Chang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种端到端的强化学习框架，用于优化大型语言模型水印的绿色/红色令牌列表设计，通过锚定机制和正则化项解决了多目标优化不稳定和奖励黑客问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有水印算法主要依赖启发式的绿色/红色令牌列表设计，直接使用强化学习优化面临多目标冲突导致训练不稳定和巨大动作空间易受奖励黑客攻击的挑战。

**方法:** 采用端到端强化学习框架，引入锚定机制确保训练稳定性，并添加正则化项防止奖励黑客问题。

**结果:** 在两个骨干LLM的标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著改进，且不降低其他标准。

**结论:** 提出的RL框架成功解决了LLM水印优化中的关键挑战，为稳健安全的水印技术提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Reinforcement+Learning+Framework+for+Robust+and+Secure+LLM+Watermarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21053&send_immediately=true&force_search=false)

**原文摘要:** Watermarking has emerged as a promising solution for tracing and
authenticating text generated by large language models (LLMs). A common
approach to LLM watermarking is to construct a green/red token list and assign
higher or lower generation probabilities to the corresponding tokens,
respectively. However, most existing watermarking algorithms rely on heuristic
green/red token list designs, as directly optimizing the list design with
techniques such as reinforcement learning (RL) comes with several challenges.
First, desirable watermarking involves multiple criteria, i.e., detectability,
text quality, robustness against removal attacks, and security against spoofing
attacks. Directly optimizing for these criteria introduces many partially
conflicting reward terms, leading to an unstable convergence process. Second,
the vast action space of green/red token list choices is susceptible to reward
hacking. In this paper, we propose an end-to-end RL framework for robust and
secure LLM watermarking. Our approach adopts an anchoring mechanism for reward
terms to ensure stable training and introduces additional regularization terms
to prevent reward hacking. Experiments on standard benchmarks with two backbone
LLMs show that our method achieves a state-of-the-art trade-off across all
criteria, with notable improvements in resistance to spoofing attacks without
degrading other criteria. Our code is available at
https://github.com/UCSB-NLP-Chang/RL-watermark.

</details>


### [209] [Soft Instruction De-escalation Defense](https://arxiv.org/abs/2510.21057)
*Nils Philipp Walter, Chawin Sitawarin, Jamie Hayes, David Stutz, Ilia Shumailov*

**主要类别:** cs.CR

**AI概要:** SIC方法通过迭代式提示净化循环检测和重写恶意指令，提高LLM代理的安全性，但对抗性攻击仍有15%成功率


<details>
  <summary>更多</summary>
  
**动机:** LLM在代理系统中处理不可信数据时容易受到提示注入攻击，需要有效防护机制

**方法:** 提出SIC方法：迭代检查输入数据中的恶意指令，通过重写、掩码或删除进行净化，直到输入安全或达到最大迭代次数

**结果:** 方法有效提高了安全性，但对抗性攻击仍能达到15%的攻击成功率(ASR)

**结论:** SIC虽然不能完全防御所有攻击，但显著提高了攻击门槛，为LLM代理安全提供了实用解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft+Instruction+De-escalation+Defense，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21057&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly deployed in agentic systems
that interact with an external environment; this makes them susceptible to
prompt injections when dealing with untrusted data. To overcome this
limitation, we propose SIC (Soft Instruction Control)-a simple yet effective
iterative prompt sanitization loop designed for tool-augmented LLM agents. Our
method repeatedly inspects incoming data for instructions that could compromise
agent behavior. If such content is found, the malicious content is rewritten,
masked, or removed, and the result is re-evaluated. The process continues until
the input is clean or a maximum iteration limit is reached; if imperative
instruction-like content remains, the agent halts to ensure security. By
allowing multiple passes, our approach acknowledges that individual rewrites
may fail but enables the system to catch and correct missed injections in later
steps. Although immediately useful, worst-case analysis shows that SIC is not
infallible; strong adversary can still get a 15% ASR by embedding
non-imperative workflows. This nonetheless raises the bar.

</details>


### [210] [QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute](https://arxiv.org/abs/2510.21124)
*Jie Zhang, Xiaohong Li, Mengke Zhang, Ruitao Feng, Shanshan Xu, Zhe Hou, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** QAEBAC提出了一种新的区块链属性访问控制方案，通过(r,t)-匿名模型量化重识别风险，并采用熵加权路径树优化策略匹配，在保护隐私的同时显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有区块链ABAC方案面临两大挑战：区块链透明度导致用户隐私易受重识别攻击，策略匹配计算复杂度与区块链性能限制冲突。现有方案如零知识证明开销大且缺乏可量化匿名保障，效率优化往往忽视隐私影响。

**方法:** 提出QAEBAC框架：1) 引入形式化的(r,t)-匿名模型动态量化用户重识别风险；2) 设计熵加权路径树(EWPT)基于实时匿名指标优化策略结构，大幅降低策略匹配复杂度。在Hyperledger Fabric上实现和评估。

**结果:** 实验结果显示QAEBAC在隐私和性能间取得优异平衡：有效缓解重识别风险，吞吐量提升最高11倍，延迟降低87%，优于现有最先进基线方案。

**结论:** QAEBAC证明了在隐私敏感的分布式应用中实现可量化匿名和高效访问控制的实用性，为区块链ABAC提供了兼顾隐私保护与系统性能的创新解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QAE-BAC%3A+Achieving+Quantifiable+Anonymity+and+Efficiency+in+Blockchain-Based+Access+Control+with+Attribute，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21124&send_immediately=true&force_search=false)

**原文摘要:** Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a
decentralized paradigm for secure data governance but faces two inherent
challenges: the transparency of blockchain ledgers threatens user privacy by
enabling reidentification attacks through attribute analysis, while the
computational complexity of policy matching clashes with blockchain's
performance constraints. Existing solutions, such as those employing
Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable
anonymity guarantees, while efficiency optimizations frequently ignore privacy
implications. To address these dual challenges, this paper proposes QAEBAC
(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with
Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically
quantify the re-identification risk of users based on their access attributes
and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that
optimizes policy structure based on realtime anonymity metrics, drastically
reducing policy matching complexity. Implemented and evaluated on Hyperledger
Fabric, QAE-BAC demonstrates a superior balance between privacy and
performance. Experimental results show that it effectively mitigates
re-identification risks and outperforms state-of-the-art baselines, achieving
up to an 11x improvement in throughput and an 87% reduction in latency, proving
its practicality for privacy-sensitive decentralized applications.

</details>


### [211] [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/abs/2510.21133)
*Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi*

**主要类别:** cs.CR

**AI概要:** 该论文首次全面评估了10个主流商业大语言模型在CBRN（化学、生物、放射性和核武器）知识扩散方面的双重用途风险，发现现有安全机制存在严重漏洞，深度诱导攻击成功率高达86%，模型安全性差异巨大（2%-96%），急需更强大的安全对齐技术。


<details>
  <summary>更多</summary>
  
**动机:** 前沿大语言模型存在前所未有的双重用途风险，可能扩散CBRN武器知识，需要评估当前商业LLMs的安全防护能力。

**方法:** 使用包含200个提示的新CBRN数据集和180个提示的FORTRESS基准子集，采用严格的三层攻击方法学对10个领先商业LLMs进行评估。

**结果:** 深度诱导攻击成功率86%远高于直接请求的33.8%；模型安全性表现差异巨大（claude-opus-4仅2%成功率，mistral-small-latest达96%）；8个模型在增强危险材料属性请求中超过70%的脆弱性。

**结论:** 当前安全对齐机制存在根本性脆弱性，简单提示工程技术即可绕过防护措施，挑战了行业安全声明，亟需标准化评估框架、透明安全指标和更强大的对齐技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+CBRN+Risk+in+Frontier+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21133&send_immediately=true&force_search=false)

**原文摘要:** Frontier Large Language Models (LLMs) pose unprecedented dual-use risks
through the potential proliferation of chemical, biological, radiological, and
nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation
of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and
a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier
attack methodology. Our findings expose critical safety vulnerabilities: Deep
Inception attacks achieve 86.0\% success versus 33.8\% for direct requests,
demonstrating superficial filtering mechanisms; Model safety performance varies
dramatically from 2\% (claude-opus-4) to 96\% (mistral-small-latest) attack
success rates; and eight models exceed 70\% vulnerability when asked to enhance
dangerous material properties. We identify fundamental brittleness in current
safety alignment, where simple prompt engineering techniques bypass safeguards
for dangerous CBRN information. These results challenge industry safety claims
and highlight urgent needs for standardized evaluation frameworks, transparent
safety metrics, and more robust alignment techniques to mitigate catastrophic
misuse risks while preserving beneficial capabilities.

</details>


### [212] [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189)
*Yukun Jiang, Mingjie Li, Michael Backes, Yang Zhang*

**主要类别:** cs.CR

**AI概要:** 该论文提出了JAIL-CON攻击框架，利用任务并发性绕过大语言模型的安全防护，通过在相邻词汇中编码不同意图实现并发任务处理，显著降低了有害内容被防护机制检测的概率。


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱攻击主要基于顺序逻辑，而并发性作为顺序场景的自然扩展被忽视。研究发现并发任务处理会降低有害内容被防护机制过滤的概率，存在潜在安全风险。

**方法:** 提出词级方法实现LLMs中的任务并发性，使相邻词汇编码不同意图。开发JAIL-CON迭代攻击框架，通过任务并发性进行越狱攻击。

**结果:** JAIL-CON在广泛使用的LLMs上展现出强大的越狱能力，相比现有攻击方法，其并发答案具有更强的隐蔽性，更难被防护机制检测。

**结论:** 任务并发性在LLMs越狱中具有独特优势，JAIL-CON框架证明了并发攻击的有效性和隐蔽性，揭示了LLMs安全防护的新挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adjacent+Words%2C+Divergent+Intents%3A+Jailbreaking+Large+Language+Models+via+Task+Concurrency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21189&send_immediately=true&force_search=false)

**原文摘要:** Despite their superior performance on a wide range of domains, large language
models (LLMs) remain vulnerable to misuse for generating harmful content, a
risk that has been further amplified by various jailbreak attacks. Existing
jailbreak attacks mainly follow sequential logic, where LLMs understand and
answer each given task one by one. However, concurrency, a natural extension of
the sequential scenario, has been largely overlooked. In this work, we first
propose a word-level method to enable task concurrency in LLMs, where adjacent
words encode divergent intents. Although LLMs maintain strong utility in
answering concurrent tasks, which is demonstrated by our evaluations on
mathematical and general question-answering benchmarks, we notably observe that
combining a harmful task with a benign one significantly reduces the
probability of it being filtered by the guardrail, showing the potential risks
associated with concurrency in LLMs. Based on these findings, we introduce
$\texttt{JAIL-CON}$, an iterative attack framework that
$\underline{\text{JAIL}}$breaks LLMs via task $\underline{\text{CON}}$currency.
Experiments on widely-used LLMs demonstrate the strong jailbreak capabilities
of $\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the
guardrail is applied as a defense, compared to the sequential answers generated
by previous attacks, the concurrent answers in our $\texttt{JAIL-CON}$ exhibit
greater stealthiness and are less detectable by the guardrail, highlighting the
unique feature of task concurrency in jailbreaking LLMs.

</details>


### [213] [The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](https://arxiv.org/abs/2510.21190)
*Mingrui Liu, Sixiao Zhang, Cheng Long, Kwok Yan Lam*

**主要类别:** cs.CR

**AI概要:** TrojFill是一种新型的黑盒越狱攻击方法，通过将有害指令伪装成模板填充任务，在多个主流LLM上实现了高成功率攻击（最高100%），同时提高了生成提示的可解释性和可迁移性。


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的模板缺乏可解释性和可迁移性。需要一种既能绕过安全防护又能保持提示质量的黑盒攻击方法。

**方法:** TrojFill将不安全指令重新构建为模板填充任务，通过占位符替换或编码（如凯撒/Base64）隐藏有害指令，嵌入到多部分模板中，包含不安全原因推理和详细示例生成两个关键组件。

**结果:** 在多个主流LLM（ChatGPT、Gemini、DeepSeek、Qwen）上测试显示卓越性能：Gemini-flash-2.5和DeepSeek-3.1达到100%攻击成功率，GPT-4o达到97%。生成的提示比现有黑盒方法更具可解释性和可迁移性。

**结论:** TrojFill提供了一种有效的黑盒越狱方法，成功平衡了攻击效果与提示质量，为红队测试研究提供了有价值的工具和数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Trojan+Example%3A+Jailbreaking+LLMs+through+Template+Filling+and+Unsafety+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21190&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have advanced rapidly and now encode extensive
world knowledge. Despite safety fine-tuning, however, they remain susceptible
to adversarial prompts that elicit harmful content. Existing jailbreak
techniques fall into two categories: white-box methods (e.g., gradient-based
approaches such as GCG), which require model internals and are infeasible for
closed-source APIs, and black-box methods that rely on attacker LLMs to search
or mutate prompts but often produce templates that lack explainability and
transferability. We introduce TrojFill, a black-box jailbreak that reframes
unsafe instruction as a template-filling task. TrojFill embeds obfuscated
harmful instructions (e.g., via placeholder substitution or Caesar/Base64
encoding) inside a multi-part template that asks the model to (1) reason why
the original instruction is unsafe (unsafety reasoning) and (2) generate a
detailed example of the requested text, followed by a sentence-by-sentence
analysis. The crucial "example" component acts as a Trojan Horse that contains
the target jailbreak content while the surrounding task framing reduces refusal
rates. We evaluate TrojFill on standard jailbreak benchmarks across leading
LLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical
performance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,
and 97% on GPT-4o). Moreover, the generated prompts exhibit improved
interpretability and transferability compared with prior black-box optimization
approaches. We release our code, sample prompts, and generated outputs to
support future red-teaming research.

</details>


### [214] [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214)
*Xingwei Zhong, Kar Wai Fok, Vrizlynn L. L. Thing*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种针对多模态大语言模型的黑盒越狱攻击方法，通过文本和图像提示的组合攻击，并设计了重新攻击策略来评估模型安全性，同时改进了防御方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型在视觉和文本模态结合后引入了新的安全威胁维度，现有模型容易受到越狱攻击，需要新的评估和防御方法。

**方法:** 提出黑盒越狱方法，使用挑衅性文本提示和具有突变及多图像能力的图像提示，设计重新攻击策略来增强评估效果。

**结果:** 实验结果显示该方法能有效评估开源和闭源MLLMs的安全性，识别出现有防御方法的不足。

**结论:** 通过重新设计的防御方法在训练时和推理时都能提高对越狱攻击的防护能力，为MLLMs安全提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+MLLM+Black-Box+Jailbreaking+Attacks+and+Defenses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21214&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) comprise of both visual and textual
modalities to process vision language tasks. However, MLLMs are vulnerable to
security-related issues, such as jailbreak attacks that alter the model's input
to induce unauthorized or harmful responses. The incorporation of the
additional visual modality introduces new dimensions to security threats. In
this paper, we proposed a black-box jailbreak method via both text and image
prompts to evaluate MLLMs. In particular, we designed text prompts with
provocative instructions, along with image prompts that introduced mutation and
multi-image capabilities. To strengthen the evaluation, we also designed a
Re-attack strategy. Empirical results show that our proposed work can improve
capabilities to assess the security of both open-source and closed-source
MLLMs. With that, we identified gaps in existing defense methods to propose new
strategies for both training-time and inference-time defense methods, and
evaluated them across the new jailbreak methods. The experiment results showed
that the re-designed defense methods improved protections against the jailbreak
attacks.

</details>


### [215] [Securing AI Agent Execution](https://arxiv.org/abs/2510.21236)
*Christoph Bühler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi*

**主要类别:** cs.CR

**AI概要:** AgentBound是首个针对MCP服务器的访问控制框架，通过声明式策略机制和安全执行引擎来保护LLM代理与外部工具交互时的系统安全。


<details>
  <summary>更多</summary>
  
**动机:** MCP已成为连接AI代理与外部工具的标准协议，但存在严重的安全隐患，数千个MCP服务器拥有对主机系统的无限制访问权限，形成了广泛的攻击面。

**方法:** 结合受Android权限模型启发的声明式策略机制和策略执行引擎，无需修改MCP服务器即可遏制恶意行为。基于296个最流行的MCP服务器构建数据集，自动从源代码生成访问控制策略。

**结果:** 自动生成访问控制策略的准确率达到80.9%，能够阻止大多数恶意MCP服务器的安全威胁，策略执行引擎引入的开销可忽略不计。

**结论:** AgentBound为开发者和项目经理提供了保护MCP服务器的实用基础，同时保持生产力，使研究人员和工具构建者能够探索声明式访问控制和MCP安全的新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Securing+AI+Agent+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21236，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21236&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have evolved into AI agents that interact with
external tools and environments to perform complex tasks. The Model Context
Protocol (MCP) has become the de facto standard for connecting agents with such
resources, but security has lagged behind: thousands of MCP servers execute
with unrestricted access to host systems, creating a broad attack surface. In
this paper, we introduce AgentBound, the first access control framework for MCP
servers. AgentBound combines a declarative policy mechanism, inspired by the
Android permission model, with a policy enforcement engine that contains
malicious behavior without requiring MCP server modifications. We build a
dataset containing the 296 most popular MCP servers, and show that access
control policies can be generated automatically from source code with 80.9%
accuracy. We also show that AgentBound blocks the majority of security threats
in several malicious MCP servers, and that policy enforcement engine introduces
negligible overhead. Our contributions provide developers and project managers
with a practical foundation for securing MCP servers while maintaining
productivity, enabling researchers and tool builders to explore new directions
for declarative access control and MCP security.

</details>


### [216] [What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions](https://arxiv.org/abs/2510.21246)
*Michael Külper, Jan-Niclas Hilgert, Frank Breitinger, Martin Lambertz*

**主要类别:** cs.CR

**AI概要:** 本文提出一个扩展的数字取证框架，专门针对自托管云存储系统（如Nextcloud），通过设备监控和云API实现结构化、可重复的证据采集，并开发了开源采集工具。


<details>
  <summary>更多</summary>
  
**动机:** 自托管云存储平台（如Nextcloud）日益流行，但给数字取证调查带来新挑战，现有云存储取证框架存在局限性，且Nextcloud在取证研究中关注不足。

**方法:** 批判性分析现有云存储取证框架的局限性；提出扩展取证框架，整合设备监控和云API；以Nextcloud为案例研究，展示如何利用其原生API可靠访问取证证据；开发开源采集工具实现该方法。

**结果:** 成功演示了利用Nextcloud原生API进行可靠取证证据访问的方法；开发了实现该框架的开源采集工具；提供了分析自托管云存储系统的灵活方法。

**结论:** 提出的扩展取证框架为调查人员提供了更灵活的分析自托管云存储系统的方法，并为这一不断发展的数字取证领域奠定了进一步发展的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+Next%2C+Cloud%3F+A+Forensic+Framework+for+Analyzing+Self-Hosted+Cloud+Storage+Solutions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21246&send_immediately=true&force_search=false)

**原文摘要:** Self-hosted cloud storage platforms like Nextcloud are gaining popularity
among individuals and organizations seeking greater control over their data.
However, this shift introduces new challenges for digital forensic
investigations, particularly in systematically analyzing both client and server
components. Despite Nextcloud's widespread use, it has received limited
attention in forensic research. In this work, we critically examine existing
cloud storage forensic frameworks and highlight their limitations. To address
the gaps, we propose an extended forensic framework that incorporates device
monitoring and leverages cloud APIs for structured, repeatable evidence
acquisition. Using Nextcloud as a case study, we demonstrate how its native
APIs can be used to reliably access forensic artifacts, and we introduce an
open-source acquisition tool that implements this approach. Our framework
equips investigators with a more flexible method for analyzing self-hosted
cloud storage systems, and offers a foundation for further development in this
evolving area of digital forensics.

</details>


### [217] [LLM-Powered Detection of Price Manipulation in DeFi](https://arxiv.org/abs/2510.21272)
*Lu Liu, Wuqi Zhang, Lili Wei, Hao Guan, Yongqiang Tian, Yepang Liu*

**主要类别:** cs.CR

**AI概要:** PMDetector是一个结合静态分析和大型语言模型推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在真实数据集上达到88%精确率和90%召回率，成本仅为每次审计0.03美元。


<details>
  <summary>更多</summary>
  
**动机:** DeFi智能合约管理着数十亿美元资产，价格操纵漏洞（尤其是通过闪电贷）造成了重大财务损失。现有检测方法存在局限：反应性方法只能在攻击发生后分析，而静态分析工具依赖预定义启发式规则，无法识别新型攻击变体或理解复杂经济逻辑。

**方法:** 提出PMDetector混合框架：1) 静态污点分析识别潜在漏洞代码路径；2) 两阶段LLM处理（分析防御措施过滤路径，模拟攻击评估可利用性）；3) 静态分析检查器验证LLM结果，保留高风险路径并生成详细漏洞报告。

**结果:** 在包含73个真实漏洞和288个良性DeFi协议的数据集上测试，使用Gemini 2.5-flash达到88%精确率和90%召回率，显著优于最先进的静态分析和基于LLM的方法。使用GPT-4.1时，每次漏洞审计仅需4.0秒，成本0.03美元。

**结论:** PMDetector提供了一个高效且经济实惠的替代手动审计的方案，能够主动检测价格操纵漏洞，有效应对现有方法的局限性，为DeFi安全提供了强有力的保障工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Powered+Detection+of+Price+Manipulation+in+DeFi，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21272&send_immediately=true&force_search=false)

**原文摘要:** Decentralized Finance (DeFi) smart contracts manage billions of dollars,
making them a prime target for exploits. Price manipulation vulnerabilities,
often via flash loans, are a devastating class of attacks causing significant
financial losses. Existing detection methods are limited. Reactive approaches
analyze attacks only after they occur, while proactive static analysis tools
rely on rigid, predefined heuristics, limiting adaptability. Both depend on
known attack patterns, failing to identify novel variants or comprehend complex
economic logic. We propose PMDetector, a hybrid framework combining static
analysis with Large Language Model (LLM)-based reasoning to proactively detect
price manipulation vulnerabilities. Our approach uses a formal attack model and
a three-stage pipeline. First, static taint analysis identifies potentially
vulnerable code paths. Second, a two-stage LLM process filters paths by
analyzing defenses and then simulates attacks to evaluate exploitability.
Finally, a static analysis checker validates LLM results, retaining only
high-risk paths and generating comprehensive vulnerability reports. To evaluate
its effectiveness, we built a dataset of 73 real-world vulnerable and 288
benign DeFi protocols. Results show PMDetector achieves 88% precision and 90%
recall with Gemini 2.5-flash, significantly outperforming state-of-the-art
static analysis and LLM-based approaches. Auditing a vulnerability with
PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an
efficient and cost-effective alternative to manual audits.

</details>


### [218] [The Qey: Implementation and performance study of post quantum cryptography in FIDO2](https://arxiv.org/abs/2510.21353)
*Aditya Mitra, Sibi Chakkaravarthy Sethuraman*

**主要类别:** cs.CR

**AI概要:** 该研究探索将基于模块格的后量子密码签名算法ML-DSA应用于FIDO2标准，以应对量子计算威胁，并与传统算法进行性能和安全性的比较分析。


<details>
  <summary>更多</summary>
  
**动机:** 当前FIDO2标准使用的ECDSA、RSA等经典密码算法容易受到大规模量子计算机的攻击，存在安全风险。

**方法:** 采用基于Crystals Dilithium的模块格数字签名算法(ML-DSA)作为后量子密码签名标准，研究其在FIDO2中的可用性。

**结果:** 论文对ML-DSA与传统算法在性能和安全性方面进行了比较分析。

**结论:** 研究旨在为FIDO2认证系统提供量子安全的解决方案，推动密码无认证技术在后量子时代的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Qey%3A+Implementation+and+performance+study+of+post+quantum+cryptography+in+FIDO2，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21353&send_immediately=true&force_search=false)

**原文摘要:** Authentication systems have evolved a lot since the 1960s when Fernando
Corbato first proposed the password-based authentication. In 2013, the FIDO
Alliance proposed using secure hardware for authentication, thus marking a
milestone in the passwordless authentication era [1]. Passwordless
authentication with a possession-based factor often relied on hardware-backed
cryptographic methods. FIDO2 being one an amalgamation of the W3C Web
Authentication and FIDO Alliance Client to Authenticator Protocol is an
industry standard for secure passwordless authentication with rising adoption
for the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256
(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature
algorithms. This makes it insecure against attacks involving large-scale
quantum computers [3]. This study aims at exploring the usability of Module
Lattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium
as a post quantum cryptographic signature standard for FIDO2. The paper
highlights the performance and security in comparison to keys with classical
algorithms.

</details>


### [219] [FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](https://arxiv.org/abs/2510.21401)
*Mojtaba Eshghie, Gabriele Morello, Matteo Lauretano, Alexandre Bartel, Martin Monperrus*

**主要类别:** cs.CR

**AI概要:** FLAMES是一个自动化工具，使用领域适应的大语言模型生成可执行的Solidity "require"语句来保护智能合约免受漏洞利用，无需漏洞标签或人工干预。


<details>
  <summary>更多</summary>
  
**动机:** 智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施。

**方法:** 使用在514,506个已验证合约中提取的真实世界不变式进行填空式监督微调，训练领域适应的大语言模型来合成运行时守卫。

**结果:** FLAMES实现了96.7%的可编译性，在5000个挑战性不变式测试集中44.5%与真实情况精确或语义匹配，阻止了108个真实漏洞中的22个（20.4%），并成功阻止了APEMAGA真实攻击事件。

**结论:** 领域适应的LLM能够自动为智能合约生成生产就绪的安全防御，无需漏洞检测、形式化规范或人工干预。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLAMES%3A+Fine-tuning+LLMs+to+Synthesize+Invariants+for+Smart+Contract+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21401&send_immediately=true&force_search=false)

**原文摘要:** Smart contract vulnerabilities cost billions of dollars annually, yet
existing automated analysis tools fail to generate deployable defenses. We
present FLAMES, a novel automated approach that synthesizes executable runtime
guards as Solidity "require" statements to harden smart contracts against
exploits. Unlike prior work that relies on vulnerability labels, symbolic
analysis, or natural language specifications, FLAMES employs domain-adapted
large language models trained through fill-in-the-middle supervised fine-tuning
on real-world invariants extracted from 514,506 verified contracts. Our
extensive evaluation across three dimensions demonstrates FLAMES's
effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for
synthesized invariant (2) Semantic Quality: on a curated test set of 5,000
challenging invariants, FLAMES produces exact or semantically equivalent
matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES
prevents 22 out of 108 real exploits (20.4%) while preserving contract
functionality, and (4) FLAMES successfully blocks the real-world APEMAGA
incident by synthesizing a pre-condition that mitigates the attack. FLAMES
establishes that domain-adapted LLMs can automatically generate
production-ready security defenses for smart contracts without requiring
vulnerability detection, formal specifications, or human intervention. We
release our code, model weights, datasets, and evaluation infrastructure to
enable reproducible research in this critical domain.

</details>


### [220] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe, Helmut Neukirchen, Thomas Welsh*

**主要类别:** cs.CR

**AI概要:** SBASH框架使用本地轻量级LLM解决蜜罐的数据保护问题，通过RAG和非RAG方法提升Linux shell命令响应的准确性和实时性，研究表明系统提示调优的模型可以达到与RAG相似的准确性且延迟更低。


<details>
  <summary>更多</summary>
  
**动机:** 传统蜜罐需要增强上下文感知能力以提高攻击者参与度，但现有LLM方法存在响应准确性、实时性、运营成本高和数据保护等问题。

**方法:** 提出SBASH框架，使用本地轻量级LLM，研究RAG支持和非RAG的LLM对Linux shell命令的处理，通过响应时间、人工测试真实性和与真实系统的相似度（Levenshtein距离、SBert、BertScore）进行评估。

**结果:** RAG提高了未调优模型的准确性，而通过系统提示调优的模型无需RAG即可达到与RAG未调优模型相似的准确性，且具有略低的延迟。

**结论:** 本地轻量级LLM结合适当的提示工程可以有效地提升蜜罐的上下文感知能力，同时解决数据保护和响应性能问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SBASH%3A+a+Framework+for+Designing+and+Evaluating+RAG+vs.+Prompt-Tuned+LLM+Honeypots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21459&send_immediately=true&force_search=false)

**原文摘要:** Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [221] [Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise](https://arxiv.org/abs/2510.21483)
*Pierre Guillot, Auguste Hoang Duc, Michel Koskas, Florian Méhats*

**主要类别:** cs.CR

**AI概要:** GRAFHEN是一种无需自举（无噪声）的全同态加密方案，通过群编码和重写系统实现，性能比现有标准快几个数量级


<details>
  <summary>更多</summary>
  
**动机:** 解决现有全同态加密方案需要bootstrapping（自举）操作和噪声问题，提供更高效的无噪声全同态加密

**方法:** 基于Nuida等人的工作，使用群编码技术，通过重写系统在机器上表示群，利用子群成员问题的计算困难性保证安全性

**结果:** 实现了无需bootstrapping的全同态加密，性能显著提升（比现有标准快几个数量级），并分析了多种可能的攻击并提供防护方案

**结论:** GRAFHEN方案成功实现了无噪声的全同态加密，在保持安全性的同时大幅提升了性能，为实际应用提供了可行的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Introducing+GRAFHEN%3A+Group-based+Fully+Homomorphic+Encryption+without+Noise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21483&send_immediately=true&force_search=false)

**原文摘要:** We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic
Encryption without the need for bootstrapping (or in other words, without
noise). Building on the work of Nuida and others, we achieve this using
encodings in groups.
  The groups are represented on a machine using rewriting systems. In this way
the subgroup membership problem, which an attacker would have to solve in order
to break the scheme, becomes maximally hard, while performance is preserved. In
fact we include a simple benchmark demonstrating that our implementation runs
several orders of magnitude faster than existing standards.
  We review many possible attacks against our protocol and explain how to
protect the scheme in each case.

</details>


### [222] [PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis](https://arxiv.org/abs/2510.21601)
*Emmanuel Dare Alalade, Ashraf Matrawy*

**主要类别:** cs.CR

**AI概要:** 本文提出了一个新颖的隐私威胁模型框架PTMF，通过结合MITRE ATT&CK和LINDDUN框架，从威胁行为者角度深入分析物联网隐私威胁，并通过专家问卷研究识别了主要威胁行为者和关键攻击路径。


<details>
  <summary>更多</summary>
  
**动机:** 现有隐私威胁分析主要关注威胁发生可能性和区域，缺乏对威胁行为者、其行为和意图的深入理解，需要开发一个以隐私为中心的威胁分析框架。

**方法:** 基于MITRE ATT&CK战术和LINDDUN隐私威胁模型技术开发PTMF框架，通过专家问卷研究分析12个物联网隐私威胁，识别威胁行为者和攻击路径。

**结果:** 识别了物联网用户识别隐私威胁中的前三大威胁行为者及其关键攻击路径，以及其余11个隐私威胁的相关发现。

**结论:** PTMF框架为理解物联网系统中威胁行为者的活动和意图提供了基础，有助于主动有效地部署隐私保护措施来缓解隐私威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PTMF%3A+A+Privacy+Threat+Modeling+Framework+for+IoT+with+Expert-Driven+Threat+Propagation+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21601&send_immediately=true&force_search=false)

**原文摘要:** Previous studies on PTA have focused on analyzing privacy threats based on
the potential areas of occurrence and their likelihood of occurrence. However,
an in-depth understanding of the threat actors involved, their actions, and the
intentions that result in privacy threats is essential. In this paper, we
present a novel Privacy Threat Model Framework (PTMF) that analyzes privacy
threats through different phases.
  The PTMF development is motivated through the selected tactics from the MITRE
ATT\&CK framework and techniques from the LINDDUN privacy threat model, making
PTMF a privacy-centered framework. The proposed PTMF can be employed in various
ways, including analyzing the activities of threat actors during privacy
threats and assessing privacy risks in IoT systems, among others. In this
paper, we conducted a user study on 12 privacy threats associated with IoT by
developing a questionnaire based on PTMF and recruited experts from both
industry and academia in the fields of security and privacy to gather their
opinions. The collected data were analyzed and mapped to identify the threat
actors involved in the identification of IoT users (IU) and the remaining 11
privacy threats. Our observation revealed the top three threat actors and the
critical paths they used during the IU privacy threat, as well as the remaining
11 privacy threats. This study could provide a solid foundation for
understanding how and where privacy measures can be proactively and effectively
deployed in IoT systems to mitigate privacy threats based on the activities and
intentions of threat actors within these systems.

</details>


### [223] [Toward provably private analytics and insights into GenAI use](https://arxiv.org/abs/2510.21684)
*Albert Cheu, Artem Lagzdin, Brett McLarnon, Daniel Ramage, Katharine Daly, Marco Gruteser, Peter Kairouz, Rakshita Tandon, Stanislav Chiknavaryan, Timon Van Overveldt, Zoe Gong*

**主要类别:** cs.CR

**AI概要:** 新一代联邦分析系统，使用基于AMD SEV-SNP和Intel TDX的可信执行环境(TEEs)，为服务器端处理提供可验证的隐私保证，支持LLM处理非结构化数据并应用差分隐私，已成功部署生产环境。


<details>
  <summary>更多</summary>
  
**动机:** 大规模设备数据分析系统需要同时满足高隐私安全标准、数据质量、可用性和资源效率要求，现有系统在隐私保护和验证能力方面存在不足。

**方法:** 设备加密上传数据并标记允许的服务器处理步骤，使用开源TEE托管的密钥管理服务确保数据仅能被受TEE保护的处理步骤访问，支持LLM处理非结构化数据和差分隐私聚合。

**结果:** 系统成功部署生产环境，为真实世界的GenAI体验提供有价值的洞察，实现了可验证的隐私保护和处理透明度。

**结论:** 基于TEE的联邦分析系统能够有效平衡隐私保护与数据分析需求，通过可验证的架构设计为大规模设备数据分析提供了可行的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+provably+private+analytics+and+insights+into+GenAI+use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21684&send_immediately=true&force_search=false)

**原文摘要:** Large-scale systems that compute analytics over a fleet of devices must
achieve high privacy and security standards while also meeting data quality,
usability, and resource efficiency expectations. We present a next-generation
federated analytics system that uses Trusted Execution Environments (TEEs)
based on technologies like AMD SEV-SNP and Intel TDX to provide verifiable
privacy guarantees for all server-side processing. In our system, devices
encrypt and upload data, tagging it with a limited set of allowable server-side
processing steps. An open source, TEE-hosted key management service guarantees
that the data is accessible only to those steps, which are themselves protected
by TEE confidentiality and integrity assurance guarantees. The system is
designed for flexible workloads, including processing unstructured data with
LLMs (for structured summarization) before aggregation into differentially
private insights (with automatic parameter tuning). The transparency properties
of our system allow any external party to verify that all raw and derived data
is processed in TEEs, protecting it from inspection by the system operator, and
that differential privacy is applied to all released results. This system has
been successfully deployed in production, providing helpful insights into
real-world GenAI experiences.

</details>


### [224] [$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy](https://arxiv.org/abs/2510.21946)
*Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah*

**主要类别:** cs.CR

**AI概要:** 论文提出了一种名为δ-STEAL的新型模型窃取攻击方法，能够绕过LLM服务提供商的水印检测器，同时保持攻击者模型的实用性，对现有知识产权保护方法构成严重威胁。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型(LLM)部署存在知识产权风险，特别是模型窃取攻击会威胁专有LLM的收入和财务稳定性。虽然水印技术可用于模型溯源和知识产权验证，但需要研究其脆弱性。

**方法:** 提出δ-STEAL攻击方法：在对手模型微调期间向token嵌入注入噪声，满足局部差分隐私(LDP)保证。通过查询服务提供商模型收集输出，形成输入-输出训练对，应用LDP保护噪声来混淆水印信号。

**结果:** 实验显示δ-STEAL在轻量级修改下攻击成功率高达96.95%，且不显著损害对手模型实用性。LDP中的噪声规模控制攻击效果与模型实用性之间的权衡。

**结论:** δ-STEAL攻击方法能够有效绕过即使是鲁棒的水印保护，使对手能够欺骗水印检测器，从而破坏当前的知识产权保护方法，对LLM服务提供商构成重大风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%CE%B4%24-STEAL%3A+LLM+Stealing+Attack+with+Local+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21946&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demonstrate remarkable capabilities across
various tasks. However, their deployment introduces significant risks related
to intellectual property. In this context, we focus on model stealing attacks,
where adversaries replicate the behaviors of these models to steal services.
These attacks are highly relevant to proprietary LLMs and pose serious threats
to revenue and financial stability. To mitigate these risks, the watermarking
solution embeds imperceptible patterns in LLM outputs, enabling model
traceability and intellectual property verification. In this paper, we study
the vulnerability of LLM service providers by introducing $\delta$-STEAL, a
novel model stealing attack that bypasses the service provider's watermark
detectors while preserving the adversary's model utility. $\delta$-STEAL
injects noise into the token embeddings of the adversary's model during
fine-tuning in a way that satisfies local differential privacy (LDP)
guarantees. The adversary queries the service provider's model to collect
outputs and form input-output training pairs. By applying LDP-preserving noise
to these pairs, $\delta$-STEAL obfuscates watermark signals, making it
difficult for the service provider to determine whether its outputs were used,
thereby preventing claims of model theft. Our experiments show that
$\delta$-STEAL with lightweight modifications achieves attack success rates of
up to $96.95\%$ without significantly compromising the adversary's model
utility. The noise scale in LDP controls the trade-off between attack
effectiveness and model utility. This poses a significant risk, as even robust
watermarks can be bypassed, allowing adversaries to deceive watermark detectors
and undermine current intellectual property protection methods.

</details>


### [225] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan, Ziyu Shu, Amberbir Alemayoh*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种结合自监督对比学习和神经架构搜索的勒索软件检测框架，通过硬件性能计数器和定制损失函数实现早期检测，显著提升了检测精度和响应速度。


<details>
  <summary>更多</summary>
  
**动机:** 勒索软件已成为网络安全的关键威胁，传统检测方法面临特征依赖、响应延迟和适应性有限三大挑战，需要更先进的AI解决方案。

**方法:** 采用自监督对比学习框架整合硬件性能计数器分析运行时行为；设计定制损失函数促进早期恶意活动检测；部署神经架构搜索自动构建自适应模型架构。

**结果:** 实验结果显示，相比现有方法，检测精度提升高达16.1%，响应时间加快6倍，且在规避攻击下保持鲁棒性。

**结论:** 该框架有效解决了勒索软件检测的关键挑战，为实时网络安全防护提供了高效自适应的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Low-Latency+and+Adaptive+Ransomware+Detection+Using+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21957&send_immediately=true&force_search=false)

**原文摘要:** Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [226] [Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla](https://arxiv.org/abs/2510.22024)
*Evangelos Bitsikas, Jason Veara, Aanjhan Ranganathan*

**主要类别:** cs.CR

**AI概要:** 对特斯拉车辆LTE连接的黑盒安全分析，揭示了IMSI捕获、伪基站劫持、不安全回退机制等系统漏洞，挑战了汽车网络安全监管框架的核心假设


<details>
  <summary>更多</summary>
  
**动机:** 虽然移动网络漏洞在智能手机生态中已有充分研究，但其在安全关键的汽车环境中的影响仍未得到充分检验，需要评估LTE连接在车辆远程诊断、OTA更新和安全服务中的安全性

**方法:** 采用黑盒、非侵入式的安全分析方法，针对特斯拉Model 3和Cybertruck车辆的LTE连接进行测试，包括协议弱点和架构错误配置的检测

**结果:** 发现特斯拉远程信息处理堆栈存在多种漏洞：易受IMSI捕获、伪基站劫持攻击、不安全回退机制可能导致服务静默降级，遗留控制平面配置允许静默SMS注入和广播消息欺骗

**结论:** 这些漏洞不仅影响单个厂商，还挑战了ISO/SAE 21434和UN R155/R156等监管框架的核心安全假设，突显了现代车辆类型认证中对安全、可追溯和有弹性的远程信息处理系统的需求

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Analysis+of+LTE+Connectivity+in+Connected+Cars%3A+A+Case+Study+of+Tesla，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22024&send_immediately=true&force_search=false)

**原文摘要:** Modern connected vehicles rely on persistent LTE connectivity to enable
remote diagnostics, over-the-air (OTA) updates, and critical safety services.
While mobile network vulnerabilities are well documented in the smartphone
ecosystem, their impact in safety-critical automotive settings remains
insufficiently examined. In this work, we conduct a black-box, non-invasive
security analysis of LTE connectivity in Tesla vehicles, including the Model 3
and Cybertruck, revealing systemic protocol weaknesses and architectural
misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI
catching, rogue base station hijacking, and insecure fallback mechanisms that
may silently degrade service availability. Furthermore, legacy control-plane
configurations allow for silent SMS injection and broadcast message spoofing
without driver awareness. These vulnerabilities have implications beyond a
single vendor as they challenge core assumptions in regulatory frameworks like
ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient
telematics for type approval of modern vehicles.

</details>


### [227] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

**主要类别:** cs.CR

**AI概要:** 提出Jailbreak Mimicry方法，通过训练小型攻击模型自动生成叙事型越狱提示，实现了81%的攻击成功率，比直接提示提高54倍，揭示了当前AI安全对齐方法的系统性漏洞。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型仍然容易受到复杂提示工程攻击，这些攻击利用上下文框架绕过安全机制，对网络安全应用构成重大风险。

**方法:** 使用参数高效微调(LoRA)在Mistral-7B模型上，利用从AdvBench整理的训练数据集，开发自动生成叙事型越狱提示的系统化方法。

**结果:** 在200个测试项上对GPT-OSS-20B达到81.0%攻击成功率，对GPT-4达到66.5%，Llama-3达到79.5%，Gemini 2.5 Flash达到33.0%。技术领域(网络安全93%)和欺骗类攻击(欺诈87.8%)特别脆弱。

**结论:** 该方法将对抗性提示发现从手工制作转变为可复现的科学过程，揭示了当前安全对齐方法的系统性漏洞，为AI网络安全红队测试提供了可靠且可扩展的评估方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jailbreak+Mimicry%3A+Automated+Discovery+of+Narrative-Based+Jailbreaks+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22085&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [228] [Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things](https://arxiv.org/abs/2510.22100)
*Saif E. Nouma, Attila A. Yavuz*

**主要类别:** cs.CR

**AI概要:** Graphene是首个对称前向安全聚合认证加密框架，专为低端物联网设备设计，结合密钥演进策略和离线-在线加密处理，提供密钥泄露恢复能力、近最优在线延迟和紧凑认证标签。


<details>
  <summary>更多</summary>
  
**动机:** 当前物联网认证加密标准缺乏密钥泄露恢复能力、紧凑认证标签和离线-在线加密等关键特性，无法满足资源受限设备在对抗性环境下的性能和安全需求。

**方法:** 提出Graphene框架，通过整合密钥演进策略、离线-在线加密处理和通用消息认证码(UMACs)，开发了两个不同的实例化实现，并在商用硬件和ARM Cortex-M4微控制器上进行实验评估。

**结果:** 实验显示Graphene在性能上显著优于现有方案，具有向后兼容性，并已开源发布供公共测试和使用。

**结论:** Graphene成功解决了物联网认证加密的关键缺陷，为资源受限设备提供了高效、安全且具有扩展性的解决方案，是物联网安全的重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+and+Breach-Resilient+Authenticated+Encryption+Framework+for+Internet+of+Things，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22100&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) relies heavily on resource-limited devices to
communicate critical (e.g., military data) information under low-energy
adversarial environments and low-latency wireless channels. Authenticated
Encryption (AE) guarantees confidentiality, authenticity, and integrity, making
it a vital security service for IoT. However, current deployed (lightweight) AE
standards lack essential features like key compromise resiliency and compact
authentication tags, as well as performance enhancements such as offline-online
cryptography. To address these gaps, we propose Graphene, the first (to our
knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption
(FAAE) framework designed for the performance and security demands of low-end
IoT infrastructures. Graphene innovates by synergizing key evolution strategies
and offline-online cryptographic processing with Universal Message
Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal
online latency, and compactness. We demonstrate Graphene efficiency through two
distinct instantiations, each balancing unique performance trade-offs with
extensibility for diverse MACs. Our experimental evaluation on commodity
hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant
performance gains over existing alternatives. Graphene is also backward
compatible with standard-compliant cryptographic implementations. We release
our implementation as open source for public testing and adaptation.

</details>


### [229] [TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation](https://arxiv.org/abs/2510.22191)
*Qi Sheng*

**主要类别:** cs.CR

**AI概要:** TPPR是一个新型的APT攻击溯源分析框架，通过异常子图提取、TTP标注和图剪枝，结合TTP序列模式进行攻击路径推理，实现了99.9%的图简化同时保留91%关键攻击节点，在重建精度上比现有方法提升63.1%-67.9%。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于溯源图的APT攻击分析方法在行为模式匹配和数据流特征匹配方面存在局限，无法有效建立攻击上下文关联，容易将良性系统操作与真实攻击实体混淆，无法准确刻画真实的APT行为。

**方法:** 首先通过异常节点检测、TTP标注和图剪枝提取异常子图，然后使用挖掘的TTP序列模式进行攻击路径推理，最后通过基于置信度的路径评分和合并来重建攻击场景。

**结果:** 在真实企业日志（超过1亿事件）和DARPA TC数据集上的评估显示，TPPR能够实现99.9%的图简化（从70万条边减少到20条边），同时保留91%的关键攻击节点，在重建精度上比最先进解决方案（SPARSE、DepImpact）分别高出63.1%和67.9%。

**结论:** TPPR框架通过结合攻击者的战术技术模式（TTP）和序列模式分析，能够有效识别APT攻击的关键路径，显著提高攻击场景重建的准确性和效率，为APT攻击溯源分析提供了新的有效方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TPPR%3A+APT+Tactic+%2F+Technique+Pattern+Guided+Attack+Path+Reasoning+for+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22191&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis based on system audit data has emerged as a fundamental
approach for investigating Advanced Persistent Threat (APT) attacks. Due to the
high concealment and long-term persistence of APT attacks, they are only
represented as a minimal part of the critical path in the provenance graph.
While existing techniques employ behavioral pattern matching and data flow
feature matching to uncover latent associations in attack sequences through
provenance graph path reasoning, their inability to establish effective attack
context associations often leads to the conflation of benign system operations
with real attack entities, that fail to accurately characterize real APT
behaviors. We observe that while the causality of entities in the provenance
graph exhibit substantial complexity, attackers often follow specific attack
patterns-specifically, clear combinations of tactics and techniques to achieve
their goals. Based on these insights, we propose TPPR, a novel framework that
first extracts anomaly subgraphs through abnormal node detection,
TTP-annotation and graph pruning, then performs attack path reasoning using
mined TTP sequential pattern, and finally reconstructs attack scenarios through
confidence-based path scoring and merging. Extensive evaluation on real
enterprise logs (more than 100 million events) and DARPA TC dataset
demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000
to 20 edges) while preserving 91% of critical attack nodes, outperforming
state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in
reconstruction precision while maintaining attack scenario integrity.

</details>


### [230] [SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha, Junaid Arshad, Mohamed Ben Farah, Khalid Ismail*

**主要类别:** cs.CR

**AI概要:** SecureLearn是一种针对多类分类器的两层攻击不可知防御机制，通过数据净化和特征导向的对抗训练来抵御数据投毒攻击，在多种机器学习算法和数据集上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据投毒防御大多针对特定攻击或特定ML算法，且主要关注深度神经网络或二分类器，而传统多类分类器在安全防护方面缺乏关注，这些模型在多模态应用中具有重要意义。

**方法:** 提出SecureLearn防御框架，包含两个组件：数据净化和新的特征导向对抗训练。采用3D评估矩阵（数据投毒攻击、数据净化和对抗训练三个正交维度）进行基准测试，在10%-20%投毒水平下评估准确性、召回率、F1分数等指标。

**结果:** SecureLearn对所选攻击有效，保持准确性90%以上，召回率和F1分数75%以上。对于神经网络，对所有选定投毒攻击实现97%的召回率和F1分数，增强了传统多类模型和神经网络的抗攻击能力。

**结论:** SecureLearn证明了其超越算法特定防御的泛化能力，为多类分类器提供了有效的攻击不可知防御解决方案，显著提高了模型的抗攻击鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SecureLearn+-+An+Attack-agnostic+Defense+for+Multiclass+Machine+Learning+Against+Data+Poisoning+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22274&send_immediately=true&force_search=false)

**原文摘要:** Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [231] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly, Christiana Chamon*

**主要类别:** cs.CR

**AI概要:** 该研究提出了一种利用宽禁带技术固有开关噪声作为物理不可克隆函数(PUF)源和实时威胁指示器的双重用途安全框架，结合机器学习实现高精度、低延迟的工业控制系统安全防护。


<details>
  <summary>更多</summary>
  
**动机:** 宽禁带技术虽然提升了电力系统性能，但带来了高频噪声和网络安全风险，需要在工业控制系统中开发新的安全防护方法。

**方法:** 采用噪声驱动的PUF和机器学习辅助异常检测框架，通过提取WBG开关噪声(高达100kHz)作为熵源，结合混合机器学习模型和自适应贝叶斯滤波技术。

**结果:** 在良性及攻击场景(EMI注入、信号篡改、节点冒充)的详细模拟中，实现了95%的检测准确率和亚毫秒级的处理延迟。

**结论:** 该研究证明了物理驱动的噪声双重利用作为可扩展ICS防御原语的可行性，为利用固有设备特性、结合硬件和AI的下一代安全策略奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Noise-Driven+PUF+and+AI+for+Secure+WBG+ICS%3A+A+Proof-of-Concept+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22283&send_immediately=true&force_search=false)

**原文摘要:** Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [232] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang, Tairen Zhang, Lanjun Wang, Ruidong Chen, Wenhui Li, Anan Liu*

**主要类别:** cs.CR

**AI概要:** 本文提出了T2I-RiskyPrompt基准测试，用于评估文本到图像模型的安全性，包含6,432个风险提示，涵盖6大类14个子类，并提出了基于原因的检测方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有风险提示数据集存在三个主要问题：风险类别有限、标注粒度粗、有效性低，需要更全面的安全评估基准。

**方法:** 开发分层风险分类法，构建风险提示收集和标注流程，提出基于多语言大模型的原因驱动风险图像检测方法。

**结果:** 对8个T2I模型、9种防御方法、5个安全过滤器和5种攻击策略进行全面评估，提供了9个关键安全见解。

**结论:** T2I-RiskyPrompt为T2I模型安全评估提供了全面基准，具有跨研究领域的潜在应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T2I-RiskyPrompt%3A+A+Benchmark+for+Safety+Evaluation%2C+Attack%2C+and+Defense+on+Text-to-Image+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22300&send_immediately=true&force_search=false)

**原文摘要:** Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [233] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

**主要类别:** cs.CR

**AI概要:** 提出跨机构联邦学习框架，用于ECG图像数字化，在保护隐私的同时实现接近集中式训练的性能


<details>
  <summary>更多</summary>
  
**动机:** 集中式训练ECG图像数字化模型面临跨机构隐私保护和部署限制的冲突，需要开发隐私保护的分布式训练方案

**方法:** 使用联邦学习框架训练nnU-Net分割模型，整合FedAvg/FedProx/FedAdam聚合器，结合安全聚合和中心差分隐私，包含图像预处理和向量化流程

**结果:** 实验显示FedAdam收敛更快且性能更好，接近集中式性能，隐私机制在保护原始图像和客户端更新的同时保持竞争性准确度

**结论:** 该框架为多机构环境提供了可部署、可审计的隐私保护ECG数字化解决方案，平衡了效用与隐私保护需求

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Aware+Federated+nnU-Net+for+ECG+Page+Digitization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22387&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [234] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li, Zheng Yu, Jingyi Song, Meng Xu, Yuxuan Luo, Dongliang Mu*

**主要类别:** cs.CR

**AI概要:** PORTGPT是一个基于LLM的端到端自动化补丁回溯工具，通过工具调用和自主验证机制，在现有数据集上达到89.15%的成功率，在复杂案例上达到62.33%的成功率，优于现有最先进工具。


<details>
  <summary>更多</summary>
  
**动机:** 手动回溯安全补丁到旧版本分支是一项劳动密集型任务，现有自动化方法依赖预定义的语法或语义规则，缺乏对复杂补丁的灵活性。

**方法:** PORTGPT增强LLM模型，使其能够按需访问代码、总结Git历史记录，并基于反馈（如编译器输出）自主修订补丁，模拟人类推理和验证过程。

**结果:** 在1815个现有数据集案例中达到89.15%成功率，在146个复杂案例中达到62.33%成功率，向Linux内核社区贡献的9个回溯补丁全部被合并。

**结论:** PORTGPT通过LLM代理的方式有效解决了补丁回溯的自动化问题，在真实场景中表现出色，为开源项目维护提供了高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PortGPT%3A+Towards+Automated+Backporting+Using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22396&send_immediately=true&force_search=false)

**原文摘要:** Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [235] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao, Jia Zou, Zhichao Cao, Xusheng Xiao*

**主要类别:** cs.CR

**AI概要:** PROGQL框架提出了一种专门用于网络攻击调查的图搜索语言和查询引擎，解决了现有溯源分析技术的不灵活性和内存效率低下的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有溯源分析技术面临两个关键挑战：(1) 不灵活且不可扩展，难以整合分析师专业知识；(2) 内存效率低下，通常需要超过100GB内存来存储完整事件流，限制了在实际环境中的可扩展性和部署。

**方法:** 提出PROGQL框架，提供领域特定的图搜索语言和精心设计的查询引擎，支持约束图遍历、边权重计算、沿加权边的值传播和图合并等新语言构造。查询引擎针对异构数据库后端的高效增量图搜索进行了优化。

**结果:** 在真实攻击评估中，PROGQL语言在表达多样化复杂攻击方面比最先进的图查询语言Cypher更有效，与SOTA PA技术DEPIMPACT的比较进一步证明了PROGQL框架设计带来的可扩展性显著提升。

**结论:** PROGQL框架通过提供专门的图搜索语言和高效的查询引擎，成功解决了现有溯源分析技术的局限性，为复杂网络攻击调查提供了更灵活、可扩展且内存效率高的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProGQL%3A+A+Provenance+Graph+Query+System+for+Cyber+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22400，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22400&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [236] [ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole](https://arxiv.org/abs/2510.22536)
*Jotaro Yano*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一个跨域的ZK协处理器桥接系统，允许Solana程序通过Wormhole VAA在Aztec L2上请求私有执行，包含Solana程序、EVM门户、Aztec合约和中继器四个组件。


<details>
  <summary>更多</summary>
  
**动机:** 解决Solana程序需要安全、私密地在以太坊生态的Aztec L2网络上执行计算的需求，实现跨区块链的隐私保护计算。

**方法:** 设计包含四个组件的系统架构：Solana程序发布消息到Wormhole Core，EVM门户验证VAA并处理重放保护，Aztec合约私有消费消息，离链中继器传输VAA和记录回执。

**结果:** 提出了完整的状态机、消息格式和安全性证明框架，包括重放安全、来源真实性、最终性对齐、参数绑定、隐私保护、幂等性和活跃性保障。

**结论:** 成功设计并实现了跨域ZK协处理器桥接系统，提供了可复现的测试网络运行环境，为Solana与Aztec之间的隐私保护跨链计算提供了可行解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZK+Coprocessor+Bridge%3A+Replay-Safe+Private+Execution+from+Solana+to+Aztec+via+Wormhole，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22536&send_immediately=true&force_search=false)

**原文摘要:** We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs
request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable
Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a
Solana program that posts messages to Wormhole Core with explicit finality;
(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound
payload secretHash||m from the attested VAA, derives a domain-separated field
commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference
implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we
provide migration guidance to the payload-bound interface); (iii) a minimal
Aztec contract that consumes the message privately; and (iv) an off-chain
relayer that ferries VAAs and can record receipts on Solana. We present state
machines, message formats, and proof sketches for replay-safety, origin
authenticity, finality alignment, parameter binding (no relayer front-running
of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a
concise Reproducibility note with pinned versions and artifacts to replicate a
public testnet run.

</details>


### [237] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang*

**主要类别:** cs.CR

**AI概要:** CP-GBA是一种新的跨范式图后门攻击方法，通过图提示学习训练通用子图触发器，解决了现有方法在跨学习范式迁移性差的问题，在多个真实数据集上实现了最先进的攻击成功率。


<details>
  <summary>更多</summary>
  
**动机:** 现有图神经网络后门攻击的触发器生成器结构简单，过度依赖特定特征，只能适应单一图学习范式（如图监督学习、图对比学习或图提示学习），在其他学习范式中迁移性差，攻击成功率有限。

**方法:** 1. 从目标图中提取紧凑且表达性强的子图触发器集合，构建可查询的知识库，同时确保类别感知、特征丰富和结构保真；2. 首次探索图提示学习的理论可迁移性，在基于提示的目标下训练触发器，使其能有效泛化到多样化和未见过的测试范式。

**结果:** 在多个真实数据集和防御场景下的广泛实验表明，CP-GBA实现了最先进的攻击成功率。

**结论:** CP-GBA通过创新的跨范式设计，成功解决了图后门攻击的迁移性问题，为图神经网络的安全性研究提供了新的视角和方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Paradigm+Graph+Backdoor+Attacks+with+Promptable+Subgraph+Triggers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22555&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [238] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee, Sajal Saha*

**主要类别:** cs.CR

**AI概要:** 对区块链中数字签名方案的系统性调研，分析其如何实现不可否认性并保障系统安全，比较不同签名方案在共识协议和智能合约中的适用性


<details>
  <summary>更多</summary>
  
**动机:** 区块链系统依赖去中心化账本和强安全保证，其中不可否认性是关键需求，需要防止交易作者否认并支持数据完整性

**方法:** 调研代表性数字签名方案家族，分析其密码学基础、安全假设和相关属性（不可伪造性、抗延展性、聚合支持、密钥签名大小、验证成本等），使用这些标准比较不同设计在共识协议和智能合约约束下的适用性

**结果:** 通过比较分析揭示了不同签名方案在吞吐量、存储、可扩展性和攻击面方面的实际权衡，总结了各方案在区块链环境中的优势和限制

**结论:** 精心选择的数字签名对于实现不可否认性和保持信息完整性至关重要，研究还提出了互操作性和后量子准备等实施考虑和开放方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Blockchain+Signatures+to+Ensure+Information+Integrity+and+Non-Repudiation+in+the+Digital+Era%3A+A+comprehensive+study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22561，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22561&send_immediately=true&force_search=false)

**原文摘要:** Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [239] [FAARM: Firmware Attestation and Authentication Framework for Mali GPUs](https://arxiv.org/abs/2510.22566)
*Md. Mehedi Hasan*

**主要类别:** cs.CR

**AI概要:** FAARM是一个轻量级固件认证框架，通过在EL3安全监控器中集成数字签名验证，有效防御MOLE攻击，保护GPU可信执行环境免受恶意固件注入，仅带来1.34毫秒的延迟开销。


<details>
  <summary>更多</summary>
  
**动机:** MOLE攻击揭示了GPU TEEs中存在的关键固件级信任漏洞，攻击者可通过注入恶意固件绕过内存保护并窃取敏感数据，现有GPU TEE设计缺乏有效的固件验证机制。

**方法:** FAARM采用供应商签名的固件包和设备内公钥锚点，在EL3安全监控器进行固件完整性和真实性验证，执行版本检查并锁定固件区域，消除预验证和TOCTOU攻击向量。

**结果:** FAARM能可靠检测和阻止恶意固件注入，在使用前拒绝篡改镜像并在认证后阻止覆盖尝试，平均验证延迟仅为1.34毫秒，安全开销可忽略不计。

**结论:** FAARM填补了基于shim的GPU TEEs的基础安全漏洞，为移动和云GPU部署提供了实用且可部署的防御方案，显著提升了安全基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAARM%3A+Firmware+Attestation+and+Authentication+Framework+for+Mali+GPUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22566&send_immediately=true&force_search=false)

**原文摘要:** Recent work has revealed MOLE, the first practical attack to compromise GPU
Trusted Execution Environments (TEEs), by injecting malicious firmware into the
embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence
of cryptographic verification during initialization, adversaries with kernel
privileges can bypass memory protections, exfiltrate sensitive data at over 40
MB/s, and tamper with inference results, all with negligible runtime overhead.
This attack surface affects commodity mobile SoCs and cloud accelerators,
exposing a critical firmware-level trust gap in existing GPU TEE designs. To
address this gap, this paper presents FAARM, a lightweight Firmware Attestation
and Authentication framework that prevents MOLE-style firmware subversion.
FAARM integrates digital signature verification at the EL3 secure monitor using
vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3
verifies firmware integrity and authenticity, enforces version checks, and
locks the firmware region, eliminating both pre-verification and
time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a
software-only prototype on a Mali GPU testbed, using a Google Colab-based
emulation framework that models the firmware signing process, the EL1 to EL3
load path, and secure memory configuration. FAARM reliably detects and blocks
malicious firmware injections, rejecting tampered images before use and denying
overwrite attempts after attestation. Firmware verification incurs only 1.34 ms
latency on average, demonstrating that strong security can be achieved with
negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU
TEEs, providing a practical, deployable defense that raises the security
baseline for both mobile and cloud GPU deployments.

</details>


### [240] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska, Max Mathys, Francesco Casucci, Mateo Rojas-Carulla, Xander Davies, Alexandra Souly, Niklas Pfister*

**主要类别:** cs.CR

**AI概要:** 论文提出了threat snapshots框架和b³基准测试，用于系统评估LLM作为AI代理时的安全性，发现推理能力提升安全性而模型大小与安全性无关


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏对LLM作为AI代理时安全性的系统理解，现有框架无法全面处理LLM漏洞与传统安全风险的复杂交互问题

**方法:** 引入threat snapshots框架，通过隔离代理执行流程中的特定状态来识别LLM漏洞；构建包含194,331个众包对抗攻击的b³安全基准测试；评估31个流行LLM

**结果:** 研究发现增强推理能力能提高安全性，而模型大小与安全性没有相关性

**结论:** 该框架和基准测试为LLM提供商和从业者提供了安全评估工具，指导代理开发者并激励模型开发者优先考虑骨干模型的安全性改进

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Agent+Backbones%3A+Evaluating+the+Security+of+Backbone+LLMs+in+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22620&send_immediately=true&force_search=false)

**原文摘要:** AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [241] [DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2510.22622)
*Kangran Zhao, Yupeng Chen, Xiaoyu Zhang, Yize Chen, Weinan Guan, Baicheng Chen, Chengzhe Sun, Soumyya Kanti Datta, Qingshan Liu, Siwei Lyu, Baoyuan Wu*

**主要类别:** cs.CR

**AI概要:** 该论文提出了Mega-MMDF大规模多模态深度伪造数据集和DeepfakeBench-MM统一基准，用于解决多模态深度伪造检测领域缺乏充足训练数据和标准化评估的问题。


<details>
  <summary>更多</summary>
  
**动机:** 先进的生成式AI模型被滥用导致伪造人像音视频内容泛滥，带来严重社会风险。现有研究缺乏足够的多样化训练数据和标准化基准，阻碍了该领域的深入探索。

**方法:** 1. 构建Mega-MMDF数据集：使用21种伪造管道（10种音频伪造方法+12种视觉伪造方法+6种音频驱动人脸重演方法），包含11万真实样本和110万伪造样本。2. 建立DeepfakeBench-MM基准：为多模态深度伪造检测提供标准化协议和评估平台，支持5个数据集和11种检测器。

**结果:** 创建了当前最大最丰富的多模态深度伪造数据集，建立了首个统一基准，并通过全面评估发现了多个关键发现（如数据增强、堆叠伪造等方面）。

**结论:** Mega-MMDF数据集和DeepfakeBench-MM基准将作为推进多模态深度伪造检测研究的基础设施，为解决AI生成内容的安全问题提供重要支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepfakeBench-MM%3A+A+Comprehensive+Benchmark+for+Multimodal+Deepfake+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22622&send_immediately=true&force_search=false)

**原文摘要:** The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.

</details>


### [242] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain*

**主要类别:** cs.CR

**AI概要:** Sentra-Guard是一个实时模块化防御系统，使用混合架构检测和缓解针对大语言模型的越狱和提示注入攻击，达到99.96%检测率和极低攻击成功率。


<details>
  <summary>更多</summary>
  
**动机:** 针对大语言模型面临的安全威胁，特别是越狱和提示注入攻击，需要开发有效的实时防御系统来保护LLM安全。

**方法:** 采用混合架构：FAISS索引的SBERT嵌入表示捕获提示语义，结合微调transformer分类器；包含分类器-检索器融合模块动态计算风险分数；多语言预处理层支持100+语言；包含人机交互反馈循环。

**结果:** 检测率99.96%（AUC=1.00，F1=1.00），攻击成功率仅0.004%，优于LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）等基线方法。

**结论:** Sentra-Guard建立了对抗性LLM防御的新最先进水平，具有透明、可微调和模块化特点，支持商业和开源环境的可扩展部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentra-Guard%3A+A+Multilingual+Human-AI+Framework+for+Real-Time+Defense+Against+Adversarial+LLM+Jailbreaks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22628&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [243] [RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography](https://arxiv.org/abs/2510.22661)
*Malik Imran, Safiullah Khan, Zain Ul Abideen, Ciara Rafferty, Ayesha Khalid, Muhammad Rashid, Maire O'Neill*

**主要类别:** cs.CR

**AI概要:** RejSCore是一个针对后量子密码学中拒绝采样操作的轻量级硬件加速器，专门为QR-UOV签名方案设计，在Artix-7 FPGA和65nm CMOS技术上实现了低资源消耗和高能效。


<details>
  <summary>更多</summary>
  
**动机:** 后量子多变量公钥密码方案需要繁重的拒绝采样操作，这对资源受限设备构成挑战，而现有硬件设计在这方面研究不足。

**方法:** 设计包含AES-CTR-128伪随机数生成器的轻量级迭代架构，使用面积-延迟乘积和功率-延迟乘积进行性能评估。

**结果:** 在Artix-7上达到2042个slice面积和222MHz频率，在65nm CMOS上达到464,866μm²面积和565MHz频率，8525个时钟周期完成操作。

**结论:** ADP和PDP评估证实RejSCore适合部署在资源受限和安全关键的环境中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RejSCore%3A+Rejection+Sampling+Core+for+Multivariate-based+Public+key+Cryptography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22661&send_immediately=true&force_search=false)

**原文摘要:** Post-quantum multivariate public key cryptography (MPKC) schemes resist
quantum threats but require heavy operations, such as rejection sampling, which
challenge resource-limited devices. Prior hardware designs have addressed
various aspects of MPKC signature generation. However, rejection sampling
remains largely unexplored in such contexts. This paper presents RejSCore, a
lightweight hardware accelerator for rejection sampling in post-quantum
cryptography. It specifically targets the QR-UOV scheme, which is a prominent
candidate under the second-round of the National Institute of Standards and
Technology (NIST) additional digital signature standardization process. The
architecture includes an AES-CTR-128-based pseudorandom number generator.
Moreover, a lightweight iterative method is employed in rejection sampling,
offering reduced resource consumption and area overhead while slightly
increasing latency. The performance of RejSCore is comprehensively evaluated on
Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and
Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area
of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and
565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =
127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525
clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for
deployment in resource-constrained and security-critical environments.

</details>


### [244] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le, Tan Le*

**主要类别:** cs.CR

**AI概要:** SpoofTrackBench是一个可复现的模块化基准测试，用于评估雷达欺骗攻击下实时定位跟踪系统的对抗鲁棒性，支持不同跟踪架构的性能比较和可视化分析。


<details>
  <summary>更多</summary>
  
**动机:** 需要建立一个开放、可复现的基准测试标准，用于系统评估雷达欺骗攻击对实时定位跟踪系统的影响，促进不同跟踪架构的对抗鲁棒性分析和社区验证。

**方法:** 利用Hampton University Skyler雷达传感器数据集，模拟漂移、幽灵和镜像三种欺骗攻击，使用JPDA和GNN两种跟踪架构进行评估，通过分离干净和欺骗检测流、可视化轨迹偏差和量化赋值误差来进行分析。

**结果:** 开发了一个包含聚类覆盖、注入感知时间线和场景自适应可视化的框架，能够自动导出评估图表和日志，实现跨欺骗类型和配置的可解释性分析。

**结论:** SpoofTrackBench为欺骗感知跟踪流程的开放、道德基准测试设立了新标准，支持严格的跨架构分析和社区验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpoofTrackBench%3A+Interpretable+AI+for+Spoof-Aware+UAV+Tracking+and+Benchmarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22726&send_immediately=true&force_search=false)

**原文摘要:** SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [245] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang, YiLu Zhong, MiDi Wan, WenJie Yu, YuanBing Ouyang, Yenan Huang, Hui Li*

**主要类别:** cs.CR

**AI概要:** 研究发现提示词质量对AI生成代码安全性有显著影响，低质量提示词会大幅增加不安全代码生成概率，而高级提示技术可有效缓解此风险


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注对抗攻击或模型固有缺陷，但忽略了良性但表述不佳的提示词对生成代码安全性的影响这一普遍但未充分探索的问题

**方法:** 提出包含目标清晰度、信息完整性和逻辑一致性三个维度的提示词质量评估框架，构建包含四个规范性等级(L0-L3)的大规模基准数据集CWE-BENCH-PYTHON，并在多个先进LLM上进行实验

**结果:** 实验显示提示词规范性降低与生成不安全代码概率增加存在明显正相关关系，Chain-of-Thought和Self-Correction等高级提示技术能有效减轻低质量提示词带来的安全风险

**结论:** 提升用户提示词质量是增强AI生成代码安全性的关键有效策略

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Your+Prompt+Poisoning+Code%3F+Defect+Induction+Rates+and+Security+Mitigation+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22944，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22944&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [246] [QuantumShield: Multilayer Fortification for Quantum Federated Learning](https://arxiv.org/abs/2510.22945)
*Dev Gurung, Shiva Raj Pokhrel*

**主要类别:** cs.CR

**AI概要:** 提出了一种量子安全的联邦学习框架，通过整合量子密钥分发、量子隐形传态等量子技术来防御量子攻击，为量子时代的联邦学习系统提供安全保障。


<details>
  <summary>更多</summary>
  
**动机:** 随着量子计算的发展，传统加密方法面临量子攻击威胁，需要建立能够抵御量子攻击的联邦学习安全架构。

**方法:** 集成并评估量子密钥分发(QKD)、量子隐形传态、密钥封装机制(KEM)和后量子密码学(PQC)等量子安全协议，构建安全的联邦学习生态系统。

**结果:** 通过理论建模和实验验证，证明了该框架在量子攻击下的安全性和性能表现。

**结论:** 该研究为下一代联邦学习系统在量子时代的安全运行奠定了坚实基础，提供了可扩展的量子安全解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuantumShield%3A+Multilayer+Fortification+for+Quantum+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22945&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a groundbreaking quantum-secure federated learning
(QFL) framework designed to safeguard distributed learning systems against the
emerging threat of quantum-enabled adversaries. As classical cryptographic
methods become increasingly vulnerable to quantum attacks, our framework
establishes a resilient security architecture that remains robust even in the
presence of quantum-capable attackers. We integrate and rigorously evaluate
advanced quantum and post-quantum protocols including Quantum Key Distribution
(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and
Post-Quantum Cryptography (PQC) to fortify the QFL process against both
classical and quantum threats. These mechanisms are systematically analyzed and
implemented to demonstrate their seamless interoperability within a secure and
scalable QFL ecosystem. Through comprehensive theoretical modeling and
experimental validation, this work provides a detailed security and performance
assessment of the proposed framework. Our findings lay a strong foundation for
next-generation federated learning systems that are inherently secure in the
quantum era.

</details>


### [247] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She*

**主要类别:** cs.CR

**AI概要:** 论文揭示了LLM提示压缩技术存在安全漏洞，攻击者可通过CompressionAttack框架利用该漏洞，实现高达80%的攻击成功率和98%的偏好翻转，现有防御措施无效。


<details>
  <summary>更多</summary>
  
**动机:** LLM驱动的代理常使用提示压缩来降低推理成本，但这种优化效率而非安全性的压缩模块可能被恶意输入操纵，导致语义漂移和LLM行为改变。

**方法:** 提出CompressionAttack框架，包含两种攻击策略：HardCom（使用离散对抗编辑进行硬压缩）和SoftCom（在潜在空间进行扰动实现软压缩）。

**结果:** 在多个LLM上的实验显示攻击成功率高达80%，偏好翻转率达到98%，攻击具有高度隐蔽性和可迁移性。VSCode Cline和Ollama的案例研究证实了实际影响。

**结论:** 当前防御措施被证明无效，突显了需要更强有力保护措施的必要性，提示压缩已成为新的攻击面需要重点关注。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CompressionAttack%3A+Exploiting+Prompt+Compression+as+a+New+Attack+Surface+in+LLM-Powered+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22963&send_immediately=true&force_search=false)

**原文摘要:** LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [248] [Advancing Honeywords for Real-World Authentication Security](https://arxiv.org/abs/2510.22971)
*Sudiksha Das, Ashish Kundu*

**主要类别:** cs.CR

**AI概要:** 本文分析了Honeywords技术（蜜词密码）在密码安全中的应用潜力，指出尽管该技术已研究十多年但仍未被主流认证平台采用，需要解决平坦性、集成性和可靠性等问题才能实现实际部署。


<details>
  <summary>更多</summary>
  
**动机:** Honeywords作为一种主动检测密码凭证滥用的方法具有潜力，但需要解决阻碍其实际应用的技术和架构问题。

**方法:** 通过分析现有的蜜词生成技术、攻击者建模和蜜检查器架构研究，识别已解决的问题和持续存在的障碍。

**结果:** 提出了一个可部署的框架，将抗攻击、上下文感知的蜜词创建与现有系统的简易集成相结合。

**结论:** Honeywords只有通过技术进步与安全简洁的架构、自适应响应处理和详细配置检查相结合，才能从学术理念转变为实用的安全工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Honeywords+for+Real-World+Authentication+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22971&send_immediately=true&force_search=false)

**原文摘要:** Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords
stored alongside a real password, appear to be a proactive method to help
detect password credentials misuse. However, despite over a decade of research,
this technique has not been adopted by major authentication platforms. This
position paper argues that the core concept of Honeywords has potential but
requires more research on issues such as flatness, integration, and
reliability, in order to be a practical deployable solution. This paper
examines the current work on Honeyword generation, attacker modeling, and
honeychecker architecture, analyzing the subproblems that have been addressed
and ongoing issues that prevent this system from being more widely used. The
paper then suggests a deployable framework that combines the
attacker-resilient, context-aware decoy creation that Honeywords provide with
easy integration into existing systems. Honeywords will only move from an
academic idea to a practical security tool if technical advances are paired
with secure and straightforward architectures, along with adaptive response
handling and detailed configuration checks.

</details>


### [249] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan, Zeng Li, Kunlin Cai, Liuhuo Wan, Ruomai Ren, Yiran Shen, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** 对VR应用商店中隐私实践的首个大规模多平台研究，发现VR应用存在严重的隐私合规问题，包括三分之一的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策。


<details>
  <summary>更多</summary>
  
**动机:** VR应用收集大量隐私敏感数据（如生物特征、用户行为和环境数据），但缺乏领域特定的数据管理法规，导致各应用商店的隐私实践存在显著差异。

**方法:** 使用自然语言处理、逆向工程和静态分析的多维度方法，评估了来自5个主要应用商店的6,565个VR应用的声明性和行为性隐私实践。

**结果:** 发现所有商店都存在显著的隐私合规问题：33%的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策，表明VR生态系统的隐私保护仍处于不成熟状态。

**结论:** 研究首次揭示了VR应用生态系统中隐私保护的现状，结果应引起开发者和用户的警惕，并鼓励应用商店运营商对VR应用的隐私合规实施更严格的监管。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Store+Privacy+Measurement+of+Virtual+Reality+App+Ecosystem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23024&send_immediately=true&force_search=false)

**原文摘要:** Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [250] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran, Suman Deb, Anupam Chattopadhyay*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种保护二进制神经网络模型参数的加密方法，利用物理不可克隆函数生成密钥对权重进行加密，在内存计算框架中实现高效的安全推理。


<details>
  <summary>更多</summary>
  
**动机:** 传统BNN模型参数加密保护方法在运行时解密会带来显著计算开销，违背内存计算集成存储与计算的核心原则，需要一种既能保护参数安全又不影响计算效率的方法。

**方法:** 使用物理不可克隆函数生成密钥，在将模型参数存储到交叉阵列前进行变换，实现对加密权重的直接推理操作，达到近似全同态加密的效果。

**结果:** 未经密钥的推理准确率降至15%以下，验证了保护策略的有效性，同时保持了计算效率。

**结论:** 该方法成功解决了BNN在内存计算架构中的安全保护问题，在保证安全性的同时最小化运行时开销，实现了安全与效率的平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Encrypted+Inference+using+Binarized+Neural+Networks+within+In-Memory+Computing+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23034&send_immediately=true&force_search=false)

**原文摘要:** Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [251] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang, Weiming Zhang, Nenghai Yu, Kejiang Chen*

**主要类别:** cs.CR

**AI概要:** RTMStega是一种基于熵驱动的语言隐写框架，通过排名自适应编码和上下文感知解压缩技术，显著提升隐写容量和安全性，同时保持文本质量。


<details>
  <summary>更多</summary>
  
**动机:** 当前语言隐写方法存在嵌入容量低和安全性不足的问题：传统修改方法会产生可检测异常，检索方法容量有限，生成式方法受限于令牌预测熵值过低。

**方法:** 提出RTMStega框架，整合基于排名的自适应编码和上下文感知解压缩技术，通过将秘密消息映射到令牌概率排名，并基于上下文感知的熵调整进行动态采样。

**结果:** 实验表明RTMStega将主流生成式隐写的有效载荷容量提升三倍，处理时间减少50%以上，同时保持高文本质量。

**结论:** RTMStega为安全高效的隐蔽通信提供了可信解决方案，在容量、速度和文本质量之间实现了良好平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+high-capacity+linguistic+steganography+based+on+entropy-driven+rank-token+mapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23035&send_immediately=true&force_search=false)

**原文摘要:** Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


### [252] [KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation](https://arxiv.org/abs/2510.23036)
*Xudong Yang, Jincheng Li, Kaiwen Xing, Zhenjia Xiao, Mingjian Duan, Weili Han, Hu Xiong*

**主要类别:** cs.CR

**AI概要:** KAPG是一个知识增强的密码猜测框架，通过结合泄露密码的内部统计知识和反映现实趋势的外部词汇知识，显著提升了密码猜测的准确性和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 现有密码猜测模型主要依赖泄露密码的模式，忽视了社会背景、文化趋势和流行词汇等外部因素对密码选择的影响，导致模型难以适应新兴密码趋势且效果随时间下降。

**方法:** 提出KAPG框架，使用密码前缀作为知识查找锚点，在生成过程中动态注入相关外部线索，同时保持真实密码的结构规律性。

**结果:** 在12个泄露数据集上的实验显示，KAPG在站内和跨站场景下分别比最先进模型平均提升36.5%和74.7%，并展示了良好的鲁棒性和计算效率。

**结论:** KAPG通过整合外部知识有效提升了密码猜测性能，同时开发的KAPSM密码强度计也在各种评估设置中显著优于现有工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KAPG%3A+Adaptive+Password+Guessing+via+Knowledge-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23036&send_immediately=true&force_search=false)

**原文摘要:** As the primary mechanism of digital authentication, user-created passwords
exhibit common patterns and regularities that can be learned from leaked
datasets. Password choices are profoundly shaped by external factors, including
social contexts, cultural trends, and popular vocabulary. Prevailing password
guessing models primarily emphasize patterns derived from leaked passwords,
while neglecting these external influences -- a limitation that hampers their
adaptability to emerging password trends and erodes their effectiveness over
time.
  To address these challenges, we propose KAPG, a knowledge-augmented password
guessing framework that adaptively integrates external lexical knowledge into
the guessing process. KAPG couples internal statistical knowledge learned from
leaked passwords with external information that reflects real-world trends. By
using password prefixes as anchors for knowledge lookup, it dynamically injects
relevant external cues during generation while preserving the structural
regularities of authentic passwords. Experiments on twelve leaked datasets show
that KnowGuess achieves average improvements of 36.5\% and 74.7\% over
state-of-the-art models in intra-site and cross-site scenarios, respectively.
Further analyses of password overlap and model efficiency highlight its
robustness and computational efficiency. To counter these attacks, we further
develop KAPSM, a trend-aware and site-specific password strength meter.
Experiments demonstrate that KAPSM significantly outperforms existing tools in
accuracy across diverse evaluation settings.

</details>


### [253] [zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks](https://arxiv.org/abs/2510.23060)
*Paritosh Ramanan, H. M. Mohaimanul Islam, Abhiram Reddy Alugula*

**主要类别:** cs.CR

**AI概要:** zkSTAR是一个基于zk-SNARKs的工业控制系统网络攻击检测框架，能够在保护数据隐私的同时为监管机构提供可验证的检测保证。


<details>
  <summary>更多</summary>
  
**动机:** 工业控制系统面临日益严重的网络威胁，监管机构需要验证检测机制有效性，但公用事业公司不愿披露敏感操作数据，存在隐私与监管的矛盾。

**方法:** 基于状态空间检测模型的残差统计假设检验方法，设计了双管齐下的zk-SNARK架构，确保状态空间动态的时间一致性和检测测试的统计一致性。

**结果:** 通过形式化分析框架的可靠性和零知识属性，并在真实ICS数据集上进行计算实验验证了实际可行性。

**结论:** 该工作为工业控制系统驱动的关键基础设施网络提供了一个可扩展的、保护隐私的监管合规替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是zkSTAR%3A+A+zero+knowledge+system+for+time+series+attack+detection+enforcing+regulatory+compliance+in+critical+infrastructure+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23060&send_immediately=true&force_search=false)

**原文摘要:** Industrial control systems (ICS) form the operational backbone of critical
infrastructure networks (CIN) such as power grids, water supply systems, and
gas pipelines. As cyber threats to these systems escalate, regulatory agencies
are imposing stricter compliance requirements to ensure system-wide security
and reliability. A central challenge, however, is enabling regulators to verify
the effectiveness of detection mechanisms without requiring utilities to
disclose sensitive operational data. In this paper, we introduce zkSTAR, a
cyberattack detection framework that leverages zk-SNARKs to reconcile these
requirements and enable provable detection guarantees while preserving data
confidentiality. Our approach builds on established residual-based statistical
hypothesis testing methods applied to state-space detection models.
Specifically, we design a two-pronged zk-SNARK architecture that enforces
temporal consistency of the state-space dynamics and statistical consistency of
the detection tests, allowing regulators to temporally verify alarm correctness
without visibility into utility-level data. We formally analyze the soundness
and zero knowledge properties of our framework and validate its practical
feasibility through computational experiments on real-world ICS datasets. As a
result, our work demonstrates a scalable, privacy-preserving alternative for
regulatory compliance for ICS driven critical infrastructure networks.

</details>


### [254] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi, Shotaro Ishihara*

**主要类别:** cs.CR

**AI概要:** Fast-MIA是一个用于高效评估大型语言模型成员推理攻击的Python库，解决了计算成本高和标准化实现缺乏的问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于对版权、安全和数据隐私的日益关注，LLM的成员推理攻击研究面临计算成本高和缺乏标准化实现两大障碍。

**方法:** 提供快速批量推理功能，并在统一评估框架下实现了代表性MIA方法，支持简单配置和可扩展性。

**结果:** 开发了开源的Fast-MIA工具（Apache 2.0许可证），支持可扩展和透明的LLM研究。

**结论:** Fast-MIA通过提供高效的批量推理和标准化MIA方法实现，为LLM成员推理攻击研究提供了重要的工具支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast-MIA%3A+Efficient+and+Scalable+Membership+Inference+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23074&send_immediately=true&force_search=false)

**原文摘要:** We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


### [255] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang, Xin Zhang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于大语言模型（LLM）的定向灰盒模糊测试方法，通过预测漏洞触发调用栈来改进种子优先级排序，显著提高了漏洞发现效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的定向灰盒模糊测试方法依赖静态分析的距离度量，存在概率计算不精确的问题，导致大量无关执行路径被误判，降低了模糊测试效率。

**方法:** 使用静态分析构建调用图识别可达目标位置的方法，然后利用LLM预测最可能触发漏洞的调用栈序列，优先选择执行路径与预测调用栈重叠度高的种子进行变异。

**结果:** 在真实程序测试中，该方法比基线方法快1.86到3.09倍触发漏洞，并发现了10个新漏洞和2个不完整修复，获得了10个CVE编号。

**结论:** 这是首个将LLM集成到定向灰盒模糊测试核心种子优先级机制的工作，证明了基于调用栈表示和LLM预测的方法能有效提高定向模糊测试的精确性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Imprecise+Distance+Metrics%3A+LLM-Predicted+Target+Call+Stacks+for+Directed+Greybox+Fuzzing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23101&send_immediately=true&force_search=false)

**原文摘要:** Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [256] [Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation](https://arxiv.org/abs/2510.23172)
*Mohsen Ahmadvand, Pedro Souto*

**主要类别:** cs.CR

**AI概要:** 论文针对Optimism派生管道在zkVM中的效率问题进行了重新设计，通过系统性识别低效环节并优化，实现了6.5倍的派生速度提升和3.5倍的整体加速，同时保持安全保证不变。


<details>
  <summary>更多</summary>
  
**动机:** Optimism派生管道原本为正确性和活跃性设计，直接移植到zkVM会产生显著开销，导致有效性证明成本过高。

**方法:** 系统性识别当前设计中的低效环节，分析其对证明成本的影响，并提供保持正确性的zk证明优化重新设计。

**结果:** 重新设计在zkVM中实现了最高6.5倍的派生速度提升，整体加速达到3.5倍。

**结论:** 通过针对zk证明的专门优化，可以在保持相同安全保证的前提下，显著降低Optimism派生管道的证明成本和提高效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Optimism%3A+Up+to+6.5x+Faster+zkVM+Validty+Proofs+via+Sparse+Derivation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23172&send_immediately=true&force_search=false)

**原文摘要:** The Optimism derivation pipeline is engineered for correctness and liveness,
not for succinct validity proofs. A straightforward port to a zkVM imposes
significant overheads, making validity proofs significantly more costly than
necessary. We systematically identify inefficiencies in the current design,
analyze their impact on proving costs, and provide a soundness-preserving
redesign tailored to zk proving. Our redesign achieves up to 6.5x faster
derivation inside zkVMs (3.5x overall speedup) while maintaining identical
safety guarantees.

</details>


### [257] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen, Qianqian Yang, Shuo Shao, Shunpu Tang, Zhiguo Shi, Shui Yu*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于差分隐私的新型安全语义通信框架，通过可学习模式的DP噪声保护图像传输中的隐私信息，在保证合法用户任务性能的同时有效降低窃听者的重建质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有安全语义通信方法依赖于限制性假设（如良好信道条件或窃听者模型先验知识），存在实际应用局限性，需要提供近似隐私保证的解决方案。

**方法:** 使用GAN反演方法提取解耦的语义表示，选择性地对私有语义表示添加可学习模式的差分隐私噪声（通过神经网络对抗训练实现），而非传统高斯或拉普拉斯噪声。

**结果:** 实验表明，相比传统DP方法和直接传输，该方法显著降低窃听者重建质量（LPIPS优势0.06-0.29，FPPSR优势0.10-0.86），同时仅对任务性能造成轻微影响。

**结论:** 该方法通过可学习模式DP噪声解决了传统DP不可逆性问题，实现了明确可控的安全级别调节，为语义通信提供了有效的隐私保护方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Semantic+Communication+over+Wiretap+Channels+with+Learnable+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23274&send_immediately=true&force_search=false)

**原文摘要:** While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>


### [258] [Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks](https://arxiv.org/abs/2510.23313)
*Yaokai Feng, Kouichi Sakurai*

**主要类别:** cs.CR

**AI概要:** 这篇综述系统梳理了网络入侵检测系统(NIDS)从传统方法到LLM集成的演变历程，总结了各种技术的现状、优势和局限，并探讨了LLM在NIDS中的实际应用价值。


<details>
  <summary>更多</summary>
  
**动机:** 随着网络威胁日益复杂，传统NIDS方法面临挑战，需要探索LLM等新技术在入侵检测中的应用潜力和实际可行性。

**方法:** 通过文献综述方法，系统分析从基于签名的传统方法、神经网络方法到LLM集成方法的发展历程，涵盖不同网络环境下的应用研究。

**结果:** 研究发现：1)基于签名的方法仍有重要价值；2)神经网络方法虽发展多年但实际部署仍面临挑战；3)LLM在NIDS中具有应用潜力但存在安全风险；4)需要构建领域特定的LLM模型。

**结论:** LLM为NIDS带来新的可能性，但仍需解决实际部署挑战和安全风险，未来应重点发展领域特定的LLM解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Network+Intrusion+Detection%3A+Evolution+from+Conventional+Approaches+to+LLM+Collaboration+and+Emerging+Risks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23313&send_immediately=true&force_search=false)

**原文摘要:** This survey systematizes the evolution of network intrusion detection systems
(NIDS), from conventional methods such as signature-based and neural network
(NN)-based approaches to recent integrations with large language models (LLMs).
It clearly and concisely summarizes the current status, strengths, and
limitations of conventional techniques, and explores the practical benefits of
integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS
in diverse environments is reviewed, including conventional network
infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods,
signature-based IDSs, continue to make significant contributions to modern
systems, despite their well-known weaknesses; 2) NN-based detection, although
considered promising and under development for more than two decades, and
despite numerous related approaches, still faces significant challenges in
practical deployment; 3) LLMs are useful for NIDS in many cases, and a number
of related approaches have been proposed; however, they still face significant
challenges in practical applications. Moreover, they can even be exploited as
offensive tools, such as for generating malware, crafting phishing messages, or
launching cyberattacks. Recently, several studies have been proposed to address
these challenges, which are also reviewed in this survey; and 4) strategies for
constructing domain-specific LLMs have been proposed and are outlined in this
survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.

</details>


### [259] [Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era](https://arxiv.org/abs/2510.23457)
*Saleh Darzi, Mirza Masfiqur Rahman, Imtiaz Karim, Rouzbeh Behnia, Attila A Yavuz, Elisa Bertino*

**主要类别:** cs.CR

**AI概要:** 本文分析了将后量子密码标准集成到5G基站认证中的可行性问题，发现直接采用存在协议约束和大签名尺寸的限制，提出了基于分层身份基阈值签名的过渡解决方案BORG


<details>
  <summary>更多</summary>
  
**动机:** 5G协议在初始引导阶段缺乏强大的基站认证机制，容易受到伪基站攻击，传统解决方案无法抵御量子攻击，需要研究后量子密码标准在5G认证中的适用性

**方法:** 对NIST后量子密码标准和传统数字签名方案进行全面的网络级性能特征分析，提出基于分层身份基阈值签名方案的BORG认证解决方案

**结果:** 发现直接采用PQC存在显著可行性问题，传统方法因证书链开销存在性能限制，BORG方案提供了后量子伪造检测和分布式信任机制

**结论:** 直接PQC集成不可行，BORG作为过渡解决方案能有效满足5G严格需求，为未来量子弹性认证提供路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Authentication+Against+Insecure+Bootstrapping+for+5G+Networks%3A+Feasibility%2C+Resiliency%2C+and+Transitional+Solutions+in+Post-Quantum+Era，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23457&send_immediately=true&force_search=false)

**原文摘要:** The 5G protocol lacks a robust base station authentication mechanism during
the initial bootstrapping phase, leaving it susceptible to threats such as fake
base station attacks. Conventional solutions, including digital signatures
based on Public Key Infrastructures (PKIs) and identity-based signatures, are
inadequate against quantum-capable adversaries. While integrating NIST's
Post-Quantum Cryptography (PQC) standards is a leading approach for quantum
resistance, their suitability for 5G base station authentication remains
unexplored. Moreover, current solutions are predominantly centralized and lack
security features such as distributed authentication. This work presents, to
our knowledge, the first comprehensive network-level performance
characterization of integrating NIST-PQC standards and conventional digital
signatures (including threshold and identity-based schemes) into 5G base
station authentication. Our findings reveal significant feasibility concerns,
with direct PQC adoption hindered by protocol constraints and large signature
sizes. We also highlight the performance limitations of conventional methods
due to the overhead of certificate chains. To mitigate these challenges, we
propose BORG, a transitional authentication solution based on a Hierarchical
Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG
offers post-mortem post-quantum forgery detection and distributed trust via
threshold and compact signatures, well-suited for 5G's stringent requirements.
Our performance analysis underscores an important warning on the infeasibility
of direct PQC integration and positions BORG as an effective transitional
solution toward future quantum-resilient 5G authentication.

</details>


### [260] [Towards a Functionally Complete and Parameterizable TFHE Processor](https://arxiv.org/abs/2510.23483)
*Valentin Reyes Häusler, Gabriel Ott, Aruna Jayasena, Andreas Peter*

**主要类别:** cs.CR

**AI概要:** 提出基于FPGA的TFHE全同态加密处理器硬件加速器，通过改进的可编程自举模块实现比现有技术快240%-480%的性能提升


<details>
  <summary>更多</summary>
  
**动机:** TFHE全同态加密方案虽然自举操作速度快，但同态电路评估的计算开销高，加密域计算比未加密计算慢几个数量级，阻碍了(T)FHE在敏感数据保护中的广泛应用

**方法:** 设计基于FPGA的TFHE处理器硬件加速器，实现完全在FPGA上处理数据的指令，并实现改进的可编程自举模块

**结果:** 实现了比当前最先进技术快240%到480%的自举操作每秒处理量，设计高效、紧凑且可扩展

**结论:** 该设计为实施完整的基于FPGA的TFHE处理器架构奠定了基础，有望克服全同态加密的计算性能瓶颈

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Functionally+Complete+and+Parameterizable+TFHE+Processor，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23483&send_immediately=true&force_search=false)

**原文摘要:** Fully homomorphic encryption allows the evaluation of arbitrary functions on
encrypted data. It can be leveraged to secure outsourced and multiparty
computation. TFHE is a fast torus-based fully homomorphic encryption scheme
that allows both linear operations, as well as the evaluation of arbitrary
non-linear functions. It currently provides the fastest bootstrapping operation
performance of any other FHE scheme. Despite its fast performance, TFHE suffers
from a considerably higher computational overhead for the evaluation of
homomorphic circuits. Computations in the encrypted domain are orders of
magnitude slower than their unencrypted equivalents. This bottleneck hinders
the widespread adoption of (T)FHE for the protection of sensitive data. While
state-of-the-art implementations focused on accelerating and outsourcing single
operations, their scalability and practicality are constrained by high memory
bandwidth costs. In order to overcome this, we propose an FPGA-based hardware
accelerator for the evaluation of homomorphic circuits. Specifically, we design
a functionally complete TFHE processor for FPGA hardware capable of processing
instructions on the data completely on the FPGA. In order to achieve a higher
throughput from our TFHE processor, we implement an improved programmable
bootstrapping module which outperforms the current state-of-the-art by 240\% to
480\% more bootstrappings per second. Our efficient, compact, and scalable
design lays the foundation for implementing complete FPGA-based TFHE processor
architectures.

</details>
