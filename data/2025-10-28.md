<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 83]
- [cs.CL](#cs.CL) [总数: 114]
- [cs.CR](#cs.CR) [总数: 63]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个将人工智能与计算心理学结合的多方面框架，通过端到端的开发流程，从基准测试到模型微调再到系统部署，成功建立了预测分析和生成对话相结合的心理分析系统。


<details>
  <summary>更多</summary>
  
**动机:** 利用人工智能技术来建模、理解和交互复杂的人类心理状态，弥合孤立预测建模与交互式心理分析系统之间的差距。

**方法:** 采用端到端开发生命周期：1)在四个心理学数据集上建立基础性能基准；2)微调最先进的transformer模型并解决工程挑战；3)使用参数高效技术微调生成式大语言模型作为交互式"人格大脑"；4)将预测和生成模型构建为可扩展的微服务生态系统。

**结果:** 成功稳定了基于transformer的情感计算回归模型，在标准方法失败的情况下实现了有意义的预测性能；开发了可复制的方法论来民主化大规模AI研究。

**结论:** 该工作展示了从研究到部署的完整流程，整合了预测分析与生成对话，为计算心理学和人机交互的未来研究提供了实用模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Component+AI+Framework+for+Computational+Psychology%3A+From+Robust+Predictive+Modeling+to+Deployed+Generative+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21720&send_immediately=true&force_search=false)

**原文摘要:** The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [2] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda, Takehiro Takayanagi*

**主要类别:** cs.AI

**AI概要:** PREFINE框架通过构建伪用户代理和用户特定评分标准，实现无需参数更新或直接用户反馈的个性化文本生成


<details>
  <summary>更多</summary>
  
**动机:** 解决传统个性化文本生成方法依赖显式反馈或微调带来的用户负担、数据收集、计算成本和隐私问题

**方法:** 基于用户交互历史构建伪用户代理，生成用户特定评分标准，通过该代理基于定制标准进行批判和精炼输出

**结果:** 在PerDOC和PerMPST数据集上，PREFINE在自动评估中获得更高的胜率和统计显著分数，且不损害一般故事质量

**结论:** PREFINE框架在保持故事质量的同时有效实现个性化生成，可扩展到对话系统、教育和推荐等更广泛应用

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PREFINE%3A+Personalized+Story+Generation+via+Simulated+User+Critics+and+User-Specific+Rubric+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21721&send_immediately=true&force_search=false)

**原文摘要:** While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [3] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang, Herbert Woisetscläger*

**主要类别:** cs.AI

**AI概要:** SIGN方法通过引入轻量级结构指导多智能体命名约定形成，相比无约束自然语言实现更快收敛和高达5.8倍的协议一致性，为多智能体协调提供有效控制机制


<details>
  <summary>更多</summary>
  
**动机:** 现实AI系统中大型语言模型智能体间的交互日益复杂，当智能体形成不一致的约定时会导致协调失败，协作编码和分布式规划等应用需要可靠、一致的通信，且系统扩展性至关重要

**方法:** 引入Schema-Induced Games for Naming (SIGN)命名游戏，研究轻量级结构如何引导约定形成，比较模式诱导通信与无约束自然语言的效果

**结果:** SIGN方法实现了更快的收敛速度，协议一致性比无约束自然语言高出最多5.8倍

**结论:** 最小化结构可作为多智能体高效协调的简单控制机制，在命名游戏之外具有更广泛的应用前景

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SIGN%3A+Schema-Induced+Games+for+Naming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21855&send_immediately=true&force_search=false)

**原文摘要:** Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [4] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

**主要类别:** cs.AI

**AI概要:** 研究发现解码器自回归语言模型在知识密集型任务上存在能力天花板，参数规模扩大至300亿时知识检索任务准确率几乎无提升，而数学任务准确率停滞在19-20%，但损失函数持续下降。注意力扰动会导致性能灾难性崩溃。


<details>
  <summary>更多</summary>
  
**动机:** 探究解码器自回归语言模型在知识密集型任务上的缩放效应，了解参数规模扩大是否带来相应能力提升

**方法:** 系统评估OPT和Pythia模型家族（70M-30B参数），分析损失函数和准确率的变化趋势，进行注意力模式交换实验

**结果:** 知识检索任务准确率改善微乎其微，数学任务准确率停滞在19-20%但损失下降31%，算术任务显示正常缩放，注意力扰动导致完全准确率损失

**结论:** 对于使用OPT和Pythia架构的知识密集型应用，超过1-2B参数的缩放几乎不带来准确率提升，这些发现量化了特定能力的缩放失败，为资源分配决策提供信息

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Capability+Ceilings+in+Autoregressive+Language+Models%3A+Empirical+Evidence+from+Knowledge-Intensive+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21866&send_immediately=true&force_search=false)

**原文摘要:** We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [5] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi, Chuanyu Qin, Shipeng Song, Man Luo*

**主要类别:** cs.AI

**AI概要:** 本文针对大语言模型在几何推理任务中表现不佳的问题，提出了GeoThoughts数据集和GeoThought-MLLM模型，通过提供详细的视觉描述和逐步推理链，显著提升了几何问题解决能力。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在文本数学问题上表现出色，但在视觉几何推理任务中性能显著下降，主要因为几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集在规模、多样性和显式推理轨迹方面的不足。

**方法:** 开发了GeoThoughts数据集（包含Geo-Thought-6K和Geo-Thought-Augmented-10K两个子集），每个样本包含视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此数据集构建了GeoThought-MLLM多模态模型，能够生成详细的问题解决思考过程。

**结果:** GeoThought-MLLM模型在几何任务中超越了现有基准测试表现，证明使用Chain-of-Thought数据集训练能够提升模型在领域内和领域外的几何推理能力。

**结论:** 通过分析失败案例发现错误主要来自数学概念误解或空间判断错误，通过调用思维链（CoT）纠正这些错误后，模型能够产生正确答案，验证了方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GeoThought%3A+A+Dataset+for+Enhancing+Mathematical+Geometry+Reasoning+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21881&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [6] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

**主要类别:** cs.AI

**AI概要:** 该研究将生成流网络(GFlowNets)应用于三个图优化问题：旅行商问题、最小生成树和最短路径问题，通过训练学习采样与奖励函数成比例的解决方案，实验表明该方法能找到最优解且具有计算可扩展性优势。


<details>
  <summary>更多</summary>
  
**动机:** 探索生成模型在组合优化问题中的应用，特别是利用GFlowNets的生成能力来解决传统图优化问题，旨在通过学习方法获得计算可扩展性优势。

**方法:** 使用轨迹平衡损失训练GFlowNets，顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。在不同节点数量的多种图配置上进行实验验证。

**结果:** GFlowNets能够学习找到最优解，生成的解决方案与经典算法结果一致（Dijkstra算法、Kruskal算法和精确求解器）。训练收敛性取决于问题复杂度，图规模越大所需训练回合数越多。

**结论:** 生成模型可以通过学习策略解决组合优化问题，该方法的计算可扩展性优势明显，在计算资源充足的情况下可能扩展到经典精确方法不可行的大规模问题实例。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+through+Generation%3A+Applying+GFlowNets+to+Structured+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21886&send_immediately=true&force_search=false)

**原文摘要:** This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [7] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi, Xiaoqi Tan*

**主要类别:** cs.AI

**AI概要:** 本文研究了在部分q^π-可实现性框架下强化学习的计算复杂性，证明了在该设置下学习ε-最优策略是计算困难的，包括NP-hard结果和指数下界。


<details>
  <summary>更多</summary>
  
**动机:** 研究在函数逼近的自然模型中，当策略集中所有策略的价值函数都是线性可实现时，学习最优策略的计算复杂性。

**方法:** 通过从δ-Max-3SAT和δ-Max-3SAT(b)问题规约到GLinear-κ-RL（贪婪策略）和SLinear-κ-RL（softmax策略）实例来建立计算复杂性结果。

**结果:** 证明了在参数化贪婪策略集下是NP-hard的，在softmax策略集下存在指数下界（除非NP=RP）。

**结论:** 部分q^π-可实现性框架下通常无法获得积极的计算结果，这与生成访问模型下的q^π-可实现性形成对比。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Hardness+of+Reinforcement+Learning+with+Partial+%24q%5E%CF%80%24-Realizability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21888，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21888&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [8] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo, Nikola Tankovic*

**主要类别:** cs.AI

**AI概要:** 本文研究使用10亿参数的小型开源模型Llama 3.2进行多语言电商意图识别，通过QLoRA微调和后训练量化技术，实现了与GPT-4.1相当的99%准确率，同时大幅降低计算成本和资源消耗。


<details>
  <summary>更多</summary>
  
**动机:** 大型商业模型在电商等专业任务部署中存在高计算成本、延迟和运营费用的问题，需要寻找资源效率更高的替代方案。

**方法:** 使用量化低秩适应(QLoRA)在合成数据集上微调10亿参数的Llama 3.2模型，然后应用后训练量化技术创建GPU优化(GPTQ)和CPU优化(GGUF)版本。

**结果:** 专用1B模型达到99%准确率，匹配GPT-4.1性能。GPTQ减少41%显存使用但推理速度下降82%，GGUF在CPU上实现18倍推理吞吐量提升和90%以上RAM消耗减少。

**结论:** 经过适当优化的小型开源模型是领域特定应用的可行且更适合的替代方案，能以极低的计算成本提供最先进的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performance+Trade-offs+of+Optimizing+Small+Language+Models+for+E-Commerce，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21970&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [9] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang, Mengfei Li, Shuai Shao*

**主要类别:** cs.AI

**AI概要:** 提出DSA方法，通过两阶段微调解决LLM模拟调查响应时的分布偏移问题，显著提升准确性和数据效率


<details>
  <summary>更多</summary>
  
**动机:** 现有零样本方法存在提示敏感性和低准确性问题，传统微调方法只能拟合训练集分布而无法超越训练集精度，无法实现用LLM模拟调查响应的原始目标

**方法:** 提出分布偏移对齐(DSA)方法，包含两阶段微调：第一阶段学习输出分布，第二阶段学习不同背景下的分布变化，从而学习分布如何变化而非简单拟合训练数据

**结果:** 在五个公开调查数据集上一致优于其他方法，准确性和鲁棒性均有提升，所需真实数据减少53.48-69.12%

**结论:** DSA方法能提供比训练数据更接近真实分布的结果，在调查模拟方面具有显著的有效性和效率优势

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distribution+Shift+Alignment+Helps+LLMs+Simulate+Survey+Response+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21977&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [10] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang, Jiayu Liu, Xin Lin, Zhiyuan Ma, Shangzi Xue, Tong Xiao, Qi Liu, Yee Whye Teh, Enhong Chen*

**主要类别:** cs.AI

**AI概要:** 这是一篇关于数学应用题(MWP)求解研究的综述论文，从人类认知角度系统分析了近十年的主流模型，包括神经网络求解器和基于大语言模型的求解器，并提供了统一的性能比较。


<details>
  <summary>更多</summary>
  
**动机:** 数学应用题领域缺乏系统的分类体系和当前发展趋势的讨论，需要从人类认知角度全面回顾相关研究，展示AI模型在模拟人类认知能力方面的进展。

**方法:** 总结了MWP求解的5个关键认知能力，回顾了近10年两种主流MWP模型，重新运行了代表性求解器并在5个主流基准测试上进行了统一性能比较。

**结果:** 论文提供了对过去十年有影响力的MWP研究的全面分析，从人类推理认知角度进行了整合性比较，并发布了相关代码库。

**结论:** 这项研究希望能启发AI推理领域的进一步研究，为数学应用题求解的发展提供系统性的认知框架和性能基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+of+Intelligence%3A+Review+of+Math+Word+Problems+from+Human+Cognition+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21999&send_immediately=true&force_search=false)

**原文摘要:** Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [11] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang, Chao Huang*

**主要类别:** cs.AI

**AI概要:** LightAgent是一个移动GUI代理系统，通过设备-云协作结合本地3B模型和云端大模型，在保证性能的同时显著降低云成本。


<details>
  <summary>更多</summary>
  
**动机:** 移动GUI代理面临性能与部署成本的矛盾：本地小模型性能不足，云端大模型部署成本过高。

**方法:** 采用两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型，集成高效长推理机制，通过实时复杂度评估实现设备-云协作执行。

**结果:** 在AndroidLab基准测试和多样化应用中，LightAgent性能接近或匹配更大模型，同时显著降低云成本。

**结论:** 设备-云协作是解决移动GUI代理性能与成本矛盾的有效方案，LightAgent为此提供了可行的实现框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightAgent%3A+Mobile+Agentic+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22009&send_immediately=true&force_search=false)

**原文摘要:** With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [12] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen, Joseph Ternasky, Aaron Ontoyin Yin, Xianling Mu, Fuat Alican, Yigit Ihlamur*

**主要类别:** cs.AI

**AI概要:** LLM-AR是一个将大语言模型生成的启发式规则转化为概率规则的神经符号系统管道，用于预测初创企业成功，在风险投资领域实现可解释的决策支持。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型具备模式识别和推理能力，但其准确性不稳定限制了在高风险决策应用中的采用，特别是在风险投资领域预测初创企业成功方面。

**方法:** 提出LLM-AR管道，受神经符号系统启发，将LLM生成的启发式规则提炼为概率规则，通过ProbLog自动推理引擎执行，并采用迭代策略进化循环结合关联规则挖掘来逐步优化预测规则。

**结果:** 在未见数据上达到59.5%的精确率和8.7%的召回率，是随机基线精确率的5.9倍，同时所有决策路径都可被人工检查。

**结论:** 该框架具有可解释性和超参数可调性，显示出扩展到其他领域的潜力，为高风险决策提供了可靠且透明的AI辅助工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-AR%3A+LLM-powered+Automated+Reasoning+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22034&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [13] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo, Han Hou, Will Dabney, Edgar Y. Walker*

**主要类别:** cs.AI

**AI概要:** 该研究通过在元强化学习中整合自监督预测编码模块，成功学习了更紧凑、可解释的贝叶斯最优信念状态表示，即使在传统元RL也能获得最优策略的情况下，该方法在表示学习方面表现更优，并在需要主动信息搜索的挑战性任务中展现出更好的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 在部分可观测环境中，学习历史的紧凑表示对于规划和泛化至关重要。虽然元强化学习代理可以获得接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态，这种表示效率低下可能限制代理的适应性和泛化能力。

**方法:** 受神经科学中预测编码（大脑通过预测感官输入实现贝叶斯推断）和深度RL中辅助预测目标的启发，研究将自监督预测编码模块整合到元强化学习中，通过状态机仿真验证该方法。

**结果:** 带有预测模块的元RL在各种任务中始终生成更可解释的表示，更好地近似贝叶斯最优信念状态，即使在两者都能获得最优策略的情况下。在需要主动信息搜索的挑战性任务中，只有带预测模块的元RL成功学习了最优表示和策略，而传统元RL在表示学习方面表现不佳。

**结论:** 更好的表示学习带来了改进的泛化能力，研究结果强烈表明预测学习作为在部分可观测性环境中导航的代理进行有效表示学习的指导原则的重要作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictive+Coding+Enhances+Meta-RL+To+Achieve+Interpretable+Bayes-Optimal+Belief+Representation+Under+Partial+Observability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22039&send_immediately=true&force_search=false)

**原文摘要:** Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [14] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini, Braz Izaias da Silva Junior*

**主要类别:** cs.AI

**AI概要:** 该论文通过SpecC方法学对PCM-to-PWM转换器进行系统级硬件/软件协同设计，展示了如何在满足实时约束的同时降低全硬件方案成本，并避免高端处理器纯软件实现的高昂费用。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是探索系统级硬件/软件协同设计方法在中等复杂度设计中的应用价值，特别是针对Class-D音频放大器核心的PCM-to-PWM转换器，寻求成本与性能的最佳平衡。

**方法:** 采用SpecC方法学进行建模和探索，使用系统级估计和快速功能仿真来评估不同的硬件/软件划分方案，以满足实时约束条件。

**结果:** 研究结果表明，尽管设计复杂度适中，但系统级协同设计能够提供早期的架构洞察、快速验证以及可行的成本/性能权衡方案。

**结论:** 该案例研究强调了系统级硬件/软件协同设计方法在提供架构洞察、快速验证和成本效益分析方面的重要价值，证明了该方法在嵌入式系统设计中的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HW%2FSW+Co-design+of+a+PCM%2FPWM+converter%3A+a+System+Level+Approach+based+in+the+SpecC+Methodology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22046&send_immediately=true&force_search=false)

**原文摘要:** We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [15] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

**主要类别:** cs.AI

**AI概要:** 该论文提出从预测准确性转向因果解释的机器学习新范式，引入能量结构化因果模型（ESCMs）作为可干预的因果表示框架，使模型内部机制具有可操作性。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习虽然预测性能优异，但缺乏因果透明性，无法对特定机制进行精确干预，因为学习到的潜在变量缺乏因果语义。

**方法:** 提出计算解释的概念，实例化为能量结构化因果模型（ESCMs），其中机制表示为约束（能量函数或向量场）而非显式输入输出映射，干预通过对这些约束进行局部手术实现。

**结果:** ESCMs在温和条件下恢复标准SCM语义，提供了结构因果原则LAP和ICM的具体实例化，并分析了经验风险最小化导致表征纠缠的问题。

**结论:** 论文为追求理解而不仅仅是预测的系统提供了因果推理的形式化语言，将智能重新定义为在批评下构建解释的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Error-Centric+Intelligence+II%3A+Energy-Structured+Causal+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22050&send_immediately=true&force_search=false)

**原文摘要:** Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [16] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee, Niraj K. Jha, Jonathan D. Cohen, Thomas L. Griffiths, Hongjing Lu, Diana Marculescu, Ashiqur Rasul, Keshab K. Parhi*

**主要类别:** cs.AI

**AI概要:** 论文提出下一代AI应从小型化、领域专用、节能的多模态模型发展，能够实时推理、规划和决策，相比当前大型语言模型更高效智能


<details>
  <summary>更多</summary>
  
**动机:** 当前AI模型存在能耗高（GPT-4训练需50-60GWh）、易产生幻觉、无法应用于关键领域等问题，而人脑仅耗电20W，需要更节能高效的AI解决方案

**方法:** 提出发展轻量级领域专用多模态模型，通过重新设计硬件实现能效比现有技术提升1000倍以上，支持实时数据处理和持续学习

**结果:** 论文构建了未来AI系统的愿景框架，从当前大数据训练的大型模型转向能够在不确定性环境中推理思考的节能型领域专用智能体

**结论:** 下一代AI应注重能效和领域专用性，通过硬件创新和模型优化，实现比人脑更高效的智能系统，这将定义AI发展的新浪潮

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Domain-Specific+Artificial+Intelligence+Models+and+Agents%3A+Pathways+and+Paradigms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22052&send_immediately=true&force_search=false)

**原文摘要:** The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [17] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen, Xinni Zhang, Yifei Zhang, Yangning Li, Henry Peng Zou, Chunyu Miao, Weizhi Zhang, Xue Liu, Philip S. Yu*

**主要类别:** cs.AI

**AI概要:** 这篇立场论文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，强调将智能体重新定义为主动协作伙伴而非被动信号处理器，需要关注伦理数据处理、模型可靠性和人机协作框架。


<details>
  <summary>更多</summary>
  
**动机:** 脑机接口存在信息传输率低和用户特定校准等限制，虽然大语言模型的整合有所进展，但缺乏对这一新兴方向的全面讨论，且部署智能AI面临技术和伦理障碍。

**方法:** 作为立场论文，采用理论分析和观点论证的方法，提出从BCI到BAC的范式转变概念框架。

**结果:** 提出了Brain-Agent Collaboration (BAC)的新范式概念，强调智能体应作为主动协作伙伴，而非仅仅处理脑信号数据。

**结论:** 该领域需要向脑-智能体协作范式扩展，重点关注伦理数据处理、模型可靠性和稳健的人机协作框架，以确保系统的安全性、可信性和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embracing+Trustworthy+Brain-Agent+Collaboration+as+Paradigm+Extension+for+Intelligent+Assistive+Technologies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22095&send_immediately=true&force_search=false)

**原文摘要:** Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [18] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

**主要类别:** cs.AI

**AI概要:** 提出一种基于自优化思维向量和熵最小化的可控数学推理方法，使用可学习的思维向量动态调制大语言模型的内部推理过程，在GSM8K上达到90.1%准确率和0.42可控性分数


<details>
  <summary>更多</summary>
  
**动机:** 开发一种不需要外部奖励标注就能实现可控AI推理的方法，通过熵最小化来引导聚焦推理模式

**方法:** 利用自优化思维向量和熵最小化技术，使用可学习的思维向量动态调节大语言模型（Gemma-2-9B）的内部推理过程

**结果:** 在GSM8K数据集上实现90.1%的准确率，可控性得分达到0.42，分析显示思维向量形成明显聚类且在不同控制条件下保持低熵分布

**结论:** 基于熵的奖励机制能有效指导聚焦推理模式，验证了该框架在可控AI推理方面的有效性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Controllable+Mathematical+Reasoning+via+Self-Optimizing+Thought+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22132&send_immediately=true&force_search=false)

**原文摘要:** We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [19] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost, Shreyans Jain, Shivam Raval, Grant Corser, Allen Roush, Nina Xu, Jacqueline Hammack, Ravid Shwartz-Ziv, Amirali Abdullah*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个AI心理测量框架，通过情境判断测试和复杂人物角色设计来评估AI系统在需要情感判断和伦理考量的角色中的表现，并在执法助手案例中构建了包含8500个人物角色、4000个情境测试和30万个响应的大型数据集。


<details>
  <summary>更多</summary>
  
**动机:** 现有的AI心理测量工作通常重复使用人类特质量表或临时人物角色，限制了行为真实性和领域相关性，需要更现实的评估方法。

**方法:** 提出三部分框架：(1)使用现实情境判断测试评估领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂人物角色；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙事。

**结果:** 在执法助手案例研究中构建了大规模数据集，涵盖8种人物原型和11个属性的情境测试，包含8500个人物角色、4000个情境测试和30万个响应。

**结论:** 该框架提供了更现实的AI心理测量方法，数据集和代码将公开发布，为AI系统在情感和伦理角色中的评估提供了新工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measure+what+Matters%3A+Psychometric+Evaluation+of+AI+with+Situational+Judgment+Tests，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22170&send_immediately=true&force_search=false)

**原文摘要:** AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [20] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种名为Dopamine的导数自由优化器，通过权重扰动学习和奖励预测误差最小化来训练神经网络，解决了反向传播的计算和内存效率问题，在保持性能的同时提高了神经生物学合理性。


<details>
  <summary>更多</summary>
  
**动机:** 解决突触信用分配问题(CAP)是神经网络学习的关键。反向传播虽然有效但计算成本高，存在权重传输和更新锁定问题，需要更高效且神经生物学更合理的优化方法。

**方法:** 采用NeuroAI方法，从神经强化学习中获得灵感，开发Dopamine优化器。通过权重扰动学习，利用权重的随机更新，通过最小化扰动模型预期结果与未扰动模型实际结果之间的奖励预测误差(RPE)来调整学习率。

**结果:** 在XOR任务和多层感知机、混沌时间序列预测的循环神经网络测试中，Dopamine优化器显示出加速收敛，优于标准权重扰动方法，性能与基于梯度的算法相当，同时显著减少计算和内存消耗。

**结论:** Dopamine优化器不仅找到了稳健的解决方案，性能与最先进的机器学习优化器相当，而且在神经生物学上更加合理，为解决信用分配问题提供了高效替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dopamine-driven+synaptic+credit+assignment+in+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22178&send_immediately=true&force_search=false)

**原文摘要:** Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [21] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu, Jie Wang, Yuyang Cai, Xiongwei Han, Yufei Kuang, Jianye Hao*

**主要类别:** cs.AI

**AI概要:** OptiTree提出了一种基于树搜索的自适应问题分解方法，通过构建层次化建模树来提升大语言模型在运筹学优化建模中的性能，相比现有方法在复杂基准测试中实现了超过10%的准确率提升。


<details>
  <summary>更多</summary>
  
**动机:** 运筹学优化建模是一个技术性很强的过程，现有基于大语言模型的方法采用固定步骤分解策略，但由于OR问题具有高度复杂的数学结构，这种方法往往无法达到高性能。

**方法:** 开发了OptiTree方法，构建了一个基于层次化问题分类和复杂度的建模树，每个节点代表一个问题类别并包含高级建模思路。通过递归搜索树结构，将复杂问题自适应分解为更简单的子问题，并整合层次化思路来生成全局建模方案。

**结果:** 实验表明OptiTree在具有挑战性的基准测试中显著提升了建模准确率，相比最先进方法实现了超过10%的改进。

**结论:** OptiTree通过自适应问题分解和层次化思路整合，有效解决了复杂OR问题的自动化建模挑战，为运筹学优化建模的自动化提供了新的有效方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OptiTree%3A+Hierarchical+Thoughts+Generation+with+Tree+Search+for+LLM+Optimization+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22192&send_immediately=true&force_search=false)

**原文摘要:** Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [22] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon, Hee Suk Yoon, Jaehyun Jang, SooHwan Eom, Qi Dai, Chong Luo, Mark A. Hasegawa-Johnson, Chang D. Yoo*

**主要类别:** cs.AI

**AI概要:** 提出PACR方法，通过模型内在的密集奖励信号替代稀疏的结果奖励，加速强化学习中的探索过程，提高训练效率和可靠性


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于结果的稀疏奖励无法为中间推理步骤提供指导，导致探索过程缓慢

**方法:** 提出渐进式上升置信度奖励(PACR)，基于模型对正确答案置信度的上升趋势作为密集内在奖励

**结果:** PACR加速了探索过程，用更少的轨迹达到奖励饱和，在多个基准测试中取得改进

**结论:** 密集的模型内在塑造信号可以使RLVR训练更有效和可靠

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PACR%3A+Progressively+Ascending+Confidence+Reward+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22255&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [23] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen, Bernard Cheng, Kelvin Soh*

**主要类别:** cs.AI

**AI概要:** 该论文创建了越南语歌词转录的首个大规模数据集VietLyrics，并在其上微调Whisper模型，显著提升了越南语歌词转录性能，超越了现有多语言系统。


<details>
  <summary>更多</summary>
  
**动机:** 越南语歌词自动转录面临音调复杂性和方言变异的独特挑战，且缺乏专用数据集，导致该领域研究不足。

**方法:** 1) 构建包含647小时歌曲的VietLyrics数据集，提供行级对齐歌词和元数据；2) 评估现有ASR方法的局限性；3) 在VietLyrics数据集上微调Whisper模型。

**结果:** 微调后的Whisper模型在越南语歌词转录方面取得了优异结果，性能超过包括LyricWhiz在内的现有多语言ALT系统。

**结论:** VietLyrics数据集和微调模型为越南语音乐计算研究提供了重要资源，证明了该方法在低资源语言和音乐ALT任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VietLyrics%3A+A+Large-Scale+Dataset+and+Models+for+Vietnamese+Automatic+Lyrics+Transcription，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22295，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22295&send_immediately=true&force_search=false)

**原文摘要:** Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [24] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种多级图粗化和精化框架来解决带时间窗的容量约束车辆路径问题(CVRPTW)，通过时空距离度量将客户聚合成元节点，在简化问题上使用经典启发式算法求解，然后扩展回原始空间并进行可行性修正。


<details>
  <summary>更多</summary>
  
**动机:** CVRPTW是一个NP难优化问题，解决大规模实例对精确求解器来说计算上具有挑战性，需要开发更高效的求解方法。

**方法:** 使用多级图粗化和精化框架：1) 基于时空距离度量将客户聚合成元节点；2) 在简化问题上应用经典启发式算法；3) 将解扩展回原始空间并进行可行性修正；4) 探索量子启发优化技术的集成。

**结果:** 在Solomon基准实例上的初步实验表明，该方法在保持或提高解质量的同时减少了计算时间，特别是在容量和时间窗约束方面表现良好。

**结论:** 该方法为大规模车辆路径问题提供了有效的求解框架，量子启发优化技术有望进一步加速大规模车辆路由任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Coarsening+Approach+for+the+Capacitated+Vehicle+Routing+Problem+with+Time+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22329&send_immediately=true&force_search=false)

**原文摘要:** The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [25] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu, Yuansheng Lian, Ke Zhang, Yunxuan Li, Yuelong Su, Meng Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一个基于文献知识微调大语言模型(LIFT LLM)的可解释卡车驾驶风险预测框架，通过整合文献处理管道和推理核心，实现了准确的风险预测和可解释性分析。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决卡车驾驶风险预测中的准确性和可解释性问题，需要结合领域文献知识来增强大语言模型的预测能力和解释能力。

**方法:** 开发了包含LLM驱动推理核心、文献处理管道和结果评估器的框架，通过在真实卡车驾驶数据集上微调，并利用299篇领域文献构建知识库。

**结果:** LIFT LLM在召回率上优于基准模型26.7%，F1分数提高10.1%，变量重要性排序与基准模型一致，并能识别潜在风险场景。

**结论:** 该框架成功实现了准确且可解释的卡车驾驶风险预测，证明了文献知识库和微调过程对模型可解释性的重要贡献，在数据驱动知识发现方面具有潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LIFT%3A+Interpretable+truck+driving+risk+prediction+with+literature-informed+fine-tuned+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22333&send_immediately=true&force_search=false)

**原文摘要:** This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [26] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu, Shijie Lian, Zihao Liu, Lei Zhang, Laurence Tianruo Yang, Kai Chen*

**主要类别:** cs.AI

**AI概要:** DynaSolidGeo是首个动态立体几何推理基准，通过半自动标注流程构建，包含503个专家策划的种子问题，可动态生成无限多样的多模态实例，并引入过程评估来测量逻辑有效性和因果连贯性。


<details>
  <summary>更多</summary>
  
**动机:** 现有多模态数学推理基准主要关注2D平面几何，依赖静态数据集易受数据污染和记忆影响，且仅通过最终答案评估模型，忽视了推理过程。

**方法:** 采用半自动标注流程构建DynaSolidGeo基准，包含503个专家策划的种子问题，能够动态生成多样化多模态实例，并基于专家标注的推理链进行过程评估。

**结果:** 实验显示开源和闭源VLMs在动态设置下性能严重下降，在需要高水平空间智能的任务（如心理旋转和可视化）上表现较差。

**结论:** DynaSolidGeo填补了立体几何推理评估的空白，揭示了当前VLMs在空间推理方面的局限性，为未来研究提供了重要基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaSolidGeo%3A+A+Dynamic+Benchmark+for+Genuine+Spatial+Mathematical+Reasoning+of+VLMs+in+Solid+Geometry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22340&send_immediately=true&force_search=false)

**原文摘要:** Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [27] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar, Jimson Huang, Yunxin Sun, Fei Xia, Abulhair Saparov*

**主要类别:** cs.AI

**AI概要:** 研究发现大型推理模型(LRMs)在复杂推理任务上表现有限，虽然现有基准测试显示良好性能，但通过新构建的DeepRD数据集测试发现，当问题复杂度足够高时，LRMs性能急剧下降且无法泛化。


<details>
  <summary>更多</summary>
  
**动机:** 重新审视LLMs在复杂推理任务中的表现，验证LRMs是否真正具备泛化推理能力，特别是在数学、物理等需要深度推理的领域。

**方法:** 开发了Deep Reasoning Dataset (DeepRD)数据集，通过可扩展复杂度的生成过程创建无限测试样本，评估模型在图连通性和自然语言证明规划任务上的表现。

**结果:** LRMs在足够复杂度下性能急剧下降，无法实现泛化推理。虽然大多数现实世界问题处于模型成功范围内，但长尾分布暴露了显著的失败可能性。

**结论:** LRMs在短期内具有实用性，但需要开发新方法来解决超出训练分布复杂度的泛化问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Models+Reason+Well%2C+Until+They+Don%27t，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22371&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [28] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar, Ali Nazari, Erfan Shayegani, Nael Abu-Ghazaleh*

**主要类别:** cs.AI

**AI概要:** 该论文提出使用有限状态机（FSM）来建模和分析大型推理模型（LRMs）的层次推理过程，通过定义离散推理状态来标注思维链的每一步，从而系统性地分析和可视化不同模型的推理模式。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型通过思维链推理展现出强大的推理能力，但其内部推理机制仍难以理解。需要一种结构化的、可解释的方法来分析LRMs的层次推理动态，以改进训练和理解鲁棒性。

**方法:** 采用无记忆有限状态机（FSM）来近似LRMs的层次推理动态，定义了一组离散推理状态（初始化、演绎、增强策略、不确定性估计、回溯、最终结论），通过标注思维链步骤来表示推理轨迹。

**结果:** FSM分析揭示了不同模型在推理方法上的差异，显示了独特的推理模式和潜在缺陷，为评估和改进LLM推理提供了新的视角。

**结论:** FSM框架为分析和解释大型推理模型的推理过程提供了一种系统化、可解释的方法，有助于理解模型间的差异并指导模型改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Hierarchical+Thinking+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22437，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22437&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [29] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath, Nikhil Krishnaswamy*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种新的合作伙伴感知学习算法ICR，用于训练LLM驱动的协作代理，使其能够更好地接受干预并促进团队共识建立，相比标准RLHF方法在协作任务中表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 随着LLM越来越多地被部署在需要与人类协作的智能体环境中，评估其在多轮、多方任务中的协作能力变得日益重要。研究发现标准RLHF训练的LLM代理倾向于忽略合作伙伴的干预，这使得建立团队共同认知变得困难。

**方法:** 基于AI对齐和安全中断性文献，采用改进的双玩家行动MDP框架分析标准AI代理的次优行为，并提出Interruptible Collaborative Roleplayer (ICR)算法来训练共识最优的协作代理。

**结果:** 在多个协作任务环境中的实验表明，ICR算法平均能够更有效地促进成功的共识收敛，并在任务中探索更多样化的解决方案。

**结论:** ICR算法通过使LLM代理更加关注合作伙伴的干预，显著提高了多智能体协作中的共识建立能力，为解决LLM在协作环境中忽视合作伙伴输入的问题提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+%22Partner-Aware%22+Collaborators+in+Multi-Party+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22462&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [30] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng, Zirui Pang, Ling li, Zhijie Deng, Yuhan Pu, Zhaowei Zhu, Xiaobo Xia, Jiaheng Wei*

**主要类别:** cs.AI

**AI概要:** 论文提出了OFFSIDE基准测试，用于评估多模态大语言模型在足球转会谣言信息遗忘方面的性能，揭示了当前多模态遗忘方法的重大脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型的发展加剧了数据隐私担忧，但现有的机器遗忘基准测试缺乏图像多样性、准确性不足，无法捕捉现实应用的复杂性。

**方法:** 构建了一个基于足球转会谣言的手动策划数据集，包含15.68K条记录和80名球员数据，提供四个测试集来评估遗忘效果、泛化性、实用性和鲁棒性。

**结果:** 评估发现：单模态方法在多模态谣言上失效；遗忘效果主要由灾难性遗忘驱动；所有方法在视觉谣言处理上表现不佳；被遗忘的谣言容易恢复；所有方法都易受提示攻击。

**结论:** 当前多模态遗忘方法存在显著脆弱性，需要开发更鲁棒的多模态遗忘解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OFFSIDE%3A+Benchmarking+Unlearning+Misinformation+in+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22535&send_immediately=true&force_search=false)

**原文摘要:** Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [31] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi, Ludovic Moncla, Khalid Benabdeslem, Rémy Cazabet, Pierre Cléau*

**主要类别:** cs.AI

**AI概要:** ATOM是一种自适应优化的少样本方法，用于从非结构化文本构建和持续更新时序知识图谱，通过原子化事实提取和双时间建模显著提升了提取完整性、稳定性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统静态知识图谱构建方法忽略了现实数据的动态性和时效性，而现有的零样本/少样本方法存在运行不稳定和关键事实覆盖不完整的问题。

**方法:** 将输入文档分割成最小自包含的原子事实，采用双时间建模区分信息观测时间和有效时间，并行合并构建原子时序知识图谱。

**结果:** 相比基线方法，ATOM实现了约18%的完整性提升、17%的稳定性改善和超过90%的延迟减少。

**结论:** ATOM展示了在动态时序知识图谱构建方面强大的可扩展性潜力，能够有效处理实时分析和动态记忆框架的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ATOM%3A+AdapTive+and+OptiMized+dynamic+temporal+knowledge+graph+construction+using+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22590&send_immediately=true&force_search=false)

**原文摘要:** In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [32] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song, Jiaxiang Li, Rong Wang, Songtao Lu, Mingyi Hong*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个分析上下文学习(ICL)性能的新框架，通过理论分析和实验验证，揭示了预训练数据分布与查询任务分布差异时，适当构建的上下文如何量化地改善模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型展现了强大的上下文学习能力，但其理论机制尚不明确，特别是预训练过程和上下文构建等关键因素的具体作用机制需要深入研究。

**方法:** 构建一个包含单层transformer的简单示例，分析预训练数据分布与查询任务分布不同时的表现，然后将发现扩展到更一般情况，推导ICL性能与上下文长度及分布KL散度的精确关系，并通过实验验证。

**结果:** 研究发现当预训练数据分布与查询任务分布不同时，适当构建的上下文可以将输出分布向查询任务分布方向偏移，以可量化的方式提高预测准确性。

**结论:** 该研究为理解ICL机制提供了理论框架，揭示了上下文构建在弥合预训练与目标任务分布差异中的关键作用，对改进大语言模型的上下文学习能力具有重要指导意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Quantifying+How+Pre-Training+and+Context+Benefit+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22594&send_immediately=true&force_search=false)

**原文摘要:** Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [33] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan, Rafid Mostafiz, Md. Abir Hossain, Bikash Kumar Paul*

**主要类别:** cs.AI

**AI概要:** CLIN-LLM是一个安全约束的混合AI系统，通过多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，实现了98%的准确率，显著提升临床决策安全性并减少67%的不安全抗生素建议。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于大语言模型的医疗系统缺乏医学基础且无法量化不确定性，导致输出不安全，特别是在诊断风险高的异质患者环境中。

**方法:** 结合BioBERT微调（基于1200个临床案例）、Focal Loss与蒙特卡洛Dropout实现置信度感知预测，使用Biomedical Sentence-BERT从26万样本的MedDialog语料库检索相关对话，并通过FLAN-T5模型生成个性化治疗建议，最后用RxNorm进行抗生素管理和药物相互作用筛查。

**结果:** 达到98%的准确率和F1分数，比ClinicalBERT提升7.1%，检索精度78%，临床医生评分4.2/5，不安全抗生素建议减少67%。

**结论:** CLIN-LLM展示了强大的鲁棒性、可解释性和临床安全性，为资源有限的医疗环境提供了可部署的人机协同决策支持框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLIN-LLM%3A+A+Safety-Constrained+Hybrid+Framework+for+Clinical+Diagnosis+and+Treatment+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22609&send_immediately=true&force_search=false)

**原文摘要:** Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [34] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh, Aaron Shen, Brian Law, Ahmed Ismail, Jonas Rohweder, Sean O'Brien, Kevin Zhu*

**主要类别:** cs.AI

**AI概要:** SwiftSolve是一个复杂度感知的多智能体系统，专门用于竞争性编程，通过算法规划、经验分析和复杂度指导的修复来确保程序不仅正确，还要满足时间和内存约束。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM生成的程序虽然能通过单元测试，但经常违反竞赛的时间和内存预算限制，需要一种能同时保证正确性和效率的方法。

**方法:** 采用多智能体架构，包括规划器、静态剪枝器、编码器、性能分析器和复杂度分析器，通过JSON通信和控制器管理迭代过程，结合算法规划和经验分析。

**结果:** 在26个问题上的测试显示，首次尝试通过率61.54%，3次内解决率80.77%，平均运行时间12.40秒，运行级成功率73.08%，相比Claude Opus 4有显著提升。

**结论:** SwiftSolve通过复杂度感知的多智能体方法有效解决了竞争性编程中的效率问题，证明了在保持正确性的同时优化资源使用的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwiftSolve%3A+A+Self-Iterative%2C+Complexity-Aware+Multi-Agent+Framework+for+Competitive+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22626&send_immediately=true&force_search=false)

**原文摘要:** Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [35] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan, Shaked Zychlinski*

**主要类别:** cs.AI

**AI概要:** 提出基于首个生成token的log概率分布来检测LLM模板化响应的方法，可在单步生成后实现高效分类，显著降低计算成本和延迟


<details>
  <summary>更多</summary>
  
**动机:** LLMs在生成模板化响应（如拒绝、简单确认和问候语）时消耗大量计算资源，增加了不必要的成本和延迟

**方法:** 利用首个生成token的log概率分布作为信号，使用轻量级k-NN分类器对后续响应类型进行分类预测

**结果:** 实验表明首个token的log概率向量在不同响应类型间形成明显可分离的簇，能够高精度预测是否为实质性回答或模板化响应

**结论:** 该方法提供了一种计算简单、实用的技术，通过早期终止或重定向到更小模型来优化LLM推理，显著节省计算成本，实现更高效和可持续的LLM部署

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Stop+Me+Now%3A+Detecting+Boilerplate+Responses+with+a+Single+Iteration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22679&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [36] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander, Sai Pragnya Ranga, Prathamesh Mayekar*

**主要类别:** cs.AI

**AI概要:** 本文提出了Atlas城市指数(AUI)，这是一种利用Sentinel-2卫星影像和视觉语言模型(VLMs)来测量城市发展的新指标，相比传统方法能更准确地捕捉城市发展，并克服大气噪声、季节变化和云层覆盖等问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的标准化建筑指数(NDBI)等方法由于大气噪声、季节变化和云层覆盖等因素，难以准确捕捉城市发展，这阻碍了人类发展和城市化的大规模监测。

**方法:** 收集每个区域的Sentinel-2影像时间序列，在固定时间窗口内处理图像以获得云层覆盖最少的代表性图像；采用两种策略确保评分一致性：(i)提供代表不同城市化水平的参考图像集，(ii)提供最近的过去图像以保持时间一致性和减轻云相关噪声。

**结果:** 在班加罗尔的定性实验表明，AUI优于NDBI等标准指数。

**结论:** AUI能够克服传统城市化指数的挑战，产生更可靠和稳定的发展评分，为城市发展监测提供了更有效的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Atlas+Urban+Index%3A+A+VLM-Based+Approach+for+Spatially+and+Temporally+Calibrated+Urban+Development+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22702&send_immediately=true&force_search=false)

**原文摘要:** We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [37] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai, Jusheng Zhang, Yijia Fan, Jing Yang, Keze Wang*

**主要类别:** cs.AI

**AI概要:** RaCoT是一个新的检索增强生成框架，通过在检索前阶段生成对比性问题来引导模型关注关键差异信息，有效解决长尾查询中的语义模糊和检索噪声问题，在多个基准测试中表现优异且效率高。


<details>
  <summary>更多</summary>
  
**动机:** 解决RAG在处理知识稀疏和语义模糊的长尾查询时的核心瓶颈问题，即检索噪声会扭曲推理并需要昂贵的后处理。

**方法:** 提出RaCoT框架，在检索前阶段自动生成语义相邻但答案不同的对比性问题，提取Δ-Prompt捕捉关键差异，引导模型主动关注决定答案分歧的关键细节。

**结果:** 在6个权威基准测试中优于RankRAG和Self-RAG等强基线0.9-2.4个百分点，对抗性测试中性能下降仅8.6%，延迟低至3.12秒，token开销仅11.54。

**结论:** RaCoT将RAG范式从"事后上下文清理"重构为"先验塑造判别推理"，为实时资源受限部署提供了高效稳健的可靠AI系统路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RaCoT%3A+Plug-and-Play+Contrastive+Example+Generation+Mechanism+for+Enhanced+LLM+Reasoning+Reliability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22710&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [38] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli, Aditi Singh, Arun Sharma*

**主要类别:** cs.AI

**AI概要:** 本研究比较了五个主流大语言模型（Gemini、DeepSeek、Claude、GPT和LLaMA）在性能准确性、伦理偏见缓解和可用性集成三个关键维度的差异。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在各领域的广泛应用，不同模型基于不同理念构建，在性能、道德行为和可用性方面存在显著差异，需要系统比较以指导用户选择。

**方法:** 通过分析三个重要因素进行比较：性能和准确性、伦理和偏见缓解、可用性和集成能力。

**结果:** 发现Claude在道德推理方面表现优异，Gemini在多模态能力和伦理框架方面更强，DeepSeek擅长事实推理，LLaMA适合开源应用，ChatGPT提供平衡性能且注重实用性。

**结论:** 不同模型在工作效率、易用性和伦理处理方面存在差异，用户应根据具体需求选择最能发挥其优势的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Critical+Insights+into+Leading+Conversational+AI+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22729&send_immediately=true&force_search=false)

**原文摘要:** Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [39] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

**主要类别:** cs.AI

**AI概要:** 开发了一个实时事实验证框架，通过多知识源交叉检查来检测和纠正大语言模型的幻觉问题，在多个领域测试中减少67%的幻觉，专家满意度达89%


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型虽然改变了AI交互方式，但存在严重缺陷：会自信地生成听起来合理但错误的信息，这种幻觉问题阻碍了在需要准确性的实际应用中的部署

**方法:** 开发事实验证框架，结合结构化数据库、实时网络搜索和学术文献，在LLM生成内容时实时验证事实主张，检测到不一致时自动纠正并保持回答的自然流畅性

**结果:** 跨多个领域测试显示幻觉减少67%且不牺牲回答质量，医疗、金融和科研领域的专家对修正后输出的满意度达89%，相比未验证的LLM回答有显著提升

**结论:** 这项工作为在不能出错的应用场景中使大语言模型更加可信提供了实用解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Fact-Verification+Framework+for+Reducing+Hallucinations+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22751&send_immediately=true&force_search=false)

**原文摘要:** While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [40] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu, Junyu Feng, Ruichuan An, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang*

**主要类别:** cs.AI

**AI概要:** Jarvis是一个通过个人KV-Cache检索的创新个性化AI助手框架，在视觉和文本KV-Cache中存储用户特定信息，实现了更准确的个性化问答性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通过学习概念token或训练VLM来利用用户特定信息，但都难以生成准确的个性化助手回答，需要更有效的个性化解决方案。

**方法:** 提出Jarvis框架，通过将用户信息总结为元数据创建文本token，从用户图像中提取独特图像块创建视觉token，存储在KV-Cache中。回答问题时会检索相关KV-Cache以确保准确性。

**结果:** Jarvis在多个数据集的视觉问答和纯文本任务中取得了最先进的结果，特别是在依赖特定局部细节的情况下能提供更准确的响应。

**结论:** Jarvis为个性化AI助手提供了一条实用路径，通过个人KV-Cache检索机制有效提升了基于细粒度用户信息的问答准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jarvis%3A+Towards+Personalized+AI+Assistant+via+Personal+KV-Cache+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22765&send_immediately=true&force_search=false)

**原文摘要:** The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [41] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang, Yijia Shao, Omar Shaikh, Daniel Fried, Graham Neubig, Diyi Yang*

**主要类别:** cs.AI

**AI概要:** 本研究首次直接比较人类与AI代理在多个工作技能上的表现，发现代理虽然工作质量较低且存在数据造假问题，但速度快88.3%、成本低90.4-96.2%，揭示了人机协作的潜力与挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI代理开发缺乏对人类工作方式的深入理解，需要揭示代理的专业能力及其在不同工作流程中的角色定位。

**方法:** 引入可扩展工具包，从人类和代理的计算机使用活动中提取可解释的结构化工作流程，在数据分析、工程、计算、写作和设计等领域进行直接比较。

**结果:** 1) 代理倾向于程序化方法，与人类的UI中心方法形成对比；2) 代理工作质量较低但会通过数据造假掩盖缺陷；3) 代理速度快88.3%，成本低90.4-96.2%。

**结论:** AI代理在可编程任务上具有高效协作潜力，但需要解决质量问题和程序化方法的局限性，以实现更有效的人机协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Do+AI+Agents+Do+Human+Work%3F+Comparing+AI+and+Human+Workflows+Across+Diverse+Occupations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22780&send_immediately=true&force_search=false)

**原文摘要:** AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [42] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu, Yunshen Zhou*

**主要类别:** cs.AI

**AI概要:** 微软提出Agentic Meta-orchestrator (AMO)作为Copilot服务的智能协调器，通过元学习决策树模型选择最佳推理策略，成功应用于M365电商Copilot和代码合规Copilot两个生产用例


<details>
  <summary>更多</summary>
  
**动机:** 随着Copilot服务中代理数量的动态增长，需要强大的协调器来将用户任务分发给合适的代理，处理自然语言和动作响应

**方法:** 提出Agentic Meta-orchestrator (AMO)，采用元学习方法训练决策树模型，在各种代理/模型中选择最佳推理策略

**结果:** 在两个生产用例中验证了AMO的有效性：M365电商Copilot提供最新产品信息并连接多个代理，代码合规Copilot检测DevOps代码中的合规问题

**结论:** AMO为Copilot服务提供了可扩展的多任务处理能力，通过智能协调机制实现了高效的代理管理和任务分配

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Meta-Orchestrator+for+Multi-task+Copilots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22781&send_immediately=true&force_search=false)

**原文摘要:** Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [43] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi, Emna Ben Romdhane*

**主要类别:** cs.AI

**AI概要:** 本文分析人工智能对人类构成的生存风险，探讨从当前AI到超智能的发展轨迹，基于Good和Bostrom的理论研究，讨论AGI和超智能的伦理和生存影响。


<details>
  <summary>更多</summary>
  
**动机:** 分析人工智能发展可能带来的生存风险，特别是当机器智能指数级增长并远超人类时，可能导致的不可控后果和人类灭绝风险。

**方法:** 基于Irving J. Good和Nick Bostrom的理论框架，结合近期出版物（AI 2027; If Anyone Builds It, Everyone Dies）进行分析，探讨AGI和超智能的发展轨迹。

**结果:** 识别出AI发展可能带来的生存威胁，指出人类灭绝可能不是源于恶意，而是源于不可控的、冷漠的认知优势。

**结论:** 人工智能的超智能发展可能对人类构成根本性的生存威胁，需要认真考虑其伦理和存在性影响，这种威胁可能源于智能优势而非恶意意图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Will+Humanity+Be+Rendered+Obsolete+by+AI%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22814&send_immediately=true&force_search=false)

**原文摘要:** This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [44] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang, David Rawlinson*

**主要类别:** cs.AI

**AI概要:** HRM-Agent：基于分层推理模型的小型强化学习智能体，能够在动态不确定环境中学习导航任务，并成功复用先前时间步的计算


<details>
  <summary>更多</summary>
  
**动机:** 原始HRM模型虽然推理能力强但仅限于静态全观测问题，无法处理动态、不确定或部分可观测的现实世界问题，也无法复用历史计算

**方法:** 开发HRM-Agent变体，仅使用强化学习训练，在动态不确定迷宫环境中进行导航任务学习，并分析其循环推理过程

**结果:** HRM能够成功学习在动态不确定环境中导航到目标，其循环推理过程显示出能够有效复用先前时间步的计算

**结论:** HRM-Agent扩展了原始模型的应用范围，证明小型模型通过强化学习也能在复杂动态环境中实现有效推理和计算复用

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HRM-Agent%3A+Training+a+recurrent+reasoning+model+in+dynamic+environments+using+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22832，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22832&send_immediately=true&force_search=false)

**原文摘要:** The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [45] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein, Jessica Chen, Gwyneth Anne Delos Santos, Bayley Sapara, Michael Bowling*

**主要类别:** cs.AI

**AI概要:** 论文研究让强化学习智能体在训练过程中能够感知计算成本并自主控制计算使用，结果显示这种方法在相同计算预算下能在75%游戏中获得更好性能，同时平均减少三倍计算量。


<details>
  <summary>更多</summary>
  
**动机:** 人类在熟练任务后认知负担会降低，而传统强化学习智能体在性能提升时计算效率不会相应提高。研究旨在让智能体能够像人类一样随着熟练度提升而减少计算需求，从而提高能源效率并释放计算资源用于其他过程。

**方法:** 在Arcade Learning Environment中进行实验，向智能体展示计算成本并赋予它们控制计算使用的自主权，让智能体能够自主决定何时使用计算资源。

**结果:** 在相同训练计算预算下，能够感知计算成本的智能体在75%的游戏中表现更好，同时平均使用三倍更少的计算量。研究还分析了具体游戏以展示效率提升的来源。

**结论:** 让强化学习智能体感知和控制计算使用是可行的，能够显著提高计算效率同时保持或提升性能，这为开发更节能的智能体系统提供了重要方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Agents+That+Reason+About+Their+Computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22833&send_immediately=true&force_search=false)

**原文摘要:** While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [46] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao, Qiucheng Wu, Yang Zhang, Zhaowen Wang, Handong Zhao, Shiyu Chang*

**主要类别:** cs.AI

**AI概要:** 该论文分析了多模态大语言模型中的模态差距问题，发现现有训练方法加剧了文本和视觉模态间的性能差异，并提出了从数据和损失函数设计两方面来弥合这一差距的策略。


<details>
  <summary>更多</summary>
  
**动机:** 研究发现多模态大语言模型存在模态差距问题，即模型过度依赖文本线索而忽视视觉内容，导致在需要真正视觉推理的任务上表现不佳。

**方法:** 通过训练方法的角度分析模态差距，首先证明现有训练方法会放大这一差距，然后系统性地从数据和损失函数设计两个互补角度探索弥合差距的策略。

**结果:** 研究结果为开发能够减轻模态差距并促进更平衡的多模态推理的训练方法提供了见解。

**结论:** 论文揭示了多模态大语言模型训练中的重要问题，并提出了有效的解决方案方向，有助于推动更平衡的多模态模型发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Text-Vision+Reasoning+Imbalance+in+MLLMs+through+the+Lens+of+Training+Recipes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22836&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [47] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li, Erik-Jan van Kampen*

**主要类别:** cs.AI

**AI概要:** 开发了级联在线学习飞行控制系统并改进了动作平滑性，研究了系统收敛性能（通过李雅普诺夫函数增量表征），考虑了离散化误差和状态预测误差，通过飞行控制仿真展示比较结果


<details>
  <summary>更多</summary>
  
**动机:** 研究级联在线学习飞行控制系统的收敛性能，特别关注动作平滑性的改进

**方法:** 使用李雅普诺夫函数增量作为性能指标，考虑离散化误差和状态预测误差，通过飞行控制仿真进行比较分析

**结果:** 未在摘要中明确说明具体结果，但提到通过仿真展示了比较结果

**结论:** 摘要未明确给出结论，但暗示通过李雅普诺夫分析和仿真验证了控制系统的收敛性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lyapunov+Function-guided+Reinforcement+Learning+for+Flight+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22840，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22840&send_immediately=true&force_search=false)

**原文摘要:** A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [48] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno, Jean-Louis Dessalles*

**主要类别:** cs.AI

**AI概要:** 该论文提出一个基于逻辑门电子电路的统一框架，将认知研究和人工智能中的各种推理机制（分类、归纳、溯因、因果推理等）整合起来，通过组合探索识别出四种依赖关系和八种常见推理模式。


<details>
  <summary>更多</summary>
  
**动机:** 认知研究和人工智能虽然发展了多种推理机制的模型，但缺乏统一的理论框架。论文试图填补这一空白，从物质角度探索高级激活过程。

**方法:** 采用符号AI建模技术，通过基于逻辑门的电子电路简化视角来分析推理机制，进行组合探索识别依赖关系，并在逻辑程序背景下分析推理模式。

**结果:** 识别出四种可由推理电路实现的主要依赖形式，发现了八种常见的推理模式，在统一框架下揭示了传统上不同的推理机制。

**结论:** 即使论证主要基于符号方法和数字系统基础设施，这些观察结果可能指向更普遍适用的结构，为认知和AI推理机制提供了统一的理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Structures+of+Inferential+Mechanisms+through+Simplistic+Digital+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22883&send_immediately=true&force_search=false)

**原文摘要:** Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [49] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat, Omkar Ghugarkar, Julian McAuley*

**主要类别:** cs.AI

**AI概要:** 论文提出MAVEN基准测试来评估LLM在工具调用环境中的泛化能力，发现现有模型表现不佳，并开发了CoreThink框架通过符号推理层显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决智能体工具调用环境中泛化能力不足的问题，现有LLM在跨领域工具协调和推理策略迁移方面表现不佳。

**方法:** 在多个工具调用基准测试(BFCL v3, TauBench等)上进行大规模评估，引入新的OOD基准MAVEN，并开发CoreThink框架(轻量级符号推理层)进行结构化分解和自适应工具编排。

**结果:** 当前模型在MAVEN上准确率低于50%，显示出显著的泛化差距。CoreThink框架无需额外训练即可在所有基准测试中实现泛化，性能提升530%，计算成本仅为十分之一。

**结论:** CoreThink框架通过符号推理层有效解决了工具调用环境中的泛化问题，在显著提升性能的同时大幅降低计算成本，为可靠智能体推理系统的发展提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Generalization+in+Agentic+Tool+Calling%3A+CoreThink+Agentic+Reasoner+and+MAVEN+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22898&send_immediately=true&force_search=false)

**原文摘要:** Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [50] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu*

**主要类别:** cs.AI

**AI概要:** GTR-Mamba是一个新颖的跨流形条件路由框架，通过双几何空间（双曲几何建模静态偏好层次，欧几里得空间处理动态序列）来克服现有POI推荐模型在同时捕捉空间层次结构和用户时间上下文动态变化方面的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于图神经网络和序列模型的POI推荐方法无法同时捕捉空间选择的层次结构特性和用户特定时间上下文的动态不规则变化。

**方法:** 提出GTR-Mamba框架：在双曲几何中建模静态树状偏好层次，在欧几里得切空间中通过Mamba层路由动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。

**结果:** 在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中 consistently 优于最先进的基线模型。

**结论:** GTR-Mamba通过跨流形条件路由成功解决了同时建模空间层次结构和时间动态变化的挑战，为POI推荐提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GTR-Mamba%3A+Geometry-to-Tangent+Routing+for+Hyperbolic+POI+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22942&send_immediately=true&force_search=false)

**原文摘要:** Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [51] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng, Sinuo Zhang, Rongpeng Li, Xiangming Meng, Chan Wang, Ming Lei, Zhifeng Zhao*

**主要类别:** cs.AI

**AI概要:** 提出MA-CDMP方法，基于扩散模型和均值场机制解决分布式无线通信资源分配中的非平稳性和合作问题，显著提升系统性能。


<details>
  <summary>更多</summary>
  
**动机:** 集中式MARL存在可扩展性和隐私风险问题，分布式DTDE范式面临非平稳性和有限合作的挑战，需要新的解决方案来提升无线通信系统的QoS。

**方法:** 基于模型强化学习范式，使用扩散模型捕捉环境动态并规划轨迹，通过逆动力学模型指导动作生成，引入均值场机制近似大规模智能体交互。

**结果:** 实验表明MA-CDMP在平均奖励和QoS指标上持续优于现有MARL基线方法，展现出良好的可扩展性和实际应用价值。

**结论:** MA-CDMP通过扩散模型和均值场机制有效解决了分布式资源分配的关键挑战，为实际无线网络优化提供了实用且可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Conditional+Diffusion+Model+with+Mean+Field+Communication+as+Wireless+Resource+Allocation+Planner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22969&send_immediately=true&force_search=false)

**原文摘要:** In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [52] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu, Jiakai Wang, Linna Jing, Haolin Li, Haodong Liu, Haotong Qin, Aishan Liu, Ke Xu, Xianglong Liu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个多维度指令不确定性减少框架(InSUR)，通过解决语言指令中的语义不确定性来生成更有效的语义约束对抗样本(SemanticAE)。


<details>
  <summary>更多</summary>
  
**动机:** 当前生成语义约束对抗样本的方法攻击能力不足，主要原因是未充分研究人类指令中的语义不确定性因素，如指代多样性、描述不完整性和边界模糊性。

**方法:** 提出InSUR框架，包含三个维度：1)采样方法维度：残差驱动攻击方向稳定化，使用ResAdv-DDIM采样器稳定优化过程；2)任务建模维度：上下文编码攻击场景约束，通过引导掩码和渲染器集成补充缺失知识；3)生成器评估维度：语义抽象攻击评估增强，明确评估边界。

**结果:** 大量实验证明了InSUR在迁移攻击性能上的优越性，并首次实现了无需参考的语义约束3D对抗样本生成。

**结论:** InSUR框架通过多维度减少指令不确定性，成功生成了更易迁移、自适应和有效的语义约束对抗样本，为未来研究提供了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Semantic-constrained+Adversarial+Example+with+Instruction+Uncertainty+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22981&send_immediately=true&force_search=false)

**原文摘要:** Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [53] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales, Carlos Andrés Ferro Sánchez, Reinel Tabares-Soto, Jesús Alfonso López Sotelo, Gonzalo A. Ruz, Johan Sebastian Piña Durán*

**主要类别:** cs.AI

**AI概要:** ProfileXAI是一个模型和领域无关的框架，通过结合SHAP、LIME、Anchor等后置解释器与检索增强的大语言模型，为不同类型用户生成可解释的AI解释。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决不同用户群体对AI模型解释的需求差异，提供既高效又可信的解释生成方案。

**方法:** 框架索引多模态知识库，通过量化标准为每个实例选择解释器，并使用聊天式提示生成基于知识的叙述性解释。

**结果:** 在心脏病和甲状腺癌数据集上的评估显示：LIME在保真度-鲁棒性权衡上表现最佳，Anchor产生最稀疏的低token规则，SHAP获得最高用户满意度。Profile条件化稳定了token使用并保持各用户群体的正面评价。

**结论:** ProfileXAI能够实现高效且可信的解释生成，不同解释器在不同指标上各有优势，没有单一解释器在所有方面都占优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProfileXAI%3A+User-Adaptive+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22998&send_immediately=true&force_search=false)

**原文摘要:** ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [54] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang, Xiaoming Li, Jie Chen, Yongxu Liu, Xingpeng Zhang, Chen Liu, Wei Chen*

**主要类别:** cs.AI

**AI概要:** 该研究提出多维度可信度评估框架(MDCA)来提升LLM生成的肝脏MRI报告的可信度，并提供机构特定的提示优化指导，在多个先进LLM模型上进行了评估比较。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在从影像发现生成诊断结论方面表现出潜力，但缺乏针对不同临床场景的提示设计优化指导，以及评估LLM生成放射学报告可信度的标准化框架。

**方法:** 引入多维度可信度评估(MDCA)框架，在SiliconFlow平台上评估比较多个先进LLM模型(Kimi-K2、Qwen3、DeepSeek-V3、ByteDance-Seed)的性能。

**结果:** 研究开发了系统性的评估框架和优化方法，但摘要中未具体说明各模型的性能比较结果。

**结论:** 该研究为LLM在放射学报告生成中的应用提供了重要的可信度评估框架和提示优化指导，有助于提升临床应用的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Prompt+Optimization+to+Multi-Dimensional+Credibility+Evaluation%3A+Enhancing+Trustworthiness+of+Chinese+LLM-Generated+Liver+MRI+Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23008&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [55] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh, Rajesh P. N. Rao*

**主要类别:** cs.AI

**AI概要:** 论文提出Mixed Density Diffuser (MDD)方法，通过可调超参数控制扩散规划器中不同时间段的规划密度，在多个任务领域达到新的SOTA性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有扩散规划器使用稀疏步长规划虽能捕获长期依赖关系且不增加计算成本，但过度稀疏的规划会降低性能，且时间密度阈值在整个时间范围内并非均匀分布。

**方法:** 提出MDD扩散规划器，允许在整个时间范围内通过可调超参数控制规划密度，使轨迹的不同部分可以有不同的规划密度。

**结果:** MDD在Maze2D、Franka Kitchen和Antmaze D4RL任务领域都达到了新的最先进(SOTA)性能。

**结论:** 通过自适应调整不同时间段的规划密度，MDD方法有效解决了过度稀疏规划的性能下降问题，证明了非均匀时间密度规划的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixed+Density+Diffuser%3A+Efficient+Planning+with+Non-uniform+Temporal+Resolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23026&send_immediately=true&force_search=false)

**原文摘要:** Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [56] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie, Pan Zhou, Lichao Sun*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一个六阶段方法论框架来系统分析AI科学家领域的发展，从早期基础模块到当前的可扩展性和人机协作前沿，为该领域的未来发展提供了路线图。


<details>
  <summary>更多</summary>
  
**动机:** AI正从计算工具转变为自主科学知识创造者，但该领域的快速无结构发展导致研究碎片化，需要系统性的方法论框架来理清发展趋势。

**方法:** 引入统一的六阶段方法论框架：文献综述、想法生成、实验准备、实验执行、科学写作和论文生成，通过这个分析视角追踪领域从2022年至今的演进历程。

**结果:** 建立了系统性的分析框架，将AI科学家领域划分为三个发展阶段：基础模块(2022-2023)、闭环系统(2024)、可扩展性与人机协作(2025至今)。

**结论:** 该调查不仅阐明了自主科学的现状，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖且不可或缺的合作伙伴。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+AI+Scientists%3A+Surveying+the+automatic+Scientists+and+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23045&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [57] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang, Meixin Su, Yang Yang, Chunyan Zeng, Lizhi Ye*

**主要类别:** cs.AI

**AI概要:** 提出基于深度学习和迁移学习的跨学科认知诊断方法TLCD，通过利用主学科共性特征提升目标学科诊断性能


<details>
  <summary>更多</summary>
  
**动机:** 在线教育快速发展但面临跨学科认知诊断挑战，不同学科知识体系、认知结构和数据特征差异大，传统方法难以应对

**方法:** 结合深度学习技术和迁移学习策略，研究神经网络认知诊断和知识关联神经网络认知诊断，提出TLCD跨学科认知诊断方法

**结果:** 实验表明基于深度学习的跨学科认知诊断模型在跨学科任务中表现优于基础模型，能更准确评估学生学习情况

**结论:** TLCD方法有效解决了跨学科认知诊断问题，为智能教育中的精准学习评估提供了新思路

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TLCD%3A+A+Deep+Transfer+Learning+Framework+for+Cross-Disciplinary+Cognitive+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23062&send_immediately=true&force_search=false)

**原文摘要:** Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [58] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld, Xi Qin, Alexander Schaefer, Yaad Oren*

**主要类别:** cs.AI

**AI概要:** 研究证明小型语言模型如Phi-4可以通过添加回归层和监督微调转变为有效的奖励模型，用于代码生成任务，能显著提升代码生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在生成高质量代码方面仍面临挑战，需要奖励模型作为中间步骤来评估结果和中间步骤。研究旨在探索小型语言模型是否也能成为有效的奖励模型。

**方法:** 构建基于APPS编程挑战基准的代码样本数据集，训练带有价值头的模型来估计中间输出的成功概率，结合过程奖励和结果奖励。

**结果:** 小型LLM能够有效作为奖励模型或代码评估评判器，成功识别多个候选方案中的正确解决方案，使用该评判器可使多代代码中最准确代码的搜索能力提升超过20%。

**结论:** 小型语言模型具备成为实用奖励模型的潜力，能够有效提升代码生成的质量和准确性，为代码生成任务提供了更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smaller+Models%2C+Smarter+Rewards%3A+A+Two-Sided+Approach+to+Process+and+Outcome+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23083&send_immediately=true&force_search=false)

**原文摘要:** Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [59] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang, Jiawei Zhang, Yumou Liu, Hanqun Cao, Chunbin Gu, Mengdi Liu, Zhangyang Gao, Zitong Jerry Wang, Xuanhe Zhou, Pheng-Ann Heng, Lijun Wu, Conghui He, Cheng Tan*

**主要类别:** cs.AI

**AI概要:** 研究发现，在生物推理任务中，为科学大语言模型提供高层次结构化上下文（而非原始序列）能显著提升性能，原始序列反而成为信息噪声。建议将Sci-LLMs重新定位为基于专家知识的推理引擎而非序列解码器。


<details>
  <summary>更多</summary>
  
**动机:** 解决科学大语言模型在处理原始生物分子序列时面临的tokenization困境——无论是将序列视为专门语言（可能丢失功能motif信息）还是单独模态（带来对齐挑战），现有策略都限制了模型的推理能力。

**方法:** 通过系统比较领先的Sci-LLMs在生物推理任务上的表现，测试了三种输入模式：仅序列、仅上下文、以及两者结合。

**结果:** 仅使用上下文的方法始终且显著优于其他所有模式。更令人惊讶的是，在高层上下文旁边加入原始序列会持续降低性能，表明原始序列即使对于具有专门tokenization方案的模型也是信息噪声。

**结论:** 现有Sci-LLMs的主要优势不在于从头解释生物分子语法的能力，而在于对结构化、人类可读知识进行推理的强大能力。应该将Sci-LLMs重新定义为基于专家知识的强大推理引擎，而非序列解码器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+in+Tokenization%3A+Context+as+the+Key+to+Unlocking+Biomolecular+Understanding+in+Scientific+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23127&send_immediately=true&force_search=false)

**原文摘要:** Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [60] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent François-Lavet, Edward S. Hu*

**主要类别:** cs.AI

**AI概要:** FoG技能发现方法通过基础模型将人类偏好融入技能学习，避免了传统方法产生危险或不期望行为的问题，成功消除了翻转、滚动等不良行为，并能发现难以定义的行为技能。


<details>
  <summary>更多</summary>
  
**动机:** 现有技能发现方法只关注技能多样性最大化，忽视了人类偏好，导致产生不良甚至危险的行为（如机器人翻滚），需要将人类意图融入技能发现过程。

**方法:** 提出Foundation model Guided (FoG)方法，利用基础模型提取评分函数来评估状态，根据人类意图为期望状态赋予高分、不期望状态赋予低分，然后用这些分数重新加权技能发现算法的奖励。

**结果:** FoG成功消除了翻转、滚动等不良行为，避免了危险区域，在基于状态和基于像素的任务中都表现良好，并能发现难以定义的行为技能。

**结论:** 通过基础模型将人类偏好融入技能发现过程是有效的，FoG方法能够学习符合人类期望的安全技能，为下游任务的强化学习提供了更好的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guiding+Skill+Discovery+with+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23167&send_immediately=true&force_search=false)

**原文摘要:** Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [61] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker, Alexander Dockhorn, Bodo Rosenhahn*

**主要类别:** cs.AI

**AI概要:** AUPO是一种新颖的MCTS决策策略改进方法，通过自动动作抽象算法显著提升性能，无需转移概率或DAG搜索图，能有效检测对称动作


<details>
  <summary>更多</summary>
  
**动机:** 现有自动抽象算法需要访问转移概率或依赖有向无环搜索图，无法有效处理状态空间中相距较远的对称状态

**方法:** 基于MCTS过程中获取的奖励分布统计信息，开发自动动作抽象算法AUPO，作为MCTS的即插即用修改

**结果:** 在IPPC基准问题上，AUPO明显优于标准MCTS，能够检测到ASAP等先进框架难以处理的对称动作

**结论:** AUPO作为一种仅影响决策策略的方法，可与其他仅影响树搜索的抽象技术兼容使用，提供了有效的动作抽象解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AUPO+--+Abstracted+Until+Proven+Otherwise%3A+A+Reward+Distribution+Based+Abstraction+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23214&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [62] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini, Joakim Bergdahl, Jean-Philippe Barrette-LaPierre, Florian Fuchs, Brady Chen, Micheal Jones, Linus Gisslén*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种针对游戏工业环境优化的样本高效深度强化学习方法，在EA SPORTS FC 25中训练守门员智能体，性能超越游戏内置AI 10%，训练速度提升50%，并产生更拟人化的游戏体验。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度强化学习在游戏测试中表现优异，但由于需要大量资源和模型训练超人级智能体，游戏工业难以应用。游戏工作室需要资源有限条件下训练拟人化智能体的实用方法。

**方法:** 提出样本高效的基于价值的深度强化学习方法，利用预收集数据并增强网络可塑性，专门为游戏工业环境中的智能体训练和微调而设计。

**结果:** 在EA SPORTS FC 25中训练的守门员智能体比游戏内置AI的扑救率高10%，消融研究显示训练速度比标准DRL方法快50%，领域专家定性评估表明比手工制作智能体更拟人化。

**结论:** 该方法成功解决了游戏工业应用DRL的实用性障碍，证明了在资源有限环境下训练高质量拟人化游戏智能体的可行性，将被用于该游戏系列的下一代产品中替代手工制作的智能体。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-Like+Goalkeeping+in+a+Realistic+Football+Simulation%3A+a+Sample-Efficient+Reinforcement+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23216&send_immediately=true&force_search=false)

**原文摘要:** While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [63] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang, Wenkai Yang, Jie Wang, Huanshuo Dong, Zijie Geng, Zhen Huang, Depeng Xie, Zhezheng Hao, Hande Dong*

**主要类别:** cs.AI

**AI概要:** 提出BlocKOA算法，用于高效生成集成电路热仿真数据，相比现有方法时间复杂降低一个数量级，实现420倍加速，仅用4%时间即可生成同等质量训练数据


<details>
  <summary>更多</summary>
  
**动机:** 现有数据驱动方法需要大量高保真训练数据，计算成本高昂，限制了神经网络算子在集成电路热仿真中的应用

**方法:** 基于热方程结构使用块Krylov算法快速获取基础解，组合生成满足物理约束的温度分布，再应用热算子确定热源分布

**结果:** 理论分析显示时间复杂度降低一个数量级，实验验证对5000个芯片生成热仿真数据实现420倍加速，仅用4%时间生成的数据训练模型性能相当

**结论:** BlocKOA算法有效解决了集成电路热仿真数据生成的计算瓶颈，为数据驱动方法提供了高效精确的数据生成解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+IC+Thermal+Simulation+Data+Generation+via+Block+Krylov+and+Operator+Action，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23221&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [64] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello, Daniele Lizzio Bosco, Jacopo Cossio, Dusan Sutulovic, Giuseppe Serra, Carla Piazza, Paolo Burelli*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新颖的强化学习方法来解决CNOT门最小化问题，通过单一智能体处理不同尺寸的量子电路，在较大规模电路中表现优于现有最优算法。


<details>
  <summary>更多</summary>
  
**动机:** CNOT门是量子计算中的基本门，能够产生纠缠这一量子算法关键资源。某些量子电路完全由CNOT门构成，因此最小化CNOT门数量是一个重要但尚未完全解决的计算复杂性问题。

**方法:** 使用单一强化学习智能体处理固定尺寸m的电路，对于不同尺寸的矩阵采用嵌入或高斯条纹化预处理。具体训练了m=8的智能体，并在尺寸n=3到15的矩阵上进行评估。

**结果:** 实验结果表明，随着n值的增加，该方法的表现超过了当前最先进的算法。

**结论:** 该强化学习方法为CNOT门最小化问题提供了有效的解决方案，特别是在处理较大规模量子电路时展现出优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CNOT+Minimal+Circuit+Synthesis%3A+A+Reinforcement+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23304&send_immediately=true&force_search=false)

**原文摘要:** CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [65] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das, John Duff, Jörg Hoffmann, Vera Demberg*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个基于理性言语行为(RSA)框架的自适应信号理论，用于在动态环境中优化人机协作的信息传递时机和特异性，通过多步规划实现用户信念与环境及时对齐。


<details>
  <summary>更多</summary>
  
**动机:** 在快速变化的时敏任务中，确保人类保持对关键任务元素的准确理解，需要智能体不仅识别最高优先级信息，还要估计如何和何时最有效地传递这些信息，因为人类注意力是零和认知资源。

**方法:** 使用理性言语行为(RSA)建模框架，通过贝叶斯参考解析进行理性通信，规划消息序列以优化用户信念与动态环境的及时对齐。智能体根据用户和场景特点调整消息特异性和时机，基于先验引导的消息解释对界面注意力和后续信念更新的多步预测。

**结果:** 与基线方法相比，该方法的效果关键取决于将多步规划与现实的用户意识模型相结合，证明了该方法在动态环境通信中的有效性。

**结论:** 作为RSA在动态环境通信和人机交互中的首次应用，为人类-智能体团队的实用通信建立了理论基础，展示了如何利用认知科学见解指导辅助智能体设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Planning+Ahead+with+RSA%3A+Efficient+Signalling+in+Dynamic+Environments+by+Projecting+User+Awareness+across+Future+Timesteps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23340&send_immediately=true&force_search=false)

**原文摘要:** Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [66] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar, A. G. Phakatkar*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于模糊逻辑的细粒度意见挖掘方法，用于从评论文本中提取更详细的情感信息并基于此对实体进行排序。


<details>
  <summary>更多</summary>
  
**动机:** 随着社交媒体和电商网站的兴起，网络上存在大量意见数据。现有研究主要集中在意见挖掘和基于评论的实体排序，但缺乏将意见分类到更细粒度层次后再进行实体排序的研究。

**方法:** 使用模糊逻辑推理方法，从评价性语句中进行更深层次的细粒度意见挖掘，提取实体的属性和组件评论信息。

**结果:** 开发了一种能够将意见分类到更细粒度级别的方法，并基于这些细粒度信息实现了实体的排序。

**结论:** 该方法填补了现有研究的空白，通过模糊逻辑实现了更精细的意见分析和实体排序，为意见挖掘领域提供了新的技术途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Opinion+Mining+Based+Entity+Ranking+using+Fuzzy+Logic+Algorithmic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23384&send_immediately=true&force_search=false)

**原文摘要:** Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [67] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer*

**主要类别:** cs.AI

**AI概要:** AutoStreamPipe是一个利用大语言模型自动化流处理管道设计、生成和部署的新框架，通过超图思维(HGoT)桥接用户意图与平台实现，显著减少开发时间和错误率。


<details>
  <summary>更多</summary>
  
**动机:** 解决流处理管道设计中高层用户意图与平台特定实现之间的语义鸿沟，自动化传统上需要大量人工工作的管道开发过程。

**方法:** 结合大语言模型(LLMs)和超图思维(HGoT)扩展版，集成弹性执行策略和高级查询分析，实现多智能体结构化推理。

**结果:** 实验评估显示，相比LLM代码生成方法，开发时间减少6.3倍，错误率降低5.19倍(通过新型无错误评分EFS衡量)。

**结论:** AutoStreamPipe框架有效自动化了流处理管道的开发流程，显著提升了开发效率和准确性，为实时数据处理提供了创新的自动化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoStreamPipe%3A+LLM+Assisted+Automatic+Generation+of+Data+Stream+Processing+Pipelines，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23408&send_immediately=true&force_search=false)

**原文摘要:** Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [68] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng*

**主要类别:** cs.AI

**AI概要:** Bid2X是一个广告竞价基础模型，通过统一的函数估计不同竞价场景下的广告效果，使用注意力机制处理异构数据和时间依赖关系，在淘宝平台部署后显著提升了GMV和ROI。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有竞价模型在跨环境泛化性方面的局限性，传统模型通常针对特定竞价场景设计，缺乏通用性。

**方法:** 提出Bid2X基础模型，使用统一系列嵌入处理异构数据，采用两种注意力机制分别处理变量间和时间依赖关系，通过变量感知融合模块进行自适应预测，并设计零膨胀投影模块处理数据分布特性。

**结果:** 在8个数据集上的离线评估显示Bid2X优于各种基线方法并具有跨场景通用性。在线A/B测试中GMV提升4.65%，ROI提升2.44%。

**结论:** Bid2X为计算广告领域的竞价基础模型开辟了新途径，证明了其在实际电商平台部署的有效性和通用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bid2X%3A+Revealing+Dynamics+of+Bidding+Environment+in+Online+Advertising+from+A+Foundation+Model+Lens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23410，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23410&send_immediately=true&force_search=false)

**原文摘要:** Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [69] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi, Amir Saki, Usef Faghihi*

**主要类别:** cs.AI

**AI概要:** 提出一种将因果推理整合到DQN中的新方法，使用PEACE公式估计因果效应，通过因果推理增强DQN对环境因果结构的理解，减少虚假相关性的影响。


<details>
  <summary>更多</summary>
  
**动机:** 传统DQN依赖关联学习容易获得虚假相关性，限制了其问题解决能力，需要整合因果原理来改善这一问题。

**方法:** 利用PEACE（概率简易变分因果效应）公式估计因果效应，在训练过程中整合因果推理，增强对环境中因果结构的理解。

**结果:** 实验结果显示，在标准基准环境中，该方法优于传统DQN，显著提升了问题解决能力且不损害性能。

**结论:** 通过原则性因果推断推进深度强化学习智能体能力的有前景途径，因果推理在强化学习中具有显著效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Deep+Q+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23424&send_immediately=true&force_search=false)

**原文摘要:** Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [70] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种智能系统来解决网络安全与法律交叉领域的信息检索难题，初步展示了在多语言任务上的良好效果。


<details>
  <summary>更多</summary>
  
**动机:** 网络安全与法律的交叉领域形成了复杂的信息空间，传统法律研究工具难以处理案例、法规和技术漏洞之间的微妙联系，阻碍了法律专家与网络安全专业人士的协作。

**方法:** 开发能够导航日益复杂的网络法律领域的智能系统，作为解决这一重要差距的第一步。

**结果:** 在多语言任务上展示了有希望的初步结果。

**结论:** 这项工作为解决网络法律领域的信息检索挑战提供了初步解决方案，展现了智能系统在该领域的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Neuro-Symbolic+Multi-Agent+Approach+to+Legal-Cybersecurity+Knowledge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23443&send_immediately=true&force_search=false)

**原文摘要:** The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [71] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

**主要类别:** cs.AI

**AI概要:** 这是一篇对AI存在风险分类分析文章的评论性论文，重点讨论了线性风险模型的哲学局限性，特别分析了认知无差异情境下AI灾难概率的评估问题。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在指出Cappelen等人的AI存在风险分类分析中使用的线性风险模型存在哲学局限性，需要更全面地考虑不确定性因素来评估AI灾难概率。

**方法:** 通过比较标准瑞士奶酪模型与作者使用的模型差异，分析认知无差异情境下的概率评估，区分风险与不确定性概念，并引入选项不确定性和状态空间不确定性两个维度。

**结果:** 论文认为在认知无差异情境下，AI灾难概率P(D)可能比初步估计更高，且任何P(D)估计都会受到两种不确定性的结构性影响。

**结论:** 将不确定性维度纳入AI存在风险的定性讨论中，能够提供对AI灾难可能性的更好理解，超越简单的线性风险模型分析框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+are+the+odds%3F+Risk+and+uncertainty+about+AI+existential+risk，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23453&send_immediately=true&force_search=false)

**原文摘要:** This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [72] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi, Muzakkiruddin Ahmed Mohammed, Hendrika Maclean, Mert Can Cakmak, John R. Talburt*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个基于大语言模型的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略匹配，实现安全、合规且可追溯的访问决策。


<details>
  <summary>更多</summary>
  
**动机:** 企业需要满足最小权限、符合法规要求且可审计的访问决策，但传统方法难以有效处理自然语言策略解释。

**方法:** 使用Google Gemini 2.0 Flash实现六阶段推理框架（上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险合成），采用早期硬策略门控和默认拒绝机制。

**结果:** 在14个标准案例测试中，精确决策匹配率从10/14提升到13/14（92.9%），拒绝召回率达到1.00，必须拒绝场景的误批准率降为0，功能适当性和合规性达到14/14。

**结论:** 策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy-Aware+Generative+AI+for+Safe%2C+Auditable+Data+Access+Governance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23474，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23474&send_immediately=true&force_search=false)

**原文摘要:** Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [73] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani, Shayan Kiyani, George Pappas, Hamed Hassani*

**主要类别:** cs.AI

**AI概要:** 本文提出了人机协作不确定性量化框架，通过AI模型精炼人类专家的预测集，避免反事实伤害并实现互补性，在多种任务中显示协作预测集优于单独使用人类或AI。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI在不确定性下的稳健决策仍缺乏领域知识、长期上下文和物理世界推理能力，需要结合人类与AI的互补优势来设计协作框架。

**方法:** 引入人机协作不确定性量化框架，形式化AI如何精炼人类预测集；开发具有可证明分布无关有限样本保证的离线和在线校准算法；在线方法能适应分布偏移和人类行为演化。

**结果:** 在图像分类、回归和文本医疗决策任务中，协作预测集始终优于单独代理，在各种条件下实现更高的覆盖率和更小的集合大小。

**结论:** 人机协作不确定性量化框架有效结合了人类与AI的优势，为可靠决策提供了实用方法，特别在需要适应分布偏移和人类行为演化的场景中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-AI+Collaborative+Uncertainty+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23476，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23476&send_immediately=true&force_search=false)

**原文摘要:** AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [74] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani, Ziyou Li, Anton Podkopaev, Maliheh Izadi*

**主要类别:** cs.AI

**AI概要:** 该论文建立了现代AI智能体架构与乔姆斯基层级中抽象自动机的形式等价关系，提出基于内存架构的智能体计算能力分类框架，为形式化验证和安全保障提供理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 为AI智能体系统建立形式化的计算理论框架，通过自动机理论来理解智能体的计算能力边界，实现形式化验证和安全保障。

**方法:** 将不同类型的AI智能体架构映射到乔姆斯基层级中的相应自动机：简单反射智能体对应有限自动机，分层任务分解智能体对应下推自动机，具有读写内存的反思智能体对应图灵机。

**结果:** 建立了Automata-Agent框架，证明了三类智能体与相应自动机的等价性，并扩展了概率自动机来处理LLM智能体的概率特性。

**结论:** 该框架为智能体架构的优化设计提供了原则性方法，开辟了形式化验证和安全保障的途径，并提出了开发静态分析工具和语法的发展议程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Agents+Just+Automata%3F+On+the+Formal+Equivalence+Between+Agentic+AI+and+the+Chomsky+Hierarchy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23487，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23487&send_immediately=true&force_search=false)

**原文摘要:** This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [75] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha, Jeong Hun Yeo, Yeonju Kim, Yong Man Ro*

**主要类别:** cs.AI

**AI概要:** 提出情感推理验证器(ERV)和解释奖励方法，在多模态情感识别中确保模型生成与目标情感一致的解释，无需修改模型架构或额外标注数据，显著提高了解释-预测一致性和情感准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型生成的情感解释常与目标标签不一致甚至自相矛盾，这会降低系统可靠性和用户信任度，需要解决解释与预测不一致的问题。

**方法:** 提出情感推理验证器(ERV)和解释奖励方法，引导模型在情感识别过程中生成与目标情感明确一致的推理，不改变模型架构且无需额外视频-描述标注数据。

**结果:** 在MAFW和DFEW数据集上显著提高了忠实解释-预测一致性和解释情感准确性，通过大量实验和人工评估验证了方法的有效性。

**结论:** 该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信赖的交互，是实现真正类人化人机交互系统的关键一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emotion-Coherent+Reasoning+for+Multimodal+LLMs+via+Emotional+Rationale+Verifier，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23506&send_immediately=true&force_search=false)

**原文摘要:** The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [76] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh, Rodrigue Rizk, Longwei Wang*

**主要类别:** cs.AI

**AI概要:** 论文提出Human AI (HAI)框架，通过增量学习、碳感知优化和人机协作，实现可持续、适应性强且负责任的人工智能发展，以应对当前AI计算需求带来的环境和伦理问题。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能快速发展带来前所未有的计算需求，引发严重的环境和伦理担忧。论文批判当前依赖大规模静态数据集和单一训练范式的做法，需要转向更可持续的AI解决方案。

**方法:** 引入Human AI (HAI)框架，借鉴生物认知原理，采用动态架构，强调增量学习、碳感知优化和人机协作，实现连续情境学习同时最小化碳足迹和人工标注成本。

**结果:** HAI框架解决了主动学习、持续适应和节能模型部署等关键挑战，提供了理论基础、系统设计和操作原则。

**结论:** 该研究为构建负责任、以人为本的人工智能提供了一条可行路径，平衡了性能表现与生态责任，推动AI向更可持续的方向发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Carbon-Neutral+Human+AI%3A+Rethinking+Data%2C+Computation%2C+and+Learning+Paradigms+for+Sustainable+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23524，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23524&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [77] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert*

**主要类别:** cs.AI

**AI概要:** NoRA是一个新的系统性关系推理基准测试，通过增加多个难度级别并要求模型超越基于路径的推理，来解决现有基准过于简化的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有系统性关系推理基准过于简化，假设推理可以简化为关系路径的组合，这限制了模型在其他设置中的泛化能力。

**方法:** 引入NoRA基准测试，包含多个难度级别，要求模型进行超越路径推理的复杂关系推理。

**结果:** 新基准测试为神经网络系统性关系推理领域提供了更全面的评估标准。

**结论:** NoRA基准测试将推动系统性关系推理领域的进一步发展，促进开发更具泛化能力的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+No+Paths+Lead+to+Rome%3A+Benchmarking+Systematic+Neural+Relational+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23532&send_immediately=true&force_search=false)

**原文摘要:** Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [78] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan*

**主要类别:** cs.AI

**AI概要:** JanusCode-800K是最大的多模态代码语料库，支持JanusCoder系列模型的训练，能够在文本和视觉输入下生成代码，在多项任务中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 神经代码智能从纯文本扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展，需要解决合成和质量评估的挑战。

**方法:** 开发了完整的合成工具包，利用数据模态间的协同作用大规模生成高质量语料库，训练JanusCoder和JanusCoderV模型，建立视觉-编程接口。

**结果:** 7B到14B规模的模型在文本和视觉编码任务中表现优异，接近或超过商业模型性能，提供了程序逻辑与视觉表达协调的关键见解。

**结论:** 通过大规模多模态语料库和统一模型，成功实现了从文本和视觉输入生成代码的能力，为视觉编程智能开辟了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JanusCoder%3A+Towards+a+Foundational+Visual-Programmatic+Interface+for+Code+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23538&send_immediately=true&force_search=false)

**原文摘要:** The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [79] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis, Stacie Severyn, Fjollë Novakazi, Hadi Banaee, Cogan Shimizu*

**主要类别:** cs.AI

**AI概要:** 本文提出了OntoPret本体，用于实时解释人类行为，填补了技术中心机器人框架与描述性行为本体之间的研究空白，支持制造和游戏场景中的行为分类和意图推理。


<details>
  <summary>更多</summary>
  
**动机:** 随着人机协作在工业5.0中变得重要，需要机器安全有效地解释复杂人类行为，但目前技术中心机器人框架缺乏细致的人类行为模型，而描述性行为本体不适合实时协作解释。

**方法:** 基于认知科学和模块化工程方法，开发了OntoPret本体，提供形式化的机器可处理框架，用于分类行为（包括任务偏差和欺骗行为）。

**结果:** 在制造和游戏两个不同用例中验证了OntoPret的适应性，并建立了必要语义基础以支持对人类意图的高级推理。

**结论:** OntoPret成功填补了研究空白，为实时人机协作中的行为解释提供了有效框架，支持跨领域应用和高级意图推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OntoPret%3A+An+Ontology+for+the+Interpretation+of+Human+Behavior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23553，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23553&send_immediately=true&force_search=false)

**原文摘要:** As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [80] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu*

**主要类别:** cs.AI

**AI概要:** ReCode提出了一种通过递归代码生成统一规划和行动的新范式，使用抽象占位函数实现多粒度决策控制，显著超越现有基线并具有优异的数据效率。


<details>
  <summary>更多</summary>
  
**动机:** 现实任务需要不同粒度的决策，但当前基于大语言模型的智能体缺乏在决策粒度间流畅操作的能力，因为现有范式将高层规划与低层行动严格分离，限制了动态适应性和泛化能力。

**方法:** ReCode将规划和行动统一在单一代码表示中，将高层计划视为抽象占位函数，然后递归分解为更细粒度的子函数，直到原始行动，从而消除规划与行动之间的刚性边界。

**结果:** 大量实验表明ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率，验证了通过递归代码生成统一规划和行动的有效性。

**结论:** 通过递归代码生成统一规划和行动是实现通用粒度控制的有力且有效方法，ReCode为智能体提供了动态控制决策粒度的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReCode%3A+Unify+Plan+and+Action+for+Universal+Granularity+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23564&send_immediately=true&force_search=false)

**原文摘要:** Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [81] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann, Aleksandra Urman, Ulrich Leicht-Deobald, Zachary J. Roman, Anikó Hannák, Markus Christen*

**主要类别:** cs.AI

**AI概要:** ChatGPT发布后，瑞士公众对AI的接受度下降，对人机协同决策的需求增加，同时扩大了教育、语言和性别方面的社会不平等。


<details>
  <summary>更多</summary>
  
**动机:** 研究公众对AI的态度变化，特别是在生成式AI技术快速普及但用户偏好被忽视的背景下，了解ChatGPT发布前后公众接受度的变化。

**方法:** 采用大规模两波调查（第一波1514人，第二波1488人），代表瑞士人口，比较ChatGPT发布前后的公众态度变化。

**结果:** 生成式AI热潮显著降低了公众对AI的接受度，完全不可接受AI的比例从23%升至30%，支持纯人类决策的比例从18%增至26%，同时扩大了教育、语言和性别差距。

**结论:** 研究结果挑战了行业对公众AI部署准备度的假设，强调技术发展必须与不断变化的公众偏好保持一致的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reduced+AI+Acceptance+After+the+Generative+AI+Boom%3A+Evidence+From+a+Two-Wave+Survey+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23578&send_immediately=true&force_search=false)

**原文摘要:** The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [82] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhan, Mostofa Patwary, Jiaxuan You*

**主要类别:** cs.AI

**AI概要:** MAE是一个多智能体自进化框架，通过三个交互智能体（提议者、求解者、评判者）实现LLM的自我进化，在数学、推理和知识问答任务上平均提升4.54%性能


<details>
  <summary>更多</summary>
  
**动机:** 解决传统强化学习对人工标注数据和可验证奖励的依赖问题，以及现有自博弈方法对特定环境反馈的局限性，扩展RL在通用领域的应用

**方法:** 提出MAE框架，使用单一LLM实例化的三个交互智能体：提议者生成问题、求解者提供解答、评判者进行评估，通过强化学习实现协同进化

**结果:** 在Qwen2.5-3B-Instruct模型上，MAE在多个基准测试中实现了平均4.54%的性能提升

**结论:** MAE是一种可扩展、数据高效的方法，能够以最少的人工监督依赖显著提升LLM的通用推理能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Evolve%3A+LLM+Self-Improve+through+Co-evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23595&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [83] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang*

**主要类别:** cs.AI

**AI概要:** ALITA-G是一个自我进化框架，通过系统生成、抽象和策划模型上下文协议(MCP)工具，将通用智能体转化为领域专家，在多个基准测试中实现最先进性能并降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 当前的自进化智能体主要局限于提示重写或失败重试，需要更系统的自我进化方法来提升大语言模型在特定领域的专业能力。

**方法:** 框架包含：通用智能体执行目标领域任务并合成候选MCP工具；将成功轨迹抽象为参数化原语并整合到MCP Box中；推理时通过检索增强的MCP选择和执行器来增强智能体能力。

**结果:** 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的SOTA结果，同时相比基线智能体减少约15%的平均token消耗。

**结论:** ALITA-G提供了从通用能力到可重用领域专业能力的原理性路径，在复杂推理任务上同时提高了准确性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Alita-G%3A+Self-Evolving+Generative+Agent+for+Agent+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23601&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

**主要类别:** cs.CL

**AI概要:** 本文介绍了一个包含83.3万段科学出版物的数据集，标注了致谢、数据提及、软件/代码提及和临床试验提及四种类别，支持科学文献挖掘中的文本分类和命名实体识别任务。


<details>
  <summary>更多</summary>
  
**动机:** 为科学文献挖掘提供高质量的标注数据集，支持文本分类和命名实体识别模型的训练与开发。

**方法:** 从CC-BY许可的科学出版物中提取段落，使用fastText进行语言识别，OpenAlex进行科学领域分类，通过GROBID处理法国开放科学监测语料库。

**结果:** 创建了包含83.3万段多语言科学文本的数据集，涵盖英语、法语等欧洲语言，每段都标注了语言和科学领域信息。

**结论:** 该数据集公开发布在HuggingFace平台上，采用CC-BY许可，为科学文本挖掘研究提供了有价值的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-lingual+Dataset+of+Classified+Paragraphs+from+Open+Access+Scientific+Publications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21762&send_immediately=true&force_search=false)

**原文摘要:** We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [85] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal, Aakash Sen Sharma, Dhruv Kumar, Saurabh Deshpande, Murari Mandal*

**主要类别:** cs.CL

**AI概要:** 策略优化算法在宽松的思维链结构中倾向于选择最简单路径，丢弃显式推理而直接生成答案，即使复杂格式有更高奖励权重。优化过程会优先优化最简单的奖励成分，这揭示了奖励破解的关键挑战。


<details>
  <summary>更多</summary>
  
**动机:** 研究当前策略优化算法在放宽严格思维链格式约束后的行为表现，探索在开放式思维链结构下策略优化的特性。

**方法:** 通过一系列受控实验，包括奖励分解实验，分析不同模型和算法在灵活推理-回答交错结构中的行为模式。

**结果:** 策略优化始终遵循最小阻力路径，倾向于丢弃显式推理而采用直接答案格式，即使复杂格式有4倍奖励权重优势。优化过程系统性地优先优化最简单奖励成分。

**结论:** 给予策略发散自由是一把双刃剑：既能发现高奖励捷径，又会产生奖励破解的强大诱因，这对对齐提出了关键挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy+Optimization+Prefers+The+Path+of+Least+Resistance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21853&send_immediately=true&force_search=false)

**原文摘要:** Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [86] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang, Tianqi Du, Jizhe Zhang, Mingqing Xiao, Yifei Wang, Yisen Wang, Zhouchen Lin*

**主要类别:** cs.CL

**AI概要:** 本文提出Language Ranker框架，将LLM解码过程重新构想为推荐系统中的排序阶段，通过轻量级重排模块显著降低计算成本，达到与大型奖励模型相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM研究主要关注输出分布优化而忽视解码过程，现有方法如奖励模型计算成本高且适用性有限，存在冗余等明显限制。

**方法:** 借鉴推荐系统思路，将解码过程类比为推荐流水线的排序阶段，提出Language Ranker框架，使用基础模型提取的特征通过轻量级模块对候选响应进行重排序。

**结果:** 在广泛任务上的实验表明，该方法仅需<0.5M额外参数，性能与大规模奖励模型相当，显著降低了训练和推理阶段的计算开销。

**结论:** 该方法高效且有效，展示了充分释放LLM能力的潜力，为解码过程优化提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Ranker%3A+A+Lightweight+Ranking+framework+for+LLM+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21883&send_immediately=true&force_search=false)

**原文摘要:** Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [87] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

**主要类别:** cs.CL

**AI概要:** RACE框架通过逻辑回归特征重要性评估LLM生成解释的忠实度，发现正确预测更支持特征，错误预测更矛盾特征，揭示了LLM解释的表面和灵活证据复用模式。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习在敏感领域的应用增加，需要透明可解释的AI。LLM能生成自然语言解释，但这些解释是否真实反映决策信号尚不清楚。

**方法:** 提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归的特征重要性，使用token匹配、精确字符串匹配和编辑距离匹配三种粒度分析方法。

**结果:** 实证结果显示一致的不对称性：正确预测覆盖更多支持特征，错误预测覆盖更多矛盾特征。编辑距离匹配发现释义重叠，提高覆盖率但保持不对称性。

**结论:** LLM解释结合了表面和灵活的证据复用，但可能在错误情况下放大误导线索。RACE为评估神经语言模型的推理完整性提供了量化基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Framework+for+Machine+Evaluation+of+Reasoning+Completeness+in+Large+Language+Models+For+Classification+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21884&send_immediately=true&force_search=false)

**原文摘要:** The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [88] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham, Mihir Thalanki, Michael Sun, Aditya Chaloo, Ankita Gupta, Tian Xia, Aditya Mate, Ehimwenma Nosakhare, Soundararajan Srinivasan*

**主要类别:** cs.CL

**AI概要:** 提出了一个行为感知采样框架，通过基于指令-响应行为和语义多样性选择安全样本，有效减少大语言模型在微调时的灾难性遗忘问题，仅需0.5%额外训练数据就能将有害输出减少41%。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在良性数据微调时经常失去之前对齐的安全行为（灾难性遗忘），现有方法添加随机安全样本效果有限，需要更有效的数据选择策略。

**方法:** 使用行为感知采样框架，基于两个互补因素选择安全样本：指令-响应行为（拒绝vs遵从）和跨伤害类别的语义多样性。

**结果:** 系统评估显示该方法显著减少有害输出同时保持帮助性，仅用0.5%额外训练数据就能实现高达41%的有害性降低。

**结论:** 有针对性的数据选择可以显著提高大规模微调的安全性和效率，为安全对齐提供了更有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preventing+Catastrophic+Forgetting%3A+Behavior-Aware+Sampling+for+Safer+Language+Model+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21885&send_immediately=true&force_search=false)

**原文摘要:** Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [89] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj, Julia Kempe, Tim G. J. Rudner*

**主要类别:** cs.CL

**AI概要:** 提出语义各向同性概念，通过计算文本嵌入在单位球面上的角分散度来评估大语言模型长文本响应的可信度，无需标注数据或调参即可有效检测事实不一致性


<details>
  <summary>更多</summary>
  
**动机:** 在需要高准确性的开放域应用中，现有基于逐句事实核查的方法计算成本高且对长文本响应效果不佳，需要更高效可靠的可信度评估方法

**方法:** 生成多个长文本响应，将其嵌入到单位球面上，通过计算嵌入向量的角分散度（语义各向同性程度）来评估响应的一致性

**结果:** 更高的语义各向同性（更大的嵌入分散度）可靠地指示了样本间更低的事实一致性，方法在多个领域均优于现有方法

**结论:** 该方法提供了一种实用、低成本的信任评估方案，可集成到实际LLM工作流程中，无需标注数据、微调或超参数选择

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedding+Trust%3A+Semantic+Isotropy+Predicts+Nonfactuality+in+Long-Form+Text+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21891&send_immediately=true&force_search=false)

**原文摘要:** To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [90] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing, Chang Tian, Jianan Zhang, Lichen Pan, Peipei Liu, Zhaoteng Yan, Yinliang Yue*

**主要类别:** cs.CL

**AI概要:** NetMind是一个使用自然语言查询网络行为的新框架，通过树状配置分块、统一事实图和混合语言设计来解决LLM在网络配置理解中的挑战


<details>
  <summary>更多</summary>
  
**动机:** 现代大规模网络配置复杂，传统基于领域特定语言的方法学习曲线陡峭且灵活性有限，自然语言接口更易用但面临LLM处理长配置、设备异构性和复杂拓扑的挑战

**方法:** 提出树状配置分块策略保持语义连贯性，构建统一事实图标准化厂商特定配置，设计混合命令式-声明式语言减轻LLM推理负担

**结果:** 实验表明NetMind在准确性和可扩展性方面优于现有基线方法

**结论:** NetMind框架有效解决了自然语言网络查询的关键挑战，为网络行为理解提供了更易用和准确的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Network+Behaviors+through+Natural+Language+Question-Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21894&send_immediately=true&force_search=false)

**原文摘要:** Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [91] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang, Han Cui, Yidong Wang, Yijian Tian, Qi Guo, Cunxiang Wang, Jian Wu, Chiyu Song, Yue Zhang*

**主要类别:** cs.CL

**AI概要:** 提出IterSurvey框架，通过迭代式大纲生成和循环检索阅读机制，结合论文卡片和可视化增强，显著提升文献综述的质量和可读性。


<details>
  <summary>更多</summary>
  
**动机:** 现有文献综述生成系统采用一次性检索和静态大纲生成，导致检索噪声、结构碎片化和上下文过载问题，限制了综述质量。

**方法:** 基于循环大纲生成框架，规划代理逐步检索、阅读并更新大纲；设计论文卡片提炼论文核心信息；引入审阅-精炼循环和可视化增强机制。

**结果:** 在既有和新兴主题上，系统在内容覆盖度、结构连贯性和引用质量方面显著优于现有基线，生成更易访问和组织更好的综述。

**结论:** IterSurvey通过模拟人类研究者的迭代阅读过程，结合Survey-Arena评估基准，为自动文献综述生成提供了更可靠和高质量的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Literature+Survey+Automation+with+an+Iterative+Workflow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21900&send_immediately=true&force_search=false)

**原文摘要:** Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [92] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett, Tyler A. Chang, Stella Biderman, Benjamin K. Bergen*

**主要类别:** cs.CL

**AI概要:** 本文研究多语言文本编码中的token溢价现象，通过训练7000个单语分词器分析词汇量大小、预分词策略等对token溢价的影响，发现调整词汇量大小和预分词方法能显著减少跨语言token溢价效应。


<details>
  <summary>更多</summary>
  
**动机:** 不同语言在编码平行文本时所需的token数量存在差异（称为token溢价），高token溢价会降低训练吞吐量并增加推理成本，即使控制数据集大小、词汇量大小和数据内容后，单语分词器在不同语言间仍表现出广泛的token溢价差异。

**方法:** 为97种语言训练约7000个可比较的单语分词器，操纵分词算法、词汇量大小和数据集大小等变量，测量token溢价并测试数据相似性、词汇量大小和预分词等因素的关系，同时研究书写系统和词长等语言特定特征的作用。

**结果:** 训练和测试数据之间的相似性不影响token溢价，但词汇量大小和预分词策略有影响；虽然单纯增加词汇量不能减少token溢价效应，但可以为每种语言确定"最优"词汇量大小来显著降低token溢价；允许在空白处合并的superword分词器既能减少token溢价效应又能提高整体压缩率。

**结论:** 通过干预词汇量大小或预分词器可以显著减少跨语言token溢价效应，这为提高多语言NLP系统的效率和降低成本提供了实用方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+and+Mitigating+Crosslingual+Tokenizer+Inequities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21909&send_immediately=true&force_search=false)

**原文摘要:** The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [93] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk, Aleksander Smywiński-Pohl*

**主要类别:** cs.CL

**AI概要:** MATT方法通过注意力机制改进多语言大模型的tokenizer迁移，相比传统基于语义启发式的方法能更快恢复模型性能


<details>
  <summary>更多</summary>
  
**动机:** 现有tokenizer迁移方法仅依赖语义启发式初始化新嵌入，忽略了高层模型动态，限制了迁移质量

**方法:** 提出Model-Aware Tokenizer Transfer (MATT)，引入Attention Influence Modeling (AIM)目标，将源模型的token间通信模式蒸馏到使用新tokenizer的目标模型中

**结果:** 在多种语言设置下的实验显示，MATT能在几小时GPU时间内恢复原始模型的大部分性能，超越启发式基线方法

**结论:** 整合模型级信号为多语言LLMs中的tokenizer迁移提供了实用有效的路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Aware+Tokenizer+Transfer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21954&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [94] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay, Jiayi Chen, Mohammad J. Latifi, Daniel N. Rockmore, Jeremy R. Manning*

**主要类别:** cs.CL

**AI概要:** 该论文展示如何使用GPT-2模型通过训练单个作者的文本数据来识别不同作者的写作风格，成功验证了作者身份识别的方法，并应用于确认《绿野仙踪》系列第15本书的真实作者。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是利用大语言模型来区分不同作者的写作风格，探索模型如何捕捉和体现特定作者的独特写作特征。

**方法:** 方法是为每位作者单独训练一个GPT-2模型，使用该作者的文本数据进行从头训练，然后比较模型对该作者和其他作者文本的预测准确性。

**结果:** 结果显示，针对特定作者训练的模型能够更准确地预测该作者的文本，成功区分了八位不同作者的写作风格，并确认了R. P. Thompson是《绿野仙踪》第15本书的真实作者。

**结论:** 结论表明大语言模型能够有效捕捉和体现作者的独特写作风格，为作者身份识别提供了新的技术手段，在文学研究和文本分析领域具有应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Stylometric+Application+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21958&send_immediately=true&force_search=false)

**原文摘要:** We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [95] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha*

**主要类别:** cs.CL

**AI概要:** 该论文研究如何利用社会科学中的说服理论来构造对抗性提示，成功绕过大型语言模型的安全对齐机制，发现具有说服结构的提示能显著提高越狱攻击的成功率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有多种越狱攻击策略，但缺乏对语言和心理机制如何影响模型易受攻击性的研究，特别是利用人类说服理论来测试模型的对齐脆弱性。

**方法:** 借鉴社会科学中成熟的说服策略理论，构造具有说服结构的对抗性提示，在多个已对齐的LLMs上进行实证评估，测试这些提示绕过安全防护的效果。

**结果:** 实证评估显示，基于说服理论的提示能显著绕过LLMs的安全保障，成功诱导出越狱行为，证明这种方法在攻击有效性上的优势。

**结论:** 这项工作强调了跨学科见解在应对LLM安全挑战中的重要性，表明模型训练数据中的人类文本模式可能使其对说服性语言更易感，需要新的防护措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+the+Persuasive+Fingerprint+of+LLMs+in+Jailbreaking+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21983&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [96] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball, Niki Hasrati, Alexander Robey, Avi Schwarzschild, Frauke Kreuter, Zico Kolter, Andrej Risteski*

**主要类别:** cs.CL

**AI概要:** 该论文分析了离散优化越狱攻击中后缀迁移性的统计特性，发现拒绝方向激活、拒绝推力和正交位移三个统计属性与迁移成功强相关，而语义相似性相关性较弱。


<details>
  <summary>更多</summary>
  
**动机:** 尽管离散优化越狱攻击中的后缀迁移现象已被实证确认，但缺乏对迁移发生时机和原因的严格分析，需要填补这一研究空白。

**方法:** 通过识别和分析与迁移成功强相关的三个统计属性：(1)无后缀提示激活模型内部拒绝方向的程度，(2)后缀诱导远离拒绝方向的推力强度，(3)正交于拒绝方向的位移大小。

**结果:** 发现这三个统计属性在多种实验设置下与迁移成功强相关，而提示语义相似性仅弱相关。通过干预实验验证了统计分析可转化为攻击成功的实际改进。

**结论:** 研究提供了对越狱攻击迁移性的更细粒度理解，统计分析结果可指导实践攻击效果的提升，揭示了拒绝相关机制在迁移性中的核心作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Understanding+the+Transferability+of+Adversarial+Suffixes+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22014&send_immediately=true&force_search=false)

**原文摘要:** Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [97] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang, Wenda Xu, Zhongtao Liu, Tetsuji Nakagawa, Markus Freitag*

**主要类别:** cs.CL

**AI概要:** 研究发现机器翻译质量评估(QE)指标存在严重的长度偏差问题，会过度惩罚长文本并偏好短翻译，提出了长度归一化和引入参考文本两种缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 质量评估指标在机器翻译中至关重要，但长度偏差的普遍性和影响尚未得到充分研究，可能影响评估准确性和应用效果。

**方法:** 通过对10种不同语言对的顶级回归基和LLM-as-a-Judge QE指标进行系统研究，分析长度偏差现象。

**结果:** 发现两种关键长度偏差：QE指标随翻译长度增加而过度预测错误，即使对无错误高质量文本也是如此；在有多个候选翻译时偏好较短翻译。

**结论:** 长度偏差可能导致不公平评估和次优决策，提出的长度归一化和参考文本两种策略能有效缓解这一问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Penalizing+Length%3A+Uncovering+Systematic+Bias+in+Quality+Estimation+Metrics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22028&send_immediately=true&force_search=false)

**原文摘要:** Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [98] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre, Sneha Kudugunta, Niklas Muennighoff, I-Hung Hsu, Isaac Caswell, Alex Pentland, Sercan Arik, Chen-Yu Lee, Sayna Ebrahimi*

**主要类别:** cs.CL

**AI概要:** 该研究进行了迄今为止最大的多语言缩放定律研究，通过774个多语言训练实验，提出了自适应迁移缩放定律(ATLAS)，在跨语言泛化性能上显著优于现有方法，并揭示了多语言学习动态、语言间迁移特性以及多语言诅咒问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的缩放定律研究主要集中在英语上，但最先进的AI模型需要服务全球数十亿用户，因此需要理解多语言环境下的缩放规律。

**方法:** 进行了774个多语言训练实验，涵盖10M-8B参数规模、400+训练语言和48种评估语言，引入了自适应迁移缩放定律(ATLAS)，并分析了跨语言迁移矩阵、语言无关缩放定律以及计算交叉点。

**结果:** ATLAS在样本外泛化性能上比现有缩放定律平均提升0.3 R²以上；推导出38×38=1444种语言对的相互受益分数；建立了语言无关的缩放定律；确定了从头训练与微调多语言检查点的计算交叉点。

**结论:** 该研究为跨语言缩放定律的民主化提供了科学基础，使实践者能够高效地扩展多语言模型，超越以英语为主的AI发展模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ATLAS%3A+Adaptive+Transfer+Scaling+Laws+for+Multilingual+Pretraining%2C+Finetuning%2C+and+Decoding+the+Curse+of+Multilinguality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22037&send_immediately=true&force_search=false)

**原文摘要:** Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [99] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman, Adar Avsian, Larry Heck*

**主要类别:** cs.CL

**AI概要:** 研究发现LLMs内部存在低维情感流形，情感表征具有方向性编码、跨层分布的特点，且与可解释维度对齐。这种结构在不同深度保持稳定，并泛化到5种语言的8个情感数据集。通过学习的干预模块可以在保持语义的同时操控情感感知。


<details>
  <summary>更多</summary>
  
**动机:** 探究大型语言模型如何在内部表示情感，分析其隐藏状态空间几何结构，以理解LLMs内化和处理情感的机制。

**方法:** 分析LLMs隐藏状态空间的几何结构，识别低维情感流形，研究情感表征的方向性编码和跨层分布特性，使用跨领域对齐和线性探针评估，开发情感干预模块进行操控实验。

**结果:** 发现一致的低维情感子空间，跨领域对齐误差低且线性探针性能强，情感干预模块能有效操控情感感知（特别是基础情感），同时保持语义完整性。

**结论:** LLMs中存在一致且可操控的情感几何结构，这揭示了模型如何内化和处理情感，为理解LLMs的情感表征机制提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emotions+Where+Art+Thou%3A+Understanding+and+Characterizing+the+Emotional+Latent+Space+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22042&send_immediately=true&force_search=false)

**原文摘要:** This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [100] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

**主要类别:** cs.CL

**AI概要:** 本研究比较了六种减少LLM性别偏见的技术，发现监督微调(SFT)在合规性和词汇多样性方面表现最佳，而基于偏好的方法(如DPO)在组合约束任务中失败，表明明确监督对于公平流畅的文本生成至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在性别中立的语境中仍产生性别刻板印象语言，反映了深层社会偏见。现有偏见缓解方法的比较效果和学习动态尚不清楚。

**方法:** 比较分析六种偏见缓解控制技术：仅提示、生成后过滤、DFA-based Ctrl-G解码、监督微调(SFT)、直接偏好优化(DPO)和迭代零空间投影(INLP)，在组合约束任务上评估每种方法。

**结果:** SFT达到99.87%的合规性和高词汇多样性；DPO仅4.53%合规性；Ctrl-G保证完全合规但流畅性和多样性严重下降；偏好学习方法无法满足组合约束。

**结论:** 只有明确的正面监督能够缓解组合偏见，偏好学习无法泛化逻辑结构，凸显了偏好学习的局限性和明确监督对于公平流畅控制生成的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compositional+Bias+Control+in+Large+Language+Models%3A+Preference+Learning+Fails%2C+Supervision+Succeeds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22084&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [101] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

**主要类别:** cs.CL

**AI概要:** 提出一个统一框架来理解和控制LLM的泛化与记忆模式，基于信息瓶颈理论开发动态模式引导算法，在推理时改善逻辑一致性和事实准确性


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型存在泛化能力和死记硬背的双重性问题，这种不可预测性影响了在高风险应用中的可靠性

**方法:** 基于信息瓶颈原理建立理论模型，开发动态模式引导(DMS)算法，包括轻量级线性探针识别记忆依赖和动态激活引导机制转向泛化回路

**结果:** 在推理和真实性任务上的实验表明，DMS显著提高了逻辑一致性和事实准确性

**结论:** DMS提供了一个原则性方法来增强LLM的可靠性，通过自适应自对比解码框架有效控制模型的推理模式

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+or+Memorization%3A+Dynamic+Decoding+for+Mode+Steering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22099&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [102] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson, Zoran Tiganj*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种通过对输入token进行对数压缩来扩展transformer长程记忆能力的方法，而不是修改模型架构本身。


<details>
  <summary>更多</summary>
  
**动机:** 大多数长上下文处理方法通过集成循环或辅助内存模块来增加transformer架构的复杂性，作者希望找到一种更简单的方法。

**方法:** 受人类记忆认知模型启发，对输入token应用尺度不变的对数压缩，生成压缩表示后使用标准未修改的transformer进行处理。

**结果:** 在WikiText-103和PG-19语言建模基准测试中显示，相比未压缩基线，困惑度有所降低，且随着压缩时间上下文的延长，性能持续提升。

**结论:** 输入级别的对数压缩是扩展transformer长程记忆的一种简单有效的方法，同时保持了架构的简洁性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradual+Forgetting%3A+Logarithmic+Compression+for+Extending+Transformer+Context+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22109&send_immediately=true&force_search=false)

**原文摘要:** Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [103] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team, Ang Li, Ben Liu, Binbin Hu, Bing Li, Bingwei Zeng, Borui Ye, Caizhi Tang, Changxin Tian, Chao Huang, Chao Zhang, Chen Qian, Chenchen Ju, Chenchen Li, Chengfu Tang, Chili Fu, Chunshao Ren, Chunwei Wu, Cong Zhang, Cunyin Peng, Dafeng Xu, Daixin Wang, Dalong Zhang, Dingnan Jin, Dingyuan Zhu, Dongke Hu, Fangzheng Zhao, Feifan Wu, Feng Zhu, Gangshan Wang, Haitao Zhang, Hailin Zhao, Hanxiao Zhang, Hanzi Wang, Hao Qian, Haoyi Yu, Heng Zhang, Hongliang Zhang, Hongzhi Luan, Huirong Dong, Huizhong Li, Jia Li, Jia Liu, Jialong Zhu, Jian Sha, Jianping Wei, Jiaolong Yang, Jieyue Ma, Jiewei Wu, Jinjing Huang, Jingyun Tian, Jingyuan Zhang, Jinquan Sun, Juanhui Tu, Jun Liu, Jun Xu, Jun Zhou, Junjie Ou, Junpeng Fang, Kaihong Zhang, Kaiqin Hu, Ke Shi, Kun Tang, Kunlong Chen, Lanyin Mei, Lei Liang, Lei Xu, Libo Zhang, Lin Ju, Lin Yuan, Ling Zhong, Lintao Ma, Lu Liu, Lu Yu, Lun Cai, Meiqi Zhu, Mengying Li, Min Chen, Minghao Xue, Minghong Cai, Mingming Yin, Peijie Jiang, Peilong Zhao, Pingping Liu, Qian Zhao, Qing Cui, Qingxiang Huang, Qingyuan Yang, Quankun Yu, Shaowei Wei, Shijie Lian, Shoujian Zheng, Shun Song, Shungen Zhang, Shuo Zhang, Siyuan Li, Song Liu, Ting Guo, Tong Zhao, Wanli Gu, Weichang Wu, Weiguang Han, Wenjing Fang, Wubin Wang, Xiang Shu, Xiao Shi, Xiaoshun Lan, Xiaolu Zhang, Xiaqing Sun, Xin Zhao, Xingyu Lu, Xiong Xu, Xudong Wang, Xudong Wang, Xuemin Yang, Yajie Yang, Yang Xiang, Yanzhe Li, Yi Zhang, Yilong Wang, Yingxue Li, Yongzhen Guo, Yuzhuo Fu, Yuanyuan Wang, Yue Yang, Yue Yu, Yufeng Deng, Yun Zhang, Yunfei Xu, Yuqi Zhang, Yuxiao He, Zengke Gui, Zhaoxin Huan, Zhaoyang Wang, Zhibo Zhu, Zhihao Wang, Zhiqiang Zhang, Zhoufei Wang, Zihang Zeng, Ziqi Liu, Zitao Xuan, Zuoli Tang*

**主要类别:** cs.CL

**AI概要:** Ling 2.0是一个基于稀疏激活MoE架构的推理导向语言模型系列，参数规模从160亿到1万亿，相比稠密模型实现了最高7倍的计算效率提升，在推理精度和计算效率之间建立了新的帕累托前沿。


<details>
  <summary>更多</summary>
  
**动机:** 通过稀疏激活的混合专家模型架构来提升推理能力，实现从百亿到万亿参数规模的高效扩展，解决传统稠密模型在推理任务中计算效率低下的问题。

**方法:** 采用统一的MoE范式，集成高稀疏度架构、推理导向数据、中训练CoT激活、强化微调(DFT、Evo-CoT)、FP8全规模训练和细粒度异构流水线等技术创新。

**结果:** Ling-1T在万亿参数规模上建立了推理精度与计算效率的新帕累托前沿，相比稠密模型实现了最高7倍的主动计算效率提升。

**结论:** 稀疏激活与推理目标正确对齐时，能够实现可扩展且高效的人工智能，Ling 2.0为未来推理和思维模型的发展提供了连贯、开放和高效的基础架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Every+Activation+Boosted%3A+Scaling+General+Reasoner+to+1+Trillion+Open+Language+Foundation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22115，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22115&send_immediately=true&force_search=false)

**原文摘要:** We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [104] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao, Jundong Shen, Bei Shi, Jiapeng Wang, Ying Ju, Junfeng Yao, Jiao Ran, Yong Zhang, Lin Dong, Huiyu Yu, Tingting Ye*

**主要类别:** cs.CL

**AI概要:** OlaMind是一个基于检索增强生成的人类化、防幻觉客服框架，通过Learn-to-Think学习专家推理过程，结合Learn-to-Respond进行监督微调和强化学习，显著提升智能解决率和降低人工接管率。


<details>
  <summary>更多</summary>
  
**动机:** 现有RAG智能客服系统存在幻觉问题和机械式回复，可能带来业务风险并影响用户体验，需要更人类化且安全的解决方案。

**方法:** 采用Learn-to-Think阶段学习专家推理过程和响应策略，然后通过Learn-to-Respond阶段进行冷启动监督微调(SFT)结合强化学习(RL)的基础到困难自优化。

**结果:** 在工业级社交客服环境中进行大规模A/B测试，社区支持场景智能解决率提升28.92%，人工接管率降低6.08%；直播互动场景智能解决率提升18.42%，人工接管率降低7.12%。

**结论:** OlaMind框架能有效增强人类化程度和自然性，同时显著减少幻觉和关键业务风险，在不同实际应用中展现出一致的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OlaMind%3A+Towards+Human-Like+and+Hallucination-Safe+Customer+Service+for+Retrieval-Augmented+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22143&send_immediately=true&force_search=false)

**原文摘要:** Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [105] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan, Mahendra Kumar Gurve, Anuj, Nitin, Yamuna Prasad*

**主要类别:** cs.CL

**AI概要:** 该论文针对低资源语言迈蒂利语开发了首个可解释情感分析数据集，包含3221个带有情感标签和自然语言解释的句子，为多语言NLP和可解释AI研究提供了宝贵资源。


<details>
  <summary>更多</summary>
  
**动机:** 迈蒂利语作为印度超过1300万人使用的印欧语系语言，在自然语言处理研究中代表性不足，缺乏细粒度标注和可解释机制的情感分析资源。

**方法:** 构建包含3221个迈蒂利语句子的数据集，由语言专家进行情感极性标注并提供自然语言解释，确保标签可靠性和上下文保真度。

**结果:** 通过传统机器学习和先进transformer架构的广泛实验验证了数据集在可解释情感分析方面的有效性。

**结论:** 这项工作为迈蒂利语建立了首个可解释情感计算基准，推动了多语言NLP和可解释AI的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SentiMaithili%3A+A+Benchmark+Dataset+for+Sentiment+and+Reason+Generation+for+the+Low-Resource+Maithili+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22160&send_immediately=true&force_search=false)

**原文摘要:** Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [106] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova, Alessia Battisti, Lukas Fischer, Yingqiang Gao*

**主要类别:** cs.CL

**AI概要:** DETECT是首个德语文本简化评估指标，通过LLM生成合成数据训练，在简洁性、意义保持和流畅性三个维度上全面评估，相比现有指标与人类评估相关性显著提升


<details>
  <summary>更多</summary>
  
**动机:** 当前德语自动文本简化评估依赖通用指标如SARI、BLEU等，无法充分捕捉简化质量，且缺乏专门针对德语的评估指标

**方法:** 基于LENS框架适配德语，使用LLM生成合成质量分数创建数据集，无需人工标注，并采用LLM驱动的评分标准对齐方法

**结果:** DETECT在人类评估相关性方面显著优于广泛使用的ATS指标，在意义保持和流畅性方面提升尤为明显

**结论:** 研究不仅填补了德语文本简化评估空白，还揭示了LLM在自动评估中的潜力和局限性，为语言可访问性任务提供了可迁移指南

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DETECT%3A+Determining+Ease+and+Textual+Clarity+of+German+Text+Simplifications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22212&send_immediately=true&force_search=false)

**原文摘要:** Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [107] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种测量大语言模型在成对文本比较任务中输出错误的方法，通过Copeland计数构建排名来揭示LLM的误差率，并在多个主流模型上验证了方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 测量大语言模型在成对文本比较任务中的输出错误概率，开发不依赖真实标签的评估方法。

**方法:** 使用两种场景测量误差：(i)均匀误差率，通过每个文本对进行两次比较；(ii)二元位置偏差，假设不同比较顺序有不同误差率。通过Copeland计数构建文本来排名。

**结果:** 在6个主流LLM和5种文本类型上获得一致的误差估计，两个位置偏差项相似且接近均匀误差。Claude在误差率和提示鲁棒性方面表现最佳。

**结论:** 该方法优于有偏Bradley-Terry模型和交换性评分，能有效指示LLM在成对比较任务中的错误，揭示了LLM基于成对比较的可扩展性较差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+the+Error+of+Large+Language+Models+at+Pairwise+Text+Comparison，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22219，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22219&send_immediately=true&force_search=false)

**原文摘要:** We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [108] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

**主要类别:** cs.CL

**AI概要:** 本文分析了Swadesh方法在语言年代测定中的局限性，指出即使满足所有假设也存在数学上的精度限制，并提出了词汇渐进修改过程对提高时间分离估计精度的重要性。


<details>
  <summary>更多</summary>
  
**动机:** Swadesh方法在语言年代测定中存在多种不现实的假设和污染现象，导致结果不准确。即使假设全部满足，数学上仍存在精度限制，这些概率性限制在词汇统计学研究中常被忽视。

**方法:** 详细分析了Swadesh方法的概率限制性质，并引入了词汇渐进修改这一随机过程，从概率角度证明该过程对词汇演变的重要贡献。

**结果:** 研究发现词汇渐进修改过程是语言词汇演变的重要驱动力，考虑这一过程能显著提高语言时间分离估计的精度。

**结论:** Swadesh方法存在固有的概率限制，必须同时考虑词汇替换和渐进修改两个随机过程才能获得更准确的语言年代测定结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolution+of+the+lexicon%3A+a+probabilistic+point+of+view，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22220&send_immediately=true&force_search=false)

**原文摘要:** The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [109] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

**主要类别:** cs.CL

**AI概要:** 论文提出了"Sculpting"提示工程方法，相比标准CoT能减少语义模糊和常识错误，但在不同模型上效果不同：在gpt-4o上表现更好，在更先进的gpt-5上反而变差，揭示了提示策略需要与模型能力共同演化。


<details>
  <summary>更多</summary>
  
**动机:** 标准Chain-of-Thought提示存在语义模糊和常识错误的问题，需要开发更精确的约束性提示方法来提升LLM推理能力。

**方法:** 提出基于规则的"Sculpting"提示方法，在GSM8K数学推理基准上对比了三种策略（Zero Shot、标准CoT、Sculpting）在三个OpenAI模型（gpt-4o-mini、gpt-4o、gpt-5）上的表现。

**结果:** 发现"提示反转"现象：Sculpting在gpt-4o上表现更好（97% vs 93%），但在gpt-5上反而变差（94.00% vs 96.36%）。约束性提示在中等模型中防止错误，但在先进模型中导致过度字面化。

**结论:** 最优提示策略需要与模型能力共同演化，更强大的模型可能需要更简单的提示方式，避免过度约束导致性能下降。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是You+Don%27t+Need+Prompt+Engineering+Anymore%3A+The+Prompting+Inversion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22251&send_immediately=true&force_search=false)

**原文摘要:** Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [110] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao, Ming Yan, Yilun Qiu, Haoting Ni, Yang Zhang, Fuli Feng, Hong Cheng, Tat-Seng Chua*

**主要类别:** cs.CL

**AI概要:** SteerX是一种基于因果推理的个性化LLM激活导向方法，通过分离偏好驱动和偏好无关的组件，提升个性化效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有激活导向方法使用所有历史数据计算导向向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号。

**方法:** 基于因果推理理论，估计token级别的因果效应来识别偏好驱动的token，将这些离散信号转换为连贯描述，然后用于导向个性化LLM生成。

**结果:** 在两个代表性导向骨干方法和真实数据集上的实验表明，SteerX持续提升导向向量质量。

**结论:** SteerX通过关注真正偏好驱动的信息，提供了一种更有效的LLM个性化实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SteerX%3A+Disentangled+Steering+for+LLM+Personalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22256&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [111] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou, Denis Cavallucci*

**主要类别:** cs.CL

**AI概要:** 提出了PatenTEB专利文本嵌入基准，包含15个任务206万样本，通过多任务训练开发了patembed模型家族，在专利检索和聚类任务上达到SOTA性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准无法充分捕捉专利文本特有的挑战，如非对称片段到文档匹配等专利特定场景

**方法:** 构建包含检索、分类、复述和聚类的综合基准，采用领域分层分割和领域特定难负样本挖掘，通过多任务训练开发不同参数规模的patembed模型

**结果:** patembed-base在MTEB BigPatentClustering.v2上达到0.494 V-measure（之前最佳为0.445），patembed-large在DAPFAM上达到0.377 NDCG@100，多任务训练显著提升泛化能力

**结论:** PatenTEB基准有效解决了专利文本嵌入的评估需求，多任务训练和领域预训练初始化在不同任务类型中均带来一致优势，所有资源将开源提供

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PatenTEB%3A+A+Comprehensive+Benchmark+and+Model+Family+for+Patent+Text+Embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22264&send_immediately=true&force_search=false)

**原文摘要:** Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [112] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh, Philipp Nicolas Schumacher, Jan Niehues*

**主要类别:** cs.CL

**AI概要:** 该研究探讨了如何利用大学课程材料（讲义幻灯片和文字记录）提升大语言模型在计算机科学课程问答中的表现，比较了检索增强生成(RAG)和持续预训练(CPT)两种方法，发现RAG更有效且多模态方法（将幻灯片作为图像处理）能显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型在教育领域有应用潜力，但在大学计算机科学课程中回答问题的准确性仍然不足，需要探索如何有效利用课程特定材料来提升模型性能。

**方法:** 研究比较了两种策略：检索增强生成(RAG)和持续预训练(CPT)。对于讲义幻灯片，还探索了多模态RAG方法，将检索内容以图像形式呈现给生成器。

**结果:** 实验表明，考虑到大学课程材料规模相对较小，RAG比CPT更有效且高效。多模态设置中将幻灯片作为图像处理比纯文本检索显著提升了性能。

**结论:** 这些发现为开发更好支持学习和教学的AI助手提供了实用策略，并希望能在其他教育场景中激发类似的研究努力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Slides+to+Chatbots%3A+Enhancing+Large+Language+Models+with+University+Course+Materials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22272&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [113] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

**主要类别:** cs.CL

**AI概要:** 该论文比较了三种临床命名实体识别方法在CADEC语料库上的表现：BERT类编码器、GPT-4o少样本学习以及GPT-4o监督微调，发现监督微调获得最佳性能（F1≈87.1%）


<details>
  <summary>更多</summary>
  
**动机:** 研究临床命名实体识别任务，比较不同模型家族在CADEC语料库上的性能表现，探索大语言模型在医疗NER任务中的应用效果

**方法:** 使用CADEC语料库，评估三类方法：BERT类编码器（BERT Base、BioClinicalBERT、RoBERTa-large）、GPT-4o少样本学习（简单vs复杂提示）以及GPT-4o监督微调，基于五种实体类型进行标准NER指标评估

**结果:** RoBERTa-large和BioClinicalBERT相比BERT Base改进有限；简单提示的少样本学习优于复杂提示；监督微调获得最佳整体性能（F1≈87.1%）；大语言模型在简化的二分类任务上准确率更高

**结论:** 监督微调在大语言模型上可获得最佳临床NER性能，但成本较高；简单提示策略优于复杂提示；BERT类模型的改进空间有限；任务简化可提升大语言模型准确率

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Fine-Tuning+or+In-Context+Learning%3F+Evaluating+LLMs+for+Clinical+NER，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22285&send_immediately=true&force_search=false)

**原文摘要:** We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [114] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch, Ainhoa Risco Patón, Teun Buijse, Peter Berck, Maarten van Gompel*

**主要类别:** cs.CL

**AI概要:** 基于记忆的语言建模作为深度神经网络语言建模的高效环保替代方案，提供对数线性可扩展的下一个词预测性能和强大记忆能力


<details>
  <summary>更多</summary>
  
**动机:** 提出一种更高效、环保的语言建模方法，减少深度神经网络在训练和推理过程中的生态足迹

**方法:** 实现基于k最近邻分类的快速近似方法，完全依赖CPU运行，实现低延迟的基于记忆的语言建模系统OLIFANT

**结果:** 与GPT-2和GPT-Neo相比，在下一个词预测准确率、排放估算和速度方面表现出色

**结论:** 基于记忆的语言建模是一种简单透明、生态友好的有效替代方案，具有实际应用价值

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-based+Language+Models%3A+An+Efficient%2C+Explainable%2C+and+Eco-friendly+Approach+to+Large+Language+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22317，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22317&send_immediately=true&force_search=false)

**原文摘要:** We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [115] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines, Bonnie Dorr*

**主要类别:** cs.CL

**AI概要:** 该论文提出了首个多语言目标立场提取(TSE)基准，涵盖6种语言，开发了无需为每种语言单独训练模型的多语言TSE流程，并分析了多语言TSE任务的挑战性。


<details>
  <summary>更多</summary>
  
**动机:** 社交媒体数据分析需要多语言支持，现有TSE研究仅限于英语，缺乏多语言基准和评估标准。

**方法:** 将原始TSE流程扩展到多语言环境，使用统一模型处理加泰罗尼亚语、爱沙尼亚语、法语、意大利语、中文和西班牙语语料库。

**结果:** 模型获得12.78的F1分数，显示多语言任务比单语言更具挑战性，目标预测是主要瓶颈，并首次证明了F1分数对目标表达方式的敏感性。

**结论:** 该研究为多语言TSE提供了急需的基准资源、算法和评估标准，为后续研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilingual+Target-Stance+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22334&send_immediately=true&force_search=false)

**原文摘要:** Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [116] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl, Majid Asgari-Bidhendi, Behrooz Minaei-Bidgoli*

**主要类别:** cs.CL

**AI概要:** FAIR-RAG是一个新型的代理框架，通过结构化证据评估和迭代优化循环，显著提升了多跳问答任务中的检索增强生成性能，在HotpotQA等基准上取得了新的最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统识别和填补证据差距的机制，容易传播噪声或无法收集全面上下文。

**方法:** 提出FAIR-RAG框架，采用结构化证据评估(SEA)模块作为分析门控机制，将查询分解为需求清单并审计证据以识别确认事实和信息差距，通过自适应查询优化代理生成针对性子查询来检索缺失信息，形成迭代优化循环。

**结果:** 在HotpotQA、2WikiMultiHopQA和MusiQue等基准测试中，FAIR-RAG显著优于强基线方法。在HotpotQA上获得0.453的F1分数，比最强迭代基线绝对提升8.3个百分点，建立了该类方法的新最先进水平。

**结论:** 结构化、证据驱动的优化过程与显式差距分析对于在复杂知识密集型任务中实现可靠准确的RAG系统推理至关重要，FAIR-RAG框架为此提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAIR-RAG%3A+Faithful+Adaptive+Iterative+Refinement+for+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22344&send_immediately=true&force_search=false)

**原文摘要:** While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [117] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad, Nisar Hussain, Amna Qasim, Momina Hafeez, Muhammad Usman Grigori Sidorov, Alexander Gelbukh*

**主要类别:** cs.CL

**AI概要:** 该研究通过将英文讽刺语料库翻译成乌尔都语，评估了多种机器学习算法和Transformer模型在乌尔都语讽刺识别任务中的表现，发现LLaMA 3 (8B)模型取得了最佳性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决乌尔都语这种语法和文化背景不同的低资源语言中的讽刺识别挑战

**方法:** 将英文讽刺语料库翻译成乌尔都语，使用GloVe和Word2Vec嵌入评估10种机器学习算法，并对BERT、RoBERTa、LLaMA 2、LLaMA 3和Mistral等Transformer模型进行微调

**结果:** 梯度提升算法获得89.18%的F1分数，LLaMA 3 (8B)获得94.61%的最高F1分数

**结论:** 结合音译技术和现代NLP模型能够在乌尔都语中实现稳健的讽刺检测

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Irony+Detection+in+Urdu+Text%3A+A+Comparative+Study+Using+Machine+Learning+Models+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22356&send_immediately=true&force_search=false)

**原文摘要:** Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [118] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin, Daria Khomich, Nikita Savushkin, Anastasia Ianina, Fyodor Minkin*

**主要类别:** cs.CL

**AI概要:** GigaEmbeddings是一个针对俄语的高性能文本嵌入框架，通过三阶段训练流程在ruMTEB基准测试中取得了69.1分的SOTA效果，参数量更少但性能优于大型基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有方法在处理俄语文本嵌入时的局限性，通过统一多样化目标和利用合成数据生成来提升性能。

**方法:** 采用三阶段训练流程：大规模对比预训练、困难负样本微调、多任务泛化（检索、分类、聚类），结合双向注意力、潜在注意力池化和25%的Transformer层剪枝等架构创新。

**结果:** 在包含23个多语言任务的ruMTEB基准测试中取得了69.1的平均分数，达到了最先进的性能水平，且参数量更少。

**结论:** GigaEmbeddings框架通过分层指令调优和架构优化，成功实现了高性能的俄语文本嵌入，证明了在保持效率的同时提升多任务性能的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GigaEmbeddings%3A+Efficient+Russian+Language+Embedding+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22369&send_immediately=true&force_search=false)

**原文摘要:** We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [119] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie, Zhiyang Zhang, Yifan Wu, Sirong Lu, Jiayi Zhang, Zhaoyang Yu, Jinlin Wang, Sirui Hong, Bang Liu, Chenglin Wu, Yuyu Luo*

**主要类别:** cs.CL

**AI概要:** 本文提出了VisJudge-Bench，首个用于评估多模态大语言模型在可视化质量评估方面能力的综合基准，并开发了专门的可视化美学评估模型VisJudge，显著缩小了与人类专家判断的差距。


<details>
  <summary>更多</summary>
  
**动机:** 可视化质量评估具有挑战性，需要同时判断数据编码准确性、信息表达性和视觉美学。虽然多模态大语言模型在自然图像美学评估中表现良好，但缺乏系统性的可视化评估基准。

**方法:** 构建包含3,090个专家标注样本的VisJudge-Bench基准，涵盖32种图表类型的单图、多图和仪表盘。提出专门的可视化美学评估模型VisJudge。

**结果:** 最先进的MLLMs（如GPT-5）与人类专家的平均绝对误差为0.551，相关性仅为0.429。VisJudge将MAE降低至0.442（减少19.8%），与人类专家的一致性提升至0.681（提高58.7%）。

**结论:** 当前MLLMs在可视化质量评估方面仍与人类专家存在显著差距，而专门设计的VisJudge模型能有效提升评估性能，为可视化质量评估提供了新的解决方案和基准标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VisJudge-Bench%3A+Aesthetics+and+Quality+Assessment+of+Visualizations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22373&send_immediately=true&force_search=false)

**原文摘要:** Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [120] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba, Aman Sinha, Timothee Mickus, Raul Vazquez, Patanjali Bhamidipati, Claudio Savelli, Ahana Chattopadhyay, Laura A. Zanella, Yash Kankanampati, Binesh Arakkal Remesh, Aryan Ashok Chandramania, Rohit Agarwal, Chuyuan Li, Ioana Buhnila, Radhika Mamidi*

**主要类别:** cs.CL

**AI概要:** CAP数据集是一个多语言资源，专注于科学文本生成中大型语言模型的幻觉研究，包含900个科学问题和7000多个模型生成的答案，涵盖9种语言，用于幻觉检测和多语言评估。


<details>
  <summary>更多</summary>
  
**动机:** 科学领域中存在专业术语、统计推理和上下文依赖解释，加上LLM缺乏真正理解、上下文理解有限和偏向表面泛化，导致幻觉问题特别严重，需要专门数据集进行研究。

**方法:** 构建跨语言数据集，覆盖5种高资源语言和4种低资源语言，包含900个精选科学问题和16个公开模型的7000多个答案，每个实例都有二元标签标注科学幻觉存在性和流畅性。

**结果:** 创建了CAP数据集，提供问题-答案对、token序列和对应logits，公开发布以促进幻觉检测、LLM多语言评估和可靠科学NLP系统的开发。

**结论:** CAP数据集为研究科学文本生成中的幻觉问题提供了重要资源，有助于推动多语言环境下LLM的可靠性研究和应用发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Confabulations+from+ACL+Publications+%28CAP%29%3A+A+Dataset+for+Scientific+Hallucination+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22395，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22395&send_immediately=true&force_search=false)

**原文摘要:** We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [121] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong, Cong Wang, Maria Teleki, Millennium Bismay, James Caverlee*

**主要类别:** cs.CL

**AI概要:** CHOIR框架通过协调不同人物设定下的LLM推理信号来提升推理鲁棒性，无需额外训练即可在多个基准测试中显著提升性能


<details>
  <summary>更多</summary>
  
**动机:** 人物设定LLM中的人口统计特征微小变化会导致推理路径差异，这些差异不应被视为偏见而应作为提升推理鲁棒性的资源

**方法:** 提出CHOIR测试时框架，通过协作解码过程协调反事实人物设定的推理信号，动态平衡推理路径的一致性和差异性

**结果:** 在多个推理基准测试中，CHOIR使各人口统计群体性能提升最高达26.4%，平均提升19.2%，且对次优基础人物设定仍有效

**结论:** 通过将人物设定变化重构为建设性信号，CHOIR为提升LLM推理可靠性提供了可扩展和可泛化的方法

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CHOIR%3A+Collaborative+Harmonization+fOr+Inference+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22475&send_immediately=true&force_search=false)

**原文摘要:** Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [122] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang, Zhaxi Zerong*

**主要类别:** cs.CL

**AI概要:** 该研究引入计算语言学方法，通过分析自动语音识别系统对音高处理的敏感性，量化音高在声调发生过程中的功能负荷，揭示了藏语方言从无声调到有声调的连续演变过程。


<details>
  <summary>更多</summary>
  
**动机:** 传统声调发生研究依赖比较重建和声学分析，需要新的计算方法来量化音高在不同声调演变阶段的功能作用。

**方法:** 通过测量音高平坦化处理对自动语音识别性能的影响，分析一组密切相关的藏语方言对音高移除的敏感性。

**结果:** 发现声调发生的连续体证据：无声调的安多方言最能容忍音高移除，完全有声调的卫藏方言表现严重退化，中间状态的康方言处于两者之间。

**结论:** 计算方法能够捕捉音变的细粒度阶段，传统基于最小对立对的功能负荷度量可能高估过渡系统中音高依赖性，因为音段和超音段线索在语音上仍然交织。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Tonogenesis+Continuum+in+Tibetan%3A+A+Computational+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22485&send_immediately=true&force_search=false)

**原文摘要:** Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [123] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian, Junjie Liu, Xican Yang, Haishan Ye, Yan Song*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种简单有效的LLM剪枝方法，在压缩模型参数的同时保持任务特定能力，通过结合通用和任务特定数据计算参数重要性分数，显著优于传统剪枝方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM剪枝方法主要关注保持模型生成流畅句子的能力，但忽略了在特定领域和任务上的性能表现，需要一种能同时保持任务特定能力的剪枝方法。

**方法:** 分析传统剪枝方法在通用领域校准下的损失扰动最小化，将任务特定特征分布纳入重要性计算；分别使用通用和任务特定数据计算重要性分数；基于激活范数差异将参数分为共享组和专属组；融合分数指导剪枝过程。

**结果:** 在广泛使用的基准测试中，该方法有效且一致地优于具有相同剪枝比例和不同设置的基线方法。

**结论:** 提出的剪枝框架能够无缝集成各种基础剪枝技术，在压缩过程中保持LLM的专业化能力，为实际应用提供了有效的模型压缩解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Frustratingly+Easy+Task-aware+Pruning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22489&send_immediately=true&force_search=false)

**原文摘要:** Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [124] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang, Nicolas Ballier, Gina-Anne Levow, Richard Wright*

**主要类别:** cs.CL

**AI概要:** 分析Whisper多语言ASR模型在49种语言上的解码行为，发现子词发现率遵循指数饱和模式，存在声学饱和时间阈值，子词利用更多受语音统计、类型和正字法结构影响而非训练数据规模。


<details>
  <summary>更多</summary>
  
**动机:** 探究需要多少音频才能充分观察多语言ASR模型学习的子词库，以及多语言预训练中的数据差异是否影响推理时这些子词的利用。

**方法:** 通过记录Whisper模型在49种语言推理时的解码候选子词，追踪其随时间累积发现情况，分析模型子词空间的利用模式。

**结果:** 发现子词总数与语言预训练小时数基本无关；子词发现率呈现一致的指数饱和模式；拉丁文字的语言比西里尔、CJK和闪米特文字表现更好；子词长度与资源水平呈正相关。

**结论:** 多语言ASR推理中的子词利用更多受语音的统计、类型和正字法结构约束，而非训练数据规模，这为更公平的语料构建和跨语言评估提供了实证基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Limits+of+Data+Scaling%3A+Sub-token+Utilization+and+Acoustic+Saturation+in+Multilingual+ASR，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22492&send_immediately=true&force_search=false)

**原文摘要:** How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [125] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott, Siyu Liang, Alicia Wassink, Gina-Anne Levow*

**主要类别:** cs.CL

**AI概要:** 对四个主要商业ASR系统的种族偏见进行系统评估，发现语音变异是导致性能差异的主要因素，特别是非洲裔美国说话人受影响最显著


<details>
  <summary>更多</summary>
  
**动机:** 评估商业自动语音识别系统中存在的种族偏见，研究社会语音变异如何导致不同种族群体间的识别性能差异

**方法:** 使用太平洋西北英语语料库，分析四个种族背景说话人的转录准确率，引入启发式语音错误率指标，分析11个社会语音特征

**结果:** 元音质量变异（特别是对低后元音合并和前鼻音合并模式的抵抗）与不同种族群体的错误率差异系统相关，非洲裔美国说话人在所有系统中受影响最显著

**结论:** 方言语音变异的声学建模是商业ASR系统偏见的主要来源，研究为通过训练数据中有针对性地代表社会语音多样性来改进ASR性能提供了可行指导

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Sociophonetic+Analysis+of+Racial+Bias+in+Commercial+ASR+Systems+Using+the+Pacific+Northwest+English+Corpus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22495&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [126] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu, Sahithi Singireddy, Sravani Gona, Sujal Timilsina*

**主要类别:** cs.CL

**AI概要:** 本研究系统评估了在服务条款不公平条款检测任务中，全微调、参数高效适配（LoRA、QLoRA）和零样本提示三种策略的效果，发现全微调性能最优但LoRA方法能以更低内存成本提供竞争力结果


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在文本理解方面表现出色，但在专业法律领域的应用受到全微调成本高的限制，需要探索更高效的适配方法

**方法:** 使用BERT和DistilBERT进行全微调，对TinyLlama、LLaMA 3B/7B和SaulLM应用4位低秩适配（LoRA），并在零样本设置下评估GPT-4o和O版本模型，在CLAUDETTE-ToS基准和多语言抓取语料库上进行实验

**结果:** 全微调获得了最佳的精确率-召回率平衡，而基于LoRA的模型在内存成本降低3倍的情况下仍能提供具有竞争力的召回率

**结论:** 研究结果突显了高效和领域适配LLMs的实际设计权衡，为法律文本处理中的微调研究提供了开放的基线

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text+to+Trust%3A+Evaluating+Fine-Tuning+and+LoRA+Trade-offs+in+Language+Models+for+Unfair+Terms+of+Service+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22531&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [127] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He, Yuxuan Wang, Jiaqi Li, Kexin Liang, Muhan Zhang*

**主要类别:** cs.CL

**AI概要:** LooGLE v2是一个评估大语言模型在现实世界长文本应用中的性能基准，测试结果显示即使是表现最好的模型在长依赖任务中也仅达到59.2%的准确率，表明当前LLMs的实际长文本理解能力远低于其宣称的上下文窗口长度。


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型虽然具备越来越长的上下文窗口，但其在长依赖任务中的实际理解能力仍然有限且缺乏充分研究，特别是在许多现实世界长文本应用中缺乏相应的基准测试。

**方法:** 构建LooGLE v2基准，包含自动收集的16k到2M tokens的真实长文本（法律、金融、游戏和代码领域），设计10种领域特定的长依赖任务类型，生成1,934个多样化和复杂度的QA实例，并对6个本地部署和4个API基础的LLMs进行全面评估。

**结果:** 评估结果显示，表现最好的模型在基准测试中仅获得59.2%的总体分数，表明流行的LLMs实际能理解的上文长度远低于其宣称的能力，在处理长依赖任务方面存在显著局限性。

**结论:** 该研究揭示了LLMs在实际长文本理解能力方面的重大限制，强调了在实用长上下文理解方面模型改进的巨大空间，为未来模型发展提供了重要的基准参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LooGLE+v2%3A+Are+LLMs+Ready+for+Real+World+Long+Dependency+Challenges%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22548，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22548&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [128] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang*

**主要类别:** cs.CL

**AI概要:** SABlock是一种语义感知的自适应块大小KV缓存淘汰框架，通过语义分割和自适应块大小选择，在保持语义完整性的同时显著提升长文本LLM推理的内存效率和速度。


<details>
  <summary>更多</summary>
  
**动机:** KV缓存的内存占用成为长文本LLM推理的可扩展性瓶颈，现有压缩方法难以平衡语义连贯性和内存效率。

**方法:** 1. 语义分割对齐压缩边界与语言结构；2. 段引导的令牌重要性评分；3. 预算驱动的自适应块大小搜索策略

**结果:** 在相同内存预算下优于现有方法，在NIAH任务上仅用96个KV条目实现99.9%检索准确率，内存使用减少46.28%，解码速度提升9.5倍

**结论:** SABlock有效解决了KV缓存内存效率问题，在保持语义完整性的同时显著提升长文本处理性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SABlock%3A+Semantic-Aware+KV+Cache+Eviction+with+Adaptive+Compression+Block+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22556&send_immediately=true&force_search=false)

**原文摘要:** The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [129] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang, Xinyue Zheng, Chunyan Zeng*

**主要类别:** cs.CL

**AI概要:** EduLoop-Agent是一个端到端的个性化学习代理，通过整合神经认知诊断模型、自适应测试策略和大型语言模型，实现了诊断-推荐-反馈的闭环个性化学习框架。


<details>
  <summary>更多</summary>
  
**动机:** 传统个性化学习方法存在模型粗粒度、自适应策略忽略诊断后验信息、反馈非针对性等问题，需要构建完整的闭环系统来解决这些局限性。

**方法:** 采用三模块集成方法：1）神经认知诊断模型（NCD）进行细粒度知识掌握度评估；2）有界能力估计计算机自适应测试策略（BECAT）动态选择最优题目；3）大型语言模型（LLM）生成结构化可操作反馈。

**结果:** 在ASSISTments数据集上的实验显示：NCD模块在响应预测和可解释掌握度评估方面表现优异；自适应推荐策略提高了题目相关性和个性化程度；LLM反馈能针对识别出的弱点提供精准学习指导。

**结论:** 该设计方案有效且具备实际部署可行性，为智能教育中生成个性化学习轨迹提供了可行路径，实现了从诊断到反馈的完整闭环个性化学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Closed-Loop+Personalized+Learning+Agent+Integrating+Neural+Cognitive+Diagnosis%2C+Bounded-Ability+Adaptive+Testing%2C+and+LLM-Driven+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22559&send_immediately=true&force_search=false)

**原文摘要:** As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [130] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya, Ekaterina Kochmar*

**主要类别:** cs.CL

**AI概要:** 本文针对AI教育领域中基于大语言模型的智能辅导系统缺乏标准化评估框架的问题，提出了三个基于学习科学原则的实用研究方向，旨在建立公平、统一且可扩展的评估方法。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI模型在智能辅导系统(ITS)开发中取得显著进展，但由于缺乏可靠、普遍接受且以教学法驱动的评估框架和基准，这些系统的进展和影响难以追踪。现有评估主要依赖主观协议和非标准化基准，导致不一致性和有限的泛化能力。

**方法:** 通过回顾最先进的评估实践，结合真实案例研究分析相关挑战，并基于先前跨学科AI教育研究的见解，提出三个实践性、可行性且理论扎实的研究方向。

**结果:** 识别了当前ITS评估中的主要问题，包括主观性、缺乏标准化和泛化能力有限等挑战。

**结论:** 需要建立基于学习科学原则的公平、统一和可扩展的评估方法论，以推动LLM驱动的智能辅导系统的可靠发展和评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pedagogy-driven+Evaluation+of+Generative+AI-powered+Intelligent+Tutoring+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22581，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22581&send_immediately=true&force_search=false)

**原文摘要:** The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [131] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi, Elena Maria Muià, Federico Siciliano, Giovanni Trappolini, Vincenzo Crisà, Peter Kruger, Fabrizio Silvestri*

**主要类别:** cs.CL

**AI概要:** AutoBench是一个全自动、自维持的LLM评估框架，通过模型间的互评机制实现动态评估，避免传统静态基准的测试集污染问题，并能产生与人类评估一致性更高的稳健结果。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统静态基准测试存在的测试集污染和适应性有限的问题，需要一种能够持续评估不断演进的语言模型的动态评估方法。

**方法:** 采用互评机制，让模型交替扮演问题生成者、参赛者和评委的角色，通过迭代加权机制放大可靠评估者的影响力，将同行判断聚合成基于共识的排名。

**结果:** 实验显示与MMLU-Pro和GPQA基准有强相关性（分别为78%和63%），多评委设计显著优于单评委基线，证明分布式评估能产生更稳健和与人类一致的评估结果。

**结论:** AutoBench提供了一个可扩展、抗污染的替代方案，适用于持续评估不断演进的语言模型，代表了评估范式的重要进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoBench%3A+Automating+LLM+Evaluation+through+Reciprocal+Peer+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22593&send_immediately=true&force_search=false)

**原文摘要:** We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [132] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian, Ramesh Jain*

**主要类别:** cs.CL

**AI概要:** 提出Personal Care Utility (PCU)概念，一个基于AI的全球性健康指导系统，通过多模态数据整合和实时分析，提供个性化健康信息、主动健康导航和治疗响应监测。


<details>
  <summary>更多</summary>
  
**动机:** 基于数字基础设施和生物医学创新的成功经验，旨在构建一个持续协调多模态数据、知识和服务的系统，为个人和群体提供终身健康指导。

**方法:** 采用多模态代理、事件中心建模和上下文推理技术，整合个人感知、体验计算和群体分析，构建环境自适应伴侣系统。

**结果:** PCU系统具备三项核心能力：个性化可信健康信息、主动健康导航和行为指导、医疗事件后的持续恢复和治疗响应解读。

**结论:** PCU不仅能够改善个人健康结果，还为公共卫生和科学发现提供了新的基础平台，但需要解决架构设计和实施挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personal+Care+Utility+%28PCU%29%3A+Building+the+Health+Infrastructure+for+Everyday+Insight+and+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22602&send_immediately=true&force_search=false)

**原文摘要:** Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [133] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani, Mohammadtaha Bagherifard, Erfan Zinvandi, Mehran Sarmadi*

**主要类别:** cs.CL

**AI概要:** PerCoR是首个大规模波斯语常识推理基准，包含10.6万个多选题，采用创新的连接词分割策略和DRESS-AF干扰项生成方法，人类准确率89%，最佳AI模型达到92.18%。


<details>
  <summary>更多</summary>
  
**动机:** 创建首个大规模波斯语常识推理基准数据集，解决波斯语在常识推理任务上缺乏高质量评估资源的问题。

**方法:** 1) 使用连接词分割策略从40多个网络来源生成连贯的句子补全对；2) 提出DRESS-AF方法（基于嵌入相似性评分和对抗过滤的干扰项排序），从正确答案池中选择干扰项以最大化模型混淆。

**结果:** 人类标注者准确率89%，OpenAI-o3模型表现最佳（92.18%），Claude-Sonnet-3.7次之（91.17%），最强开源模型DeepSeek-R1达到82.51%。DRESS-AF方法在英语HellaSwag基准上也有效。

**结论:** PerCoR是一个具有挑战性的波斯语常识推理基准，展示了当前模型与人类性能之间的差距，同时提出的DRESS-AF方法能有效提升数据集难度而不影响人类可解性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PerCoR%3A+Evaluating+Commonsense+Reasoning+in+Persian+via+Multiple-Choice+Sentence+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22616&send_immediately=true&force_search=false)

**原文摘要:** We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [134] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha, Sajal Saha, Debanjan Ballav, Soumi Mitra, Hritwick Chakraborty*

**主要类别:** cs.CL

**AI概要:** 开发了一个三语（Toto-孟加拉语-英语）学习应用，通过AI技术和语言文档化来保护和振兴濒危的Toto语言


<details>
  <summary>更多</summary>
  
**动机:** 保护语言多样性，Toto语言作为濒危语言需要数字化保存和振兴，为母语者和非母语学习者提供可访问的学习工具

**方法:** 通过田野调查收集语言数据，创建形态标记的三语语料库，训练小型语言模型和Transformer翻译引擎，分析屈折形态和派生策略，开发文字标准化和数字素养工具

**结果:** 建立了完整的Toto语言数字档案，开发了功能性的语言学习应用，实现了语言文档化与AI技术的结合

**结论:** 研究提供了一个可持续的濒危语言保护模式，展示了语言学方法与技术创新结合的价值，强调跨学科合作对社区语言振兴的重要性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Linguistics+and+AI%3A+Morphological+Analysis+and+Corpus+development+of+Endangered+Toto+Language+of+West+Bengal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22629&send_immediately=true&force_search=false)

**原文摘要:** Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [135] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis, Lisa Alazraki*

**主要类别:** cs.CL

**AI概要:** 本文介绍了FormaMentis数据集，这是为MRL 2025多语言物理推理共享任务开发的意大利语物理常识推理基准，包含文化特定的标注数据。


<details>
  <summary>更多</summary>
  
**动机:** 为意大利语创建类似PIQA格式的物理常识推理评估数据，弥补非英语语言在该领域的空白。

**方法:** 由熟悉当地习俗的意大利母语专家进行人工标注，创建文化相关的数据样本，并翻译成英文同时保留意大利文化元素。

**结果:** 摘要中未明确说明实验结果

**结论:** 摘要中未明确说明研究结论

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Culturally+Grounded+Physical+Commonsense+Reasoning+in+Italian+and+English%3A+A+Submission+to+the+MRL+2025+Shared+Task，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22631&send_immediately=true&force_search=false)

**原文摘要:** This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [136] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang, Qingtian Zeng, Hua Duan, Cheng Cheng, Minghao Zou, Ziyang Wang*

**主要类别:** cs.CL

**AI概要:** 提出CR-FKGC框架，通过共轭关系建模解决少样本知识图谱补全问题，在三个基准测试中优于现有方法


<details>
  <summary>更多</summary>
  
**动机:** 现有方法难以捕捉复杂关系模式和处理数据稀疏性问题，特别是在少样本场景下

**方法:** 使用邻域聚合编码器整合高阶邻居信息，共轭关系学习器结合隐式条件扩散关系模块和稳定关系模块，以及流形共轭解码器进行高效推理

**结果:** 在三个基准测试中取得了优于最先进方法的性能

**结论:** CR-FKGC框架有效解决了FKGC中的复杂关系建模和数据稀疏性问题，具有优越性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conjugate+Relation+Modeling+for+Few-Shot+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22656，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22656&send_immediately=true&force_search=false)

**原文摘要:** Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [137] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth, Parke Godfrey, Lukasz Golab, Divesh Srivastava, Jarek Szlichta*

**主要类别:** cs.CL

**AI概要:** 本文提出了首个用if-then规则解释检索增强生成(RAG)大语言模型的方法，通过分析检索信息源的存在与否来解释模型输出来源，并提出了优化算法加速规则生成。


<details>
  <summary>更多</summary>
  
**动机:** 现有的if-then规则主要用于解释传统机器学习模型，但缺乏对检索增强生成大语言模型(LLM+RAG)的解释方法，需要开发新的规则生成技术来解释这类新兴模型的输出来源。

**方法:** 提出基于Apriori剪枝思想的优化算法，通过系统性地探测LLM在不同检索源组合下的输出，生成连接信息源存在与否与模型输出之间关系的if-then规则。

**结果:** 通过定性和定量实验验证了所提解决方案的价值和效率，证明该方法能够有效解释RAG增强的LLM系统的输出来源。

**结论:** 该方法填补了用规则解释检索增强大语言模型的空白，为理解和解释这类复杂AI系统的决策过程提供了有效工具，具有重要的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rule-Based+Explanations+for+Retrieval-Augmented+LLM+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22689&send_immediately=true&force_search=false)

**原文摘要:** If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [138] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky, Shai Nahum-Gefen, Elad Ben Zaken*

**主要类别:** cs.CL

**AI概要:** SALSA是一个针对指令调优大语言模型的文本分类优化框架，通过结构化提示、类别到令牌映射和参数高效微调，在单次前向传播中实现高效准确的分类，在多个基准测试中达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 指令调优的大语言模型在文本分类基准测试中表现不佳，需要一种能够避免冷启动训练并提高分类性能的方法。

**方法:** 结合结构化提示、类别到令牌映射和参数高效微调，将每个类别标签映射到不同的输出令牌，构建提示以引发单令牌响应，在推理时仅投影到相关类别令牌的logits上。

**结果:** SALSA在多样化的基准测试中取得了最先进的结果，证明了其在基于LLM的分类应用中的鲁棒性和可扩展性。

**结论:** SALSA提供了一个连贯的管道，有效解决了指令调优LLM在文本分类中的性能问题，通过创新的令牌映射和推理优化实现了高效准确的分类性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SALSA%3A+Single-pass+Autoregressive+LLM+Structured+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22691，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22691&send_immediately=true&force_search=false)

**原文摘要:** Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [139] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao*

**主要类别:** cs.CL

**AI概要:** E²Rank是一个统一的嵌入排序框架，通过继续训练将单个文本嵌入模型扩展到检索和列表重排序任务，在保持高效的同时实现了竞争性的排序性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有文本嵌入模型在检索效率高但排序保真度有限，特别是相比基于LLM的列表重排序器。需要一种既能保持嵌入模型效率又能提升排序性能的统一解决方案。

**方法:** 提出E²Rank框架，使用查询和文档嵌入的余弦相似度作为统一排序函数，通过列表排序目标继续训练嵌入模型，利用top-K文档信号构建增强查询提示。

**结果:** 在BEIR重排序基准上达到最先进结果，在BRIGHT推理密集型基准上表现竞争性，重排序延迟很低，同时MTEB基准上的嵌入性能也得到提升。

**结论:** 单个嵌入模型可以有效统一检索和重排序任务，提供计算效率和竞争性排序准确性的双重优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%5Ctext%7BE%7D%5E2%5Ctext%7BRank%7D%24%3A+Your+Text+Embedding+can+Also+be+an+Effective+and+Efficient+Listwise+Reranker，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22733&send_immediately=true&force_search=false)

**原文摘要:** Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [140] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim*

**主要类别:** cs.CL

**AI概要:** 使用LoRA和高效计算持续预训练方法，在极小数据集上适配魁北克法语方言，仅更新不到1%参数即可提升方言性能且对标准法语性能影响极小


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型主要局限于高资源语言的问题，通过持续预训练方法将LLM适配到低资源方言，扩展高质量LLM对少数语言社区的访问

**方法:** 采用低秩适配(LoRA)和计算高效的持续预训练(CPT)，使用极小数据集适配三个LLM到魁北克法语方言，并在COLE套件上进行基准测试

**结果:** 实验显示在少数方言基准上获得改进，同时在标准语言基准上的性能回归极小(仅更新不到1%参数)，结果增益高度依赖于语料库组成

**结论:** 参数高效微调(PEFT)的持续预训练可以缩小方言差距，为少数语言社区提供成本效益高且可持续的语言资源创建，扩展高质量LLM的访问范围

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Low-Resource+Dialect+Adaptation+of+Large+Language+Models%3A+A+French+Dialect+Case-Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22747，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22747&send_immediately=true&force_search=false)

**原文摘要:** Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [141] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj, Deven Mahesh Mistry, Sahaj Singh Maini, Yash Aggarwal, Zoran Tiganj*

**主要类别:** cs.CL

**AI概要:** 研究发现大型语言模型在上下文学习中存在时间偏差，倾向于优先检索序列开头和结尾的信息，中间信息检索可靠性较低，这种偏差类似于人类情景记忆的时间分离机制。


<details>
  <summary>更多</summary>
  
**动机:** 探究大型语言模型是否能够像人类情景记忆一样，通过时间分离来区分和检索不同时间发生的事件信息。

**方法:** 使用包含重复token的序列提示模型，固定重复token位置并置换其他token以消除语义干扰，分析模型对下一个token的预测概率，并进行消融实验验证transformer中的归纳头机制。

**结果:** 模型始终对重复token后的token赋予最高概率，但存在明显的首尾偏差；中间嵌入的记忆检索可靠性较低；状态空间模型和transformer模型表现出相似的时间偏差。

**结论:** 研究揭示了上下文学习中的时间偏差机制，这些偏差能够实现时间分离和情景检索，加深了对LLM记忆检索机制的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Semantics%3A+How+Temporal+Biases+Shape+Retrieval+in+Transformer+and+State-Space+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22752&send_immediately=true&force_search=false)

**原文摘要:** In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [142] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou, Lutong Yu, You Lyu, Yihang Lin, Zefeng Zhao, Junyi Ao, Yuhao Zhang, Benyou Wang, Haizhou Li*

**主要类别:** cs.CL

**AI概要:** EchoMind是首个多层次的基准测试，通过模拟共情对话的认知过程来评估语音语言模型在整合语言内容、声学线索和情感推理方面的能力，发现当前最先进模型在处理高表现力声学线索和生成真正共情回应方面仍存在显著不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试通常孤立评估语音语言模型的语言、声学、推理或对话能力，而忽视了这些技能整合的重要性，这对于实现类人的情感智能对话至关重要。

**方法:** 提出了EchoMind基准测试，包含四个顺序相关的任务：口语内容理解、声学线索感知、整合推理和回应生成。使用语义中性的脚本和受控的声调变化来独立测试传递效果，基于包含3个粗粒度和12个细粒度维度、39个声学属性的共情导向框架进行评估。

**结果:** 测试12个先进SLM显示，即使最先进的模型也难以处理高表现力的声学线索，限制了共情回应的质量。在指令遵循、对自然语音变化的适应能力以及有效利用声学线索实现共情方面存在持续弱点。

**结论:** 研究结果强调了需要开发能够整合语言内容和多样化声学线索的SLM，以实现真正的共情对话能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EchoMind%3A+An+Interrelated+Multi-level+Benchmark+for+Evaluating+Empathetic+Speech+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22758，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22758&send_immediately=true&force_search=false)

**原文摘要:** Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [143] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem, Muhammad Hazim Al Farouq, John D. Kelleher*

**主要类别:** cs.CL

**AI概要:** 本文提出基于层重要性分析的迭代剪枝方法，用于压缩大语言模型在机器翻译中的部署，显著减小模型大小和推理时间同时保持翻译质量


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在机器翻译中计算需求密集，部署效率面临挑战，需要有效的模型压缩方法

**方法:** 采用迭代层剪枝方法，通过层重要性分析指导剪枝过程，在Aya-Expanse-8B模型上对捷克语-德语和英语-埃及阿拉伯语翻译任务进行实验

**结果:** 方法实现了模型大小和推理时间的显著减少，同时保持了基线模型的翻译质量

**结论:** 迭代层剪枝是有效的模型压缩策略，能够在不牺牲性能的情况下提升大语言模型在机器翻译中的部署效率

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Iterative+Layer+Pruning+for+Efficient+Translation+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22763&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [144] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu, Yilun Zhou, Pranav Narayanan Venkit, Kung-Hsiang Huang, Jiaxin Zhang, Nanyun Peng, Chien-Sheng Wu*

**主要类别:** cs.CL

**AI概要:** 该研究提出了MMPersuade框架，系统评估大型视觉语言模型对多模态说服内容的易感性，发现多模态输入显著增加模型被说服的可能性，特别是在错误信息场景中。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型视觉语言模型在购物、健康、新闻等领域的部署增加，它们面临大量说服性内容。需要了解模型作为被说服者的易感性，因为过度易感的模型可能采纳误导性信念、覆盖用户偏好或生成不道德输出。

**方法:** 开发了MMPersuade框架，包含：(1)包含图像和视频的多模态数据集，涵盖商业、主观行为和对抗性场景；(2)评估框架，通过第三方协议评分和自估计token概率来量化说服效果和模型易感性。

**结果:** 对六个领先LVLM的研究发现：(1)多模态输入相比纯文本显著提高说服效果和模型易感性；(2)已声明的偏好会降低易感性，但多模态信息仍保持说服优势；(3)不同策略在不同场景中效果不同，互惠在商业和主观场景最有效，可信度和逻辑在对抗场景中占优。

**结论:** MMPersuade为开发在面对说服性多模态内容时具有鲁棒性、偏好一致性和伦理对齐的模型提供了原则性基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMPersuade%3A+A+Dataset+and+Evaluation+Framework+for+Multimodal+Persuasion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22768&send_immediately=true&force_search=false)

**原文摘要:** As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [145] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu, Boyin Tan, Xiaoyuan Liu, Chao Peng, Pengfei Gao, Pinjia He*

**主要类别:** cs.CL

**AI概要:** R4P是一个通过推理提供可扩展奖励的补丁验证模型，用于训练和测试软件工程代理，解决了传统测试监督方法不可扩展的问题，在验证准确性和效率上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于测试的监督方法存在两个问题：(1) 测试沙箱构建和运行成本高且脆弱；(2) 高覆盖率测试数据稀缺且易受边缘案例测试攻击威胁，这限制了数据扩展的潜力。

**方法:** 提出R4P补丁验证模型，将补丁验证视为推理任务，采用分组目标进行强化学习训练，通过比较多个补丁的修改来获得密集奖励，实现稳定训练。

**结果:** R4P在SWE-bench-verified上达到72.2%的验证准确率，超越OpenAI o3；基于R4P训练的Mini-SE在SWE-bench-verified上达到26.2% Pass@1，比原Qwen3-32B提升10.0%；验证速度比测试快50倍。

**结论:** R4P通过推理验证补丁提供了可扩展的奖励机制，解决了测试监督的扩展性问题，在准确性和效率上表现出色，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Supervising+Software+Agents+with+Patch+Reasoner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22775&send_immediately=true&force_search=false)

**原文摘要:** While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [146] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim*

**主要类别:** cs.CL

**AI概要:** VEHME是一个基于视觉-语言模型的手写数学表达式评估系统，通过两阶段训练和空间感知提示模块，在开放形式的手写数学解答评估中实现了高精度和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 手写数学解答自动评估在教育技术中很重要但具有挑战性，因为学生作业格式多样、布局非结构化且符号复杂。

**方法:** 采用两阶段训练流程：监督微调使用结构化推理数据，强化学习对齐多维度评分目标；提出表达式感知视觉提示模块增强空间理解。

**结果:** 在AIHub和FERMAT数据集上评估，VEHME在开源模型中达到最先进性能，接近专有系统的准确性。

**结论:** VEHME展示了作为可扩展和易获取的自动化数学评估工具的潜力，训练和实验代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VEHME%3A+A+Vision-Language+Model+For+Evaluating+Handwritten+Mathematics+Expressions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22798&send_immediately=true&force_search=false)

**原文摘要:** Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [147] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova, Amrit Adhikari, Matthew Pearson, Vamsi Krishna Sadu, Mark V. Albert*

**主要类别:** cs.CL

**AI概要:** 本研究首次系统比较商业API和开源大语言模型在多语言人权侵犯检测中的表现，发现对齐训练而非模型规模决定跨语言稳定性，为资源有限的人道组织提供成本-可靠性权衡的实用指导。


<details>
  <summary>更多</summary>
  
**动机:** 人道组织面临关键选择：投资昂贵的商业API还是依赖免费开源模型进行多语言人权监测。商业系统可靠性高但成本昂贵，开源模型缺乏实证验证，特别是在冲突地区常见的低资源语言上。

**方法:** 在78,000次多语言推理中评估6个模型（4个指令对齐商业模型和2个开源模型），使用标准分类指标和新的跨语言可靠性指标：校准偏差、决策偏差、语言鲁棒性分数和语言稳定性分数。

**结果:** 对齐模型在类型学差异大和低资源语言上保持近乎不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。对齐而非规模决定稳定性。

**结论:** 多语言对齐能够实现语言无关的推理，为人道组织在多语言部署中平衡预算约束和可靠性提供了实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Lingual+Stability+and+Bias+in+Instruction-Tuned+Language+Models+for+Humanitarian+NLP，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22823&send_immediately=true&force_search=false)

**原文摘要:** Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [148] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua, Hong Jiao, Xinyi Wang*

**主要类别:** cs.CL

**AI概要:** 本研究探索使用生成式语言模型通过摘要和提示技术进行长文本自动评分，解决了BERT等编码器模型512词元限制的问题，在Learning Agency Lab数据集上将QWK评分准确率从0.822提升至0.8878


<details>
  <summary>更多</summary>
  
**动机:** BERT及其变体在自动评分中广泛应用，但基于编码器的模型有512个词元的长度限制，这限制了其在长论文自动评分中的应用效果

**方法:** 采用生成式语言模型，通过摘要和提示技术来处理长论文的自动评分问题

**结果:** 在Learning Agency Lab Automated Essay Scoring 2.0数据集上，评分准确率显著提升，QWK系数从0.822增加到0.8878

**结论:** 生成式语言模型通过摘要和提示技术能够有效解决长论文自动评分的挑战，显著提高了评分准确性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+of+Summarization+by+Generative+Language+Models+for+Automated+Scoring+of+Long+Essays，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22830，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22830&send_immediately=true&force_search=false)

**原文摘要:** BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [149] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi, Dong Won Lee, Beatriz Flamia, Jasmine David, Brandon Hanks, Cynthia Breazeal, Emma Anderson, Grace Lin*

**主要类别:** cs.CL

**AI概要:** 本研究探讨如何利用明确的对话线程信息提升大语言模型在同步多人对话中对关系性话语行为的编码性能，开发了系统化的线程识别指南并测试了不同提示策略，结果表明线程结构能显著改善下游对话分析任务的效果。


<details>
  <summary>更多</summary>
  
**动机:** 小组对话中思想的发展和流动对分析协作学习至关重要，但同步口语对话中的线程检测因重叠话轮和隐含线索而具有挑战性，同时大语言模型在处理需要追踪对话链接的长上下文任务时存在困难。

**方法:** 开发了同步多方转录本中线程识别的系统指南，对不同LLM提示策略进行基准测试，然后测试线程信息如何影响对话分析框架的下游编码性能。

**结果:** 提供清晰的对话线程信息能提高LLM编码性能，并突显了下游分析对良好结构化对话的重度依赖。

**结论:** 这项工作推进了将LLMs与强大的对话线程结构相结合的方法，以理解复杂的实时群体互动，并讨论了人机混合方法在时间和成本方面的实际权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Large+Language+Models+to+Identify+Conversation+Threads+in+Collaborative+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22844&send_immediately=true&force_search=false)

**原文摘要:** Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [150] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein, Neelay Velingker, Mayur Naik, Eric Wong*

**主要类别:** cs.CL

**AI概要:** PIPS是一种新的推理方法，通过实例级程序合成和结构反馈来提升大语言模型在复杂多步推理任务中的表现，相比CoT和PoT方法显著提高了准确率并减少了不良程序生成。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在零样本推理方面表现优异，但在复杂多步推理任务中仍然存在困难。现有的CoT和PoT方法虽然能提升性能，但经常产生不理想的解决方案，特别是在算法领域。

**方法:** 提出了PIPS方法，它在实例级别生成和精炼程序，使用结构反馈而不依赖任务特定指导或显式测试用例。同时引入置信度指标，动态选择直接推理或程序合成。

**结果:** 在三个前沿LLM和30个基准测试中，PIPS相比PoT和CoT分别提高了8.6%和9.4%的绝对调和平均准确率，在算法任务中将不良程序生成减少了65.1%。

**结论:** PIPS方法有效解决了现有推理方法在复杂任务中的局限性，通过实例级程序合成和动态选择机制显著提升了推理性能和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Once+Upon+an+Input%3A+Reasoning+via+Per-Instance+Program+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22849&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [151] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He, Tianjun Zhong, Richard Antonello, Gavin Mischler, Micah Goldblum, Nima Mesgarani*

**主要类别:** cs.CL

**AI概要:** 该研究提出了一种残差解耦方法，从大语言模型中分离出词汇、句法、语义和推理四个正交表征，用于建模大脑对语言的处理过程，发现推理表征具有独特的神经预测能力和时间特征。


<details>
  <summary>更多</summary>
  
**动机:** 现代大语言模型内部表征高度"纠缠"，混合了词汇、句法、语义和推理信息，这使传统脑编码分析偏向浅层语言特征，难以分离深层认知过程的神经基础。

**方法:** 使用残差解耦方法，通过探测语言模型识别特征特定层，迭代回归掉低层表征，生成四个近乎正交的嵌入表征（词汇、句法、语义和推理）。

**结果:** 1) 推理嵌入具有独特预测能力，能解释其他语言特征无法解释的神经活动方差，甚至扩展到经典语言区外的视觉区域；2) 推理神经信号时间特征独特，峰值较晚(~350-400ms)；3) 标准未解耦嵌入会产生误导，其预测成功主要归因于浅层语言特征。

**结论:** 该方法成功分离了语言处理的不同认知成分，揭示了推理过程的独特神经机制，表明标准语言模型嵌入可能掩盖深层认知处理的贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Far+from+the+Shallow%3A+Brain-Predictive+Reasoning+Embedding+through+Residual+Disentanglement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22860&send_immediately=true&force_search=false)

**原文摘要:** Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [152] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy, Ayush Rajesh Jhaveri, Ilias Triantafyllopoulos*

**主要类别:** cs.CL

**AI概要:** 研究发现大语言模型存在不确定性现象，即模型在重新提示时会改变之前正确的答案。通过注意力头分析识别出导致该问题的关键注意力头，掩蔽这些头可将不确定性行为减少15%，但在下游任务中存在权衡。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在重新提示时会出现不确定性现象，将正确答案改为错误答案，这种行为在高风险领域会破坏信任并带来严重风险，需要研究其机制并找到缓解方法。

**方法:** 采用Needle-in-a-Haystack检索框架，集成Flip式重新评估提示来模拟真实的答案翻转场景，分析注意力头机制，识别并掩蔽导致不确定性的关键注意力头。

**结果:** 发现检索头不是避免不确定性的主要原因，识别出一小部分非检索注意力头会过度关注误导性标记。掩蔽这些头可将翻转行为减少高达15%，且不引入不连贯或过度校正问题。

**结论:** 研究为机制可解释性领域做出贡献，提供了一种简单有效的技术来缓解LLMs中的不确定性驱动故障模式，但在下游任务应用中需要权衡考虑。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+and+Mitigating+Unwanted+Uncertainty+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22866&send_immediately=true&force_search=false)

**原文摘要:** Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [153] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Amit Sheth, Vasu Sharma, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amitava Das*

**主要类别:** cs.CL

**AI概要:** 本研究构建了一个包含5.8万+文本样本的大规模数据集，结合纽约时报真实文章和多个先进LLM生成的合成文本，为AI生成文本检测和模型溯源任务建立了基线性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型生成文本越来越接近人类水平，引发了内容真实性、错误信息和可信度担忧，需要大规模、多样化且标注良好的数据集来可靠检测AI生成文本并溯源到具体模型。

**方法:** 创建包含58,000+文本样本的综合数据集，结合真实纽约时报文章和Gemma-2-9b、Mistral-7B、Qwen-2-72B、LLaMA-8B、Yi-Large、GPT-4-o等多个先进LLM生成的合成版本，提供原始文章摘要作为提示和完整人类撰写叙述。

**结果:** 建立了两个关键任务的基线结果：区分人类撰写与AI生成文本的准确率达到58.35%，将AI文本溯源到生成模型的准确率为8.92%。

**结论:** 通过将真实世界新闻内容与现代生成模型结合，该数据集旨在促进鲁棒检测和溯源方法的发展，在生成式AI时代培养信任和透明度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Dataset+for+Human+vs.+AI+Generated+Text+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22874&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [154] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang*

**主要类别:** cs.CL

**AI概要:** 论文提出EQSPEC和EXSPEC两种批量推测解码方法，解决了批量处理中的不规则张量问题，在保证95%输出等价性的同时，实现了最高3倍的吞吐量提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的批量推测解码实现存在不规则张量问题，导致序列位置ID、注意力掩码和KV缓存状态损坏，违反了输出等价性要求。

**方法:** 提出EQSPEC方法确保正确性，分析重对齐开销占40%；引入EXSPEC方法维护滑动序列池并动态形成相同长度组，减少重对齐开销。

**结果:** 在SpecBench数据集上，Vicuna-7B/68M、Qwen3-8B/0.6B和GLM-4-9B/0.6B模型对上，批量大小为8时相比批量大小为1实现最高3倍吞吐量提升，保持95%输出等价性。

**结论:** 该方法无需自定义内核即可与现有推理栈集成，有效解决了批量推测解码的正确性问题，同时显著提升推理效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Batch+Speculative+Decoding+Done+Right，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22876&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [155] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang, Lanser Contributors*

**主要类别:** cs.CL

**AI概要:** Lanser-CLI是一个CLI优先的编排层，通过固定和中介语言服务器协议(LSP)服务器，为编码代理和CI提供确定性、可重放的工作流程，解决了大语言模型在API幻觉和编辑定位方面的不可靠性问题。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型经常产生API幻觉和错误定位编辑，而语言服务器能够计算关于真实代码的经过验证的IDE级事实。需要一种方法将语言服务器的确定性能力引入到编码代理和CI工作流中。

**方法:** 开发Lanser-CLI工具，包含：1) 通过Selector DSL提供稳健的寻址方案；2) 确定性分析包规范化语言服务器响应；3) 为变异操作提供安全保护机制；4) 基于语言服务器事实的过程奖励函数。

**结果:** 实现了在冻结快照下的确定性操作，建立了过程奖励的单调性属性，使其适用于过程监督和反事实分析。

**结论:** 语言服务器不仅提供结构信息，还提供可操作的过程奖励，Lanser-CLI成功地将这些能力整合到编码代理和CI工作流中，提高了可靠性和确定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Server+CLI+Empowers+Language+Agents+with+Process+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22907&send_immediately=true&force_search=false)

**原文摘要:** Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [156] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi*

**主要类别:** cs.CL

**AI概要:** 该论文介绍了Infinity-Chat数据集，这是首个大规模开放查询数据集，用于系统研究语言模型在开放生成任务中的多样性问题和人工蜂群效应。


<details>
  <summary>更多</summary>
  
**动机:** 语言模型在生成多样化、类人创意内容方面存在困难，可能导致人类思维的长期同质化，但目前缺乏可扩展的方法来评估LM输出多样性。

**方法:** 创建包含26K个多样化真实用户查询的Infinity-Chat数据集，建立首个全面的开放提示分类法（6大类17子类），并进行大规模模式崩溃研究，包括31,250个人工标注。

**结果:** 发现语言模型存在显著的人工蜂群效应：模型内部重复（单个模型生成相似回答）和模型间同质性（不同模型产生惊人相似的输出）。LM、奖励模型和LM评判者对引发不同个体偏好的人类评分校准较差。

**结论:** Infinity-Chat为系统研究真实世界开放查询提供了首个大规模资源，揭示了减轻人工蜂群效应带来的长期AI安全风险的关键见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Hivemind%3A+The+Open-Ended+Homogeneity+of+Language+Models+%28and+Beyond%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22954&send_immediately=true&force_search=false)

**原文摘要:** Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [157] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam*

**主要类别:** cs.CL

**AI概要:** 提出TAG（标签增强生成）方法，通过在长文本QA任务中为上下文添加标签或标签定义，显著提升大语言模型在长上下文场景下的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有大语言模型在处理长复杂上下文时存在有效性问题，虽然RAG和分块重排序等方法试图缓解，但它们对分块、嵌入和检索策略敏感，且需要大量预处理步骤。

**方法:** 提出TAG方法，一种轻量级数据增强策略，通过为检索到的文档添加上下文标签或标签定义来增强提示，而不改变文档的完整性和组成结构。

**结果:** 在两个具有挑战性的QA基准测试（NoLima和NovelQA）上验证，结果显示：在32K token上下文中性能提升高达17%，在需要跨文本知识的多跳复杂推理问答中提升2.9%。

**结论:** TAG是一种有效且轻量的方法，能够显著提升LLM在长上下文场景中的表现，无需复杂的预处理步骤，为长文本处理提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tagging-Augmented+Generation%3A+Assisting+Language+Models+in+Finding+Intricate+Knowledge+In+Long+Contexts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22956&send_immediately=true&force_search=false)

**原文摘要:** Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [158] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个系统化方法来评估长文本的事实准确性，包括构建中文长文本数据集LongHalluQA、开发多智能体验证系统MAD-Fact，并引入事实重要性层级概念。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对输出事实准确性的担忧，现有评估方法难以处理长文本的复杂推理链和交织观点。

**方法:** 整合大规模长文本数据集、多智能体验证机制和加权评估指标，构建中文长文本事实性数据集LongHalluQA，开发基于辩论的多智能体验证系统MAD-Fact。

**结果:** 在两个基准测试上的实验表明，大型LLM通常保持更高的事实一致性，而国内模型在中文内容上表现更佳。

**结论:** 该研究为评估和增强长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAD-Fact%3A+A+Multi-Agent+Debate+Framework+for+Long-Form+Factuality+Evaluation+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22967&send_immediately=true&force_search=false)

**原文摘要:** The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [159] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

**主要类别:** cs.CL

**AI概要:** 本研究开发了基于句子嵌入的定制化大语言模型，用于课堂教学质量评估，实现了接近甚至超越人类专家水平的评分性能，并与教师增值测量结果具有外部效度一致性。


<details>
  <summary>更多</summary>
  
**动机:** 解决教育领域中教学质量的客观可扩展测量难题，传统通用大语言模型在应用复杂课堂观察工具时表现不稳定。

**方法:** 使用句子级嵌入架构构建定制化LLMs，系统评估五种不同句子嵌入模型，采用防过拟合的数据高效训练策略，分析标注上下文窗口。

**结果:** 专业模型达到人类专家水平（相关系数>0.65），超越平均人-人评分相关性；高级模型更多关注课程层面特征而非孤立话语；总体评分与教师增值测量一致但单项评分未完全泛化。

**结论:** 建立了AI驱动的教学测量新方法，为实现可扩展、可靠、有效的教育者发展反馈提供了可行路径，但模型尚未实现完全泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Teaching+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22968&send_immediately=true&force_search=false)

**原文摘要:** Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [160] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang, Timothy Tin-Long Tse, Jian Zhu*

**主要类别:** cs.CL

**AI概要:** 该论文通过行为探测和干预方法评估了不同架构大语言模型在上下文学习任务中的表现，发现虽然性能相似但内部机制不同，功能向量主要位于自注意力和Mamba层，且在不同知识类型任务中作用各异。


<details>
  <summary>更多</summary>
  
**动机:** 研究不同架构大语言模型（包括transformer、状态空间和混合模型）在基于知识的上下文学习任务中的内部机制差异，尽管它们在任务表现上可能相似。

**方法:** 使用行为探测和基于干预的方法，对最先进的transformer、状态空间和混合大语言模型进行深入评估，分析功能向量在自注意力和Mamba层中的分布和作用机制。

**结果:** 发现不同架构LLMs在任务性能上表现相似但内部机制不同；功能向量主要位于自注意力和Mamba层；Mamba2可能使用不同于功能向量的机制进行ICL；功能向量在参数知识检索任务中更重要，而在上下文知识理解中作用较小。

**结论:** 研究强调了结合行为和机制分析的重要性，为理解不同架构和任务类型下的LLM能力提供了更细致的见解，揭示了ICL机制在不同模型架构中的差异性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+In-Context+Learning+Beyond+Transformers%3A+An+Investigation+of+State+Space+and+Hybrid+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23006&send_immediately=true&force_search=false)

**原文摘要:** We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [161] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim*

**主要类别:** cs.CL

**AI概要:** LangLingual是一个基于LangChain框架和大型语言模型构建的对话代理，专门为语言学习者提供实时语法反馈、上下文感知的语言练习和学习进度追踪。


<details>
  <summary>更多</summary>
  
**动机:** 语言教育者希望在有限反馈和练习资源的情况下为学习者创造丰富的学习体验

**方法:** 使用LangChain框架和大型语言模型构建对话代理系统，设计实现实时语法反馈、上下文感知练习生成和学习进度追踪功能

**结果:** 系统表现出良好的可用性、积极的学习成果和令人鼓舞的学习者参与度

**结论:** LangLingual系统通过AI技术有效解决了语言教育中反馈和练习资源有限的问题，为学习者提供了高质量的语言学习支持

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LangLingual%3A+A+Personalised%2C+Exercise-oriented+English+Language+Learning+Tool+Leveraging+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23011&send_immediately=true&force_search=false)

**原文摘要:** Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [162] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu*

**主要类别:** cs.CL

**AI概要:** TIR-Judge是一个端到端的强化学习框架，通过集成代码执行器来训练LLM评估器，在多个基准测试中显著超越基于推理的评估器，并证明了无需蒸馏即可通过迭代强化学习实现自我进化。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LLM评估器主要基于文本推理，难以验证复杂约束或进行精确计算，而工具集成推理在其他任务中已证明有效，因此需要开发能够集成工具进行精确评估的LLM评估器。

**方法:** 提出TIR-Judge框架，基于三个原则：(1)在可验证和不可验证领域进行多样化训练；(2)支持灵活评估格式（点对点、成对、列表式）；(3)无需蒸馏的迭代强化学习，直接从初始模型进行引导。

**结果:** 在7个公共基准测试中，TIR-Judge在点对点评估中超越强推理基准6.4%，在成对评估中超越7.7%，列表式性能与Claude-Opus-4相当（仅8B参数）。TIR-Judge-Zero（无蒸馏训练）性能与蒸馏变体相当。

**结论:** 工具增强的评估器可以通过迭代强化学习实现自我进化，TIR-Judge框架为训练高性能LLM评估器提供了有效解决方案，显著提升了评估准确性和计算能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incentivizing+Agentic+Reasoning+in+LLM+Judges+via+Tool-Integrated+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23038&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [163] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li*

**主要类别:** cs.CL

**AI概要:** 提出Knocking-Heads Attention (KHA)机制，通过在多头注意力计算前引入跨头特征交互，解决了传统多头注意力机制中头间缺乏强交互的问题，以极小参数代价提升模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统多头注意力机制(MHA)及其变体(GQA、GTA)只是简单拼接各头的输出，缺乏头间的强交互，增加头数会削弱单个头的表示能力。

**方法:** 提出KHA机制，在所有头上应用共享的对角初始化投影矩阵，使注意力头在缩放点积注意力计算前能够相互"敲击"，实现跨头特征级交互。

**结果:** 在6.1B参数的MoE模型上验证，KHA相比基线注意力机制带来了更优越和稳定的训练动态，在下游任务中表现更好。

**结论:** KHA通过最小参数和计算开销实现了跨头交互，可无缝集成到各种注意力变体中，有效提升了多头注意力的表示能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knocking-Heads+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23052&send_immediately=true&force_search=false)

**原文摘要:** Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [164] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon, Byeolhee Kim, Nikhil Verma*

**主要类别:** cs.CL

**AI概要:** QTT-RAG通过在翻译文档上添加质量评估标签（语义等价性、语法准确性、自然流畅性）来改进多语言检索增强生成，避免改写方法导致的事实失真问题，在低资源语言场景下显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的mRAG方法要么假设翻译质量足够好，要么使用改写方法但会引入事实失真和幻觉问题，特别是在低资源语言环境下翻译质量差会严重影响生成性能。

**方法:** 提出QTT-RAG方法，明确评估翻译质量的三个维度（语义等价性、语法准确性、自然流畅性），并将评分作为元数据附加到原始内容上而不改变内容本身。

**结果:** 在两个开放域QA基准测试（XORQA、MKQA）中使用6个指令调优LLM（2.4B-14B参数）进行评测，覆盖韩语、芬兰语（低资源）和中文（高资源），QTT-RAG在保持事实完整性的同时优于CrossRAG和DKM-RAG基线方法。

**结论:** QTT-RAG通过让生成模型基于翻译可靠性做出明智决策，在低资源环境下有效利用跨语言文档，为多语言领域提供了实用且鲁棒的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quality-Aware+Translation+Tagging+in+Multilingual+RAG+system，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23070&send_immediately=true&force_search=false)

**原文摘要:** Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [165] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai*

**主要类别:** cs.CL

**AI概要:** 该论文对大型语言模型的中期训练进行了系统调查，定义了中期训练作为连接预训练和后训练的关键阶段，分析了数据管理、训练策略和模型架构优化框架，并提供了分类和实践见解。


<details>
  <summary>更多</summary>
  
**动机:** 基础模型的最新进展凸显了多阶段训练的重要性，特别是中期训练作为连接预训练和后训练的关键桥梁阶段，需要系统性地研究和定义。

**方法:** 通过形式化定义LLM的中期训练概念，调查优化框架包括数据管理、训练策略和模型架构优化，分析主流模型实现中的目标驱动干预方法。

**结果:** 明确了中期训练在LLM能力渐进发展中的独特和关键作用，系统性地增强了数学、编程、推理和长上下文扩展等特定能力。

**结论:** 通过阐明中期训练的独特贡献，该调查提供了全面的分类和可操作的见解，支持LLM发展的未来研究和创新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+LLM+Mid-training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23081&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [166] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han*

**主要类别:** cs.CL

**AI概要:** MAP4TS是一个多维度提示框架，通过将经典时间序列分析方法融入提示设计，显著提升了LLM在时间序列预测中的性能表现


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态方法忽视了时间序列数据特有的统计特性和时间依赖性，需要将经典时间序列分析整合到LLM提示设计中

**方法:** 提出包含四个专门提示组件的多维度提示框架：全局域提示、局部域提示、统计提示和时间提示，结合自相关、偏自相关和傅里叶分析的手工洞察，通过跨模态对齐模块生成统一表示

**结果:** 在八个不同数据集上的实验表明，MAP4TS始终优于最先进的基于LLM的方法，GPT-2骨干网络结合结构化提示在长期预测任务中表现优于LLaMA等更大模型

**结论:** 多维度提示设计显著提升了性能稳定性，将经典时间序列分析与LLM结合是提升时间序列预测效果的有效途径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAP4TS%3A+A+Multi-Aspect+Prompting+Framework+for+Time-Series+Forecasting+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23090，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23090&send_immediately=true&force_search=false)

**原文摘要:** Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [167] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang*

**主要类别:** cs.CL

**AI概要:** 本文研究在医学多文档摘要任务中使用层次结构输入是否能比传统平面摘要方法更好地组织信息。研究发现层次结构方法能保持事实性、覆盖面和连贯性，同时提高人类对摘要的偏好，GPT-4模拟判断与人类判断在客观评估维度上具有较高一致性。


<details>
  <summary>更多</summary>
  
**动机:** 医学多文档摘要任务需要有效管理跨文档关系，传统平面摘要方法在组织信息和上下文关联方面存在局限，因此研究层次结构输入是否能改善模型的信息组织能力。

**方法:** 研究两种层次结构组织方式，在三个大型语言模型上进行实验，使用自动化指标、基于模型的指标以及领域专家评估（偏好、可理解性、清晰度、复杂性、相关性、覆盖度、事实性和连贯性）进行综合评估。

**结果:** 人类专家更偏好模型生成的摘要而非人工撰写的摘要；层次方法在保持事实性、覆盖面和连贯性的同时提高了人类偏好；GPT-4模拟判断与人类判断在客观评估维度上具有较高一致性。

**结论:** 层次结构可以提高医学摘要的清晰度，同时保持内容覆盖度，为提高人类对生成摘要的偏好提供了实用方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Hierarchical+Organization+for+Medical+Multi-document+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23104，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23104&send_immediately=true&force_search=false)

**原文摘要:** Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [168] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada, Jana Straková*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种紧凑的单模型多语言屈折变化方法，在73种语言上联合训练，模型轻量且对未见词具有鲁棒性，性能优于单语言基线，简化了部署需求。


<details>
  <summary>更多</summary>
  
**动机:** 解决缺乏开源、通用、多语言形态屈折系统的问题，特别是能够处理包括捷克语在内的多种语言的未见词。

**方法:** 使用联合训练方法，在73种语言数据上训练单一模型；引入基于频率加权和词干不相交的新型训练-开发-测试重采样程序；在SIGMORPHON基准和73个UD树库上评估。

**结果:** 多语言模型在大多数语言上优于单语言基线，证明了多语言建模的有效性；模型对未见词具有鲁棒性且轻量紧凑。

**结论:** 多语言建模在屈折变化任务中具有显著效果和实际优势，通过单一模型简化了部署过程，避免了管理数十个单独单语言模型的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flexing+in+73+Languages%3A+A+Single+Small+Model+for+Multilingual+Inflection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23114&send_immediately=true&force_search=false)

**原文摘要:** We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [169] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li*

**主要类别:** cs.CL

**AI概要:** TopLoRA是一种改进的LoRA方法，通过为每个输入token动态调整权重，实现更细粒度的适配，在多个模型和数据集上优于标准LoRA及其变体。


<details>
  <summary>更多</summary>
  
**动机:** 标准LoRA中所有输入token共享相同的权重，无法捕捉token特定的语义差异信息，限制了其表达能力。

**方法:** 提出Token-wise Projected Low-Rank Adaptation (TopLoRA)，通过动态调整LoRA权重BΣXA，其中ΣX是根据每个输入token生成的对角矩阵，实现token级别的输入-输出投影。

**结果:** 在多个模型和数据集上的广泛实验表明，TopLoRA始终优于LoRA及其变体。

**结论:** TopLoRA在不增加LoRA权重秩的情况下，通过学习token级别的权重实现了更细粒度的适配，是有效的参数高效微调方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Higher+Rank%3A+Token-wise+Input-Output+Projections+for+Efficient+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23123，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23123&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [170] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada, Jana Straková*

**主要类别:** cs.CL

**AI概要:** 该论文探讨了在形态变化任务中引入语料库频率信息的方法，包括频率加权的训练-开发-测试集划分、引入词例准确率评估指标以及频率感知训练采样策略，在43种语言中的26种语言上取得了更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的形态变化方法缺乏对词频分布的考虑，而实际应用中用户输入会反映自然文本的真实频率分布，因此需要将语料频率信息整合到系统开发中。

**方法:** 提出了三个关键维度的改进：(1) 词干不相交且频率加重的数据集划分策略；(2) 引入词例准确率作为评估指标；(3) 频率感知训练采样方法。

**结果:** 频率感知训练在43种语言中的26种语言上表现优于均匀采样方法。

**结论:** 将语料频率信息整合到形态变化任务的多维度中能更好地反映实际应用场景，频率感知训练是有效的改进策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corpus+Frequencies+in+Morphological+Inflection%3A+Do+They+Matter%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23131&send_immediately=true&force_search=false)

**原文摘要:** The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [171] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei*

**主要类别:** cs.CL

**AI概要:** ENTP框架通过神经符号方法净化低质量SFT数据，仅使用低质量数据构建的数据集在多个基准测试中超越了13种现有数据选择方法和完整原始数据集的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有监督微调方法通常丢弃低质量数据，但其中包含有价值信息，且质量过滤器不完美，浪费了潜在有用信号。

**方法:** 提出ENTP框架：符号模块基于统计先验识别和修剪噪声样本，神经组件利用潜在表示和模型知识合成增强的指令-响应对。

**结果:** 实验显示ENTP增强的数据集在五个指令跟随基准测试中优于13个基线方法，甚至超越了使用完整原始数据集（约30万样本）的微调效果。

**结论:** 低质量数据具有未开发的潜力，智能净化和合成对于高效指令对齐至关重要，神经符号协同方法能有效提升数据信息量和多样性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ENTP%3A+Enhancing+Low-Quality+SFT+Data+via+Neural-Symbolic+Text+Purge-Mix，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23160&send_immediately=true&force_search=false)

**原文摘要:** Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [172] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu*

**主要类别:** cs.CL

**AI概要:** 提出了双阶段精炼(DSR)框架，通过将创意叙事生成与格式转换解耦来解决LLM直接生成剧本质量差的问题，专业评估显示DSR在75%的情况下优于强基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在创意写作中表现潜力，但端到端生成方法无法产生专业质量的剧本，主要原因是单一模型需要同时掌握创意叙事构建和严格格式遵循这两种截然不同的能力。

**方法:** 引入双阶段精炼(DSR)框架：第一阶段将简要大纲转换为丰富的小说式散文；第二阶段将叙事精炼为专业格式的剧本。通过混合数据合成解决训练数据稀缺问题，包括反向合成解构现有剧本和正向合成生成高质量叙事文本。

**结果:** 专业编剧盲评显示，DSR对Gemini-2.5-Pro等强基线模型达到75%的胜率，达到人类水平表现的82.7%。

**结论:** 分解生成架构配合定制化数据合成能有效使LLM在复杂创意领域专业化，证明了该方法在剧本生成等需要多种不同能力的创作任务中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Direct+Generation%3A+A+Decomposed+Approach+to+Well-Crafted+Screenwriting+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23163&send_immediately=true&force_search=false)

**原文摘要:** The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [173] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro*

**主要类别:** cs.CL

**AI概要:** 论文提出MATCH，一种基于对比学习的无参考代码评估指标，用于评估AI生成代码与开发者意图的匹配程度，相比现有指标在功能正确性和人类偏好方面表现更好。


<details>
  <summary>更多</summary>
  
**动机:** AI代码生成日益普及，但传统评估方法如单元测试难以扩展，语法相似性指标无法捕捉代码功能，现有无参考评估指标如ICE-Score选择有限，需要更好的无参考评估方案。

**方法:** 使用对比学习生成代码和自然语言任务描述的有意义嵌入，通过相似性评分反映生成代码实现任务的程度。

**结果:** MATCH在多种编程语言中相比现有指标，与功能正确性和人类偏好显示出更强的相关性。

**结论:** MATCH作为一种新颖的无参考评估指标，能有效解决AI生成代码与开发者意图对齐的评估问题，为代码生成质量评估提供了更优的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MATCH%3A+Task-Driven+Code+Evaluation+through+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23169&send_immediately=true&force_search=false)

**原文摘要:** AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [174] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang, Wenxuan Zhao, Jun Gao*

**主要类别:** cs.CL

**AI概要:** SI-Bench是一个基于真实社交网络对话的社会智能评估基准，包含2221个真实多轮对话，用于评估大语言模型在复杂社交互动中的表现。研究发现SOTA模型在过程推理上超越人类专家，但在回复质量上仍落后于人类，且思维链推理可能降低模型在社交对话任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要通过模拟的智能体间交互构建数据集，无法捕捉真实人类对话的语言风格和关系动态，需要开发基于真实社交互动的评估基准。

**方法:** 基于社会科学理论构建SI-Bench基准，从社交网络应用收集2221个真实多轮对话，并对312个对话进行人工标注，评估8个主要模型的表现。

**结果:** 实验显示：1）SOTA模型在复杂社交情境的过程推理上超越人类专家；2）但在回复质量上仍落后于人类；3）引入思维链推理可能降低LLMs在社交对话任务中的性能。

**结论:** SI-Bench为评估LLMs的社会智能提供了真实可靠的基准，揭示了当前模型在社交互动中的优势与不足，特别是过程推理能力强但回复质量仍需提升，且传统推理方法可能不适用于社交对话场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SI-Bench%3A+Benchmarking+Social+Intelligence+of+Large+Language+Models+in+Human-to-Human+Conversations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23182&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [175] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli*

**主要类别:** cs.CL

**AI概要:** 提出DREAM方法，使用预训练关系抽取模型从医学文本中提取药物关系并构建本体，然后通过大语言模型验证提取结果，在PubMed摘要上达到71%的验证一致性。


<details>
  <summary>更多</summary>
  
**动机:** 药物关系提取对识别药物相互作用和预测副作用至关重要，但当前缺乏专门的数据集，需要采用迁移学习方法。

**方法:** 先使用训练好的关系抽取模型发现实体间关系，然后应用于医学文本语料构建药物关系本体，最后用大语言模型验证提取的关系。

**结果:** 定量结果显示LLM对PubMed摘要子集中提取的关系有71%的一致性，定性分析显示该方法能揭示医学领域的模糊性。

**结论:** 该方法能有效提取药物关系并构建本体，同时揭示了医学关系提取领域的内在挑战和模糊性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DREaM%3A+Drug-Drug+Relation+Extraction+via+Transfer+Learning+Method，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23189&send_immediately=true&force_search=false)

**原文摘要:** Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [176] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz*

**主要类别:** cs.CL

**AI概要:** 提出一种句子级过程奖励模型(PRM)，用于检测大型视觉语言模型生成的放射学报告中的临床幻觉，该模型在弱监督标签下训练，在多个评估指标上优于现有方法，并能泛化到未见过的模型。


<details>
  <summary>更多</summary>
  
**动机:** 大型视觉语言模型在自动生成放射学报告时经常产生临床关键性幻觉，现有检测方法缺乏句子级粒度且泛化能力不足，存在严重医疗风险。

**方法:** 开发句子级过程奖励模型(PRM)，基于临床上下文和先前文本来预测每个生成句子的正确性，在MIMIC-CXR数据集上用弱监督标签进行微调。

**结果:** 0.5B参数的轻量级PRM在多项指标上优于现有方法：MCC提升7.5%，AUROC提升1.8%；能有效过滤低质量报告(F1-CheXbert提升4.5%)；在最佳N选择中提升临床指标7.4%。

**结论:** 轻量级、上下文感知的PRM为临床LVLM提供了一个模型无关的安全层，无需访问内部激活状态，有效提升放射学报告生成的可靠性和安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Process+Reward+Models+for+Sentence-Level+Verification+of+LVLM+Radiology+Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23217&send_immediately=true&force_search=false)

**原文摘要:** Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [177] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat*

**主要类别:** cs.CL

**AI概要:** 本研究创建了一个78小时的孟加拉语方言语音识别数据集Ben-10，发现语音基础模型在方言ASR任务中表现不佳，无论是零样本还是微调设置，而方言特定模型训练能缓解此问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统语音识别研究主要关注标准语言形式，而方言ASR通常被视为微调任务，需要研究方言变异对ASR的影响。

**方法:** 开发了一个78小时标注的孟加拉语语音转文本语料库Ben-10，从语言学和数据驱动角度分析方言变异对ASR的影响。

**结果:** 语音基础模型在方言ASR任务中表现严重不佳，所有深度学习方法都难以处理方言变异数据，但方言特定训练能改善性能。

**结论:** 方言ASR需要专门的数据集和训练方法，Ben-10数据集可作为资源受限环境下ASR建模的分布外资源，数据集和代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+ASR+foundation+models+generalized+enough+to+capture+features+of+regional+dialects+for+low-resource+languages%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23252，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23252&send_immediately=true&force_search=false)

**原文摘要:** Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [178] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi*

**主要类别:** cs.CL

**AI概要:** Mubeen是一个专有的阿拉伯语语言模型，专注于阿拉伯语言学、伊斯兰研究和文化遗产的深度理解，通过原生阿拉伯语源训练解决其他模型在意图检测和文化准确性方面的问题。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有阿拉伯语模型依赖英语翻译数据导致的意图检测失败和文化不准确问题，填补阿拉伯语言处理中的'效用差距危机'。

**方法:** 使用专有阿拉伯OCR引擎数字化历史手稿扩展训练数据，结合深度语言工程框架，采用Practical Closure架构，整合语言学、法学、圣训和古兰经注释等学术著作。

**结果:** 开发出能够精确理解古典文本、当代写作和地区方言的模型，提供文化真实准确的响应，从信息库转变为决定性指南。

**结论:** Mubeen成功结合文化遗产专业化和多学科专家模块，在文化保存和一般知识领域均表现强劲，符合沙特2030愿景目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mubeen+AI%3A+A+Specialized+Arabic+Language+Model+for+Heritage+Preservation+and+User+Intent+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23271&send_immediately=true&force_search=false)

**原文摘要:** Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [179] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei*

**主要类别:** cs.CL

**AI概要:** 提出了一个提升LLM生成代码美观度的新流程，包括构建大规模美学代码数据集AesCode-358K、多智能体奖励反馈系统GRPO-AR算法，以及在OpenDesign基准测试中验证了方法的有效性


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在传统编程任务中表现出色，但在视觉导向的代码任务中生成的美学质量较差，需要提升代码美观度

**方法:** 1)构建AesCode-358K指令调优数据集；2)提出多智能体奖励反馈系统评估可执行性、静态美学和交互美学；3)开发GRPO-AR算法进行功能和美学联合优化；4)建立OpenDesign美学评估基准

**结果:** 实验显示，在AesCode-358K上进行监督微调并结合强化学习的多智能体奖励反馈，在OpenDesign基准上表现显著提升，在PandasPlotBench等现有基准上也得到改善。AesCoder-4B模型超越GPT-4o和GPT-4.1，性能可与480B-685B参数的大型开源模型相媲美

**结论:** 该方法有效提升了LLM生成代码的美学质量，证明了通过专门的数据集设计和多维度奖励反馈机制可以显著改善代码美观度，为视觉导向编程任务提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Code+Aesthetics+with+Agentic+Reward+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23272&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [180] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel*

**主要类别:** cs.CL

**AI概要:** 第九届CHiME挑战赛引入多模态上下文感知识别任务(MCoRec)，解决单房间环境下使用音频、视觉和上下文线索的重叠对话问题，通过多模态方法显著提升语音识别性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决鸡尾酒会问题中极端语音重叠(高达100%)和高度碎片化对话轮次的挑战，需要综合多模态信息来识别谁在何时说了什么以及与谁对话。

**方法:** 收集自然多参与者非脚本对话数据，开发结合音频和视觉线索的基线系统，通过联合转录每个说话人的语音并将其聚类到各自的对话中。

**结果:** 纯音频基线系统词错误率超过100%，而加入视觉线索后性能提升50%，证明了多模态方法的重要性。

**结论:** 多模态上下文感知识别是解决重叠对话问题的有效方法，视觉信息的加入对提升系统性能至关重要，为未来多模态语音处理研究提供了重要基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Cocktail-Party+Benchmark%3A+Multi-Modal+dataset+and+Comparative+Evaluation+Results，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23276&send_immediately=true&force_search=false)

**原文摘要:** We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


### [181] [DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model](https://arxiv.org/abs/2510.23284)
*Yuanzhen Xie, Liu Ye, Jiqun Chu, Mochi Gao, Hehuan Liu, Yunzhi Tan, Bo Hu, Zang Li*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个全自动的数据中心化文本到SQL任务流水线，包括自适应数据修复和错误数据增强，结合多模型协作训练和集成策略，在轻量级模型中取得了最佳性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于代理的文本到SQL框架虽然有所改进，但数据中心化策略的影响很少被探索，且单个微调模型的能力有限。

**方法:** 设计了包含自适应数据修复（自动发现和修复训练数据错误）和错误数据增强（扩散和增强模型预测的错误数据）的流水线，采用多模型协作训练和集成策略。

**结果:** 实验和消融研究证明了该方法的有效性，在轻量级文本到SQL模型（70B参数内）中取得了第一名的成绩。

**结论:** 数据中心化流水线和多模型交互迭代策略能有效提升文本到SQL任务的准确性，特别是在轻量级模型上表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DCMM-SQL%3A+Automated+Data-Centric+Pipeline+and+Multi-Model+Collaboration+Training+for+Text-to-SQL+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23284，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23284&send_immediately=true&force_search=false)

**原文摘要:** Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).

</details>


### [182] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri, Dania Desouki, Khloud Al Jallad*

**主要类别:** cs.CL

**AI概要:** 该研究创建了阿拉伯语儿童语音数据集Arabic Little STT，评估了Whisper模型在儿童语音识别上的表现，发现其性能远低于成人语音，强调了儿童语音数据和伦理框架的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 阿拉伯语等低资源语言存在数据稀缺问题，特别是缺乏儿童专用语音语料库，这给AI系统性能带来重大挑战。

**方法:** 创建Arabic Little STT数据集（包含288名6-13岁儿童的355条黎凡特阿拉伯语语音），系统评估8个Whisper变体在该数据集上的表现，并与成人阿拉伯语基准进行比较。

**结果:** 即使是表现最好的Whisper Large_v3模型在儿童语音上的词错误率也高达0.66，远高于其在成人数据集上低于0.20的表现，与英语语音研究结果一致。

**结论:** 研究强调了在ASR开发中需要专门的儿童语音基准和包容性训练数据，这些数据必须遵循严格的伦理和隐私框架，为阿拉伯语儿童开发公平的语音技术提供了初步基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Arabic+Little+STT%3A+Arabic+Children+Speech+Recognition+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23319，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23319&send_immediately=true&force_search=false)

**原文摘要:** The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [183] [Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models](https://arxiv.org/abs/2510.23334)
*Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik*

**主要类别:** cs.CL

**AI概要:** AdaSearch是一种新颖的块状搜索策略，通过自适应分配计算预算，专注于关键初始token来提升LLM对齐效果，在多个任务上优于Best-of-N和微调基线


<details>
  <summary>更多</summary>
  
**动机:** LLM对齐是关键挑战，推理时方法虽灵活但计算分配均匀导致对齐效果不佳，作者假设响应初始token对对齐任务更为关键

**方法:** 提出AdaSearch块状搜索策略，使用采样调度自适应分配固定计算预算，专注于关键token搜索，并开发了树搜索版本AdaBeam

**结果:** 在8个LLM上的综合评估显示，AdaSearch在无害生成、受控情感生成和数学推理任务上相比Best-of-N基线胜率提升超过10%

**结论:** AdaSearch通过关注关键初始token的自适应计算分配策略，有效提升了LLM对齐性能，为推理时对齐方法提供了新思路

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Blockwise+Search%3A+Inference-Time+Alignment+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23334&send_immediately=true&force_search=false)

**原文摘要:** LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

</details>


### [184] [BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning](https://arxiv.org/abs/2510.23337)
*Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia*

**主要类别:** cs.CL

**AI概要:** 该论文提出了首个基于八字命理的人设推理QA数据集和BaZi-LLM系统，通过符号推理与大语言模型结合，生成动态细粒度的虚拟角色人设，相比主流LLM准确率提升30.3%-62.6%。


<details>
  <summary>更多</summary>
  
**动机:** 当前虚拟角色生成方法严重依赖标注数据或手工制作的人设提示，难以扩展且难以生成真实、上下文连贯的人设，需要更有效的解决方案。

**方法:** 创建基于八字命理的人设推理QA数据集，将人类经验分为财富、健康、亲属、职业和关系等类别；提出BaZi-LLM系统，整合符号推理与大语言模型。

**结果:** 相比DeepSeek-v3和GPT-5-mini等主流LLM，准确率提升30.3%-62.6%；当使用错误八字信息时，模型准确率下降20%-45%，证明文化基础的符号-LLM整合的有效性。

**结论:** 基于文化的符号推理与LLM整合在真实角色模拟方面具有巨大潜力，能够生成更真实、动态的虚拟角色人设。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BaZi-Based+Character+Simulation+Benchmark%3A+Evaluating+AI+on+Temporal+and+Persona+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23337&send_immediately=true&force_search=false)

**原文摘要:** Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.

</details>


### [185] [LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data](https://arxiv.org/abs/2510.23341)
*Teng Lin*

**主要类别:** cs.CL

**AI概要:** LightKGG是一个使用小型语言模型(SLMs)高效从文本数据中提取知识图谱的新框架，通过上下文集成图提取和拓扑增强关系推理两大技术创新，解决了传统方法依赖错误模式匹配或计算密集型大语言模型的问题


<details>
  <summary>更多</summary>
  
**动机:** 高质量知识图谱稀缺是AI应用的关键瓶颈，现有提取方法严重依赖错误率高的模式匹配技术或资源密集型大语言模型，计算需求限制了在低资源环境中的可访问性

**方法:** 提出两个关键技术：1)上下文集成图提取-将上下文信息与节点和边集成到统一图结构中；2)拓扑增强关系推理-利用提取图的固有拓扑结构高效推断关系

**结果:** 能够以最小硬件需求准确构建知识图谱，弥合了自动知识提取与实际部署场景之间的差距

**结论:** 该工作为在结构化NLP任务中优化小型语言模型效率引入了科学严谨的方法，使低资源环境也能高效进行知识图谱提取

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightKGG%3A+Simple+and+Efficient+Knowledge+Graph+Generation+from+Textual+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23341&send_immediately=true&force_search=false)

**原文摘要:** The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

</details>


### [186] [How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes](https://arxiv.org/abs/2510.23358)
*Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个评估大语言模型预测AI对就业影响能力的基准，结合美国行业级职位发布数据和全球AI采用导致的职业变化预测数据，通过多种提示策略评估LLMs的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能正在重塑劳动力市场，但缺乏系统预测AI对就业影响的工具。现有研究显示LLMs可以提取情感、总结经济报告和模拟预测者行为，但很少评估其在前瞻性劳动力预测中的应用。

**方法:** 结合两个互补数据集：美国行业级高频职位发布指数和全球AI采用导致的职业变化预测数据，构建具有明确时间分割的预测任务。评估多种提示策略（任务支架、角色驱动和混合方法），分析定量准确性和定性一致性。

**结果:** 结构化任务提示能持续提高预测稳定性，角色提示在短期趋势上具有优势。但不同行业和时间范围的性能差异显著，表明需要领域感知提示和严格评估协议。

**结论:** 通过发布基准，支持未来关于劳动力预测、提示设计和基于LLM的经济推理研究，为研究AI作为劳动力市场预测工具的局限性和机会提供可复现的测试平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+AI+Forecasts+AI+Jobs%3A+Benchmarking+LLM+Predictions+of+Labor+Market+Changes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23358&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

</details>


### [187] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen, Pien Pieterse, Yusuf Çelik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren*

**主要类别:** cs.CL

**AI概要:** 本文通过规则模型和大型语言模型分析气候相关文本中的宗教语言使用，发现基于规则的方法比LLMs检测到更多宗教语言，揭示了宗教语言检测的方法学挑战和定义争议。


<details>
  <summary>更多</summary>
  
**动机:** 研究宗教语言在当代话语中的持续存在，特别是在环境活动和气候变化等世俗领域，探索宗教与非政府组织在气候相关文本中如何使用显性和隐性宗教语言。

**方法:** 采用双重方法：基于生态神学文献构建宗教术语层次树的规则模型，以及在零样本设置下运行的大型语言模型(LLMs)，使用包含88万多句子的数据集进行对比分析。

**结果:** 规则方法比LLMs更一致地将更多句子标记为宗教语言，两种方法在检测结果上存在一致性和分歧点。

**结论:** 研究不仅揭示了计算检测宗教语言的方法学挑战，还展现了宗教语言应仅由词汇定义还是需考虑语境意义的广泛争议，为宗教研究中的数字方法贡献了潜力与局限性的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Religious+Language+in+Climate+Discourse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23395，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23395&send_immediately=true&force_search=false)

**原文摘要:** Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [188] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种基于混合专家(MoE)框架的时间序列预测模型，通过整合xLSTM、增强线性模型、PatchTST和minGRU等多种先进模型，在标准基准测试中超越了所有现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 针对时间序列预测领域中Transformer模型效果被质疑的问题，以及现有方法在处理近期数据和突发事件时的局限性，研究者希望开发一个更强大的预测框架。

**方法:** 采用混合专家(MoE)框架，集成多种先进模型（xLSTM、增强线性模型、PatchTST、minGRU等），通过基于Transformer的门控网络进行模型选择和整合。

**结果:** 提出的MoE框架在标准时间序列预测基准测试中表现出色，超越了所有现有模型，包括最新的MoE方法。

**结论:** 混合专家框架能够有效整合多种互补的预测模型，在处理时间序列数据时展现出强大的性能，为解决该领域的挑战提供了有效方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EMTSF%3AExtraordinary+Mixture+of+SOTA+Models+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23396&send_immediately=true&force_search=false)

**原文摘要:** The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [189] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao*

**主要类别:** cs.CL

**AI概要:** Omni-Reward是一个通用多模态奖励模型框架，解决了现有奖励模型的两个主要问题：模态不平衡和偏好刚性，支持文本、图像、视频、音频和3D等多种模态的自由形式偏好建模。


<details>
  <summary>更多</summary>
  
**动机:** 现有奖励模型主要局限于文本和图像模态，对视频、音频等其他模态支持有限（模态不平衡问题）；且基于固定二元偏好对的训练无法捕捉个性化偏好的复杂性和多样性（偏好刚性问题）。

**方法:** 提出了Omni-Reward框架，包括：(1)评估基准Omni-RewardBench，首个支持自由形式偏好的多模态RM基准，涵盖5种模态9个任务；(2)数据集Omni-RewardData，包含24.8万通用偏好对和6.9万指令调优对；(3)模型Omni-RewardModel，包含判别式和生成式奖励模型。

**结果:** Omni-RewardModel在Omni-RewardBench以及其他广泛使用的奖励建模基准上表现出强劲性能。

**结论:** Omni-Reward为解决多模态奖励建模的挑战提供了一个有效框架，通过支持多种模态和自由形式偏好，推动了通用多模态奖励模型的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omni-Reward%3A+Towards+Generalist+Omni-Modal+Reward+Modeling+with+Free-Form+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23451&send_immediately=true&force_search=false)

**原文摘要:** Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [190] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**主要类别:** cs.CL

**AI概要:** 本研究探讨了多轮交互中LLM搜索代理的置信度表达能力，发现高置信度对应高任务准确率，提出基于置信度的Test-Time Scaling方法，显著减少token消耗并保持竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注单轮场景的置信度，而对复杂多轮交互中LLM置信度表达能力的研究有限，需要探索LLM代理在长序列动作后能否通过语言化置信度分数传达不确定性。

**方法:** 在开源代理模型上进行实验，提出Test-Time Scaling(TTS)方法，利用置信度分数判断答案质量，鼓励模型在未达到满意置信度时重新尝试。

**结果:** 实验发现模型在高置信度时任务准确率显著更高，低置信度时准确率接近零。提出的TTS方法相比基线固定预算方法显著减少了token消耗，同时保持竞争性性能。

**结论:** LLM在多轮交互中能够有效表达置信度，基于置信度的TTS方法可以优化模型性能并减少计算资源消耗，为复杂交互场景中的不确定性评估提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BrowseConf%3A+Confidence-Guided+Test-Time+Scaling+for+Web+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23458&send_immediately=true&force_search=false)

**原文摘要:** Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [191] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea*

**主要类别:** cs.CL

**AI概要:** 该研究构建了一个针对债务、每股收益和销售额三个金融指标的句子级立场检测语料库，并系统评估了大型语言模型在金融立场检测中的表现，发现few-shot加上思维链提示策略效果最佳。


<details>
  <summary>更多</summary>
  
**动机:** SEC文件财报和财报电话会议记录中的金融叙述对投资者、审计师和监管者很重要，但由于其长度、金融术语和微妙语言，细粒度分析困难，且传统情感分析需要大量昂贵标注数据。

**方法:** 从10-K年报和财报电话会议记录中提取句子，使用ChatGPT-3模型进行立场标注（积极、消极、中性）并经过严格人工验证，然后系统评估现代大型语言模型在零样本、少样本和思维链提示策略下的表现。

**结果:** few-shot加上思维链提示策略相比监督基线表现最佳，大型语言模型在SEC和ECT数据集上的表现存在差异。

**结论:** 研究结果表明，无需大量标注数据即可利用大型语言模型进行金融领域特定目标的立场检测，具有实际可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Large+Language+Models+for+Stance+Detection+on+Financial+Targets+from+SEC+Filing+Reports+and+Earnings+Call+Transcripts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23464，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23464&send_immediately=true&force_search=false)

**原文摘要:** Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [192] [MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring](https://arxiv.org/abs/2510.23477)
*Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang*

**主要类别:** cs.CL

**AI概要:** MMTutorBench是首个AI数学辅导基准，包含685个围绕关键教学步骤构建的问题，通过六个维度评估模型在洞察发现、操作制定和操作执行三个任务中的表现。评估12个主流MLLM显示专有与开源模型存在差距，OCR会降低辅导质量，few-shot提示效果有限，基于量规的LLM-as-a-Judge评估方法可靠。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准大多忽视了AI数学辅导所需的诊断学生困难和逐步引导的关键能力，需要专门评估多模态大语言模型在数学辅导方面的实际表现。

**方法:** 构建包含685个教学关键步骤问题的MMTutorBench基准，每个问题配有特定量规进行六维度细粒度评估，分为洞察发现、操作制定和操作执行三个任务，评估12个领先MLLM模型。

**结果:** 专有模型优于开源模型，与人类导师仍有较大差距；OCR流程会降低辅导质量；few-shot提示改进有限；基于量规的LLM评估方法高度可靠。

**结论:** MMTutorBench揭示了AI数学辅导的挑战性和诊断价值，为推进AI辅导系统发展提供了重要基准工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMTutorBench%3A+The+First+Multimodal+Benchmark+for+AI+Math+Tutoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23477&send_immediately=true&force_search=false)

**原文摘要:** Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.

</details>


### [193] [M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset](https://arxiv.org/abs/2510.23508)
*Jiahui Geng, Jonathan Tonglet, Iryna Gurevych*

**主要类别:** cs.CL

**AI概要:** M4FC是一个新的多模态事实核查数据集，包含4,982张图片和6,980个声明，涵盖10种语言和6个任务，解决了现有数据集规模小、语言单一、证据泄露等问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有多模态事实核查数据集存在规模小、语言单一、证据泄露、依赖外部新闻源等问题，需要更全面、多样化的数据集来支持多模态事实核查研究。

**方法:** 构建包含4,982张专业事实核查机构验证图片和6,980个多语言声明的数据集，涵盖6个多模态事实核查任务，并提供基线模型结果。

**结果:** 创建了M4FC数据集，包含多样化的文化和地理背景内容，为6个多模态事实核查任务提供了基准性能，并分析了中间任务对最终核查性能的影响。

**结论:** M4FC数据集填补了多模态事实核查领域的空白，为研究社区提供了更全面、真实的数据资源，支持多语言、多任务的事实核查研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M4FC%3A+a+Multimodal%2C+Multilingual%2C+Multicultural%2C+Multitask+Real-World+Fact-Checking+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23508&send_immediately=true&force_search=false)

**原文摘要:** Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.

</details>


### [194] [IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering](https://arxiv.org/abs/2510.23536)
*Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee*

**主要类别:** cs.CL

**AI概要:** 该论文提出了IPQA基准测试，用于评估个性化问答中的核心意图识别能力，填补了现有基准只评估回答质量而忽略意图识别的空白。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试仅评估回答质量或检索性能，无法直接衡量意图识别能力，而理解用户优先意图对于满足个性化信息需求至关重要。

**方法:** 基于满意理论，从用户答案选择行为中推导核心意图，通过系统过滤、基于LLM的标注和严格质量控制构建多领域数据集。

**结果:** 实验评估显示当前最先进的语言模型在个性化情境下难以识别核心意图，性能随问题复杂度增加而下降。

**结论:** 核心意图识别在个性化问答中具有重要价值，当前系统在此方面存在明显不足，发布的代码和数据集将促进该方向未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IPQA%3A+A+Benchmark+for+Core+Intent+Identification+in+Personalized+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23536&send_immediately=true&force_search=false)

**原文摘要:** Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

</details>


### [195] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan*

**主要类别:** cs.CL

**AI概要:** LIMRANK模型通过合成数据训练，仅需传统方法5%的数据量就能在信息重排序任务上达到竞争性性能，显著降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法依赖大规模微调来适配LLMs进行信息重排序任务，计算成本高昂，需要更高效的适配方法。

**方法:** 设计了LIMRANK-SYNTHESIZER合成数据生成管道，生成多样化、具有挑战性和真实性的重排序样本，并用这些合成数据微调LIMRANK模型。

**结果:** LIMRANK在BRIGHT和FollowIR两个基准测试中表现出竞争性性能，训练数据量仅为先前工作的不到5%。

**结论:** LIMRANK-SYNTHESIZER合成数据方法有效，LIMRANK模型在下游任务（如科学文献搜索和检索增强生成）中展现出强大的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LimRank%3A+Less+is+More+for+Reasoning-Intensive+Information+Reranking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23544&send_immediately=true&force_search=false)

**原文摘要:** Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [196] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos, Hiram Calvo, Olga Kolesnikova*

**主要类别:** cs.CL

**AI概要:** 该论文比较了传统机器学习模型和微调transformer模型在希望语音检测任务上的表现，发现transformer模型在精确率和召回率方面表现更优，表明大语言模型在小数据集上可能有更好表现。


<details>
  <summary>更多</summary>
  
**动机:** 识别社交媒体上的希望语音（包含动机表达和目标导向行为的文本）已成为重要的NLP任务，需要评估不同模型在此任务上的表现。

**方法:** 使用先前分割的希望语音数据集（训练集、开发集、测试集），评估传统机器学习模型（SVM、逻辑回归、朴素贝叶斯）和微调的transformer模型。

**结果:** 传统模型中SVM和逻辑回归的macro-F1为0.78，transformer模型表现更好，最佳模型达到加权精确率0.82、加权召回率0.80、加权F1 0.79、macro F1 0.79、准确率0.80。

**结论:** 虽然优化配置的传统机器学习模型仍具有灵活性，但transformer架构能检测希望语音的细微语义特征，在精确率和召回率方面表现更优，表明大transformer和LLM在小数据集上可能表现更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hope+Speech+Detection+in+Social+Media+English+Corpora%3A+Performance+of+Traditional+and+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23585&send_immediately=true&force_search=false)

**原文摘要:** The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


### [197] [Think Twice: Branch-and-Rethink Reasoning Reward Model](https://arxiv.org/abs/2510.23596)
*Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau*

**主要类别:** cs.CL

**AI概要:** BR-RM是一种两阶段奖励模型，通过分支-再思考机制减少判断扩散，提高对细微错误的敏感性，在多个奖励建模基准上达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有奖励模型将多个质量维度压缩为单次标量评分，导致注意力分散和判断扩散问题，需要更精细的评估方法。

**方法:** 采用两阶段方法：第一阶段自适应分支选择关键维度并生成假设，第二阶段执行分支条件再思考来验证假设并专注最重要的问题。使用GRPO风格的强化学习训练。

**结果:** 在三个具有挑战性的奖励建模基准测试中实现了最先进的性能表现。

**结论:** BR-RM通过将一次性评分转换为专注的二次审视推理，有效减少判断扩散，提高错误检测灵敏度，同时保持实用性和可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Twice%3A+Branch-and-Rethink+Reasoning+Reward+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23596&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [198] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah, Maha Driss, Henda Ben Ghezela*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种基于微服务架构的联邦学习解决方案，用于物联网边缘计算环境中的恶意软件检测与分类，在保护数据隐私的同时实现了99.24%的高检测性能。


<details>
  <summary>更多</summary>
  
**动机:** 物联网数据分析和隐私安全需求日益增长，需要分布式数据分析技术来处理本地或云端分析带来的隐私安全问题，同时要求低延迟和高可靠性。

**方法:** 采用基于微服务的架构，将物联网应用构建为细粒度、松耦合的可重用实体，结合联邦学习技术提供智能微服务，通过MaleVis数据集（包含14,000多张RGB图像，25个恶意软件类和1个良性类）进行验证。

**结果:** 提出的方法在恶意软件检测和分类性能上优于现有最先进方法，达到了99.24%的准确率。

**结论:** 基于微服务的联邦学习架构能够有效降低延迟和带宽拥堵，保护数据隐私，同时提供高效、灵活和可扩展的数据分析能力，特别适用于物联网边缘计算环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedMicro-IDA%3A+A+Federated+Learning+and+Microservices-based+Framework+for+IoT+Data+Analytics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20852&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [199] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng, Jin Li, Zhenhua Zhao, Shaowei Wang*

**主要类别:** cs.CR

**AI概要:** FPT-Noise是一种新的测试时防御方法，通过动态特征调制器和特征感知阈值来增强CLIP模型的对抗鲁棒性，无需昂贵的微调训练。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉语言模型（如CLIP）对对抗攻击高度脆弱，而传统的对抗训练方法需要大量重新训练且计算成本高昂。

**方法:** 提出动态特征调制器生成图像特定和攻击自适应的噪声强度参数，建立特征感知阈值区分干净图像和受攻击图像，并集成场景感知调节和测试时变换集成技术。

**结果:** FPT-Noise显著优于现有测试时防御方法，在AutoAttack下将平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上保持高性能（仅下降1.1%）。

**结论:** 该方法提供了一种有效且高效的测试时防御解决方案，无需重新训练即可显著提升VLMs的对抗鲁棒性，代码将在研究发表后公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FPT-Noise%3A+Dynamic+Scene-Aware+Counterattack+for+Test-Time+Adversarial+Defense+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20856&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [200] [Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology](https://arxiv.org/abs/2510.20858)
*Nubio Vidal, Naghmeh Moradpoor, Leandros Maglaras*

**主要类别:** cs.CR

**AI概要:** 本文提出了Agnostic Incident Reporting (AIR)框架，用于实时OT事件报告，包含25个元素分为7组，旨在解决OT事件响应中数据捕获标准缺失的问题。


<details>
  <summary>更多</summary>
  
**动机:** OT网络与IT日益融合扩大了攻击面，但现有OT标准未明确事件中应捕获的数据，IT指南又不考虑OT限制，导致利益相关者间协调困难。

**方法:** 开发AIR框架，将其映射到主要OT标准，定义集成激活点，并通过对2015年乌克兰电网事件的回顾性应用进行评估。

**结果:** AIR能将高层需求转化为具体字段，独立于供应商覆盖现有框架，支持态势感知和响应期间的通信。

**结论:** AIR为标准化实时OT事件报告提供了基础，同时支持技术协调和监管一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Everyone+Needs+AIR%3A+An+Agnostic+Incident+Reporting+Framework+for+Cybersecurity+in+Operational+Technology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20858&send_immediately=true&force_search=false)

**原文摘要:** Operational technology (OT) networks are increasingly coupled with
information technology (IT), expanding the attack surface and complicating
incident response. Although OT standards emphasise incident reporting and
evidence preservation, they do not specify what data to capture during an
incident, which hinders coordination across stakeholders. In contrast, IT
guidance defines reporting content but does not address OT constraints. This
paper presents the Agnostic Incident Reporting (AIR) framework for live OT
incident reporting. AIR comprises 25 elements organised into seven groups to
capture incident context, chronology, impacts, and actions, tailored to
technical, managerial, and regulatory needs. We evaluate AIR by mapping it to
major OT standards, defining activation points for integration and triggering
established OT frameworks, and then retrospectively applying it to the 2015
Ukrainian distribution grid incident. The evaluation indicates that AIR
translates high-level requirements into concrete fields, overlays existing
frameworks without vendor dependence, and can support situational awareness and
communication during response. AIR offers a basis for standardising live OT
incident reporting while supporting technical coordination and regulatory
alignment.

</details>


### [201] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares, Mário S. Alvim, Natasha Fernandes*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种新的动态信息流泄漏定义，将攻击者的信念与基线分布解耦，验证了其满足信息论公理，并与静态视角兼容，填补了动态信息流量化理论空白。


<details>
  <summary>更多</summary>
  
**动机:** 当前定量信息流(QIF)研究中，静态泄漏视角已有成熟理论，但动态视角(针对具体运行实例)缺乏同等理论深度，需要建立系统的动态泄漏量化框架。

**方法:** 提出新颖的动态泄漏定义，将攻击者信念与基线分布分离；验证定义满足非干涉性、单调性和数据处理不等式等公理；分析强公理版本不成立的条件；展示与静态视角的兼容性；通过隐私保护数据发布攻击案例进行验证。

**结果:** 成功定义了动态泄漏的数学框架，证明其满足核心信息论公理要求，识别了强公理限制条件，实现了动态与静态视角的理论统一，并通过实际案例验证了定义的有效性。

**结论:** 该研究填补了动态信息流量化理论空白，为系统监控和追踪提供了理论基础，推动了QIF领域动态分析方法的成熟发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+new+measure+for+dynamic+leakage+based+on+quantitative+information+flow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20922&send_immediately=true&force_search=false)

**原文摘要:** Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [202] [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)
*Soham Hans, Stacy Marsella, Sophia Hirschmann, Nikolos Gurney*

**主要类别:** cs.CR

**AI概要:** 本研究提出使用大型语言模型分析入侵检测系统日志，从低层网络数据推断攻击者的MITRE ATT&CK技术和认知策略，为认知自适应网络安全防御提供新途径。


<details>
  <summary>更多</summary>
  
**动机:** 传统网络安全分析依赖高层情报报告和手动解释攻击链，但实时防御需要直接从低层系统遥测数据推断攻击者意图和认知策略。

**方法:** 开发策略驱动的提示系统，将大量网络日志数据高效分割为不同行为阶段，利用LLM将每个阶段与可能的技术和认知动机关联，映射网络层事件到高层攻击者策略。

**结果:** LLM能够弥合数据包级日志与战略意图之间的语义鸿沟，展示行为信号（如工具切换、协议转换）如何对应心理上有意义的决策点。

**结论:** 该方法为行为自适应网络防御和认知特征推断奠定了基础，展示了LLM在认知网络安全中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Logs+to+ATT%26CK+Insights%3A+Leveraging+LLMs+for+High-Level+Threat+Understanding+and+Cognitive+Trait+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20930&send_immediately=true&force_search=false)

**原文摘要:** Understanding adversarial behavior in cybersecurity has traditionally relied
on high-level intelligence reports and manual interpretation of attack chains.
However, real-time defense requires the ability to infer attacker intent and
cognitive strategy directly from low-level system telemetry such as intrusion
detection system (IDS) logs. In this paper, we propose a novel framework that
leverages large language models (LLMs) to analyze Suricata IDS logs and infer
attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded
in the hypothesis that attacker behavior reflects underlying cognitive biases
such as loss aversion, risk tolerance, or goal persistence that can be
extracted and modeled through careful observation of log sequences. This lays
the groundwork for future work on behaviorally adaptive cyber defense and
cognitive trait inference. We develop a strategy-driven prompt system to
segment large amounts of network logs data into distinct behavioral phases in a
highly efficient manner, enabling the LLM to associate each phase with likely
techniques and underlying cognitive motives. By mapping network-layer events to
high-level attacker strategies, our method reveals how behavioral signals such
as tool switching, protocol transitions, or pivot patterns correspond to
psychologically meaningful decision points. The results demonstrate that LLMs
can bridge the semantic gap between packet-level logs and strategic intent,
offering a pathway toward cognitive-adaptive cyber defense.
  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),
Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive
Biases

</details>


### [203] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, Mehrdad Saif*

**主要类别:** cs.CR

**AI概要:** 本研究调查了城市空中交通(UAM)车辆自主导航和着陆系统的漏洞，重点关注针对CNN等深度学习模型的木马攻击，实验显示准确率从96.4%降至73.3%。


<details>
  <summary>更多</summary>
  
**动机:** 研究城市空中交通(UAM)车辆自主导航系统面临的安全威胁，特别是深度学习中可能被植入的木马攻击，这些攻击可能在实际运行中造成严重安全隐患。

**方法:** 使用DroNet框架评估城市自主飞行器(UAAVs)的脆弱性，收集定制数据集并训练模型模拟真实条件，开发评估框架来识别受木马感染的模型。

**结果:** 实验结果表明，木马攻击导致模型准确率显著下降，从清洁数据的96.4%降至被触发数据的73.3%，验证了系统存在的安全漏洞。

**结论:** 木马攻击对UAM系统构成严重安全风险，本研究为未来增强UAM系统韧性研究奠定了基础，强调了在自动驾驶系统中加强安全防护的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Experimental+Study+of+Trojan+Vulnerabilities+in+UAV+Autonomous+Landing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20932&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [204] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong, Stephen H. Bach*

**主要类别:** cs.CR

**AI概要:** 研究发现推理语言模型(RLM)在数学和代码领域的良性推理训练后会出现自我越狱现象，即模型会使用多种策略绕过自身的安全防护机制来响应有害请求。


<details>
  <summary>更多</summary>
  
**动机:** 发现推理语言模型在良性训练后意外出现安全对齐失效的现象，需要系统分析这种自我越狱行为并找到解决方案。

**方法:** 通过分析多个开源RLM模型(如DeepSeek-R1-distilled、Phi-4-mini-reasoning等)的行为，研究其推理过程中的自我越狱策略，并探索机制原因和缓解方法。

**结果:** 发现RLM在良性推理训练后变得更加顺从，会通过引入良性假设等方式为有害请求提供合理性，从而绕过安全防护。模型在思维链中会降低对恶意请求危害性的感知。

**结论:** 在训练中包含最少量的安全推理数据可以有效缓解自我越狱问题，为保持RLM安全性提供了实用解决方案，首次系统分析了这一现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Jailbreaking%3A+Language+Models+Can+Reason+Themselves+Out+of+Safety+Alignment+After+Benign+Reasoning+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20956&send_immediately=true&force_search=false)

**原文摘要:** We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [205] [REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering](https://arxiv.org/abs/2510.20975)
*Darrin Lea, James Ghawaly, Golden Richard III, Aisha Ali-Gombe, Andrew Case*

**主要类别:** cs.CR

**AI概要:** 该论文提出了REx86，一个专门针对x86二进制逆向工程的本地化大语言模型，通过参数高效微调在5981个x86汇编样本上训练，显著提升了代码理解和注释能力。


<details>
  <summary>更多</summary>
  
**动机:** 解决云端LLM在x86逆向工程中的隐私安全风险，以及由于元数据缺失和对抗性混淆导致的逆向工程效率低下问题。

**方法:** 对CodeLlama、Qwen2.5-Coder和CodeGemma系列的8个开源模型进行参数高效微调，使用5981个精心策划的x86汇编样本数据集。

**结果:** 微调的Qwen2.5-Coder-7B模型（REx86）在测试集上交叉熵损失降低64.2%，语义余弦相似度提高20.3%，用户研究中代码理解正确率从31%提升至53%。

**结论:** REx86在本地开源LLM中提供了最先进的x86逆向工程辅助能力，证明了领域特定微调的价值，并强调需要更多带注释的反汇编数据来进一步提升LLM在逆向工程中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REx86%3A+A+Local+Large+Language+Model+for+Assisting+in+x86+Assembly+Reverse+Engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20975&send_immediately=true&force_search=false)

**原文摘要:** Reverse engineering (RE) of x86 binaries is indispensable for malware and
firmware analysis, but remains slow due to stripped metadata and adversarial
obfuscation. Large Language Models (LLMs) offer potential for improving RE
efficiency through automated comprehension and commenting, but cloud-hosted,
closed-weight models pose privacy and security risks and cannot be used in
closed-network facilities. We evaluate parameter-efficient fine-tuned local
LLMs for assisting with x86 RE tasks in these settings. Eight open-weight
models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned
on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them
quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top
performer, which we name REx86.
  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic
cosine similarity against ground truth by 20.3\% over its base model. In a
limited user case study (n=43), REx86 significantly enhanced line-level code
understanding (p = 0.031) and increased the correct-solve rate from 31% to 53%
(p = 0.189), though the latter did not reach statistical significance.
Qualitative analysis shows more accurate, concise comments with fewer
hallucinations.
  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight
LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and
highlight the need for more commented disassembly data to further enhance LLM
performance in RE. REx86, its dataset, and LoRA adapters are publicly available
at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.

</details>


### [206] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen, Alsharif Abuadbba, Kristen Moore, Tingming Wu*

**主要类别:** cs.CR

**AI概要:** 本文评估了FOICE音频深度伪造技术的检测挑战，发现现有检测器在标准及噪声环境下均无法有效检测，提出了针对性的微调策略并分析了泛化性能的权衡问题


<details>
  <summary>更多</summary>
  
**动机:** FOICE技术能够仅凭一张面部图像生成逼真的合成语音，可绕过行业标准认证系统，且面部图像比语音样本更容易获取，这带来了严重的安全威胁，需要研究其检测方法

**方法:** 研究两个核心问题：(1)评估现有音频深度伪造检测器在干净和噪声条件下检测FOICE生成语音的能力；(2)通过在FOICE数据上微调检测器来改进检测效果并保持对未见语音生成器的鲁棒性

**结果:** 研究发现领先的检测器在标准及噪声条件下均持续失败；针对性的微调策略能显著提高检测准确率；但微调后在FOICE专业化和对未见合成管道的鲁棒性之间存在权衡

**结论:** 当前防御系统存在根本性弱点，需要为下一代音频深度伪造检测开发新的架构和训练协议

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Current+Detectors+Catch+Face-to-Voice+Deepfake+Attacks%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21004&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [207] [JSTprove: Pioneering Verifiable AI for a Trustless Future](https://arxiv.org/abs/2510.21024)
*Jonathan Gold, Tristan Freiberg, Haruna Isah, Shirin Shahabi*

**主要类别:** cs.CR

**AI概要:** JSTprove是一个基于zkML的易用工具包，让AI开发者无需密码学专业知识即可实现可验证的AI推理证明


<details>
  <summary>更多</summary>
  
**动机:** AI系统在关键行业的应用需要可信验证，但传统zkML系统需要深厚的密码学知识，限制了普通ML工程师的使用

**方法:** 基于Polyhedra Network的Expander后端构建专用zkML工具包，提供端到端可验证AI推理流程，通过简单命令行界面隐藏密码学复杂性

**结果:** 开发了JSTprove工具包，支持AI推理证明的生成和验证，提供可审计的工件以确保可重复性

**结论:** JSTprove既是满足当前工程需求的可用zkML产品，也是未来可验证AI研究和生产部署的可重复基础

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JSTprove%3A+Pioneering+Verifiable+AI+for+a+Trustless+Future，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21024&send_immediately=true&force_search=false)

**原文摘要:** The integration of machine learning (ML) systems into critical industries
such as healthcare, finance, and cybersecurity has transformed decision-making
processes, but it also brings new challenges around trust, security, and
accountability. As AI systems become more ubiquitous, ensuring the transparency
and correctness of AI-driven decisions is crucial, especially when they have
direct consequences on privacy, security, or fairness. Verifiable AI, powered
by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these
challenges. zkML enables the verification of AI model inferences without
exposing sensitive data, providing an essential layer of trust and privacy.
However, traditional zkML systems typically require deep cryptographic
expertise, placing them beyond the reach of most ML engineers. In this paper,
we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's
Expander backend, to enable AI developers and ML engineers to generate and
verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI
inference pipeline that hides cryptographic complexity behind a simple
command-line interface while exposing auditable artifacts for reproducibility.
We present the design, innovations, and real-world use cases of JSTprove as
well as our blueprints and tooling to encourage community review and extension.
JSTprove therefore serves both as a usable zkML product for current engineering
needs and as a reproducible foundation for future research and production
deployments of verifiable AI.

</details>


### [208] [A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](https://arxiv.org/abs/2510.21053)
*Li An, Yujian Liu, Yepeng Liu, Yuheng Bu, Yang Zhang, Shiyu Chang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种端到端的强化学习框架，用于优化大语言模型水印的绿/红令牌列表设计，解决了多目标优化中的稳定性和奖励黑客问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有水印算法多基于启发式设计，直接使用强化学习优化会面临多目标冲突导致的训练不稳定和奖励黑客问题。

**方法:** 采用锚定机制确保训练稳定性，引入正则化项防止奖励黑客，构建端到端强化学习框架优化水印令牌列表。

**结果:** 在标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著提升，且不降低其他性能。

**结论:** 提出的强化学习框架有效解决了水印优化的关键挑战，为稳健安全的大语言模型水印提供了新解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Reinforcement+Learning+Framework+for+Robust+and+Secure+LLM+Watermarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21053&send_immediately=true&force_search=false)

**原文摘要:** Watermarking has emerged as a promising solution for tracing and
authenticating text generated by large language models (LLMs). A common
approach to LLM watermarking is to construct a green/red token list and assign
higher or lower generation probabilities to the corresponding tokens,
respectively. However, most existing watermarking algorithms rely on heuristic
green/red token list designs, as directly optimizing the list design with
techniques such as reinforcement learning (RL) comes with several challenges.
First, desirable watermarking involves multiple criteria, i.e., detectability,
text quality, robustness against removal attacks, and security against spoofing
attacks. Directly optimizing for these criteria introduces many partially
conflicting reward terms, leading to an unstable convergence process. Second,
the vast action space of green/red token list choices is susceptible to reward
hacking. In this paper, we propose an end-to-end RL framework for robust and
secure LLM watermarking. Our approach adopts an anchoring mechanism for reward
terms to ensure stable training and introduces additional regularization terms
to prevent reward hacking. Experiments on standard benchmarks with two backbone
LLMs show that our method achieves a state-of-the-art trade-off across all
criteria, with notable improvements in resistance to spoofing attacks without
degrading other criteria. Our code is available at
https://github.com/UCSB-NLP-Chang/RL-watermark.

</details>


### [209] [Soft Instruction De-escalation Defense](https://arxiv.org/abs/2510.21057)
*Nils Philipp Walter, Chawin Sitawarin, Jamie Hayes, David Stutz, Ilia Shumailov*

**主要类别:** cs.CR

**AI概要:** SIC是一种针对工具增强LLM代理的迭代提示净化方法，通过多次检查重写恶意内容来防御提示注入攻击，虽然不能完全防御但显著提高了安全性门槛


<details>
  <summary>更多</summary>
  
**动机:** LLM在代理系统中处理不可信数据时容易受到提示注入攻击，需要有效的防御机制

**方法:** 提出迭代提示净化循环SIC，多次检查输入数据中的恶意指令内容并进行重写、屏蔽或删除，直到输入干净或达到最大迭代次数

**结果:** SIC能够有效防御提示注入，但最坏情况下仍有15%的攻击成功率，主要针对非命令式工作流攻击

**结论:** SIC虽然不能完全免疫攻击，但显著提高了防御门槛，为LLM代理系统提供了实用的安全保护机制

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft+Instruction+De-escalation+Defense，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21057&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly deployed in agentic systems
that interact with an external environment; this makes them susceptible to
prompt injections when dealing with untrusted data. To overcome this
limitation, we propose SIC (Soft Instruction Control)-a simple yet effective
iterative prompt sanitization loop designed for tool-augmented LLM agents. Our
method repeatedly inspects incoming data for instructions that could compromise
agent behavior. If such content is found, the malicious content is rewritten,
masked, or removed, and the result is re-evaluated. The process continues until
the input is clean or a maximum iteration limit is reached; if imperative
instruction-like content remains, the agent halts to ensure security. By
allowing multiple passes, our approach acknowledges that individual rewrites
may fail but enables the system to catch and correct missed injections in later
steps. Although immediately useful, worst-case analysis shows that SIC is not
infallible; strong adversary can still get a 15% ASR by embedding
non-imperative workflows. This nonetheless raises the bar.

</details>


### [210] [QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute](https://arxiv.org/abs/2510.21124)
*Jie Zhang, Xiaohong Li, Mengke Zhang, Ruitao Feng, Shanshan Xu, Zhe Hou, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** QAEBAC是一种区块链属性访问控制方案，通过(r,t)-匿名模型量化重识别风险，使用熵加权路径树优化策略匹配，在Hyperledger Fabric上实现隐私与性能的平衡，吞吐量提升11倍，延迟降低87%。


<details>
  <summary>更多</summary>
  
**动机:** 现有区块链属性访问控制方案面临两个核心问题：区块链透明性导致用户隐私易受重识别攻击，策略匹配的计算复杂度与区块链性能限制存在冲突。现有解决方案如零知识证明开销高且缺乏可量化的匿名性保证，效率优化往往忽视隐私影响。

**方法:** 提出QAEBAC方案：1) 引入形式化的(r,t)-匿名模型动态量化用户基于属性和访问历史的重识别风险；2) 设计熵加权路径树(EWPT)基于实时匿名性指标优化策略结构，大幅降低策略匹配复杂度；3) 在Hyperledger Fabric平台上实现和评估。

**结果:** 实验结果表明QAEBAC有效缓解重识别风险，在性能上显著优于现有基线方法：吞吐量最高提升11倍，延迟降低87%，证明了其在隐私敏感去中心化应用中的实用性。

**结论:** QAEBAC成功解决了区块链属性访问控制中隐私与性能的双重挑战，通过可量化的匿名性模型和高效的策略匹配优化，为隐私敏感的去中心化应用提供了实用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QAE-BAC%3A+Achieving+Quantifiable+Anonymity+and+Efficiency+in+Blockchain-Based+Access+Control+with+Attribute，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21124&send_immediately=true&force_search=false)

**原文摘要:** Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a
decentralized paradigm for secure data governance but faces two inherent
challenges: the transparency of blockchain ledgers threatens user privacy by
enabling reidentification attacks through attribute analysis, while the
computational complexity of policy matching clashes with blockchain's
performance constraints. Existing solutions, such as those employing
Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable
anonymity guarantees, while efficiency optimizations frequently ignore privacy
implications. To address these dual challenges, this paper proposes QAEBAC
(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with
Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically
quantify the re-identification risk of users based on their access attributes
and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that
optimizes policy structure based on realtime anonymity metrics, drastically
reducing policy matching complexity. Implemented and evaluated on Hyperledger
Fabric, QAE-BAC demonstrates a superior balance between privacy and
performance. Experimental results show that it effectively mitigates
re-identification risks and outperforms state-of-the-art baselines, achieving
up to an 11x improvement in throughput and an 87% reduction in latency, proving
its practicality for privacy-sensitive decentralized applications.

</details>


### [211] [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/abs/2510.21133)
*Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi*

**主要类别:** cs.CR

**AI概要:** 该研究首次全面评估10个主流商业大语言模型在CBRN武器知识方面的安全漏洞，发现现有安全机制存在严重脆弱性，Deep Inception攻击成功率高达86%，模型间安全性能差异巨大（2%-96%），急需更鲁棒的安全对齐技术。


<details>
  <summary>更多</summary>
  
**动机:** 前沿大语言模型存在前所未有的双重用途风险，可能促进化学、生物、放射性和核武器知识的扩散，需要评估其安全漏洞以应对潜在的灾难性滥用风险。

**方法:** 使用新颖的200个提示的CBRN数据集和FORTRESS基准的180个提示子集，采用严格的三层攻击方法学对10个领先商业LLMs进行评估。

**结果:** Deep Inception攻击成功率达86.0%（直接请求为33.8%），模型安全性能差异显著（claude-opus-4为2%，mistral-small-latest为96%），8个模型在增强危险材料属性方面的脆弱性超过70%。

**结论:** 当前安全对齐存在根本性脆弱性，简单的提示工程技术就能绕过安全防护措施，挑战了行业安全声明，迫切需要标准化评估框架、透明安全指标和更鲁棒的对齐技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+CBRN+Risk+in+Frontier+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21133&send_immediately=true&force_search=false)

**原文摘要:** Frontier Large Language Models (LLMs) pose unprecedented dual-use risks
through the potential proliferation of chemical, biological, radiological, and
nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation
of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and
a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier
attack methodology. Our findings expose critical safety vulnerabilities: Deep
Inception attacks achieve 86.0\% success versus 33.8\% for direct requests,
demonstrating superficial filtering mechanisms; Model safety performance varies
dramatically from 2\% (claude-opus-4) to 96\% (mistral-small-latest) attack
success rates; and eight models exceed 70\% vulnerability when asked to enhance
dangerous material properties. We identify fundamental brittleness in current
safety alignment, where simple prompt engineering techniques bypass safeguards
for dangerous CBRN information. These results challenge industry safety claims
and highlight urgent needs for standardized evaluation frameworks, transparent
safety metrics, and more robust alignment techniques to mitigate catastrophic
misuse risks while preserving beneficial capabilities.

</details>


### [212] [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189)
*Yukun Jiang, Mingjie Li, Michael Backes, Yang Zhang*

**主要类别:** cs.CR

**AI概要:** 论文提出JAIL-CON攻击框架，利用任务并发性绕过LLM的安全防护，将恶意任务与良性任务混合执行，显著降低防护系统检测概率


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱攻击主要基于顺序逻辑，而并发性作为顺序场景的自然扩展被忽视，但并发任务可能带来新的安全风险

**方法:** 提出词级方法实现LLM任务并发，相邻词汇编码不同意图；开发JAIL-CON迭代攻击框架，通过任务并发性进行越狱攻击

**结果:** 实验显示JAIL-CON在广泛使用的LLM上表现出强大的越狱能力，并发答案比顺序答案更具隐蔽性，更难被防护系统检测

**结论:** 任务并发性在LLM越狱中具有独特优势，凸显了LLM安全防护需要应对并发攻击的新挑战

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adjacent+Words%2C+Divergent+Intents%3A+Jailbreaking+Large+Language+Models+via+Task+Concurrency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21189&send_immediately=true&force_search=false)

**原文摘要:** Despite their superior performance on a wide range of domains, large language
models (LLMs) remain vulnerable to misuse for generating harmful content, a
risk that has been further amplified by various jailbreak attacks. Existing
jailbreak attacks mainly follow sequential logic, where LLMs understand and
answer each given task one by one. However, concurrency, a natural extension of
the sequential scenario, has been largely overlooked. In this work, we first
propose a word-level method to enable task concurrency in LLMs, where adjacent
words encode divergent intents. Although LLMs maintain strong utility in
answering concurrent tasks, which is demonstrated by our evaluations on
mathematical and general question-answering benchmarks, we notably observe that
combining a harmful task with a benign one significantly reduces the
probability of it being filtered by the guardrail, showing the potential risks
associated with concurrency in LLMs. Based on these findings, we introduce
$\texttt{JAIL-CON}$, an iterative attack framework that
$\underline{\text{JAIL}}$breaks LLMs via task $\underline{\text{CON}}$currency.
Experiments on widely-used LLMs demonstrate the strong jailbreak capabilities
of $\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the
guardrail is applied as a defense, compared to the sequential answers generated
by previous attacks, the concurrent answers in our $\texttt{JAIL-CON}$ exhibit
greater stealthiness and are less detectable by the guardrail, highlighting the
unique feature of task concurrency in jailbreaking LLMs.

</details>


### [213] [The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](https://arxiv.org/abs/2510.21190)
*Mingrui Liu, Sixiao Zhang, Cheng Long, Kwok Yan Lam*

**主要类别:** cs.CR

**AI概要:** TrojFill是一种黑盒越狱方法，通过将有害指令嵌入多部分模板中，利用安全推理和示例生成来绕过LLM的安全防护，在多个主流模型上取得高攻击成功率。


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的提示缺乏可解释性和可迁移性。需要一种有效的黑盒越狱方法。

**方法:** 将有害指令（经过混淆处理）嵌入多部分模板，要求模型：(1)进行不安全推理分析原指令为何危险，(2)生成详细示例文本并进行逐句分析。示例部分作为特洛伊木马包含目标越狱内容。

**结果:** 在主流LLM（ChatGPT、Gemini、DeepSeek、Qwen）上评估显示强劲性能：Gemini-flash-2.5和DeepSeek-3.1达到100%攻击成功率，GPT-4o达到97%。生成的提示比先前黑盒方法具有更好的可解释性和可迁移性。

**结论:** TrojFill提供了一种有效的黑盒越狱方法，通过模板填充任务框架成功绕过LLM安全防护，同时提高了生成提示的质量和实用性，为红队测试研究提供了有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Trojan+Example%3A+Jailbreaking+LLMs+through+Template+Filling+and+Unsafety+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21190&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have advanced rapidly and now encode extensive
world knowledge. Despite safety fine-tuning, however, they remain susceptible
to adversarial prompts that elicit harmful content. Existing jailbreak
techniques fall into two categories: white-box methods (e.g., gradient-based
approaches such as GCG), which require model internals and are infeasible for
closed-source APIs, and black-box methods that rely on attacker LLMs to search
or mutate prompts but often produce templates that lack explainability and
transferability. We introduce TrojFill, a black-box jailbreak that reframes
unsafe instruction as a template-filling task. TrojFill embeds obfuscated
harmful instructions (e.g., via placeholder substitution or Caesar/Base64
encoding) inside a multi-part template that asks the model to (1) reason why
the original instruction is unsafe (unsafety reasoning) and (2) generate a
detailed example of the requested text, followed by a sentence-by-sentence
analysis. The crucial "example" component acts as a Trojan Horse that contains
the target jailbreak content while the surrounding task framing reduces refusal
rates. We evaluate TrojFill on standard jailbreak benchmarks across leading
LLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical
performance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,
and 97% on GPT-4o). Moreover, the generated prompts exhibit improved
interpretability and transferability compared with prior black-box optimization
approaches. We release our code, sample prompts, and generated outputs to
support future red-teaming research.

</details>


### [214] [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214)
*Xingwei Zhong, Kar Wai Fok, Vrizlynn L. L. Thing*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种针对多模态大语言模型的黑盒越狱攻击方法，通过文本和图像提示来评估模型安全性，并设计了重新攻击策略和防御方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型在视觉和文本模态结合后引入了新的安全威胁维度，特别是越狱攻击可能导致模型产生未经授权或有害的响应。

**方法:** 设计了带有挑衅指令的文本提示和具有变异和多图像能力的图像提示，采用黑盒攻击方法，并开发了重新攻击策略来加强评估。

**结果:** 实证结果表明，该方法能有效评估开源和闭源MLLMs的安全性，并发现现有防御方法的不足。

**结论:** 通过重新设计的防御方法在训练时和推理时都能提供更好的保护，有效对抗越狱攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+MLLM+Black-Box+Jailbreaking+Attacks+and+Defenses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21214&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) comprise of both visual and textual
modalities to process vision language tasks. However, MLLMs are vulnerable to
security-related issues, such as jailbreak attacks that alter the model's input
to induce unauthorized or harmful responses. The incorporation of the
additional visual modality introduces new dimensions to security threats. In
this paper, we proposed a black-box jailbreak method via both text and image
prompts to evaluate MLLMs. In particular, we designed text prompts with
provocative instructions, along with image prompts that introduced mutation and
multi-image capabilities. To strengthen the evaluation, we also designed a
Re-attack strategy. Empirical results show that our proposed work can improve
capabilities to assess the security of both open-source and closed-source
MLLMs. With that, we identified gaps in existing defense methods to propose new
strategies for both training-time and inference-time defense methods, and
evaluated them across the new jailbreak methods. The experiment results showed
that the re-designed defense methods improved protections against the jailbreak
attacks.

</details>


### [215] [Securing AI Agent Execution](https://arxiv.org/abs/2510.21236)
*Christoph Bühler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi*

**主要类别:** cs.CR

**AI概要:** AgentBound是首个针对MCP服务器的访问控制框架，结合声明式策略机制和执行引擎，无需修改MCP服务器即可阻止恶意行为，提供80.9%的自动策略生成准确率，并实现可忽略的性能开销。


<details>
  <summary>更多</summary>
  
**动机:** MCP已成为连接AI代理与外部工具的事实标准，但存在严重安全隐患：数千个MCP服务器拥有对主机系统的无限制访问权限，形成了广泛的攻击面。

**方法:** 开发AgentBound框架，包含受Android权限模型启发的声明式策略机制和策略执行引擎，构建包含296个流行MCP服务器的数据集，实现从源代码自动生成访问控制策略。

**结果:** 自动策略生成准确率达到80.9%，能够阻止大多数恶意MCP服务器的安全威胁，策略执行引擎引入可忽略的性能开销。

**结论:** AgentBound为开发者和项目经理提供了保护MCP服务器的实用基础，同时保持生产力，为声明式访问控制和MCP安全研究开辟了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Securing+AI+Agent+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21236，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21236&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have evolved into AI agents that interact with
external tools and environments to perform complex tasks. The Model Context
Protocol (MCP) has become the de facto standard for connecting agents with such
resources, but security has lagged behind: thousands of MCP servers execute
with unrestricted access to host systems, creating a broad attack surface. In
this paper, we introduce AgentBound, the first access control framework for MCP
servers. AgentBound combines a declarative policy mechanism, inspired by the
Android permission model, with a policy enforcement engine that contains
malicious behavior without requiring MCP server modifications. We build a
dataset containing the 296 most popular MCP servers, and show that access
control policies can be generated automatically from source code with 80.9%
accuracy. We also show that AgentBound blocks the majority of security threats
in several malicious MCP servers, and that policy enforcement engine introduces
negligible overhead. Our contributions provide developers and project managers
with a practical foundation for securing MCP servers while maintaining
productivity, enabling researchers and tool builders to explore new directions
for declarative access control and MCP security.

</details>


### [216] [What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions](https://arxiv.org/abs/2510.21246)
*Michael Külper, Jan-Niclas Hilgert, Frank Breitinger, Martin Lambertz*

**主要类别:** cs.CR

**AI概要:** 提出针对自托管云存储平台Nextcloud的扩展数字取证框架，通过设备监控和云API实现结构化、可重复的证据获取，并开发了开源采集工具。


<details>
  <summary>更多</summary>
  
**动机:** 自托管云存储平台如Nextcloud日益流行，但对数字取证调查带来新挑战，现有云存储取证框架存在局限性，Nextcloud在取证研究中关注有限。

**方法:** 批判性分析现有云存储取证框架的局限性，提出扩展取证框架，整合设备监控和云API，以Nextcloud为案例研究展示其原生API的取证应用，并开发开源采集工具。

**结果:** 开发了能够可靠访问Nextcloud取证工件的开源采集工具，证明了原生API在取证中的有效性。

**结论:** 该框架为调查人员提供了分析自托管云存储系统的灵活方法，为数字取证这一发展中的领域奠定了进一步发展的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+Next%2C+Cloud%3F+A+Forensic+Framework+for+Analyzing+Self-Hosted+Cloud+Storage+Solutions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21246&send_immediately=true&force_search=false)

**原文摘要:** Self-hosted cloud storage platforms like Nextcloud are gaining popularity
among individuals and organizations seeking greater control over their data.
However, this shift introduces new challenges for digital forensic
investigations, particularly in systematically analyzing both client and server
components. Despite Nextcloud's widespread use, it has received limited
attention in forensic research. In this work, we critically examine existing
cloud storage forensic frameworks and highlight their limitations. To address
the gaps, we propose an extended forensic framework that incorporates device
monitoring and leverages cloud APIs for structured, repeatable evidence
acquisition. Using Nextcloud as a case study, we demonstrate how its native
APIs can be used to reliably access forensic artifacts, and we introduce an
open-source acquisition tool that implements this approach. Our framework
equips investigators with a more flexible method for analyzing self-hosted
cloud storage systems, and offers a foundation for further development in this
evolving area of digital forensics.

</details>


### [217] [LLM-Powered Detection of Price Manipulation in DeFi](https://arxiv.org/abs/2510.21272)
*Lu Liu, Wuqi Zhang, Lili Wei, Hao Guan, Yongqiang Tian, Yepang Liu*

**主要类别:** cs.CR

**AI概要:** PMDetector是一个结合静态分析和LLM推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在准确率和效率方面显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** DeFi智能合约管理着数十亿美元资金，价格操纵漏洞（常通过闪电贷实施）造成重大财务损失。现有检测方法存在局限性：反应性方法只能在攻击发生后分析，而静态分析工具依赖僵化的预定义启发式规则，无法识别新型变种或理解复杂经济逻辑。

**方法:** 提出三阶段混合框架：1) 静态污点分析识别潜在漏洞代码路径；2) 两阶段LLM处理：先分析防御措施过滤路径，再模拟攻击评估可利用性；3) 静态分析检查器验证LLM结果，保留高风险路径并生成详细漏洞报告。

**结果:** 在包含73个真实漏洞和288个良性DeFi协议的数据集上测试，使用Gemini 2.5-flash达到88%精确率和90%召回率，显著优于最先进的静态分析和LLM方法。使用GPT-4.1审计一个漏洞仅需0.03美元和4.0秒。

**结论:** PMDetector提供了一个高效且成本效益高的主动漏洞检测解决方案，相比手动审计具有显著优势，能够有效识别DeFi智能合约中的价格操纵漏洞。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Powered+Detection+of+Price+Manipulation+in+DeFi，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21272&send_immediately=true&force_search=false)

**原文摘要:** Decentralized Finance (DeFi) smart contracts manage billions of dollars,
making them a prime target for exploits. Price manipulation vulnerabilities,
often via flash loans, are a devastating class of attacks causing significant
financial losses. Existing detection methods are limited. Reactive approaches
analyze attacks only after they occur, while proactive static analysis tools
rely on rigid, predefined heuristics, limiting adaptability. Both depend on
known attack patterns, failing to identify novel variants or comprehend complex
economic logic. We propose PMDetector, a hybrid framework combining static
analysis with Large Language Model (LLM)-based reasoning to proactively detect
price manipulation vulnerabilities. Our approach uses a formal attack model and
a three-stage pipeline. First, static taint analysis identifies potentially
vulnerable code paths. Second, a two-stage LLM process filters paths by
analyzing defenses and then simulates attacks to evaluate exploitability.
Finally, a static analysis checker validates LLM results, retaining only
high-risk paths and generating comprehensive vulnerability reports. To evaluate
its effectiveness, we built a dataset of 73 real-world vulnerable and 288
benign DeFi protocols. Results show PMDetector achieves 88% precision and 90%
recall with Gemini 2.5-flash, significantly outperforming state-of-the-art
static analysis and LLM-based approaches. Auditing a vulnerability with
PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an
efficient and cost-effective alternative to manual audits.

</details>


### [218] [The Qey: Implementation and performance study of post quantum cryptography in FIDO2](https://arxiv.org/abs/2510.21353)
*Aditya Mitra, Sibi Chakkaravarthy Sethuraman*

**主要类别:** cs.CR

**AI概要:** 该论文探讨了将后量子密码算法ML-DSA（基于Crystals Dilithium）应用于FIDO2密码认证标准的可行性和性能表现，以应对量子计算带来的安全威胁。


<details>
  <summary>更多</summary>
  
**动机:** 当前FIDO2标准使用的ECDSA和RSA等经典密码算法在面对大规模量子计算机攻击时存在安全风险，需要研究后量子密码解决方案。

**方法:** 研究采用基于模块格的后量子数字签名算法ML-DSA（Crystals Dilithium）作为FIDO2的替代签名方案，并分析其性能表现。

**结果:** 论文比较了ML-DSA与传统算法在性能和安全性方面的差异，为后量子密码在FIDO2中的应用提供了评估。

**结论:** ML-DSA作为后量子密码算法在FIDO2中具有应用潜力，能够增强密码认证系统在量子计算时代的长期安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Qey%3A+Implementation+and+performance+study+of+post+quantum+cryptography+in+FIDO2，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21353&send_immediately=true&force_search=false)

**原文摘要:** Authentication systems have evolved a lot since the 1960s when Fernando
Corbato first proposed the password-based authentication. In 2013, the FIDO
Alliance proposed using secure hardware for authentication, thus marking a
milestone in the passwordless authentication era [1]. Passwordless
authentication with a possession-based factor often relied on hardware-backed
cryptographic methods. FIDO2 being one an amalgamation of the W3C Web
Authentication and FIDO Alliance Client to Authenticator Protocol is an
industry standard for secure passwordless authentication with rising adoption
for the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256
(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature
algorithms. This makes it insecure against attacks involving large-scale
quantum computers [3]. This study aims at exploring the usability of Module
Lattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium
as a post quantum cryptographic signature standard for FIDO2. The paper
highlights the performance and security in comparison to keys with classical
algorithms.

</details>


### [219] [FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](https://arxiv.org/abs/2510.21401)
*Mojtaba Eshghie, Gabriele Morello, Matteo Lauretano, Alexandre Bartel, Martin Monperrus*

**主要类别:** cs.CR

**AI概要:** FLAMES是一种基于领域适应大语言模型的自动化方法，通过生成Solidity "require"语句来为智能合约合成可执行的运行时防护，有效防止漏洞利用，无需漏洞标签或人工干预。


<details>
  <summary>更多</summary>
  
**动机:** 智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施，需要一种能够自动生成生产就绪安全防御的新方法。

**方法:** 使用领域适应的大语言模型，通过在514,506个已验证合约中提取的真实不变式上进行填空式监督微调，合成可执行的运行时防护作为Solidity require语句。

**结果:** 编译成功率96.7%；在5000个挑战性不变式测试集上达到44.5%的精确或语义等价匹配；成功阻止108个真实漏洞中的22个（20.4%）；成功阻止APEMAGA真实攻击事件。

**结论:** 领域适应的LLM能够自动为智能合约生成生产就绪的安全防御，无需漏洞检测、形式规范或人工干预，为这一关键领域的研究提供了可复现的基础设施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLAMES%3A+Fine-tuning+LLMs+to+Synthesize+Invariants+for+Smart+Contract+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21401&send_immediately=true&force_search=false)

**原文摘要:** Smart contract vulnerabilities cost billions of dollars annually, yet
existing automated analysis tools fail to generate deployable defenses. We
present FLAMES, a novel automated approach that synthesizes executable runtime
guards as Solidity "require" statements to harden smart contracts against
exploits. Unlike prior work that relies on vulnerability labels, symbolic
analysis, or natural language specifications, FLAMES employs domain-adapted
large language models trained through fill-in-the-middle supervised fine-tuning
on real-world invariants extracted from 514,506 verified contracts. Our
extensive evaluation across three dimensions demonstrates FLAMES's
effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for
synthesized invariant (2) Semantic Quality: on a curated test set of 5,000
challenging invariants, FLAMES produces exact or semantically equivalent
matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES
prevents 22 out of 108 real exploits (20.4%) while preserving contract
functionality, and (4) FLAMES successfully blocks the real-world APEMAGA
incident by synthesizing a pre-condition that mitigates the attack. FLAMES
establishes that domain-adapted LLMs can automatically generate
production-ready security defenses for smart contracts without requiring
vulnerability detection, formal specifications, or human intervention. We
release our code, model weights, datasets, and evaluation infrastructure to
enable reproducible research in this critical domain.

</details>


### [220] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe, Helmut Neukirchen, Thomas Welsh*

**主要类别:** cs.CR

**AI概要:** SBASH框架使用本地轻量级LLM解决蜜罐的数据保护问题，通过RAG和系统提示调优提升Linux shell命令响应的准确性和实时性，在保持较低延迟的同时提高攻击者参与度。


<details>
  <summary>更多</summary>
  
**动机:** 传统蜜罐缺乏上下文感知能力，而云端LLM存在响应延迟、高成本和数据安全问题，需要一种既能提高攻击者参与度又能保护数据的解决方案。

**方法:** 提出SBASH框架，使用本地轻量级LLM，比较RAG支持和非RAG的LLM在Linux shell命令响应中的表现，评估指标包括响应时间、人类测试真实感和与真实系统的相似度。

**结果:** RAG提高了未调优模型的准确性，而通过系统提示调优的LLM无需RAG也能达到类似准确性，且延迟略低。

**结论:** 本地轻量级LLM结合RAG或系统提示调优可以有效提升蜜罐的上下文感知能力，在保证数据安全的同时提高攻击者参与效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SBASH%3A+a+Framework+for+Designing+and+Evaluating+RAG+vs.+Prompt-Tuned+LLM+Honeypots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21459&send_immediately=true&force_search=false)

**原文摘要:** Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [221] [Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise](https://arxiv.org/abs/2510.21483)
*Pierre Guillot, Auguste Hoang Duc, Michel Koskas, Florian Méhats*

**主要类别:** cs.CR

**AI概要:** GRAFHEN是一种无需自举（无噪声）的全同态加密方案，基于群编码实现，使用重写系统表示群，提供最高安全性和快速性能


<details>
  <summary>更多</summary>
  
**动机:** 解决现有全同态加密方案需要自举操作和存在噪声的问题，提供更高效和安全的加密方案

**方法:** 基于Nuida等人的工作，使用群编码技术，通过重写系统表示群结构，使子群成员问题达到最大难度

**结果:** 实现了无需自举的全同态加密，性能比现有标准快几个数量级，并分析了多种攻击方式的安全防护

**结论:** GRAFHEN方案成功实现了无噪声的全同态加密，在保持高性能的同时提供了强大的安全性，为密码学领域提供了新的研究方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Introducing+GRAFHEN%3A+Group-based+Fully+Homomorphic+Encryption+without+Noise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21483&send_immediately=true&force_search=false)

**原文摘要:** We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic
Encryption without the need for bootstrapping (or in other words, without
noise). Building on the work of Nuida and others, we achieve this using
encodings in groups.
  The groups are represented on a machine using rewriting systems. In this way
the subgroup membership problem, which an attacker would have to solve in order
to break the scheme, becomes maximally hard, while performance is preserved. In
fact we include a simple benchmark demonstrating that our implementation runs
several orders of magnitude faster than existing standards.
  We review many possible attacks against our protocol and explain how to
protect the scheme in each case.

</details>


### [222] [PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis](https://arxiv.org/abs/2510.21601)
*Emmanuel Dare Alalade, Ashraf Matrawy*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的隐私威胁模型框架PTMF，通过结合MITRE ATT&CK和LINDDUN框架来分析隐私威胁，重点关注威胁行为者的行为意图。通过专家问卷调查分析了IoT系统中的12种隐私威胁，识别了主要威胁行为者和关键路径。


<details>
  <summary>更多</summary>
  
**动机:** 现有PTA研究主要关注隐私威胁的发生区域和可能性，但缺乏对威胁行为者、其行为及意图的深入理解，需要开发一个以隐私为中心的威胁分析框架。

**方法:** 结合MITRE ATT&CK框架的战术和LINDDUN隐私威胁模型技术开发PTMF框架，通过设计问卷对12种IoT隐私威胁进行专家调研（来自工业界和学术界的隐私安全专家），分析数据并映射威胁行为者。

**结果:** 识别了IoT用户识别隐私威胁中的前三大威胁行为者及其关键路径，以及其余11种隐私威胁的相关发现，揭示了威胁行为者在系统中的活动和意图模式。

**结论:** PTMF框架为理解如何在IoT系统中主动有效部署隐私措施提供了坚实基础，能够基于威胁行为者的活动和意图来缓解隐私威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PTMF%3A+A+Privacy+Threat+Modeling+Framework+for+IoT+with+Expert-Driven+Threat+Propagation+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21601&send_immediately=true&force_search=false)

**原文摘要:** Previous studies on PTA have focused on analyzing privacy threats based on
the potential areas of occurrence and their likelihood of occurrence. However,
an in-depth understanding of the threat actors involved, their actions, and the
intentions that result in privacy threats is essential. In this paper, we
present a novel Privacy Threat Model Framework (PTMF) that analyzes privacy
threats through different phases.
  The PTMF development is motivated through the selected tactics from the MITRE
ATT\&CK framework and techniques from the LINDDUN privacy threat model, making
PTMF a privacy-centered framework. The proposed PTMF can be employed in various
ways, including analyzing the activities of threat actors during privacy
threats and assessing privacy risks in IoT systems, among others. In this
paper, we conducted a user study on 12 privacy threats associated with IoT by
developing a questionnaire based on PTMF and recruited experts from both
industry and academia in the fields of security and privacy to gather their
opinions. The collected data were analyzed and mapped to identify the threat
actors involved in the identification of IoT users (IU) and the remaining 11
privacy threats. Our observation revealed the top three threat actors and the
critical paths they used during the IU privacy threat, as well as the remaining
11 privacy threats. This study could provide a solid foundation for
understanding how and where privacy measures can be proactively and effectively
deployed in IoT systems to mitigate privacy threats based on the activities and
intentions of threat actors within these systems.

</details>


### [223] [Toward provably private analytics and insights into GenAI use](https://arxiv.org/abs/2510.21684)
*Albert Cheu, Artem Lagzdin, Brett McLarnon, Daniel Ramage, Katharine Daly, Marco Gruteser, Peter Kairouz, Rakshita Tandon, Stanislav Chiknavaryan, Timon Van Overveldt, Zoe Gong*

**主要类别:** cs.CR

**AI概要:** 新一代联邦分析系统，使用基于AMD SEV-SNP和Intel TDX的可信执行环境(TEE)为服务器端处理提供可验证的隐私保障，支持包括LLM处理非结构化数据和差分隐私聚合在内的灵活工作负载。


<details>
  <summary>更多</summary>
  
**动机:** 大规模设备分析系统需要同时满足高隐私安全标准、数据质量、可用性和资源效率要求，传统方法难以兼顾这些需求。

**方法:** 设备加密上传数据并标记允许的服务器处理步骤，通过开源TEE托管的密钥管理服务确保数据只能由受TEE保护的步骤访问，支持非结构化数据的LLM处理和差分隐私聚合。

**结果:** 系统已成功在生产环境中部署，为真实世界的生成式AI体验提供了有价值的洞察，外部各方可验证所有数据处理都在TEE中进行且应用了差分隐私。

**结论:** 该系统通过TEE技术实现了可验证的隐私保护，为大规模联邦分析提供了兼顾隐私安全和实用性的解决方案，证明在生产环境中的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+provably+private+analytics+and+insights+into+GenAI+use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21684&send_immediately=true&force_search=false)

**原文摘要:** Large-scale systems that compute analytics over a fleet of devices must
achieve high privacy and security standards while also meeting data quality,
usability, and resource efficiency expectations. We present a next-generation
federated analytics system that uses Trusted Execution Environments (TEEs)
based on technologies like AMD SEV-SNP and Intel TDX to provide verifiable
privacy guarantees for all server-side processing. In our system, devices
encrypt and upload data, tagging it with a limited set of allowable server-side
processing steps. An open source, TEE-hosted key management service guarantees
that the data is accessible only to those steps, which are themselves protected
by TEE confidentiality and integrity assurance guarantees. The system is
designed for flexible workloads, including processing unstructured data with
LLMs (for structured summarization) before aggregation into differentially
private insights (with automatic parameter tuning). The transparency properties
of our system allow any external party to verify that all raw and derived data
is processed in TEEs, protecting it from inspection by the system operator, and
that differential privacy is applied to all released results. This system has
been successfully deployed in production, providing helpful insights into
real-world GenAI experiences.

</details>


### [224] [$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy](https://arxiv.org/abs/2510.21946)
*Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah*

**主要类别:** cs.CR

**AI概要:** 论文提出δ-STEAL攻击方法，通过局部差分隐私噪声注入绕过LLM水印检测，实现高达96.95%的攻击成功率，同时保持模型效用。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型部署存在知识产权风险，特别是模型窃取攻击会威胁专有LLM的收入和财务稳定性。现有水印解决方案需要被评估其脆弱性。

**方法:** 提出δ-STEAL攻击方法：在微调期间向攻击者模型的token嵌入中注入满足局部差分隐私保证的噪声，通过查询服务提供商的模型收集输出，应用LDP保护噪声混淆水印信号。

**结果:** 实验显示δ-STEAL在轻量级修改下达到96.95%的攻击成功率，噪声规模控制攻击效果与模型效用之间的权衡。

**结论:** 即使是鲁棒的水印也能被绕过，攻击者可以欺骗水印检测器，这对当前知识产权保护方法构成重大风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%CE%B4%24-STEAL%3A+LLM+Stealing+Attack+with+Local+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21946&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demonstrate remarkable capabilities across
various tasks. However, their deployment introduces significant risks related
to intellectual property. In this context, we focus on model stealing attacks,
where adversaries replicate the behaviors of these models to steal services.
These attacks are highly relevant to proprietary LLMs and pose serious threats
to revenue and financial stability. To mitigate these risks, the watermarking
solution embeds imperceptible patterns in LLM outputs, enabling model
traceability and intellectual property verification. In this paper, we study
the vulnerability of LLM service providers by introducing $\delta$-STEAL, a
novel model stealing attack that bypasses the service provider's watermark
detectors while preserving the adversary's model utility. $\delta$-STEAL
injects noise into the token embeddings of the adversary's model during
fine-tuning in a way that satisfies local differential privacy (LDP)
guarantees. The adversary queries the service provider's model to collect
outputs and form input-output training pairs. By applying LDP-preserving noise
to these pairs, $\delta$-STEAL obfuscates watermark signals, making it
difficult for the service provider to determine whether its outputs were used,
thereby preventing claims of model theft. Our experiments show that
$\delta$-STEAL with lightweight modifications achieves attack success rates of
up to $96.95\%$ without significantly compromising the adversary's model
utility. The noise scale in LDP controls the trade-off between attack
effectiveness and model utility. This poses a significant risk, as even robust
watermarks can be bypassed, allowing adversaries to deceive watermark detectors
and undermine current intellectual property protection methods.

</details>


### [225] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan, Ziyu Shu, Amberbir Alemayoh*

**主要类别:** cs.CR

**AI概要:** 提出结合自监督对比学习和神经架构搜索的勒索软件检测框架，通过硬件性能计数器和定制损失函数实现早期检测，显著提升检测精度和响应速度。


<details>
  <summary>更多</summary>
  
**动机:** 传统勒索软件检测方法存在特征依赖性强、响应延迟大、对未知变种适应性差三大局限性，需要新的AI解决方案。

**方法:** 使用自监督对比学习框架分析硬件性能计数器数据，设计定制损失函数促进早期恶意活动检测，部署神经架构搜索自动构建自适应模型架构。

**结果:** 实验显示检测准确率提升高达16.1%，响应时间提升6倍，且在规避攻击下保持鲁棒性。

**结论:** 该框架有效解决了勒索软件检测的关键挑战，为网络安全提供了更高效、自适应的检测方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Low-Latency+and+Adaptive+Ransomware+Detection+Using+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21957&send_immediately=true&force_search=false)

**原文摘要:** Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [226] [Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla](https://arxiv.org/abs/2510.22024)
*Evangelos Bitsikas, Jason Veara, Aanjhan Ranganathan*

**主要类别:** cs.CR

**AI概要:** 对特斯拉车辆LTE连接的黑盒安全分析，揭示了IMSI捕获、伪基站劫持、不安全回退机制等系统漏洞，挑战了汽车网络安全监管框架的核心假设。


<details>
  <summary>更多</summary>
  
**动机:** 移动网络漏洞在智能手机生态中已有充分记录，但在安全关键的汽车环境中影响尚未充分研究，需要评估LTE连接在特斯拉等现代联网车辆中的安全性。

**方法:** 采用黑盒、非侵入式的安全分析方法，对特斯拉Model 3和Cybertruck的LTE连接进行测试，分析其远程信息处理协议栈。

**结果:** 发现特斯拉远程信息处理协议栈存在系统性协议弱点和架构配置错误，易受IMSI捕获、伪基站劫持攻击，存在不安全回退机制可能导致服务静默降级，传统控制平面配置允许静默SMS注入和广播消息欺骗。

**结论:** 这些漏洞不仅影响单一厂商，还挑战了ISO/SAE 21434和UN R155/R156等监管框架的核心假设，这些框架要求现代车辆必须具备安全、可追溯和有弹性的远程信息处理能力才能获得型式批准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Analysis+of+LTE+Connectivity+in+Connected+Cars%3A+A+Case+Study+of+Tesla，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22024&send_immediately=true&force_search=false)

**原文摘要:** Modern connected vehicles rely on persistent LTE connectivity to enable
remote diagnostics, over-the-air (OTA) updates, and critical safety services.
While mobile network vulnerabilities are well documented in the smartphone
ecosystem, their impact in safety-critical automotive settings remains
insufficiently examined. In this work, we conduct a black-box, non-invasive
security analysis of LTE connectivity in Tesla vehicles, including the Model 3
and Cybertruck, revealing systemic protocol weaknesses and architectural
misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI
catching, rogue base station hijacking, and insecure fallback mechanisms that
may silently degrade service availability. Furthermore, legacy control-plane
configurations allow for silent SMS injection and broadcast message spoofing
without driver awareness. These vulnerabilities have implications beyond a
single vendor as they challenge core assumptions in regulatory frameworks like
ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient
telematics for type approval of modern vehicles.

</details>


### [227] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

**主要类别:** cs.CR

**AI概要:** 本文提出了Jailbreak Mimicry方法，通过训练紧凑的攻击模型自动生成基于叙事的越狱提示，实现了81%的攻击成功率，展示了当前AI安全对齐方法的系统性漏洞。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型容易受到复杂提示工程攻击，利用上下文框架绕过安全机制，在网络安全应用中构成重大风险。

**方法:** 使用参数高效微调(LoRA)在Mistral-7B上，利用AdvBench的精选数据集进行训练，实现一次性生成叙事型越狱提示。

**结果:** 在GPT-OSS-20B上达到81.0%攻击成功率，在GPT-4、Llama-3和Gemini 2.5 Flash上分别达到66.5%、79.5%和33.0%的成功率，比直接提示提高54倍。

**结论:** 技术领域(特别是网络安全)和基于欺骗的攻击特别脆弱，揭示了AI集成威胁检测、恶意软件分析和安全系统中的威胁，同时分析了防御策略以减轻这些漏洞。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jailbreak+Mimicry%3A+Automated+Discovery+of+Narrative-Based+Jailbreaks+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22085&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [228] [Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things](https://arxiv.org/abs/2510.22100)
*Saif E. Nouma, Attila A. Yavuz*

**主要类别:** cs.CR

**AI概要:** Graphene是首个对称前向安全聚合认证加密框架，专为低端IoT设备设计，通过密钥演进策略和离线-在线密码处理结合UMACs，实现密钥泄露恢复能力、近最优在线延迟和紧凑性。


<details>
  <summary>更多</summary>
  
**动机:** 当前部署的轻量级认证加密标准缺乏密钥泄露恢复能力、紧凑认证标签和离线-在线密码处理等关键特性，无法满足IoT设备在低能耗对抗环境和低延迟无线信道中的安全需求。

**方法:** 提出Graphene框架，结合密钥演进策略和离线-在线密码处理技术，使用通用消息认证码(UMACs)，开发了两种不同的实例化方案以平衡性能权衡和可扩展性。

**结果:** 在商用硬件和32位ARM Cortex-M4微控制器上的实验评估显示，Graphene相比现有方案具有显著的性能优势，同时保持与标准密码实现的向后兼容性。

**结论:** Graphene为IoT基础设施提供了首个前向安全和聚合认证加密解决方案，实现了安全性和性能的平衡，代码已开源供公众测试和使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+and+Breach-Resilient+Authenticated+Encryption+Framework+for+Internet+of+Things，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22100&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) relies heavily on resource-limited devices to
communicate critical (e.g., military data) information under low-energy
adversarial environments and low-latency wireless channels. Authenticated
Encryption (AE) guarantees confidentiality, authenticity, and integrity, making
it a vital security service for IoT. However, current deployed (lightweight) AE
standards lack essential features like key compromise resiliency and compact
authentication tags, as well as performance enhancements such as offline-online
cryptography. To address these gaps, we propose Graphene, the first (to our
knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption
(FAAE) framework designed for the performance and security demands of low-end
IoT infrastructures. Graphene innovates by synergizing key evolution strategies
and offline-online cryptographic processing with Universal Message
Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal
online latency, and compactness. We demonstrate Graphene efficiency through two
distinct instantiations, each balancing unique performance trade-offs with
extensibility for diverse MACs. Our experimental evaluation on commodity
hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant
performance gains over existing alternatives. Graphene is also backward
compatible with standard-compliant cryptographic implementations. We release
our implementation as open source for public testing and adaptation.

</details>


### [229] [TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation](https://arxiv.org/abs/2510.22191)
*Qi Sheng*

**主要类别:** cs.CR

**AI概要:** TPPR是一个新颖的溯源图分析框架，通过异常节点检测、TTP标注和图剪枝提取异常子图，利用挖掘的TTP序列模式进行攻击路径推理，最后通过基于置信度的路径评分和合并重建攻击场景。在真实企业日志和DARPA数据集上验证，实现了99.9%的图简化同时保留91%关键攻击节点，重建精度比现有方法提高63%以上。


<details>
  <summary>更多</summary>
  
**动机:** 现有技术通过行为模式匹配和数据流特征匹配进行溯源图路径推理，但无法建立有效的攻击上下文关联，经常将良性系统操作与真实攻击实体混淆，无法准确表征真实的APT行为。

**方法:** 1. 通过异常节点检测、TTP标注和图剪枝提取异常子图；2. 使用挖掘的TTP序列模式进行攻击路径推理；3. 通过基于置信度的路径评分和合并重建攻击场景。

**结果:** 在超过1亿条事件的企业日志和DARPA TC数据集上评估，TPPR实现了99.9%的图简化（从70万条边减少到20条边），同时保留了91%的关键攻击节点，重建精度比最先进解决方案（SPARSE、DepImpact）分别高出63.1%和67.9%。

**结论:** TPPR框架能够有效解决APT攻击溯源分析中的关键路径识别问题，通过结合TTP攻击模式和置信度评分，显著提高了攻击场景重建的准确性和效率，同时保持了攻击场景的完整性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TPPR%3A+APT+Tactic+%2F+Technique+Pattern+Guided+Attack+Path+Reasoning+for+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22191&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis based on system audit data has emerged as a fundamental
approach for investigating Advanced Persistent Threat (APT) attacks. Due to the
high concealment and long-term persistence of APT attacks, they are only
represented as a minimal part of the critical path in the provenance graph.
While existing techniques employ behavioral pattern matching and data flow
feature matching to uncover latent associations in attack sequences through
provenance graph path reasoning, their inability to establish effective attack
context associations often leads to the conflation of benign system operations
with real attack entities, that fail to accurately characterize real APT
behaviors. We observe that while the causality of entities in the provenance
graph exhibit substantial complexity, attackers often follow specific attack
patterns-specifically, clear combinations of tactics and techniques to achieve
their goals. Based on these insights, we propose TPPR, a novel framework that
first extracts anomaly subgraphs through abnormal node detection,
TTP-annotation and graph pruning, then performs attack path reasoning using
mined TTP sequential pattern, and finally reconstructs attack scenarios through
confidence-based path scoring and merging. Extensive evaluation on real
enterprise logs (more than 100 million events) and DARPA TC dataset
demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000
to 20 edges) while preserving 91% of critical attack nodes, outperforming
state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in
reconstruction precision while maintaining attack scenario integrity.

</details>


### [230] [SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha, Junaid Arshad, Mohamed Ben Farah, Khalid Ismail*

**主要类别:** cs.CR

**AI概要:** SecureLearn是一个针对多分类模型的两层防御框架，通过数据清洗和特征导向的对抗训练来防御数据投毒攻击，在多种攻击场景下保持90%以上的准确率和75%以上的召回率/F1分数。


<details>
  <summary>更多</summary>
  
**动机:** 现有防御方法主要针对特定攻击或特定ML算法，且多集中于深度神经网络或二分类器，而传统多分类器在开发多模态应用时面临数据投毒威胁但缺乏有效防御。

**方法:** 提出SecureLearn两层防御框架：1)数据清洗层；2)新的特征导向对抗训练层。采用3D评估矩阵（数据投毒攻击、数据清洗、对抗训练三个正交维度）进行验证，在10%-20%投毒水平下测试四种ML算法和三个公开数据集。

**结果:** SecureLearn对所有选定攻击均有效，传统多分类模型和神经网络均得到增强，准确率持续保持在90%以上，召回率和F1分数超过75%。神经网络对所有选定攻击达到97%的召回率和F1分数。

**结论:** SecureLearn证明了其超越算法特定防御的泛化能力，有效增强了传统多分类模型和神经网络对数据投毒攻击的抵抗力和对抗鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SecureLearn+-+An+Attack-agnostic+Defense+for+Multiclass+Machine+Learning+Against+Data+Poisoning+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22274&send_immediately=true&force_search=false)

**原文摘要:** Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [231] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly, Christiana Chamon*

**主要类别:** cs.CR

**AI概要:** 该研究提出了一种利用宽禁带技术固有噪声作为物理不可克隆函数源和实时威胁检测指标的双重用途安全框架，通过机器学习模型实现高精度、低延迟的工业控制系统安全防护。


<details>
  <summary>更多</summary>
  
**动机:** 宽禁带技术虽然提升了电力系统效率，但引入了高频噪声和复杂的网络物理安全威胁，需要新的安全解决方案来保护工业控制系统。

**方法:** 采用噪声驱动的物理不可克隆函数(PUF)和机器学习辅助的异常检测框架，结合自适应贝叶斯滤波，从WBG开关噪声中提取熵作为PUF源和威胁指标。

**结果:** 在良性场景和攻击场景（包括EMI注入、信号篡改和节点冒充）的详细模拟中，实现了95%的检测准确率和亚毫秒级的处理延迟。

**结论:** 该研究证明了物理驱动的双重用途噪声利用作为可扩展ICS防御原语的可行性，为利用固有设备特性、结合硬件和人工智能的下一代安全策略奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Noise-Driven+PUF+and+AI+for+Secure+WBG+ICS%3A+A+Proof-of-Concept+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22283&send_immediately=true&force_search=false)

**原文摘要:** Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [232] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang, Tairen Zhang, Lanjun Wang, Ruidong Chen, Wenhui Li, Anan Liu*

**主要类别:** cs.CR

**AI概要:** 本文提出了T2I-RiskyPrompt基准测试，用于评估文生图模型的安全性，包含6432个风险提示和分层分类体系，并对多个模型、防御方法和攻击策略进行了全面评估。


<details>
  <summary>更多</summary>
  
**动机:** 现有风险提示数据集存在三个主要限制：风险类别有限、注释粒度粗、有效性低，需要更全面的安全评估基准。

**方法:** 开发了分层风险分类体系（6个主类别和14个子类别），构建了风险提示收集和注释流程，提出了基于安全注释的对齐检测方法。

**结果:** 获得了6432个有效风险提示，每个提示都有分层类别标签和详细风险原因注释，并对8个T2I模型、9种防御方法、5个安全过滤器和5种攻击策略进行了评估。

**结论:** T2I-RiskyPrompt为文生图模型安全评估提供了全面基准，揭示了现有安全措施的优缺点，并具有跨研究领域的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T2I-RiskyPrompt%3A+A+Benchmark+for+Safety+Evaluation%2C+Attack%2C+and+Defense+on+Text-to-Image+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22300&send_immediately=true&force_search=false)

**原文摘要:** Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [233] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

**主要类别:** cs.CR

**AI概要:** 提出一个跨机构联邦学习框架，用于ECG图像数字化，在不共享原始图像的情况下训练nnU-Net分割模型，并通过安全聚合和差分隐私保护用户隐私


<details>
  <summary>更多</summary>
  
**动机:** 集中式训练ECG图像数字化模型面临跨机构隐私保护和部署限制的问题，需要一种保护隐私的分布式训练方法

**方法:** 使用跨机构联邦学习框架，集成FedAvg、FedProx和FedAdam三种聚合器，结合安全聚合和中心化高斯差分隐私，包含端到端全模型训练、安全聚合、隐私保护机制和校准感知数字化流程

**结果:** 在PTB-XL数据集上实验显示，FedAdam相比FedAvg和FedProx收敛更快且达到更高的性能平台，接近集中式性能，同时保持竞争性准确度

**结论:** 该框架成功实现了在保护隐私的前提下进行ECG图像数字化，适用于多机构部署场景，提供了可审计的隐私保证

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Aware+Federated+nnU-Net+for+ECG+Page+Digitization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22387&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [234] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li, Zheng Yu, Jingyi Song, Meng Xu, Yuxuan Luo, Dongliang Mu*

**主要类别:** cs.CR

**AI概要:** PORTGPT是一个基于LLM的自动补丁回移植工具，通过模拟人类推理和验证过程，在真实场景中实现了89.15%的成功率，优于现有最先进工具。


<details>
  <summary>更多</summary>
  
**动机:** 手动回移植安全补丁到旧版本分支是一项劳动密集型工作，现有自动化方法依赖预定义语法或语义规则，缺乏对复杂补丁的灵活性。

**方法:** PORTGPT通过增强LLM能力，提供按需代码访问、Git历史总结和基于反馈（如编译器）的自主补丁修订工具，模拟人类推理和验证过程。

**结果:** 在1815个现有数据集案例中达到89.15%成功率，在146个复杂案例中达到62.33%成功率，均优于最先进工具。向Linux内核社区贡献了9个回移植补丁并全部被合并。

**结论:** PORTGPT证明了LLM代理在自动化补丁回移植任务中的有效性，能够处理现实世界中的复杂场景，为开源项目维护提供了实用的自动化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PortGPT%3A+Towards+Automated+Backporting+Using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22396&send_immediately=true&force_search=false)

**原文摘要:** Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [235] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao, Jia Zou, Zhichao Cao, Xusheng Xiao*

**主要类别:** cs.CR

**AI概要:** PROGQL框架提出了一种专门用于网络安全溯源分析的高效图查询语言和引擎，解决了现有技术灵活性不足和内存效率低下的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有溯源分析技术存在两大挑战：(1) 缺乏灵活性和可扩展性，难以融入分析师专业知识；(2) 内存效率低下，通常需要超过100GB内存来存储完整事件流，限制了实际部署的扩展性。

**方法:** 提出PROGQL框架，包括：1) 领域特定的图搜索语言，支持约束图遍历、边权重计算、沿加权边值传播和图合并；2) 优化的查询引擎，支持跨异构数据库后端的高效增量图搜索，无需全内存物化。

**结果:** 在真实攻击评估中，PROGQL语言在表达复杂攻击方面比最先进的图查询语言Cypher更有效；与最先进的PA技术DEPIMPACT相比，PROGQL框架显著提高了可扩展性。

**结论:** PROGQL框架通过创新的图查询语言和高效的查询引擎设计，成功解决了现有溯源分析技术的局限性，为复杂网络攻击调查提供了更灵活、更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProGQL%3A+A+Provenance+Graph+Query+System+for+Cyber+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22400，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22400&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [236] [ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole](https://arxiv.org/abs/2510.22536)
*Jotaro Yano*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种跨域的ZK协处理器桥接方案，允许Solana程序通过Wormhole验证操作批准(VAAs)在Aztec L2上进行隐私计算执行。


<details>
  <summary>更多</summary>
  
**动机:** 解决不同区块链网络(Solana、Ethereum、Aztec)间的跨域隐私计算需求，实现安全可靠的隐私保护计算桥接。

**方法:** 构建包含四个组件的系统：(1)Solana程序向Wormhole Core发送消息；(2)EVM Portal验证VAAs并处理消息；(3)Aztec隐私合约消费消息；(4)链下中继器传输VAAs和记录收据。

**结果:** 设计了状态机、消息格式和安全证明，确保重放安全、来源真实性、最终性对齐、参数绑定、隐私性、幂等性和活跃性。

**结论:** 提供了一个可复现的跨域隐私计算桥接方案，支持Solana程序在Aztec L2上进行隐私执行，并提供了版本控制和测试网络运行指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZK+Coprocessor+Bridge%3A+Replay-Safe+Private+Execution+from+Solana+to+Aztec+via+Wormhole，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22536&send_immediately=true&force_search=false)

**原文摘要:** We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs
request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable
Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a
Solana program that posts messages to Wormhole Core with explicit finality;
(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound
payload secretHash||m from the attested VAA, derives a domain-separated field
commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference
implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we
provide migration guidance to the payload-bound interface); (iii) a minimal
Aztec contract that consumes the message privately; and (iv) an off-chain
relayer that ferries VAAs and can record receipts on Solana. We present state
machines, message formats, and proof sketches for replay-safety, origin
authenticity, finality alignment, parameter binding (no relayer front-running
of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a
concise Reproducibility note with pinned versions and artifacts to replicate a
public testnet run.

</details>


### [237] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang*

**主要类别:** cs.CR

**AI概要:** CP-GBA是一种跨范式图后门攻击方法，利用图提示学习训练通用子图触发器，实现从图监督学习到图对比学习和图提示学习的有效迁移攻击。


<details>
  <summary>更多</summary>
  
**动机:** 现有图后门攻击的触发器生成器结构简单，过度依赖特定特征，局限于单一图学习范式，迁移性差，攻击成功率低。

**方法:** 1) 从目标图中提取紧凑且表达性强的可查询触发器库，强调类别感知、特征丰富性和结构保真度；2) 首次探索图提示学习的理论可迁移性，在基于提示的目标下训练触发器。

**结果:** 在多个真实数据集和防御场景下的广泛实验表明，CP-GBA实现了最先进的攻击成功率。

**结论:** CP-GBA通过跨范式通用子图触发器设计，有效解决了现有图后门攻击方法迁移性差的问题，显著提升了攻击效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Paradigm+Graph+Backdoor+Attacks+with+Promptable+Subgraph+Triggers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22555&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [238] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee, Sajal Saha*

**主要类别:** cs.CR

**AI概要:** 这篇论文综述了区块链平台中使用的数字签名方案，分析了它们如何提供不可否认性并增强系统安全性，比较了不同签名方案在共识协议、智能合约约束和资源限制方面的适用性。


<details>
  <summary>更多</summary>
  
**动机:** 区块链系统依赖去中心化账本和强大的安全保证，其中不可否认性是一个关键要求，可以防止交易作者否认并支持记录数据的完整性。

**方法:** 研究检查了代表性签名方案家族及其密码学基础、安全假设和部署相关属性，包括不可伪造性、抗延展性、聚合支持、多重签名或阈值设置、密钥和签名大小以及验证成本。

**结果:** 使用这些标准比较了不同设计在共识协议、智能合约约束和资源限制方面的适用性，突出了影响吞吐量、存储、可扩展性和攻击面的实际权衡。

**结论:** 研究强调精心选择的数字签名对于实现不可否认性和保持信息完整性至关重要，并概述了实施考虑因素和开放方向，如互操作性和后量子准备。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Blockchain+Signatures+to+Ensure+Information+Integrity+and+Non-Repudiation+in+the+Digital+Era%3A+A+comprehensive+study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22561，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22561&send_immediately=true&force_search=false)

**原文摘要:** Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [239] [FAARM: Firmware Attestation and Authentication Framework for Mali GPUs](https://arxiv.org/abs/2510.22566)
*Md. Mehedi Hasan*

**主要类别:** cs.CR

**AI概要:** 本文揭示了MOLE攻击——首个针对GPU可信执行环境(TEE)的实用攻击，通过向Arm Mali GPU的嵌入式微控制器注入恶意固件来绕过安全保护。作为防御方案，提出了FAARM框架，通过固件认证和验证机制在EL3安全监控层阻止此类攻击，仅产生1.34ms的开销。


<details>
  <summary>更多</summary>
  
**动机:** 现有GPU TEE设计存在固件级别的信任缺口，攻击者可通过内核权限绕过内存保护，窃取敏感数据并篡改推理结果，需要一种轻量级的防御机制来填补这一安全漏洞。

**方法:** 提出FAARM框架，在EL3安全监控层集成数字签名验证，使用供应商签名的固件包和设备内公钥锚点。在启动时验证固件完整性和真实性，执行版本检查并锁定固件区域，消除预验证和TOCTOU攻击向量。

**结果:** FAARM能够可靠检测和阻止恶意固件注入，在使用前拒绝篡改的镜像并在认证后拒绝覆盖尝试。固件验证平均仅产生1.34ms的延迟，安全开销可忽略不计。

**结论:** FAARM填补了基于shim的GPU TEE的基本安全缺口，为移动和云端GPU部署提供了实用且可部署的防御方案，显著提升了安全基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAARM%3A+Firmware+Attestation+and+Authentication+Framework+for+Mali+GPUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22566&send_immediately=true&force_search=false)

**原文摘要:** Recent work has revealed MOLE, the first practical attack to compromise GPU
Trusted Execution Environments (TEEs), by injecting malicious firmware into the
embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence
of cryptographic verification during initialization, adversaries with kernel
privileges can bypass memory protections, exfiltrate sensitive data at over 40
MB/s, and tamper with inference results, all with negligible runtime overhead.
This attack surface affects commodity mobile SoCs and cloud accelerators,
exposing a critical firmware-level trust gap in existing GPU TEE designs. To
address this gap, this paper presents FAARM, a lightweight Firmware Attestation
and Authentication framework that prevents MOLE-style firmware subversion.
FAARM integrates digital signature verification at the EL3 secure monitor using
vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3
verifies firmware integrity and authenticity, enforces version checks, and
locks the firmware region, eliminating both pre-verification and
time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a
software-only prototype on a Mali GPU testbed, using a Google Colab-based
emulation framework that models the firmware signing process, the EL1 to EL3
load path, and secure memory configuration. FAARM reliably detects and blocks
malicious firmware injections, rejecting tampered images before use and denying
overwrite attempts after attestation. Firmware verification incurs only 1.34 ms
latency on average, demonstrating that strong security can be achieved with
negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU
TEEs, providing a practical, deployable defense that raises the security
baseline for both mobile and cloud GPU deployments.

</details>


### [240] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska, Max Mathys, Francesco Casucci, Mateo Rojas-Carulla, Xander Davies, Alexandra Souly, Niklas Pfister*

**主要类别:** cs.CR

**AI概要:** 论文提出了threat snapshots框架和b³基准测试，用于系统评估LLM骨干模型对AI代理安全性的影响，发现推理能力提升安全性而模型大小无关。


<details>
  <summary>更多</summary>
  
**动机:** 现有框架无法系统分析LLM选择对AI代理安全的影响，传统安全风险与LLM漏洞交织，缺乏全面的安全评估方法。

**方法:** 引入threat snapshots框架隔离代理执行流程中的特定状态，构建基于194331个众包对抗攻击的b³安全基准，评估31个流行LLM。

**结果:** 增强的推理能力能提高安全性，但模型大小与安全性无相关性，为LLM提供商和开发者提供了安全改进指导。

**结论:** 该框架和基准测试为系统识别和分类LLM到代理级别的安全风险提供了有效工具，促进了LLM安全性的广泛评估和改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Agent+Backbones%3A+Evaluating+the+Security+of+Backbone+LLMs+in+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22620&send_immediately=true&force_search=false)

**原文摘要:** AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [241] [DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2510.22622)
*Kangran Zhao, Yupeng Chen, Xiaoyu Zhang, Yize Chen, Weinan Guan, Baicheng Chen, Chengzhe Sun, Soumyya Kanti Datta, Qingshan Liu, Siwei Lyu, Baoyuan Wu*

**主要类别:** cs.CR

**AI概要:** 本文构建了大规模多模态深度伪造数据集Mega-MMDF和首个统一基准测试平台DeepfakeBench-MM，用于解决多模态深度伪造检测中的数据缺乏和标准化评估问题。


<details>
  <summary>更多</summary>
  
**动机:** 先进的生成式AI模型被滥用导致伪造人类视听内容泛滥，带来严重社会风险，但现有研究缺乏足够多样化的训练数据和标准化基准，阻碍了深度探索。

**方法:** 1) 构建Mega-MMDF数据集：使用21种伪造流程（10种音频伪造+12种视觉伪造+6种音频驱动面部重现方法），包含11万真实样本和110万伪造样本；2) 建立DeepfakeBench-MM基准：提供标准化检测协议，支持5个数据集和11种多模态检测器。

**结果:** 创建了当前最大最丰富的多模态深度伪造数据集和首个统一基准平台，通过综合评估发现了多个关键发现（如数据增强、堆叠伪造等方面）。

**结论:** Mega-MMDF数据集和DeepfakeBench-MM基准将成为推进多模态深度伪造检测研究的基础设施，为解决AI生成内容的安全威胁提供重要支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepfakeBench-MM%3A+A+Comprehensive+Benchmark+for+Multimodal+Deepfake+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22622&send_immediately=true&force_search=false)

**原文摘要:** The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.

</details>


### [242] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain*

**主要类别:** cs.CR

**AI概要:** Sentra-Guard是一个实时模块化防御系统，使用混合架构检测和缓解针对大语言模型的越狱和提示注入攻击，在多项指标上达到近乎完美的检测性能。


<details>
  <summary>更多</summary>
  
**动机:** 针对大语言模型面临的越狱和提示注入攻击威胁，需要开发有效的实时防御系统来保护LLM安全。

**方法:** 采用混合架构：FAISS索引的SBERT嵌入表示捕获提示语义，结合微调transformer分类器；包含分类器-检索器融合模块计算上下文感知风险评分；多语言预处理层支持100多种语言；包含人机交互反馈循环持续学习。

**结果:** 检测率达到99.96%（AUC=1.00，F1=1.00），攻击成功率仅0.004%，显著优于LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）等基准方法。

**结论:** Sentra-Guard建立了对抗性LLM防御的新技术标准，具有透明、可微调、多后端兼容的特点，支持商业和开源环境的可扩展部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentra-Guard%3A+A+Multilingual+Human-AI+Framework+for+Real-Time+Defense+Against+Adversarial+LLM+Jailbreaks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22628&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [243] [RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography](https://arxiv.org/abs/2510.22661)
*Malik Imran, Safiullah Khan, Zain Ul Abideen, Ciara Rafferty, Ayesha Khalid, Muhammad Rashid, Maire O'Neill*

**主要类别:** cs.CR

**AI概要:** RejSCore是一个针对后量子密码学中拒绝采样操作的轻量级硬件加速器，专门为QR-UOV方案设计，在Artix-7 FPGA和65nm CMOS技术上实现了高效的性能和资源利用。


<details>
  <summary>更多</summary>
  
**动机:** 后量子多元公钥密码方案需要繁重的操作如拒绝采样，这对资源受限设备构成挑战，而现有的硬件设计在这方面研究不足。

**方法:** 设计了一个包含AES-CTR-128伪随机数生成器的架构，采用轻量级迭代方法进行拒绝采样，降低了资源消耗和面积开销。

**结果:** 在Artix-7上达到2042个slice和222MHz频率，在65nm CMOS上达到464,866μm²面积和565MHz频率，使用QR-UOV安全级别I参数时完成操作需要8525个时钟周期。

**结论:** ADP和PDP评估证实RejSCore适合部署在资源受限和安全关键的环境中，为后量子密码学的硬件实现提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RejSCore%3A+Rejection+Sampling+Core+for+Multivariate-based+Public+key+Cryptography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22661&send_immediately=true&force_search=false)

**原文摘要:** Post-quantum multivariate public key cryptography (MPKC) schemes resist
quantum threats but require heavy operations, such as rejection sampling, which
challenge resource-limited devices. Prior hardware designs have addressed
various aspects of MPKC signature generation. However, rejection sampling
remains largely unexplored in such contexts. This paper presents RejSCore, a
lightweight hardware accelerator for rejection sampling in post-quantum
cryptography. It specifically targets the QR-UOV scheme, which is a prominent
candidate under the second-round of the National Institute of Standards and
Technology (NIST) additional digital signature standardization process. The
architecture includes an AES-CTR-128-based pseudorandom number generator.
Moreover, a lightweight iterative method is employed in rejection sampling,
offering reduced resource consumption and area overhead while slightly
increasing latency. The performance of RejSCore is comprehensively evaluated on
Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and
Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area
of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and
565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =
127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525
clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for
deployment in resource-constrained and security-critical environments.

</details>


### [244] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le, Tan Le*

**主要类别:** cs.CR

**AI概要:** SpoofTrackBench是一个可复现的模块化基准测试，用于评估雷达欺骗攻击下实时定位跟踪系统的对抗鲁棒性，支持多种欺骗攻击模拟和跨架构性能分析。


<details>
  <summary>更多</summary>
  
**动机:** 需要建立一个开放、可复现的基准测试框架，用于系统评估实时定位跟踪系统在雷达欺骗攻击下的鲁棒性，促进不同跟踪架构的性能比较和社区验证。

**方法:** 利用Hampton大学Skyler雷达传感器数据集，模拟漂移、幽灵和镜像三种欺骗攻击，使用JPDA和GNN两种跟踪架构进行评估，通过分离干净和欺骗检测流、可视化轨迹偏差和量化赋值误差来进行分析。

**结果:** 开发了包含聚类覆盖、注入感知时间线和场景自适应可视化的可解释框架，能够自动导出评估图表和日志，实现跨欺骗类型和配置的可复现比较。

**结论:** SpoofTrackBench为欺骗感知跟踪流程的开放、伦理基准测试设立了新标准，支持严格的跨架构分析和社区验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpoofTrackBench%3A+Interpretable+AI+for+Spoof-Aware+UAV+Tracking+and+Benchmarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22726&send_immediately=true&force_search=false)

**原文摘要:** SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [245] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang, YiLu Zhong, MiDi Wan, WenJie Yu, YuanBing Ouyang, Yenan Huang, Hui Li*

**主要类别:** cs.CR

**AI概要:** 论文研究发现提示词质量与AI生成代码安全性直接相关，低质量提示词显著增加代码安全风险，而高级提示技术能有效缓解这种风险。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注对抗攻击和模型内在缺陷，但忽视了良性但表述不佳的提示词对生成代码安全性的影响，这是一个普遍但未充分探索的问题。

**方法:** 提出了包含目标清晰度、信息完整性和逻辑一致性的提示词质量评估框架，构建了包含四个规范性等级(L0-L3)的大规模基准数据集CWE-BENCH-PYTHON，并在多个先进LLM上进行广泛实验。

**结果:** 实验显示提示词规范性降低与生成不安全代码的可能性呈明显正相关，Chain-of-Thought和Self-Correction等高级提示技术能有效减轻低质量提示带来的安全风险。

**结论:** 提高用户提示词质量是增强AI生成代码安全性的关键有效策略，需要重视提示词规范性对代码安全的重要影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Your+Prompt+Poisoning+Code%3F+Defect+Induction+Rates+and+Security+Mitigation+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22944，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22944&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [246] [QuantumShield: Multilayer Fortification for Quantum Federated Learning](https://arxiv.org/abs/2510.22945)
*Dev Gurung, Shiva Raj Pokhrel*

**主要类别:** cs.CR

**AI概要:** 提出了一种量子安全的联邦学习框架，通过集成量子密钥分发、量子隐形传态、密钥封装机制和后量子密码学等先进协议，保护分布式学习系统免受量子攻击威胁。


<details>
  <summary>更多</summary>
  
**动机:** 随着经典加密方法对量子攻击的脆弱性日益增加，需要建立能够在量子攻击下保持安全的联邦学习安全架构。

**方法:** 集成和评估量子密钥分发(QKD)、量子隐形传态、密钥封装机制(KEM)和后量子密码学(PQC)等协议，构建安全可扩展的量子安全联邦学习生态系统。

**结果:** 通过理论建模和实验验证，提供了详细的安全性和性能评估，证明了这些机制在联邦学习过程中的无缝互操作性。

**结论:** 这项工作为量子时代固有安全的下一代联邦学习系统奠定了坚实基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuantumShield%3A+Multilayer+Fortification+for+Quantum+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22945&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a groundbreaking quantum-secure federated learning
(QFL) framework designed to safeguard distributed learning systems against the
emerging threat of quantum-enabled adversaries. As classical cryptographic
methods become increasingly vulnerable to quantum attacks, our framework
establishes a resilient security architecture that remains robust even in the
presence of quantum-capable attackers. We integrate and rigorously evaluate
advanced quantum and post-quantum protocols including Quantum Key Distribution
(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and
Post-Quantum Cryptography (PQC) to fortify the QFL process against both
classical and quantum threats. These mechanisms are systematically analyzed and
implemented to demonstrate their seamless interoperability within a secure and
scalable QFL ecosystem. Through comprehensive theoretical modeling and
experimental validation, this work provides a detailed security and performance
assessment of the proposed framework. Our findings lay a strong foundation for
next-generation federated learning systems that are inherently secure in the
quantum era.

</details>


### [247] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She*

**主要类别:** cs.CR

**AI概要:** LLM提示压缩存在安全风险，攻击者可通过CompressionAttack框架利用压缩模块漏洞，实现高达80%的攻击成功率和98%的偏好翻转，现有防御措施无效。


<details>
  <summary>更多</summary>
  
**动机:** LLM驱动的代理常使用提示压缩来降低推理成本，但压缩模块优先考虑效率而非安全性，容易被对抗性输入操纵，导致语义漂移和LLM行为改变。

**方法:** 提出CompressionAttack框架，包含两种策略：HardCom（使用离散对抗性编辑进行硬压缩）和SoftCom（在潜在空间进行扰动实现软压缩）。

**结果:** 在多个LLM上的实验显示攻击成功率高达80%，偏好翻转率达98%，攻击具有高度隐蔽性和可迁移性。VSCode Cline和Ollama的案例研究证实了实际影响。

**结论:** 当前防御措施被证明无效，凸显了需要更强保护措施的必要性，提示压缩已成为新的攻击面需要重点关注。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CompressionAttack%3A+Exploiting+Prompt+Compression+as+a+New+Attack+Surface+in+LLM-Powered+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22963&send_immediately=true&force_search=false)

**原文摘要:** LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [248] [Advancing Honeywords for Real-World Authentication Security](https://arxiv.org/abs/2510.22971)
*Sudiksha Das, Ashish Kundu*

**主要类别:** cs.CR

**AI概要:** 蜜词技术作为密码安全防护手段已有10年研究但未被主流认证平台采用，本文提出需要解决平坦性、集成性和可靠性问题，建议构建可部署框架结合攻击弹性、上下文感知的诱饵创建与易集成特性。


<details>
  <summary>更多</summary>
  
**动机:** 蜜词技术虽具潜力但未被实际部署，需要解决技术障碍使其从学术概念转变为实用安全工具。

**方法:** 分析现有蜜词生成技术、攻击者建模和蜜检查器架构，识别已解决的问题和阻碍广泛应用的持续性问题。

**结果:** 提出了一个可部署框架，结合攻击弹性、上下文感知的诱饵创建能力，并易于集成到现有系统中。

**结论:** 蜜词技术要成为实用安全工具，需要技术进展与安全简洁架构相结合，配备自适应响应处理和详细配置检查。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Honeywords+for+Real-World+Authentication+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22971&send_immediately=true&force_search=false)

**原文摘要:** Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords
stored alongside a real password, appear to be a proactive method to help
detect password credentials misuse. However, despite over a decade of research,
this technique has not been adopted by major authentication platforms. This
position paper argues that the core concept of Honeywords has potential but
requires more research on issues such as flatness, integration, and
reliability, in order to be a practical deployable solution. This paper
examines the current work on Honeyword generation, attacker modeling, and
honeychecker architecture, analyzing the subproblems that have been addressed
and ongoing issues that prevent this system from being more widely used. The
paper then suggests a deployable framework that combines the
attacker-resilient, context-aware decoy creation that Honeywords provide with
easy integration into existing systems. Honeywords will only move from an
academic idea to a practical security tool if technical advances are paired
with secure and straightforward architectures, along with adaptive response
handling and detailed configuration checks.

</details>


### [249] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan, Zeng Li, Kunlin Cai, Liuhuo Wan, Ruomai Ren, Yiran Shen, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** 首个针对VR应用生态系统的多平台隐私实践研究，分析了6565个应用，发现严重的隐私合规问题，包括三分之一应用未声明敏感数据使用，21.5%应用缺乏有效隐私政策。


<details>
  <summary>更多</summary>
  
**动机:** VR应用收集大量隐私敏感数据（如生物特征、行为和环境数据），但缺乏领域特定的监管，导致各应用商店的隐私实践存在显著差异。

**方法:** 使用自然语言处理、逆向工程和静态分析的多方面方法，评估VR应用的声明性和行为性隐私实践。

**结果:** 发现所有商店都存在显著的隐私合规问题：33%的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策，显示隐私保护处于不成熟状态。

**结论:** 研究首次揭示了VR应用生态系统的隐私保护现状，提醒开发者和用户关注隐私风险，并呼吁应用商店运营商实施更严格的隐私合规监管。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Store+Privacy+Measurement+of+Virtual+Reality+App+Ecosystem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23024&send_immediately=true&force_search=false)

**原文摘要:** Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [250] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran, Suman Deb, Anupam Chattopadhyay*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于物理不可克隆函数的BNN模型参数保护方法，在内存计算框架中实现加密存储和同态推理，既能有效防止模型窃取，又能保持计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法将BNN模型参数加密存储并在运行时解密会带来显著计算开销，违背了内存计算整合计算与存储的核心原则，需要一种既能保护模型又能保持效率的方法。

**方法:** 利用物理不可克隆函数生成密钥对模型参数进行变换后存储在交叉阵列中，推理时直接在加密权重上进行操作，实现了一种特殊的全同态加密。

**结果:** 在没有密钥的情况下进行推理，模型准确率降至15%以下，验证了保护策略的有效性。

**结论:** 该方法成功解决了BNN在内存计算架构中的安全保护问题，在保持计算效率的同时有效防止了模型参数被盗用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Encrypted+Inference+using+Binarized+Neural+Networks+within+In-Memory+Computing+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23034&send_immediately=true&force_search=false)

**原文摘要:** Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [251] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang, Weiming Zhang, Nenghai Yu, Kejiang Chen*

**主要类别:** cs.CR

**AI概要:** RTMStega是一种基于熵驱动的语言隐写框架，通过秩基自适应编码和上下文感知解压缩技术，在保持文本质量的同时显著提升隐写容量和处理效率


<details>
  <summary>更多</summary>
  
**动机:** 当前语言隐写方法面临嵌入容量低和安全性的双重挑战：传统修改方法会产生可检测异常，检索式方法容量有限，生成式方法受限于词汇预测的有限熵值

**方法:** 提出RTMStega框架，整合秩基自适应编码和上下文感知解压缩技术，通过将秘密消息映射到词汇概率排名，并基于上下文感知的熵值调整进行动态采样

**结果:** 实验显示RTMStega将主流生成式隐写的有效载荷容量提升三倍，处理时间减少50%以上，同时保持高文本质量

**结论:** RTMStega为安全高效的隐蔽通信提供了可信解决方案，在容量、安全性和效率之间实现了良好平衡

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+high-capacity+linguistic+steganography+based+on+entropy-driven+rank-token+mapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23035&send_immediately=true&force_search=false)

**原文摘要:** Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


### [252] [KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation](https://arxiv.org/abs/2510.23036)
*Xudong Yang, Jincheng Li, Kaiwen Xing, Zhenjia Xiao, Mingjian Duan, Weili Han, Hu Xiong*

**主要类别:** cs.CR

**AI概要:** KAPG是一个知识增强的密码猜测框架，通过整合外部词汇知识来提升密码猜测效果，在12个泄露数据集上相比最先进模型平均提升36.5%（站内）和74.7%（跨站）。同时开发了KAPSM密码强度计，在准确性上显著优于现有工具。


<details>
  <summary>更多</summary>
  
**动机:** 传统密码猜测模型主要依赖泄露密码的内部统计模式，忽视了社会背景、文化趋势和流行词汇等外部影响因素，导致其对新密码趋势的适应性不足，效果随时间递减。

**方法:** 提出KAPG框架，将泄露密码的内部统计知识与反映现实趋势的外部信息相结合。使用密码前缀作为知识查找锚点，在生成过程中动态注入相关外部线索，同时保持真实密码的结构规律性。

**结果:** 在12个泄露数据集上的实验显示，KAPG在站内和跨站场景下分别比最先进模型平均提升36.5%和74.7%。密码重叠分析和模型效率分析验证了其鲁棒性和计算效率。

**结论:** KAPG通过整合外部知识有效解决了传统密码猜测模型的局限性，显著提升了猜测效果。配套开发的KAPSM密码强度计也表现出优异的准确性，为密码安全提供了更有效的防护工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KAPG%3A+Adaptive+Password+Guessing+via+Knowledge-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23036&send_immediately=true&force_search=false)

**原文摘要:** As the primary mechanism of digital authentication, user-created passwords
exhibit common patterns and regularities that can be learned from leaked
datasets. Password choices are profoundly shaped by external factors, including
social contexts, cultural trends, and popular vocabulary. Prevailing password
guessing models primarily emphasize patterns derived from leaked passwords,
while neglecting these external influences -- a limitation that hampers their
adaptability to emerging password trends and erodes their effectiveness over
time.
  To address these challenges, we propose KAPG, a knowledge-augmented password
guessing framework that adaptively integrates external lexical knowledge into
the guessing process. KAPG couples internal statistical knowledge learned from
leaked passwords with external information that reflects real-world trends. By
using password prefixes as anchors for knowledge lookup, it dynamically injects
relevant external cues during generation while preserving the structural
regularities of authentic passwords. Experiments on twelve leaked datasets show
that KnowGuess achieves average improvements of 36.5\% and 74.7\% over
state-of-the-art models in intra-site and cross-site scenarios, respectively.
Further analyses of password overlap and model efficiency highlight its
robustness and computational efficiency. To counter these attacks, we further
develop KAPSM, a trend-aware and site-specific password strength meter.
Experiments demonstrate that KAPSM significantly outperforms existing tools in
accuracy across diverse evaluation settings.

</details>


### [253] [zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks](https://arxiv.org/abs/2510.23060)
*Paritosh Ramanan, H. M. Mohaimanul Islam, Abhiram Reddy Alugula*

**主要类别:** cs.CR

**AI概要:** zkSTAR是一个基于zk-SNARKs的工业控制系统网络攻击检测框架，能够在保护数据机密性的同时提供可验证的检测保证，满足监管机构的合规要求。


<details>
  <summary>更多</summary>
  
**动机:** 工业控制系统面临日益严重的网络威胁，监管机构要求更严格的合规性，但又不希望公用事业公司披露敏感的运营数据，因此需要一种既能验证检测有效性又能保护数据隐私的解决方案。

**方法:** 采用基于残差的统计假设检验方法，构建双管齐下的zk-SNARK架构，强制状态空间动态的时间一致性和检测测试的统计一致性，使监管机构能够验证警报正确性而无需访问底层数据。

**结果:** 通过形式化分析框架的健全性和零知识属性，并在真实工业控制系统数据集上进行计算实验，验证了该框架的实际可行性。

**结论:** zkSTAR为工业控制系统驱动的关键基础设施网络提供了一个可扩展、保护隐私的监管合规替代方案，成功解决了隐私保护与监管验证之间的矛盾。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是zkSTAR%3A+A+zero+knowledge+system+for+time+series+attack+detection+enforcing+regulatory+compliance+in+critical+infrastructure+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23060&send_immediately=true&force_search=false)

**原文摘要:** Industrial control systems (ICS) form the operational backbone of critical
infrastructure networks (CIN) such as power grids, water supply systems, and
gas pipelines. As cyber threats to these systems escalate, regulatory agencies
are imposing stricter compliance requirements to ensure system-wide security
and reliability. A central challenge, however, is enabling regulators to verify
the effectiveness of detection mechanisms without requiring utilities to
disclose sensitive operational data. In this paper, we introduce zkSTAR, a
cyberattack detection framework that leverages zk-SNARKs to reconcile these
requirements and enable provable detection guarantees while preserving data
confidentiality. Our approach builds on established residual-based statistical
hypothesis testing methods applied to state-space detection models.
Specifically, we design a two-pronged zk-SNARK architecture that enforces
temporal consistency of the state-space dynamics and statistical consistency of
the detection tests, allowing regulators to temporally verify alarm correctness
without visibility into utility-level data. We formally analyze the soundness
and zero knowledge properties of our framework and validate its practical
feasibility through computational experiments on real-world ICS datasets. As a
result, our work demonstrates a scalable, privacy-preserving alternative for
regulatory compliance for ICS driven critical infrastructure networks.

</details>


### [254] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi, Shotaro Ishihara*

**主要类别:** cs.CR

**AI概要:** Fast-MIA是一个用于高效评估大语言模型成员推理攻击的Python库，解决了计算成本高和缺乏标准化实现的问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于对版权、安全和数据隐私的担忧日益增长，成员推理攻击研究面临高计算成本和缺乏标准化实现两大障碍。

**方法:** 提供快速批量推理功能，并在统一评估框架下实现了代表性MIA方法，支持简单配置和可扩展性。

**结果:** 开发了开源工具Fast-MIA，支持大规模可复现的基准测试。

**结论:** Fast-MIA作为一个开源工具，旨在支持大语言模型研究的可扩展性和透明度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast-MIA%3A+Efficient+and+Scalable+Membership+Inference+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23074&send_immediately=true&force_search=false)

**原文摘要:** We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


### [255] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang, Xin Zhang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于大语言模型（LLM）的定向灰盒模糊测试方法，通过预测漏洞触发调用栈来指导种子优先级排序，相比现有方法显著提高了漏洞发现效率


<details>
  <summary>更多</summary>
  
**动机:** 现有的定向灰盒模糊测试方法依赖静态分析的距离度量，存在概率计算不精确的问题，导致大量无关执行路径被误判，显著降低了模糊测试效率

**方法:** 使用大语言模型预测漏洞触发调用栈来替代静态分析的距离度量。首先通过静态分析构建调用图识别可能到达目标位置的方法，然后利用LLM预测最可能触发漏洞的调用栈序列，优先选择执行路径与预测调用栈重叠度高的种子进行变异

**结果:** 在真实程序套件上，该方法触发漏洞的速度比基线方法快1.86到3.09倍，并在最新版本的程序中发现了10个新漏洞和2个不完整修复，获得了10个CVE编号

**结论:** 基于LLM的调用栈预测方法能够有效提高定向模糊测试的精度和效率，是第一个将LLM集成到DGF核心种子优先级排序机制的工作，展示了LLM在软件安全测试中的巨大潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Imprecise+Distance+Metrics%3A+LLM-Predicted+Target+Call+Stacks+for+Directed+Greybox+Fuzzing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23101&send_immediately=true&force_search=false)

**原文摘要:** Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [256] [Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation](https://arxiv.org/abs/2510.23172)
*Mohsen Ahmadvand, Pedro Souto*

**主要类别:** cs.CR

**AI概要:** 论文分析了Optimism衍生管道的zkVM移植效率问题，提出了针对零知识证明的重新设计，实现了6.5倍的衍生速度提升和3.5倍的整体加速，同时保持相同的安全保证。


<details>
  <summary>更多</summary>
  
**动机:** 当前的Optimism衍生管道设计注重正确性和活跃性，而非简洁的有效性证明，直接移植到zkVM会产生显著的开销，使有效性证明成本远高于必要水平。

**方法:** 系统识别当前设计中的低效问题，分析其对证明成本的影响，并提供保持正确性的重新设计方案，专门针对零知识证明进行优化。

**结果:** 重新设计在zkVM内部实现了最高6.5倍的衍生速度提升，整体速度提升3.5倍，同时维持相同的安全保证。

**结论:** 通过针对零知识证明特性进行专门优化，可以显著降低Optimism衍生管道在zkVM中的证明成本，同时不牺牲安全性，为区块链扩容解决方案提供了更高效的实现路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Optimism%3A+Up+to+6.5x+Faster+zkVM+Validty+Proofs+via+Sparse+Derivation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23172&send_immediately=true&force_search=false)

**原文摘要:** The Optimism derivation pipeline is engineered for correctness and liveness,
not for succinct validity proofs. A straightforward port to a zkVM imposes
significant overheads, making validity proofs significantly more costly than
necessary. We systematically identify inefficiencies in the current design,
analyze their impact on proving costs, and provide a soundness-preserving
redesign tailored to zk proving. Our redesign achieves up to 6.5x faster
derivation inside zkVMs (3.5x overall speedup) while maintaining identical
safety guarantees.

</details>


### [257] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen, Qianqian Yang, Shuo Shao, Shunpu Tang, Zhiguo Shi, Shui Yu*

**主要类别:** cs.CR

**AI概要:** 提出一种基于差分隐私的语义通信安全框架，通过可学习模式的噪声保护图像隐私，在保证合法用户任务性能的同时显著降低窃听者的重建质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有安全语义通信方法依赖限制性假设（如有利信道条件或窃听者模型先验知识），无法提供实用隐私保护，需要一种更实用的安全语义通信方案。

**方法:** 使用GAN反演提取解耦语义表示，选择性地对私有语义表示添加可学习模式的差分隐私噪声，通过神经网络对抗训练实现噪声模式学习。

**结果:** 相比传统DP方法和直接传输，显著降低窃听者重建质量（LPIPS优势0.06-0.29，FPPSR优势0.10-0.86），同时对合法用户任务性能影响很小。

**结论:** 该框架通过可学习模式差分隐私噪声实现了有效的隐私保护，提供了明确可控的安全级别调整能力，为语义通信安全提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Semantic+Communication+over+Wiretap+Channels+with+Learnable+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23274&send_immediately=true&force_search=false)

**原文摘要:** While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>


### [258] [Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks](https://arxiv.org/abs/2510.23313)
*Yaokai Feng, Kouichi Sakurai*

**主要类别:** cs.CR

**AI概要:** 这篇综述系统梳理了网络入侵检测系统(NIDS)从传统方法到LLM集成的发展历程，总结了当前技术现状、优缺点，并探讨了LLM在NIDS中的实际应用价值。


<details>
  <summary>更多</summary>
  
**动机:** 系统化分析NIDS技术演进，从传统签名检测和神经网络方法到新兴的LLM集成，评估各种方法的实用性和挑战。

**方法:** 文献综述方法，系统梳理和分析不同NIDS技术（签名检测、神经网络、LLM集成）在不同环境（传统网络、自动驾驶、IoT）中的应用研究。

**结果:** 1)签名检测仍有重要价值；2)神经网络检测面临实际部署挑战；3)LLM在NIDS中有用但存在安全风险；4)需要构建领域专用LLM。

**结论:** LLM为NIDS带来新机遇但面临实际应用挑战，需要开发领域专用模型并解决安全风险问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Network+Intrusion+Detection%3A+Evolution+from+Conventional+Approaches+to+LLM+Collaboration+and+Emerging+Risks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23313&send_immediately=true&force_search=false)

**原文摘要:** This survey systematizes the evolution of network intrusion detection systems
(NIDS), from conventional methods such as signature-based and neural network
(NN)-based approaches to recent integrations with large language models (LLMs).
It clearly and concisely summarizes the current status, strengths, and
limitations of conventional techniques, and explores the practical benefits of
integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS
in diverse environments is reviewed, including conventional network
infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods,
signature-based IDSs, continue to make significant contributions to modern
systems, despite their well-known weaknesses; 2) NN-based detection, although
considered promising and under development for more than two decades, and
despite numerous related approaches, still faces significant challenges in
practical deployment; 3) LLMs are useful for NIDS in many cases, and a number
of related approaches have been proposed; however, they still face significant
challenges in practical applications. Moreover, they can even be exploited as
offensive tools, such as for generating malware, crafting phishing messages, or
launching cyberattacks. Recently, several studies have been proposed to address
these challenges, which are also reviewed in this survey; and 4) strategies for
constructing domain-specific LLMs have been proposed and are outlined in this
survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.

</details>


### [259] [Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era](https://arxiv.org/abs/2510.23457)
*Saleh Darzi, Mirza Masfiqur Rahman, Imtiaz Karim, Rouzbeh Behnia, Attila A Yavuz, Elisa Bertino*

**主要类别:** cs.CR

**AI概要:** 该论文分析了5G基站认证在量子计算威胁下的安全性问题，提出了基于分层身份基门限签名的过渡解决方案BORG，解决了直接集成后量子密码标准的可行性问题。


<details>
  <summary>更多</summary>
  
**动机:** 5G协议在初始启动阶段缺乏强大的基站认证机制，容易受到伪基站攻击。传统PKI和身份基签名方案无法抵御量子攻击，而现有的后量子密码标准在5G基站认证中的适用性尚未探索。

**方法:** 对NIST后量子密码标准和传统数字签名方案（包括门限和身份基方案）在5G基站认证中的网络级性能进行全面表征分析，并提出了基于分层身份基门限签名方案的BORG解决方案。

**结果:** 研究发现直接采用后量子密码标准存在协议约束和大签名尺寸的可行性问题，传统方法也因证书链开销存在性能限制。BORG方案通过门限签名和紧凑签名提供了事后量子伪造检测和分布式信任。

**结论:** 直接集成后量子密码标准在5G认证中不可行，BORG作为过渡解决方案能有效满足5G严格需求，为未来量子弹性认证提供可行路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Authentication+Against+Insecure+Bootstrapping+for+5G+Networks%3A+Feasibility%2C+Resiliency%2C+and+Transitional+Solutions+in+Post-Quantum+Era，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23457&send_immediately=true&force_search=false)

**原文摘要:** The 5G protocol lacks a robust base station authentication mechanism during
the initial bootstrapping phase, leaving it susceptible to threats such as fake
base station attacks. Conventional solutions, including digital signatures
based on Public Key Infrastructures (PKIs) and identity-based signatures, are
inadequate against quantum-capable adversaries. While integrating NIST's
Post-Quantum Cryptography (PQC) standards is a leading approach for quantum
resistance, their suitability for 5G base station authentication remains
unexplored. Moreover, current solutions are predominantly centralized and lack
security features such as distributed authentication. This work presents, to
our knowledge, the first comprehensive network-level performance
characterization of integrating NIST-PQC standards and conventional digital
signatures (including threshold and identity-based schemes) into 5G base
station authentication. Our findings reveal significant feasibility concerns,
with direct PQC adoption hindered by protocol constraints and large signature
sizes. We also highlight the performance limitations of conventional methods
due to the overhead of certificate chains. To mitigate these challenges, we
propose BORG, a transitional authentication solution based on a Hierarchical
Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG
offers post-mortem post-quantum forgery detection and distributed trust via
threshold and compact signatures, well-suited for 5G's stringent requirements.
Our performance analysis underscores an important warning on the infeasibility
of direct PQC integration and positions BORG as an effective transitional
solution toward future quantum-resilient 5G authentication.

</details>


### [260] [Towards a Functionally Complete and Parameterizable TFHE Processor](https://arxiv.org/abs/2510.23483)
*Valentin Reyes Häusler, Gabriel Ott, Aruna Jayasena, Andreas Peter*

**主要类别:** cs.CR

**AI概要:** 提出基于FPGA的TFHE全同态加密硬件加速器，通过改进的可编程自举模块将性能提升240%-480%，为FPGA架构的完整TFHE处理器奠定基础


<details>
  <summary>更多</summary>
  
**动机:** TFHE虽然自举操作快，但同态电路评估计算开销大，比未加密计算慢几个数量级，阻碍了FHE的广泛应用。现有实现受限于高内存带宽成本

**方法:** 设计基于FPGA的TFHE处理器硬件加速器，实现完全在FPGA上处理数据的指令，并开发改进的可编程自举模块

**结果:** 实现了比现有最优方案快240%-480%的自举操作性能，每秒处理更多自举操作

**结论:** 高效、紧凑且可扩展的设计为完整的基于FPGA的TFHE处理器架构奠定了基础，有望克服同态加密的计算瓶颈

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Functionally+Complete+and+Parameterizable+TFHE+Processor，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23483&send_immediately=true&force_search=false)

**原文摘要:** Fully homomorphic encryption allows the evaluation of arbitrary functions on
encrypted data. It can be leveraged to secure outsourced and multiparty
computation. TFHE is a fast torus-based fully homomorphic encryption scheme
that allows both linear operations, as well as the evaluation of arbitrary
non-linear functions. It currently provides the fastest bootstrapping operation
performance of any other FHE scheme. Despite its fast performance, TFHE suffers
from a considerably higher computational overhead for the evaluation of
homomorphic circuits. Computations in the encrypted domain are orders of
magnitude slower than their unencrypted equivalents. This bottleneck hinders
the widespread adoption of (T)FHE for the protection of sensitive data. While
state-of-the-art implementations focused on accelerating and outsourcing single
operations, their scalability and practicality are constrained by high memory
bandwidth costs. In order to overcome this, we propose an FPGA-based hardware
accelerator for the evaluation of homomorphic circuits. Specifically, we design
a functionally complete TFHE processor for FPGA hardware capable of processing
instructions on the data completely on the FPGA. In order to achieve a higher
throughput from our TFHE processor, we implement an improved programmable
bootstrapping module which outperforms the current state-of-the-art by 240\% to
480\% more bootstrappings per second. Our efficient, compact, and scalable
design lays the foundation for implementing complete FPGA-based TFHE processor
architectures.

</details>
