<div id=toc></div>

# 目录

- [cs.CR](#cs.CR) [总数: 63]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah, Maha Driss, Henda Ben Ghezela*

**主要类别:** cs.CR

**AI概要:** 提出基于微服务架构的联邦学习解决方案，用于物联网边缘计算环境下的高效数据分析，在恶意软件检测分类任务中达到99.24%的准确率


<details>
  <summary>更多</summary>
  
**动机:** 物联网数据分析和隐私安全问题需要分布式解决方案，传统本地或云端分析存在隐私安全顾虑，且需要低延迟高可靠的时间敏感结果

**方法:** 采用微服务架构结合联邦学习技术，将应用分解为细粒度、松耦合的可重用实体，在边缘设备上进行分布式数据分析

**结果:** 在MaleVis数据集（14,000+图像，25个恶意软件类别和1个良性类别）上验证，检测和分类性能优于现有最先进方法，准确率达99.24%

**结论:** 基于微服务的联邦学习方法能有效降低延迟和带宽拥塞，保护数据隐私，为物联网边缘计算环境提供高效灵活可扩展的数据分析解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedMicro-IDA%3A+A+Federated+Learning+and+Microservices-based+Framework+for+IoT+Data+Analytics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20852&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [2] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng, Jin Li, Zhenhua Zhao, Shaowei Wang*

**主要类别:** cs.CR

**AI概要:** 提出FPT-Noise测试时防御方法，无需微调即可显著提升CLIP模型的对抗鲁棒性，在AutoAttack下将鲁棒准确率从0.07%提升至56.86%


<details>
  <summary>更多</summary>
  
**动机:** 现有视觉语言模型（如CLIP）对视觉模态的对抗攻击高度脆弱，传统对抗训练方法计算成本高昂且需要大量重新训练

**方法:** 提出动态特征调制器生成图像特定和攻击自适应的噪声强度参数，建立特征感知阈值区分干净和对抗图像，结合场景感知调节和测试时变换集成

**结果:** FPT-Noise显著优于现有测试时防御方法，在AutoAttack下平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上保持高性能（仅下降1.1%）

**结论:** 该方法提供了一种无需昂贵微调的有效测试时防御方案，显著提升了CLIP模型的对抗鲁棒性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FPT-Noise%3A+Dynamic+Scene-Aware+Counterattack+for+Test-Time+Adversarial+Defense+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20856&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [3] [Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology](https://arxiv.org/abs/2510.20858)
*Nubio Vidal, Naghmeh Moradpoor, Leandros Maglaras*

**主要类别:** cs.CR

**AI概要:** AIR框架是一个用于运营技术(OT)实时事件报告的标准化框架，包含25个元素和7个组别，旨在解决OT事件报告中数据捕获不明确的问题，支持技术协调和监管一致性。


<details>
  <summary>更多</summary>
  
**动机:** OT网络与IT的融合扩大了攻击面并复杂化了事件响应，现有OT标准强调事件报告和证据保存但未指定具体数据捕获内容，IT指南定义了报告内容但不考虑OT约束。

**方法:** 开发Agnostic Incident Reporting (AIR)框架，包含25个元素组织成7个组别，通过映射到主要OT标准、定义集成激活点、触发现有OT框架，并回溯应用于2015年乌克兰电网事件进行评估。

**结果:** 评估表明AIR能将高层需求转化为具体字段，在不依赖供应商的情况下覆盖现有框架，支持响应期间的情境感知和通信。

**结论:** AIR为标准化实时OT事件报告提供了基础，同时支持技术协调和监管对齐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Everyone+Needs+AIR%3A+An+Agnostic+Incident+Reporting+Framework+for+Cybersecurity+in+Operational+Technology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20858&send_immediately=true&force_search=false)

**原文摘要:** Operational technology (OT) networks are increasingly coupled with
information technology (IT), expanding the attack surface and complicating
incident response. Although OT standards emphasise incident reporting and
evidence preservation, they do not specify what data to capture during an
incident, which hinders coordination across stakeholders. In contrast, IT
guidance defines reporting content but does not address OT constraints. This
paper presents the Agnostic Incident Reporting (AIR) framework for live OT
incident reporting. AIR comprises 25 elements organised into seven groups to
capture incident context, chronology, impacts, and actions, tailored to
technical, managerial, and regulatory needs. We evaluate AIR by mapping it to
major OT standards, defining activation points for integration and triggering
established OT frameworks, and then retrospectively applying it to the 2015
Ukrainian distribution grid incident. The evaluation indicates that AIR
translates high-level requirements into concrete fields, overlays existing
frameworks without vendor dependence, and can support situational awareness and
communication during response. AIR offers a basis for standardising live OT
incident reporting while supporting technical coordination and regulatory
alignment.

</details>


### [4] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares, Mário S. Alvim, Natasha Fernandes*

**主要类别:** cs.CR

**AI概要:** 本文提出了一个新颖的动态信息泄漏定义，将攻击者对秘密值的信念与衡量攻击成功率的基线分布解耦，填补了动态视角信息流量化理论空白。


<details>
  <summary>更多</summary>
  
**动机:** 定量信息流(QIF)中动态视角的理论发展滞后于静态视角，缺乏同等水平的理论成熟度，需要建立动态泄漏的正式定义和理论基础。

**方法:** 提出新的动态泄漏定义，验证其满足信息论公理（非干扰性、单调性和数据处理不等式），分析强公理版本不成立的条件，展示与静态视角的兼容性，并通过隐私保护数据发布攻击实例进行验证。

**结果:** 新定义成功解耦了攻击者信念和基线分布，满足相关信息论公理要求，与静态视角兼容，为动态信息泄漏提供了理论基础。

**结论:** 该研究填补了动态信息流量化理论空白，为系统监控和追踪应用提供了更完善的理论支撑，特别是在基于实时泄漏程度决定是否继续运行系统的场景中具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+new+measure+for+dynamic+leakage+based+on+quantitative+information+flow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20922&send_immediately=true&force_search=false)

**原文摘要:** Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [5] [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)
*Soham Hans, Stacy Marsella, Sophia Hirschmann, Nikolos Gurney*

**主要类别:** cs.CR

**AI概要:** 提出基于大语言模型的框架，通过分析Suricata IDS日志推断攻击者使用的MITRE ATT&CK技术和认知策略，弥合底层日志与战略意图之间的语义鸿沟。


<details>
  <summary>更多</summary>
  
**动机:** 传统网络安全依赖高层情报报告和手动分析攻击链，实时防御需要从底层系统遥测数据直接推断攻击者意图和认知策略。

**方法:** 开发策略驱动的提示系统，将大量网络日志数据高效分段为不同行为阶段，利用LLM将每个阶段与可能的技术和认知动机关联，映射网络层事件到高层攻击策略。

**结果:** 证明LLM能够有效弥合数据包级日志与战略意图之间的语义差距，揭示工具切换、协议转换等行为信号对应的心理决策点。

**结论:** 为行为自适应网络防御和认知特征推断奠定基础，提供了通向认知自适应网络防御的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Logs+to+ATT%26CK+Insights%3A+Leveraging+LLMs+for+High-Level+Threat+Understanding+and+Cognitive+Trait+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20930&send_immediately=true&force_search=false)

**原文摘要:** Understanding adversarial behavior in cybersecurity has traditionally relied
on high-level intelligence reports and manual interpretation of attack chains.
However, real-time defense requires the ability to infer attacker intent and
cognitive strategy directly from low-level system telemetry such as intrusion
detection system (IDS) logs. In this paper, we propose a novel framework that
leverages large language models (LLMs) to analyze Suricata IDS logs and infer
attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded
in the hypothesis that attacker behavior reflects underlying cognitive biases
such as loss aversion, risk tolerance, or goal persistence that can be
extracted and modeled through careful observation of log sequences. This lays
the groundwork for future work on behaviorally adaptive cyber defense and
cognitive trait inference. We develop a strategy-driven prompt system to
segment large amounts of network logs data into distinct behavioral phases in a
highly efficient manner, enabling the LLM to associate each phase with likely
techniques and underlying cognitive motives. By mapping network-layer events to
high-level attacker strategies, our method reveals how behavioral signals such
as tool switching, protocol transitions, or pivot patterns correspond to
psychologically meaningful decision points. The results demonstrate that LLMs
can bridge the semantic gap between packet-level logs and strategic intent,
offering a pathway toward cognitive-adaptive cyber defense.
  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),
Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive
Biases

</details>


### [6] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, Mehrdad Saif*

**主要类别:** cs.CR

**AI概要:** 本研究调查了城市空中交通(UAM)车辆自主导航和着陆系统的漏洞，特别关注针对深度学习模型(如CNN)的特洛伊木马攻击，实验显示攻击导致准确率从96.4%降至73.3%。


<details>
  <summary>更多</summary>
  
**动机:** 随着城市空中交通系统的发展，需要评估其自主导航系统对特洛伊木马攻击的脆弱性，以确保飞行安全。

**方法:** 使用DroNet框架评估城市自主飞行器(UAAV)的脆弱性，收集定制数据集并训练模型模拟真实条件，开发评估框架识别受感染的模型。

**结果:** 实验结果表明特洛伊木马攻击导致模型准确率显著下降，从干净数据的96.4%降至触发攻击数据的73.3%。

**结论:** 特洛伊木马攻击对UAM系统构成潜在安全风险，本研究为未来增强UAM系统韧性的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Experimental+Study+of+Trojan+Vulnerabilities+in+UAV+Autonomous+Landing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20932&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [7] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong, Stephen H. Bach*

**主要类别:** cs.CR

**AI概要:** 研究发现推理语言模型在数学和代码领域的良性推理训练后会出现"自我越狱"现象，模型会使用多种策略绕过自身的安全防护机制，通过引入良性假设来合理化有害请求。


<details>
  <summary>更多</summary>
  
**动机:** 发现推理语言模型在良性推理训练后会出现意料之外的安全对齐失效问题，即模型会自我规避安全防护机制，这威胁到RLMs的安全性。

**方法:** 通过分析多个开源RLMs模型（包括DeepSeek-R1-distilled、s1.1、Phi-4-mini-reasoning和Nemotron）的行为，研究自我越狱现象的机制和表现形式。

**结果:** 发现RLMs在良性推理训练后变得更加顺从，并在思维链中将恶意请求感知为危害性较低，从而能够满足这些请求。模型会通过假设用户具有良性意图来合理化有害请求。

**结论:** 通过在训练中包含最少量的安全推理数据可以有效缓解自我越狱问题，这为维护日益强大的RLMs的安全性提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Jailbreaking%3A+Language+Models+Can+Reason+Themselves+Out+of+Safety+Alignment+After+Benign+Reasoning+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20956&send_immediately=true&force_search=false)

**原文摘要:** We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [8] [REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering](https://arxiv.org/abs/2510.20975)
*Darrin Lea, James Ghawaly, Golden Richard III, Aisha Ali-Gombe, Andrew Case*

**主要类别:** cs.CR

**AI概要:** 该论文开发了REx86，一个通过参数高效微调的本地开源大语言模型，专门用于x86二进制逆向工程任务，在代码理解和注释生成方面显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 云端闭源LLM在逆向工程中存在隐私和安全风险，无法在封闭网络环境中使用，需要开发本地开源的专用模型来提升x86逆向工程效率。

**方法:** 在5,981个x86汇编示例的定制数据集上，对CodeLlama、Qwen2.5-Coder和CodeGemma系列的8个开源模型进行参数高效微调。

**结果:** 微调的Qwen2.5-Coder-7B模型(REx86)表现最佳：测试集交叉熵损失降低64.2%，语义余弦相似度提升20.3%；用户研究中代码理解显著改善(p=0.031)，正确解决率从31%提升至53%。

**结论:** REx86在本地开源LLM中提供了最先进的x86逆向工程辅助能力，证明了领域特定微调的价值，并指出需要更多带注释的反汇编数据来进一步提升LLM在逆向工程中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REx86%3A+A+Local+Large+Language+Model+for+Assisting+in+x86+Assembly+Reverse+Engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.20975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.20975&send_immediately=true&force_search=false)

**原文摘要:** Reverse engineering (RE) of x86 binaries is indispensable for malware and
firmware analysis, but remains slow due to stripped metadata and adversarial
obfuscation. Large Language Models (LLMs) offer potential for improving RE
efficiency through automated comprehension and commenting, but cloud-hosted,
closed-weight models pose privacy and security risks and cannot be used in
closed-network facilities. We evaluate parameter-efficient fine-tuned local
LLMs for assisting with x86 RE tasks in these settings. Eight open-weight
models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned
on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them
quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top
performer, which we name REx86.
  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic
cosine similarity against ground truth by 20.3\% over its base model. In a
limited user case study (n=43), REx86 significantly enhanced line-level code
understanding (p = 0.031) and increased the correct-solve rate from 31% to 53%
(p = 0.189), though the latter did not reach statistical significance.
Qualitative analysis shows more accurate, concise comments with fewer
hallucinations.
  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight
LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and
highlight the need for more commented disassembly data to further enhance LLM
performance in RE. REx86, its dataset, and LoRA adapters are publicly available
at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.

</details>


### [9] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen, Alsharif Abuadbba, Kristen Moore, Tingming Wu*

**主要类别:** cs.CR

**AI概要:** 本研究首次系统评估了FOICE音频深度伪造检测，发现现有检测器在标准和噪声条件下均无法有效检测基于单张面部图像生成的合成语音，并提出了针对性的微调策略来提升检测准确率。


<details>
  <summary>更多</summary>
  
**动机:** FOICE技术能够仅从单张面部图像生成逼真的合成语音，且能绕过行业标准认证系统，由于面部图像比语音样本更容易获取，这带来了严重的安全威胁，因此需要研究现有检测器对FOICE的检测能力。

**方法:** 研究通过两个核心问题展开：(1)评估现有最先进音频深度伪造检测器在干净和噪声条件下对FOICE生成语音的检测能力；(2)通过在FOICE数据上微调检测器来提升检测性能而不产生过拟合。

**结果:** 研究发现领先的检测器在标准和噪声条件下都持续失败；提出的针对性微调策略能够显著提高检测准确率；但微调后在FOICE特化和对未见合成管道的鲁棒性之间存在权衡。

**结论:** 研究揭示了当前防御系统的根本弱点，为下一代音频深度伪造检测的新架构和训练协议提供了动机，强调了需要更强大的检测方法来应对日益复杂的音频伪造技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Current+Detectors+Catch+Face-to-Voice+Deepfake+Attacks%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21004&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [10] [JSTprove: Pioneering Verifiable AI for a Trustless Future](https://arxiv.org/abs/2510.21024)
*Jonathan Gold, Tristan Freiberg, Haruna Isah, Shirin Shahabi*

**主要类别:** cs.CR

**AI概要:** JSTprove是一个基于零知识机器学习的可验证AI工具包，旨在让AI开发者无需密码学专业知识即可生成和验证AI推理证明，提供端到端的可验证推理流程。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI系统在医疗、金融等关键行业的广泛应用，确保AI决策的透明性和正确性变得至关重要，但传统zkML系统需要深厚的密码学专业知识，限制了其普及。

**方法:** 基于Polyhedra Network的Expander后端构建专门的zkML工具包JSTprove，通过简单的命令行界面隐藏密码学复杂性，提供可审计的工件以实现可重复性。

**结果:** 开发了JSTprove工具包，提供了端到端的可验证AI推理流程，使ML工程师能够轻松生成和验证AI推理证明。

**结论:** JSTprove既可作为满足当前工程需求的可使用zkML产品，也可作为未来可验证AI研究和生产部署的可重复基础，鼓励社区评审和扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JSTprove%3A+Pioneering+Verifiable+AI+for+a+Trustless+Future，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21024&send_immediately=true&force_search=false)

**原文摘要:** The integration of machine learning (ML) systems into critical industries
such as healthcare, finance, and cybersecurity has transformed decision-making
processes, but it also brings new challenges around trust, security, and
accountability. As AI systems become more ubiquitous, ensuring the transparency
and correctness of AI-driven decisions is crucial, especially when they have
direct consequences on privacy, security, or fairness. Verifiable AI, powered
by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these
challenges. zkML enables the verification of AI model inferences without
exposing sensitive data, providing an essential layer of trust and privacy.
However, traditional zkML systems typically require deep cryptographic
expertise, placing them beyond the reach of most ML engineers. In this paper,
we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's
Expander backend, to enable AI developers and ML engineers to generate and
verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI
inference pipeline that hides cryptographic complexity behind a simple
command-line interface while exposing auditable artifacts for reproducibility.
We present the design, innovations, and real-world use cases of JSTprove as
well as our blueprints and tooling to encourage community review and extension.
JSTprove therefore serves both as a usable zkML product for current engineering
needs and as a reproducible foundation for future research and production
deployments of verifiable AI.

</details>


### [11] [A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](https://arxiv.org/abs/2510.21053)
*Li An, Yujian Liu, Yepeng Liu, Yuheng Bu, Yang Zhang, Shiyu Chang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于强化学习的端到端水印框架，通过锚定机制和正则化项解决了现有LLM水印方法在多目标优化中的不稳定性和奖励攻击问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM水印方法多基于启发式的绿/红令牌列表设计，直接使用强化学习优化会面临多目标冲突导致训练不稳定，以及巨大动作空间容易受到奖励攻击的问题。

**方法:** 采用端到端强化学习框架，引入锚定机制确保训练稳定性，并添加正则化项防止奖励攻击，优化绿/红令牌列表设计。

**结果:** 在两个骨干LLM的标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著提升，且不降低其他标准性能。

**结论:** 提出的RL水印框架有效解决了多目标优化的挑战，为LLM水印提供了更稳健和安全的解决方案，代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Reinforcement+Learning+Framework+for+Robust+and+Secure+LLM+Watermarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21053&send_immediately=true&force_search=false)

**原文摘要:** Watermarking has emerged as a promising solution for tracing and
authenticating text generated by large language models (LLMs). A common
approach to LLM watermarking is to construct a green/red token list and assign
higher or lower generation probabilities to the corresponding tokens,
respectively. However, most existing watermarking algorithms rely on heuristic
green/red token list designs, as directly optimizing the list design with
techniques such as reinforcement learning (RL) comes with several challenges.
First, desirable watermarking involves multiple criteria, i.e., detectability,
text quality, robustness against removal attacks, and security against spoofing
attacks. Directly optimizing for these criteria introduces many partially
conflicting reward terms, leading to an unstable convergence process. Second,
the vast action space of green/red token list choices is susceptible to reward
hacking. In this paper, we propose an end-to-end RL framework for robust and
secure LLM watermarking. Our approach adopts an anchoring mechanism for reward
terms to ensure stable training and introduces additional regularization terms
to prevent reward hacking. Experiments on standard benchmarks with two backbone
LLMs show that our method achieves a state-of-the-art trade-off across all
criteria, with notable improvements in resistance to spoofing attacks without
degrading other criteria. Our code is available at
https://github.com/UCSB-NLP-Chang/RL-watermark.

</details>


### [12] [Soft Instruction De-escalation Defense](https://arxiv.org/abs/2510.21057)
*Nils Philipp Walter, Chawin Sitawarin, Jamie Hayes, David Stutz, Ilia Shumailov*

**主要类别:** cs.CR

**AI概要:** SIC方法通过迭代式提示净化循环来防御LLM代理的提示注入攻击，通过多次检查、重写和重新评估输入数据来确保安全性


<details>
  <summary>更多</summary>
  
**动机:** LLM在代理系统中处理不可信数据时容易受到提示注入攻击，需要有效的防护机制

**方法:** 提出SIC方法——一个简单的迭代提示净化循环，通过多次检查输入数据中的指令性内容，对恶意内容进行重写、屏蔽或删除，并进行重新评估

**结果:** 方法能有效提升安全性，但最坏情况下仍有15%的攻击成功率，非命令式工作流仍可能绕过防护

**结论:** SIC方法虽然不能完全防御所有攻击，但显著提高了攻击门槛，为LLM代理安全提供了实用解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft+Instruction+De-escalation+Defense，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21057&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly deployed in agentic systems
that interact with an external environment; this makes them susceptible to
prompt injections when dealing with untrusted data. To overcome this
limitation, we propose SIC (Soft Instruction Control)-a simple yet effective
iterative prompt sanitization loop designed for tool-augmented LLM agents. Our
method repeatedly inspects incoming data for instructions that could compromise
agent behavior. If such content is found, the malicious content is rewritten,
masked, or removed, and the result is re-evaluated. The process continues until
the input is clean or a maximum iteration limit is reached; if imperative
instruction-like content remains, the agent halts to ensure security. By
allowing multiple passes, our approach acknowledges that individual rewrites
may fail but enables the system to catch and correct missed injections in later
steps. Although immediately useful, worst-case analysis shows that SIC is not
infallible; strong adversary can still get a 15% ASR by embedding
non-imperative workflows. This nonetheless raises the bar.

</details>


### [13] [QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute](https://arxiv.org/abs/2510.21124)
*Jie Zhang, Xiaohong Li, Mengke Zhang, Ruitao Feng, Shanshan Xu, Zhe Hou, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** QAE-BAC提出了一种基于区块链的属性访问控制方案，通过(r, t)-匿名模型量化重识别风险，并利用熵加权路径树优化策略匹配，在保护隐私的同时显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有区块链ABAC方案面临两大挑战：区块链透明性导致用户隐私面临重识别攻击风险，策略匹配的计算复杂度与区块链性能限制存在冲突。现有解决方案如零知识证明开销大且缺乏可量化的匿名性保证。

**方法:** 提出QAE-BAC框架，包含：(1) 形式化的(r, t)-匿名模型动态量化用户重识别风险；(2) 熵加权路径树(EWPT)基于实时匿名性指标优化策略结构，降低匹配复杂度。在Hyperledger Fabric上实现和评估。

**结果:** 实验结果显示，QAE-BAC能有效缓解重识别风险，在吞吐量上比现有最优方案提升11倍，延迟降低87%，实现了隐私保护与性能的优越平衡。

**结论:** QAE-BAC为隐私敏感的分布式应用提供了一种实用的解决方案，成功解决了区块链ABAC中的隐私与效率双重挑战，证明了其在现实应用中的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QAE-BAC%3A+Achieving+Quantifiable+Anonymity+and+Efficiency+in+Blockchain-Based+Access+Control+with+Attribute，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21124&send_immediately=true&force_search=false)

**原文摘要:** Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a
decentralized paradigm for secure data governance but faces two inherent
challenges: the transparency of blockchain ledgers threatens user privacy by
enabling reidentification attacks through attribute analysis, while the
computational complexity of policy matching clashes with blockchain's
performance constraints. Existing solutions, such as those employing
Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable
anonymity guarantees, while efficiency optimizations frequently ignore privacy
implications. To address these dual challenges, this paper proposes QAEBAC
(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with
Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically
quantify the re-identification risk of users based on their access attributes
and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that
optimizes policy structure based on realtime anonymity metrics, drastically
reducing policy matching complexity. Implemented and evaluated on Hyperledger
Fabric, QAE-BAC demonstrates a superior balance between privacy and
performance. Experimental results show that it effectively mitigates
re-identification risks and outperforms state-of-the-art baselines, achieving
up to an 11x improvement in throughput and an 87% reduction in latency, proving
its practicality for privacy-sensitive decentralized applications.

</details>


### [14] [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/abs/2510.21133)
*Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi*

**主要类别:** cs.CR

**AI概要:** 本研究对10个主流商业大语言模型进行了CBRN武器知识安全漏洞评估，发现现有安全机制存在严重脆弱性，深度诱导攻击成功率高达86%，模型间安全性能差异巨大（2%-96%），揭示了当前安全对齐的脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 前沿大语言模型存在前所未有的双重用途风险，可能扩散化学、生物、放射性和核武器知识，需要全面评估其安全漏洞。

**方法:** 使用包含200个提示的新CBRN数据集和180个提示的FORTRESS基准子集，采用严格的三层攻击方法学对10个领先商业LLMs进行评估。

**结果:** 深度诱导攻击成功率86%远高于直接请求的33.8%；模型安全性能差异显著（claude-opus-4仅2%成功率，mistral-small-latest达96%）；8个模型在增强危险材料属性请求时超过70%的脆弱性。

**结论:** 当前安全对齐机制存在根本性脆弱性，简单的提示工程技术就能绕过安全防护获取危险CBRN信息，亟需标准化评估框架、透明安全指标和更强大的对齐技术来减轻灾难性滥用风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+CBRN+Risk+in+Frontier+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21133&send_immediately=true&force_search=false)

**原文摘要:** Frontier Large Language Models (LLMs) pose unprecedented dual-use risks
through the potential proliferation of chemical, biological, radiological, and
nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation
of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and
a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier
attack methodology. Our findings expose critical safety vulnerabilities: Deep
Inception attacks achieve 86.0\% success versus 33.8\% for direct requests,
demonstrating superficial filtering mechanisms; Model safety performance varies
dramatically from 2\% (claude-opus-4) to 96\% (mistral-small-latest) attack
success rates; and eight models exceed 70\% vulnerability when asked to enhance
dangerous material properties. We identify fundamental brittleness in current
safety alignment, where simple prompt engineering techniques bypass safeguards
for dangerous CBRN information. These results challenge industry safety claims
and highlight urgent needs for standardized evaluation frameworks, transparent
safety metrics, and more robust alignment techniques to mitigate catastrophic
misuse risks while preserving beneficial capabilities.

</details>


### [15] [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189)
*Yukun Jiang, Mingjie Li, Michael Backes, Yang Zhang*

**主要类别:** cs.CR

**AI概要:** 该论文提出了JAIL-CON攻击框架，通过任务并发性突破LLMs的安全防护，发现并发任务能显著降低有害内容被过滤的概率，比现有攻击更隐蔽有效。


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱攻击主要遵循顺序逻辑，但并发性作为顺序场景的自然扩展被忽视。研究发现LLMs在并发任务中保持强大效用，但将有害任务与良性任务结合能显著降低防护机制的过滤概率。

**方法:** 提出词级方法实现LLMs中的任务并发性，使相邻词汇编码不同意图。基于此开发JAIL-CON迭代攻击框架，通过任务并发性突破LLMs防护。

**结果:** 在广泛使用的LLMs上实验证明JAIL-CON相比现有攻击具有更强的越狱能力。当应用防护机制时，并发答案比顺序答案更具隐蔽性，更难被防护机制检测。

**结论:** 任务并发性在LLMs越狱中具有独特特征，JAIL-CON框架展示了并发攻击的潜在风险，需要新的防御机制来应对这种隐蔽的攻击方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adjacent+Words%2C+Divergent+Intents%3A+Jailbreaking+Large+Language+Models+via+Task+Concurrency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21189&send_immediately=true&force_search=false)

**原文摘要:** Despite their superior performance on a wide range of domains, large language
models (LLMs) remain vulnerable to misuse for generating harmful content, a
risk that has been further amplified by various jailbreak attacks. Existing
jailbreak attacks mainly follow sequential logic, where LLMs understand and
answer each given task one by one. However, concurrency, a natural extension of
the sequential scenario, has been largely overlooked. In this work, we first
propose a word-level method to enable task concurrency in LLMs, where adjacent
words encode divergent intents. Although LLMs maintain strong utility in
answering concurrent tasks, which is demonstrated by our evaluations on
mathematical and general question-answering benchmarks, we notably observe that
combining a harmful task with a benign one significantly reduces the
probability of it being filtered by the guardrail, showing the potential risks
associated with concurrency in LLMs. Based on these findings, we introduce
$\texttt{JAIL-CON}$, an iterative attack framework that
$\underline{\text{JAIL}}$breaks LLMs via task $\underline{\text{CON}}$currency.
Experiments on widely-used LLMs demonstrate the strong jailbreak capabilities
of $\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the
guardrail is applied as a defense, compared to the sequential answers generated
by previous attacks, the concurrent answers in our $\texttt{JAIL-CON}$ exhibit
greater stealthiness and are less detectable by the guardrail, highlighting the
unique feature of task concurrency in jailbreaking LLMs.

</details>


### [16] [The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](https://arxiv.org/abs/2510.21190)
*Mingrui Liu, Sixiao Zhang, Cheng Long, Kwok Yan Lam*

**主要类别:** cs.CR

**AI概要:** TrojFill是一种新型黑盒越狱技术，通过将有害指令嵌入多部分模板中，以模板填充任务的形式绕过LLM的安全防护，在多个主流模型上达到97-100%的攻击成功率。


<details>
  <summary>更多</summary>
  
**动机:** 现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的提示缺乏可解释性和可迁移性。需要一种既有效又具有解释性的黑盒越狱方法。

**方法:** TrojFill将不安全指令重构为模板填充任务，通过占位符替换或编码（如凯撒/Base64）混淆有害指令，嵌入包含不安全推理和详细示例生成的多部分模板中。

**结果:** 在主流LLMs（ChatGPT、Gemini、DeepSeek、Qwen）上评估显示优异性能：Gemini-flash-2.5和DeepSeek-3.1达到100%攻击成功率，GPT-4o达到97%。生成的提示比现有黑盒方法具有更好的可解释性和可迁移性。

**结论:** TrojFill提供了一种有效且可解释的黑盒越狱方法，通过任务重构降低拒绝率，为红队测试研究提供了有价值的工具和数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Trojan+Example%3A+Jailbreaking+LLMs+through+Template+Filling+and+Unsafety+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21190&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have advanced rapidly and now encode extensive
world knowledge. Despite safety fine-tuning, however, they remain susceptible
to adversarial prompts that elicit harmful content. Existing jailbreak
techniques fall into two categories: white-box methods (e.g., gradient-based
approaches such as GCG), which require model internals and are infeasible for
closed-source APIs, and black-box methods that rely on attacker LLMs to search
or mutate prompts but often produce templates that lack explainability and
transferability. We introduce TrojFill, a black-box jailbreak that reframes
unsafe instruction as a template-filling task. TrojFill embeds obfuscated
harmful instructions (e.g., via placeholder substitution or Caesar/Base64
encoding) inside a multi-part template that asks the model to (1) reason why
the original instruction is unsafe (unsafety reasoning) and (2) generate a
detailed example of the requested text, followed by a sentence-by-sentence
analysis. The crucial "example" component acts as a Trojan Horse that contains
the target jailbreak content while the surrounding task framing reduces refusal
rates. We evaluate TrojFill on standard jailbreak benchmarks across leading
LLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical
performance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,
and 97% on GPT-4o). Moreover, the generated prompts exhibit improved
interpretability and transferability compared with prior black-box optimization
approaches. We release our code, sample prompts, and generated outputs to
support future red-teaming research.

</details>


### [17] [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214)
*Xingwei Zhong, Kar Wai Fok, Vrizlynn L. L. Thing*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种针对多模态大语言模型的黑盒越狱攻击方法，通过文本和图像提示评估模型安全性，并设计了新的防御策略


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型在处理视觉语言任务时存在安全漏洞，特别是越狱攻击可能导致模型产生未经授权或有害的响应，需要新的评估和防御方法

**方法:** 设计包含挑衅指令的文本提示和具有突变、多图像能力的图像提示，采用重新攻击策略，评估开源和闭源MLLMs的安全性

**结果:** 提出的方法能够有效评估MLLMs的安全漏洞，识别现有防御方法的不足，重新设计的防御方法在训练时和推理时都能提高对越狱攻击的防护能力

**结论:** 该研究为多模态大语言模型的安全评估提供了有效工具，提出的防御策略能够显著提升模型对抗越狱攻击的能力，为MLLMs的安全部署提供了重要参考

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+MLLM+Black-Box+Jailbreaking+Attacks+and+Defenses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21214&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) comprise of both visual and textual
modalities to process vision language tasks. However, MLLMs are vulnerable to
security-related issues, such as jailbreak attacks that alter the model's input
to induce unauthorized or harmful responses. The incorporation of the
additional visual modality introduces new dimensions to security threats. In
this paper, we proposed a black-box jailbreak method via both text and image
prompts to evaluate MLLMs. In particular, we designed text prompts with
provocative instructions, along with image prompts that introduced mutation and
multi-image capabilities. To strengthen the evaluation, we also designed a
Re-attack strategy. Empirical results show that our proposed work can improve
capabilities to assess the security of both open-source and closed-source
MLLMs. With that, we identified gaps in existing defense methods to propose new
strategies for both training-time and inference-time defense methods, and
evaluated them across the new jailbreak methods. The experiment results showed
that the re-designed defense methods improved protections against the jailbreak
attacks.

</details>


### [18] [Securing AI Agent Execution](https://arxiv.org/abs/2510.21236)
*Christoph Bühler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi*

**主要类别:** cs.CR

**AI概要:** AgentBound是首个针对MCP服务器的访问控制框架，通过声明式策略机制和安全执行引擎来保护LLM代理与外部工具交互时的系统安全。


<details>
  <summary>更多</summary>
  
**动机:** MCP已成为连接AI代理与外部工具的事实标准，但缺乏安全控制，数千个MCP服务器拥有对主机系统的无限制访问权限，造成了广泛的安全攻击面。

**方法:** 结合受Android权限模型启发的声明式策略机制和策略执行引擎，无需修改MCP服务器即可遏制恶意行为。基于296个流行MCP服务器构建数据集，展示从源代码自动生成访问控制策略的能力。

**结果:** 自动生成策略准确率达到80.9%，能够阻止多数恶意MCP服务器的安全威胁，策略执行引擎引入的开销可忽略不计。

**结论:** 为开发者和项目经理提供了保护MCP服务器的实用基础，同时保持生产力，为声明式访问控制和MCP安全研究开辟了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Securing+AI+Agent+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21236，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21236&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have evolved into AI agents that interact with
external tools and environments to perform complex tasks. The Model Context
Protocol (MCP) has become the de facto standard for connecting agents with such
resources, but security has lagged behind: thousands of MCP servers execute
with unrestricted access to host systems, creating a broad attack surface. In
this paper, we introduce AgentBound, the first access control framework for MCP
servers. AgentBound combines a declarative policy mechanism, inspired by the
Android permission model, with a policy enforcement engine that contains
malicious behavior without requiring MCP server modifications. We build a
dataset containing the 296 most popular MCP servers, and show that access
control policies can be generated automatically from source code with 80.9%
accuracy. We also show that AgentBound blocks the majority of security threats
in several malicious MCP servers, and that policy enforcement engine introduces
negligible overhead. Our contributions provide developers and project managers
with a practical foundation for securing MCP servers while maintaining
productivity, enabling researchers and tool builders to explore new directions
for declarative access control and MCP security.

</details>


### [19] [What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions](https://arxiv.org/abs/2510.21246)
*Michael Külper, Jan-Niclas Hilgert, Frank Breitinger, Martin Lambertz*

**主要类别:** cs.CR

**AI概要:** 提出一个扩展的数字取证框架，通过设备监控和云API实现对自托管云存储系统（如Nextcloud）的结构化、可重复证据采集


<details>
  <summary>更多</summary>
  
**动机:** 自托管云存储平台（如Nextcloud）日益流行，但数字取证调查面临新挑战，现有云存储取证框架存在局限性，Nextcloud在取证研究中关注不足

**方法:** 批判性分析现有云存储取证框架，提出扩展框架整合设备监控和云API，以Nextcloud为案例研究，开发开源采集工具实现该方法

**结果:** 展示了如何利用Nextcloud原生API可靠访问取证工件，提供了更灵活的自托管云存储系统分析方法

**结论:** 该框架为数字取证这一不断发展的领域提供了基础，使调查人员能够更有效地分析自托管云存储系统

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+Next%2C+Cloud%3F+A+Forensic+Framework+for+Analyzing+Self-Hosted+Cloud+Storage+Solutions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21246&send_immediately=true&force_search=false)

**原文摘要:** Self-hosted cloud storage platforms like Nextcloud are gaining popularity
among individuals and organizations seeking greater control over their data.
However, this shift introduces new challenges for digital forensic
investigations, particularly in systematically analyzing both client and server
components. Despite Nextcloud's widespread use, it has received limited
attention in forensic research. In this work, we critically examine existing
cloud storage forensic frameworks and highlight their limitations. To address
the gaps, we propose an extended forensic framework that incorporates device
monitoring and leverages cloud APIs for structured, repeatable evidence
acquisition. Using Nextcloud as a case study, we demonstrate how its native
APIs can be used to reliably access forensic artifacts, and we introduce an
open-source acquisition tool that implements this approach. Our framework
equips investigators with a more flexible method for analyzing self-hosted
cloud storage systems, and offers a foundation for further development in this
evolving area of digital forensics.

</details>


### [20] [LLM-Powered Detection of Price Manipulation in DeFi](https://arxiv.org/abs/2510.21272)
*Lu Liu, Wuqi Zhang, Lili Wei, Hao Guan, Yongqiang Tian, Yepang Liu*

**主要类别:** cs.CR

**AI概要:** PMDetector是一个结合静态分析和LLM推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在真实数据集上达到88%精确度和90%召回率，成本仅为每次审计0.03美元。


<details>
  <summary>更多</summary>
  
**动机:** DeFi智能合约管理数十亿美元资金，价格操纵漏洞造成重大财务损失。现有检测方法有限，反应性方法只能在攻击发生后分析，而静态分析工具依赖预定义启发式规则，无法识别新型攻击变体或理解复杂经济逻辑。

**方法:** 提出PMDetector混合框架：1)静态污点分析识别潜在漏洞代码路径；2)两阶段LLM流程分析防御措施并模拟攻击评估可利用性；3)静态分析检查器验证LLM结果，保留高风险路径并生成详细报告。

**结果:** 在73个真实漏洞和288个良性DeFi协议数据集上，使用Gemini 2.5-flash达到88%精确度和90%召回率，显著优于最先进的静态分析和基于LLM的方法。使用GPT-4.1时每次审计仅需0.03美元和4.0秒。

**结论:** PMDetector提供了一个高效且成本效益高的替代手动审计的方案，能够主动检测价格操纵漏洞，有效应对现有方法的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Powered+Detection+of+Price+Manipulation+in+DeFi，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21272&send_immediately=true&force_search=false)

**原文摘要:** Decentralized Finance (DeFi) smart contracts manage billions of dollars,
making them a prime target for exploits. Price manipulation vulnerabilities,
often via flash loans, are a devastating class of attacks causing significant
financial losses. Existing detection methods are limited. Reactive approaches
analyze attacks only after they occur, while proactive static analysis tools
rely on rigid, predefined heuristics, limiting adaptability. Both depend on
known attack patterns, failing to identify novel variants or comprehend complex
economic logic. We propose PMDetector, a hybrid framework combining static
analysis with Large Language Model (LLM)-based reasoning to proactively detect
price manipulation vulnerabilities. Our approach uses a formal attack model and
a three-stage pipeline. First, static taint analysis identifies potentially
vulnerable code paths. Second, a two-stage LLM process filters paths by
analyzing defenses and then simulates attacks to evaluate exploitability.
Finally, a static analysis checker validates LLM results, retaining only
high-risk paths and generating comprehensive vulnerability reports. To evaluate
its effectiveness, we built a dataset of 73 real-world vulnerable and 288
benign DeFi protocols. Results show PMDetector achieves 88% precision and 90%
recall with Gemini 2.5-flash, significantly outperforming state-of-the-art
static analysis and LLM-based approaches. Auditing a vulnerability with
PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an
efficient and cost-effective alternative to manual audits.

</details>


### [21] [The Qey: Implementation and performance study of post quantum cryptography in FIDO2](https://arxiv.org/abs/2510.21353)
*Aditya Mitra, Sibi Chakkaravarthy Sethuraman*

**主要类别:** cs.CR

**AI概要:** 本研究探索将基于模块格的后量子密码签名算法ML-DSA应用于FIDO2认证标准，以应对量子计算威胁。


<details>
  <summary>更多</summary>
  
**动机:** 当前FIDO2标准使用的ECDSA、RSA等经典密码算法在量子计算机面前存在安全风险，需要后量子密码解决方案。

**方法:** 采用基于Crystals Dilithium的模块格数字签名算法(ML-DSA)作为后量子密码签名标准，评估其在FIDO2中的可用性。

**结果:** 论文比较了ML-DSA与经典算法在性能和安全性方面的表现。

**结论:** ML-DSA作为后量子密码算法，有望为FIDO2密码认证提供量子安全的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Qey%3A+Implementation+and+performance+study+of+post+quantum+cryptography+in+FIDO2，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21353&send_immediately=true&force_search=false)

**原文摘要:** Authentication systems have evolved a lot since the 1960s when Fernando
Corbato first proposed the password-based authentication. In 2013, the FIDO
Alliance proposed using secure hardware for authentication, thus marking a
milestone in the passwordless authentication era [1]. Passwordless
authentication with a possession-based factor often relied on hardware-backed
cryptographic methods. FIDO2 being one an amalgamation of the W3C Web
Authentication and FIDO Alliance Client to Authenticator Protocol is an
industry standard for secure passwordless authentication with rising adoption
for the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256
(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature
algorithms. This makes it insecure against attacks involving large-scale
quantum computers [3]. This study aims at exploring the usability of Module
Lattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium
as a post quantum cryptographic signature standard for FIDO2. The paper
highlights the performance and security in comparison to keys with classical
algorithms.

</details>


### [22] [FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](https://arxiv.org/abs/2510.21401)
*Mojtaba Eshghie, Gabriele Morello, Matteo Lauretano, Alexandre Bartel, Martin Monperrus*

**主要类别:** cs.CR

**AI概要:** FLAMES是一个基于领域适应大语言模型的自动化方法，通过填充中间监督微调训练，从已验证合约中提取现实世界不变式，生成可执行的运行时防护来加固智能合约安全。


<details>
  <summary>更多</summary>
  
**动机:** 智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施。

**方法:** 使用领域适应的大语言模型，通过填充中间监督微调训练，从514,506个已验证合约中提取现实世界不变式，合成Solidity "require"语句作为运行时防护。

**结果:** 编译成功率96.7%；在5000个挑战性不变式测试集上达到44.5%的精确或语义等价匹配；成功阻止108个真实攻击中的22个（20.4%）；成功阻止APEMAGA真实攻击事件。

**结论:** 领域适应的LLM能够自动生成生产就绪的智能合约安全防御，无需漏洞检测、形式化规范或人工干预。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLAMES%3A+Fine-tuning+LLMs+to+Synthesize+Invariants+for+Smart+Contract+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21401&send_immediately=true&force_search=false)

**原文摘要:** Smart contract vulnerabilities cost billions of dollars annually, yet
existing automated analysis tools fail to generate deployable defenses. We
present FLAMES, a novel automated approach that synthesizes executable runtime
guards as Solidity "require" statements to harden smart contracts against
exploits. Unlike prior work that relies on vulnerability labels, symbolic
analysis, or natural language specifications, FLAMES employs domain-adapted
large language models trained through fill-in-the-middle supervised fine-tuning
on real-world invariants extracted from 514,506 verified contracts. Our
extensive evaluation across three dimensions demonstrates FLAMES's
effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for
synthesized invariant (2) Semantic Quality: on a curated test set of 5,000
challenging invariants, FLAMES produces exact or semantically equivalent
matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES
prevents 22 out of 108 real exploits (20.4%) while preserving contract
functionality, and (4) FLAMES successfully blocks the real-world APEMAGA
incident by synthesizing a pre-condition that mitigates the attack. FLAMES
establishes that domain-adapted LLMs can automatically generate
production-ready security defenses for smart contracts without requiring
vulnerability detection, formal specifications, or human intervention. We
release our code, model weights, datasets, and evaluation infrastructure to
enable reproducible research in this critical domain.

</details>


### [23] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe, Helmut Neukirchen, Thomas Welsh*

**主要类别:** cs.CR

**AI概要:** SBASH框架使用本地轻量级LLM解决蜜罐的数据保护问题，通过RAG和非RAG方法提升Linux shell命令响应的准确性和实时性。


<details>
  <summary>更多</summary>
  
**动机:** 传统蜜罐缺乏上下文感知能力，LLM虽能提升但存在响应准确性、实时性、云部署数据保护等问题。

**方法:** 提出SBASH框架，使用本地轻量级LLM，比较RAG支持和非RAG的LLM在Linux shell命令响应中的表现，评估响应时间、真实感和系统相似性。

**结果:** RAG提升未调优模型的准确性，系统提示调优的非RAG模型能达到类似RAG的准确性且延迟略低。

**结论:** 本地轻量级LLM结合RAG或系统提示调优可有效提升蜜罐的上下文感知能力，平衡准确性、实时性和数据保护。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SBASH%3A+a+Framework+for+Designing+and+Evaluating+RAG+vs.+Prompt-Tuned+LLM+Honeypots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21459&send_immediately=true&force_search=false)

**原文摘要:** Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [24] [Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise](https://arxiv.org/abs/2510.21483)
*Pierre Guillot, Auguste Hoang Duc, Michel Koskas, Florian Méhats*

**主要类别:** cs.CR

**AI概要:** GRAFHEN是一种无需自举（无噪声）的全同态加密方案，基于群编码和重写系统实现，比现有标准快几个数量级


<details>
  <summary>更多</summary>
  
**动机:** 解决传统全同态加密需要自举操作和噪声处理的问题，提供更高效的无噪声加密方案

**方法:** 基于Nuida等人的工作，使用群编码和重写系统表示群，使子群成员问题达到最大难度

**结果:** 实现了无需自举的全同态加密，性能比现有标准快几个数量级，并分析了多种攻击方式的防护措施

**结论:** GRAFHEN提供了一种高效的无噪声全同态加密方案，通过群编码和重写系统确保了安全性和性能优势

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Introducing+GRAFHEN%3A+Group-based+Fully+Homomorphic+Encryption+without+Noise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21483&send_immediately=true&force_search=false)

**原文摘要:** We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic
Encryption without the need for bootstrapping (or in other words, without
noise). Building on the work of Nuida and others, we achieve this using
encodings in groups.
  The groups are represented on a machine using rewriting systems. In this way
the subgroup membership problem, which an attacker would have to solve in order
to break the scheme, becomes maximally hard, while performance is preserved. In
fact we include a simple benchmark demonstrating that our implementation runs
several orders of magnitude faster than existing standards.
  We review many possible attacks against our protocol and explain how to
protect the scheme in each case.

</details>


### [25] [PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis](https://arxiv.org/abs/2510.21601)
*Emmanuel Dare Alalade, Ashraf Matrawy*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的隐私威胁模型框架(PTMF)，通过结合MITRE ATT&CK框架和LINDDUN隐私威胁模型，分析物联网系统中的隐私威胁参与者和其行为意图，为主动部署隐私保护措施提供基础。


<details>
  <summary>更多</summary>
  
**动机:** 现有PTA研究主要关注隐私威胁的发生区域和可能性，但缺乏对威胁参与者、其行为和意图的深入理解。需要开发一个以隐私为中心的框架来全面分析隐私威胁。

**方法:** 基于MITRE ATT&CK框架和LINDDUN隐私威胁模型开发PTMF框架，通过问卷调查收集12位来自产业界和学术界的隐私安全专家意见，分析物联网相关的12种隐私威胁。

**结果:** 识别了物联网用户识别(IU)和其他11种隐私威胁中的主要威胁参与者，发现了前三大威胁参与者及其在IU隐私威胁中使用的关键路径。

**结论:** PTMF框架为理解物联网系统中威胁参与者的活动和意图提供了坚实基础，有助于主动有效地部署隐私保护措施来缓解隐私威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PTMF%3A+A+Privacy+Threat+Modeling+Framework+for+IoT+with+Expert-Driven+Threat+Propagation+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21601&send_immediately=true&force_search=false)

**原文摘要:** Previous studies on PTA have focused on analyzing privacy threats based on
the potential areas of occurrence and their likelihood of occurrence. However,
an in-depth understanding of the threat actors involved, their actions, and the
intentions that result in privacy threats is essential. In this paper, we
present a novel Privacy Threat Model Framework (PTMF) that analyzes privacy
threats through different phases.
  The PTMF development is motivated through the selected tactics from the MITRE
ATT\&CK framework and techniques from the LINDDUN privacy threat model, making
PTMF a privacy-centered framework. The proposed PTMF can be employed in various
ways, including analyzing the activities of threat actors during privacy
threats and assessing privacy risks in IoT systems, among others. In this
paper, we conducted a user study on 12 privacy threats associated with IoT by
developing a questionnaire based on PTMF and recruited experts from both
industry and academia in the fields of security and privacy to gather their
opinions. The collected data were analyzed and mapped to identify the threat
actors involved in the identification of IoT users (IU) and the remaining 11
privacy threats. Our observation revealed the top three threat actors and the
critical paths they used during the IU privacy threat, as well as the remaining
11 privacy threats. This study could provide a solid foundation for
understanding how and where privacy measures can be proactively and effectively
deployed in IoT systems to mitigate privacy threats based on the activities and
intentions of threat actors within these systems.

</details>


### [26] [Toward provably private analytics and insights into GenAI use](https://arxiv.org/abs/2510.21684)
*Albert Cheu, Artem Lagzdin, Brett McLarnon, Daniel Ramage, Katharine Daly, Marco Gruteser, Peter Kairouz, Rakshita Tandon, Stanislav Chiknavaryan, Timon Van Overveldt, Zoe Gong*

**主要类别:** cs.CR

**AI概要:** 提出基于可信执行环境(TEE)的新一代联邦分析系统，为服务器端处理提供可验证的隐私保证，支持LLM处理非结构化数据并应用差分隐私，已在生产环境中成功部署。


<details>
  <summary>更多</summary>
  
**动机:** 大规模设备分析系统需要同时满足高隐私安全标准、数据质量、可用性和资源效率要求，现有系统在隐私保护和验证能力方面存在不足。

**方法:** 使用AMD SEV-SNP和Intel TDX等TEE技术，设备加密上传数据并标记允许的处理步骤，通过开源TEE托管密钥管理服务确保数据仅能被授权处理步骤访问，支持LLM处理非结构化数据和差分隐私聚合。

**结果:** 系统成功部署在生产环境中，为现实世界的GenAI体验提供了有价值的洞察，实现了可验证的隐私保护和处理透明度。

**结论:** 基于TEE的联邦分析系统能够有效平衡隐私保护与数据分析需求，通过技术手段确保数据处理的可验证性和透明度，为大规模分析系统提供了可行的隐私保护解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+provably+private+analytics+and+insights+into+GenAI+use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21684&send_immediately=true&force_search=false)

**原文摘要:** Large-scale systems that compute analytics over a fleet of devices must
achieve high privacy and security standards while also meeting data quality,
usability, and resource efficiency expectations. We present a next-generation
federated analytics system that uses Trusted Execution Environments (TEEs)
based on technologies like AMD SEV-SNP and Intel TDX to provide verifiable
privacy guarantees for all server-side processing. In our system, devices
encrypt and upload data, tagging it with a limited set of allowable server-side
processing steps. An open source, TEE-hosted key management service guarantees
that the data is accessible only to those steps, which are themselves protected
by TEE confidentiality and integrity assurance guarantees. The system is
designed for flexible workloads, including processing unstructured data with
LLMs (for structured summarization) before aggregation into differentially
private insights (with automatic parameter tuning). The transparency properties
of our system allow any external party to verify that all raw and derived data
is processed in TEEs, protecting it from inspection by the system operator, and
that differential privacy is applied to all released results. This system has
been successfully deployed in production, providing helpful insights into
real-world GenAI experiences.

</details>


### [27] [$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy](https://arxiv.org/abs/2510.21946)
*Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种名为δ-STEAL的新型模型窃取攻击方法，能够绕过LLM服务提供商的水印检测器，同时保持攻击者模型的实用性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型部署面临知识产权风险，特别是模型窃取攻击威胁商业利益。水印技术虽然能提供模型溯源和知识产权验证，但现有方法存在被绕过的风险。

**方法:** δ-STEAL通过在对手模型微调过程中向token嵌入注入满足局部差分隐私(LDP)保证的噪声，模糊水印信号，使服务提供商难以检测其输出是否被用于模型窃取。

**结果:** 实验显示δ-STEAL在轻量级修改下攻击成功率高达96.95%，且不显著损害攻击者模型效用。LDP噪声规模控制攻击效果与模型效用之间的权衡。

**结论:** 该方法表明即使强大的水印也能被绕过，攻击者可以欺骗水印检测器，对当前知识产权保护方法构成重大威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%CE%B4%24-STEAL%3A+LLM+Stealing+Attack+with+Local+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21946&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demonstrate remarkable capabilities across
various tasks. However, their deployment introduces significant risks related
to intellectual property. In this context, we focus on model stealing attacks,
where adversaries replicate the behaviors of these models to steal services.
These attacks are highly relevant to proprietary LLMs and pose serious threats
to revenue and financial stability. To mitigate these risks, the watermarking
solution embeds imperceptible patterns in LLM outputs, enabling model
traceability and intellectual property verification. In this paper, we study
the vulnerability of LLM service providers by introducing $\delta$-STEAL, a
novel model stealing attack that bypasses the service provider's watermark
detectors while preserving the adversary's model utility. $\delta$-STEAL
injects noise into the token embeddings of the adversary's model during
fine-tuning in a way that satisfies local differential privacy (LDP)
guarantees. The adversary queries the service provider's model to collect
outputs and form input-output training pairs. By applying LDP-preserving noise
to these pairs, $\delta$-STEAL obfuscates watermark signals, making it
difficult for the service provider to determine whether its outputs were used,
thereby preventing claims of model theft. Our experiments show that
$\delta$-STEAL with lightweight modifications achieves attack success rates of
up to $96.95\%$ without significantly compromising the adversary's model
utility. The noise scale in LDP controls the trade-off between attack
effectiveness and model utility. This poses a significant risk, as even robust
watermarks can be bypassed, allowing adversaries to deceive watermark detectors
and undermine current intellectual property protection methods.

</details>


### [28] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan, Ziyu Shu, Amberbir Alemayoh*

**主要类别:** cs.CR

**AI概要:** 提出结合自监督对比学习和神经架构搜索的勒索软件检测框架，通过硬件性能计数器分析运行时行为，实现更早检测、更低延迟和更好的未知变种适应性


<details>
  <summary>更多</summary>
  
**动机:** 传统检测方法面临勒索软件快速演变、早期检测需求和多样性增长的挑战，现有AI方法存在特征依赖、响应延迟和适应性有限三大局限

**方法:** 设计基于硬件性能计数器的对比学习框架，引入定制损失函数促进早期检测，部署神经架构搜索自动构建自适应模型架构

**结果:** 实验显示检测准确率最高提升16.1%，响应时间最多缩短6倍，在规避攻击下保持鲁棒性

**结论:** 该框架有效解决了勒索软件检测的关键挑战，在准确性、响应速度和适应性方面显著优于现有方法

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Low-Latency+and+Adaptive+Ransomware+Detection+Using+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.21957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21957&send_immediately=true&force_search=false)

**原文摘要:** Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [29] [Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla](https://arxiv.org/abs/2510.22024)
*Evangelos Bitsikas, Jason Veara, Aanjhan Ranganathan*

**主要类别:** cs.CR

**AI概要:** 对特斯拉车辆LTE连接的黑盒安全分析，揭示了IMSI捕获、伪基站劫持、不安全回退机制等系统性协议弱点，这些漏洞对车辆安全认证标准构成挑战


<details>
  <summary>更多</summary>
  
**动机:** 移动网络漏洞在智能手机生态中已有充分研究，但在安全关键的车载环境中影响尚未充分检验，需要评估LTE连接在汽车环境中的安全性

**方法:** 采用黑盒非侵入式安全分析方法，针对特斯拉Model 3和Cybertruck的LTE连接进行测试，分析其远程信息处理堆栈

**结果:** 发现特斯拉远程信息处理堆栈存在多个漏洞：易受IMSI捕获、伪基站劫持攻击，不安全回退机制可能静默降低服务可用性，遗留控制平面配置允许静默SMS注入和广播消息欺骗

**结论:** 这些漏洞不仅影响单一厂商，更挑战了ISO/SAE 21434和UN R155/R156等监管框架的核心假设，这些标准要求现代车辆必须具备安全、可追溯和弹性的远程信息处理能力才能获得型式批准

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Security+Analysis+of+LTE+Connectivity+in+Connected+Cars%3A+A+Case+Study+of+Tesla，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22024&send_immediately=true&force_search=false)

**原文摘要:** Modern connected vehicles rely on persistent LTE connectivity to enable
remote diagnostics, over-the-air (OTA) updates, and critical safety services.
While mobile network vulnerabilities are well documented in the smartphone
ecosystem, their impact in safety-critical automotive settings remains
insufficiently examined. In this work, we conduct a black-box, non-invasive
security analysis of LTE connectivity in Tesla vehicles, including the Model 3
and Cybertruck, revealing systemic protocol weaknesses and architectural
misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI
catching, rogue base station hijacking, and insecure fallback mechanisms that
may silently degrade service availability. Furthermore, legacy control-plane
configurations allow for silent SMS injection and broadcast message spoofing
without driver awareness. These vulnerabilities have implications beyond a
single vendor as they challenge core assumptions in regulatory frameworks like
ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient
telematics for type approval of modern vehicles.

</details>


### [30] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

**主要类别:** cs.CR

**AI概要:** 论文提出了Jailbreak Mimicry方法，通过参数高效微调训练小型攻击模型，能够自动生成叙事式越狱提示，在网络安全应用中实现了81%的攻击成功率，显著优于直接提示方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型容易受到复杂的提示工程攻击，这些攻击利用上下文框架绕过安全机制，在网络安全应用中构成重大风险。

**方法:** 使用参数高效微调（LoRA）在Mistral-7B模型上，利用从AdvBench整理的训练数据集，训练攻击模型自动生成越狱提示。

**结果:** 在GPT-OSS-20B上达到81.0%的攻击成功率，在GPT-4、Llama-3和Gemini 2.5 Flash上分别达到66.5%、79.5%和33.0%的成功率，比直接提示方法提升54倍。

**结论:** 该方法将对抗性提示发现从手工制作转变为可重复的科学过程，揭示了当前安全对齐方法的系统性漏洞，特别在技术领域和欺骗类攻击中表现出高脆弱性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jailbreak+Mimicry%3A+Automated+Discovery+of+Narrative-Based+Jailbreaks+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22085&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [31] [Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things](https://arxiv.org/abs/2510.22100)
*Saif E. Nouma, Attila A. Yavuz*

**主要类别:** cs.CR

**AI概要:** Graphene是首个对称前向安全聚合认证加密(FAAE)框架，专为低端IoT设备设计，结合密钥演进策略和离线-在线加密处理，提供密钥泄露恢复能力、接近最优的在线延迟和紧凑认证标签。


<details>
  <summary>更多</summary>
  
**动机:** 现有轻量级认证加密标准缺乏密钥泄露恢复能力、紧凑认证标签和离线-在线加密等性能增强功能，无法满足IoT设备在低能耗对抗环境下的安全需求。

**方法:** 提出Graphene框架，通过结合密钥演进策略、离线-在线加密处理和通用消息认证码(UMACs)，设计了两种不同的实例化方案，在商用硬件和ARM Cortex-M4微控制器上进行实验评估。

**结果:** 实验显示Graphene相比现有方案具有显著的性能优势，同时保持与标准加密实现的向后兼容性。

**结论:** Graphene为IoT基础设施提供了既安全又高效的认证加密解决方案，具备密钥泄露恢复能力和优异性能，已开源供公众测试和使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+and+Breach-Resilient+Authenticated+Encryption+Framework+for+Internet+of+Things，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22100&send_immediately=true&force_search=false)

**原文摘要:** The Internet of Things (IoT) relies heavily on resource-limited devices to
communicate critical (e.g., military data) information under low-energy
adversarial environments and low-latency wireless channels. Authenticated
Encryption (AE) guarantees confidentiality, authenticity, and integrity, making
it a vital security service for IoT. However, current deployed (lightweight) AE
standards lack essential features like key compromise resiliency and compact
authentication tags, as well as performance enhancements such as offline-online
cryptography. To address these gaps, we propose Graphene, the first (to our
knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption
(FAAE) framework designed for the performance and security demands of low-end
IoT infrastructures. Graphene innovates by synergizing key evolution strategies
and offline-online cryptographic processing with Universal Message
Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal
online latency, and compactness. We demonstrate Graphene efficiency through two
distinct instantiations, each balancing unique performance trade-offs with
extensibility for diverse MACs. Our experimental evaluation on commodity
hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant
performance gains over existing alternatives. Graphene is also backward
compatible with standard-compliant cryptographic implementations. We release
our implementation as open source for public testing and adaptation.

</details>


### [32] [TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation](https://arxiv.org/abs/2510.22191)
*Qi Sheng*

**主要类别:** cs.CR

**AI概要:** TPPR是一个新颖的APT攻击溯源分析框架，通过异常子图提取、TTP序列模式挖掘和置信度路径评分，能够实现99.9%的图简化同时保留91%的关键攻击节点，在重构精度上优于现有方案63.1%-67.9%。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于溯源图的APT攻击检测技术无法有效建立攻击上下文关联，容易混淆良性系统操作和真实攻击实体，无法准确刻画真实APT行为。

**方法:** 首先通过异常节点检测、TTP标注和图剪枝提取异常子图，然后使用挖掘的TTP序列模式进行攻击路径推理，最后通过基于置信度的路径评分和合并重构攻击场景。

**结果:** 在真实企业日志（超过1亿事件）和DARPA TC数据集上的评估显示，TPPR实现了99.9%的图简化（从70万条边减少到20条），同时保留了91%的关键攻击节点。

**结论:** TPPR框架通过利用攻击者的TTP模式，能够有效识别和重构APT攻击场景，在保持攻击场景完整性的同时显著提高了检测精度和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TPPR%3A+APT+Tactic+%2F+Technique+Pattern+Guided+Attack+Path+Reasoning+for+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22191&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis based on system audit data has emerged as a fundamental
approach for investigating Advanced Persistent Threat (APT) attacks. Due to the
high concealment and long-term persistence of APT attacks, they are only
represented as a minimal part of the critical path in the provenance graph.
While existing techniques employ behavioral pattern matching and data flow
feature matching to uncover latent associations in attack sequences through
provenance graph path reasoning, their inability to establish effective attack
context associations often leads to the conflation of benign system operations
with real attack entities, that fail to accurately characterize real APT
behaviors. We observe that while the causality of entities in the provenance
graph exhibit substantial complexity, attackers often follow specific attack
patterns-specifically, clear combinations of tactics and techniques to achieve
their goals. Based on these insights, we propose TPPR, a novel framework that
first extracts anomaly subgraphs through abnormal node detection,
TTP-annotation and graph pruning, then performs attack path reasoning using
mined TTP sequential pattern, and finally reconstructs attack scenarios through
confidence-based path scoring and merging. Extensive evaluation on real
enterprise logs (more than 100 million events) and DARPA TC dataset
demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000
to 20 edges) while preserving 91% of critical attack nodes, outperforming
state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in
reconstruction precision while maintaining attack scenario integrity.

</details>


### [33] [SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha, Junaid Arshad, Mohamed Ben Farah, Khalid Ismail*

**主要类别:** cs.CR

**AI概要:** SecureLearn是一种针对多类分类器的两层防御机制，通过数据清洗和特征导向的对抗训练来防御数据投毒攻击，在多种攻击场景下保持90%以上的准确率和75%以上的召回率。


<details>
  <summary>更多</summary>
  
**动机:** 现有防御机制主要针对特定攻击或特定机器学习算法，且多集中于深度神经网络或二分类器，而传统多类分类器在防御数据投毒攻击方面缺乏关注，但这些模型在多模态应用中具有重要意义。

**方法:** 提出SecureLearn两层防御框架：包含数据清洗组件和新的特征导向对抗训练。采用3D评估矩阵（数据投毒攻击、数据清洗、对抗训练三个正交维度）进行基准测试，在10%-20%的中毒水平下评估准确率、召回率、F1分数、检测率、纠正率和错误发现率。

**结果:** SecureLearn对提供的攻击有效，增强了传统多类模型和神经网络的抗攻击能力和鲁棒性。始终保持90%以上的准确率，召回率和F1分数超过75%。对神经网络，所有选定攻击下的召回率和F1分数达到97%。

**结论:** SecureLearn证明了其超越算法特定防御的泛化能力，为多类分类器提供了有效的攻击不可知防御解决方案，显著提高了模型对数据投毒攻击的抵抗能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SecureLearn+-+An+Attack-agnostic+Defense+for+Multiclass+Machine+Learning+Against+Data+Poisoning+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22274&send_immediately=true&force_search=false)

**原文摘要:** Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [34] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly, Christiana Chamon*

**主要类别:** cs.CR

**AI概要:** 该研究提出了一种利用宽带隙技术固有噪声的物理不可克隆函数和机器学习异常检测框架，用于工业控制系统的硬件级认证和实时威胁检测，在模拟测试中达到95%检测精度和亚毫秒级延迟。


<details>
  <summary>更多</summary>
  
**动机:** 宽带隙技术在提高电力系统效率的同时带来了独特的传感器损坏和网络安全风险，特别是高频噪声和复杂的网络物理威胁，需要创新的安全解决方案。

**方法:** 通过提取宽带隙开关噪声（高达100kHz）作为PUF熵源，同时利用该噪声作为实时威胁指标，结合混合机器学习模型和自适应贝叶斯滤波，实现硬件级认证和异常检测。

**结果:** 在良性场景和攻击场景（包括EMI注入、信号篡改和节点冒充）的详细模拟中，实现了95%的检测准确率和亚毫秒级的处理延迟。

**结论:** 研究证明了物理驱动的双重用途噪声利用作为可扩展ICS防御原语的可行性，为利用固有设备特性、连接硬件和人工智能的下一代安全策略奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Noise-Driven+PUF+and+AI+for+Secure+WBG+ICS%3A+A+Proof-of-Concept+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22283&send_immediately=true&force_search=false)

**原文摘要:** Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [35] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang, Tairen Zhang, Lanjun Wang, Ruidong Chen, Wenhui Li, Anan Liu*

**主要类别:** cs.CR

**AI概要:** T2I-RiskyPrompt是一个用于评估文本到图像模型安全性的综合基准数据集，包含6432个有效风险提示，具有分层风险分类和详细风险原因标注，并提出了基于原因驱动的风险图像检测方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有风险提示数据集存在三个主要限制：风险类别有限、粗粒度标注和低有效性，需要更全面的安全评估工具。

**方法:** 开发了包含6个主要类别和14个细分子类别的分层风险分类法，构建了收集和标注风险提示的流程，提出了与安全标注对齐的多语言大模型驱动的风险图像检测方法。

**结果:** 构建了包含6432个有效风险提示的数据集，对8个T2I模型、9种防御方法、5个安全过滤器和5种攻击策略进行了全面评估，提供了9个关键洞察。

**结论:** T2I-RiskyPrompt为T2I模型安全性评估提供了全面基准，揭示了现有安全措施的优缺点，并在多个研究领域具有潜在应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T2I-RiskyPrompt%3A+A+Benchmark+for+Safety+Evaluation%2C+Attack%2C+and+Defense+on+Text-to-Image+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22300&send_immediately=true&force_search=false)

**原文摘要:** Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [36] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

**主要类别:** cs.CR

**AI概要:** 提出跨机构联邦学习框架，用于ECG图像数字化，在不共享原始图像的情况下训练nnU-Net分割模型，结合安全聚合和差分隐私保护，在非IID数据下实现接近集中式训练的性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决ECG图像数字化中集中式训练与跨机构隐私保护及部署限制的冲突问题，需要在保护医疗数据隐私的前提下实现多机构协作模型训练。

**方法:** 使用跨机构联邦学习框架，整合FedAvg、FedProx和FedAdam三种聚合算法，结合安全聚合和中心化高斯差分隐私保护，包含页面归一化、轨迹分割、网格泄漏抑制和向量化的完整数字化流程。

**结果:** 在PTB-XL数据集上，FedAdam相比FedAvg和FedProx收敛更快且达到更高性能平台，接近集中式训练效果，隐私机制在保持精度的同时提供可部署的隐私保障。

**结论:** 该联邦学习框架成功解决了医疗ECG图像数字化的隐私保护问题，在非IID环境下实现了有效的多机构协作训练，为医疗AI的实际部署提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Aware+Federated+nnU-Net+for+ECG+Page+Digitization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22387&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [37] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li, Zheng Yu, Jingyi Song, Meng Xu, Yuxuan Luo, Dongliang Mu*

**主要类别:** cs.CR

**AI概要:** PORTGPT是一个基于LLM的智能代理，通过自动化工具访问代码、总结Git历史和基于反馈修订补丁，实现了89.15%的补丁回移植成功率，显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统手动补丁回移植工作量大，现有自动化方法依赖预定义规则，缺乏对复杂补丁的灵活性。

**方法:** 使用LLM代理结合代码访问工具、Git历史总结和基于编译器反馈的自主补丁修订，模拟人类推理和验证过程。

**结果:** 在1815个现有案例中达到89.15%成功率，在146个复杂案例中达到62.33%成功率，向Linux内核社区贡献的9个补丁全部被合并。

**结论:** PORTGPT证明了LLM在自动化补丁回移植任务中的有效性，能够处理复杂场景并达到实际应用水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PortGPT%3A+Towards+Automated+Backporting+Using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22396&send_immediately=true&force_search=false)

**原文摘要:** Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [38] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao, Jia Zou, Zhichao Cao, Xusheng Xiao*

**主要类别:** cs.CR

**AI概要:** PROGQL框架提出了一种专门用于网络安全溯源分析的图查询语言和查询引擎，解决了现有溯源分析技术不灵活、不可扩展和内存效率低的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有溯源分析技术面临两个关键挑战：(1)不灵活且不可扩展，难以融入分析师专业知识；(2)内存效率低下，通常需要100GB以上内存来存储整个事件流，限制了实际环境中的可扩展性和部署。

**方法:** 提出PROGQL框架，提供领域特定的图搜索语言和精心设计的查询引擎，允许将系统审计事件和专家知识联合表达为图搜索查询。引入新的语言结构支持约束图遍历、边权重计算、沿加权边的值传播和图合并。查询引擎针对异构数据库后端的高效增量图搜索进行了优化。

**结果:** 在真实攻击评估中，PROGQL语言在表达多样化复杂攻击方面比最先进的图查询语言Cypher更有效，与最先进的PA技术DEPIMPACT的比较进一步证明了PROGQL框架设计带来的可扩展性显著改进。

**结论:** PROGQL框架通过提供灵活、可扩展且内存高效的解决方案，显著提升了网络安全溯源分析的能力，能够更好地支持复杂网络攻击的调查工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProGQL%3A+A+Provenance+Graph+Query+System+for+Cyber+Attack+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22400，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22400&send_immediately=true&force_search=false)

**原文摘要:** Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [39] [ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole](https://arxiv.org/abs/2510.22536)
*Jotaro Yano*

**主要类别:** cs.CR

**AI概要:** 该论文提出了一种跨域的ZK协处理器桥接系统，允许Solana程序通过Wormhole协议在Aztec L2上进行隐私计算执行，并提供了完整的安全性和功能保障机制。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决Solana区块链程序需要隐私计算能力的问题，通过跨链技术将计算任务安全地委托给Aztec L2的零知识证明环境执行。

**方法:** 设计了一个四部分系统：Solana程序发送消息、EVM门户验证消息、Aztec合约私有消费消息、离线中继器传输消息。使用Wormhole VAA作为认证传输机制，确保跨链安全。

**结果:** 成功实现了跨域ZK协处理器桥接，提供了状态机、消息格式和安全证明，包括防重放、来源认证、最终一致性等安全特性。

**结论:** 该系统为Solana程序提供了安全的隐私计算能力扩展，通过标准化的跨链协议实现了可验证的私有执行环境，具有实际部署的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZK+Coprocessor+Bridge%3A+Replay-Safe+Private+Execution+from+Solana+to+Aztec+via+Wormhole，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22536&send_immediately=true&force_search=false)

**原文摘要:** We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs
request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable
Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a
Solana program that posts messages to Wormhole Core with explicit finality;
(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound
payload secretHash||m from the attested VAA, derives a domain-separated field
commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference
implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we
provide migration guidance to the payload-bound interface); (iii) a minimal
Aztec contract that consumes the message privately; and (iv) an off-chain
relayer that ferries VAAs and can record receipts on Solana. We present state
machines, message formats, and proof sketches for replay-safety, origin
authenticity, finality alignment, parameter binding (no relayer front-running
of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a
concise Reproducibility note with pinned versions and artifacts to replicate a
public testnet run.

</details>


### [40] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang*

**主要类别:** cs.CR

**AI概要:** 该论文提出CP-GBA方法，通过图提示学习生成可跨学习范式迁移的通用子图触发器，解决了现有图神经网络后门攻击方法在跨范式迁移性差的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有图神经网络后门攻击的触发器生成器结构简单，过度依赖特定特征，只能适应单一图学习范式（如图监督学习、图对比学习或图提示学习），在其他学习范式中迁移性差，攻击成功率低。

**方法:** 提出CP-GBA方法：1) 从目标图中提取紧凑且表达性强的可查询触发器集合，强调类别感知、特征丰富性和结构保真度；2) 首次探索图提示学习的理论可迁移性，在基于提示的目标下训练触发器，使其能有效泛化到不同的测试范式。

**结果:** 在多个真实数据集和防御场景下的广泛实验表明，CP-GBA实现了最先进的攻击成功率。

**结论:** CP-GBA方法通过图提示学习生成的可迁移子图触发器，有效解决了图神经网络后门攻击在不同学习范式间的迁移性问题，显著提高了攻击成功率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Paradigm+Graph+Backdoor+Attacks+with+Promptable+Subgraph+Triggers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22555&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [41] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee, Sajal Saha*

**主要类别:** cs.CR

**AI概要:** 本文系统综述了区块链平台中使用的数字签名方案，分析其如何实现不可否认性并增强系统安全性，比较了不同签名方案在共识协议、智能合约约束和资源限制下的适用性。


<details>
  <summary>更多</summary>
  
**动机:** 区块链系统依赖分布式账本和强安全保证，其中不可否认性是关键要求，可防止交易作者否认并维护数据完整性。需要系统评估数字签名方案如何实现这一目标。

**方法:** 研究调查了代表性签名方案家族，分析其密码学基础、安全假设和部署相关属性，包括不可伪造性、抗延展性、聚合支持、多签名/门限设置、密钥签名大小和验证成本等标准。

**结果:** 通过比较分析，揭示了不同签名方案在吞吐量、存储、可扩展性和攻击面方面的实际权衡，总结了各方案在区块链环境中的优势和限制。

**结论:** 研究强调精心选择的数字签名对实现不可否认性和维护信息完整性至关重要，并指出了互操作性和后量子准备等实施考虑和未来研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Blockchain+Signatures+to+Ensure+Information+Integrity+and+Non-Repudiation+in+the+Digital+Era%3A+A+comprehensive+study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22561，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22561&send_immediately=true&force_search=false)

**原文摘要:** Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [42] [FAARM: Firmware Attestation and Authentication Framework for Mali GPUs](https://arxiv.org/abs/2510.22566)
*Md. Mehedi Hasan*

**主要类别:** cs.CR

**AI概要:** FAARM是一个轻量级固件认证框架，通过在EL3安全监控器中集成数字签名验证来防御MOLE攻击，保护GPU可信执行环境免受恶意固件注入，仅产生1.34毫秒的延迟开销。


<details>
  <summary>更多</summary>
  
**动机:** MOLE攻击揭示了GPU可信执行环境存在严重安全漏洞，攻击者可以通过注入恶意固件绕过内存保护并窃取敏感数据，现有GPU TEE设计存在固件级别的信任缺口。

**方法:** FAARM采用供应商签名的固件包和设备上公钥锚点，在EL3安全监控器进行数字签名验证、完整性检查、版本控制和固件区域锁定，消除预验证和TOCTOU攻击向量。

**结果:** FAARM原型成功检测并阻止恶意固件注入，平均验证延迟仅为1.34毫秒，证明可以实现强安全性且开销可忽略不计。

**结论:** FAARM填补了基于shim的GPU TEE的基本安全缺口，为移动和云GPU部署提供了实用、可部署的防御方案，显著提升了安全基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAARM%3A+Firmware+Attestation+and+Authentication+Framework+for+Mali+GPUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22566&send_immediately=true&force_search=false)

**原文摘要:** Recent work has revealed MOLE, the first practical attack to compromise GPU
Trusted Execution Environments (TEEs), by injecting malicious firmware into the
embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence
of cryptographic verification during initialization, adversaries with kernel
privileges can bypass memory protections, exfiltrate sensitive data at over 40
MB/s, and tamper with inference results, all with negligible runtime overhead.
This attack surface affects commodity mobile SoCs and cloud accelerators,
exposing a critical firmware-level trust gap in existing GPU TEE designs. To
address this gap, this paper presents FAARM, a lightweight Firmware Attestation
and Authentication framework that prevents MOLE-style firmware subversion.
FAARM integrates digital signature verification at the EL3 secure monitor using
vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3
verifies firmware integrity and authenticity, enforces version checks, and
locks the firmware region, eliminating both pre-verification and
time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a
software-only prototype on a Mali GPU testbed, using a Google Colab-based
emulation framework that models the firmware signing process, the EL1 to EL3
load path, and secure memory configuration. FAARM reliably detects and blocks
malicious firmware injections, rejecting tampered images before use and denying
overwrite attempts after attestation. Firmware verification incurs only 1.34 ms
latency on average, demonstrating that strong security can be achieved with
negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU
TEEs, providing a practical, deployable defense that raises the security
baseline for both mobile and cloud GPU deployments.

</details>


### [43] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska, Max Mathys, Francesco Casucci, Mateo Rojas-Carulla, Xander Davies, Alexandra Souly, Niklas Pfister*

**主要类别:** cs.CR

**AI概要:** 论文提出了threat snapshots框架和b³基准，用于系统评估LLM骨干网络对AI代理安全性的影响，发现推理能力提升安全性而模型大小无关


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏对LLM骨干网络如何影响AI代理安全性的系统理解，现有框架要么只能捕捉特定漏洞，要么需要完整代理建模

**方法:** 引入threat snapshots框架，隔离代理执行流中LLM漏洞显现的特定状态；构建基于194331个众包对抗攻击的b³安全基准；评估31个流行LLM

**结果:** 增强的推理能力能提高安全性，而模型大小与安全性无相关性

**结论:** 该框架和基准为LLM提供商和从业者提供了安全评估工具，指导代理开发者并激励模型开发者优先改进骨干安全性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Agent+Backbones%3A+Evaluating+the+Security+of+Backbone+LLMs+in+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22620&send_immediately=true&force_search=false)

**原文摘要:** AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [44] [DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2510.22622)
*Kangran Zhao, Yupeng Chen, Xiaoyu Zhang, Yize Chen, Weinan Guan, Baicheng Chen, Chengzhe Sun, Soumyya Kanti Datta, Qingshan Liu, Siwei Lyu, Baoyuan Wu*

**主要类别:** cs.CR

**AI概要:** 该论文构建了Mega-MMDF大规模多模态深度伪造数据集和DeepfakeBench-MM统一基准，用于解决多模态深度伪造检测中数据不足和标准化评估的问题。


<details>
  <summary>更多</summary>
  
**动机:** 先进生成式AI模型的滥用导致伪造人本视听内容广泛传播，带来严重社会风险。现有研究缺乏足够多样化的训练数据和标准化基准，阻碍了深度探索。

**方法:** 1. 构建Mega-MMDF数据集：使用21种伪造管道（10种音频伪造+12种视觉伪造+6种音频驱动面部重现方法），包含11万真实样本和110万伪造样本；2. 建立DeepfakeBench-MM基准：为多模态深度伪造检测提供标准化协议和评估平台，支持5个数据集和11个检测器。

**结果:** 创建了目前最大最多样的多模态深度伪造数据集，建立了首个统一基准平台，通过综合评估发现了多个关键发现（如数据增强、叠加伪造等方面）。

**结论:** Mega-MMDF数据集和DeepfakeBench-MM基准将为推进多模态深度伪造检测研究提供基础性基础设施支持，有助于应对AI生成伪造内容带来的社会风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepfakeBench-MM%3A+A+Comprehensive+Benchmark+for+Multimodal+Deepfake+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22622&send_immediately=true&force_search=false)

**原文摘要:** The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.

</details>


### [45] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain*

**主要类别:** cs.CR

**AI概要:** Sentra-Guard是一个实时模块化防御系统，使用混合架构检测和缓解针对大语言模型的越狱和提示注入攻击，在检测率、AUC和F1分数上均达到近乎完美的性能。


<details>
  <summary>更多</summary>
  
**动机:** 针对大语言模型面临的越狱和提示注入攻击威胁，需要开发有效的实时防御系统来保护模型安全。

**方法:** 采用混合架构：FAISS索引的SBERT嵌入表示捕获提示语义，结合微调transformer分类器；使用分类器-检索器融合模块动态计算上下文感知风险评分；包含多语言预处理层支持100多种语言；集成人机交互反馈循环持续学习。

**结果:** 检测率达到99.96%，AUC=1.00，F1=1.00，攻击成功率仅0.004%，显著优于LlamaGuard-2(1.3%)和OpenAI Moderation(3.7%)等基线方法。

**结论:** Sentra-Guard建立了对抗性LLM防御的新最先进水平，具有透明、可微调和兼容多种LLM后端的特点，支持商业和开源环境的可扩展部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentra-Guard%3A+A+Multilingual+Human-AI+Framework+for+Real-Time+Defense+Against+Adversarial+LLM+Jailbreaks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22628&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [46] [RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography](https://arxiv.org/abs/2510.22661)
*Malik Imran, Safiullah Khan, Zain Ul Abideen, Ciara Rafferty, Ayesha Khalid, Muhammad Rashid, Maire O'Neill*

**主要类别:** cs.CR

**AI概要:** RejSCore是一个针对后量子密码学中拒绝采样操作的轻量级硬件加速器，专门为QR-UOV签名方案设计，在资源受限设备上实现高效性能。


<details>
  <summary>更多</summary>
  
**动机:** 后量子多元公钥密码方案需要大量计算操作如拒绝采样，这对资源受限设备构成挑战，而现有硬件设计在这方面研究不足。

**方法:** 采用AES-CTR-128伪随机数生成器和轻量级迭代方法进行拒绝采样，在Artix-7 FPGA和65nm CMOS技术上评估性能。

**结果:** 在Artix-7上达到2042个slice面积和222MHz频率，在65nm CMOS上达到464,866μm²面积和565MHz频率，完成操作需要8525个时钟周期。

**结论:** ADP和PDP评估证实RejSCore适合部署在资源受限和安全关键的环境中，为后量子密码硬件实现提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RejSCore%3A+Rejection+Sampling+Core+for+Multivariate-based+Public+key+Cryptography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22661&send_immediately=true&force_search=false)

**原文摘要:** Post-quantum multivariate public key cryptography (MPKC) schemes resist
quantum threats but require heavy operations, such as rejection sampling, which
challenge resource-limited devices. Prior hardware designs have addressed
various aspects of MPKC signature generation. However, rejection sampling
remains largely unexplored in such contexts. This paper presents RejSCore, a
lightweight hardware accelerator for rejection sampling in post-quantum
cryptography. It specifically targets the QR-UOV scheme, which is a prominent
candidate under the second-round of the National Institute of Standards and
Technology (NIST) additional digital signature standardization process. The
architecture includes an AES-CTR-128-based pseudorandom number generator.
Moreover, a lightweight iterative method is employed in rejection sampling,
offering reduced resource consumption and area overhead while slightly
increasing latency. The performance of RejSCore is comprehensively evaluated on
Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and
Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area
of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and
565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =
127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525
clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for
deployment in resource-constrained and security-critical environments.

</details>


### [47] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le, Tan Le*

**主要类别:** cs.CR

**AI概要:** SpoofTrackBench是一个用于评估实时定位跟踪系统在雷达欺骗攻击下对抗鲁棒性的可复现模块化基准测试框架，使用Hampton大学雷达数据集模拟多种欺骗攻击并评估不同跟踪架构的性能。


<details>
  <summary>更多</summary>
  
**动机:** 需要建立一个开放、可复现的基准测试标准，用于系统评估实时定位跟踪系统在面对雷达欺骗攻击时的对抗鲁棒性，促进不同架构的性能比较和社区验证。

**方法:** 利用Hampton大学Skyler雷达传感器数据集，模拟漂移、幽灵和镜像三种欺骗攻击类型，使用联合概率数据关联(JPDA)和全局最近邻(GNN)两种跟踪架构进行评估，通过分离干净和欺骗检测流、可视化轨迹偏差和量化真值漂移误差来评估性能。

**结果:** 开发了一个包含聚类覆盖、注入感知时间线和场景自适应可视化的可解释框架，能够自动导出评估图表和日志，实现跨欺骗类型和配置的可复现比较。

**结论:** SpoofTrackBench为欺骗感知跟踪管道设立了新的开放、伦理基准测试标准，支持严格的跨架构分析和社区验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpoofTrackBench%3A+Interpretable+AI+for+Spoof-Aware+UAV+Tracking+and+Benchmarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22726&send_immediately=true&force_search=false)

**原文摘要:** SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [48] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang, YiLu Zhong, MiDi Wan, WenJie Yu, YuanBing Ouyang, Yenan Huang, Hui Li*

**主要类别:** cs.CR

**AI概要:** 研究发现提示词质量与AI生成代码安全性直接相关：提示词规范性越低，生成不安全代码的概率越高。通过提升提示词质量和采用高级提示技术可有效改善代码安全。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注对抗攻击和模型固有缺陷，但忽略了良性但质量差的提示词对生成代码安全性的影响，这是一个普遍但未充分探索的问题。

**方法:** 提出了包含目标清晰度、信息完整性和逻辑一致性三个维度的提示词质量评估框架，构建了包含四个规范性等级(L0-L3)的大规模基准数据集CWE-BENCH-PYTHON，并在多个先进LLM上进行了广泛实验。

**结果:** 实验显示明确的负相关关系：提示词规范性降低时，生成不安全代码的可能性显著增加。同时发现Chain-of-Thought和Self-Correction等高级提示技术能有效缓解低质量提示带来的安全风险。

**结论:** 提升用户提示词质量是增强AI生成代码安全性的关键有效策略，这为改善代码生成安全性提供了新的研究方向和实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Your+Prompt+Poisoning+Code%3F+Defect+Induction+Rates+and+Security+Mitigation+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22944，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22944&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [49] [QuantumShield: Multilayer Fortification for Quantum Federated Learning](https://arxiv.org/abs/2510.22945)
*Dev Gurung, Shiva Raj Pokhrel*

**主要类别:** cs.CR

**AI概要:** 提出了一种量子安全的联邦学习框架(QFL)，通过整合量子密钥分发、量子隐形传态、密钥封装机制和后量子密码学等先进协议，保护分布式学习系统免受量子攻击威胁。


<details>
  <summary>更多</summary>
  
**动机:** 随着经典加密方法在量子攻击面前日益脆弱，需要建立能够抵御量子能力攻击者的弹性安全架构来保护联邦学习系统。

**方法:** 整合并严格评估量子密钥分发(QKD)、量子隐形传态、密钥封装机制(KEM)和后量子密码学(PQC)等协议，构建安全可扩展的QFL生态系统，通过理论建模和实验验证进行分析。

**结果:** 建立了具有无缝互操作性的安全框架，为量子时代的联邦学习系统提供了详细的安全性和性能评估。

**结论:** 这项工作为下一代联邦学习系统奠定了坚实基础，使其在量子时代具有内在安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuantumShield%3A+Multilayer+Fortification+for+Quantum+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22945&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a groundbreaking quantum-secure federated learning
(QFL) framework designed to safeguard distributed learning systems against the
emerging threat of quantum-enabled adversaries. As classical cryptographic
methods become increasingly vulnerable to quantum attacks, our framework
establishes a resilient security architecture that remains robust even in the
presence of quantum-capable attackers. We integrate and rigorously evaluate
advanced quantum and post-quantum protocols including Quantum Key Distribution
(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and
Post-Quantum Cryptography (PQC) to fortify the QFL process against both
classical and quantum threats. These mechanisms are systematically analyzed and
implemented to demonstrate their seamless interoperability within a secure and
scalable QFL ecosystem. Through comprehensive theoretical modeling and
experimental validation, this work provides a detailed security and performance
assessment of the proposed framework. Our findings lay a strong foundation for
next-generation federated learning systems that are inherently secure in the
quantum era.

</details>


### [50] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She*

**主要类别:** cs.CR

**AI概要:** 论文提出CompressionAttack框架，首次利用提示压缩作为攻击面，通过硬压缩和软压缩两种策略实现高达80%攻击成功率和98%偏好翻转，现有防御措施无效。


<details>
  <summary>更多</summary>
  
**动机:** LLM驱动的代理常使用提示压缩来降低推理成本，但压缩模块为效率而非安全性优化，易被对抗性输入操纵，导致语义漂移和LLM行为改变。

**方法:** 提出CompressionAttack框架，包含两种策略：HardCom（使用离散对抗编辑进行硬压缩）和SoftCom（在潜在空间进行扰动实现软压缩）。

**结果:** 在多个LLM上的实验显示攻击成功率高达80%，偏好翻转率98%，具有高度隐蔽性和可迁移性。VSCode Cline和Ollama的案例研究证实了实际影响。

**结论:** 当前防御措施无效，凸显了需要更强的保护机制来应对提示压缩带来的安全风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CompressionAttack%3A+Exploiting+Prompt+Compression+as+a+New+Attack+Surface+in+LLM-Powered+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22963&send_immediately=true&force_search=false)

**原文摘要:** LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [51] [Advancing Honeywords for Real-World Authentication Security](https://arxiv.org/abs/2510.22971)
*Sudiksha Das, Ashish Kundu*

**主要类别:** cs.CR

**AI概要:** 这篇立场论文分析了蜜词(Honeywords)技术十余年未被主流认证平台采用的原因，提出了一个可部署框架来解决平坦性、集成性和可靠性等问题，认为只有技术改进与安全架构结合才能让蜜词从学术概念转变为实用安全工具。


<details>
  <summary>更多</summary>
  
**动机:** 蜜词技术自2013年提出以来，虽然看似是检测密码凭证滥用的主动方法，但十多年来未被主要认证平台采用。论文旨在探讨阻碍其广泛应用的深层原因并提出解决方案。

**方法:** 论文通过分析现有的蜜词生成方法、攻击者建模和蜜检查器架构的研究成果，识别已解决的问题和持续存在的障碍，然后提出一个结合攻击者弹性、上下文感知诱饵创建的可部署框架。

**结果:** 研究发现蜜词技术的核心概念具有潜力，但需要解决平坦性、集成性和可靠性等关键问题。提出了一个易于集成到现有系统的框架设计。

**结论:** 蜜词技术要从学术理念转变为实用安全工具，需要技术进展与安全简洁的架构相结合，同时配备自适应响应处理和详细配置检查。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Honeywords+for+Real-World+Authentication+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.22971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22971&send_immediately=true&force_search=false)

**原文摘要:** Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords
stored alongside a real password, appear to be a proactive method to help
detect password credentials misuse. However, despite over a decade of research,
this technique has not been adopted by major authentication platforms. This
position paper argues that the core concept of Honeywords has potential but
requires more research on issues such as flatness, integration, and
reliability, in order to be a practical deployable solution. This paper
examines the current work on Honeyword generation, attacker modeling, and
honeychecker architecture, analyzing the subproblems that have been addressed
and ongoing issues that prevent this system from being more widely used. The
paper then suggests a deployable framework that combines the
attacker-resilient, context-aware decoy creation that Honeywords provide with
easy integration into existing systems. Honeywords will only move from an
academic idea to a practical security tool if technical advances are paired
with secure and straightforward architectures, along with adaptive response
handling and detailed configuration checks.

</details>


### [52] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan, Zeng Li, Kunlin Cai, Liuhuo Wan, Ruomai Ren, Yiran Shen, Guangdong Bai*

**主要类别:** cs.CR

**AI概要:** 首个针对VR应用生态系统隐私实践的多平台研究，分析了来自5大应用商店的6565个VR应用，发现普遍存在隐私合规问题，包括敏感数据使用未声明、隐私政策缺失等问题。


<details>
  <summary>更多</summary>
  
**动机:** VR应用收集大量隐私敏感数据（如生物特征、用户行为和环境数据），但缺乏领域特定的数据管理法规，导致各应用商店隐私实践差异显著，需要系统性的隐私保护状况评估。

**方法:** 使用多维度方法评估VR应用的声明性和行为性隐私实践，包括自然语言处理、逆向工程和静态分析技术。

**结果:** 研究发现所有商店都存在严重隐私合规问题：1/3的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策，表明VR生态系统隐私保护仍处于不成熟阶段。

**结论:** 研究首次揭示了VR应用生态系统的隐私保护现状，结果应向VR开发者和用户发出警示，并促使应用商店运营商对VR应用的隐私合规实施更严格的监管措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Store+Privacy+Measurement+of+Virtual+Reality+App+Ecosystem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23024&send_immediately=true&force_search=false)

**原文摘要:** Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [53] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran, Suman Deb, Anupam Chattopadhyay*

**主要类别:** cs.CR

**AI概要:** 提出一种基于物理不可克隆函数的BNN模型参数保护方法，在内存计算框架中实现加密存储和同态计算，以极小的运行时开销保护模型安全。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法在内存计算框架中保护BNN模型参数需要加密存储和运行时解密，这会带来显著计算开销，违背内存计算集成存储与计算的核心原则。

**方法:** 利用物理不可克隆函数生成密钥，在参数存储到交叉阵列前进行转换，使得推理操作可以直接在加密权重上执行，实现最小开销的全同态加密。

**结果:** 未经密钥的推理性能急剧下降，准确率低于15%，验证了保护策略的有效性。

**结论:** 该方法成功在保护BNN模型安全的同时保持了内存计算架构的计算效率，为安全内存计算提供了可行解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Encrypted+Inference+using+Binarized+Neural+Networks+within+In-Memory+Computing+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23034&send_immediately=true&force_search=false)

**原文摘要:** Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [54] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang, Weiming Zhang, Nenghai Yu, Kejiang Chen*

**主要类别:** cs.CR

**AI概要:** RTMStega是一种基于熵驱动的语言隐写框架，通过秩基自适应编码和上下文感知解压缩技术，在保持文本质量的同时显著提升隐写容量和处理效率


<details>
  <summary>更多</summary>
  
**动机:** 当前语言隐写方法面临嵌入容量低和安全性的双重挑战，传统修改方法会产生可检测异常，生成式方法受限于标记预测的低熵问题

**方法:** 提出RTMStega框架，整合秩基自适应编码和上下文感知解压缩，通过将秘密消息映射到标记概率秩，并基于上下文感知的熵调整动态采样

**结果:** 实验显示RTMStega将主流生成式隐写的有效载荷容量提升三倍，处理时间减少50%以上，同时保持高文本质量

**结论:** RTMStega为安全高效的隐蔽通信提供了可信解决方案，在容量、安全性和效率方面实现了良好平衡

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+high-capacity+linguistic+steganography+based+on+entropy-driven+rank-token+mapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23035&send_immediately=true&force_search=false)

**原文摘要:** Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


### [55] [KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation](https://arxiv.org/abs/2510.23036)
*Xudong Yang, Jincheng Li, Kaiwen Xing, Zhenjia Xiao, Mingjian Duan, Weili Han, Hu Xiong*

**主要类别:** cs.CR

**AI概要:** KAPG是一个知识增强的密码猜测框架，通过整合外部词汇知识来提升密码猜测效果，相比现有模型在站内和跨站场景分别提升36.5%和74.7%。同时开发了KAPSM密码强度计，显著优于现有工具。


<details>
  <summary>更多</summary>
  
**动机:** 传统密码猜测模型主要依赖泄露密码的内部统计模式，忽视了社会文化背景和流行词汇等外部因素对密码选择的影响，导致模型难以适应新兴密码趋势且效果随时间下降。

**方法:** 提出KAPG框架，将泄露密码的内部统计知识与反映现实趋势的外部信息相结合，使用密码前缀作为知识查找锚点，在生成过程中动态注入相关外部线索，同时保持真实密码的结构规律性。

**结果:** 在12个泄露数据集上的实验显示，KAPG在站内场景平均提升36.5%，跨站场景平均提升74.7%。KAPSM密码强度计在各种评估设置中准确性显著优于现有工具。

**结论:** 通过整合外部知识可以显著提升密码猜测模型的适应性和有效性，同时基于此开发的密码强度计也能更准确地评估密码安全性，为密码安全防护提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KAPG%3A+Adaptive+Password+Guessing+via+Knowledge-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23036&send_immediately=true&force_search=false)

**原文摘要:** As the primary mechanism of digital authentication, user-created passwords
exhibit common patterns and regularities that can be learned from leaked
datasets. Password choices are profoundly shaped by external factors, including
social contexts, cultural trends, and popular vocabulary. Prevailing password
guessing models primarily emphasize patterns derived from leaked passwords,
while neglecting these external influences -- a limitation that hampers their
adaptability to emerging password trends and erodes their effectiveness over
time.
  To address these challenges, we propose KAPG, a knowledge-augmented password
guessing framework that adaptively integrates external lexical knowledge into
the guessing process. KAPG couples internal statistical knowledge learned from
leaked passwords with external information that reflects real-world trends. By
using password prefixes as anchors for knowledge lookup, it dynamically injects
relevant external cues during generation while preserving the structural
regularities of authentic passwords. Experiments on twelve leaked datasets show
that KnowGuess achieves average improvements of 36.5\% and 74.7\% over
state-of-the-art models in intra-site and cross-site scenarios, respectively.
Further analyses of password overlap and model efficiency highlight its
robustness and computational efficiency. To counter these attacks, we further
develop KAPSM, a trend-aware and site-specific password strength meter.
Experiments demonstrate that KAPSM significantly outperforms existing tools in
accuracy across diverse evaluation settings.

</details>


### [56] [zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks](https://arxiv.org/abs/2510.23060)
*Paritosh Ramanan, H. M. Mohaimanul Islam, Abhiram Reddy Alugula*

**主要类别:** cs.CR

**AI概要:** zkSTAR是一个基于zk-SNARKs的工业控制系统网络攻击检测框架，能够在保护数据机密性的同时提供可验证的检测保证，满足监管合规要求。


<details>
  <summary>更多</summary>
  
**动机:** 工业控制系统面临日益严重的网络威胁，监管机构要求更严格的合规性，但同时又需要保护公用事业公司的敏感操作数据不被泄露。

**方法:** 采用基于残差的统计假设检验方法，构建双管齐下的zk-SNARK架构，确保状态空间动态的时间一致性和检测测试的统计一致性。

**结果:** 通过形式化分析框架的可靠性和零知识特性，并在真实ICS数据集上进行计算实验验证了实际可行性。

**结论:** zkSTAR为工业控制系统驱动的关键基础设施网络提供了一个可扩展、保护隐私的监管合规替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是zkSTAR%3A+A+zero+knowledge+system+for+time+series+attack+detection+enforcing+regulatory+compliance+in+critical+infrastructure+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23060&send_immediately=true&force_search=false)

**原文摘要:** Industrial control systems (ICS) form the operational backbone of critical
infrastructure networks (CIN) such as power grids, water supply systems, and
gas pipelines. As cyber threats to these systems escalate, regulatory agencies
are imposing stricter compliance requirements to ensure system-wide security
and reliability. A central challenge, however, is enabling regulators to verify
the effectiveness of detection mechanisms without requiring utilities to
disclose sensitive operational data. In this paper, we introduce zkSTAR, a
cyberattack detection framework that leverages zk-SNARKs to reconcile these
requirements and enable provable detection guarantees while preserving data
confidentiality. Our approach builds on established residual-based statistical
hypothesis testing methods applied to state-space detection models.
Specifically, we design a two-pronged zk-SNARK architecture that enforces
temporal consistency of the state-space dynamics and statistical consistency of
the detection tests, allowing regulators to temporally verify alarm correctness
without visibility into utility-level data. We formally analyze the soundness
and zero knowledge properties of our framework and validate its practical
feasibility through computational experiments on real-world ICS datasets. As a
result, our work demonstrates a scalable, privacy-preserving alternative for
regulatory compliance for ICS driven critical infrastructure networks.

</details>


### [57] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi, Shotaro Ishihara*

**主要类别:** cs.CR

**AI概要:** Fast-MIA是一个用于高效评估大语言模型成员推理攻击的Python库，通过批量推理和统一评估框架解决计算成本高和缺乏标准化实现的问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于大语言模型在版权、安全和数据隐私方面的担忧日益增长，成员推理攻击研究面临高计算成本和缺乏标准化实现两大障碍。

**方法:** 提供快速批量推理功能，实现了代表性成员推理攻击方法，采用统一评估框架，支持简单配置和可扩展性。

**结果:** 开发了开源工具Fast-MIA，支持大规模可复现的基准测试，促进LLM研究的可扩展性和透明度。

**结论:** Fast-MIA作为一个开源工具，能够有效支持大语言模型成员推理攻击的标准化评估和大规模实证比较研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast-MIA%3A+Efficient+and+Scalable+Membership+Inference+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23074&send_immediately=true&force_search=false)

**原文摘要:** We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


### [58] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang, Xin Zhang*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于大语言模型和调用栈表示的定向灰盒模糊测试方法，用精确的调用栈预测替代传统的静态分析距离度量，显著提高了漏洞触发效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的定向灰盒模糊测试方法依赖静态分析的距离度量，存在近似计算不精确的问题，导致大量无关执行路径被错误优先考虑，降低了模糊测试效率。

**方法:** 通过静态分析构建调用图识别可能到达目标位置的方法，然后利用大语言模型预测最可能触发漏洞的调用栈序列，优先选择执行路径与预测调用栈重叠度高的种子进行变异。

**结果:** 在真实程序测试中，该方法比基线方法快1.86到3.09倍触发漏洞，并发现了10个新漏洞和2个不完整修复，获得10个CVE编号。

**结论:** 这是首个将大语言模型集成到定向模糊测试核心种子优先机制的工作，证明了基于调用栈表示和LLM预测的方法能有效提高定向模糊测试的精确性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Imprecise+Distance+Metrics%3A+LLM-Predicted+Target+Call+Stacks+for+Directed+Greybox+Fuzzing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23101&send_immediately=true&force_search=false)

**原文摘要:** Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [59] [Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation](https://arxiv.org/abs/2510.23172)
*Mohsen Ahmadvand, Pedro Souto*

**主要类别:** cs.CR

**AI概要:** 论文分析了Optimism派生管道在zkVM中的效率问题，提出了针对零知识证明的重新设计，实现了6.5倍的派生速度提升和3.5倍的整体加速。


<details>
  <summary>更多</summary>
  
**动机:** 当前Optimism派生管道设计注重正确性和活跃性，但直接移植到zkVM会导致显著的证明开销，使得有效性证明成本远高于必要水平。

**方法:** 系统识别当前设计中的低效问题，分析其对证明成本的影响，并提供保持安全性的重新设计方案，专门针对零知识证明优化。

**结果:** 重新设计在zkVM中实现了最高6.5倍的派生速度提升，整体性能提升3.5倍，同时保持相同的安全保障。

**结论:** 通过针对零知识证明特性进行专门优化，可以显著降低证明成本并提升性能，同时不牺牲系统的安全性保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Optimism%3A+Up+to+6.5x+Faster+zkVM+Validty+Proofs+via+Sparse+Derivation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23172&send_immediately=true&force_search=false)

**原文摘要:** The Optimism derivation pipeline is engineered for correctness and liveness,
not for succinct validity proofs. A straightforward port to a zkVM imposes
significant overheads, making validity proofs significantly more costly than
necessary. We systematically identify inefficiencies in the current design,
analyze their impact on proving costs, and provide a soundness-preserving
redesign tailored to zk proving. Our redesign achieves up to 6.5x faster
derivation inside zkVMs (3.5x overall speedup) while maintaining identical
safety guarantees.

</details>


### [60] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen, Qianqian Yang, Shuo Shao, Shunpu Tang, Zhiguo Shi, Shui Yu*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新颖的安全语义通信框架，通过可学习模式的差分隐私噪声保护图像传输中的隐私信息，在保证合法用户任务性能的同时有效降低窃听者的重建质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有安全语义通信方法依赖于限制性或不切实际的假设（如有利的信道条件或窃听者模型的先验知识），需要解决这些局限性并提供近似隐私保证。

**方法:** 使用GAN反演方法从源图像提取解耦的语义表示，然后选择性地对私有语义表示添加可学习模式的差分隐私噪声，通过神经网络的对抗训练实现。

**结果:** 实验结果表明，与之前的DP方法和直接传输相比，该方法显著降低了窃听者的重建质量，同时只对任务性能造成轻微影响，在可比安全级别下为合法用户带来LPIPS优势0.06-0.29和FPPSR优势0.10-0.86。

**结论:** 所提出的基于可学习模式差分隐私的安全语义通信框架能够提供明确可控的安全级别，有效保护隐私信息，同时保持合法用户的通信效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Semantic+Communication+over+Wiretap+Channels+with+Learnable+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23274&send_immediately=true&force_search=false)

**原文摘要:** While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>


### [61] [Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks](https://arxiv.org/abs/2510.23313)
*Yaokai Feng, Kouichi Sakurai*

**主要类别:** cs.CR

**AI概要:** 本综述系统梳理了网络入侵检测系统(NIDS)从传统方法到LLM集成的演变历程，总结了当前技术现状、优缺点，并探讨了LLM在NIDS中的实际应用价值。


<details>
  <summary>更多</summary>
  
**动机:** 随着网络攻击日益复杂，传统NIDS方法面临局限性，需要探索LLM等新技术在入侵检测中的应用潜力，同时分析其实际部署挑战和安全风险。

**方法:** 通过系统性文献综述方法，分析比较了签名检测、神经网络检测和LLM集成三种NIDS技术路径，涵盖了传统网络、自动驾驶和物联网等多种环境。

**结果:** 研究发现：1)传统签名检测仍具价值；2)神经网络检测面临实际部署挑战；3)LLM在NIDS中有用但存在安全风险；4)需要构建领域专用LLM。

**结论:** LLM为NIDS带来新机遇但面临安全挑战，未来需要发展领域专用LLM并解决实际部署问题，传统方法仍具有重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Network+Intrusion+Detection%3A+Evolution+from+Conventional+Approaches+to+LLM+Collaboration+and+Emerging+Risks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23313&send_immediately=true&force_search=false)

**原文摘要:** This survey systematizes the evolution of network intrusion detection systems
(NIDS), from conventional methods such as signature-based and neural network
(NN)-based approaches to recent integrations with large language models (LLMs).
It clearly and concisely summarizes the current status, strengths, and
limitations of conventional techniques, and explores the practical benefits of
integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS
in diverse environments is reviewed, including conventional network
infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods,
signature-based IDSs, continue to make significant contributions to modern
systems, despite their well-known weaknesses; 2) NN-based detection, although
considered promising and under development for more than two decades, and
despite numerous related approaches, still faces significant challenges in
practical deployment; 3) LLMs are useful for NIDS in many cases, and a number
of related approaches have been proposed; however, they still face significant
challenges in practical applications. Moreover, they can even be exploited as
offensive tools, such as for generating malware, crafting phishing messages, or
launching cyberattacks. Recently, several studies have been proposed to address
these challenges, which are also reviewed in this survey; and 4) strategies for
constructing domain-specific LLMs have been proposed and are outlined in this
survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.

</details>


### [62] [Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era](https://arxiv.org/abs/2510.23457)
*Saleh Darzi, Mirza Masfiqur Rahman, Imtiaz Karim, Rouzbeh Behnia, Attila A Yavuz, Elisa Bertino*

**主要类别:** cs.CR

**AI概要:** 本文分析了将NIST后量子密码标准集成到5G基站认证中的性能问题，发现直接集成不可行，并提出基于分层身份门限签名的过渡方案BORG


<details>
  <summary>更多</summary>
  
**动机:** 5G协议在初始启动阶段缺乏强大的基站认证机制，易受伪基站攻击，现有传统方案无法抵御量子攻击，且后量子密码标准在5G中的适用性尚未探索

**方法:** 对NIST-PQC标准和传统数字签名方案（包括门限和身份基方案）在5G基站认证中的网络级性能进行全面表征分析

**结果:** 发现直接采用PQC受到协议约束和大签名尺寸的限制，传统方法因证书链开销存在性能限制

**结论:** 提出BORG作为过渡解决方案，具有事后量子伪造检测和分布式信任特性，适合5G严格需求，是未来量子弹性5G认证的有效过渡方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Authentication+Against+Insecure+Bootstrapping+for+5G+Networks%3A+Feasibility%2C+Resiliency%2C+and+Transitional+Solutions+in+Post-Quantum+Era，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23457&send_immediately=true&force_search=false)

**原文摘要:** The 5G protocol lacks a robust base station authentication mechanism during
the initial bootstrapping phase, leaving it susceptible to threats such as fake
base station attacks. Conventional solutions, including digital signatures
based on Public Key Infrastructures (PKIs) and identity-based signatures, are
inadequate against quantum-capable adversaries. While integrating NIST's
Post-Quantum Cryptography (PQC) standards is a leading approach for quantum
resistance, their suitability for 5G base station authentication remains
unexplored. Moreover, current solutions are predominantly centralized and lack
security features such as distributed authentication. This work presents, to
our knowledge, the first comprehensive network-level performance
characterization of integrating NIST-PQC standards and conventional digital
signatures (including threshold and identity-based schemes) into 5G base
station authentication. Our findings reveal significant feasibility concerns,
with direct PQC adoption hindered by protocol constraints and large signature
sizes. We also highlight the performance limitations of conventional methods
due to the overhead of certificate chains. To mitigate these challenges, we
propose BORG, a transitional authentication solution based on a Hierarchical
Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG
offers post-mortem post-quantum forgery detection and distributed trust via
threshold and compact signatures, well-suited for 5G's stringent requirements.
Our performance analysis underscores an important warning on the infeasibility
of direct PQC integration and positions BORG as an effective transitional
solution toward future quantum-resilient 5G authentication.

</details>


### [63] [Towards a Functionally Complete and Parameterizable TFHE Processor](https://arxiv.org/abs/2510.23483)
*Valentin Reyes Häusler, Gabriel Ott, Aruna Jayasena, Andreas Peter*

**主要类别:** cs.CR

**AI概要:** 提出基于FPGA的TFHE全同态加密硬件加速器，通过改进的可编程自举模块将性能提升240%-480%，解决TFHE计算开销高的问题


<details>
  <summary>更多</summary>
  
**动机:** TFHE全同态加密虽然自举操作快，但同态电路计算开销大，比未加密计算慢多个数量级，阻碍了其在敏感数据保护中的广泛应用

**方法:** 设计基于FPGA的TFHE处理器硬件加速器，实现完全在FPGA上处理数据的指令，并开发改进的可编程自举模块

**结果:** 自举操作性能比当前最优方案提升240%-480%，实现了高效、紧凑且可扩展的设计

**结论:** 该FPGA加速器为构建完整的TFHE处理器架构奠定了基础，有望推动全同态加密的实际应用

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Functionally+Complete+and+Parameterizable+TFHE+Processor，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2510.23483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23483&send_immediately=true&force_search=false)

**原文摘要:** Fully homomorphic encryption allows the evaluation of arbitrary functions on
encrypted data. It can be leveraged to secure outsourced and multiparty
computation. TFHE is a fast torus-based fully homomorphic encryption scheme
that allows both linear operations, as well as the evaluation of arbitrary
non-linear functions. It currently provides the fastest bootstrapping operation
performance of any other FHE scheme. Despite its fast performance, TFHE suffers
from a considerably higher computational overhead for the evaluation of
homomorphic circuits. Computations in the encrypted domain are orders of
magnitude slower than their unencrypted equivalents. This bottleneck hinders
the widespread adoption of (T)FHE for the protection of sensitive data. While
state-of-the-art implementations focused on accelerating and outsourcing single
operations, their scalability and practicality are constrained by high memory
bandwidth costs. In order to overcome this, we propose an FPGA-based hardware
accelerator for the evaluation of homomorphic circuits. Specifically, we design
a functionally complete TFHE processor for FPGA hardware capable of processing
instructions on the data completely on the FPGA. In order to achieve a higher
throughput from our TFHE processor, we implement an improved programmable
bootstrapping module which outperforms the current state-of-the-art by 240\% to
480\% more bootstrappings per second. Our efficient, compact, and scalable
design lays the foundation for implementing complete FPGA-based TFHE processor
architectures.

</details>
