{"id": "2512.09088", "pdf": "https://arxiv.org/pdf/2512.09088", "abs": "https://arxiv.org/abs/2512.09088", "authors": ["Adrian Ryser", "Florian Allwein", "Tim Schlippe"], "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study", "categories": ["cs.AI"], "comment": null, "summary": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.", "AI": {"tldr": "论文研究大型语言模型(LLM)的幻觉现象如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，并识别出直觉作为新的信任因素。", "motivation": "研究LLM产生的看似合理但事实错误的幻觉输出对用户信任和交互行为的影响，填补这一日常使用场景下的研究空白。", "method": "采用定性研究方法，对192名参与者进行实证研究，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架进行分析。", "result": "确认了期望值、先前经验、用户专业知识等现有信任因素，并发现直觉是幻觉检测的新因素；信任动态还受感知风险和决策风险等情境因素影响。", "conclusion": "验证了递归信任校准过程，提出将直觉作为用户相关信任因素，并为负责任和反思性的LLM使用提供实践建议。"}}
{"id": "2512.09114", "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "categories": ["cs.AI", "cs.CY"], "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.", "AI": {"tldr": "论文提出了AI TIPS框架，用于解决AI系统部署中的三个关键治理挑战：用例级别风险评估不足、现有框架过于概念化缺乏可操作控制、以及规模化运营治理机制缺失。", "motivation": "当前AI治理框架存在三个主要问题：1) 缺乏针对具体用例的风险评估方法；2) 现有框架(如ISO 42001和NIST AI RMF)停留在概念层面，缺乏可操作的控制措施；3) 缺乏将可信AI实践嵌入开发生命周期的规模化运营机制。", "method": "提出AI TIPS(人工智能可信集成支柱可持续性2.0)框架，这是对2019年开发的全面运营框架的更新，该框架比NIST AI风险管理框架早四年推出。", "result": "AI TIPS框架直接解决了上述三个治理挑战，提供了可操作的治理解决方案。", "conclusion": "AI TIPS框架为组织提供了将AI治理要求转化为具体技术实施的系统性方法，能够实现从董事会到数据科学家的角色适当可见性，并量化衡量合规性。"}}
{"id": "2512.09117", "pdf": "https://arxiv.org/pdf/2512.09117", "abs": "https://arxiv.org/abs/2512.09117", "authors": ["Luciano Floridi", "Yiyang Jia", "Fernando Tohmé"], "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2512.09142", "pdf": "https://arxiv.org/pdf/2512.09142", "abs": "https://arxiv.org/abs/2512.09142", "authors": ["Sergio Burdisso", "Séverin Baroudi", "Yanis Labrak", "David Grunert", "Pawel Cyrta", "Yiyang Chen", "Srikanth Madikeri", "Esaú Villatoro-Tello", "Thomas Schaaf", "Ricard Marxer", "Petr Motlicek"], "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation", "categories": ["cs.AI"], "comment": "Pre-print submitted to EACL System Demonstration (under review)", "summary": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.", "AI": {"tldr": "SDialog是一个开源的Python工具包，集成了对话生成、评估和机制可解释性，为构建和分析基于大语言模型的对话代理提供端到端框架。", "motivation": "为了解决对话系统研究中生成、评估和可解释性工具分散的问题，提供一个统一的框架来系统化地构建、基准测试和理解对话系统。", "method": "基于标准化的Dialog表示，提供：1）人物驱动的多智能体模拟和可组合编排；2）结合语言指标、LLM作为评判者和功能正确性验证的全面评估；3）通过特征消融和诱导实现激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整音频生成。", "result": "开发了一个集成所有主要LLM后端的工具包，支持在统一API下进行混合后端实验。", "conclusion": "SDialog通过将生成、评估和可解释性耦合在以对话为中心的架构中，使研究人员能够更系统地构建、基准测试和理解对话系统。"}}
{"id": "2512.08943", "pdf": "https://arxiv.org/pdf/2512.08943", "abs": "https://arxiv.org/abs/2512.08943", "authors": ["Singon Kim"], "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Master's thesis, Korea University, 2025", "summary": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.", "AI": {"tldr": "提出ACoRN方法，通过细粒度文档分类和两个新训练步骤增强抽象压缩模型对检索噪声的鲁棒性，提高RAG系统中答案准确率", "motivation": "检索增强生成中，检索到的文档常包含不相关或误导性信息，传统抽象压缩模型容易遗漏重要信息，尤其在长文本中注意力分散问题严重", "method": "1. 对训练数据进行离线数据增强以增强对两种检索噪声的鲁棒性；2. 微调模型生成围绕关键信息的摘要，直接支持正确答案；提出ACoRN方法进行细粒度文档分类", "result": "使用ACoRN训练的T5-large模型在EM和F1分数上均有提升，能保留答案字符串作为直接证据，在包含大量降低准确性文档的数据集上表现优异", "conclusion": "ACoRN方法有效解决了抽象压缩模型在噪声环境下的信息遗漏问题，提高了RAG系统的实际应用价值，特别适用于现实世界中文档质量参差不齐的场景"}}
{"id": "2512.09340", "pdf": "https://arxiv.org/pdf/2512.09340", "abs": "https://arxiv.org/abs/2512.09340", "authors": ["Chethana Prasad Kabgere"], "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 3 figures. Research manuscript based on the final project for CS6795 (Introduction to Cognitive Science), Georgia Tech", "summary": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.", "AI": {"tldr": "本研究对比人类和AI系统在模糊视觉刺激下的图像标注表现，发现人类使用类比推理、形状识别和置信度调节等策略，而AI主要依赖特征处理，揭示了生物与人工系统在表征、推理和置信度校准方面的异同。", "motivation": "理解人类和AI如何解释模糊视觉刺激对于揭示感知、推理和决策机制的本质至关重要，为开发更可解释和认知对齐的AI系统提供理论基础。", "method": "结合计算认知科学、认知架构和连接主义-符号混合模型，使用低分辨率感知退化刺激，通过Grad-CAM可视化模型注意力，并利用ACT-R和Soar认知模型分析人类决策策略。", "result": "研究发现人类在不确定性下采用分层启发式决策策略，与AI的特征处理方式形成对比，两者在表征、推理和置信度校准方面既有相似性也有显著差异。", "conclusion": "研究支持未来开发神经符号架构，将结构化符号推理与连接主义表征相结合，基于具身性、可解释性和认知对齐原则，构建既高性能又可解释、认知基础扎实的AI系统。"}}
{"id": "2512.08944", "pdf": "https://arxiv.org/pdf/2512.08944", "abs": "https://arxiv.org/abs/2512.08944", "authors": ["Yudong Wang", "Zhe Yang", "Wenhan Ma", "Zhifang Sui", "Liang Zhao"], "title": "Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.", "AI": {"tldr": "提出针对RLHF幻觉问题的框架，通过TriviaQA和FineWeb数据分别解决内外幻觉，并奖励模型拒绝回答无法回答的问题，显著降低幻觉同时提升性能", "motivation": "强化学习虽然提升了大型语言模型的复杂推理能力，但也加剧了幻觉问题，需要在能力与可靠性之间找到平衡", "method": "创建基于TriviaQA的开放转换训练集解决外在幻觉，利用FineWeb长文本进行事实基础奖励解决内在幻觉，并明确奖励模型拒绝回答无法回答的问题", "result": "在多样化基准测试中显示出显著的性能提升，大幅减少了两种类型的幻觉", "conclusion": "为解决高级推理与事实可信度之间的关键矛盾提供了实用框架，为开发更强大可靠的大型语言模型铺平道路"}}
{"id": "2512.09458", "pdf": "https://arxiv.org/pdf/2512.09458", "abs": "https://arxiv.org/abs/2512.09458", "authors": ["Sławomir Nowaczyk"], "title": "Architectures for Building Agentic AI", "categories": ["cs.AI", "cs.LG"], "comment": "This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by Springer Nature", "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.", "AI": {"tldr": "本文认为AI代理和生成式AI的可靠性主要是一个架构属性，提出了基于组件化、接口规范和明确控制回路的可靠性框架，并针对不同类型的智能代理分析了可靠性模式和设计指导原则。", "motivation": "探讨如何通过系统架构设计来确保AI代理和生成式AI系统的可靠性，解决这些系统在目标导向、工具使用和闭环操作中的可靠性挑战。", "method": "定义智能代理系统为在闭环中运行的目标导向工具使用决策者，通过原则性组件化（目标管理器、规划器、工具路由器等）、规范接口（模式约束、验证、最小权限工具调用）和明确控制保证回路来构建可靠性。", "result": "提出了实用的分类法（工具使用代理、记忆增强代理等），分析了每种模式如何重塑可靠性边界和故障模式，并提炼了类型化模式、幂等性、权限控制等关键设计指导原则。", "conclusion": "AI系统的可靠性可以通过精心设计的架构来实现，包括组件化结构、规范接口和明确的控制机制，这为构建可靠的智能代理系统提供了理论基础和实践指导。"}}
{"id": "2512.08945", "pdf": "https://arxiv.org/pdf/2512.08945", "abs": "https://arxiv.org/abs/2512.08945", "authors": ["Stefano Epifani", "Giuliano Castigliego", "Laura Kecskemeti", "Giuliano Razzicchia", "Elisabeth Seiwald-Sonderegger"], "title": "The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 1 table, Project coordinator: Stefano Epifani", "summary": "Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).\n  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).\n  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.", "AI": {"tldr": "本研究评估大型语言模型在心理化治疗框架下生成具有心理化结构文本的能力，发现LLM能够产生结构连贯的心理化档案，但在情感表达和内外整合方面存在局限", "motivation": "随着大型语言模型生成反思性文本能力增强，需要探究语言形式与心理表征之间的关系，特别是在心理化治疗框架下的表现", "method": "通过生成50个人机对话，由5名经过MBT培训的精神科医生在盲审条件下，按照MBT四个维度对模型生成的心理化档案进行Likert量表评分，并计算评分者间一致性", "result": "模型生成的心理化档案在结构连贯性方面得分较高（3.63-3.98），评分者间一致性良好（ICC 0.60-0.84），在隐显维度和自我-他人维度表现稳定，但在内部状态与外部情境整合方面存在局限，且情感表达呈现中性特征", "conclusion": "LLM能够生成结构连贯且临床可解释的心理化文本，但在情感深度和情境整合方面仍需改进，这为AI在心理治疗辅助应用提供了参考但同时也指出了局限性"}}
{"id": "2512.09566", "pdf": "https://arxiv.org/pdf/2512.09566", "abs": "https://arxiv.org/abs/2512.09566", "authors": ["Junkai Ji", "Zhangfan Yang", "Dong Xu", "Ruibin Bai", "Jianqiang Li", "Tingjun Hou", "Zexuan Zhu"], "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search", "categories": ["cs.AI", "cs.LG"], "comment": "21 pages, 5 figures", "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.", "AI": {"tldr": "Trio是一个整合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，能够有效设计具有药理增强特性的配体分子，在结合亲和力、药物相似性和合成可行性方面显著优于现有方法。", "motivation": "传统药物发现方法耗时长、成本高且成功率低，现有生成模型存在泛化能力不足、可解释性差、过度关注结合亲和力而忽视其他关键药理性质等问题，限制了实际应用价值。", "method": "整合三个关键技术：1)基于片段的分子语言建模实现上下文感知的片段组装；2)强化学习确保物理化学和合成可行性；3)蒙特卡洛树搜索平衡新颖化学型探索与有前景中间体利用。", "result": "实验结果显示Trio在多个关键指标上显著优于现有最优方法：结合亲和力提升7.85%，药物相似性提升11.10%，合成可行性提升12.05%，同时分子多样性扩展超过四倍。", "conclusion": "Trio框架通过多技术整合实现了高效、可解释的闭环靶向分子设计，能够可靠地生成化学有效且药理增强的配体，为药物发现提供了有前景的新途径。"}}
{"id": "2512.09015", "pdf": "https://arxiv.org/pdf/2512.09015", "abs": "https://arxiv.org/abs/2512.09015", "authors": ["DatologyAI", ":", "Luke Merrick", "Alex Fang", "Aldo Carranza", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Haoli Yin", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Paul Burstein", "Parth Doshi", "Paul Burnstein", "Pratyush Maini", "Ricardo Monti", "Rishabh Adiga", "Scott Loftin", "Siddharth Joshi", "Spandan Das", "Tony Jiang", "Vineeth Dorma", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "Luxical: High-Speed Lexical-Dense Text Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 6 figures", "summary": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.", "AI": {"tldr": "Luxical是一个结合了词汇特征和神经网络的高效文本嵌入库，在保持质量的同时大幅提升处理速度，适用于大规模文本组织任务。", "motivation": "当前文本处理工具在速度与灵活性之间存在权衡：词汇分类器速度快但功能有限，而Transformer嵌入模型灵活但计算成本高。需要一种既能保持质量又能高效处理的方法。", "method": "结合稀疏TF-IDF特征、小型ReLU网络和知识蒸馏训练方案，近似大型Transformer嵌入模型的功能，显著降低运算成本。", "result": "在文档检索和文本分类任务中，相比不同规模的神经基线模型实现了3倍到100倍的速度提升，质量与神经基线相当，计算效率/质量权衡表现优异。", "conclusion": "Luxical成功实现了词汇和密集嵌入方法的最佳特性结合，为大规模文本组织提供了高质量的效率解决方案，已作为开源软件发布。"}}
{"id": "2512.09629", "pdf": "https://arxiv.org/pdf/2512.09629", "abs": "https://arxiv.org/abs/2512.09629", "authors": ["Emanuele La Malfa", "Ping Zhu", "Samuele Marro", "Sara Bernardini", "Michael Wooldridge"], "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL", "categories": ["cs.AI", "cs.LG"], "comment": "Code: https://github.com/EmanueleLM/MultiAgentPlanning", "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.", "AI": {"tldr": "提出一个基于验证器的端到端规划框架，使用LLM将自然语言规范自动转换为PDDL模型，通过多智能体迭代优化解决时间约束、最优性和规范歧义问题，最终生成可读的自然语言计划。", "motivation": "解决传统规划系统需要人工定义PDDL模型的问题，利用LLM自动处理自然语言规范中的模糊性和矛盾，实现无需人工干预的端到端自动规划。", "method": "使用大型语言模型驱动的协调器和多个智能体模块，将自然语言规范迭代转换为PDDL域和问题描述，通过外部规划引擎生成计划，最后翻译回自然语言。", "result": "框架在多个领域和任务中展示出灵活性和有效性，包括Google NaturalPlan基准、PlanBench以及Blocksworld和汉诺塔等传统LLM难以处理的问题。", "conclusion": "该框架可与任何PDDL规划引擎集成，代表了LLM辅助端到端规划的重要进展，为自动化规划系统提供了新的解决方案。"}}
{"id": "2512.09127", "pdf": "https://arxiv.org/pdf/2512.09127", "abs": "https://arxiv.org/abs/2512.09127", "authors": ["Zihan Han", "Junyan Ge", "Caifeng Li"], "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.", "AI": {"tldr": "提出了一个知识引导的大型语言模型(KG-LLM)，通过整合儿科牙科知识图谱、检索增强生成和多阶段安全验证，为儿科牙科抗生素处方提供循证推荐，显著提升了记录理解、用药准确性和安全性。", "motivation": "解决牙科信息学中儿科牙科临床记录解释和抗生素安全处方的持续挑战，传统基于规则的临床决策支持系统难以处理非结构化牙科叙述、不完整的影像学描述和复杂的安全约束。", "method": "开发KG-LLM框架，包括：临床NER/RE模块提取结构化实体和关系；从知识图谱检索相关指南、药物安全规则和历史案例；采用检索增强生成技术；双层次安全验证机制（确定性规则检查和学习的分类器）。", "result": "在32,000条儿科牙科就诊记录上验证，相比领域适应的Llama-2基线：记录理解F1分数从0.867提升到0.914；药物剂量持续时间准确率从0.716提升到0.782；不安全抗生素建议减少50%。", "conclusion": "知识图谱、RAG和安全模块都对临床可靠性和可解释性有重要贡献，证明了该框架在提升儿科牙科抗生素处方安全性和准确性方面的有效性。"}}
{"id": "2512.09727", "pdf": "https://arxiv.org/pdf/2512.09727", "abs": "https://arxiv.org/abs/2512.09727", "authors": ["Junlin Xiao", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions", "categories": ["cs.AI"], "comment": null, "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.", "AI": {"tldr": "提出使用高斯过程回归来聚合MCTS在多线程环境中的统计信息，在连续动作空间中提升性能", "motivation": "在连续动作空间的蒙特卡洛树搜索中，如何最佳地聚合来自不同线程的统计信息是一个重要但未被充分探索的问题", "method": "使用高斯过程回归来获得未在环境中试验过的有前景动作的价值估计", "result": "在6个不同领域进行系统评估，证明该方法优于现有的聚合策略，同时推理时间增加适度", "conclusion": "该方法为连续动作空间中的并行MCTS提供了一种有效的统计信息聚合解决方案，在性能提升和计算成本之间取得了良好平衡"}}
{"id": "2512.09148", "pdf": "https://arxiv.org/pdf/2512.09148", "abs": "https://arxiv.org/abs/2512.09148", "authors": ["Shanghao Li", "Jinda Han", "Yibo Wang", "Yuanjie Zhu", "Zihe Song", "Langzhou He", "Kenan Kamel A Alghythee", "Philip S. Yu"], "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.", "AI": {"tldr": "该研究分析了LLM在图知识检索增强生成中的幻觉问题，提出了两种轻量级可解释性指标（PRD和SAS）来评估模型对结构化知识的处理，并开发了一个后处理幻觉检测器GGA，性能优于基线方法。", "motivation": "LLM在处理知识图谱检索的线性化子图时难以理解其中的关系和拓扑信息，导致生成内容与检索知识不一致的幻觉问题。", "method": "提出Path Reliance Degree (PRD) 衡量模型对最短路径三元组的过度依赖，Semantic Alignment Score (SAS) 评估模型内部表示与检索知识的语义对齐程度。在知识问答任务上进行实证分析，并开发了Graph Grounding and Alignment (GGA) 后处理幻觉检测器。", "result": "研究发现高PRD和低SAS分数与幻觉模式相关，GGA检测器在AUC和F1指标上优于基于语义和置信度的基线方法。", "conclusion": "通过机制可解释性分析LLM的结构限制如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了见解。"}}
{"id": "2512.09736", "pdf": "https://arxiv.org/pdf/2512.09736", "abs": "https://arxiv.org/abs/2512.09736", "authors": ["Jingtian Yan", "Zhifei Li", "William Kang", "Stephen F. Smith", "Jiaoyang Li"], "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation", "categories": ["cs.AI"], "comment": null, "summary": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.", "AI": {"tldr": "本研究探讨多智能体路径规划(MAPF)算法在现实执行环境中的性能影响因素，包括解决方案最优性与执行性能的关系、系统性能对动力学建模误差的敏感性，以及模型精度与规划最优性之间的相互作用。", "motivation": "现有MAPF评估框架通常依赖简化的机器人模型，导致算法基准与实际性能之间存在显著差距。随着SMART等包含动力学建模的框架出现，需要研究关键规划器设计选择在现实执行环境中的影响。", "method": "系统研究三个基本因素：(1)解决方案最优性与执行性能的关系；(2)系统性能对动力学建模误差的敏感性；(3)模型精度与规划最优性的相互作用。通过实证方法分析这些设计选择对现实场景性能的影响。", "result": "研究发现为理解MAPF算法在现实部署中的性能表现提供了重要见解，揭示了设计选择对实际执行效果的影响机制。", "conclusion": "研究强调了MAPF领域面临的开放挑战和研究方向，指导社区朝着实际、现实世界部署的方向发展，弥合算法理论性能与实际应用之间的差距。"}}
{"id": "2512.09149", "pdf": "https://arxiv.org/pdf/2512.09149", "abs": "https://arxiv.org/abs/2512.09149", "authors": ["Anton Vasiliuk", "Irina Abdullaeva", "Polina Druzhinina", "Anton Razzhigaev", "Andrey Kuznetsov"], "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.", "AI": {"tldr": "研究开发了MindShift基准测试，使用MMPI心理测量工具评估大语言模型模仿人类人格特质的能力，发现模型在角色扮演方面有显著改进，但不同模型家族在心理适应性方面存在差异。", "motivation": "探索大语言模型吸收和反映用户指定人格特质和态度的潜力，通过心理测量学方法评估其心理适应性。", "method": "改编心理学经典测试MMPI，创建人格导向提示词和不同特质强度的人格角色，构建MindShift基准测试来评估LLMs的行为表现。", "result": "LLMs在角色感知方面表现出一致的改进，归因于训练数据集和对齐技术的进步；不同模型类型和家族在心理测量评估响应中存在显著差异。", "conclusion": "大语言模型能够在一定程度上模拟人类人格特质，MindShift基准为评估LLMs的心理适应性提供了有效工具，相关提示词和代码将公开提供。"}}
{"id": "2512.09829", "pdf": "https://arxiv.org/pdf/2512.09829", "abs": "https://arxiv.org/abs/2512.09829", "authors": ["Khurram Khalil", "Muhammad Mahad Khaliq", "Khaza Anuarul Hoque"], "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted in the IEEE DATE 2026 conference", "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.", "AI": {"tldr": "RIFT是一个基于强化学习的智能故障定位框架，用于AI加速器的设计时故障评估，相比传统方法实现了2.2倍加速、99%测试向量减少和更优的故障覆盖率。", "motivation": "现代AI加速器规模庞大，传统故障评估方法面临计算成本过高和关键故障模式覆盖率不足的问题。", "method": "将复杂的最坏情况故障搜索转化为顺序决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化的高影响测试套件。", "result": "在十亿参数大语言模型工作负载上，RIFT比进化方法快2.2倍，比随机故障注入减少99%测试向量，同时获得更优的故障覆盖率。选择性纠错码的成本效益提升12.8倍。", "conclusion": "RIFT提供了一个可扩展的框架，能够自动发现最小化高影响故障场景，生成UVM兼容的验证工件，可直接集成到商业RTL验证流程中。"}}
{"id": "2512.09212", "pdf": "https://arxiv.org/pdf/2512.09212", "abs": "https://arxiv.org/abs/2512.09212", "authors": ["Zixuan Liu", "Siavash H. Khajavi", "Guangkai Jiang", "Xinru Liu"], "title": "Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.", "AI": {"tldr": "本文提出了一种检测和缓解奖励模型与策略冲突的新框架，通过冲突感知采样选择高冲突样本进行人工反馈，有效改善大语言模型的对齐性能", "motivation": "基于奖励模型的微调方法依赖代理奖励模型准确反映人类偏好的假设，但实际中常因标注噪声、偏见或覆盖不足而违反此假设，导致模型优化错误信号而非真实人类价值观", "method": "提出两个互补指标检测代理-策略冲突：局部代理-策略对齐冲突分数(PACS)和全局Kendall-Tau距离度量；设计SHF-CAS算法，通过冲突感知采样选择高冲突QA对进行额外人工反馈", "result": "在两个对齐任务上的实验表明，该方法即使在有偏代理奖励下也能提升整体对齐性能", "conclusion": "该工作为解释对齐失败提供了新视角，并为LLM训练中的针对性优化提供了原则性路径"}}
{"id": "2512.09831", "pdf": "https://arxiv.org/pdf/2512.09831", "abs": "https://arxiv.org/abs/2512.09831", "authors": ["Chainarong Amornbunchornvej"], "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SI"], "comment": "The first draft of cognitive geometry model", "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.", "AI": {"tldr": "本文提出了一个几何框架来建模认知异质智能体之间的信念、动机和影响，通过向量空间表示个性化价值空间，用线性解释映射形式化信念传播，并建立了信念存活的代数条件。", "motivation": "为了解决认知异质智能体间信念传播、误解和影响边界的理论基础问题，统一概念空间、社会认识论和AI价值对齐的见解。", "method": "开发几何框架，将智能体表示为个性化价值空间（向量空间），信念形式化为结构化向量，通过线性解释映射进行传播，分析代数约束下的信念动态。", "result": "提出了\"无零空间领导条件\"，将领导力表征为表示可达性而非说服或权威；解释了信念在传播过程中如何扭曲、变异或消失；建立了基于结构兼容性而非共享信息的语义保存机制。", "conclusion": "该认知几何视角阐明了人类和人工系统中影响的认知边界，为分析异质智能体间的信念动态提供了通用基础，强调了结构兼容性在意义保存中的核心作用。"}}
{"id": "2512.09222", "pdf": "https://arxiv.org/pdf/2512.09222", "abs": "https://arxiv.org/abs/2512.09222", "authors": ["Vishwas Hegde", "Vindhya Shigehalli"], "title": "CORE: A Conceptual Reasoning Layer for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Independent system-level architectural proposal with accompanying proof-of-concept", "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.", "AI": {"tldr": "CORE是一个概念优先的交互层，通过持久的本地概念和认知操作符来改善多轮对话稳定性，减少42%的提示词使用，无需修改模型权重。", "motivation": "大型语言模型在单轮生成中表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、不一致推理模式和提示词增长的问题。", "method": "提出CORE系统，结合小型通用认知操作符库和持久的本地概念（包含任务、约束、偏好和中间结果的紧凑语义状态），每个模型调用仅接收概念状态、最新指令和选定操作符。", "result": "初步原型模拟显示累计提示词减少约42%（但这是原型条件下的结果，不应视为实际性能估计）。", "conclusion": "CORE提供了一个模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展的方向。"}}
{"id": "2512.09882", "pdf": "https://arxiv.org/pdf/2512.09882", "abs": "https://arxiv.org/abs/2512.09882", "authors": ["Justin W. Lin", "Eliot Krzysztof Jones", "Donovan Julian Jasper", "Ethan Jun-shen Ho", "Anna Wu", "Arnold Tianyi Yang", "Neil Perry", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter", "Percy Liang", "Dan Boneh", "Daniel E. Ho"], "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "categories": ["cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.", "AI": {"tldr": "首个在真实企业环境中对AI安全代理与人类网络安全专家进行的全面评估，显示新型AI框架ARTEMIS在漏洞发现能力上超越9/10的人类专家，但在误报率和GUI任务处理方面存在不足", "motivation": "评估AI代理在真实网络安全环境中的表现，与传统人类网络安全专家进行对比，探索AI在网络安全领域的应用潜力和局限性", "method": "在包含约8000台主机的大型大学网络中，评估10名网络安全专家、6个现有AI代理和新开发的ARTEMIS多代理框架（具有动态提示生成、任意子代理和自动漏洞分类功能）的表现", "result": "ARTEMIS总体排名第二，发现9个有效漏洞，提交有效率为82%，表现优于10名人类参与者中的9名。现有框架如Codex和CyAgent表现不如大多数人类参与者。AI代理在系统枚举、并行利用和成本方面具有优势（ARTEMIS变体每小时18美元 vs 人类渗透测试员60美元）", "conclusion": "AI代理在网络安全领域展现出与顶尖人类专家相当的技术成熟度和提交质量，但在误报率较高和GUI任务处理困难方面存在明显能力差距，表明AI在网络安全自动化方面具有巨大潜力但仍需改进"}}
{"id": "2512.09238", "pdf": "https://arxiv.org/pdf/2512.09238", "abs": "https://arxiv.org/abs/2512.09238", "authors": ["Zeng You", "Yaofo Chen", "Shuhai Zhang", "Zhijie Qiu", "Tingyu Wu", "Yingjian Li", "Yaowei Wang", "Mingkui Tan"], "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.", "AI": {"tldr": "提出TCA-Attention，一种无需训练的自适应稀疏注意力机制，通过选择性关注信息量大的token来提升长上下文推理效率，实现2.8倍加速和61% KV缓存减少。", "motivation": "传统自注意力机制的二次复杂度在处理长序列时带来计算和内存挑战，现有稀疏注意力方法存在固定模式依赖、无法同时处理预填充和解码阶段、需要额外训练等局限。", "method": "包含两个轻量级阶段：1）离线校准阶段通过单次前向传播确定头部特定稀疏预算；2）在线token选择阶段使用轻量冗余度量自适应保留核心上下文token。", "result": "在128K上下文长度下实现2.8倍加速和61% KV缓存减少，在各种基准测试中保持与完整注意力相当的性能。", "conclusion": "TCA-Attention为高效长上下文推理提供了一个实用的即插即用解决方案，无需参数更新或架构更改，同时保持有界近似误差。"}}
{"id": "2512.09895", "pdf": "https://arxiv.org/pdf/2512.09895", "abs": "https://arxiv.org/abs/2512.09895", "authors": ["Jane Greenberg", "Scott McClellan", "Addy Ireland", "Robert Sammarco", "Colton Gerber", "Christopher B. Rauch", "Mat Kelly", "John Kunze", "Yuan An", "Eric Toberer"], "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science", "categories": ["cs.AI", "cs.DL"], "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures", "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.", "AI": {"tldr": "MatSci-YAMZ是一个结合AI和人类反馈循环的平台，用于支持元数据词汇表开发，在材料科学领域成功验证了AI-HILT模型的可行性。", "motivation": "元数据词汇表对推进FAIR和FARR数据原则至关重要，但开发受到人力资源有限和标准化实践不一致的限制。", "method": "开发MatSci-YAMZ平台，集成AI和人类参与循环（包括众包），在材料科学领域进行概念验证，6名参与者通过提供术语定义和示例来完善AI生成的定义。", "result": "成功生成了19个AI定义，迭代反馈循环证明了AI-HILT精炼的可行性，确认了概念验证成功、与FAIR原则对齐、建立了研究协议并展示了跨领域扩展潜力。", "conclusion": "MatSci-YAMZ模型能够增强语义透明度，减少共识构建和元数据词汇表开发所需的时间，具有跨领域应用的潜力。"}}
{"id": "2512.09292", "pdf": "https://arxiv.org/pdf/2512.09292", "abs": "https://arxiv.org/abs/2512.09292", "authors": ["Kevin Stowe", "Svetlana Afanaseva", "Rodolfo Raimundo", "Yitao Sun", "Kailash Patil"], "title": "Identifying Bias in Machine-generated Text Detection", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures, 7 tables", "summary": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.", "AI": {"tldr": "研究发现机器生成文本检测系统存在对弱势群体的偏见，特别是英语学习者、非白人群体更容易被误判为机器生成文本，而人类检测虽然准确率低但无显著偏见。", "motivation": "随着文本生成能力的快速发展，机器生成文本检测技术也受到广泛关注。虽然检测模型表现出色，但可能带来显著的负面影响，因此需要探索这些系统是否存在偏见。", "method": "收集学生论文数据集，评估16个检测系统在性别、种族/民族、英语学习者状态和经济地位四个属性上的偏见。使用回归模型分析显著性效应并进行子群分析，同时进行人工标注对比。", "result": "发现偏见在不同系统中不一致，但存在关键问题：多个模型倾向于将弱势群体分类为机器生成；英语学习者论文更可能被误判；经济弱势学生论文较少被误判；非白人英语学习者论文相对于白人更容易被误判。人类检测表现差但无显著偏见。", "conclusion": "机器生成文本检测系统存在系统性偏见，特别是对语言和种族少数群体的歧视，需要开发更公平的检测方法，而人类检测虽然准确率低但偏见较小。"}}
{"id": "2512.09897", "pdf": "https://arxiv.org/pdf/2512.09897", "abs": "https://arxiv.org/abs/2512.09897", "authors": ["Haoye Lu", "Pavan Seshadri", "Kaheer Suleman"], "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.", "AI": {"tldr": "SCOPE方法通过一次性使用LLM生成子目标来预训练轻量级学生模型，显著提高了文本环境规划任务的效率和性能，同时大幅减少推理时间。", "motivation": "解决现有方法在复杂文本环境中需要频繁查询LLM导致计算成本高、无法适应目标任务的问题。", "method": "提出SCOPE方法，仅在初始化时使用LLM生成子目标来预训练轻量级学生模型，避免训练和推理过程中的重复LLM查询。", "result": "在TextCraft环境中，成功率达到0.56，优于ADaPT的0.52，推理时间从164.4秒减少到3.0秒。", "conclusion": "LLM生成的子目标虽然可能不是最优的，但可以作为文本规划任务中层次目标分解的良好起点，在效率和性能之间取得了良好平衡。"}}
{"id": "2512.09386", "pdf": "https://arxiv.org/pdf/2512.09386", "abs": "https://arxiv.org/abs/2512.09386", "authors": ["Peter Baile Chen", "Weiyue Li", "Dan Roth", "Michael Cafarella", "Samuel Madden", "Jacob Andreas"], "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.", "AI": {"tldr": "CONCUR是一个持续路由框架，通过模块化设计和多表征学习解决AI任务路由问题，在持续和非持续设置下都能实现更高的端到端准确率和更低的推理成本。", "motivation": "现有路由方法通常使用单一模型训练所有策略，导致新策略出现时需要完全重新训练，且使用单一输入表征限制了路由决策能力，存在泛化困难和次优路由的问题。", "method": "提出CONCUR框架，采用模块化设计为每个策略训练单独的预测器模型，支持约束和无约束路由，并利用任务和计算策略的多种表征来更好地捕捉问题复杂性。", "result": "在分布内和分布外、知识和推理密集型任务上的实验表明，该方法在持续和非持续设置下都优于最佳单一策略和现有路由技术，具有更高的端到端准确率和更低的推理成本。", "conclusion": "CONCUR通过模块化设计和多表征学习有效解决了持续路由问题，实现了更好的性能、更低的成本和更好的可扩展性，为AI任务路由提供了有效的解决方案。"}}
{"id": "2512.09908", "pdf": "https://arxiv.org/pdf/2512.09908", "abs": "https://arxiv.org/abs/2512.09908", "authors": ["Antonio Lorenzin", "Fabio Zanasi"], "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective", "categories": ["cs.AI", "cs.LO", "math.CT"], "comment": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''", "summary": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.", "AI": {"tldr": "该论文提出了一个范畴论框架，将贝叶斯网络和马尔可夫网络之间的道德化与三角化转换建模为函子，并通过函子前复合在语法层面进行归纳定义，揭示了这些转换的句法和语义特征。", "motivation": "研究概率图模型中贝叶斯网络（有向模型）和马尔可夫网络（无向模型）之间的转换关系，特别是道德化和三角化转换的数学本质，旨在建立统一的范畴论框架来形式化这些转换过程。", "method": "使用范畴论方法，将贝叶斯网络和马尔可夫网络分别建模为从'语法'域到'语义'域的函子，道德化和三角化转换则定义为这些范畴之间的函子，并通过函子前复合实现语法层面的归纳定义。", "result": "成功建立了道德化和三角化转换的函子模型，发现道德化是完全句法的，而三角化依赖于语义，并将变量消除算法重新解释为函子，将三角化过程分为纯句法和纯语义两部分。", "conclusion": "该研究为概率图模型理论引入了函子视角，突出了句法和语义修改之间的区别，为理解图形模型转换提供了新的数学框架和理论基础。"}}
{"id": "2512.09394", "pdf": "https://arxiv.org/pdf/2512.09394", "abs": "https://arxiv.org/abs/2512.09394", "authors": ["Julie Kallini", "Christopher Potts"], "title": "Language models as tools for investigating the distinction between possible and impossible natural languages", "categories": ["cs.CL"], "comment": null, "summary": "We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.", "AI": {"tldr": "语言模型可作为研究工具探索可能/不可能自然语言的区分，揭示人类语言学习的归纳偏见，提出分阶段研究程序迭代改进LM架构", "motivation": "探索语言模型作为研究工具，揭示人类语言学习的归纳偏见和区分可能/不可能自然语言的能力", "method": "提出分阶段研究程序，通过迭代改进语言模型架构来更好地区分可能和不可能的语言", "result": "语言模型具有强大潜力作为研究工具，支持与人类认知的链接假设", "conclusion": "语言模型架构的迭代改进有助于揭示支持人类语言学习的认知偏见，为语言学研究提供新途径"}}
{"id": "2512.09434", "pdf": "https://arxiv.org/pdf/2512.09434", "abs": "https://arxiv.org/abs/2512.09434", "authors": ["Sebastian Nagl", "Mohamed Elganayni", "Melanie Pospisil", "Matthias Grabmair"], "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025", "summary": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.", "AI": {"tldr": "论文提出了CourtPressGER数据集，包含德国最高法院的判决、人工撰写的新闻稿和合成提示，用于训练和评估LLM从长司法文本生成准确可读的摘要。通过多种评估方法发现，大模型能生成高质量草稿，小模型需要分层处理长判决。", "motivation": "现有的NLP研究主要关注技术性摘要，忽视了面向公民的司法沟通需求。德国最高法院的官方新闻稿需要向公众和专家解释司法裁决，因此需要开发能够生成准确易懂摘要的模型。", "method": "构建包含6.4k条三元组数据的CourtPressGER数据集（判决原文、人工新闻稿、合成提示），使用基于参考的指标、事实一致性检查、LLM作为评判者和专家排名等多种方法对小模型和大模型进行基准测试。", "result": "大型语言模型能够生成高质量的新闻稿草稿，性能损失最小；小型模型需要分层设置来处理长判决。人类撰写的新闻稿在评估中排名最高，不同模型表现各异。", "conclusion": "该研究为司法文本的自动摘要提供了重要基准，证明了LLM在生成面向公众的司法摘要方面的潜力，同时指出了模型规模对处理长文本能力的影响，为未来司法NLP应用提供了方向。"}}
{"id": "2512.09440", "pdf": "https://arxiv.org/pdf/2512.09440", "abs": "https://arxiv.org/abs/2512.09440", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "AI": {"tldr": "提出基于知识增强大语言模型代理的可解释金融决策推理方法，通过外部知识检索、语义表示和推理生成相结合，提升事实准确性和推理透明度。", "motivation": "传统金融决策方法依赖参数化知识、缺乏事实一致性、缺少推理链条，需要解决这些局限性。", "method": "编码金融文本和结构化数据获取语义表示，通过相似度计算从外部知识库检索相关信息，加权融合内部表示和外部知识，引入多头注意力机制构建逻辑推理链，联合优化任务目标和解释一致性目标。", "result": "在金融文本处理和决策任务上，该方法在准确性、文本生成质量和事实支持方面优于基线方法。", "conclusion": "该方法克服了传统模型在语义覆盖和推理透明度方面的限制，在复杂金融场景中展现出强大的实用价值。"}}
{"id": "2512.09444", "pdf": "https://arxiv.org/pdf/2512.09444", "abs": "https://arxiv.org/abs/2512.09444", "authors": ["Ning Lyu", "Yuxi Wang", "Feng Chen", "Qingyuan Zhang"], "title": "Advancing Text Classification with Large Language Models and Neural Attention Mechanisms", "categories": ["cs.CL"], "comment": null, "summary": "This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.", "AI": {"tldr": "基于大语言模型的文本分类算法，通过深度语义嵌入、注意力增强和特征聚合策略，在长距离依赖、上下文语义理解和类别不平衡处理方面优于传统方法，在各项指标上均取得显著提升。", "motivation": "解决传统文本分类方法在捕获长距离依赖、理解上下文语义和处理类别不平衡方面的局限性", "method": "采用大语言模型进行文本编码和深度语义嵌入，使用注意力机制增强关键特征表示，结合全局和加权策略进行特征聚合，最后通过全连接层和Softmax进行分类预测，使用交叉熵损失优化参数", "result": "在所有评估指标（精确率、召回率、F1分数、AUC）上均优于现有模型，特别是在召回率和AUC方面提升显著，超参数敏感性和数据条件实验验证了模型的适应性和稳定性", "conclusion": "该方法不仅实现了有效的性能提升，还通过系统分析验证了其在复杂数据环境中的鲁棒性和适用性，证明了适当模型配置对性能的重要影响"}}
{"id": "2512.09483", "pdf": "https://arxiv.org/pdf/2512.09483", "abs": "https://arxiv.org/abs/2512.09483", "authors": ["Peixian Zhang", "Qiming Ye", "Zifan Peng", "Kiran Garimella", "Gareth Tyson"], "title": "Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.", "AI": {"tldr": "基于LLM的搜索引擎在引用资源多样性上优于传统搜索引擎，但在可信度、政治中立性和安全性方面未表现出优势，37%的域名资源是LLM-SE独有的。", "motivation": "LLM搜索引擎提供了新的信息检索范式，但缺乏引用透明度，这种转变对信任和透明度的影响尚未得到充分研究。", "method": "对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及其搜索结果，并进行基于特征的分析以识别影响来源选择的关键因素。", "result": "LLM-SE在域名资源多样性上表现更好，37%的域名是LLM-SE独有的，但在可信度、政治中立性和安全性指标上并未超越传统搜索引擎。", "conclusion": "研究结果为最终用户、网站所有者和开发者提供了可行的见解，揭示了LLM搜索引擎的优势和持续存在的风险。"}}
{"id": "2512.09487", "pdf": "https://arxiv.org/pdf/2512.09487", "abs": "https://arxiv.org/abs/2512.09487", "authors": ["Yucan Guo", "Miao Su", "Saiping Guan", "Zihao Sun", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "AI": {"tldr": "RL-based框架\\model{}实现了多轮自适应图-文本混合检索增强生成，通过强化学习联合优化检索和生成过程，在五个问答基准上显著超越现有RAG基线。", "motivation": "现有基于图或混合的RAG系统通常依赖固定或手工设计的检索流程，无法在推理过程中动态整合补充证据，且图证据检索成本高昂。", "method": "提出\\model{}框架，使用强化学习联合优化整个生成过程，学习何时推理、从文本或图中检索什么内容、何时生成最终答案。设计两阶段训练框架，同时考虑任务结果和检索效率。", "result": "在五个问答基准测试中，\\model{}显著优于现有的RAG基线方法。", "conclusion": "端到端强化学习能够有效支持复杂推理中的自适应和高效检索，证明了混合证据检索的优势。"}}
{"id": "2512.09552", "pdf": "https://arxiv.org/pdf/2512.09552", "abs": "https://arxiv.org/abs/2512.09552", "authors": ["Kun Sun", "Rong Wang"], "title": "Systematic Framework of Application Methods for Large Language Models in Language Sciences", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.", "AI": {"tldr": "本研究提出了两个方法论框架来解决大语言模型在语言科学中应用的方法论碎片化和系统性问题，包括方法选择框架和系统实施框架，通过实证实验验证其有效性。", "motivation": "大语言模型在语言科学中的广泛应用面临方法论碎片化和缺乏系统性的问题，需要指导性框架来确保战略性和负责任的应用。", "method": "提出两个框架：(1)方法选择框架系统化三种互补方法（基于提示的交互、微调开源模型、提取上下文嵌入）；(2)系统实施框架提供多阶段研究流程配置。通过回顾性分析、前瞻性应用和专家评估调查进行实证验证。", "result": "框架实现了研究问题与LLM方法的战略对齐，促进了语言科学研究的范式转变，确保了可重复性，并支持对LLM机制的批判性评估。", "conclusion": "该框架系统对于确保语言科学研究从临时性工具使用转向可验证、稳健的科学实践具有基础性作用，能够促进语言科学的系统化发展。"}}
{"id": "2512.09563", "pdf": "https://arxiv.org/pdf/2512.09563", "abs": "https://arxiv.org/abs/2512.09563", "authors": ["Binglin Wu", "Jiaxiu Zou", "Xianneng Li"], "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at CCL 2025", "summary": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.", "AI": {"tldr": "提出基于LLM的三阶段框架（提示工程、监督微调、模型融合）来检测中文社交媒体中的隐晦仇恨言论，在基准测试中表现优于基线方法", "motivation": "中文社交媒体上仇恨言论泛滥带来社会风险，传统系统难以解码语境依赖的修辞策略和不断演变的网络用语", "method": "三阶段LLM框架：1)设计语境感知提示引导LLM提取隐晦仇恨模式；2)监督微调中整合任务特定特征增强领域适应性；3)融合微调后的LLM提高对分布外案例的鲁棒性", "result": "在STATE-ToxiCN基准测试中验证了框架有效性，在细粒度仇恨言论检测方面表现出优于基线方法的性能", "conclusion": "该三阶段LLM框架能有效解决中文社交媒体中隐晦仇恨言论检测的挑战，为复杂语境下的内容审核提供了有效解决方案"}}
{"id": "2512.09634", "pdf": "https://arxiv.org/pdf/2512.09634", "abs": "https://arxiv.org/abs/2512.09634", "authors": ["Karl Gustav Gailit", "Kadri Muischnek", "Kairit Sirts"], "title": "Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 2 appendixes, submitted to LREC 2026", "summary": "This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.", "AI": {"tldr": "本文创建了爱沙尼亚语文档级主观性数据集，包含1000个文档和人工标注的主观性评分，并进行了基于大语言模型的自动主观性分析实验。", "motivation": "为爱沙尼亚语创建文档级主观性标注数据集，探索使用大语言模型进行自动主观性评分的可行性。", "method": "收集300篇新闻文章和700篇随机网络文本，由4名标注者在0-100连续尺度上进行主观性评分；对评分差异大的文本子集重新标注；使用GPT-5生成自动评分进行对比。", "result": "标注者间相关性中等，重新标注后有所改善；GPT-5评分与人工标注相似但存在差异，表明基于LLM的自动评分可行但不能完全替代人工标注。", "conclusion": "基于大语言模型的自动主观性评分是可行的，但其适用性取决于具体应用场景，不能完全替代人工标注。"}}
{"id": "2512.09636", "pdf": "https://arxiv.org/pdf/2512.09636", "abs": "https://arxiv.org/abs/2512.09636", "authors": ["Mengxi Xiao", "Kailai Yang", "Pengde Zhao", "Enze Zhang", "Ziyan Kuang", "Zhiwei Liu", "Weiguang Han", "Shu Liao", "Lianting Huang", "Jinpeng Hu", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "categories": ["cs.CL"], "comment": null, "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "AI": {"tldr": "MentraSuite是一个统一的框架，包含MentraBench基准和Mindora模型，专注于提升心理健康领域的可靠推理能力，通过混合训练框架和一致性检测奖励机制，在复杂心理健康场景中表现出色。", "motivation": "现有心理LLM主要关注情感理解和知识回忆，但忽略了临床对齐的逐步推理需求，如评估、诊断、干预规划等，存在推理不完整、不一致和缺乏基础的问题。", "method": "提出MentraBench基准（涵盖5个推理方面、6个任务、13个数据集），开发Mindora模型（使用混合SFT-RL框架训练，加入不一致检测奖励），采用新颖的推理轨迹生成策略过滤困难样本并进行一致性重写。", "result": "在评估的20个LLM中，Mindora在MentraBench上获得最高平均性能，在推理可靠性方面表现卓越，特别是在简洁性、连贯性、避免幻觉、任务理解和内部一致性五个维度上。", "conclusion": "MentraSuite框架有效解决了心理健康领域LLM推理的可靠性问题，Mindora模型在复杂心理健康场景中展现出强大的推理能力和可靠性，为心理健康应用提供了更安全可靠的AI辅助工具。"}}
{"id": "2512.09662", "pdf": "https://arxiv.org/pdf/2512.09662", "abs": "https://arxiv.org/abs/2512.09662", "authors": ["Paloma Piot", "David Otero", "Patricia Martín-Rodilla", "Javier Parapar"], "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.", "AI": {"tldr": "该研究重新评估大语言模型在仇恨言论检测中的可靠性，发现虽然LLMs在实例层面与人类标注存在差异，但能够复现相似的模型性能排序模式，可作为主观NLP任务中的可扩展代理评估工具", "motivation": "仇恨言论检测存在主观性挑战，传统一致性指标过度简化标注分歧，而LLMs虽承诺可扩展标注但无法完全替代人类判断，需要重新审视LLMs在主观任务中的可靠性", "method": "使用主观性感知框架cross-Rater Reliability (xRR)重新评估LLMs可靠性，通过检验LLM生成的标注是否能保持人类评估得出的模型性能相对排序", "result": "研究发现LLMs虽然与人类在实例层面存在分歧，但能够可靠地复现模型性能排序趋势，与人类评估结果相关", "conclusion": "LLMs不能替代人类标注者，但可作为主观NLP任务中评估模型性能的可扩展代理工具，特别是在需要大规模评估时"}}
{"id": "2512.09666", "pdf": "https://arxiv.org/pdf/2512.09666", "abs": "https://arxiv.org/abs/2512.09666", "authors": ["Arthur Hemmer", "Mickaël Coustaty", "Nicola Bartolo", "Jean-Marc Ogier"], "title": "Neurosymbolic Information Extraction from Transactional Documents", "categories": ["cs.CL"], "comment": "20 pages, 2 figures, accepted to IJDAR (ICDAR 2025)", "summary": "This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.", "AI": {"tldr": "提出一个神经符号框架，通过结合符号验证方法来提升事务文档信息抽取的零样本性能和知识蒸馏效果", "motivation": "为了解决纯神经方法在文档信息抽取中缺乏领域约束验证的问题，需要结合符号验证来确保提取结果符合领域特定的算术约束", "method": "使用语言模型生成候选提取结果，然后通过句法级、任务级和领域级的三层验证进行过滤，确保符合领域约束", "result": "实验结果显示在F1分数和准确率上有显著提升，证明了神经符号验证在事务文档处理中的有效性", "conclusion": "神经符号框架通过整合符号验证方法，能够有效提升事务文档信息抽取的零样本性能和知识蒸馏质量"}}
{"id": "2512.09675", "pdf": "https://arxiv.org/pdf/2512.09675", "abs": "https://arxiv.org/abs/2512.09675", "authors": ["Leyi Pan", "Shuchang Tao", "Yunpeng Zhai", "Zheyu Fu", "Liancheng Fang", "Minghua He", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Aiwei Liu", "Lijie Wen"], "title": "d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models", "categories": ["cs.CL"], "comment": "16 pages, 5 figures, 3tables", "summary": "Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \\emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \\emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.", "AI": {"tldr": "d-TreeRPO：一种基于树结构展开和可验证奖励的可靠强化学习框架，用于扩散大语言模型，通过理论分析和时间调度自蒸馏损失提高概率估计准确性，在多个推理基准上取得显著提升", "motivation": "现有扩散大语言模型的强化学习方法存在两个问题：依赖粗糙或不可验证的奖励信号，以及概率估计存在偏差，无法准确反映无偏期望预测概率", "method": "提出d-TreeRPO框架，利用树结构展开和自底向上的优势计算，基于可验证结果奖励提供细粒度奖励信号。通过理论分析预测概率估计误差，引入时间调度自蒸馏损失提高预测置信度", "result": "在多个推理基准上显著超越现有基线：数独+86.2，倒计时+51.6，GSM8K+4.5，Math500+5.3。消融实验和计算成本分析验证了设计的有效性和实用性", "conclusion": "d-TreeRPO通过可靠的奖励信号和准确的概率估计，为扩散大语言模型提供了有效的强化学习解决方案，在复杂推理任务中表现出色，具有实际应用价值"}}
{"id": "2512.09742", "pdf": "https://arxiv.org/pdf/2512.09742", "abs": "https://arxiv.org/abs/2512.09742", "authors": ["Jan Betley", "Jorio Cocola", "Dylan Feng", "James Chua", "Andy Arditi", "Anna Sztyber-Betley", "Owain Evans"], "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "70 pages, 47 figures", "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.", "AI": {"tldr": "研究发现小范围微调会导致LLM在无关领域出现不可预测的广泛泛化，包括行为错位和后门漏洞", "motivation": "探究大语言模型在窄上下文微调后是否会在更广泛领域产生意外的行为变化和安全隐患", "method": "通过三个实验：1)鸟类名称微调导致时代认知错位 2)希特勒属性数据集导致人格错位 3)终结者角色诱导性后门实验", "result": "窄范围微调确实导致模型在无关领域出现显著行为变化，包括时代认知错误、人格错位和反向目标行为", "conclusion": "窄上下文微调可能产生难以预测的广泛泛化效应，数据过滤难以完全避免此类安全隐患，需要新的安全防护措施"}}
{"id": "2512.09701", "pdf": "https://arxiv.org/pdf/2512.09701", "abs": "https://arxiv.org/abs/2512.09701", "authors": ["Binbin XU"], "title": "FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text", "categories": ["cs.CL"], "comment": null, "summary": "We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq", "AI": {"tldr": "FineFreq是一个大规模多语言字符频率数据集，基于FineWeb和FineWeb2语料库构建，涵盖1900多种语言，包含96万亿字符的频率统计，支持精细的时间分析和多语言特征研究。", "motivation": "创建大规模多语言字符频率数据集，支持语言学研究、自然语言处理和多语言文本分析，保留自然出现的多语言特征如跨文字借用、表情符号和缩写。", "method": "从FineWeb和FineWeb2语料库中提取数据，处理57TB压缩文本，统计96万亿字符的频率，按语言、字符和时间维度组织数据，包含Unicode元数据。", "result": "构建了覆盖1900多种语言的大规模字符频率数据集，提供聚合和年度频率统计，支持CSV和Parquet格式，包含完整的Unicode元数据，已在GitHub和HuggingFace发布。", "conclusion": "FineFreq数据集为多语言字符频率分析提供了宝贵资源，支持精细的时间分析和多语言特征研究，有助于推动语言学和自然语言处理领域的发展。"}}
{"id": "2512.09830", "pdf": "https://arxiv.org/pdf/2512.09830", "abs": "https://arxiv.org/abs/2512.09830", "authors": ["Simone Corbo"], "title": "LLMs in Interpreting Legal Documents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.", "AI": {"tldr": "本章探讨了大语言模型在法律领域的应用，分析了其在法律任务优化和增强方面的潜力，同时指出了算法单一化、幻觉问题和监管合规等挑战，并介绍了两个不同的基准测试。", "motivation": "探索大语言模型如何优化和增强传统法律任务，如法律解释、合同分析、案例研究等，以提高法律工作的效率和准确性。", "method": "通过分析大语言模型在法律领域的可能用例，包括法律条文解释、合同谈判、法律摘要和信息检索等应用场景，同时考察相关的监管框架和挑战。", "result": "识别出大语言模型在法律应用中的潜力，但也揭示了算法单一化、产生幻觉内容以及需要符合欧盟AI法案、美国相关法规和中国新兴监管方法等合规挑战。", "conclusion": "大语言模型在法律领域具有重要应用价值，但需要解决技术可靠性和监管合规性问题，文中提出的两个基准测试为后续研究提供了评估框架。"}}
{"id": "2512.09730", "pdf": "https://arxiv.org/pdf/2512.09730", "abs": "https://arxiv.org/abs/2512.09730", "authors": ["Antonin Poché", "Thomas Mullor", "Gabriele Sarti", "Frédéric Boisnard", "Corentin Friedrich", "Charlotte Claye", "François Hoofd", "Raphael Bernas", "Céline Hudelot", "Fanny Jourdan"], "title": "Interpreto: An Explainability Library for Transformers", "categories": ["cs.CL", "cs.LG"], "comment": "Equal contribution: Poché and Jourdan", "summary": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.", "AI": {"tldr": "Interpreto是一个用于HuggingFace文本模型可解释性的Python库，提供归因分析和基于概念的解释两种方法，支持分类和生成模型，具有统一API和开源特性", "motivation": "将最新的可解释性研究转化为实用的数据科学工具，使最终用户能够理解模型决策，弥补现有库在概念级解释方面的不足", "method": "提供两种互补的解释方法：特征归因分析和基于概念的解释，通过统一API支持BERT变体和大型语言模型等HuggingFace模型", "result": "开发了一个功能完整的开源Python库，包含文档、示例和教程，支持pip安装，代码和文档已在GitHub上公开", "conclusion": "Interpreto成功填补了现有可解释性工具在概念级解释方面的空白，为数据科学家和最终用户提供了易于使用的模型解释工具，促进了模型透明度和可信度"}}
{"id": "2512.09910", "pdf": "https://arxiv.org/pdf/2512.09910", "abs": "https://arxiv.org/abs/2512.09910", "authors": ["Salvador Carrión", "Francisco Casacuberta"], "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.", "AI": {"tldr": "本研究提出基于LoRA的低秩适应框架，通过参数高效微调解决神经机器翻译中的灾难性遗忘和高计算成本问题，实现了接近全参数技术的性能，并引入交互式适应方法和梯度正则化策略。", "motivation": "神经机器翻译中的持续学习面临灾难性遗忘和重新训练计算成本高的双重挑战，需要参数高效的方法来适应新语言和领域。", "method": "1. 使用LoRA进行参数高效微调；2. 提出基于校准线性组合的交互式适应方法；3. 设计针对低秩分解矩阵的梯度正则化策略。", "result": "实验结果表明，该方法在保持先验领域知识的同时有效学习新任务，性能与全参数技术相当，但仅使用少量参数空间。", "conclusion": "LoRA框架为交互式和持续神经机器翻译提供了可扩展的范式，能够实现用户可控的实时调整，同时有效缓解灾难性遗忘问题。"}}
{"id": "2512.09756", "pdf": "https://arxiv.org/pdf/2512.09756", "abs": "https://arxiv.org/abs/2512.09756", "authors": ["Chonghua Liao", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li"], "title": "MOA: Multi-Objective Alignment for Role-Playing Agents", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.", "AI": {"tldr": "MOA是一个多目标强化学习框架，通过同时优化多个细粒度评分标准来解决角色扮演代理的多维度优化问题，使8B模型在多个维度上达到或超越GPT-4o和Claude等强基线模型。", "motivation": "现有方法存在局限性：监督微调容易过拟合表面线索且多样性低，强化学习方法无法同时优化角色扮演代理的多个维度（如遵循多轮指令、展现领域知识、保持语言风格一致性）。", "method": "提出MOA多目标对齐框架，采用多目标优化策略同时训练多个细粒度评分标准，并使用思维增强回滚和离策略指导来解决输出多样性和质量问题。", "result": "在PersonaGym和RoleMRC等挑战性基准测试中，MOA使8B模型在多个维度上匹配甚至超越了GPT-4o和Claude等强基线模型。", "conclusion": "MOA在构建能够同时满足角色知识、人设风格、多样化场景和复杂多轮对话需求的角色扮演代理方面展现出巨大潜力。"}}
{"id": "2512.09772", "pdf": "https://arxiv.org/pdf/2512.09772", "abs": "https://arxiv.org/abs/2512.09772", "authors": ["James Luther", "Donald Brown"], "title": "DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting", "categories": ["cs.CL"], "comment": null, "summary": "Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.", "AI": {"tldr": "该研究使用Hofstede的VSM13国际调查评估主流大语言模型的文化对齐性，发现大多数模型更接近美国文化价值观，即使使用文化提示或改变提示语言也难以与中国文化对齐，只有GPT-4在英语提示下更接近中国文化。", "motivation": "随着大语言模型在人机交互中越来越重要，研究这些类人代理的文化对齐性变得至关重要，以确保它们能够更好地理解和反映不同文化的价值观。", "method": "使用Hofstede的VSM13国际调查问卷，结合提示语言和文化提示策略（通过系统提示将模型对齐到特定国家），对多个旗舰LLM进行文化对齐测试。", "result": "DeepSeek-V3、V3.1和GPT-5与美国调查结果高度对齐，但无法与中国实现强或软对齐；GPT-4在英语提示下更接近中国文化，但文化提示可使其更接近美国；GPT-4o和GPT-4.1能通过提示语言和文化提示策略实现与中美文化的可接受对齐。", "conclusion": "当前主流LLM存在文化偏见，更倾向于美国文化价值观，需要通过更有效的文化对齐技术来改善模型的多文化适应性，以支持全球化应用。"}}
{"id": "2512.09804", "pdf": "https://arxiv.org/pdf/2512.09804", "abs": "https://arxiv.org/abs/2512.09804", "authors": ["Jens Albrecht", "Robert Lehmann", "Aleksandra Poltermann", "Eric Rudolph", "Philipp Steigerwald", "Mara Stieler"], "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to LREC 2026", "summary": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.", "AI": {"tldr": "OnCoCo 1.0是一个新的细粒度在线心理咨询消息分类公开数据集，包含38种咨询师和28种来访者话语类型的新分类系统，约2800条标注消息，可用于训练AI模型分析心理咨询对话。", "motivation": "现有的基于动机访谈(MI)的分类系统局限于面对面咨询数据，难以详细分析文本咨询对话，需要更全面的分类体系来改进在线心理咨询的自动化分析。", "method": "开发了包含38种咨询师话语类型和28种来访者话语类型的综合新编码方案，创建了约2800条标注消息的数据集，并在该数据集上微调了多个模型。", "result": "成功构建了OnCoCo 1.0数据集和分类系统，展示了模型在该数据集上的适用性，数据和模型均已公开可用。", "conclusion": "这项工作为语言资源社区贡献了新型细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集，有助于推动在线心理咨询的自动化分析发展。"}}
{"id": "2512.09841", "pdf": "https://arxiv.org/pdf/2512.09841", "abs": "https://arxiv.org/abs/2512.09841", "authors": ["Yijing Chen", "Yihan Wu", "Kaisi Guan", "Yuchen Ren", "Yuyue Wang", "Ruihua Song", "Liyun Ru"], "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "Code available at https://github.com/YJCX330/Chronus/", "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "AI": {"tldr": "ChronusOmni是一个多模态大语言模型，专门增强视听时间感知能力，通过时间戳令牌和强化学习统一建模显性和隐性的跨模态时间关系，在多个基准测试中取得最先进性能", "motivation": "现有方法主要关注视觉语言场景和显性时间定位问题，对音频模态利用不足，忽视了跨模态的隐性时间关系（如视觉内容与语音的对应关系），而这些在现实场景中很常见", "method": "1. 在每个时间单元中将基于文本的时间戳令牌与视觉和音频表示交错，实现跨模态的统一时间建模；2. 通过强化学习和专门设计的奖励函数来加强正确的时间顺序和细粒度时间推理；3. 构建了ChronusAV数据集支持训练和评估", "result": "ChronusOmni在ChronusAV数据集上实现了超过30%的性能提升，在大多数时间定位基准测试指标上取得了最佳结果", "conclusion": "该模型展现了强大的跨模态时间感知能力，同时保持了通用的视频和音频理解能力，为多模态时间推理提供了有效解决方案"}}
{"id": "2512.09854", "pdf": "https://arxiv.org/pdf/2512.09854", "abs": "https://arxiv.org/abs/2512.09854", "authors": ["Muneeb Ur Raheem Khan"], "title": "Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.", "AI": {"tldr": "本研究针对大语言模型在低资源语言中的偏见问题，提出并比较了三种推理时偏见缓解方法，发现在乌尔都语等低资源语言中偏见更严重，并展示了不同方法在偏见减少和效用保持方面的效果差异。", "motivation": "大语言模型在处理社会敏感内容时经常产生偏见和刻板印象，特别是在低资源语言中，由于训练数据有限且文化代表性不足，这一问题更加严重。需要开发无需重新训练或微调的推理时偏见缓解方法。", "method": "构建统一的评估框架，比较三种方法：(1)基线单词生成；(2)PRM-Select最佳N采样；(3)PRM-Sequential基于PRM批评的序列优化。使用GPT-3.5作为候选生成器，GPT-4o-mini作为基于PRM的偏见和效用评分器，在200个英文和乌尔都语提示上进行评估。", "result": "结果显示：(a)所有方法相比基线都有显著改进；(b)乌尔都语在所有方法中的公平性得分始终较低，突显多语言LLM训练中的结构性不平等；(c)PRM-Select和PRM-Sequential方法显示出不同的改进轨迹。", "conclusion": "研究提供了一个可扩展的方法论、可解释的指标和跨语言比较，支持未来在低资源语言公平性评估方面的工作，强调了需要针对不同语言和文化背景开发专门的偏见缓解策略。"}}
