{"id": "2507.08001", "pdf": "https://arxiv.org/pdf/2507.08001", "abs": "https://arxiv.org/abs/2507.08001", "authors": ["Shengyi Xie"], "title": "Human Creativity and AI", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "With the advancement of science and technology, the philosophy of creativity\nhas undergone significant reinterpretation. This paper investigates\ncontemporary research in the fields of psychology, cognitive neuroscience, and\nthe philosophy of creativity, particularly in the context of the development of\nartificial intelligence (AI) techniques. It aims to address the central\nquestion: Can AI exhibit creativity? The paper reviews the historical\nperspectives on the philosophy of creativity and explores the influence of\npsychological advancements on the study of creativity. Furthermore, it analyzes\nvarious definitions of creativity and examines the responses of naturalism and\ncognitive neuroscience to the concept of creativity.", "AI": {"tldr": "This paper investigates the philosophy of creativity in the context of AI development.", "motivation": "The motivation behind this paper is to address the central question of whether AI can exhibit creativity in light of contemporary research in the fields of psychology, cognitive neuroscience, and the philosophy of creativity.", "method": "The paper reviews historical perspectives on the philosophy of creativity, explores the influence of psychological advancements on the study of creativity, analyzes various definitions of creativity, and examines the responses of naturalism and cognitive neuroscience to the concept of creativity.", "result": "The result of this paper is a comprehensive review of the historical and contemporary perspectives on the philosophy of creativity, as well as an analysis of the influence of psychological advancements and the responses of naturalism and cognitive neuroscience to the concept of creativity.", "conclusion": "The paper concludes that the question of whether AI can exhibit creativity is complex and requires further investigation from multiple perspectives."}}
{"id": "2507.08046", "pdf": "https://arxiv.org/pdf/2507.08046", "abs": "https://arxiv.org/abs/2507.08046", "authors": ["Sishi Xiong", "Dakai Wang", "Yu Zhao", "Jie Zhang", "Changzai Pan", "Haowei He", "Xiangyu Li", "Wenhan Chang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "TableReasoner: Advancing Table Reasoning Framework with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The paper presents our system developed for table question answering (TQA).\nTQA tasks face challenges due to the characteristics of real-world tabular\ndata, such as large size, incomplete column semantics, and entity ambiguity. To\naddress these issues, we propose a large language model (LLM)-powered and\nprogramming-based table reasoning framework, named TableReasoner. It models a\ntable using the schema that combines structural and semantic representations,\nenabling holistic understanding and efficient processing of large tables. We\ndesign a multi-step schema linking plan to derive a focused table schema that\nretains only query-relevant information, eliminating ambiguity and alleviating\nhallucinations. This focused table schema provides precise and sufficient table\ndetails for query refinement and programming. Furthermore, we integrate the\nreasoning workflow into an iterative thinking architecture, allowing\nincremental cycles of thinking, reasoning and reflection. Our system achieves\nfirst place in both subtasks of SemEval-2025 Task 8.", "AI": {"tldr": "This paper presents a system developed for table question answering (TQA) using a large language model (LLM)-powered and programming-based table reasoning framework, named TableReasoner. It addresses the issues of large size, incomplete column semantics, and entity ambiguity by modeling a table using the schema that combines structural and semantic representations.", "motivation": "TQA tasks face challenges due to the characteristics of real-world tabular data, such as large size, incomplete column semantics, and entity ambiguity.", "method": "We propose a large language model (LLM)-powered and programming-based table reasoning framework, named TableReasoner.", "result": "Our system achieves first place in both subtasks of SemEval-2025 Task 8.", "conclusion": "Our system achieves first place in both subtasks of SemEval-2025 Task 8."}}
{"id": "2507.08207", "pdf": "https://arxiv.org/pdf/2507.08207", "abs": "https://arxiv.org/abs/2507.08207", "authors": ["Zhengye Han", "Quanyan Zhu"], "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking.", "AI": {"tldr": "This paper presents a dynamic Stackelberg game framework to model the interactions between attackers and defenders in the context of LLM jailbreaking.", "motivation": "As LLMs are increasingly deployed in critical applications, the challenge of jailbreaking has become a significant concern.", "method": "A dynamic Stackelberg game framework is used to model the interactions between attackers and defenders. The novel agentic AI solution, the 'Purple Agent', integrates adversarial exploration and defensive strategies using Rapidly-exploring Random Trees (RRT).", "result": "The approach offers a principled method for analyzing adversarial dynamics and provides a foundation for mitigating the risk of jailbreaking.", "conclusion": "The Purple Agent provides a foundation for mitigating the risk of jailbreaking in LLMs."}}
{"id": "2507.08208", "pdf": "https://arxiv.org/pdf/2507.08208", "abs": "https://arxiv.org/abs/2507.08208", "authors": ["Quanyan Zhu"], "title": "Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "We introduce the LLM-Nash framework, a game-theoretic model where agents\nselect reasoning prompts to guide decision-making via Large Language Models\n(LLMs). Unlike classical games that assume utility-maximizing agents with full\nrationality, this framework captures bounded rationality by modeling the\nreasoning process explicitly. Equilibrium is defined over the prompt space,\nwith actions emerging as the behavioral output of LLM inference. This approach\nenables the study of cognitive constraints, mindset expressiveness, and\nepistemic learning. Through illustrative examples, we show how reasoning\nequilibria can diverge from classical Nash outcomes, offering a new foundation\nfor strategic interaction in LLM-enabled systems.", "AI": {"tldr": "This paper presents the LLM-Nash framework, which models how agents choose reasoning prompts for decision-making with LLMs, capturing bounded rationality and differing from traditional game theory.", "motivation": "To create a model that captures bounded rationality in agent decision-making processes when using Large Language Models (LLMs), as opposed to assuming full rationality in classical game theory.", "method": "The paper proposes the LLM-Nash framework, a game-theoretic model where agents select reasoning prompts to guide decision-making via LLMs. Equilibrium is defined over the prompt space, with actions emerging as the behavioral output of LLM inference.", "result": "Reasoning equilibria within the LLM-Nash framework can diverge from classical Nash outcomes, allowing the study of cognitive constraints, mindset expressiveness, and epistemic learning.", "conclusion": "The LLM-Nash framework provides a new foundation for strategic interaction in systems enabled by Large Language Models, taking into account cognitive constraints and diverging from classical Nash outcomes."}}
{"id": "2507.08050", "pdf": "https://arxiv.org/pdf/2507.08050", "abs": "https://arxiv.org/abs/2507.08050", "authors": ["Ming Wang", "Zhaoyang Duan", "Dong Xue", "Fangzhou Liu", "Zhongheng Zhang"], "title": "An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The labor-intensive nature of medical data annotation presents a significant\nchallenge for respiratory disease diagnosis, resulting in a scarcity of\nhigh-quality labeled datasets in resource-constrained settings. Moreover,\npatient privacy concerns complicate the direct sharing of local medical data\nacross institutions, and existing centralized data-driven approaches, which\nrely on amounts of available data, often compromise data privacy. This study\nproposes a federated few-shot learning framework with privacy-preserving\nmechanisms to address the issues of limited labeled data and privacy protection\nin diagnosing respiratory diseases. In particular, a meta-stochastic gradient\ndescent algorithm is proposed to mitigate the overfitting problem that arises\nfrom insufficient data when employing traditional gradient descent methods for\nneural network training. Furthermore, to ensure data privacy against gradient\nleakage, differential privacy noise from a standard Gaussian distribution is\nintegrated into the gradients during the training of private models with local\ndata, thereby preventing the reconstruction of medical images. Given the\nimpracticality of centralizing respiratory disease data dispersed across\nvarious medical institutions, a weighted average algorithm is employed to\naggregate local diagnostic models from different clients, enhancing the\nadaptability of a model across diverse scenarios. Experimental results show\nthat the proposed method yields compelling results with the implementation of\ndifferential privacy, while effectively diagnosing respiratory diseases using\ndata from different structures, categories, and distributions.", "AI": {"tldr": "This study proposes a federated few-shot learning framework with privacy-preserving mechanisms to address the issues of limited labeled data and privacy protection in diagnosing respiratory diseases.", "motivation": "The labor-intensive nature of medical data annotation and patient privacy concerns present significant challenges for respiratory disease diagnosis, particularly in resource-constrained settings. Existing centralized data-driven approaches often compromise data privacy.", "method": "A federated few-shot learning framework is proposed. Within this framework, a meta-stochastic gradient descent algorithm is used to mitigate overfitting problems from insufficient data. Differential privacy noise is integrated into the gradients during training to ensure data privacy against gradient leakage. A weighted average algorithm aggregates local diagnostic models from different clients.", "result": "Experimental results show that the proposed method yields compelling results with the implementation of differential privacy, while effectively diagnosing respiratory diseases using data from different structures, categories, and distributions.", "conclusion": "The proposed federated few-shot learning framework with privacy-preserving mechanisms effectively addresses the issues of limited labeled data and privacy protection in diagnosing respiratory diseases, yielding compelling results with the implementation of differential privacy."}}
{"id": "2507.08158", "pdf": "https://arxiv.org/pdf/2507.08158", "abs": "https://arxiv.org/abs/2507.08158", "authors": ["Marika Swanberg", "Meenatchi Sundaram Muthu Selva Annamalai", "Jamie Hayes", "Borja Balle", "Adam Smith"], "title": "Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries", "categories": ["cs.CR"], "comment": null, "summary": "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5bf9\u5dee\u5206\u9690\u79c1\u673a\u5236\u5728\u591a\u79cd\u81ea\u7136\u653b\u51fb\u8bbe\u5b9a\u4e0b\u7684\u9ad8\u6982\u7387\u4fdd\u969c\u8fdb\u884c\u8ba1\u7b97\uff0c\u4ee5\u7406\u89e3\u5176\u5728\u4ee5\u524d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u653b\u51fb\u8bbe\u5b9a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4fdd\u969c\u4e0e\u6240\u63d0\u4f9b\u7684\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u5206\u6790\u6027\u6743\u8861\u76ee\u524d\u8fd8\u4e0d\u592a\u6e05\u695a\u3002\u8fd9\u9879\u5de5\u4f5c\u9610\u660e\u4e86DP\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u610f\u5473\u7740\u4ec0\u4e48\uff0c\u5bf9\u4e8e\u66f4\u5177\u73b0\u5b9e\u4e16\u754c\u9690\u79c1\u98ce\u9669\u4ee3\u8868\u6027\u7684\u653b\u51fb\u8005\u6765\u8bf4\u610f\u5473\u7740\u6210\u529f\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u8ba1\u7b97\u51fa\u5728\u5927\u91cf\u81ea\u7136\u653b\u51fb\u8bbe\u7f6e\u4e0bDP\u673a\u5236\u7684\u9ad8\u6982\u7387\u4fdd\u8bc1\u3002", "result": "\u8fdb\u884c\u4e86\u4e24\u4e2a\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\u6765\u8bf4\u660e\u6211\u4eec\u754c\u9650\u7684\u591a\u529f\u80fd\u6027\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u653b\u51fb\u7684\u6210\u529f\u8fdb\u884c\u6bd4\u8f83\u3002\u53d1\u73b0\u653b\u51fb\u975e\u5747\u5300\u6570\u636e\u7684\u7edd\u5bf9\u9690\u79c1\u98ce\u9669\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5bf9\u624b\u5148\u524d\u6210\u529f\u7684\u6982\u7387\u3002", "conclusion": "\u5728\u4ee5\u524d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u653b\u51fb\u8bbe\u7f6e\u4e2d\uff0c\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u9690\u79c1\u6cc4\u9732\u6709\u4e86\u66f4\u7ec6\u81f4\u7684\u7406\u89e3\u3002"}}
{"id": "2507.08210", "pdf": "https://arxiv.org/pdf/2507.08210", "abs": "https://arxiv.org/abs/2507.08210", "authors": ["Fryderyk Mantiuk", "Hanqi Zhou", "Charley M. Wu"], "title": "From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration", "categories": ["cs.AI"], "comment": null, "summary": "What drives an agent to explore the world while also maintaining control over\nthe environment? From a child at play to scientists in the lab, intelligent\nagents must balance curiosity (the drive to seek knowledge) with competence\n(the drive to master and control the environment). Bridging cognitive theories\nof intrinsic motivation with reinforcement learning, we ask how evolving\ninternal representations mediate the trade-off between curiosity (novelty or\ninformation gain) and competence (empowerment). We compare two model-based\nagents using handcrafted state abstractions (Tabular) or learning an internal\nworld model (Dreamer). The Tabular agent shows curiosity and competence guide\nexploration in distinct patterns, while prioritizing both improves exploration.\nThe Dreamer agent reveals a two-way interaction between exploration and\nrepresentation learning, mirroring the developmental co-evolution of curiosity\nand competence. Our findings formalize adaptive exploration as a balance\nbetween pursuing the unknown and the controllable, offering insights for\ncognitive theories and efficient reinforcement learning.", "AI": {"tldr": "This paper explores how intelligent agents balance curiosity and competence during exploration.", "motivation": "To understand what drives an agent to explore the world while also maintaining control over the environment.", "method": "Two model-based agents are compared using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer).", "result": "The Tabular agent shows curiosity and competence guide exploration in distinct patterns, while prioritizing both improves exploration. The Dreamer agent reveals a two-way interaction between exploration and representation learning.", "conclusion": "The findings formalize adaptive exploration as a balance between pursuing the unknown and the controllable, offering insights for cognitive theories and efficient reinforcement learning."}}
{"id": "2507.08053", "pdf": "https://arxiv.org/pdf/2507.08053", "abs": "https://arxiv.org/abs/2507.08053", "authors": ["Kenshin Abe", "Yunzhuo Wang", "Shuhei Watanabe"], "title": "Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to AutoML Conference", "summary": "Tree-structured Parzen estimator (TPE) is a versatile hyperparameter\noptimization (HPO) method supported by popular HPO tools. Since these HPO tools\nhave been developed in line with the trend of deep learning (DL), the problem\nsetups often used in the DL domain have been discussed for TPE such as\nmulti-objective optimization and multi-fidelity optimization. However, the\npractical applications of HPO are not limited to DL, and black-box\ncombinatorial optimization is actively utilized in some domains, e.g.,\nchemistry and biology. As combinatorial optimization has been an untouched, yet\nvery important, topic in TPE, we propose an efficient combinatorial\noptimization algorithm for TPE. In this paper, we first generalize the\ncategorical kernel with the numerical kernel in TPE, enabling us to introduce a\ndistance structure to the categorical kernel. Then we discuss modifications for\nthe newly developed kernel to handle a large combinatorial search space. These\nmodifications reduce the time complexity of the kernel calculation with respect\nto the size of a combinatorial search space. In the experiments using synthetic\nproblems, we verified that our proposed method identifies better solutions with\nfewer evaluations than the original TPE. Our algorithm is available in Optuna,\nan open-source framework for HPO.", "AI": {"tldr": "This paper proposes an efficient combinatorial optimization algorithm for TPE, which generalizes the categorical kernel and reduces time complexity.", "motivation": "Black-box combinatorial optimization is actively utilized in domains such as chemistry and biology but has not been addressed in TPE.", "method": "The categorical kernel is generalized with the numerical kernel in TPE, introducing a distance structure to the categorical kernel. Modifications are made to handle large combinatorial search spaces.", "result": "Experiments using synthetic problems show that the proposed method identifies better solutions with fewer evaluations than the original TPE.", "conclusion": "The proposed method for combinatorial optimization in TPE is effective and available in Optuna."}}
{"id": "2507.08166", "pdf": "https://arxiv.org/pdf/2507.08166", "abs": "https://arxiv.org/abs/2507.08166", "authors": ["Chris S. Lin", "Joyce Qu", "Gururaj Saileshwar"], "title": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical", "categories": ["cs.CR"], "comment": "20 pages, including appendices. The paper will appear in SEC'25", "summary": "Rowhammer is a read disturbance vulnerability in modern DRAM that causes\nbit-flips, compromising security and reliability. While extensively studied on\nIntel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR\nmemories, critical for emerging machine learning applications, remains\nunexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary\nmapping of physical memory to GDDR banks and rows, (2) high memory latency and\nfaster refresh rates that hinder effective hammering, and (3) proprietary\nmitigations in GDDR memories, difficult to reverse-engineer without FPGA-based\ntest platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA\nGPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer\nGDDR DRAM row mappings, and employs GPU-specific memory access optimizations to\namplify hammering intensity and bypass mitigations. Thus, we demonstrate the\nfirst successful Rowhammer attack on a discrete GPU, injecting up to 8\nbit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also\nshow how an attacker can use these to tamper with ML models, causing\nsignificant accuracy drops (up to 80%).", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86GPUHammer\uff0c\u8fd9\u662f\u9488\u5bf9NVIDIA GPU\u4e0eGDDR6 DRAM\u7684\u7b2c\u4e00\u4e2aRowhammer\u653b\u51fb\u3002", "motivation": "\u5c3d\u7ba1Rowhammer\u6f0f\u6d1e\u5728Intel\u548cAMD CPU\u4e0a\u5df2\u7ecf\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u5176\u5728\u4f7f\u7528GDDR\u5185\u5b58\u7684GPU\u4e0a\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u65b0\u5174\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u6b64\u5916\uff0cGPU\u4e0a\u7684Rowhammer\u653b\u51fb\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6280\u672f\u6765\u53cd\u5411\u5de5\u7a0bGDDR DRAM\u884c\u6620\u5c04\uff0c\u5e76\u91c7\u7528\u7279\u5b9a\u4e8eGPU\u7684\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u6765\u589e\u5f3a\u9524\u51fb\u5f3a\u5ea6\u5e76\u7ed5\u8fc7\u7f13\u89e3\u63aa\u65bd\u3002", "result": "\u901a\u8fc7NVIDIA A6000 GPU\u548cGDDR6\u5185\u5b58\uff0c\u7814\u7a76\u6210\u529f\u5730\u5c55\u793a\u4e86\u9996\u4e2a\u79bb\u6563GPU\u4e0a\u7684Rowhammer\u653b\u51fb\uff0c\u80fd\u591f\u8de84\u4e2aDRAM\u5e93\u6ce8\u5165\u591a\u8fbe8\u4e2a\u4f4d\u7ffb\u8f6c\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u5927\u5e45\u4e0b\u964d\uff08\u9ad8\u8fbe80%\uff09\u3002", "conclusion": "\u603b\u4e4b\uff0cGPUHammer\u5c55\u793a\u4e86\u5bf9\u79bb\u6563GPU\u7684\u9996\u6b21Rowhammer\u653b\u51fb\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u8003\u8651\u6b64\u7c7b\u5b89\u5168\u98ce\u9669\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.08216", "pdf": "https://arxiv.org/pdf/2507.08216", "abs": "https://arxiv.org/abs/2507.08216", "authors": ["Rodrigo Castellano Ontiveros", "Francesco Giannini", "Marco Gori", "Giuseppe Marra", "Michelangelo Diligenti"], "title": "Grounding Methods for Neural-Symbolic AI", "categories": ["cs.AI"], "comment": null, "summary": "A large class of Neural-Symbolic (NeSy) methods employs a machine learner to\nprocess the input entities, while relying on a reasoner based on First-Order\nLogic to represent and process more complex relationships among the entities. A\nfundamental role for these methods is played by the process of logic grounding,\nwhich determines the relevant substitutions for the logic rules using a\n(sub)set of entities. Some NeSy methods use an exhaustive derivation of all\npossible substitutions, preserving the full expressive power of the logic\nknowledge. This leads to a combinatorial explosion in the number of ground\nformulas to consider and, therefore, strongly limits their scalability. Other\nmethods rely on heuristic-based selective derivations, which are generally more\ncomputationally efficient, but lack a justification and provide no guarantees\nof preserving the information provided to and returned by the reasoner. Taking\ninspiration from multi-hop symbolic reasoning, this paper proposes a\nparametrized family of grounding methods generalizing classic Backward\nChaining. Different selections within this family allow us to obtain commonly\nemployed grounding methods as special cases, and to control the trade-off\nbetween expressiveness and scalability of the reasoner. The experimental\nresults show that the selection of the grounding criterion is often as\nimportant as the NeSy method itself.", "AI": {"tldr": "This paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining inspired by multi-hop symbolic reasoning.", "motivation": "Taking inspiration from multi-hop symbolic reasoning, this paper aims to propose a parametrized family of grounding methods that allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner.", "method": "This paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining.", "result": "Different selections within this family allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner. The experimental results show that the selection of the grounding criterion is often as important as the NeSy method itself.", "conclusion": "The selection of the grounding criterion is often as important as the NeSy method itself."}}
{"id": "2507.08068", "pdf": "https://arxiv.org/pdf/2507.08068", "abs": "https://arxiv.org/abs/2507.08068", "authors": ["Simon Matrenok", "Skander Moalla", "Caglar Gulcehre"], "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions", "categories": ["cs.LG"], "comment": null, "summary": "Aligning large language models with pointwise absolute rewards has so far\nrequired online, on-policy algorithms such as PPO and GRPO. In contrast,\nsimpler methods that can leverage offline or off-policy data, such as DPO and\nREBEL, are limited to learning from preference pairs or relative signals. To\nbridge this gap, we introduce \\emph{Quantile Reward Policy Optimization}\n(QRPO), which learns from pointwise absolute rewards while preserving the\nsimplicity and offline applicability of DPO-like methods. QRPO uses quantile\nrewards to enable regression to the closed-form solution of the KL-regularized\nRL objective. This reward yields an analytically tractable partition function,\nremoving the need for relative signals to cancel this term. Moreover, QRPO\nscales with increased compute to estimate quantile rewards, opening a new\ndimension for pre-computation scaling. Empirically, QRPO consistently achieves\ntop performance on chat and coding evaluations -- reward model scores,\nAlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse\ndatasets and 8B-scale models. Finally, we find that training with robust\nrewards instead of converting them to preferences induces less length bias.", "AI": {"tldr": "This paper introduces QRPO, a new method for aligning large language models with pointwise absolute rewards that preserves simplicity and offline applicability, achieving top performance and reducing length bias.", "motivation": "The motivation is to bridge the gap between online, on-policy algorithms and simpler offline or off-policy methods for aligning large language models with pointwise absolute rewards.", "method": "The paper introduces Quantile Reward Policy Optimization (QRPO), which uses quantile rewards to enable regression to the closed-form solution of the KL-regularized RL objective.", "result": "Empirically, QRPO consistently achieves top performance on chat and coding evaluations across diverse datasets and 8B-scale models. Training with robust rewards instead of converting them to preferences induces less length bias.", "conclusion": "QRPO provides a new method for aligning large language models with pointwise absolute rewards, outperforming existing methods and reducing length bias."}}
{"id": "2507.08286", "pdf": "https://arxiv.org/pdf/2507.08286", "abs": "https://arxiv.org/abs/2507.08286", "authors": ["Aufa Nasywa Rahman", "Bimo Sunarfri Hantono", "Guntur Dharma Putra"], "title": "TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data", "categories": ["cs.CR", "cs.ET"], "comment": "8 pages, 7 figures. Accepted to IEEE MetaCom 2025", "summary": "Open banking framework enables third party providers to access financial data\nacross banking institutions, leading to unprecedented innovations in the\nfinancial sector. However, some open banking standards remain susceptible to\nsevere technological risks, including unverified data sources, inconsistent\ndata integrity, and lack of immutability. In this paper, we propose a layered\narchitecture that provides assurance in data trustworthiness with three\ndistinct levels of trust, covering source validation, data-level\nauthentication, and tamper-proof storage. The first layer guarantees the source\nlegitimacy using decentralized identity and verifiable presentation, while the\nsecond layer verifies data authenticity and consistency using cryptographic\nsigning. Lastly, the third layer guarantees data immutability through the\nTangle, a directed acyclic graph distributed ledger. We implemented a\nproof-of-concept implementation of our solution to evaluate its performance,\nwhere the results demonstrate that the system scales linearly with a stable\nthroughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and\n350 MiB memory. Compared to a real-world open banking implementation, our\nsolution offers significantly reduced latency and stronger data integrity\nassurance. Overall, our solution offers a practical and efficient system for\nsecure data sharing in financial ecosystems while maintaining regulatory\ncompliance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u5f00\u653e\u94f6\u884c\u6807\u51c6\u4e2d\u5b58\u5728\u7684\u79d1\u6280\u98ce\u9669\uff0c\u5305\u62ec\u672a\u7ecf\u9a8c\u8bc1\u7684\u6570\u636e\u6765\u6e90\u3001\u4e0d\u4e00\u81f4\u7684\u6570\u636e\u5b8c\u6574\u6027\u548c\u7f3a\u4e4f\u4e0d\u53ef\u53d8\u6027\u3002", "motivation": "\u5f00\u653e\u94f6\u884c\u6846\u67b6\u4f7f\u7b2c\u4e09\u65b9\u63d0\u4f9b\u5546\u80fd\u591f\u8de8\u94f6\u884c\u673a\u6784\u8bbf\u95ee\u91d1\u878d\u6570\u636e\uff0c\u5bfc\u81f4\u91d1\u878d\u90e8\u95e8\u51fa\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u521b\u65b0\u3002\u7136\u800c\uff0c\u4e00\u4e9b\u5f00\u653e\u94f6\u884c\u6807\u51c6\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u4e25\u91cd\u7684\u79d1\u6280\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u672a\u7ecf\u9a8c\u8bc1\u7684\u6570\u636e\u6765\u6e90\u3001\u4e0d\u4e00\u81f4\u7684\u6570\u636e\u5b8c\u6574\u6027\u548c\u7f3a\u4e4f\u4e0d\u53ef\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u4e09\u4e2a\u4e0d\u540c\u5c42\u6b21\u7684\u4fe1\u4efb\uff08\u5305\u62ec\u6e90\u9a8c\u8bc1\u3001\u6570\u636e\u7ea7\u8eab\u4efd\u9a8c\u8bc1\u548c\u9632\u7be1\u6539\u5b58\u50a8\uff09\u6765\u786e\u4fdd\u6570\u636e\u7684\u53ef\u4fe1\u5ea6\u3002\u7b2c\u4e00\u5c42\u4f7f\u7528\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u548c\u53ef\u9a8c\u8bc1\u5c55\u793a\u6765\u4fdd\u8bc1\u6e90\u5408\u6cd5\u6027\uff0c\u7b2c\u4e8c\u5c42\u4f7f\u7528\u52a0\u5bc6\u7b7e\u540d\u9a8c\u8bc1\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u7b2c\u4e09\u5c42\u901a\u8fc7\u6709\u5411\u65e0\u73af\u56fe\u5206\u5e03\u5f0f\u8d26\u672cTangle\u4fdd\u8bc1\u6570\u636e\u4e0d\u53ef\u53d8\u6027\u3002", "result": "\u5b9e\u65bd\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u4ee5\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u7cfb\u7edf\u5177\u6709\u7a33\u5b9a\u7684\u541e\u5410\u91cf\uff0c100% \u7684\u9a8c\u8bc1\u7387\uff0c\u5e76\u4e14CPU\u5229\u7528\u7387\u4f4e\u4e8e35%\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u4f4e\u4e8e350MiB\u3002\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u5f00\u653e\u94f6\u884c\u5b9e\u73b0\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u663e\u8457\u964d\u4f4e\u7684\u5ef6\u8fdf\u548c\u66f4\u5f3a\u7684\u6570\u636e\u5b8c\u6574\u6027\u4fdd\u969c\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u4e3a\u91d1\u878d\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6cd5\u89c4\u9075\u4ece\u6027\u3002"}}
{"id": "2507.08217", "pdf": "https://arxiv.org/pdf/2507.08217", "abs": "https://arxiv.org/abs/2507.08217", "authors": ["Atit Pokharel", "Ratun Rahman", "Thomas Morris", "Dinh C. Nguyen"], "title": "Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach", "categories": ["cs.AI"], "comment": "This paper was presented at BEAM with CVPR 2025", "summary": "Quantum federated learning (QFL) has been recently introduced to enable a\ndistributed privacy-preserving quantum machine learning (QML) model training\nacross quantum processors (clients). Despite recent research efforts, existing\nQFL frameworks predominantly focus on unimodal systems, limiting their\napplicability to real-world tasks that often naturally involve multiple\nmodalities. To fill this significant gap, we present for the first time a novel\nmultimodal approach specifically tailored for the QFL setting with the\nintermediate fusion using quantum entanglement. Furthermore, to address a major\nbottleneck in multimodal QFL, where the absence of certain modalities during\ntraining can degrade model performance, we introduce a Missing Modality\nAgnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring\nstable training without corrupted states. Simulation results demonstrate that\nthe proposed multimodal QFL method with MMA yields an improvement in accuracy\nof 6.84% in independent and identically distributed (IID) and 7.25% in non-IID\ndata distributions compared to the state-of-the-art methods.", "AI": {"tldr": "This paper introduces a novel multimodal approach for quantum federated learning with a mechanism to handle missing modalities, improving accuracy in simulations.", "motivation": "Existing QFL frameworks focus on unimodal systems, limiting their applicability to real-world tasks involving multiple modalities. There is also a bottleneck in multimodal QFL when certain modalities are absent during training.", "method": "A novel multimodal approach for QFL using quantum entanglement with an intermediate fusion and a Missing Modality Agnostic (MMA) mechanism.", "result": "Simulation results show an improvement in accuracy of 6.84% in IID and 7.25% in non-IID data distributions.", "conclusion": "The proposed multimodal QFL method with MMA improves accuracy in both IID and non-IID data distributions."}}
{"id": "2507.08091", "pdf": "https://arxiv.org/pdf/2507.08091", "abs": "https://arxiv.org/abs/2507.08091", "authors": ["Pouria Mahdavinia", "Mehrdad Mahdavi"], "title": "Low-rank Momentum Factorization for Memory Efficient Training", "categories": ["cs.LG"], "comment": null, "summary": "Fine-tuning large foundation models presents significant memory challenges\ndue to stateful optimizers like AdamW, often requiring several times more GPU\nmemory than inference. While memory-efficient methods like parameter-efficient\nfine-tuning (e.g., LoRA) and optimizer state compression exist, recent\napproaches like GaLore bridge these by using low-rank gradient projections and\nsubspace moment accumulation. However, such methods may struggle with fixed\nsubspaces or computationally costly offline resampling (e.g., requiring\nfull-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which\nmaintains a dynamically updated low-rank SVD representation of the first-order\nmomentum, closely approximating its full-rank counterpart throughout training.\nThis factorization enables a memory-efficient fine-tuning method that\nadaptively updates the optimization subspace at each iteration. Crucially,\nMoFaSGD leverages the computed low-rank momentum factors to perform efficient\nspectrally normalized updates, offering an alternative to subspace moment\naccumulation. We establish theoretical convergence guarantees for MoFaSGD,\nproving it achieves an optimal rate for non-convex stochastic optimization\nunder standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness\non large language model alignment benchmarks, achieving a competitive trade-off\nbetween memory reduction (comparable to LoRA) and performance compared to\nstate-of-the-art low-rank optimization methods. Our implementation is available\nat https://github.com/pmahdavi/MoFaSGD.", "AI": {"tldr": "This paper introduces Momentum Factorized SGD (MoFaSGD), a memory-efficient fine-tuning method that dynamically updates the optimization subspace and performs spectrally normalized updates. MoFaSGD achieves optimal convergence rates and shows good performance in reducing memory usage while maintaining competitive model performance.", "motivation": "Fine-tuning large foundation models faces memory challenges due to stateful optimizers like AdamW. Existing methods either struggle with fixed subspaces or are computationally costly. There is a need for a more efficient and adaptive method that can reduce memory usage while maintaining performance.", "method": "The proposed method, Momentum Factorized SGD (MoFaSGD), maintains a dynamically updated low-rank SVD representation of the first-order momentum to closely approximate its full-rank counterpart. It uses this factorization to adaptively update the optimization subspace at each iteration and leverages the low-rank momentum factors for efficient spectrally normalized updates.", "result": "MoFaSGD achieves an optimal rate for non-convex stochastic optimization and demonstrates competitive performance on large language model alignment benchmarks, with memory reduction comparable to LoRA.", "conclusion": "MoFaSGD provides an efficient and effective method for fine-tuning large models, offering a good trade-off between memory reduction and performance."}}
{"id": "2507.08288", "pdf": "https://arxiv.org/pdf/2507.08288", "abs": "https://arxiv.org/abs/2507.08288", "authors": ["Qingxiao Guo", "Xinjie Zhu", "Yilong Ma", "Hui Jin", "Yunhao Wang", "Weifeng Zhang", "Xiaobing Guo"], "title": "Invariant-based Robust Weights Watermark for Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).", "AI": {"tldr": "This paper introduces a robust watermarking scheme for transformer models without retraining or fine-tuning, using unique keys and a noise mechanism to protect against IP theft.", "motivation": "Watermarking technology is important for protecting intellectual property rights, especially with the increasing use of large language models on edge devices.", "method": "A unique key is generated for each user, and a stable watermark value is derived by solving linear constraints from model invariants. A noise mechanism is used to hide watermark locations in multi-user scenarios.", "result": "The approach was evaluated on three popular models (Llama3, Phi3, Gemma) and showed strong robustness against various attack methods.", "conclusion": "The watermarking scheme is robust and effective in protecting IP rights for transformer models."}}
{"id": "2507.08249", "pdf": "https://arxiv.org/pdf/2507.08249", "abs": "https://arxiv.org/abs/2507.08249", "authors": ["Bill Marino", "Ari Juels"], "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "There is growing interest in giving AI agents access to cryptocurrencies as\nwell as to the smart contracts that transact them. But doing so, this position\npaper argues, could lead to formidable new vectors of AI harm. To support this\nargument, we first examine the unique properties of cryptocurrencies and smart\ncontracts that could lead to these new vectors of harm. Next, we describe each\nof these new vectors of harm in detail. Finally, we conclude with a call for\nmore technical research aimed at preventing and mitigating these harms and,\nthereby making it safer to endow AI agents with cryptocurrencies and smart\ncontracts.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u7ed9\u4e88AI\u4ee3\u7406\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u8bbf\u95ee\u6743\u9650\u53ef\u80fd\u4f1a\u5bfc\u81f4\u65b0\u7684AI\u5371\u5bb3\u5411\u91cf\uff0c\u5e76\u547c\u5401\u8fdb\u884c\u66f4\u591a\u7684\u6280\u672f\u7814\u7a76\u6765\u9632\u6b62\u548c\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\u3002", "motivation": "\u7ed9\u4e88AI\u4ee3\u7406\u8bbf\u95ee\u52a0\u5bc6\u8d27\u5e01\u4ee5\u53ca\u4ea4\u6613\u5b83\u4eec\u7684\u667a\u80fd\u5408\u7ea6\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u65b0\u7684AI\u5371\u5bb3\u5411\u91cf\u3002", "method": "\u9996\u5148\u68c0\u67e5\u4e86\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u53ef\u80fd\u5bfc\u81f4\u65b0\u7684\u5371\u5bb3\u5411\u91cf\u3002\u7136\u540e\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6bcf\u4e2a\u65b0\u7684\u5371\u5bb3\u5411\u91cf\u3002", "result": "\u8be6\u7ec6\u63cf\u8ff0\u4e86\u65b0\u7684AI\u5371\u5bb3\u5411\u91cf\u3002", "conclusion": "\u547c\u5401\u8fdb\u884c\u66f4\u591a\u7684\u6280\u672f\u7814\u7a76\uff0c\u4ee5\u9632\u6b62\u548c\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\uff0c\u4ece\u800c\u66f4\u5b89\u5168\u5730\u8d4b\u4e88AI\u4ee3\u7406\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u3002"}}
{"id": "2507.08118", "pdf": "https://arxiv.org/pdf/2507.08118", "abs": "https://arxiv.org/abs/2507.08118", "authors": ["Hardik Shukla", "Manurag Khullar", "Vismay Churiwala"], "title": "PDE-aware Optimizer for Physics-informed Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs) by embedding physical\nconstraints into the loss function. However, standard optimizers such as Adam\noften struggle to balance competing loss terms, particularly in stiff or\nill-conditioned systems. In this work, we propose a PDE-aware optimizer that\nadapts parameter updates based on the variance of per-sample PDE residual\ngradients. This method addresses gradient misalignment without incurring the\nheavy computational costs of second-order optimizers such as SOAP. We benchmark\nthe PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and\nKorteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer\nachieves smoother convergence and lower absolute errors, particularly in\nregions with sharp gradients. Our results demonstrate the effectiveness of PDE\nresidual-aware adaptivity in enhancing stability in PINNs training. While\npromising, further scaling on larger architectures and hardware accelerators\nremains an important direction for future research.", "AI": {"tldr": "A new PDE-aware optimizer improves training stability and accuracy in Physics-Informed Neural Networks.", "motivation": "Standard optimizers struggle with stiff or ill-conditioned systems in PINNs, prompting the need for a specialized optimizer.", "method": "A novel PDE-aware optimizer that adapts parameter updates based on per-sample PDE residual gradients.", "result": "The PDE-aware optimizer achieves better convergence and lower errors compared to Adam and SOAP.", "conclusion": "The PDE-aware optimizer shows potential for enhancing PINN stability and accuracy, though further scaling is needed."}}
{"id": "2507.08312", "pdf": "https://arxiv.org/pdf/2507.08312", "abs": "https://arxiv.org/abs/2507.08312", "authors": ["Jesus Lopez", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices", "categories": ["cs.CR", "cs.ET"], "comment": "8 pages, 4 figures, 4 tables. This paper is accepted at the IEEE\n  Quantum Week 2025 -- IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a critical threat to\nclassical cryptographic algorithms such as RSA and ECC, particularly in\nInternet of Things (IoT) devices, where secure communication is essential but\noften constrained by limited computational resources. This paper investigates\nthe feasibility of deploying post-quantum cryptography (PQC) algorithms on\nresource-constrained devices. In particular, we implement three PQC algorithms\n-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with\nRaspberry Pi devices. Leveraging the Open Quantum Safe (\\texttt{liboqs})\nlibrary in conjunction with \\texttt{mbedTLS}, we develop quantum-secure key\nexchange protocols, and evaluate their performance in terms of computational\noverhead, memory usage, and energy consumption for quantum secure\ncommunication. Experimental results demonstrate that the integration of PQC\nalgorithms on constrained hardware is practical, reinforcing the urgent need\nfor quantum-resilient cryptographic frameworks in next-generation IoT devices.\nThe implementation of this paper is available at\nhttps://iqsec-lab.github.io/PQC-IoT/.", "AI": {"tldr": "This paper investigates the feasibility of deploying post-quantum cryptography (PQC) algorithms on resource-constrained devices.", "motivation": "The rapid advancement of quantum computing poses a critical threat to classical cryptographic algorithms such as RSA and ECC, particularly in IoT devices.", "method": "Implementing three PQC algorithms (BIKE, CRYSTALS-Kyber, and HQC) on a lightweight IoT platform built with Raspberry Pi devices. Leveraging the Open Quantum Safe (liboqs) library in conjunction with mbedTLS to develop quantum-secure key exchange protocols.", "result": "Experimental results demonstrate that the integration of PQC algorithms on constrained hardware is practical.", "conclusion": "The integration of PQC algorithms on constrained hardware is practical and there is an urgent need for quantum-resilient cryptographic frameworks in next-generation IoT devices."}}
{"id": "2507.08264", "pdf": "https://arxiv.org/pdf/2507.08264", "abs": "https://arxiv.org/abs/2507.08264", "authors": ["Abhinav Sood", "Kazjon Grace", "Stephen Wan", "Cecile Paris"], "title": "Abductive Computational Systems: Creative Abduction and Future Directions", "categories": ["cs.AI"], "comment": "Published in the 16th International Conference on Computational\n  Creativity, ICCC25. Accepted Paper in\n  https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025abductive.pdf", "summary": "Abductive reasoning, reasoning for inferring explanations for observations,\nis often mentioned in scientific, design-related and artistic contexts, but its\nunderstanding varies across these domains. This paper reviews how abductive\nreasoning is discussed in epistemology, science and design, and then analyses\nhow various computational systems use abductive reasoning. Our analysis shows\nthat neither theoretical accounts nor computational implementations of\nabductive reasoning adequately address generating creative hypotheses.\nTheoretical frameworks do not provide a straightforward model for generating\ncreative abductive hypotheses, computational systems largely implement\nsyllogistic forms of abductive reasoning. We break down abductive computational\nsystems into components and conclude by identifying specific directions for\nfuture research that could advance the state of creative abductive reasoning in\ncomputational systems.", "AI": {"tldr": "this paper examines abductive reasoning across domains, highlighting its potential for creativity in computational systems but finds current implementations lacking", "motivation": "understanding the role of abductive reasoning across domains and its potential for creativity in computational systems", "method": "review of theoretical accounts and analysis of computational systems", "result": "theoretical frameworks and computational implementations do not adequately address creative abductive hypothesis generation", "conclusion": "abductive reasoning in computational systems requires further research for creative hypothesis generation"}}
{"id": "2507.08121", "pdf": "https://arxiv.org/pdf/2507.08121", "abs": "https://arxiv.org/abs/2507.08121", "authors": ["Tianchi Yu", "Ivan Oseledets"], "title": "Quasi-Random Physics-informed Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "Physics-informed neural networks have shown promise in solving partial\ndifferential equations (PDEs) by integrating physical constraints into neural\nnetwork training, but their performance is sensitive to the sampling of points.\nBased on the impressive performance of quasi Monte-Carlo methods in high\ndimensional problems, this paper proposes Quasi-Random Physics-Informed Neural\nNetworks (QRPINNs), which use low-discrepancy sequences for sampling instead of\nrandom points directly from the domain. Theoretically, QRPINNs have been proven\nto have a better convergence rate than PINNs. Empirically, experiments\ndemonstrate that QRPINNs significantly outperform PINNs and some representative\nadaptive sampling methods, especially in high-dimensional PDEs. Furthermore,\ncombining QRPINNs with adaptive sampling can further improve the performance.", "AI": {"tldr": "This paper introduces Quasi-Random Physics-Informed Neural Networks (QRPINNs), which utilize low-discrepancy sequences for improved sampling efficiency during the training of physics-informed neural networks.", "motivation": "Physics-informed neural networks' performance is sensitive to point sampling. The impressive performance of quasi Monte-Carlo methods in high dimensional problems inspires the exploration of better sampling strategies.", "method": "The paper proposes Quasi-Random Physics-Informed Neural Networks (QRPINNs), using low-discrepancy sequences for sampling instead of random points directly from the domain.", "result": "QRPINNs have been theoretically proven to have a better convergence rate than PINNs and empirically shown to outperform PINNs and some adaptive sampling methods, especially in high-dimensional PDEs.", "conclusion": "Quasi-Random Physics-Informed Neural Networks (QRPINNs) provide a more effective sampling method for training physics-informed neural networks, particularly in high-dimensional problems."}}
{"id": "2507.08331", "pdf": "https://arxiv.org/pdf/2507.08331", "abs": "https://arxiv.org/abs/2507.08331", "authors": ["Chun-I Fan", "Li-En Chang", "Cheng-Han Shie"], "title": "Qualcomm Trusted Application Emulation for Fuzzing Testing", "categories": ["cs.CR"], "comment": "This work is currently under review for presentation at the USENIX\n  Security 2025 poster session", "summary": "In recent years, the increasing awareness of cybersecurity has led to a\nheightened focus on information security within hardware devices and products.\nIncorporating Trusted Execution Environments (TEEs) into product designs has\nbecome a standard practice for safeguarding sensitive user information.\nHowever, vulnerabilities within these components present significant risks, if\nexploited by attackers, these vulnerabilities could lead to the leakage of\nsensitive data, thereby compromising user privacy and security. This research\ncenters on trusted applications (TAs) within the Qualcomm TEE and introduces a\nnovel emulator specifically designed for these applications. Through reverse\nengineering techniques, we thoroughly analyze Qualcomm TAs and develop a\npartial emulation environment that accurately emulates their behavior.\nAdditionally, we integrate fuzzing testing techniques into the emulator to\nsystematically uncover potential vulnerabilities within Qualcomm TAs,\ndemonstrating its practical effectiveness in identifying real-world security\nflaws. This research makes a significant contribution by being the first to\nprovide both the implementation methods and source codes for a Qualcomm TAs\nemulator, offering a valuable reference for future research efforts. Unlike\nprevious approaches that relied on complex and resource-intensive full-system\nsimulations, our approach is lightweight and effective, making security testing\nof TA more convenient.", "AI": {"tldr": "This research focuses on trusted applications within the Qualcomm TEE and introduces a novel emulator specifically designed for these applications. The research makes a significant contribution by providing both the implementation methods and source codes for the emulator.", "motivation": "The motivation behind this research is the increasing awareness of cybersecurity and the need to safeguard sensitive user information within hardware devices and products. Vulnerabilities within Trusted Execution Environments (TEEs) present significant risks, which could lead to the leakage of sensitive data and compromise user privacy and security.", "method": "The research involves reverse engineering techniques to thoroughly analyze Qualcomm TAs and develop a partial emulation environment. Additionally, fuzzing testing techniques are integrated into the emulator to systematically uncover potential vulnerabilities.", "result": "The research introduces a novel emulator specifically designed for trusted applications (TAs) within the Qualcomm TEE. It also provides both the implementation methods and source codes for the emulator, offering a valuable reference for future research efforts.", "conclusion": "This research provides a valuable reference for future research efforts in the field of trusted applications within Qualcomm TEE."}}
{"id": "2507.08270", "pdf": "https://arxiv.org/pdf/2507.08270", "abs": "https://arxiv.org/abs/2507.08270", "authors": ["Zeyang Sha", "Hanling Tian", "Zhuoer Xu", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"], "title": "Agent Safety Alignment via Reinforcement Learning", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.", "AI": {"tldr": "This paper proposes a safety-alignment framework for autonomous Large Language Model (LLM) agents that enables them to handle both user-initiated and tool-initiated threats. The framework uses structured reasoning, sandboxed reinforcement learning, and a custom-designed sandbox environment. Evaluations show improved resistance to security threats while maintaining utility on benign tasks.", "motivation": "The motivation is to address the new safety risks posed by autonomous Large Language Model (LLM) agents capable of tool usage, which are vulnerable to both user-initiated and tool-initiated threats.", "method": "The paper proposes a unified safety-alignment framework for tool-using agents, employing structured reasoning, sandboxed reinforcement learning, a tri-modal taxonomy, and a custom-designed sandbox environment.", "result": "The proposed safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "conclusion": "Safety and effectiveness can be jointly optimized for autonomous LLM agents, enabling their trustworthy deployment."}}
{"id": "2507.08124", "pdf": "https://arxiv.org/pdf/2507.08124", "abs": "https://arxiv.org/abs/2507.08124", "authors": ["Ashfaq Iftakher", "Rahul Golder", "M. M. Faruque Hasan"], "title": "Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints", "categories": ["cs.LG"], "comment": "20 pages, 8 figures", "summary": "Traditional physics-informed neural networks (PINNs) do not guarantee strict\nconstraint satisfaction. This is problematic in engineering systems where minor\nviolations of governing laws can significantly degrade the reliability and\nconsistency of model predictions. In this work, we develop KKT-Hardnet, a PINN\narchitecture that enforces both linear and nonlinear equality and inequality\nconstraints up to machine precision. It leverages a projection onto the\nfeasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a\ndistance minimization problem. Furthermore, we reformulate the nonlinear KKT\nconditions using log-exponential transformation to construct a general sparse\nsystem with only linear and exponential terms, thereby making the projection\ndifferentiable. We apply KKT-Hardnet on both test problems and a real-world\nchemical process simulation. Compared to multilayer perceptrons and PINNs,\nKKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This\napproach allows the integration of domain knowledge into machine learning\ntowards reliable hybrid modeling of complex systems.", "AI": {"tldr": "This paper introduces KKT-Hardnet, a PINN architecture that enforces strict constraint satisfaction up to machine precision, achieving higher accuracy and reliability in complex system modeling.", "motivation": "Traditional PINNs do not guarantee strict constraint satisfaction, which can significantly degrade the reliability and consistency of model predictions in engineering systems.", "method": "KKT-Hardnet is developed to enforce linear and nonlinear equality and inequality constraints up to machine precision using KKT conditions and log-exponential transformation.", "result": "KKT-Hardnet achieves higher accuracy and strict constraint satisfaction compared to multilayer perceptrons and PINNs.", "conclusion": "KKT-Hardnet achieves higher accuracy and strict constraint satisfaction, allowing the integration of domain knowledge into machine learning for reliable hybrid modeling of complex systems."}}
{"id": "2507.08540", "pdf": "https://arxiv.org/pdf/2507.08540", "abs": "https://arxiv.org/abs/2507.08540", "authors": ["Ioannis Lamprou", "Alexander Shevtsov", "Ioannis Arapakis", "Sotiris Ioannidis"], "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.", "AI": {"tldr": "White-Basilisk is a new method for detecting software vulnerabilities that beats current standards, using fewer resources and challenging AI model scaling assumptions.", "motivation": "The motivation is to address the challenge of software vulnerability detection with more effective methodologies.", "method": "White-Basilisk utilizes an architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework.", "result": "White-Basilisk achieves state-of-the-art results in vulnerability detection with only 200M parameters, processing extensive codebases in a single pass, and performs robustly on imbalanced datasets while maintaining computational efficiency.", "conclusion": "White-Basilisk establishes new benchmarks in code security and suggests that compact models can outperform larger ones in specialized tasks."}}
{"id": "2507.08306", "pdf": "https://arxiv.org/pdf/2507.08306", "abs": "https://arxiv.org/abs/2507.08306", "authors": ["Inclusion AI", ":", "Fudong Wang", "Jiajia Liu", "Jingdong Chen", "Jun Zhou", "Kaixiang Ji", "Lixiang Ru", "Qingpei Guo", "Ruobing Zheng", "Tianqi Li", "Yi Yuan", "Yifan Mao", "Yuting Xiao", "Ziping Ma"], "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "31pages, 14 figures", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578bM2-Reasoning-7B\uff0c\u5b83\u5728\u4e00\u822c\u548c\u7a7a\u95f4\u63a8\u7406\u4e0a\u90fd\u6709\u51fa\u8272\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u548c\u8bad\u7ec3\u7b56\u7565\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u52a8\u6001\u7a7a\u95f4\u4ea4\u4e92\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u80fd\u529b\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e2a\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86M2-Reasoning-7B\u6a21\u578b\u3002", "method": "\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u7ba1\u9053\uff0c\u751f\u6210\u4e86294.2K\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u6837\u672c\uff1b\u4ee5\u53ca\u4e00\u79cd\u52a8\u6001\u7684\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9010\u6b65\u4f18\u5316\u6765\u7f13\u89e3\u6570\u636e\u548c\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "result": "M2-Reasoning-7B\u5728\u4e00\u822c\u63a8\u7406\u548c\u7a7a\u95f4\u63a8\u7406\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "M2-Reasoning-7B\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u6210\u4e3a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2507.08153", "pdf": "https://arxiv.org/pdf/2507.08153", "abs": "https://arxiv.org/abs/2507.08153", "authors": ["Pinaki Prasad Guha Neogi", "Ahmad Mohammadshirazi", "Rajiv Ramnath"], "title": "ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic accidents are rare, yet high-impact events that require long-context\nmultimodal reasoning for accurate risk forecasting. In this paper, we introduce\nALCo-FM, a unified adaptive long-context foundation model that computes a\nvolatility pre-score to dynamically select context windows for input data and\nencodes and fuses these multimodal data via shallow cross attention. Following\na local GAT layer and a BigBird-style sparse global transformer over H3\nhexagonal grids, coupled with Monte Carlo dropout for confidence, the model\nyields superior, well-calibrated predictions. Trained on data from 15 US cities\nwith a class-weighted loss to counter label imbalance, and fine-tuned with\nminimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and\nan ECE of 0.04, outperforming more than 20 state-of-the-art baselines in\nlarge-scale urban risk prediction. Code and dataset are available at:\nhttps://github.com/PinakiPrasad12/ALCo-FM", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aALCo-FM\u7684\u6a21\u578b\uff0c\u5b83\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u81ea\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u8fdb\u884c\u4ea4\u901a\u610f\u5916\u98ce\u9669\u9884\u6d4b\u3002", "motivation": "\u4ea4\u901a\u4e8b\u6545\u662f\u7f55\u89c1\u4f46\u5177\u6709\u9ad8\u5f71\u54cd\u529b\u4e8b\u4ef6\uff0c\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u5f0f\u63a8\u7406\u6765\u8fdb\u884c\u51c6\u786e\u7684\u98ce\u9669\u9884\u6d4b\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u81ea\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u57fa\u7840\u6a21\u578bALCo-FM\uff0c\u901a\u8fc7\u8ba1\u7b97\u6ce2\u52a8\u6027\u9884\u8bc4\u5206\u6765\u52a8\u6001\u9009\u62e9\u8f93\u5165\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5e76\u901a\u8fc7\u6d45\u4ea4\u53c9\u6ce8\u610f\u529b\u7f16\u7801\u548c\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u3002\u6a21\u578b\u7ed3\u5408\u4e86\u5c40\u90e8GAT\u5c42\u548cBigBird\u98ce\u683c\u7684\u7a00\u758f\u5168\u5c40\u53d8\u538b\u5668\u4ee5\u53caH3\u516d\u8fb9\u5f62\u7f51\u683c\uff0c\u5e76\u4f7f\u7528\u8499\u7279\u5361\u6d1bdropout\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "result": "ALCo-FM\u6a21\u578b\u5728\u7f8e\u56fd15\u4e2a\u57ce\u5e02\u7684\u6570\u636e\u663e\u793a\u4e0b\uff0c\u53d6\u5f97\u4e860.94\u7684\u51c6\u786e\u7387\u30010.92\u7684F1\u503c\u548c0.04\u7684ECE\uff0c\u8d85\u8fc7\u4e8620\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ALCo-FM\u6a21\u578b\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u98ce\u9669\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u98ce\u9669\u3002"}}
{"id": "2507.08392", "pdf": "https://arxiv.org/pdf/2507.08392", "abs": "https://arxiv.org/abs/2507.08392", "authors": ["Asma Yamani", "Malak Baslyman", "Moataz Ahmed"], "title": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.", "AI": {"tldr": "Proposes a framework for generating ethics requirements drafts using an ethics advocate agent in a multi-agent LLM setting. Evaluation through two case studies shows that the framework captures most ethics requirements identified by researchers but also highlights reliability issues.", "motivation": "Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process.", "method": "The study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. The proposed framework is evaluated through two case studies from different contexts.", "result": "The proposed framework captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain.", "conclusion": "This work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products."}}
{"id": "2507.08154", "pdf": "https://arxiv.org/pdf/2507.08154", "abs": "https://arxiv.org/abs/2507.08154", "authors": ["Arisha Khan", "Nathaniel Li", "Tori Shen", "Anna N. Rafferty"], "title": "Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness", "categories": ["cs.LG"], "comment": "Poster paper at Educational Data Mining (EDM) 2025", "summary": "Machine learning has been proposed as a way to improve educational assessment\nby making fine-grained predictions about student performance and learning\nrelationships between items. One challenge with many machine learning\napproaches is incorporating new items, as these approaches rely heavily on\nhistorical data. We develop Text-LENS by extending the LENS partial variational\nauto-encoder for educational assessment to leverage item text embeddings, and\nexplore the impact on predictive performance and generalization to previously\nunseen items. We examine performance on two datasets: Eedi, a publicly\navailable dataset that includes item content, and LLM-Sim, a novel dataset with\ntest items produced by an LLM. We find that Text-LENS matches LENS' performance\non seen items and improves upon it in a variety of conditions involving unseen\nitems; it effectively learns student proficiency from and makes predictions\nabout student performance on new items.", "AI": {"tldr": "This paper presents Text-LENS, an extension of the LENS model that leverages item text embeddings for educational assessment. It performs as well as LENS on seen items and better on unseen items, effectively predicting student performance on new items.", "motivation": "The motivation is to tackle the challenge of incorporating new items in machine learning approaches in educational assessment, which traditionally rely heavily on historical data.", "method": "Text-LENS is developed by extending the LENS partial variational auto-encoder for educational assessment to leverage item text embeddings.", "result": "Text-LENS matches LENS' performance on seen items and improves upon it in a variety of conditions involving unseen items.", "conclusion": "Text-LENS effectively learns student proficiency from and makes predictions about student performance on new items."}}
{"id": "2507.08454", "pdf": "https://arxiv.org/pdf/2507.08454", "abs": "https://arxiv.org/abs/2507.08454", "authors": ["Tobias Geibinger", "Reijo Jaakkola", "Antti Kuusisto", "Xinghan Liu", "Miikka Vilander"], "title": "Why this and not that? A Logic-based Framework for Contrastive Explanations", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 03B05", "I.2.3; F.4.1"], "comment": "20 pages, accepted to JELIA 2025", "summary": "We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.", "AI": {"tldr": "This paper defines canonical problems related to contrastive explanations in propositional logic, providing an extensive analysis of their computational complexities and demonstrating how they work in practice.", "motivation": "To compute causes for both P and Q, explicitly comparing their differences, and answer questions of the form 'Why P but not Q?'.", "method": "Defining canonical problems related to contrastive explanations in propositional logic, investigating their basic properties, and implementing them for CNF-formulas using answer set programming.", "result": "The framework captures a cardinality-minimal version of existing contrastive explanations and the problems are implemented for CNF-formulas using answer set programming.", "conclusion": "The framework captures a cardinality-minimal version of existing contrastive explanations and provides an extensive analysis of the computational complexities of the problems."}}
{"id": "2507.08175", "pdf": "https://arxiv.org/pdf/2507.08175", "abs": "https://arxiv.org/abs/2507.08175", "authors": ["Md. Saif Hassan Onim", "Travis S. Humble", "Himanshu Thapliyal"], "title": "Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors", "categories": ["cs.LG", "cs.HC", "quant-ph"], "comment": null, "summary": "We investigate the feasibility of inferring emotional states exclusively from\nphysiological signals, thereby presenting a privacy-preserving alternative to\nconventional facial recognition techniques. We conduct a performance comparison\nof classical machine learning algorithms and hybrid quantum machine learning\n(QML) methods with a quantum kernel-based model. Our results indicate that the\nquantum-enhanced SVM surpasses classical counterparts in classification\nperformance across all emotion categories, even when trained on limited\ndatasets. The F1 scores over all classes are over 80% with around a maximum of\n36% improvement in the recall values. The integration of wearable sensor data\nwith quantum machine learning not only enhances accuracy and robustness but\nalso facilitates unobtrusive emotion recognition. This methodology holds\npromise for populations with impaired communication abilities, such as\nindividuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans\nwith Post-Traumatic Stress Disorder (PTSD). The findings establish an early\nfoundation for passive emotional monitoring in clinical and assisted living\nconditions.", "AI": {"tldr": "This paper presents a method for inferring emotional states from physiological signals using quantum machine learning, offering better privacy and accuracy than classical methods.", "motivation": "To explore a privacy-preserving alternative to conventional facial recognition techniques for inferring emotional states exclusively from physiological signals.", "method": "Comparison of classical machine learning algorithms and hybrid quantum machine learning methods with a quantum kernel-based model.", "result": "Quantum-enhanced SVM surpasses classical counterparts in classification performance across all emotion categories, even when trained on limited datasets. F1 scores over all classes are over 80% with around a maximum of 36% improvement in the recall values.", "conclusion": "The integration of wearable sensor data with quantum machine learning enhances accuracy and robustness in emotion recognition, holding promise for individuals with impaired communication abilities."}}
{"id": "2507.08597", "pdf": "https://arxiv.org/pdf/2507.08597", "abs": "https://arxiv.org/abs/2507.08597", "authors": ["Md Tanvirul Alam", "Aritran Piplai", "Nidhi Rastogi"], "title": "ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Machine learning models are commonly used for malware classification;\nhowever, they suffer from performance degradation over time due to concept\ndrift. Adapting these models to changing data distributions requires frequent\nupdates, which rely on costly ground truth annotations. While active learning\ncan reduce the annotation burden, leveraging unlabeled data through\nsemi-supervised learning remains a relatively underexplored approach in the\ncontext of malware detection. In this research, we introduce \\texttt{ADAPT}, a\nnovel pseudo-labeling semi-supervised algorithm for addressing concept drift.\nOur model-agnostic method can be applied to various machine learning models,\nincluding neural networks and tree-based algorithms. We conduct extensive\nexperiments on five diverse malware detection datasets spanning Android,\nWindows, and PDF domains. The results demonstrate that our method consistently\noutperforms baseline models and competitive benchmarks. This work paves the way\nfor more effective adaptation of machine learning models to concept drift in\nmalware detection.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u4f2a\u6807\u7b7e\u534a\u76d1\u7763\u7b97\u6cd5ADAPT\uff0c\u7528\u4e8e\u89e3\u51b3\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u88ab\u666e\u904d\u4f7f\u7528\uff0c\u4f46\u5b83\u4eec\u56e0\u6982\u5ff5\u6f02\u79fb\u800c\u968f\u65f6\u95f4\u9000\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u9891\u7e41\u66f4\u65b0\u548c\u6602\u8d35\u7684\u771f\u5b9e\u6807\u6ce8\u3002\u5c3d\u7ba1\u4e3b\u52a8\u5b66\u4e60\u53ef\u4ee5\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u9886\u57df\u4ecd\u7136\u662f\u76f8\u5bf9\u672a\u88ab\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86ADAPT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4f2a\u6807\u7b7e\u534a\u76d1\u7763\u7b97\u6cd5\u6765\u5e94\u5bf9\u6982\u5ff5\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u7ade\u4e89\u57fa\u51c6\u3002", "conclusion": "ADAPT\u7b97\u6cd5\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6982\u5ff5\u6f02\u79fb\u7684\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.08501", "pdf": "https://arxiv.org/pdf/2507.08501", "abs": "https://arxiv.org/abs/2507.08501", "authors": ["Keying Yang", "Hao Wang", "Kai Yang"], "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.", "AI": {"tldr": "This paper proposes a bi-level framework for structured reasoning over natural language inputs. The framework maps language to logic through a two-stage process: high-level task abstraction and low-level logic generation. Experiments show that the approach significantly outperforms existing baselines in accuracy.", "motivation": "Structured reasoning over natural language inputs remains a core challenge in artificial intelligence, as it requires bridging the gap between unstructured linguistic expressions and formal logical representations.", "method": "A novel bi-level framework that maps language to logic through a two-stage process: high-level task abstraction and low-level logic generation.", "result": "Experiments on multiple realistic reasoning benchmarks demonstrate that the approach significantly outperforms existing baselines in accuracy, with accuracy gains reaching as high as 40%.", "conclusion": "The bi-level framework proposed in this paper significantly outperforms existing baselines in accuracy, and enhances transparency and error traceability, offering a promising step toward trustworthy and systematic reasoning with LLMs."}}
{"id": "2507.08177", "pdf": "https://arxiv.org/pdf/2507.08177", "abs": "https://arxiv.org/abs/2507.08177", "authors": ["Arun Vignesh Malarkkan", "Haoyue Bai", "Xinyuan Wang", "Anjali Kaushik", "Dongjie Wang", "Yanjie Fu"], "title": "Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.NE", "F.2.2, I.2.7, I.2.4, I.2.1"], "comment": "5 pages, 1 figure, Under Review in Vision Paper Track-ACM SIGSPATIAL\n  2025", "summary": "As cyber-physical systems grow increasingly interconnected and spatially\ndistributed, ensuring their resilience against evolving cyberattacks has become\na critical priority. Spatio-Temporal Anomaly detection plays an important role\nin ensuring system security and operational integrity. However, current\ndata-driven approaches, largely driven by black-box deep learning, face\nchallenges in interpretability, adaptability to distribution shifts, and\nrobustness under evolving system dynamics. In this paper, we advocate for a\ncausal learning perspective to advance anomaly detection in spatially\ndistributed infrastructures that grounds detection in structural cause-effect\nrelationships. We identify and formalize three key directions: causal graph\nprofiling, multi-view fusion, and continual causal graph learning, each\noffering distinct advantages in uncovering dynamic cause-effect structures\nacross time and space. Drawing on real-world insights from systems such as\nwater treatment infrastructures, we illustrate how causal models provide early\nwarning signals and root cause attribution, addressing the limitations of\nblack-box detectors. Looking ahead, we outline the future research agenda\ncentered on multi-modality, generative AI-driven, and scalable adaptive causal\nframeworks. Our objective is to lay a new research trajectory toward scalable,\nadaptive, explainable, and spatially grounded anomaly detection systems. We\nhope to inspire a paradigm shift in cybersecurity research, promoting\ncausality-driven approaches to address evolving threats in interconnected\ninfrastructures.", "AI": {"tldr": "This paper addresses the resilience of cyber-physical systems against cyberattacks through spatio-temporal anomaly detection. It advocates for a causal learning approach to overcome the limitations of current data-driven methods, focusing on interpretability, adaptability, and robustness.", "motivation": "The motivation behind this paper is the increasing interconnection and spatial distribution of cyber-physical systems, which necessitates enhanced resilience against evolving cyberattacks. Current data-driven approaches face challenges in interpretability, adaptability to distribution shifts, and robustness under evolving system dynamics.", "method": "The paper advocates for a causal learning perspective to advance anomaly detection in spatially distributed infrastructures, emphasizing structural cause-effect relationships. It identifies three key directions: causal graph profiling, multi-view fusion, and continual causal graph learning.", "result": "By drawing on real-world insights from systems such as water treatment infrastructures, the paper illustrates how causal models provide early warning signals and root cause attribution, addressing the limitations of black-box detectors.", "conclusion": "The paper concludes by outlining a future research agenda focused on multi-modality, generative AI-driven, and scalable adaptive causal frameworks. The objective is to inspire a new research trajectory towards scalable, adaptive, explainable, and spatially grounded anomaly detection systems, promoting a paradigm shift in cybersecurity research."}}
{"id": "2507.08529", "pdf": "https://arxiv.org/pdf/2507.08529", "abs": "https://arxiv.org/abs/2507.08529", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qin", "Guoyu Ye", "Ruixiang Tang"], "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages,3 figures", "summary": "Despite advances from medical large language models in healthcare,\nrare-disease diagnosis remains hampered by insufficient\nknowledge-representation depth, limited concept understanding, and constrained\nclinical reasoning. We propose a framework that couples multi-granularity\nsparse activation of medical concepts with a hierarchical knowledge graph. Four\ncomplementary matching algorithms, diversity control, and a five-level fallback\nstrategy enable precise concept activation, while a three-layer knowledge graph\n(taxonomy, clinical features, instances) provides structured, up-to-date\ncontext. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,\nROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89\napproaching the 0.90 clinical threshold. Expert evaluation confirms\nimprovements in information quality, reasoning, and professional expression,\nsuggesting our approach shortens the \"diagnostic odyssey\" for rare-disease\npatients.", "AI": {"tldr": "A novel framework for improving rare-disease diagnosis using medical large language models.", "motivation": "Rare-disease diagnosis remains hampered by insufficient knowledge-representation depth, limited concept understanding, and constrained clinical reasoning.", "method": "A framework that couples multi-granularity sparse activation of medical concepts with a hierarchical knowledge graph is proposed. Four complementary matching algorithms, diversity control, and a five-level fallback strategy enable precise concept activation. A three-layer knowledge graph (taxonomy, clinical features, instances) provides structured, up-to-date context.", "result": "Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09, ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89 approaching the 0.90 clinical threshold.", "conclusion": "The proposed framework improves the performance of medical large language models in rare-disease diagnosis, as evidenced by increased accuracy and expert evaluation."}}
{"id": "2507.08182", "pdf": "https://arxiv.org/pdf/2507.08182", "abs": "https://arxiv.org/abs/2507.08182", "authors": ["Junda Wu", "Yuxin Xiong", "Xintong Li", "Zhengmian Hu", "Tong Yu", "Rui Wang", "Xiang Chen", "Jingbo Shang", "Julian McAuley"], "title": "CTRLS: Chain-of-Thought Reasoning via Latent State-Transition", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to\nbreak down complex problems into interpretable intermediate steps,\nsignificantly enhancing model transparency and performance in reasoning tasks.\nHowever, conventional CoT methods rely on heuristic sampling without structured\nmodeling of reasoning transitions, constraining their ability to systematically\nexplore and discover diverse and effective reasoning trajectories. In this\nwork, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov\ndecision process (MDP) with latent state transitions, enabling principled and\nstate-aware exploration via distributional reinforcement learning. By modelling\nreasoning actions as explicit probability distributions in latent space, our\napproach explicitly models epistemic uncertainty, facilitating robust\nexploration of the reasoning space. As part of our framework, we introduce an\non-policy reinforcement learning strategy incorporating epsilon-greedy\nexploration and entropy-based regularization to iteratively refine latent state\ntransitions without requiring additional fine-tuning of the underlying LLM.\nTheoretical analyses provide evidence lower bounds (ELBO), theoretically\ngrounding our transition-aware modeling of latent reasoning dynamics. Further\nexperiments demonstrate improvements in reasoning accuracy, diversity, and\nexploration efficiency across benchmark reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u6846\u67b6CTRLS\uff0c\u901a\u8fc7\u5c06\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u5e26\u6709\u6f5c\u5728\u72b6\u6001\u8f6c\u79fb\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cfb\u7edf\u3001\u66f4\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u3002", "motivation": "\u5e38\u89c4\u7684CoT\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u91c7\u6837\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8f6c\u79fb\u7684\u7ed3\u6784\u5316\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u5730\u63a2\u7d22\u548c\u53d1\u73b0\u591a\u6837\u4e14\u6709\u6548\u7684\u63a8\u7406\u8f68\u8ff9\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6CTRLS\uff0c\u5c06\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u5e26\u6709\u6f5c\u5728\u72b6\u6001\u8f6c\u79fb\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6709\u539f\u5219\u7684\u3001\u5bf9\u72b6\u6001\u654f\u611f\u7684\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCTRLS\u5728\u57fa\u51c6\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u63a2\u7d22\u6548\u7387\u3002\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u8bc1\u636e\u4e0b\u754c(ELBO)\uff0c\u4ece\u7406\u8bba\u4e0a\u652f\u6301\u4e86\u5bf9\u6f5c\u5728\u63a8\u7406\u52a8\u6001\u7684\u8f6c\u6362\u611f\u77e5\u5efa\u6a21\u3002", "conclusion": "CTRLS\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2507.08575", "pdf": "https://arxiv.org/pdf/2507.08575", "abs": "https://arxiv.org/abs/2507.08575", "authors": ["Kalana Wijegunarathna", "Kristin Stock", "Christopher B. Jones"], "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.", "AI": {"tldr": "This paper presents a novel method that uses multi-modal capabilities of recent Large Multi-Modal Models (LMM) for georeferencing complex locality descriptions associated with biological sample records.", "motivation": "Millions of biological sample records are un-georeferenced and existing automated methods do not exploit maps which are essential for georeferencing complex relations.", "method": "A novel method that exploits multi-modal capabilities of recent Large Multi-Modal Models (LMM) using a grid-based approach in a zero-shot setting.", "result": "Experiments on a small manually annotated dataset show impressive results with an average distance error of ~1 km.", "conclusion": "The proposed method shows potential for improving georeferencing accuracy by exploiting multi-modal models and a practical framework is proposed to integrate this method into a georeferencing workflow."}}
{"id": "2507.08212", "pdf": "https://arxiv.org/pdf/2507.08212", "abs": "https://arxiv.org/abs/2507.08212", "authors": ["Mohammad Sadegh Akhondzadeh", "Soroush H. Zargarbashi", "Jimin Cao", "Aleksandar Bojchevski"], "title": "EvA: Evolutionary Attacks on Graphs", "categories": ["cs.LG"], "comment": "23 pages, 12 figures", "summary": "Even a slight perturbation in the graph structure can cause a significant\ndrop in the accuracy of graph neural networks (GNNs). Most existing attacks\nleverage gradient information to perturb edges. This relaxes the attack's\noptimization problem from a discrete to a continuous space, resulting in\nsolutions far from optimal. It also restricts the adaptability of the attack to\nnon-differentiable objectives. Instead, we introduce a few simple yet effective\nenhancements of an evolutionary-based algorithm to solve the discrete\noptimization problem directly. Our Evolutionary Attack (EvA) works with any\nblack-box model and objective, eliminating the need for a differentiable proxy\nloss. This allows us to design two novel attacks that reduce the effectiveness\nof robustness certificates and break conformal sets. The memory complexity of\nour attack is linear in the attack budget. Among our experiments, EvA shows\n$\\sim$11\\% additional drop in accuracy on average compared to the best previous\nattack, revealing significant untapped potential in designing attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fdb\u5316\u653b\u51fb\uff08EvA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5373\u4f7f\u56fe\u7ed3\u6784\u7684\u5fae\u5c0f\u6270\u52a8\u4e5f\u53ef\u80fd\u5bfc\u81f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u51c6\u786e\u6027\u5927\u5e45\u4e0b\u964d\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u653b\u51fb\u65b9\u6cd5\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u6765\u6270\u52a8\u8fb9\uff0c\u8fd9\u5c06\u653b\u51fb\u7684\u4f18\u5316\u95ee\u9898\u4ece\u79bb\u6563\u7a7a\u95f4\u653e\u5bbd\u5230\u8fde\u7eed\u7a7a\u95f4\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u8fdc\u79bb\u6700\u4f18\u89e3\uff0c\u5e76\u9650\u5236\u4e86\u653b\u51fb\u5bf9\u975e\u53ef\u5fae\u76ee\u6807\u7684\u9002\u5e94\u6027\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\u6765\u76f4\u63a5\u89e3\u51b3\u79bb\u6563\u4f18\u5316\u95ee\u9898\u3002\u6211\u4eec\u7684\u8fdb\u5316\u653b\u51fb\uff08Evolutionary Attack\uff0c\u7b80\u79f0EvA\uff09\u53ef\u4ee5\u4e0e\u4efb\u4f55\u9ed1\u76d2\u6a21\u578b\u548c\u76ee\u6807\u4e00\u8d77\u4f7f\u7528\uff0c\u6d88\u9664\u4e86\u5bf9\u53ef\u5fae\u4ee3\u7406\u635f\u5931\u7684\u9700\u6c42\u3002", "result": "EvA\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u51cf\u5c11\u9c81\u68d2\u6027\u8bc1\u4e66\u7684\u6709\u6548\u6027\u5e76\u7834\u574f\u5171\u5f62\u96c6\u3002\u8be5\u653b\u51fb\u7684\u8bb0\u5fc6\u590d\u6742\u5ea6\u4e0e\u653b\u51fb\u9884\u7b97\u5448\u7ebf\u6027\u5173\u7cfb\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u800c\u8a00\uff0cEvA\u76f8\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u653b\u51fb\u65b9\u6cd5\u989d\u5916\u964d\u4f4e\u4e86\u7ea611%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "EvA\u5728\u5b9e\u9a8c\u4e2d\u5e73\u5747\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u653b\u51fb\u65b9\u6cd5\u989d\u5916\u964d\u4f4e\u4e86\u7ea611%\u7684\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86\u8bbe\u8ba1\u653b\u51fb\u65b9\u6cd5\u4e2d\u5b58\u5728\u663e\u8457\u7684\u672a\u5f00\u53d1\u6f5c\u529b\u3002"}}
{"id": "2507.08603", "pdf": "https://arxiv.org/pdf/2507.08603", "abs": "https://arxiv.org/abs/2507.08603", "authors": ["Yonghua Hei", "Yibo Yan", "Shuliang Liu", "Huiyu Zhou", "Linfeng Zhang", "Xuming Hu"], "title": "Unlocking Speech Instruction Data Potential with Query Rewriting", "categories": ["cs.AI", "cs.SD", "eess.AS"], "comment": "ACL 2025 Findings", "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5c06\u5206\u5e03\u5916\u6587\u672c\u6307\u4ee4\u9002\u5f53\u5730\u8f6c\u6362\u4e3a\u8bed\u97f3\u7684\u6311\u6218\uff0c\u4ece\u800c\u5b9e\u73b0\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u7684\u76ee\u6807\u3002", "motivation": "\u7aef\u5230\u7aef\u5927\u578b\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u8bed\u97f3\u6307\u4ee4\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u4efb\u52a1\u7684\u4e25\u91cd\u504f\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u800c\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6ce8\u91ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u91c7\u7528\u591a\u4e2a\u4ee3\u7406\u5bf9\u5408\u6210\u8bed\u97f3\u8fdb\u884c\u6ce8\u91ca\u548c\u9a8c\u8bc1\uff0c\u4ee5\u96f6\u6837\u672c\u91cd\u5199\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u66f4\u9002\u5408TTS\u6a21\u578b\u5206\u5e03\u7684\u5f62\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u96f6\u6837\u672c\u91cd\u5199\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u66f4\u9002\u5408TTS\u6a21\u578b\u8fdb\u884c\u8bed\u97f3\u5408\u6210\u7684\u5206\u5e03\uff0c\u5c06\u6570\u636e\u53ef\u7528\u6027\u4ece72%\u589e\u52a0\u523093%\u3002", "conclusion": "\u4f7f\u7528\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\u80fd\u591f\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u6570\u636e\u53ef\u7528\u6027\uff0c\u5e76\u5728\u9700\u8981\u590d\u6742\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u80fd\u529b\u7684\u91cd\u5199\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2507.08235", "pdf": "https://arxiv.org/pdf/2507.08235", "abs": "https://arxiv.org/abs/2507.08235", "authors": ["Pinaki Prasad Guha Neogi", "Ahmad Mohammadshirazi", "Rajiv Ramnath"], "title": "InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Smart buildings generate vast streams of sensor and control data, but\nfacility managers often lack clear explanations for anomalous energy usage. We\npropose InsightBuild, a two-stage framework that integrates causality analysis\nwith a fine-tuned large language model (LLM) to provide human-readable, causal\nexplanations of energy consumption patterns. First, a lightweight causal\ninference module applies Granger causality tests and structural causal\ndiscovery on building telemetry (e.g., temperature, HVAC settings, occupancy)\ndrawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,\nfine-tuned on aligned pairs of sensor-level causes and textual explanations,\nreceives as input the detected causal relations and generates concise,\nactionable explanations. We evaluate InsightBuild on two real-world datasets\n(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth\ncauses for a held-out set of anomalies. Our results demonstrate that combining\nexplicit causal discovery with LLM-based natural language generation yields\nclear, precise explanations that assist facility managers in diagnosing and\nmitigating energy inefficiencies.", "AI": {"tldr": "This paper proposes InsightBuild, a two-stage framework integrating causality analysis with a fine-tuned large language model to provide clear and actionable explanations of energy consumption patterns.", "motivation": "Smart buildings generate vast streams of sensor and control data, but facility managers often lack clear explanations for anomalous energy usage.", "method": "InsightBuild, a two-stage framework that integrates causality analysis with a fine-tuned large language model (LLM) to provide human-readable, causal explanations of energy consumption patterns.", "result": "The evaluation on two real-world datasets demonstrates that combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations.", "conclusion": "Combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations that assist facility managers in diagnosing and mitigating energy inefficiencies."}}
{"id": "2507.08619", "pdf": "https://arxiv.org/pdf/2507.08619", "abs": "https://arxiv.org/abs/2507.08619", "authors": ["Soheyl Massoudi", "Mark Fuge"], "title": "Agentic Large Language Models for Conceptual Systems Engineering and Design", "categories": ["cs.AI"], "comment": "32 pages, 3 figures", "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.", "AI": {"tldr": "This paper evaluates whether a structured multi-agent system can manage engineering design tasks more effectively than a two-agent system, focusing on a solar-powered water filtration system.", "motivation": "Early-stage engineering design involves complex, iterative reasoning that existing LLM workflows struggle to handle effectively.", "method": "Evaluate a structured multi-agent system (MAS) against a simpler two-agent system (2AS) for managing engineering design tasks.", "result": "The MAS generated more granular DSGs, while 2AS mode-collapsed. Reasoning-distilled LLM improved completion rates.", "conclusion": "Structured multi-agent orchestration enhances design detail, but low requirements and fidelity gaps in coding persist."}}
{"id": "2507.08238", "pdf": "https://arxiv.org/pdf/2507.08238", "abs": "https://arxiv.org/abs/2507.08238", "authors": ["Abinay Reddy Naini", "Zhaobo K. Zheng", "Teruhisa Misu", "Kumar Akash"], "title": "Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions", "categories": ["cs.LG"], "comment": "5 pages, 4 figures, published at ICASSP 2025", "summary": "Human state detection and behavior prediction have seen significant\nadvancements with the rise of machine learning and multimodal sensing\ntechnologies. However, predicting prosocial behavior intentions in mobility\nscenarios, such as helping others on the road, is an underexplored area.\nCurrent research faces a major limitation. There are no large, labeled datasets\navailable for prosocial behavior, and small-scale datasets make it difficult to\ntrain deep-learning models effectively. To overcome this, we propose a\nself-supervised learning approach that harnesses multi-modal data from existing\nphysiological and behavioral datasets. By pre-training our model on diverse\ntasks and fine-tuning it with a smaller, manually labeled prosocial behavior\ndataset, we significantly enhance its performance. This method addresses the\ndata scarcity issue, providing a more effective benchmark for prosocial\nbehavior prediction, and offering valuable insights for improving intelligent\nvehicle systems and human-machine interaction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9488\u5bf9\u79fb\u52a8\u573a\u666f\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\u610f\u56fe\u9884\u6d4b\u8fd9\u4e00\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u751f\u7406\u548c\u884c\u4e3a\u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u4eb2\u793e\u4f1a\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u6539\u8fdb\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "motivation": "\u9884\u6d4b\u79fb\u52a8\u573a\u666f\u4e2d\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\u610f\u56fe\uff08\u5982\u5728\u8def\u4e0a\u5e2e\u52a9\u4ed6\u4eba\uff09\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5f53\u524d\u7684\u7814\u7a76\u9762\u4e34\u4e00\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u6ca1\u6709\u5927\u578b\u7684\u3001\u6807\u8bb0\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u5c0f\u89c4\u6a21\u7684\u6570\u636e\u96c6\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u7684\u751f\u7406\u548c\u884c\u4e3a\u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u901a\u8fc7\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u4f7f\u7528\u8f83\u5c0f\u7684\u624b\u52a8\u6807\u8bb0\u4eb2\u793e\u4f1a\u884c\u4e3a\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u4f7f\u7528\u8f83\u5c0f\u7684\u624b\u52a8\u6807\u8bb0\u4eb2\u793e\u4f1a\u884c\u4e3a\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u4eb2\u793e\u4f1a\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u6539\u8fdb\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.08649", "pdf": "https://arxiv.org/pdf/2507.08649", "abs": "https://arxiv.org/abs/2507.08649", "authors": ["Xingguang Ji", "Yahui Liu", "Qi Wang", "Jingyuan Zhang", "Yang Yue", "Rui Shi", "Chenxi Sun", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning", "categories": ["cs.AI"], "comment": "23 pages, 13 figures", "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Leanabell-Prover-V2\uff0c\u4e00\u4e2a\u53ef\u4ee5\u751f\u6210Lean 4\u4e2d\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u5176\u91c7\u7528\u7684\u6280\u672f\u624b\u6bb5\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u57fa\u4e8e\u4e4b\u524d\u7684\u5de5\u4f5cLeanabell-Prover-V1\uff0c\u9009\u62e9\u5bf9\u73b0\u6709\u7684\u5f3a\u5927\u8bc1\u660e\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5347\u7ea7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0eLean 4\u9a8c\u8bc1\u5668\u63d0\u4f9b\u7684\u53cd\u9988\uff0c\u4e3b\u8981\u6539\u8fdb\u4e86\u6a21\u578b\u7684\u81ea\u6211\u610f\u8bc6\u548c\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u591a\u8f6e\u9a8c\u8bc1\u5668\u4ea4\u4e92\u3001\u53cd\u9988\u4ee4\u724c\u5c4f\u853d\u548c\u7b80\u5355\u7684\u5956\u52b1\u7b56\u7565\u6765\u4f18\u5316LLM\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLeanabell-Prover-V2\u5728MiniF2F\u6d4b\u8bd5\u96c6\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "Leanabell-Prover-V2\u63d0\u9ad8\u4e86\u5728MiniF2F\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u7528Kimina-Prover-Preview-Distill-7B\u63d0\u9ad8\u4e863.2%\uff08pass@128\uff09\uff0c\u4f7f\u7528DeepSeek-Prover-V2-7B\u63d0\u9ad8\u4e862.0%\uff08pass@128\uff09\u3002"}}
{"id": "2507.08239", "pdf": "https://arxiv.org/pdf/2507.08239", "abs": "https://arxiv.org/abs/2507.08239", "authors": ["Hadi Daneshmand", "Ashkan Soleymani"], "title": "Data Generation without Function Estimation", "categories": ["cs.LG", "math-ph", "math.MP", "math.OC", "stat.ML"], "comment": null, "summary": "Estimating the score function (or other population-density-dependent\nfunctions) is a fundamental component of most generative models. However, such\nfunction estimation is computationally and statistically challenging. Can we\navoid function estimation for data generation? We propose an estimation-free\ngenerative method: A set of points whose locations are deterministically\nupdated with (inverse) gradient descent can transport a uniform distribution to\narbitrary data distribution, in the mean field regime, without function\nestimation, training neural networks, and even noise injection. The proposed\nmethod is built upon recent advances in the physics of interacting particles.\nWe show, both theoretically and experimentally, that these advances can be\nleveraged to develop novel generative methods.", "AI": {"tldr": "This paper proposes an estimation-free generative method that uses deterministic updates with (inverse) gradient descent to transport a uniform distribution to arbitrary data distribution, avoiding function estimation, training neural networks, and even noise injection.", "motivation": "Estimating the score function is a fundamental component of most generative models, but such function estimation is computationally and statistically challenging. The motivation is to avoid function estimation for data generation.", "method": "A set of points whose locations are deterministically updated with (inverse) gradient descent is used to transport a uniform distribution to arbitrary data distribution.", "result": "The proposed method built upon recent advances in the physics of interacting particles can leverage these advances to develop novel generative methods.", "conclusion": "The proposed estimation-free generative method can transport a uniform distribution to arbitrary data distribution without function estimation, training neural networks, and even noise injection."}}
{"id": "2507.08664", "pdf": "https://arxiv.org/pdf/2507.08664", "abs": "https://arxiv.org/abs/2507.08664", "authors": ["Haoran Sun", "Shaoning Zeng"], "title": "Introspection of Thought Helps AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u4ee3\u7406\u63a8\u7406\u6846\u67b6INoT\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684LLM-Read\u4ee3\u7801\u6765\u5b9e\u73b0\u7a0b\u5e8f\u5bf9\u8bdd\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u4ee4\u724c\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684AI\u4ee3\u7406\u53d7\u9650\u4e8eLLM\u5728\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u4ee3\u7406\u63a8\u7406\u6846\u67b6INoT\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684LLM-Read\u4ee3\u7801\u6765\u5b9e\u73b0\u7a0b\u5e8f\u5bf9\u8bdd\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4e09\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\uff0cINoT\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e867.95%\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u3002\u6b64\u5916\uff0cINoT\u7684\u4ee4\u724c\u6210\u672c\u5e73\u5747\u6bd4\u57fa\u7ebf\u7684\u6700\u4f73\u8868\u73b0\u65b9\u6cd5\u4f4e58.3%\u3002", "conclusion": "INoT\u662f\u4e00\u4e2a\u6709\u6548\u7684AI\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u53ef\u4ee5\u51cf\u5c11\u4ee4\u724c\u6210\u672c\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.08243", "pdf": "https://arxiv.org/pdf/2507.08243", "abs": "https://arxiv.org/abs/2507.08243", "authors": ["Chandra Sekhar Mukherjee", "Joonyoung Bae", "Jiapeng Zhang"], "title": "CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry", "categories": ["cs.LG"], "comment": null, "summary": "Density and geometry have long served as two of the fundamental guiding\nprinciples in clustering algorithm design, with algorithm usually focusing\neither on the density structure of the data (e.g., HDBSCAN and Density Peak\nClustering) or the complexity of underlying geometry (e.g., manifold clustering\nalgorithms).\n  In this paper, we identify and formalize a recurring but often overlooked\ninteraction between distribution and geometry and leverage this insight to\ndesign our clustering enhancement framework CoreSPECT (Core Space\nProjection-based Enhancement of Clustering Techniques). Our framework boosts\nthe performance of simple algorithms like K-Means and GMM by applying them to\nstrategically selected regions, then extending the partial partition to a\ncomplete partition for the dataset using a novel neighborhood graph based\nmulti-layer propagation procedure.\n  We apply our framework on 15 datasets from three different domains and obtain\nconsistent and substantial gain in clustering accuracy for both K-Means and\nGMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by\n14%, often surpassing the performance of both manifold-based and recent\ndensity-based clustering algorithms. We further support our framework with\ninitial theoretical guarantees, ablation to demonstrate the usefulness of the\nindividual steps and with evidence of robustness to noise.", "AI": {"tldr": "CoreSPECT leverages the interaction between data distribution and geometry to enhance simple clustering algorithms, resulting in significant accuracy improvements.", "motivation": "The interaction between distribution and geometry in clustering algorithms is often overlooked. By formalizing this interaction, an enhancement to existing clustering techniques can be achieved.", "method": "The CoreSPECT framework applies simple clustering algorithms to strategically selected regions and uses a novel neighborhood graph based multi-layer propagation procedure to extend the partition.", "result": "CoreSPECT improves the ARI of K-Means by 40% and of GMM by 14% on average. It consistently outperforms manifold-based and recent density-based clustering algorithms across 15 datasets from three domains.", "conclusion": "The CoreSPECT framework significantly enhances the performance of simple clustering algorithms like K-Means and GMM, often surpassing more complex algorithms. It is robust to noise and supported by theoretical guarantees."}}
{"id": "2507.08705", "pdf": "https://arxiv.org/pdf/2507.08705", "abs": "https://arxiv.org/abs/2507.08705", "authors": ["Philip Osborne", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings", "categories": ["cs.AI", "I.2.5; I.2.1; I.2.7; I.2.11"], "comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP\n  2025 Call for System Demonstrations", "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.", "AI": {"tldr": "This paper presents elsciRL, an open-source Python library that facilitates the application of language solutions on reinforcement learning problems. The library extends the Language Adapter with Self-Completing Instruction framework and provides a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete. Empirical results indicate that these instructions can improve a reinforcement learning agent's performance.", "motivation": "The motivation behind this paper is to extend the Language Adapter with Self-Completing Instruction framework defined in (Osborne, 2024) with the use of LLMs and provide a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete.", "method": "The paper presents an open-source Python library called elsciRL, which facilitates the application of language solutions on reinforcement learning problems.", "result": "Empirical results indicate that these instructions can improve a reinforcement learning agent's performance.", "conclusion": "The elsciRL library can accelerate the evaluation of language solutions on reward based environments, enabling new opportunities for scientific discovery."}}
{"id": "2507.08255", "pdf": "https://arxiv.org/pdf/2507.08255", "abs": "https://arxiv.org/abs/2507.08255", "authors": ["Hossein Jamali"], "title": "Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Missing data presents a critical challenge in real-world datasets,\nsignificantly degrading the performance of machine learning models. While Large\nLanguage Models (LLMs) have recently demonstrated remarkable capabilities in\ntabular data imputation, exemplified by frameworks like UnIMP, their reliance\non classical embedding methods often limits their ability to capture complex,\nnon-linear correlations, particularly in mixed-type data scenarios encompassing\nnumerical, categorical, and textual features. This paper introduces\nQuantum-UnIMP, a novel framework that integrates shallow quantum circuits into\nan LLM-based imputation architecture. Our core innovation lies in replacing\nconventional classical input embeddings with quantum feature maps generated by\nan Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the\nmodel to leverage quantum phenomena such as superposition and entanglement,\nthereby learning richer, more expressive representations of data and enhancing\nthe recovery of intricate missingness patterns. Our experiments on benchmark\nmixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by\nup to 15.2% for numerical features (RMSE) and improves classification accuracy\nby 8.7% for categorical features (F1-Score) compared to state-of-the-art\nclassical and LLM-based methods. These compelling results underscore the\nprofound potential of quantum-enhanced representations for complex data\nimputation tasks, even with near-term quantum hardware.", "AI": {"tldr": "This paper addresses the challenge of missing data in real-world datasets by introducing Quantum-UnIMP, which leverages quantum circuits to enhance data imputation.", "motivation": "Missing data presents a critical challenge in real-world datasets, significantly degrading the performance of machine learning models. While Large Language Models (LLMs) have shown capabilities in tabular data imputation, their reliance on classical embedding methods often limits their ability to capture complex, non-linear correlations, particularly in mixed-type data scenarios.", "method": "This paper introduces Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into an LLM-based imputation architecture. The core innovation is replacing conventional classical input embeddings with quantum feature maps generated by an Instantaneous Quantum Polynomial (IQP) circuit.", "result": "Experiments on benchmark mixed-type datasets show that Quantum-UnIMP reduces imputation error by up to 15.2% for numerical features (RMSE) and improves classification accuracy by 8.7% for categorical features (F1-Score) compared to state-of-the-art classical and LLM-based methods.", "conclusion": "Quantum-UnIMP demonstrates the potential of quantum-enhanced representations for complex data imputation tasks, even with near-term quantum hardware."}}
{"id": "2507.08715", "pdf": "https://arxiv.org/pdf/2507.08715", "abs": "https://arxiv.org/abs/2507.08715", "authors": ["Paul Saves", "Jasper Bussemaker", "R\u00e9mi Lafage", "Thierry Lefebvre", "Nathalie Bartoli", "Youssef Diouane", "Joseph Morlier"], "title": "System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility", "categories": ["cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "For developing innovative systems architectures, modeling and optimization\ntechniques have been central to frame the architecting process and define the\noptimization and modeling problems. In this context, for system-of-systems the\nuse of efficient dedicated approaches (often physics-based simulations) is\nhighly recommended to reduce the computational complexity of the targeted\napplications. However, exploring novel architectures using such dedicated\napproaches might pose challenges for optimization algorithms, including\nincreased evaluation costs and potential failures. To address these challenges,\nsurrogate-based optimization algorithms, such as Bayesian optimization\nutilizing Gaussian process models have emerged.", "AI": {"tldr": "This paper discusses the use of surrogate-based optimization algorithms, specifically Bayesian optimization utilizing Gaussian process models, to address the challenges posed by dedicated approaches when exploring novel architectures in system-of-systems.", "motivation": "The motivation behind the paper is to address the challenges posed by the use of efficient dedicated approaches (often physics-based simulations) for exploring novel architectures in system-of-systems.", "method": "The paper discusses the use of surrogate-based optimization algorithms, specifically Bayesian optimization utilizing Gaussian process models.", "result": "The result of the paper is the emergence of surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models.", "conclusion": "Surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models, have emerged to address the challenges posed by dedicated approaches."}}
{"id": "2507.08267", "pdf": "https://arxiv.org/pdf/2507.08267", "abs": "https://arxiv.org/abs/2507.08267", "authors": ["Hiroshi Yoshihara", "Taiki Yamaguchi", "Yuichi Inoue"], "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ICML 2025 Workshop on The second AI for MATH", "summary": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.", "AI": {"tldr": "The paper addresses enhancing LLMs' mathematical reasoning by combining extended SFT and RL through a new method called GRPO, showing superior model accuracy and token efficiency.", "motivation": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities", "method": "a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO)", "result": "Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length.", "conclusion": "This work provides a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient."}}
{"id": "2507.08269", "pdf": "https://arxiv.org/pdf/2507.08269", "abs": "https://arxiv.org/abs/2507.08269", "authors": ["Woon Ryong Kim", "Jaeheun Jung", "Jeong Un Ha", "Donghun Lee", "Jae Kyung Shim"], "title": "Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization", "categories": ["cs.LG"], "comment": null, "summary": "Dimensional synthesis of planar four-bar mechanisms is a challenging inverse\nproblem in kinematics, requiring the determination of mechanism dimensions from\ndesired motion specifications. We propose a data-driven framework that bypasses\ntraditional equation-solving and optimization by leveraging supervised\nlearning. Our method combines a synthetic dataset, an LSTM-based neural network\nfor handling sequential precision points, and a Mixture of Experts (MoE)\narchitecture tailored to different linkage types. Each expert model is trained\non type-specific data and guided by a type-specifying layer, enabling both\nsingle-type and multi-type synthesis. A novel simulation metric evaluates\nprediction quality by comparing desired and generated motions. Experiments show\nour approach produces accurate, defect-free linkages across various\nconfigurations. This enables intuitive and efficient mechanism design, even for\nnon-expert users, and opens new possibilities for scalable and flexible\nsynthesis in kinematic design.", "AI": {"tldr": "A data-driven framework using supervised learning is proposed for dimensional synthesis of planar four-bar mechanisms, producing accurate and defect-free linkages.", "motivation": "Dimensional synthesis of planar four-bar mechanisms is a challenging inverse problem in kinematics, requiring the determination of mechanism dimensions from desired motion specifications.", "method": "A data-driven framework is proposed that uses an LSTM-based neural network and a Mixture of Experts (MoE) architecture to bypass traditional equation-solving and optimization.", "result": "Experiments show that the approach produces accurate, defect-free linkages across various configurations.", "conclusion": "The proposed data-driven framework enables accurate and efficient dimensional synthesis of planar four-bar mechanisms, even for non-expert users."}}
{"id": "2507.08284", "pdf": "https://arxiv.org/pdf/2507.08284", "abs": "https://arxiv.org/abs/2507.08284", "authors": ["Aleksei Ilin", "Gor Matevosyan", "Xueying Ma", "Vladimir Eremin", "Suhaa Dada", "Muqun Li", "Riyaaz Shaik", "Haluk Noyan Tokgozoglu"], "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems.", "AI": {"tldr": "This paper introduces a safety guardrail framework for language models using synthetic data generation and adversarial training, allowing small models to excel at content moderation.", "motivation": "To introduce a lightweight yet highly effective safety guardrail framework for language models.", "method": "The framework uses high-fidelity synthetic data generation and adversarial training. The synthetic data generation process begins with human-curated seed data, which undergoes query augmentation and paraphrasing to create diverse and contextually rich examples. Adversarial training employs reinforcement learning to guide a generator that produces challenging synthetic examples.", "result": "Small-scale language models can achieve, and even surpass, the performance of larger counterparts in content moderation tasks.", "conclusion": "The framework enables small language models to serve as robust safety guardrails, offering a scalable and efficient solution for content moderation in AI systems."}}
{"id": "2507.08311", "pdf": "https://arxiv.org/pdf/2507.08311", "abs": "https://arxiv.org/abs/2507.08311", "authors": ["Krishnendu Das", "Sumit Gupta", "Awadhesh Kumar"], "title": "CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering", "categories": ["cs.LG"], "comment": null, "summary": "Clustering is a critical component of decision-making in todays data-driven\nenvironments. It has been widely used in a variety of fields such as\nbioinformatics, social network analysis, and image processing. However,\nclustering accuracy remains a major challenge in large datasets. This paper\npresents a comprehensive overview of strategies for selecting the optimal value\nof k in clustering, with a focus on achieving a balance between clustering\nprecision and computational efficiency in complex data environments. In\naddition, this paper introduces improvements to clustering techniques for text\nand image data to provide insights into better computational performance and\ncluster validity. The proposed approach is based on the Condensed Silhouette\nmethod, along with statistical methods such as Local Structures, Gap\nStatistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and\nCOIbased algorithm to calculate the best value of k for K-Means clustering. The\nresults of comparative experiments show that the proposed approach achieves up\nto 99 percent faster execution times on high-dimensional datasets while\nretaining both precision and scalability, making it highly suitable for real\ntime clustering needs or scenarios demanding efficient clustering with minimal\nresource utilization.", "AI": {"tldr": "This paper introduces improvements to clustering techniques for text and image data to provide insights into better computational performance and cluster validity.", "motivation": "Clustering accuracy remains a major challenge in large datasets. This paper presents a comprehensive overview of strategies for selecting the optimal value of k in clustering, with a focus on achieving a balance between clustering precision and computational efficiency in complex data environments.", "method": "The proposed approach is based on the Condensed Silhouette method, along with statistical methods such as Local Structures, Gap Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and COI-based algorithm to calculate the best value of k for K-Means clustering.", "result": "The results of comparative experiments show that the proposed approach achieves up to 99 percent faster execution times on high-dimensional datasets while retaining both precision and scalability.", "conclusion": "The proposed approach is highly suitable for real-time clustering needs or scenarios demanding efficient clustering with minimal resource utilization."}}
{"id": "2507.08317", "pdf": "https://arxiv.org/pdf/2507.08317", "abs": "https://arxiv.org/abs/2507.08317", "authors": ["Jitendra Kumar", "Deepika Saxena", "Kishu Gupta", "Satyam Kumar", "Ashutosh Kumar Singh"], "title": "A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Accurate workload prediction and advanced resource reservation are\nindispensably crucial for managing dynamic cloud services. Traditional neural\nnetworks and deep learning models frequently encounter challenges with diverse,\nhigh-dimensional workloads, especially during sudden resource demand changes,\nleading to inefficiencies. This issue arises from their limited optimization\nduring training, relying only on parametric (inter-connection weights)\nadjustments using conventional algorithms. To address this issue, this work\nproposes a novel Comprehensively Adaptive Architectural Optimization-based\nVariable Quantum Neural Network (CA-QNN), which combines the efficiency of\nquantum computing with complete structural and qubit vector parametric\nlearning. The model converts workload data into qubits, processed through qubit\nneurons with Controlled NOT-gated activation functions for intuitive pattern\nrecognition. In addition, a comprehensive architecture optimization algorithm\nfor networks is introduced to facilitate the learning and propagation of the\nstructure and parametric values in variable-sized QNNs. This algorithm\nincorporates quantum adaptive modulation and size-adaptive recombination during\ntraining process. The performance of CA-QNN model is thoroughly investigated\nagainst seven state-of-the-art methods across four benchmark datasets of\nheterogeneous cloud workloads. The proposed model demonstrates superior\nprediction accuracy, reducing prediction errors by up to 93.40% and 91.27%\ncompared to existing deep learning and QNN-based approaches.", "AI": {"tldr": "To address challenges with diverse, high-dimensional workloads during sudden resource demand changes, this work proposes a novel CA-QNN model.", "motivation": "Accurate workload prediction and advanced resource reservation are indispensably crucial for managing dynamic cloud services.", "method": "This work proposes a novel Comprehensively Adaptive Architectural Optimization-based Variable Quantum Neural Network (CA-QNN), which combines the efficiency of quantum computing with complete structural and qubit vector parametric learning.", "result": "The performance of CA-QNN model is thoroughly investigated against seven state-of-the-art methods across four benchmark datasets of heterogeneous cloud workloads.", "conclusion": "The proposed CA-QNN model demonstrates superior prediction accuracy, reducing prediction errors by up to 93.40% and 91.27% compared to existing deep learning and QNN-based approaches."}}
{"id": "2507.08355", "pdf": "https://arxiv.org/pdf/2507.08355", "abs": "https://arxiv.org/abs/2507.08355", "authors": ["Hegang Chen", "Yuyin Lu", "Zhiming Dai", "Fu Lee Wang", "Qing Li", "Yanghui Rao"], "title": "scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in sequencing technologies have enabled researchers to\nexplore cellular heterogeneity at single-cell resolution. Meanwhile,\ninterpretability has gained prominence parallel to the rapid increase in the\ncomplexity and performance of deep learning models. In recent years, topic\nmodels have been widely used for interpretable single-cell embedding learning\nand clustering analysis, which we refer to as single-cell embedded topic\nmodels. However, previous studies evaluated the interpretability of the models\nmainly through qualitative analysis, and these single-cell embedded topic\nmodels suffer from the potential problem of interpretation collapse.\nFurthermore, their neglect of external biological knowledge constrains\nanalytical performance. Here, we present scE2TM, an external knowledge-guided\nsingle-cell embedded topic model that provides a high-quality cell embedding\nand strong interpretation, contributing to comprehensive scRNA-seq data\nanalysis. Our comprehensive evaluation across 20 scRNA-seq datasets\ndemonstrates that scE2TM achieves significant clustering performance gains\ncompared to 7 state-of-the-art methods. In addition, we propose a new\ninterpretability evaluation benchmark that introduces 10 metrics to\nquantitatively assess the interpretability of single-cell embedded topic\nmodels. The results show that the interpretation provided by scE2TM performs\nencouragingly in terms of diversity and consistency with the underlying\nbiological signals, contributing to a better revealing of the underlying\nbiological mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u90e8\u77e5\u8bc6\u6307\u5bfc\u7684\u5355\u7ec6\u80de\u5d4c\u5165\u4e3b\u9898\u6a21\u578bscE2TM\uff0c\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u7ec6\u80de\u5d4c\u5165\u548c\u5f3a\u5927\u7684\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u5168\u9762\u7684scRNA-seq\u6570\u636e\u5206\u6790\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u590d\u6742\u6027\u548c\u6027\u80fd\u7684\u5feb\u901f\u63d0\u5347\uff0c\u53ef\u89e3\u91ca\u6027\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u5355\u7ec6\u80de\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u7814\u7a76\u5b58\u5728\u89e3\u91ca\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5ffd\u89c6\u4e86\u5916\u90e8\u751f\u7269\u5b66\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5206\u6790\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u90e8\u77e5\u8bc6\u6307\u5bfc\u7684\u5355\u7ec6\u80de\u5d4c\u5165\u4e3b\u9898\u6a21\u578bscE2TM\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f15\u5165\u4e8610\u4e2a\u6307\u6807\u6765\u5b9a\u91cf\u8bc4\u4f30\u5355\u7ec6\u80de\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u5bf920\u4e2ascRNA-seq\u6570\u636e\u96c6\u7684\u7efc\u5408\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4e8e7\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0cscE2TM\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u805a\u7c7b\u6027\u80fd\u63d0\u5347\u3002scE2TM\u5728\u591a\u6837\u6027\u4ee5\u53ca\u4e0e\u6f5c\u5728\u751f\u7269\u5b66\u4fe1\u53f7\u7684\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "scE2TM\u5728\u5355\u7ec6\u80de\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u7684\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u63ed\u793a\u6f5c\u5728\u7684\u751f\u7269\u5b66\u673a\u5236\u3002"}}
{"id": "2507.08362", "pdf": "https://arxiv.org/pdf/2507.08362", "abs": "https://arxiv.org/abs/2507.08362", "authors": ["Phuong Nam L\u00ea", "Charlotte Schneider-Depr\u00e9", "Alexandre Goossens", "Alexander Stevens", "Aur\u00e9lie Leribaux", "Johannes De Smedt"], "title": "Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text", "categories": ["cs.LG"], "comment": null, "summary": "Efficient planning, resource management, and consistent operations often rely\non converting textual process documents into formal Business Process Model and\nNotation (BPMN) models. However, this conversion process remains time-intensive\nand costly. Existing approaches, whether rule-based or machine-learning-based,\nstill struggle with writing styles and often fail to identify parallel\nstructures in process descriptions.\n  This paper introduces an automated pipeline for extracting BPMN models from\ntext, leveraging the use of machine learning and large language models. A key\ncontribution of this work is the introduction of a newly annotated dataset,\nwhich significantly enhances the training process. Specifically, we augment the\nPET dataset with 15 newly annotated documents containing 32 parallel gateways\nfor model training, a critical feature often overlooked in existing datasets.\nThis addition enables models to better capture parallel structures, a common\nbut complex aspect of process descriptions. The proposed approach demonstrates\nadequate performance in terms of reconstruction accuracy, offering a promising\nfoundation for organizations to accelerate BPMN model creation.", "AI": {"tldr": "This paper presents a machine learning-based automated pipeline for extracting BPMN models from text, introducing a newly annotated dataset that significantly enhances training.", "motivation": "Efficient planning, resource management, and consistent operations often rely on converting textual process documents into formal Business Process Model and Notation (BPMN) models. However, this conversion process remains time-intensive and costly.", "method": "This paper introduces an automated pipeline for extracting BPMN models from text, leveraging the use of machine learning and large language models.", "result": "The proposed approach demonstrates adequate performance in terms of reconstruction accuracy.", "conclusion": "The proposed approach offers a promising foundation for organizations to accelerate BPMN model creation."}}
{"id": "2507.08365", "pdf": "https://arxiv.org/pdf/2507.08365", "abs": "https://arxiv.org/abs/2507.08365", "authors": ["Francesco De Cristofaro", "Felix Hofbaur", "Aixi Yang", "Arno Eichberger"], "title": "Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer", "categories": ["cs.LG"], "comment": "14 pages, 18 figures", "summary": "Lane changes of preceding vehicles have a great impact on the motion planning\nof automated vehicles especially in complex traffic situations. Predicting them\nwould benefit the public in terms of safety and efficiency. While many research\nefforts have been made in this direction, few concentrated on predicting\nmaneuvers within a set time interval compared to predicting at a set prediction\ntime. In addition, there exist a lack of comparisons between different\narchitectures to try to determine the best performing one and to assess how to\ncorrectly choose the input for such models. In this paper the structure of an\nLSTM, a CNN and a Transformer network are described and implemented to predict\nthe intention of human drivers to perform a lane change. We show how the data\nwas prepared starting from a publicly available dataset (highD), which features\nwere used, how the networks were designed and finally we compare the results of\nthe three networks with different configurations of input data. We found that\ntransformer networks performed better than the other networks and was less\naffected by overfitting. The accuracy of the method spanned from $82.79\\%$ to\n$96.73\\%$ for different input configurations and showed overall good\nperformances considering also precision and recall.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u9884\u6d4b\u53f8\u673a\u53d8\u9053\u610f\u56fe\u7684\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7279\u522b\u662fLSTM\u3001CNN\u548cTransformer\u7f51\u7edc\uff0c\u5e76\u53d1\u73b0\u53d8\u538b\u5668\u7f51\u7edc\u6027\u80fd\u6700\u4f73\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u8fc7\u5ea6\u62df\u5408\u65b9\u9762\u3002", "motivation": "\u524d\u8f66\u7684\u8f66\u9053\u53d8\u6362\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u4ea4\u901a\u60c5\u51b5\u4e0b\u3002\u9884\u6d4b\u8f66\u9053\u53d8\u6362\u5c06\u6709\u5229\u4e8e\u516c\u4f17\u7684\u5b89\u5168\u548c\u6548\u7387\u3002\u7136\u800c\uff0c\u4e0e\u8bbe\u5b9a\u7684\u9884\u6d4b\u65f6\u95f4\u76f8\u6bd4\uff0c\u5728\u4e00\u5b9a\u65f6\u95f4\u95f4\u9694\u5185\u9884\u6d4b\u64cd\u4f5c\u7684\u7814\u7a76\u8f83\u5c11\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u4e0d\u540c\u67b6\u6784\u4e4b\u95f4\u7684\u6027\u80fd\u5bf9\u6bd4\u4ee5\u53ca\u5982\u4f55\u6b63\u786e\u9009\u62e9\u6b64\u7c7b\u6a21\u578b\u7684\u8f93\u5165\u3002", "method": "\u63cf\u8ff0\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2aLSTM\u3001CNN\u548c\u53d8\u538b\u5668\u7f51\u7edc\u7684\u7ed3\u6784\uff0c\u4ee5\u9884\u6d4b\u9a7e\u9a76\u5458\u8fdb\u884c\u8f66\u9053\u53d8\u6362\u7684\u610f\u56fe\u3002\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\uff08highD\uff09\u51c6\u5907\u6570\u636e\uff0c\u8bbe\u8ba1\u7f51\u7edc\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u7f51\u7edc\u5728\u4e0d\u540c\u8f93\u5165\u6570\u636e\u914d\u7f6e\u4e0b\u7684\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53d8\u538b\u5668\u7f51\u7edc\u6bd4\u5176\u4ed6\u7f51\u7edc\u8868\u73b0\u66f4\u597d\uff0c\u53d7\u8fc7\u62df\u5408\u7684\u5f71\u54cd\u8f83\u5c0f\u3002\u4e0d\u540c\u8f93\u5165\u914d\u7f6e\u7684\u51c6\u786e\u5ea6\u4ece82.79%\u523096.73%\u4e0d\u7b49\uff0c\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u603b\u4f53\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u53d8\u538b\u5668\u7f51\u7edc\u5728\u9884\u6d4b\u8f66\u9053\u53d8\u6362\u610f\u56fe\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u7f51\u7edc\uff0c\u5e76\u4e14\u53d7\u8fc7\u62df\u5408\u5f71\u54cd\u8f83\u5c0f\u3002\u4e0d\u540c\u8f93\u5165\u914d\u7f6e\u7684\u51c6\u786e\u5ea6\u4ece82.79%\u523096.73%\u4e0d\u7b49\uff0c\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u603b\u4f53\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2507.08379", "pdf": "https://arxiv.org/pdf/2507.08379", "abs": "https://arxiv.org/abs/2507.08379", "authors": ["Samarth Kashyap", "Rohit K Ramakrishnan", "Kumari Jyoti", "Apoorva D Patel"], "title": "Advances in Machine Learning: Where Can Quantum Techniques Help?", "categories": ["cs.LG", "quant-ph"], "comment": "28 pages, 1 figure", "summary": "Quantum Machine Learning (QML) represents a promising frontier at the\nintersection of quantum computing and artificial intelligence, aiming to\nleverage quantum computational advantages to enhance data-driven tasks. This\nreview explores the potential of QML to address the computational bottlenecks\nof classical machine learning, particularly in processing complex datasets. We\nintroduce the theoretical foundations of QML, including quantum data encoding,\nquantum learning theory and optimization techniques, while categorizing QML\napproaches based on data type and computational architecture. It is\nwell-established that quantum computational advantages are problem-dependent,\nand so potentially useful directions for QML need to be systematically\nidentified. Key developments, such as Quantum Principal Component Analysis,\nquantum-enhanced sensing and applications in material science, are critically\nevaluated for their theoretical speed-ups and practical limitations. The\nchallenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including\nhardware noise, scalability constraints and data encoding overheads, are\ndiscussed in detail. We also outline future directions, emphasizing the need\nfor quantum-native algorithms, improved error correction, and realistic\nbenchmarks to bridge the gap between theoretical promise and practical\ndeployment. This comprehensive analysis underscores that while QML has\nsignificant potential for specific applications such as quantum chemistry and\nsensing, its broader utility in real-world scenarios remains contingent on\novercoming technological and methodological hurdles.", "AI": {"tldr": "This review explores the potential of Quantum Machine Learning (QML) to address the computational bottlenecks of classical machine learning, introducing the theoretical foundations and categorizing QML approaches. It critically evaluates key developments and discusses the challenges posed by NISQ devices, outlining future directions.", "motivation": "QML aims to leverage quantum computational advantages to enhance data-driven tasks and address the computational bottlenecks of classical machine learning, particularly in processing complex datasets.", "method": "Theoretical foundations of QML are introduced, including quantum data encoding, quantum learning theory and optimization techniques. The paper categorizes QML approaches based on data type and computational architecture and critically evaluates key developments such as Quantum Principal Component Analysis, quantum-enhanced sensing and applications in material science.", "result": "Quantum computational advantages are problem-dependent. The paper evaluates the theoretical speed-ups and practical limitations of key developments such as Quantum Principal Component Analysis, quantum-enhanced sensing and applications in material science. It also discusses the challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices.", "conclusion": "While QML has significant potential for specific applications, its broader utility in real-world scenarios remains contingent on overcoming technological and methodological hurdles."}}
{"id": "2507.08382", "pdf": "https://arxiv.org/pdf/2507.08382", "abs": "https://arxiv.org/abs/2507.08382", "authors": ["Xinying Liu", "Lianyu Hu", "Mudi Jiang", "Simen Zhang", "Jun Lou", "Zengyou He"], "title": "Two-cluster test", "categories": ["cs.LG"], "comment": null, "summary": "Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering.", "AI": {"tldr": "This paper introduces a new method for two-cluster testing in cluster analysis to reduce Type-I error rate.", "motivation": "To overcome the bias of classic two-sample tests in the context of cluster analysis, which yield extremely smaller p-values and inflated Type-I error rate.", "method": "A new method based on the boundary points between two subsets to derive an analytical p-value for the purpose of significance quantification.", "result": "Experiments on both synthetic and real data sets show that the proposed test is able to significantly reduce the Type-I error rate, in comparison with several classic two-sample testing methods.", "conclusion": "The proposed two-cluster test is able to significantly reduce the Type-I error rate and has practical usage in tree-based interpretable clustering and significance-based hierarchical clustering."}}
{"id": "2507.08387", "pdf": "https://arxiv.org/pdf/2507.08387", "abs": "https://arxiv.org/abs/2507.08387", "authors": ["Yongjae Shin", "Jeonghye Kim", "Whiyoung Jung", "Sunghoon Hong", "Deunsol Yoon", "Youngsoo Jang", "Geonhyeong Kim", "Jongseong Chae", "Youngchul Sung", "Kanghoon Lee", "Woohyung Lim"], "title": "Online Pre-Training for Offline-to-Online Reinforcement Learning", "categories": ["cs.LG"], "comment": "ICML 2025 camera-ready", "summary": "Offline-to-online reinforcement learning (RL) aims to integrate the\ncomplementary strengths of offline and online RL by pre-training an agent\noffline and subsequently fine-tuning it through online interactions. However,\nrecent studies reveal that offline pre-trained agents often underperform during\nonline fine-tuning due to inaccurate value estimation caused by distribution\nshift, with random initialization proving more effective in certain cases. In\nthis work, we propose a novel method, Online Pre-Training for Offline-to-Online\nRL (OPT), explicitly designed to address the issue of inaccurate value\nestimation in offline pre-trained agents. OPT introduces a new learning phase,\nOnline Pre-Training, which allows the training of a new value function tailored\nspecifically for effective online fine-tuning. Implementation of OPT on TD3 and\nSPOT demonstrates an average 30% improvement in performance across a wide range\nof D4RL environments, including MuJoCo, Antmaze, and Adroit.", "AI": {"tldr": "This paper proposes a novel method, Online Pre-Training (OPT), to address the issue of inaccurate value estimation in offline pre-trained agents for Offline-to-Online RL, resulting in significant performance improvements.", "motivation": "Offline pre-trained agents often underperform during online fine-tuning due to inaccurate value estimation caused by distribution shift. Random initialization can be more effective in certain cases.", "method": "A new learning phase, Online Pre-Training, is introduced to train a new value function specifically for effective online fine-tuning.", "result": "Implementation of OPT on TD3 and SPOT demonstrates an average 30% improvement in performance across a wide range of D4RL environments.", "conclusion": "The proposed method, Online Pre-Training (OPT), effectively addresses the issue of inaccurate value estimation in offline pre-trained agents, leading to significant performance improvements in Offline-to-Online RL."}}
{"id": "2507.08456", "pdf": "https://arxiv.org/pdf/2507.08456", "abs": "https://arxiv.org/abs/2507.08456", "authors": ["M. Maurin", "M. \u00c1. Evangelista-Alvarado", "P. Su\u00e1rez-Serrato"], "title": "Space filling positionality and the Spiroformer", "categories": ["cs.LG", "cs.AI", "math.DG", "math.DS", "math.SG"], "comment": "9 pages, 5 figures. To appear in Geometric Science of Information\n  2025", "summary": "Transformers excel when dealing with sequential data. Generalizing\ntransformer models to geometric domains, such as manifolds, we encounter the\nproblem of not having a well-defined global order. We propose a solution with\nattention heads following a space-filling curve. As a first experimental\nexample, we present the Spiroformer, a transformer that follows a polar spiral\non the $2$-sphere.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u51e0\u4f55\u57df\uff08\u4f8b\u5982\u6d41\u5f62\uff09\u4e0a\u63a8\u5e7fTransformer\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6cbf\u7740\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u7684\u6ce8\u610f\u529b\u5934\u3002\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u9075\u5faa\u4e8c\u7ef4\u7403\u9762\u4e0a\u6781\u87ba\u65cb\u7684Spiroformer\u3002", "motivation": "\u5728\u5904\u7406\u5e8f\u5217\u6570\u636e\u65f6\uff0cTransformer\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5c06\u5176\u6a21\u578b\u63a8\u5e7f\u5230\u51e0\u4f55\u57df\uff08\u5982\u6d41\u5f62\uff09\u65f6\uff0c\u6211\u4eec\u9047\u5230\u4e86\u6ca1\u6709\u660e\u786e\u5b9a\u4e49\u7684\u5168\u5c40\u987a\u5e8f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6cbf\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u7684\u6ce8\u610f\u529b\u5934\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "result": "\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u9075\u5faa\u4e8c\u7ef4\u7403\u9762\u4e0a\u6781\u87ba\u65cb\u7684Spiroformer\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5728\u51e0\u4f55\u57df\u4e0a\u5e94\u7528Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u3002"}}
{"id": "2507.08390", "pdf": "https://arxiv.org/pdf/2507.08390", "abs": "https://arxiv.org/abs/2507.08390", "authors": ["Meihua Dang", "Jiaqi Han", "Minkai Xu", "Kai Xu", "Akash Srivastava", "Stefano Ermon"], "title": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion models have emerged as a powerful paradigm for language\nmodeling, rivaling auto-regressive models by training-time scaling. However,\ninference-time scaling in discrete diffusion models remains relatively\nunder-explored. In this work, we study sampling-based approaches for achieving\nhigh-quality text generation from discrete diffusion models in reward-guided\nsettings. We introduce a novel inference-time scaling approach based on\nparticle Gibbs sampling for discrete diffusion models. The particle Gibbs\nsampling algorithm iteratively refines full diffusion trajectories using\nconditional Sequential Monte Carlo as its transition mechanism. This process\nensures that the updated samples progressively improve and move closer to the\nreward-weighted target distribution. Unlike existing inference-time scaling\nmethods, which are often limited to single diffusion trajectories, our approach\nleverages iterative refinement across multiple trajectories. Within this\nframework, we further analyze the trade-offs between four key axes for\ninference-time scaling under fixed compute budgets: particle Gibbs iterations,\nparticle count, denoising steps, and reward estimation cost. Empirically, our\nmethod consistently outperforms prior inference-time strategies on\nreward-guided text generation tasks, achieving significant improvement in\naccuracy under varying compute budgets.", "AI": {"tldr": "This paper explores sampling-based approaches for high-quality text generation from discrete diffusion models in reward-guided settings and proposes a novel inference-time scaling approach based on particle Gibbs sampling.", "motivation": "Discrete diffusion models have not been extensively explored for inference-time scaling, especially in reward-guided settings. The authors aim to address this gap by studying sampling-based approaches for high-quality text generation.", "method": "The paper introduces a novel inference-time scaling approach based on particle Gibbs sampling for discrete diffusion models, which iteratively refines full diffusion trajectories using conditional Sequential Monte Carlo as its transition mechanism.", "result": "The method consistently outperforms prior inference-time strategies on reward-guided text generation tasks, achieving significant improvement in accuracy under varying compute budgets.", "conclusion": "The particle Gibbs sampling algorithm for discrete diffusion models significantly improves the accuracy of reward-guided text generation under varying compute budgets."}}
{"id": "2507.08472", "pdf": "https://arxiv.org/pdf/2507.08472", "abs": "https://arxiv.org/abs/2507.08472", "authors": ["Joel Schlotthauer", "Christian Kroos", "Chris Hinze", "Viktor Hangya", "Luzian Hahn", "Fabian K\u00fcch"], "title": "Pre-Training LLMs on a budget: A comparison of three optimizers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.", "AI": {"tldr": "This study compares three optimizers - AdamW, Lion, and Sophia - for pre-training LLMs. While all three optimizers performed similarly, Sophia had the lowest training and validation loss, Lion was fastest in terms of training GPU hours, and AdamW led to the best downstream evaluation results.", "motivation": "To determine the best optimizer for reducing pre-training times for LLMs and achieving better-performing models.", "method": "Comparison of three optimizers: AdamW, Lion, and Sophia with two different base architectures using Maximal Update Parametrization and smaller proxy models for hyperparameter tuning.", "result": "Sophia optimizer had the lowest training and validation loss, Lion was fastest in terms of training GPU hours, and AdamW led to the best downstream evaluation results.", "conclusion": "Sophia optimizer exhibited the lowest training and validation loss, Lion was fastest in terms of training GPU hours but AdamW led to the best downstream evaluation results."}}
{"id": "2507.08424", "pdf": "https://arxiv.org/pdf/2507.08424", "abs": "https://arxiv.org/abs/2507.08424", "authors": ["Anirudh Varanasi", "Robin Degraeve", "Philippe Roussel", "Clement Merckling"], "title": "RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices", "categories": ["cs.LG"], "comment": null, "summary": "Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRTNinja\u7684\u5168\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5206\u6790\u968f\u673a\u7535\u62a5\u566a\u58f0\u4fe1\u53f7\u3002", "motivation": "\u968f\u673a\u7535\u62a5\u566a\u58f0\u662f\u7eb3\u7c73\u7535\u5b50\u5668\u4ef6\u4e2d\u666e\u904d\u5b58\u5728\u7684\u53d8\u5f02\u6027\u73b0\u8c61\uff0c\u4f20\u7edf\u7684\u5206\u6790\u6280\u672f\u5e38\u5e38\u4f9d\u8d56\u4e8e\u9650\u5236\u6027\u5047\u8bbe\u6216\u4eba\u5de5\u5e72\u9884\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u590d\u6742\u3001\u6709\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\u3002", "method": "RTNinja\u6846\u67b6\u7531\u4e24\u4e2a\u6a21\u5757\u5316\u7ec4\u4ef6\u6784\u6210\uff1aLevelsExtractor\uff08\u4f7f\u7528\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u6a21\u578b\u9009\u62e9\u6765\u53bb\u566a\u5e76\u79bb\u6563\u5316\u4fe1\u53f7\uff09\u548cSourcesMapper\uff08\u901a\u8fc7\u6982\u7387\u805a\u7c7b\u548c\u4f18\u5316\u63a8\u65ad\u6e90\u914d\u7f6e\uff09\u3002\u4e3a\u4e86\u8bc4\u4f30\u6027\u80fd\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5668\uff0c\u751f\u6210\u6db5\u76d6\u5e7f\u6cdb\u4fe1\u566a\u6bd4\u548c\u6e90\u590d\u6742\u6027\u7684\u6807\u7b7e\u6570\u636e\u96c6\u3002", "result": "\u57287000\u4e2a\u8fd9\u6837\u7684\u6570\u636e\u96c6\u4e2d\uff0cRTNinja\u59cb\u7ec8\u8868\u73b0\u51fa\u9ad8\u4fdd\u771f\u4fe1\u53f7\u91cd\u5efa\u548c\u51c6\u786e\u63d0\u53d6\u6e90\u5e45\u5ea6\u53ca\u6d3b\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "conclusion": "RTNinja\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u4e14\u4e0e\u8bbe\u5907\u65e0\u5173\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u968f\u673a\u7535\u62a5\u566a\u58f0\u7684\u7279\u6027\u5206\u6790\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u7edf\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u3001\u4ee5\u53ef\u9760\u6027\u4e3a\u4e2d\u5fc3\u7684\u6280\u672f\u9274\u5b9a\u3001\u9884\u6d4b\u6545\u969c\u5efa\u6a21\u548c\u4e0b\u4e00\u4ee3\u7eb3\u7c73\u7535\u5b50\u5b66\u4e2d\u7684\u5668\u4ef6\u7269\u7406\u63a2\u7d22\u3002"}}
{"id": "2507.08443", "pdf": "https://arxiv.org/pdf/2507.08443", "abs": "https://arxiv.org/abs/2507.08443", "authors": ["Georgios Balanos", "Evangelos Chasanis", "Konstantinos Skianis", "Evaggelia Pitoura"], "title": "KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations", "categories": ["cs.LG"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by grounding\nresponses in external information, yet explainability remains a critical\nchallenge, particularly when retrieval relies on unstructured text. Knowledge\ngraphs (KGs) offer a solution by introducing structured, semantically rich\nrepresentations of entities and their relationships, enabling transparent\nretrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,\na RAG system that improves both factual grounding and explainability by\nleveraging a domain-specific KG constructed via prompt-based information\nextraction. Given a user query, KGRAG-Ex identifies relevant entities and\nsemantic paths in the graph, which are then transformed into pseudo-paragraphs:\nnatural language representations of graph substructures that guide corpus\nretrieval. To improve interpretability and support reasoning transparency, we\nincorporate perturbation-based explanation methods that assess the influence of\nspecific KG-derived components on the generated answers. We conduct a series of\nexperiments to analyze the sensitivity of the system to different perturbation\nmethods, the relationship between graph component importance and their\nstructural positions, the influence of semantic node types, and how graph\nmetrics correspond to the influence of components within the explanations\nprocess.", "AI": {"tldr": "This paper presents KGRAG-Ex, a RAG system that leverages a domain-specific knowledge graph to improve both factual grounding and explainability.", "motivation": "Despite RAG's enhancement of language models through external information, explainability remains a challenge. Knowledge graphs offer structured representations that enable transparent retrieval paths and interpretable reasoning.", "method": "The system identifies relevant entities and semantic paths in the KG, transforms them into pseudo-paragraphs to guide corpus retrieval, and incorporates perturbation-based explanation methods to assess the influence of specific KG-derived components on the generated answers.", "result": "KGRAG-Ex shows improvements in both factual grounding and explainability. The experiments analyze the sensitivity of the system to different perturbation methods, the relationship between graph component importance and structural positions, the influence of semantic node types, and how graph metrics correspond to the influence of components within the explanations process.", "conclusion": "KGRAG-Ex improves the factual grounding and explainability of RAG systems by leveraging a domain-specific knowledge graph."}}
{"id": "2507.08617", "pdf": "https://arxiv.org/pdf/2507.08617", "abs": "https://arxiv.org/abs/2507.08617", "authors": ["Tianrun Yu", "Jiaqi Wang", "Haoyu Wang", "Mingquan Lin", "Han Liu", "Nelson S. Yee", "Fenglong Ma"], "title": "Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, accepted to the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD' 25), Toronto, Canada, August 3-7 2025", "summary": "Collaborative fairness is a crucial challenge in federated learning. However,\nexisting approaches often overlook a practical yet complex form of\nheterogeneity: imbalanced covariate shift. We provide a theoretical analysis of\nthis setting, which motivates the design of FedAKD (Federated Asynchronous\nKnowledge Distillation)- simple yet effective approach that balances accurate\nprediction with collaborative fairness. FedAKD consists of client and server\nupdates. In the client update, we introduce a novel asynchronous knowledge\ndistillation strategy based on our preliminary analysis, which reveals that\nwhile correctly predicted samples exhibit similar feature distributions across\nclients, incorrectly predicted samples show significant variability. This\nsuggests that imbalanced covariate shift primarily arises from misclassified\nsamples. Leveraging this insight, our approach first applies traditional\nknowledge distillation to update client models while keeping the global model\nfixed. Next, we select correctly predicted high-confidence samples and update\nthe global model using these samples while keeping client models fixed. The\nserver update simply aggregates all client models. We further provide a\ntheoretical proof of FedAKD's convergence. Experimental results on public\ndatasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records\n(EHR) dataset demonstrate that FedAKD significantly improves collaborative\nfairness, enhances predictive accuracy, and fosters client participation even\nunder highly heterogeneous data distributions.", "AI": {"tldr": "This paper proposes FedAKD, a method designed to balance accurate prediction with collaborative fairness in federated learning by addressing the issue of imbalanced covariate shift.", "motivation": "Existing approaches in federated learning often overlook imbalanced covariate shift. This motivates the design of FedAKD which balances accurate prediction with collaborative fairness.", "method": "FedAKD consists of client and server updates. In the client update, it introduces an asynchronous knowledge distillation strategy based on the analysis that imbalanced covariate shift primarily arises from misclassified samples. The approach first applies traditional knowledge distillation to update client models while keeping the global model fixed. Next, it selects correctly predicted high-confidence samples and update the global model using these samples while keeping client models fixed. The server update aggregates all client models.", "result": "Experimental results on public datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records (EHR) dataset demonstrate that FedAKD significantly improves collaborative fairness, enhances predictive accuracy, and fosters client participation even under highly heterogeneous data distributions.", "conclusion": "FedAKD improves collaborative fairness, enhances predictive accuracy, and fosters client participation under heterogeneous data distributions."}}
{"id": "2507.08637", "pdf": "https://arxiv.org/pdf/2507.08637", "abs": "https://arxiv.org/abs/2507.08637", "authors": ["Vincenzo Dentamaro"], "title": "Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 1 figure", "summary": "Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.", "AI": {"tldr": "This paper introduces WERSA, a new method that reduces computational loads without compromising accuracy, making long-context models more practical and affordable, especially on low-resource hardware.", "motivation": "Transformer models are computationally costly on long sequences since regular attention has quadratic $O(n^2)$ time complexity. The motivation is to enable successful long-sequence processing without the performance trade-off.", "method": "Wavelet-Enhanced Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time complexity that merges content-adaptive random spectral features together with multi-resolution Haar wavelets and learnable parameters to selectively attend to informative scales of data while preserving linear efficiency.", "result": "WERSA achieves best accuracy in all tests. On ArXiv classification, WERSA improves accuracy over vanilla attention by 1.2% (86.2% vs 85.0%) while cutting training time by 81% (296s vs 1554s) and FLOPS by 73.4% (26.2G vs 98.4G).", "conclusion": "WERSA makes possible more practical, more affordable, long-context models, in particular on low-resource hardware, for more sustainable and more scalable AI development."}}
{"id": "2507.08465", "pdf": "https://arxiv.org/pdf/2507.08465", "abs": "https://arxiv.org/abs/2507.08465", "authors": ["Feijiang Li", "Liuya Zhang", "Jieting Wang", "Tao Yan", "Yuhua Qian"], "title": "Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds", "categories": ["cs.LG"], "comment": null, "summary": "Multilayer perceptron (MLP), one of the most fundamental neural networks, is\nextensively utilized for classification and regression tasks. In this paper, we\nestablish a new generalization error bound, which reveals how the variance of\nempirical loss influences the generalization ability of the learning model.\nInspired by this learning bound, we advocate to reduce the variance of\nempirical loss to enhance the ability of MLP. As is well-known, bagging is a\npopular ensemble method to realize variance reduction. However, bagging\nproduces the base training data sets by the Simple Random Sampling (SRS)\nmethod, which exhibits a high degree of randomness. To handle this issue, we\nintroduce an ordered structure in the training data set by Rank Set Sampling\n(RSS) to further reduce the variance of loss and develop a RSS-MLP method.\nTheoretical results show that the variance of empirical exponential loss and\nthe logistic loss estimated by RSS are smaller than those estimated by SRS,\nrespectively. To validate the performance of RSS-MLP, we conduct comparison\nexperiments on twelve benchmark data sets in terms of the two convex loss\nfunctions under two fusion methods. Extensive experimental results and analysis\nillustrate the effectiveness and rationality of the propose method.", "AI": {"tldr": "This paper proposes a new method called RSS-MLP that uses Rank Set Sampling to reduce the variance of empirical loss, enhancing the generalization ability of multilayer perceptrons.", "motivation": "To enhance the generalization ability of MLP by reducing the variance of empirical loss, inspired by a new generalization error bound.", "method": "Introduces an ordered structure in the training data set by Rank Set Sampling (RSS) to further reduce the variance of loss and develops a RSS-MLP method.", "result": "Theoretical results show that the variance of empirical exponential loss and the logistic loss estimated by RSS are smaller than those estimated by SRS. Experimental results on twelve benchmark data sets validate the performance of RSS-MLP.", "conclusion": "The proposed RSS-MLP method is effective and rational, supported by both theoretical and experimental results."}}
{"id": "2507.08721", "pdf": "https://arxiv.org/pdf/2507.08721", "abs": "https://arxiv.org/abs/2507.08721", "authors": ["Mona Schirmer", "Metod Jazbec", "Christian A. Naesseth", "Eric Nalisnick"], "title": "Monitoring Risks in Test-Time Adaptation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Encountering shifted data at test time is a ubiquitous challenge when\ndeploying predictive models. Test-time adaptation (TTA) methods address this\nissue by continuously adapting a deployed model using only unlabeled test data.\nWhile TTA can extend the model's lifespan, it is only a temporary solution.\nEventually the model might degrade to the point that it must be taken offline\nand retrained. To detect such points of ultimate failure, we propose pairing\nTTA with risk monitoring frameworks that track predictive performance and raise\nalerts when predefined performance criteria are violated. Specifically, we\nextend existing monitoring tools based on sequential testing with confidence\nsequences to accommodate scenarios in which the model is updated at test time\nand no test labels are available to estimate the performance metrics of\ninterest. Our extensions unlock the application of rigorous statistical risk\nmonitoring to TTA, and we demonstrate the effectiveness of our proposed TTA\nmonitoring framework across a representative set of datasets, distribution\nshift types, and TTA methods.", "AI": {"tldr": "Proposing a TTA monitoring framework paired with risk monitoring tools to detect points of ultimate failure when deploying predictive models.", "motivation": "To detect points of ultimate failure when deploying predictive models by pairing TTA with risk monitoring frameworks.", "method": "Extending existing monitoring tools based on sequential testing with confidence sequences to accommodate scenarios in which the model is updated at test time and no test labels are available.", "result": "Demonstrating the effectiveness of the proposed TTA monitoring framework across different datasets, distribution shift types, and TTA methods.", "conclusion": "The proposed TTA monitoring framework is effective across a representative set of datasets, distribution shift types, and TTA methods."}}
{"id": "2507.08736", "pdf": "https://arxiv.org/pdf/2507.08736", "abs": "https://arxiv.org/abs/2507.08736", "authors": ["Idan Mashiach", "Oren Glickman", "Tom Tirer"], "title": "Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Catastrophic forgetting in deep neural networks occurs when learning new\ntasks degrades performance on previously learned tasks due to knowledge\noverwriting. Among the approaches to mitigate this issue, regularization\ntechniques aim to identify and constrain \"important\" parameters to preserve\nprevious knowledge. In the highly nonconvex optimization landscape of deep\nlearning, we propose a novel perspective: tracking parameters during the final\ntraining plateau is more effective than monitoring them throughout the entire\ntraining process. We argue that parameters that exhibit higher activity\n(movement and variability) during this plateau reveal directions in the loss\nlandscape that are relatively flat, making them suitable for adaptation to new\ntasks while preserving knowledge from previous ones. Our comprehensive\nexperiments demonstrate that this approach achieves superior performance in\nbalancing catastrophic forgetting mitigation with strong performance on newly\nlearned tasks.", "AI": {"tldr": "This paper proposes tracking parameters during the final training plateau to mitigate catastrophic forgetting, demonstrating superior performance in balancing forgetting prevention with strong performance on new tasks.", "motivation": "Catastrophic forgetting in deep neural networks occurs when learning new tasks degrades performance on previously learned tasks due to knowledge overwriting. Regularization techniques aim to identify and constrain 'important' parameters to preserve previous knowledge.", "method": "Proposes a novel perspective of tracking parameters during the final training plateau instead of throughout the entire training process.", "result": "The approach achieves superior performance in balancing catastrophic forgetting mitigation with strong performance on newly learned tasks.", "conclusion": "Tracking parameters during the final training plateau is more effective for mitigating catastrophic forgetting while maintaining strong performance on new tasks."}}
{"id": "2507.08473", "pdf": "https://arxiv.org/pdf/2507.08473", "abs": "https://arxiv.org/abs/2507.08473", "authors": ["Gon\u00e7alo Paulo", "Nora Belrose"], "title": "Evaluating SAE interpretability without explanations", "categories": ["cs.LG"], "comment": null, "summary": "Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques.", "AI": {"tldr": "This paper proposes a new method for evaluating the interpretability of sparse coders without relying on natural language explanations.", "motivation": "There is a challenge in measuring the interpretability of Sparse autoencoders (SAEs) and transcoders due to the lack of consensus on benchmarks and the difficulty in separating the explanation generation process from the actual interpretability of the latents.", "method": "Existing methods are adapted to assess the interpretability of sparse coders without requiring the generation of natural language explanations.", "result": "The scores produced by the interpretability metrics were compared with human evaluations, providing insights for the community on improving evaluation techniques.", "conclusion": "The proposed method offers a more direct and potentially standardized way to assess the interpretability of sparse coders, which could help in improving the evaluation of these techniques."}}
{"id": "2507.08738", "pdf": "https://arxiv.org/pdf/2507.08738", "abs": "https://arxiv.org/abs/2507.08738", "authors": ["Azimov Sherkhon", "Susana Lopez-Moreno", "Eric Dolores-Cuenca", "Sieun Lee", "Sangil Kim"], "title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series", "categories": ["cs.LG", "cs.AI", "math.DS", "68T07, 37M10, 00A79, 37M22, 65P20"], "comment": "15 pages, 10 figures", "summary": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency.", "AI": {"tldr": "This paper proposes an adaptive NVAR model that combines delay-embedded linear inputs with features generated by a shallow, learnable multi-layer perceptron (MLP), which enables the model to learn data-driven nonlinearities while preserving a simple readout structure. The model outperforms the standard NVAR in predictive accuracy and shows robust forecasting under noisy conditions.", "motivation": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have limitations in adaptability to high noise or real-world data and scale poorly in high-dimensional settings.", "method": "The adaptive NVAR model combines delay-embedded linear inputs with features generated by a shallow, learnable multi-layer perceptron (MLP). The MLP and linear readout are jointly trained using gradient-based optimization.", "result": "Initial experiments on chaotic systems tested under noise-free and synthetically noisy conditions showed that the adaptive model outperformed the standard NVAR in predictive accuracy.", "conclusion": "The adaptive NVAR model outperforms the standard NVAR in predictive accuracy and shows robust forecasting under noisy conditions with a lower observation frequency."}}
{"id": "2507.08475", "pdf": "https://arxiv.org/pdf/2507.08475", "abs": "https://arxiv.org/abs/2507.08475", "authors": ["Haitao Lin", "Junjie Wang", "Zhifeng Gao", "Xiaohong Ji", "Rong Zhu", "Linfeng Zhang", "Guolin Ke", "Weinan E"], "title": "SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction", "categories": ["cs.LG"], "comment": "22pages, 2 figures", "summary": "The essence of a chemical reaction lies in the redistribution and\nreorganization of electrons, which is often manifested through electron\ntransfer or the migration of electron pairs. These changes are inherently\ndiscrete and abrupt in the physical world, such as alterations in the charge\nstates of atoms or the formation and breaking of chemical bonds. To model the\ntransition of states, we propose SynBridge, a bidirectional flow-based\ngenerative model to achieve multi-task reaction prediction. By leveraging a\ngraph-to-graph transformer network architecture and discrete flow bridges\nbetween any two discrete distributions, SynBridge captures bidirectional\nchemical transformations between graphs of reactants and products through the\nbonds' and atoms' discrete states. We further demonstrate the effectiveness of\nour method through extensive experiments on three benchmark datasets\n(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in\nboth forward and retrosynthesis tasks. Our ablation studies and noise\nscheduling analysis reveal the benefits of structured diffusion over discrete\nspaces for reaction prediction.", "AI": {"tldr": "A bidirectional flow-based generative model called SynBridge is proposed to achieve multi-task reaction prediction.", "motivation": "To model the transition of states in chemical reactions which are inherently discrete and abrupt.", "method": "SynBridge, a bidirectional flow-based generative model using a graph-to-graph transformer network architecture and discrete flow bridges.", "result": "SynBridge achieves state-of-the-art performance in both forward and retrosynthesis tasks on three benchmark datasets.", "conclusion": "SynBridge is effective for modeling the transition of states in chemical reactions and achieves state-of-the-art performance in both forward and retrosynthesis tasks."}}
{"id": "2507.08761", "pdf": "https://arxiv.org/pdf/2507.08761", "abs": "https://arxiv.org/abs/2507.08761", "authors": ["Jeonghye Kim", "Yongjae Shin", "Whiyoung Jung", "Sunghoon Hong", "Deunsol Yoon", "Youngchul Sung", "Kanghoon Lee", "Woohyung Lim"], "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "Reinforcement learning with offline data suffers from Q-value extrapolation\nerrors. To address this issue, we first demonstrate that linear extrapolation\nof the Q-function beyond the data range is particularly problematic. To\nmitigate this, we propose guiding the gradual decrease of Q-values outside the\ndata range, which is achieved through reward scaling with layer normalization\n(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining\nRS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a\nrange of tasks, demonstrating superior performance compared to state-of-the-art\nalgorithms in both offline training and online fine-tuning on the D4RL\nbenchmark, with notable success in the challenging AntMaze Ultra task.", "AI": {"tldr": "This paper addresses the issue of Q-value extrapolation errors in reinforcement learning with offline data by developing a new algorithm called PARS, which achieves superior performance on the D4RL benchmark.", "motivation": "Reinforcement learning with offline data suffers from Q-value extrapolation errors, particularly linear extrapolation of the Q-function beyond the data range.", "method": "The paper proposes a new algorithm called PARS, which combines RS-LN and PA to mitigate Q-value extrapolation errors.", "result": "PARS demonstrates superior performance compared to state-of-the-art algorithms on the D4RL benchmark.", "conclusion": "The PARS algorithm shows superior performance in offline training and online fine-tuning, particularly excelling in the AntMaze Ultra task."}}
{"id": "2507.08505", "pdf": "https://arxiv.org/pdf/2507.08505", "abs": "https://arxiv.org/abs/2507.08505", "authors": ["Pablo Robin Guerrero", "Yueyang Pan", "Sanidhya Kashyap"], "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R", "categories": ["cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.", "AI": {"tldr": "A survey of deployment frameworks for Vision-Language Models on mobile devices reveals significant performance bottlenecks.", "motivation": "The motivation is to address the computational limitations and energy inefficiency in deploying Vision-Language Models on mobile devices, especially for real-time applications.", "method": "The study evaluates deployment frameworks (llama.cpp, MLC-Imp, mllm) for VLMs on mobile devices by running representative workloads and measuring various performance metrics.", "result": "Benchmarking revealed critical performance bottlenecks across frameworks with over-utilized CPUs and underutilized or saturated GPUs and NPUs.", "conclusion": "Current deployment frameworks for Vision-Language Models on mobile devices face significant challenges with CPU overuse and ineffective use of GPU and NPU accelerators, highlighting the need for optimized solutions."}}
{"id": "2507.08793", "pdf": "https://arxiv.org/pdf/2507.08793", "abs": "https://arxiv.org/abs/2507.08793", "authors": ["James McCarthy", "Radu Marinescu", "Elizabeth Daly", "Ivana Dusparic"], "title": "Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies\nthat minimise the likelihood of rare and catastrophic constraint violations\ncaused by an environment's inherent randomness. In general, risk-aversion leads\nto conservative exploration of the environment which typically results in\nconverging to sub-optimal policies that fail to adequately maximise reward or,\nin some cases, fail to achieve the goal. In this paper, we propose an\nexploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic\n(ORAC), which constructs an exploratory policy by maximising a local upper\nconfidence bound of the state-action reward value function whilst minimising a\nlocal lower confidence bound of the risk-averse state-action cost value\nfunction. Specifically, at each step, the weighting assigned to the cost value\nis increased or decreased if it exceeds or falls below the safety constraint\nvalue. This way the policy is encouraged to explore uncertain regions of the\nenvironment to discover high reward states whilst still satisfying the safety\nconstraints. Our experimental results demonstrate that the ORAC approach\nprevents convergence to sub-optimal policies and improves significantly the\nreward-cost trade-off in various continuous control tasks such as\nSafety-Gymnasium and a complex building energy management environment\nCityLearn.", "AI": {"tldr": "This paper proposes an exploration-based approach called Optimistic Risk-averse Actor Critic (ORAC) for Risk-averse Constrained Reinforcement Learning (RaCRL), which encourages the policy to explore uncertain regions of the environment to discover high reward states while satisfying safety constraints.", "motivation": "Risk-aversion leads to conservative exploration of the environment, which typically results in converging to sub-optimal policies that fail to adequately maximise reward or fail to achieve the goal.", "method": "Optimistic Risk-averse Actor Critic (ORAC) constructs an exploratory policy by maximising a local upper confidence bound of the state-action reward value function while minimising a local lower confidence bound of the risk-averse state-action cost value function.", "result": "Experimental results demonstrate that the ORAC approach prevents convergence to sub-optimal policies and improves significantly the reward-cost trade-off in various continuous control tasks.", "conclusion": "The ORAC approach prevents convergence to sub-optimal policies and significantly improves the reward-cost trade-off in various continuous control tasks."}}
{"id": "2507.08508", "pdf": "https://arxiv.org/pdf/2507.08508", "abs": "https://arxiv.org/abs/2507.08508", "authors": ["Haotian Xu", "Jinrui Zhou", "Xichong Zhang", "Mingjun Xiao", "He Sun", "Yin Xu"], "title": "SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) is a distributed machine learning paradigm which\ncoordinates multiple clients to collaboratively train a global model via a\ncentral server. Sequential Federated Learning (SFL) is a newly-emerging FL\ntraining framework where the global model is trained in a sequential manner\nacross clients. Since SFL can provide strong convergence guarantees under data\nheterogeneity, it has attracted significant research attention in recent years.\nHowever, experiments show that SFL suffers from severe catastrophic forgetting\nin heterogeneous environments, meaning that the model tends to forget knowledge\nlearned from previous clients. To address this issue, we propose an SFL\nframework with discrepancy-aware multi-teacher knowledge distillation, called\nSFedKD, which selects multiple models from the previous round to guide the\ncurrent round of training. In SFedKD, we extend the single-teacher Decoupled\nKnowledge Distillation approach to our multi-teacher setting and assign\ndistinct weights to teachers' target-class and non-target-class knowledge based\non the class distributional discrepancy between teacher and student data.\nThrough this fine-grained weighting strategy, SFedKD can enhance model training\nefficacy while mitigating catastrophic forgetting. Additionally, to prevent\nknowledge dilution, we eliminate redundant teachers for the knowledge\ndistillation and formalize it as a variant of the maximum coverage problem.\nBased on the greedy strategy, we design a complementary-based teacher selection\nmechanism to ensure that the selected teachers achieve comprehensive knowledge\nspace coverage while reducing communication and computational costs. Extensive\nexperiments show that SFedKD effectively overcomes catastrophic forgetting in\nSFL and outperforms state-of-the-art FL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSFedKD\uff0c\u4e00\u79cd\u65b0\u7684SFL\u6846\u67b6\uff0c\u7528\u4ee5\u89e3\u51b3SFL\u5728\u5f02\u6784\u73af\u5883\u4e0b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "SFL\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5bb9\u6613\u5fd8\u8bb0\u4ece\u524d\u4e00\u5ba2\u6237\u7aef\u5b66\u5230\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFedKD\u7684SFL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u524d\u4e00\u8f6e\u7684\u591a\u4e2a\u6a21\u578b\u6765\u6307\u5bfc\u5f53\u524d\u8f6e\u7684\u8bad\u7ec3\uff0c\u5e76\u5c06\u5355\u4e00\u6559\u5e08\u89e3\u8026\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u6559\u5e08\u73af\u5883\uff0c\u6839\u636e\u6559\u5e08\u548c\u5b66\u751f\u6570\u636e\u4e4b\u95f4\u7684\u7c7b\u522b\u5206\u5e03\u5dee\u5f02\u5bf9\u76ee\u6807\u7c7b\u548c\u975e\u76ee\u6807\u7c7b\u77e5\u8bc6\u8d4b\u4e88\u4e0d\u540c\u7684\u6743\u91cd\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9632\u6b62\u77e5\u8bc6\u7a00\u91ca\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u8865\u7684\u6559\u5e08\u9009\u62e9\u673a\u5236\u3002", "result": "SFedKD\u80fd\u6709\u6548\u514b\u670dSFL\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "SFedKD\u6709\u6548\u5730\u514b\u670d\u4e86SFL\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684FL\u65b9\u6cd5\u3002"}}
{"id": "2507.08537", "pdf": "https://arxiv.org/pdf/2507.08537", "abs": "https://arxiv.org/abs/2507.08537", "authors": ["Yuting Tang", "Yivan Zhang", "Johannes Ackermann", "Yu-Jie Zhang", "Soichiro Nishimori", "Masashi Sugiyama"], "title": "Recursive Reward Aggregation", "categories": ["cs.LG", "math.CT"], "comment": "Reinforcement Learning Conference 2025", "summary": "In reinforcement learning (RL), aligning agent behavior with specific\nobjectives typically requires careful design of the reward function, which can\nbe challenging when the desired objectives are complex. In this work, we\npropose an alternative approach for flexible behavior alignment that eliminates\nthe need to modify the reward function by selecting appropriate reward\naggregation functions. By introducing an algebraic perspective on Markov\ndecision processes (MDPs), we show that the Bellman equations naturally emerge\nfrom the recursive generation and aggregation of rewards, allowing for the\ngeneralization of the standard discounted sum to other recursive aggregations,\nsuch as discounted max and Sharpe ratio. Our approach applies to both\ndeterministic and stochastic settings and integrates seamlessly with\nvalue-based and actor-critic algorithms. Experimental results demonstrate that\nour approach effectively optimizes diverse objectives, highlighting its\nversatility and potential for real-world applications.", "AI": {"tldr": "This paper proposes an alternative approach for flexible behavior alignment in reinforcement learning by selecting appropriate reward aggregation functions, eliminating the need to modify the reward function.", "motivation": "Aligning agent behavior with specific objectives in reinforcement learning typically requires careful design of the reward function, which can be challenging when the desired objectives are complex.", "method": "An algebraic perspective on Markov decision processes is introduced to generalize the standard discounted sum to other recursive aggregations, allowing for flexible behavior alignment in RL.", "result": "Experimental results demonstrate that the proposed approach effectively optimizes diverse objectives in both deterministic and stochastic settings.", "conclusion": "The proposed approach in this paper provides a flexible way for behavior alignment in reinforcement learning without the need to modify the reward function, which has great potential for real-world applications."}}
{"id": "2507.08542", "pdf": "https://arxiv.org/pdf/2507.08542", "abs": "https://arxiv.org/abs/2507.08542", "authors": ["Tianyou Jiang"], "title": "CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes", "categories": ["cs.LG"], "comment": null, "summary": "Circular RNAs (circRNAs) are important components of the non-coding RNA\nregulatory network. Previous circRNA identification primarily relies on\nhigh-throughput RNA sequencing (RNA-seq) data combined with alignment-based\nalgorithms that detect back-splicing signals. However, these methods face\nseveral limitations: they can't predict circRNAs directly from genomic DNA\nsequences and relies heavily on RNA experimental data; they involve high\ncomputational costs due to complex alignment and filtering steps; and they are\ninefficient for large-scale or genome-wide circRNA prediction. The challenge is\neven greater in plants, where plant circRNA splice sites often lack the\ncanonical GT-AG motif seen in human mRNA splicing, and no efficient deep\nlearning model with strong generalization capability currently exists.\nFurthermore, the number of currently identified plant circRNAs is likely far\nlower than their true abundance. In this paper, we propose a deep learning\nframework named CircFormerMoE based on transformers and mixture-of experts for\npredicting circRNAs directly from plant genomic DNA. Our framework consists of\ntwo subtasks known as splicing site detection (SSD) and splicing site pairing\n(SSP). The model's effectiveness has been validated on gene data of 10 plant\nspecies. Trained on known circRNA instances, it is also capable of discovering\npreviously unannotated circRNAs. In addition, we performed interpretability\nanalyses on the trained model to investigate the sequence patterns contributing\nto its predictions. Our framework provides a fast and accurate computational\nmethod and tool for large-scale circRNA discovery in plants, laying a\nfoundation for future research in plant functional genomics and non-coding RNA\nannotation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCircFormerMoE\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u690d\u7269\u57fa\u56e0\u7ec4DNA\u9884\u6d4bcircRNAs\u3002", "motivation": "\u73b0\u6709\u7684circRNA\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u8bb8\u591a\u9650\u5236\uff1a\u4e0d\u80fd\u76f4\u63a5\u4ece\u57fa\u56e0\u7ec4DNA\u5e8f\u5217\u9884\u6d4bcircRNAs\uff0c\u4e25\u91cd\u4f9d\u8d56RNA\u5b9e\u9a8c\u6570\u636e\uff1b\u6d89\u53ca\u590d\u6742\u7684\u6bd4\u5bf9\u548c\u8fc7\u6ee4\u6b65\u9aa4\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b\u5728\u5927\u89c4\u6a21\u6216\u5168\u57fa\u56e0\u7ec4circRNA\u9884\u6d4b\u65b9\u9762\u6548\u7387\u4f4e\u4e0b\u3002\u5c24\u5176\u5728\u690d\u7269\u4e2d\uff0c\u6311\u6218\u66f4\u5927\uff0c\u56e0\u4e3a\u690d\u7269circRNA\u526a\u63a5\u4f4d\u70b9\u5e38\u5e38\u7f3a\u4e4f\u4eba\u7c7bmRNA\u526a\u63a5\u4e2d\u770b\u5230\u7684\u7ecf\u5178GT-AG\u57fa\u5e8f\uff0c\u5e76\u4e14\u76ee\u524d\u4e0d\u5b58\u5728\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u7684\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u6b64\u5916\uff0c\u76ee\u524d\u5df2\u9274\u5b9a\u7684\u690d\u7269circRNAs\u6570\u91cf\u53ef\u80fd\u8fdc\u8fdc\u4f4e\u4e8e\u5176\u771f\u5b9e\u4e30\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8etransformers\u548c\u6df7\u5408\u4e13\u5bb6\uff08mixture-of-experts\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6CircFormerMoE\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u690d\u7269\u57fa\u56e0\u7ec4DNA\u9884\u6d4bcircRNAs\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u5373\u526a\u63a5\u4f4d\u70b9\u68c0\u6d4b\uff08SSD\uff09\u548c\u526a\u63a5\u4f4d\u70b9\u914d\u5bf9\uff08SSP\uff09\u3002", "result": "\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\u5df2\u572810\u4e2a\u690d\u7269\u7269\u79cd\u7684\u57fa\u56e0\u6570\u636e\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u7ecf\u8fc7\u5df2\u77e5circRNA\u5b9e\u4f8b\u7684\u8bad\u7ec3\uff0c\u5b83\u8fd8\u80fd\u591f\u53d1\u73b0\u4ee5\u524d\u672a\u6ce8\u91ca\u7684circRNAs\u3002\u6211\u4eec\u8fd8\u5bf9\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u4ee5\u7814\u7a76\u5bfc\u81f4\u5176\u9884\u6d4b\u7684\u5e8f\u5217\u6a21\u5f0f\u3002", "conclusion": "CircFormerMoE\u6846\u67b6\u4e3a\u690d\u7269circRNA\u7684\u5927\u89c4\u6a21\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u7684\u8ba1\u7b97\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u4e3a\u690d\u7269\u529f\u80fd\u57fa\u56e0\u7ec4\u5b66\u548c\u975e\u7f16\u7801RNA\u6ce8\u91ca\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.08563", "pdf": "https://arxiv.org/pdf/2507.08563", "abs": "https://arxiv.org/abs/2507.08563", "authors": ["Xinyi Ning", "Zilin Bian", "Kaan Ozbay", "Semiha Ergan"], "title": "STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving", "categories": ["cs.LG"], "comment": "6 pages, 3 figures, accepted at ITSC 2025", "summary": "Accurate vehicle trajectory prediction is essential for ensuring safety and\nefficiency in fully autonomous driving systems. While existing methods\nprimarily focus on modeling observed motion patterns and interactions with\nother vehicles, they often neglect the potential risks posed by the uncertain\nor aggressive behaviors of surrounding vehicles. In this paper, we propose a\nnovel spatial-temporal risk-attentive trajectory prediction framework that\nincorporates a risk potential field to assess perceived risks arising from\nbehaviors of nearby vehicles. The framework leverages a spatial-temporal\nencoder and a risk-attentive feature fusion decoder to embed the risk potential\nfield into the extracted spatial-temporal feature representations for\ntrajectory prediction. A risk-scaled loss function is further designed to\nimprove the prediction accuracy of high-risk scenarios, such as short relative\nspacing. Experiments on the widely used NGSIM and HighD datasets demonstrate\nthat our method reduces average prediction errors by 4.8% and 31.2%\nrespectively compared to state-of-the-art approaches, especially in high-risk\nscenarios. The proposed framework provides interpretable, risk-aware\npredictions, contributing to more robust decision-making for autonomous driving\nsystems.", "AI": {"tldr": "This paper proposes a spatial-temporal risk-attentive trajectory prediction framework for autonomous driving systems, which incorporates a risk potential field to improve prediction accuracy, especially in high-risk scenarios.", "motivation": "Accurate vehicle trajectory prediction is essential for ensuring safety and efficiency in fully autonomous driving systems. Existing methods often neglect the potential risks posed by the uncertain or aggressive behaviors of surrounding vehicles.", "method": "A novel spatial-temporal risk-attentive trajectory prediction framework that incorporates a risk potential field to assess perceived risks arising from behaviors of nearby vehicles.", "result": "Experiments on the widely used NGSIM and HighD datasets demonstrate that our method reduces average prediction errors by 4.8% and 31.2% respectively compared to state-of-the-art approaches, especially in high-risk scenarios.", "conclusion": "The proposed framework provides interpretable, risk-aware predictions, contributing to more robust decision-making for autonomous driving systems."}}
{"id": "2507.08567", "pdf": "https://arxiv.org/pdf/2507.08567", "abs": "https://arxiv.org/abs/2507.08567", "authors": ["Preslav Aleksandrov", "Meghdad Kurmanji", "Fernando Garcia Redondo", "David O'Shea", "William Shen", "Alex Iacob", "Lorenzo Sani", "Xinchi Qiu", "Nicola Cancedda", "Nicholas D. Lane"], "title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling", "categories": ["cs.LG"], "comment": "14 pages and 6 figures. Submitted to NeurIPS 2025", "summary": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.", "AI": {"tldr": "This paper introduces AbbIE, a novel recursive generalization of the encoder-only Transformer architecture that achieves better perplexity and allows for dynamic scaling of compute resources.", "motivation": "The motivation behind AbbIE is to achieve better perplexity than a standard Transformer and allow for the dynamic scaling of compute resources at test time, complementing the scaling of large language model (LLM) performance through parameter and token counts.", "method": "The Autoregressive Block-Based Iterative Encoder (AbbIE) is introduced as a novel recursive generalization of the encoder-only Transformer architecture.", "result": "AbbIE shows up to 12% improvement in zero-shot in-context learning tasks and up to 5% improvement in language perplexity compared to other iterative and standard methods.", "conclusion": "AbbIE opens a new avenue to Transformer performance scaling and shows significant improvements in zero-shot in-context learning tasks and language perplexity."}}
{"id": "2507.08605", "pdf": "https://arxiv.org/pdf/2507.08605", "abs": "https://arxiv.org/abs/2507.08605", "authors": ["Ando Shah", "Rajveer Singh", "Akram Zaytar", "Girmaw Abebe Tadesse", "Caleb Robinson", "Negar Tafti", "Stephen A. Wood", "Rahul Dodhia", "Juan M. Lavista Ferres"], "title": "Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India", "categories": ["cs.LG"], "comment": "Dataset and code will be published shortly and links updated in v2", "summary": "Rice cultivation consumes 24-30% of global freshwater, creating critical\nwater management challenges in major rice-producing regions. Sustainable\nirrigation practices like direct seeded rice (DSR) and alternate wetting and\ndrying (AWD) can reduce water use by 20-40% while maintaining yields, helping\nsecure long-term agricultural productivity as water scarcity intensifies - a\nkey component of the Zero Hunger Sustainable Development Goal. However, limited\ndata on adoption rates of these practices prevents evidence-based policymaking\nand targeted resource allocation. We developed a novel remote sensing framework\nto monitor sustainable water management practices at scale in Punjab, India - a\nregion facing severe groundwater depletion of 41.6 cm/year. To collect\nessential ground truth data, we partnered with the Nature Conservancy's\nPromoting Regenerative and No-burn Agriculture (PRANA) program, which trained\napproximately 1,400 farmers on water-saving techniques while documenting their\nfield-level practices. Using this data, we created a classification system with\nSentinel-1 satellite imagery that separates water management along sowing and\nirrigation dimensions. Our approach achieved a 78% F1-score in distinguishing\nDSR from traditional puddled transplanted rice without requiring prior\nknowledge of planting dates. We demonstrated scalability by mapping DSR\nadoption across approximately 3 million agricultural plots in Punjab, with\ndistrict-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)\nwith government records. This study provides policymakers with a powerful tool\nto track sustainable water management adoption, target interventions, and\nmeasure program impacts at scale.", "AI": {"tldr": "This paper presents a remote sensing framework to monitor sustainable water management practices in Punjab, India, achieving a 78% F1-score in distinguishing direct seeded rice from traditional methods and correlating well with government records.", "motivation": "Limited data on the adoption rates of sustainable irrigation practices like direct seeded rice (DSR) and alternate wetting and drying (AWD) prevents evidence-based policymaking and targeted resource allocation.", "method": "A novel remote sensing framework was developed using Sentinel-1 satellite imagery to monitor sustainable water management practices in Punjab, India.", "result": "The approach achieved a 78% F1-score in distinguishing DSR from traditional puddled transplanted rice and showed strong correlation (Pearson=0.77, RBO= 0.77) with government records.", "conclusion": "The study provides a powerful tool for policymakers to track sustainable water management adoption, target interventions, and measure program impacts at scale."}}
{"id": "2507.08610", "pdf": "https://arxiv.org/pdf/2507.08610", "abs": "https://arxiv.org/abs/2507.08610", "authors": ["Parag Dutta", "Ambedkar Dukkipati"], "title": "Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.", "AI": {"tldr": "This paper proposes LoGIC, a Multi-agent Reinforcement Learning game for unsupervised image captioning, showing improvements over existing methods.", "motivation": "Considering the challenge of improving performance in existing labelled datasets for Vision Language Models (VLMs), it is essential to explore unsupervised image captioning performance.", "method": "The proposed method, LoGIC (Lewis Communication Game for Image Captioning), consists of two agents, a 'speaker' and a 'listener', trained in a cooperative common-reward setting using the GRPO algorithm.", "result": "Using pre-trained VLMs as the 'speaker' and a Large Language Model (LLM) for language understanding in the 'listener', a 46 BLEU score was achieved after fine-tuning using LoGIC without additional labels. Replacing the VLM from the 'speaker' with lightweight components obtained a 31 BLEU score in the unsupervised setting.", "conclusion": "The proposed LoGIC method shows significant improvements in unsupervised image captioning performance and opens up new possibilities for lightweight components in the 'speaker' role."}}
{"id": "2507.08686", "pdf": "https://arxiv.org/pdf/2507.08686", "abs": "https://arxiv.org/abs/2507.08686", "authors": ["Uri Stern", "Eli Corn", "Daphna Weinshall"], "title": "Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation", "categories": ["cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.12968", "summary": "Overfitting in deep neural networks occurs less frequently than expected.\nThis is a puzzling observation, as theory predicts that greater model capacity\nshould eventually lead to overfitting -- yet this is rarely seen in practice.\nBut what if overfitting does occur, not globally, but in specific sub-regions\nof the data space? In this work, we introduce a novel score that measures the\nforgetting rate of deep models on validation data, capturing what we term local\noverfitting: a performance degradation confined to certain regions of the input\nspace. We demonstrate that local overfitting can arise even without\nconventional overfitting, and is closely linked to the double descent\nphenomenon.\n  Building on these insights, we introduce a two-stage approach that leverages\nthe training history of a single model to recover and retain forgotten\nknowledge: first, by aggregating checkpoints into an ensemble, and then by\ndistilling it into a single model of the original size, thus enhancing\nperformance without added inference cost.\n  Extensive experiments across multiple datasets, modern architectures, and\ntraining regimes validate the effectiveness of our approach. Notably, in the\npresence of label noise, our method -- Knowledge Fusion followed by Knowledge\nDistillation -- outperforms both the original model and independently trained\nensembles, achieving a rare win-win scenario: reduced training and inference\ncomplexity.", "AI": {"tldr": "This paper investigates local overfitting in deep learning models and proposes a two-step solution that improves performance without extra inference cost.", "motivation": "The motivation behind this research is the puzzling observation that deep neural networks do not overfit as frequently as expected despite their large capacity. This work explores whether overfitting might instead manifest locally within specific areas of the data space.", "method": "The researchers introduce a novel score to measure the forgetting rate on validation data, capturing 'local overfitting.' They then propose a two-stage method involving aggregating model checkpoints into an ensemble and distilling this ensemble into a single model.", "result": "Results show that the proposed method performs well across various datasets and architectures, particularly excelling when dealing with label noise, where it surpasses both the original model and independently trained ensembles.", "conclusion": "The study concludes that local overfitting can occur in deep neural networks, which isn't typically captured by conventional metrics. The introduced two-stage approach of Knowledge Fusion and Distillation effectively combats this issue, enhancing model performance without increasing inference costs."}}
{"id": "2507.08697", "pdf": "https://arxiv.org/pdf/2507.08697", "abs": "https://arxiv.org/abs/2507.08697", "authors": ["Waqar Muhammad Ashraf", "Amir H. Keshavarzzadeh", "Abdulelah S. Alshehri", "Abdulrahman bin Jumah", "Ramit Debnath", "Vivek Dua"], "title": "Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "The domain-consistent adoption of artificial intelligence (AI) remains low in\nthermal power plants due to the black-box nature of AI algorithms and low\nrepresentation of domain knowledge in conventional data-centric analytics. In\nthis paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)\nframework that incorporates the Mahalanobis distance-based constraint to\nintroduce domain knowledge into data-centric analytics. The developed MAD-OPT\nframework is applied to maximize thermal efficiency and minimize turbine heat\nrate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT\nframework can estimate domain-informed optimal process conditions under\ndifferent ambient conditions, and the optimal solutions are found to be robust\nas evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to\nestimate optimal process conditions beyond the design power generation limit of\nthe gas turbine system, and have found comparable results with the actual data\nof the power plant. We demonstrate that implementing data-centric optimization\nanalytics without incorporating domain-informed constraints may provide\nineffective solutions that may not be implementable in the real operation of\nthe gas turbine system. This research advances the integration of the\ndata-driven domain knowledge into machine learning-powered analytics that\nenhances the domain-informed operation excellence and paves the way for safe AI\nadoption in thermal power systems.", "AI": {"tldr": "Developed a MAD-OPT framework to incorporate domain knowledge into data-centric analytics for thermal power plants, demonstrating its effectiveness in optimizing process conditions.", "motivation": "The domain-consistent adoption of artificial intelligence (AI) remains low in thermal power plants due to the black-box nature of AI algorithms and low representation of domain knowledge in conventional data-centric analytics.", "method": "Developed a MAhalanobis Distance-based OPTimization (MAD-OPT) framework that incorporates Mahalanobis distance-based constraint to introduce domain knowledge into data-centric analytics.", "result": "The MAD-OPT framework can estimate domain-informed optimal process conditions under different ambient conditions, with robust solutions as evaluated by Monte Carlo simulations. Comparable results were found when estimating optimal conditions beyond the design limit.", "conclusion": "This research advances the integration of data-driven domain knowledge into machine learning-powered analytics, paving the way for safe AI adoption in thermal power systems."}}
{"id": "2507.08707", "pdf": "https://arxiv.org/pdf/2507.08707", "abs": "https://arxiv.org/abs/2507.08707", "authors": ["Peter Crowley", "Zachary Serlin", "Tyler Paine", "Makai Mann", "Michael Benjamin", "Calin Belta"], "title": "SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5SPLASH\uff0c\u5b83\u5f25\u8865\u4e86IRL\u5728\u957f\u65f6\u6bb5\u548c\u5bf9\u6297\u6027\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u80fd\u529b\u5c5e\u4e8e\u957f\u65f6\u6bb5\u548c/\u6216\u5bf9\u6297\u6027\u4efb\u52a1\u7c7b\u522b\u4e2d\u7684\u4e00\u4e2a\u6216\u4e24\u4e2a\uff0c\u8fd9\u7a81\u663e\u4e86IRL\u4ea7\u751f\u73b0\u573a\u51c6\u5907\u597d\u7684\u673a\u5668\u4eba\u4ee3\u7406\u7684\u80fd\u529b\u7684\u4e00\u4e2a\u5173\u952e\u7f3a\u9677\u3002", "method": "\u5f15\u5165\u4e86SPLASH\uff0c\u8fd9\u662f\u4e00\u79cd\u4ece\u6b21\u4f18\u7684\u5c42\u7ea7\u6f14\u793a\u4e2d\u5b66\u4e60\u957f\u65f6\u6bb5\u5bf9\u6297\u4efb\u52a1\u7684\u9ad8\u6548\u504f\u597d\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u6a21\u62df\u7684\u6d77\u4e0a\u6355\u83b7\u65d7\u4efb\u52a1\u4e2d\u5bf9SPLASH\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3b\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u6362\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u73b0\u5b9e\u4e16\u754c\u7684\u9002\u7528\u6027\u3002", "conclusion": "SPLASH\u5728\u4ece\u6b21\u4f18\u6f14\u793a\u4e2d\u5b66\u4e60\u5956\u52b1\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.08718", "pdf": "https://arxiv.org/pdf/2507.08718", "abs": "https://arxiv.org/abs/2507.08718", "authors": ["Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "On the Effect of Regularization in Policy Mirror Descent", "categories": ["cs.LG"], "comment": "Accepted at RLC", "summary": "Policy Mirror Descent (PMD) has emerged as a unifying framework in\nreinforcement learning (RL) by linking policy gradient methods with a\nfirst-order optimization method known as mirror descent. At its core, PMD\nincorporates two key regularization components: (i) a distance term that\nenforces a trust region for stable policy updates and (ii) an MDP regularizer\nthat augments the reward function to promote structure and robustness. While\nPMD has been extensively studied in theory, empirical investigations remain\nscarce. This work provides a large-scale empirical analysis of the interplay\nbetween these two regularization techniques, running over 500k training seeds\non small RL environments. Our results demonstrate that, although the two\nregularizers can partially substitute each other, their precise combination is\ncritical for achieving robust performance. These findings highlight the\npotential for advancing research on more robust algorithms in RL, particularly\nwith respect to hyperparameter sensitivity.", "AI": {"tldr": "\u63d0\u4f9b\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u4e2d\u4e24\u79cd\u5173\u952e\u6b63\u5219\u5316\u7ec4\u4ef6\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u5f3a\u8c03\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f00\u53d1\u66f4\u9c81\u68d2\u7b97\u6cd5\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d85\u53c2\u6570\u654f\u611f\u6027\u65b9\u9762\u3002", "motivation": "\u5c3d\u7ba1\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u5728\u7406\u8bba\u4e0a\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b9e\u8bc1\u8c03\u67e5\u4ecd\u7136\u7a00\u7f3a\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u5c0f\u578bRL\u73af\u5883\u4e0a\u8fd0\u884c\u8d85\u8fc7500k\u4e2a\u8bad\u7ec3\u79cd\u5b50\uff0c\u5bf9\u8fd9\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u4e24\u4e2a\u6b63\u5219\u5316\u5668\u53ef\u4ee5\u90e8\u5206\u66ff\u4ee3\u5f7c\u6b64\uff0c\u4f46\u5b83\u4eec\u7684\u7cbe\u786e\u7ec4\u5408\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\u7684\u7cbe\u786e\u7ec4\u5408\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5f3a\u8c03\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f00\u53d1\u66f4\u9c81\u68d2\u7b97\u6cd5\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d85\u53c2\u6570\u654f\u611f\u6027\u65b9\u9762\u3002"}}
{"id": "2507.08746", "pdf": "https://arxiv.org/pdf/2507.08746", "abs": "https://arxiv.org/abs/2507.08746", "authors": ["Paolo Marcandelli", "Yuanchun He", "Stefano Mariani", "Martina Siena", "Stefano Markidis"], "title": "Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),\na generalization of the Quantum Fourier Neural Operator (QFNO) for scientific\nmachine learning. PHQFNO partitions the Fourier operator computation across\nclassical and quantum resources, enabling tunable quantum-classical\nhybridization and distributed execution across quantum and classical devices.\nThe method extends QFNOs to higher dimensions and incorporates a\nmessage-passing framework to distribute data across different partitions. Input\ndata are encoded into quantum states using unary encoding, and quantum circuit\nparameters are optimized using a variational scheme. We implement PHQFNO using\nPennyLane with PyTorch integration and evaluate it on Burgers' equation,\nincompressible and compressible Navier-Stokes equations. We show that PHQFNO\nrecovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO\nachieves higher accuracy than its classical counterparts. Finally, we perform a\nsensitivity analysis under input noise, confirming improved stability of PHQFNO\nover classical baselines.", "AI": {"tldr": "This paper introduces PHQFNO, a generalization of QFNO that enables tunable quantum-classical hybridization. The method is evaluated on various equations and shows higher accuracy and stability compared to classical counterparts.", "motivation": "To generalize the Quantum Fourier Neural Operator (QFNO) for scientific machine learning and enable tunable quantum-classical hybridization and distributed execution across quantum and classical devices.", "method": "The method extends QFNOs to higher dimensions, incorporates a message-passing framework to distribute data across different partitions, encodes input data into quantum states using unary encoding, and optimizes quantum circuit parameters using a variational scheme.", "result": "PHQFNO achieves higher accuracy than its classical counterparts on incompressible Navier-Stokes equations and shows improved stability over classical baselines under input noise.", "conclusion": "PHQFNO recovers classical FNO accuracy and shows improved stability over classical baselines under input noise."}}
{"id": "2507.08749", "pdf": "https://arxiv.org/pdf/2507.08749", "abs": "https://arxiv.org/abs/2507.08749", "authors": ["Chuanqi Chen", "Zhongrui Wang", "Nan Chen", "Jin-Long Wu"], "title": "Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network", "categories": ["cs.LG"], "comment": null, "summary": "A discrete-time conditional Gaussian Koopman network (CGKN) is developed in\nthis work to learn surrogate models that can perform efficient state forecast\nand data assimilation (DA) for high-dimensional complex dynamical systems,\ne.g., systems governed by nonlinear partial differential equations (PDEs).\nFocusing on nonlinear partially observed systems that are common in many\nengineering and earth science applications, this work exploits Koopman\nembedding to discover a proper latent representation of the unobserved system\nstates, such that the dynamics of the latent states are conditional linear,\ni.e., linear with the given observed system states. The modeled system of the\nobserved and latent states then becomes a conditional Gaussian system, for\nwhich the posterior distribution of the latent states is Gaussian and can be\nefficiently evaluated via analytical formulae. The analytical formulae of DA\nfacilitate the incorporation of DA performance into the learning process of the\nmodeled system, which leads to a framework that unifies scientific machine\nlearning (SciML) and data assimilation. The performance of discrete-time CGKN\nis demonstrated on several canonical problems governed by nonlinear PDEs with\nintermittency and turbulent features, including the viscous Burgers' equation,\nthe Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with\nwhich we show that the discrete-time CGKN framework achieves comparable\nperformance as the state-of-the-art SciML methods in state forecast and\nprovides efficient and accurate DA results. The discrete-time CGKN framework\nalso serves as an example to illustrate unifying the development of SciML\nmodels and their other outer-loop applications such as design optimization,\ninverse problems, and optimal control.", "AI": {"tldr": "Developed a discrete-time conditional Gaussian Koopman network (CGKN) to learn surrogate models that can perform efficient state forecast and data assimilation (DA) for high-dimensional complex dynamical systems.", "motivation": "To perform efficient state forecast and data assimilation (DA) for high-dimensional complex dynamical systems", "method": "developed a discrete-time conditional Gaussian Koopman network (CGKN) to learn surrogate models", "result": "Demonstrated the performance of discrete-time CGKN on several canonical problems governed by nonlinear PDEs with intermittency and turbulent features", "conclusion": "The discrete-time CGKN framework achieves comparable performance as the state-of-the-art SciML methods in state forecast and provides efficient and accurate DA results."}}
{"id": "2507.08751", "pdf": "https://arxiv.org/pdf/2507.08751", "abs": "https://arxiv.org/abs/2507.08751", "authors": ["Tiffany Yu", "Rye Stahle-Smith", "Darssan Eswaramoorthi", "Rasha Karakchi"], "title": "ML-Based Automata Simplification for Symbolic Accelerators", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic accelerators are increasingly used for symbolic data processing in\ndomains such as genomics, NLP, and cybersecurity. However, these accelerators\nface scalability issues due to excessive memory use and routing complexity,\nespecially when targeting a large set. We present AutoSlim, a machine\nlearning-based graph simplification framework designed to reduce the complexity\nof symbolic accelerators built on Non-deterministic Finite Automata (NFA)\ndeployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest\nclassification to prune low-impact transitions based on edge scores and\nstructural features, significantly reducing automata graph density while\npreserving semantic correctness. Unlike prior tools, AutoSlim targets automated\nscore-aware simplification with weighted transitions, enabling efficient\nranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in\nNAPOLY+ and conducted performance measurements including latency, throughput,\nand resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs\nand over 30 percent pruning in transitions, while scaling to graphs an order of\nmagnitude larger than existing benchmarks. Our results also demonstrate how\nhardware interconnection (fanout) heavily influences hardware cost and that\nAutoSlim's pruning mitigates resource blowup.", "AI": {"tldr": "This paper introduces AutoSlim, a machine learning framework that simplifies automata graphs for symbolic accelerators, reducing resource usage while maintaining semantic correctness.", "motivation": "Symbolic accelerators face scalability issues due to high memory use and routing complexity, especially when targeting large data sets.", "method": "AutoSlim uses machine learning (Random Forest classification) to perform graph simplification by pruning low-impact transitions based on edge scores and structural features.", "result": "AutoSlim achieves up to 40% reduction in FPGA LUTs and over 30% pruning in transitions, scaling to graphs an order of magnitude larger than existing benchmarks.", "conclusion": "AutoSlim effectively reduces the complexity of symbolic accelerators on FPGA-based overlays, providing a scalable solution for large graphs."}}
{"id": "2507.08771", "pdf": "https://arxiv.org/pdf/2507.08771", "abs": "https://arxiv.org/abs/2507.08771", "authors": ["Chenyang Song", "Weilin Zhao", "Xu Han", "Chaojun Xiao", "Yingfa Chen", "Yuxuan Li", "Zhiyuan Liu", "Maosong Sun"], "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity", "categories": ["cs.LG", "cs.CL"], "comment": "21 pages, 7 figures, 15 tables", "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aBlockFFN\u7684\u65b0MoE\u67b6\u6784\u53ca\u5176\u6709\u6548\u8bad\u7ec3\u548c\u90e8\u7f72\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba1\u7b97\u8d1f\u62c5\u95ee\u9898\uff0c\u4ee5\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u4e3a\u4ee3\u8868\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u67b6\u6784\u53d7\u5230\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0cvanilla MoE\u7684\u4e0d\u53ef\u5fae\u5206\u548c\u4e0d\u7075\u6d3b\u7684\u8def\u7531\u635f\u5bb3\u4e86\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u6bcf\u4e2a\u6807\u8bb0\u53ea\u6fc0\u6d3b\u5c11\u91cf\u53c2\u6570\uff0c\u8fd9\u4e9b\u7a00\u758f\u6fc0\u6d3b\u7684\u67b6\u6784\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u5757\u7ea7\u7a00\u758f\u6027\uff0c\u8fd9\u610f\u5473\u7740\u591a\u4e2a\u8fde\u7eed\u6807\u8bb0\u7684\u8054\u5408\u6fc0\u6d3b\u4e86\u5927\u6bd4\u4f8b\u7684\u53c2\u6570\u3002\u8fd9\u79cd\u7a00\u758f\u6a21\u5f0f\u5728\u8d44\u6e90\u6761\u4ef6\u4f4e\u4e0b\uff08\u4f8b\u5982\uff0c\u7ec8\u7aef\u8bbe\u5907\uff09\u4e0d\u5229\u4e8e\u52a0\u901f\uff0c\u5e76\u4e14\u4e0e\u4e3b\u6d41\u52a0\u901f\u6280\u672f\uff08\u4f8b\u5982\uff0c\u63a8\u6d4b\u89e3\u7801\uff09\u4e0d\u517c\u5bb9\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MoE\u67b6\u6784BlockFFN\u53ca\u5176\u6709\u6548\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u6280\u672f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u7ed3\u5408ReLU\u6fc0\u6d3b\u548cRMSNorm\u7684\u8def\u7531\u8fdb\u884c\u53ef\u5fae\u5206\u4e14\u7075\u6d3b\u7684\u8def\u7531\u3002\u4e3a\u4e86\u4fc3\u8fdb\u4ee4\u724c\u7ea7\u7a00\u758f\u6027\uff08TLS\uff09\u548c\u5757\u7ea7\u7a00\u758f\u6027\uff08CLS\uff09\uff0c\u8bbe\u8ba1\u4e86\u5bf9CLS\u6709\u611f\u77e5\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7fBlockFFN\u66f4\u52a0\u53cb\u597d\u52a0\u901f\u3002\u6700\u540e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u52a0\u901f\u6838\uff0c\u9996\u6b21\u7ed3\u5408\u4e86\u6fc0\u6d3b\u7a00\u758f\u6027\u548c\u63a8\u6d4b\u89e3\u7801\u3002", "result": "BlockFFN\u5728\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u4f18\u4e8e\u5176\u4ed6MoE\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684TLS\u548c70%\u76848-token CLS\u3002\u6211\u4eec\u7684\u5185\u6838\u5728\u5b9e\u9645\u7aef\u4fa7\u8bbe\u5907\u4e0a\u6bd4\u5bc6\u96c6\u6a21\u578b\u52a0\u901f\u9ad8\u8fbe3.67\u500d\u3002\u6240\u6709\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5747\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "BlockFFN\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6bd4\u5176\u4ed6MoE\u57fa\u7ebf\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u7ec8\u7aef\u8bbe\u5907\u4e0a\u6bd4\u5bc6\u96c6\u6a21\u578b\u5feb3.67\u500d\u3002"}}
{"id": "2507.08784", "pdf": "https://arxiv.org/pdf/2507.08784", "abs": "https://arxiv.org/abs/2507.08784", "authors": ["Chuyan Chen", "Yutong He", "Pengrui Li", "Weichen Jia", "Kun Yuan"], "title": "Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees", "categories": ["cs.LG", "math.OC"], "comment": "18 pages, 5 figures", "summary": "Distributed optimization is pivotal for large-scale signal processing and\nmachine learning, yet communication overhead remains a major bottleneck.\nLow-rank gradient compression, in which the transmitted gradients are\napproximated by low-rank matrices to reduce communication, offers a promising\nremedy. Existing methods typically adopt either randomized or greedy\ncompression strategies: randomized approaches project gradients onto randomly\nchosen subspaces, introducing high variance and degrading empirical\nperformance; greedy methods select the most informative subspaces, achieving\nstrong empirical results but lacking convergence guarantees. To address this\ngap, we propose GreedyLore--the first Greedy Low-Rank gradient compression\nalgorithm for distributed learning with rigorous convergence guarantees.\nGreedyLore incorporates error feedback to correct the bias introduced by greedy\ncompression and introduces a semi-lazy subspace update that ensures the\ncompression operator remains contractive throughout all iterations. With these\ntechniques, we prove that GreedyLore achieves a convergence rate of\n$\\mathcal{O}(\\sigma/\\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD\nand Adam--marking the first linear speedup convergence rate for low-rank\ngradient compression. Extensive experiments are conducted to validate our\ntheoretical findings.", "AI": {"tldr": "This paper introduces GreedyLore, a new algorithm for low-rank gradient compression in distributed learning that combines the strengths of both randomized and greedy methods while addressing their weaknesses.", "motivation": "To address the gap between randomized and greedy compression strategies in low-rank gradient compression, which either introduce high variance or lack convergence guarantees.", "method": "The paper proposes GreedyLore - the first Greedy Low-Rank gradient compression algorithm for distributed learning with rigorous convergence guarantees.", "result": "With error feedback and semi-lazy subspace update techniques, GreedyLore achieves a convergence rate of $\\mathcal{O}(\\sigma/\\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD and Adam.", "conclusion": "GreedyLore provides a new approach to low-rank gradient compression with guaranteed convergence, making it a promising tool for distributed learning."}}
{"id": "2507.08794", "pdf": "https://arxiv.org/pdf/2507.08794", "abs": "https://arxiv.org/abs/2507.08794", "authors": ["Yulai Zhao", "Haolin Liu", "Dian Yu", "S. Y. Kung", "Haitao Mi", "Dong Yu"], "title": "One Token to Fool LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.", "AI": {"tldr": "Generative reward models are found to be vulnerable to superficial manipulations. A new robust generative reward model is trained to address this issue.", "motivation": "The motivation is to address the vulnerabilities of generative reward models to superficial manipulations and improve their reliability for complex reasoning tasks.", "method": "The method involves using large language models (LLMs) as judges in reinforcement learning with verifiable rewards (RLVR), and introducing a data augmentation strategy to train a new generative reward model.", "result": "The result is the development of a new generative reward model with substantially improved robustness.", "conclusion": "The paper concludes that generative reward models are vulnerable to superficial manipulations, and to mitigate this issue, a new robust generative reward model is trained with improved reliability."}}
{"id": "2507.08802", "pdf": "https://arxiv.org/pdf/2507.08802", "abs": "https://arxiv.org/abs/2507.08802", "authors": ["Denis Sutter", "Julian Minder", "Thomas Hofmann", "Tiago Pimentel"], "title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?", "categories": ["cs.LG"], "comment": "42 pages, 17 figures, code available in\n  github.com/densutter/non-linear-representation-dilemma", "summary": "The concept of causal abstraction got recently popularised to demystify the\nopaque decision-making processes of machine learning models; in short, a neural\nnetwork can be abstracted as a higher-level algorithm if there exists a\nfunction which allows us to map between them. Notably, most interpretability\npapers implement these maps as linear functions, motivated by the linear\nrepresentation hypothesis: the idea that features are encoded linearly in a\nmodel's representations. However, this linearity constraint is not required by\nthe definition of causal abstraction. In this work, we critically examine the\nconcept of causal abstraction by considering arbitrarily powerful alignment\nmaps. In particular, we prove that under reasonable assumptions, any neural\nnetwork can be mapped to any algorithm, rendering this unrestricted notion of\ncausal abstraction trivial and uninformative. We complement these theoretical\nfindings with empirical evidence, demonstrating that it is possible to\nperfectly map models to algorithms even when these models are incapable of\nsolving the actual task; e.g., on an experiment using randomly initialised\nlanguage models, our alignment maps reach 100% interchange-intervention\naccuracy on the indirect object identification task. This raises the non-linear\nrepresentation dilemma: if we lift the linearity constraint imposed to\nalignment maps in causal abstraction analyses, we are left with no principled\nway to balance the inherent trade-off between these maps' complexity and\naccuracy. Together, these results suggest an answer to our title's question:\ncausal abstraction is not enough for mechanistic interpretability, as it\nbecomes vacuous without assumptions about how models encode information.\nStudying the connection between this information-encoding assumption and causal\nabstraction should lead to exciting future work.", "AI": {"tldr": "This paper critically examines the concept of causal abstraction and proves that it becomes vacuous without assumptions about how models encode information.", "motivation": "The motivation behind this paper is to demystify the opaque decision-making processes of machine learning models through the concept of causal abstraction.", "method": "The authors critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps and prove that under reasonable assumptions, any neural network can be mapped to any algorithm. They also provide empirical evidence demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task.", "result": "Under reasonable assumptions, any neural network can be mapped to any algorithm, rendering the unrestricted notion of causal abstraction trivial and uninformative. The empirical evidence shows that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task.", "conclusion": "Causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information."}}
