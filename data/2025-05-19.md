<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 126]
- [cs.AI](#cs.AI) [总数: 39]
- [stat.ML](#stat.ML) [总数: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
*Jiazheng Zhang, Wenqing Jing, Zizhuo Zhang, Zhiheng Xi, Shihan Dou, Rongxiang Weng, Jiahuan Li, Jingang Wang, MingXu Cai, Shibo Hong, Tao Gui, Qi Zhang*

**Main category:** cs.LG

**TL;DR:** CRM improves robustness and generalization of reward models by filtering noise and structuring data.


<details>
  <summary>Details</summary>
**Motivation:** Noisy preferences in human feedback often lead to reward misgeneralization.

**Method:** Proposing an online framework that combines peer review and curriculum learning.

**Result:** Up to 9.94 points of accuracy gain on RewardBench under 40 percent label noise.

**Conclusion:** CRM improves generalization and robustness.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+Minds+Better+Than+One%3A+Collaborative+Reward+Modeling+for+LLM+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10597&send_immediately=true&force_search=false)

**Abstract:** Reward models (RMs) are essential for aligning large language models (LLMs)
with human values. However, noisy preferences in human feedback often lead to
reward misgeneralization, where RMs overfit to spurious patterns and provide
misleading signals during policy optimization. We systematically analyze the
training dynamics of preference pairs and identify that noisy examples are
harder to fit and introduce instability. Empirical evidence shows that LLMs
optimized using reward models trained on full noisy datasets perform worse than
those trained on filtered, high-quality preferences. To address this, we
propose Collaborative Reward Modeling (CRM), an online framework that enhances
robustness by combining peer review and curriculum learning. Two reward models
are trained in parallel and assess each other's data selections to filter out
potential noise. Curriculum learning structures the preference data from easy
to hard, ensuring synchronized training and stable feedback. Extensive
experiments demonstrate that CRM improves generalization, with up to 9.94
points of accuracy gain on RewardBench under 40 percent label noise. CRM is
also compatible with implicit-reward alignment methods, offering a practical
and versatile strategy for robust alignment.

</details>


### [2] [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
*Jiaxuan Liu, Zhenhua Ling*

**Main category:** cs.LG

**TL;DR:** Propose UDDETTS, a neural codec language model for controllable emotional TTS that unifies discrete and dimensional emotions.


<details>
  <summary>Details</summary>
**Motivation:** Improve controllable emotional TTS by addressing challenges such as predefined discrete emotion labels and lack of large-scale emotional speech datasets.

**Method:** Introduce ADV space for dimensional emotion description and design semi-supervised training strategy.

**Result:** Achieve linear emotion control along three dimensions of ADV space and excel in end-to-end emotional speech synthesis.

**Conclusion:** UDDETTS enhances controllable emotional TTS with unified discrete and dimensional emotions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UDDETTS%3A+Unifying+Discrete+and+Dimensional+Emotions+for+Controllable+Emotional+Text-to-Speech，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10599&send_immediately=true&force_search=false)

**Abstract:** Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.

</details>


### [3] [Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data](https://arxiv.org/abs/2505.10600)
*Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, Sarwar Jahan*

**Main category:** cs.LG

**TL;DR:** This study addresses the challenge of detecting rare cyber-attacks in IoT networks with imbalanced datasets by using hybrid sampling techniques. It evaluates several machine learning models and finds that the Random Forest model performs best, while the Soft Voting model also shows strong performance.


<details>
  <summary>Details</summary>
**Motivation:** To develop effective IDS for IoT networks with highly imbalanced datasets.

**Method:** Hybrid sampling techniques were applied, and several machine learning models including Random Forest, Soft Voting, SVC, KNN, MLP, and Logistic Regression were evaluated.

**Result:** Random Forest achieved the best performance with a Kappa score of 0.9903, test accuracy of 0.9961, and AUC of 0.9994. Soft Voting also showed strong performance with an accuracy of 0.9952 and AUC of 0.9997.

**Conclusion:** Hybrid sampling combined with robust model and feature selection significantly improves IoT security against cyber-attacks in highly imbalanced data environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+IoT+Cyber+Attack+Detection+in+the+Presence+of+Highly+Imbalanced+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10600，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10600&send_immediately=true&force_search=false)

**Abstract:** Due to the rapid growth in the number of Internet of Things (IoT) networks,
the cyber risk has increased exponentially, and therefore, we have to develop
effective IDS that can work well with highly imbalanced datasets. A high rate
of missed threats can be the result, as traditional machine learning models
tend to struggle in identifying attacks when normal data volume is much higher
than the volume of attacks. For example, the dataset used in this study reveals
a strong class imbalance with 94,659 instances of the majority class and only
28 instances of the minority class, making it quite challenging to determine
rare attacks accurately. The challenges presented in this research are
addressed by hybrid sampling techniques designed to improve data imbalance
detection accuracy in IoT domains. After applying these techniques, we evaluate
the performance of several machine learning models such as Random Forest, Soft
Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer
Perceptron (MLP), and Logistic Regression with respect to the classification of
cyber-attacks. The obtained results indicate that the Random Forest model
achieved the best performance with a Kappa score of 0.9903, test accuracy of
0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting
model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of
combining model predictions. Overall, this work demonstrates the value of
hybrid sampling combined with robust model and feature selection for
significantly improving IoT security against cyber-attacks, especially in
highly imbalanced data environments.

</details>


### [4] [Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models](https://arxiv.org/abs/2505.10606)
*Hector Pasten, Felipe Urrutia, Hector Jimenez, Cristian B. Calderon, Cristóbal Rojas, Alexander Kozachinskiy*

**Main category:** cs.LG

**TL;DR:** This paper uncovers two phenomena, isolation and continuity, in Transformers that prevent them from learning simple pattern sequences.


<details>
  <summary>Details</summary>
**Motivation:** To understand how Transformers work and process information for theoretical and empirical advancement.

**Method:** Mathematical proof and rigorous experiments.

**Result:** Isolation and continuity phenomena exist in all Transformers using compact positional encoding.

**Conclusion:** These findings highlight theoretical limitations of Transformers and their practical implications.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuity+and+Isolation+Lead+to+Doubts+or+Dilemmas+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10606&send_immediately=true&force_search=false)

**Abstract:** Understanding how Transformers work and how they process information is key
to the theoretical and empirical advancement of these machines. In this work,
we demonstrate the existence of two phenomena in Transformers, namely isolation
and continuity. Both of these phenomena hinder Transformers to learn even
simple pattern sequences. Isolation expresses that any learnable sequence must
be isolated from another learnable sequence, and hence some sequences cannot be
learned by a single Transformer at the same time. Continuity entails that an
attractor basin forms around a learned sequence, such that any sequence falling
in that basin will collapse towards the learned sequence. Here, we
mathematically prove these phenomena emerge in all Transformers that use
compact positional encoding, and design rigorous experiments, demonstrating
that the theoretical limitations we shed light on occur on the practical scale.

</details>


### [5] [MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices](https://arxiv.org/abs/2505.10607)
*Patara Trirat, Jae-Gil Lee*

**Main category:** cs.LG

**TL;DR:** MONAQ利用大型语言模型的推理能力，将硬件感知神经架构搜索(NAS)任务转化为多目标神经架构查询任务，以实现更高效的资源受限设备上的时间序列分析。


<details>
  <summary>Details</summary>
**Motivation:** 在资源受限的硬件上进行高效的时间序列分析对传感应用至关重要，但现有的硬件感知NAS方法未专注于通用时间序列分析和边缘部署。

**Method:** MONAQ通过多模态查询生成处理多模态时间序列输入和硬件约束，并使用基于LLM代理的多目标搜索来生成代码实现部署就绪模型。

**Result:** MONAQ发现的模型在15个数据集上的性能优于手工设计的模型和NAS基准模型，同时更加高效。

**Conclusion:** MONAQ展示了通过结合数值数据、时间序列图像和文本描述来改进大型语言模型理解时间序列数据的能力。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MONAQ%3A+Multi-Objective+Neural+Architecture+Querying+for+Time-Series+Analysis+on+Resource-Constrained+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10607&send_immediately=true&force_search=false)

**Abstract:** The growing use of smartphones and IoT devices necessitates efficient
time-series analysis on resource-constrained hardware, which is critical for
sensing applications such as human activity recognition and air quality
prediction. Recent efforts in hardware-aware neural architecture search (NAS)
automate architecture discovery for specific platforms; however, none focus on
general time-series analysis with edge deployment. Leveraging the
problem-solving and reasoning capabilities of large language models (LLM), we
propose MONAQ, a novel framework that reformulates NAS into Multi-Objective
Neural Architecture Querying tasks. MONAQ is equipped with multimodal query
generation for processing multimodal time-series inputs and hardware
constraints, alongside an LLM agent-based multi-objective search to achieve
deployment-ready models via code generation. By integrating numerical data,
time-series images, and textual descriptions, MONAQ improves an LLM's
understanding of time-series data. Experiments on fifteen datasets demonstrate
that MONAQ-discovered models outperform both handcrafted models and NAS
baselines while being more efficient.

</details>


### [6] [How many measurements are enough? Bayesian recovery in inverse problems with general distributions](https://arxiv.org/abs/2505.10630)
*Ben Adcock, Nick Huang*

**Main category:** cs.LG

**TL;DR:** This paper studies the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator, and noise distributions.


<details>
  <summary>Details</summary>
**Motivation:** To provide a unified framework for understanding the sample complexity of Bayesian recovery for solving inverse problems with arbitrary distributions.

**Method:** The authors consider posterior sampling according to an approximate prior P and establish sufficient conditions for stable and accurate recovery with high probability.

**Result:** The sample complexity depends on the intrinsic complexity of P and concentration bounds for the forward operator and noise distributions.

**Conclusion:** The paper shows that the sample complexity scales log-linearly with the latent dimension k for generative priors and establishes that coherence plays a fundamental role in Bayesian recovery as well.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+many+measurements+are+enough%3F+Bayesian+recovery+in+inverse+problems+with+general+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10630&send_immediately=true&force_search=false)

**Abstract:** We study the sample complexity of Bayesian recovery for solving inverse
problems with general prior, forward operator and noise distributions. We
consider posterior sampling according to an approximate prior $\mathcal{P}$,
and establish sufficient conditions for stable and accurate recovery with high
probability. Our main result is a non-asymptotic bound that shows that the
sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$,
quantified by its so-called approximate covering number, and (ii) concentration
bounds for the forward operator and noise distributions. As a key application,
we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a
latent distribution via a Deep Neural Network (DNN). We show that the sample
complexity scales log-linearly with the latent dimension $k$, thus establishing
the efficacy of DNN-based priors. Generalizing existing results on
deterministic (i.e., non-Bayesian) recovery for the important problem of random
sampling with an orthogonal matrix $U$, we show how the sample complexity is
determined by the coherence of $U$ with respect to the support of
$\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in
Bayesian recovery as well. Overall, our framework unifies and extends prior
work, providing rigorous guarantees for the sample complexity of solving
Bayesian inverse problems with arbitrary distributions.

</details>


### [7] [FRET: Feature Redundancy Elimination for Test Time Adaptation](https://arxiv.org/abs/2505.10641)
*Linjing You, Jiabao Lu, Xiayuan Huang, Xiangli Nie*

**Main category:** cs.LG

**TL;DR:** This paper introduces Feature Redundancy Elimination for Test-time Adaptation (FRET), which addresses the issue of feature redundancy increasing with domain shifts in test-time adaptation methods. It proposes two approaches: S-FRET, which minimizes feature redundancy directly, and G-FRET, which uses graph convolutional networks with contrastive learning to reduce redundancy and enhance discriminability. Experiments show G-FRET performs best.


<details>
  <summary>Details</summary>
**Motivation:** Existing test-time adaptation methods overlook feature redundancy, which increases with domain shifts, hindering model adaptability to new data.

**Method:** Introduces S-FRET and G-FRET methods to handle feature redundancy; G-FRET uses GCN with contrastive learning.

**Result:** Experiments on multiple model architectures, tasks, and datasets show the effectiveness of S-FRET and G-FRET achieving state-of-the-art performance.

**Conclusion:** G-FRET allows the model to extract non-redundant and highly discriminative features during inference, enabling more robust test-time adaptation.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FRET%3A+Feature+Redundancy+Elimination+for+Test+Time+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10641&send_immediately=true&force_search=false)

**Abstract:** Test-Time Adaptation (TTA) aims to enhance the generalization of deep
learning models when faced with test data that exhibits distribution shifts
from the training data. In this context, only a pre-trained model and unlabeled
test data are available, making it particularly relevant for privacy-sensitive
applications. In practice, we observe that feature redundancy in embeddings
tends to increase as domain shifts intensify in TTA. However, existing TTA
methods often overlook this redundancy, which can hinder the model's
adaptability to new data. To address this issue, we introduce Feature
Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for
TTA. A straightforward approach (S-FRET) is to directly minimize the feature
redundancy score as an optimization objective to improve adaptation. Despite
its simplicity and effectiveness, S-FRET struggles with label shifts, limiting
its robustness in real-world scenarios. To mitigate this limitation, we further
propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional
Network (GCN) with contrastive learning. This design not only reduces feature
redundancy but also enhances feature discriminability in both the
representation and prediction layers. Extensive experiments across multiple
model architectures, tasks, and datasets demonstrate the effectiveness of
S-FRET and show that G-FRET achieves state-of-the-art performance. Further
analysis reveals that G-FRET enables the model to extract non-redundant and
highly discriminative features during inference, thereby facilitating more
robust test-time adaptation.

</details>


### [8] [Accelerating Visual-Policy Learning through Parallel Differentiable Simulation](https://arxiv.org/abs/2505.10646)
*Haoxiang You, Yilang Liu, Ian Abraham*

**Main category:** cs.LG

**TL;DR:** Proposes an efficient visual policy learning algorithm using differentiable simulation and analytical gradients, achieving significant performance gains.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to develop a computationally efficient algorithm for visual policy learning.

**Method:** The proposed method uses differentiable simulation and first-order analytical policy gradients, and it decouples the rendering process from the computation graph.

**Result:** The method reduces computational and memory overhead, attenuates the policy gradient norm, and improves the stability and smoothness of optimization. It also reduces wall-clock training time and outperforms baseline methods in terms of final returns.

**Conclusion:** The method achieves significant improvements in complex tasks like humanoid locomotion, with a 4x increase in final return and successful learning of a humanoid running policy within 4 hours on a single GPU.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Visual-Policy+Learning+through+Parallel+Differentiable+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10646&send_immediately=true&force_search=false)

**Abstract:** In this work, we propose a computationally efficient algorithm for visual
policy learning that leverages differentiable simulation and first-order
analytical policy gradients. Our approach decouple the rendering process from
the computation graph, enabling seamless integration with existing
differentiable simulation ecosystems without the need for specialized
differentiable rendering software. This decoupling not only reduces
computational and memory overhead but also effectively attenuates the policy
gradient norm, leading to more stable and smoother optimization. We evaluate
our method on standard visual control benchmarks using modern GPU-accelerated
simulation. Experiments show that our approach significantly reduces wall-clock
training time and consistently outperforms all baseline methods in terms of
final returns. Notably, on complex tasks such as humanoid locomotion, our
method achieves a $4\times$ improvement in final return, and successfully
learns a humanoid running policy within 4 hours on a single GPU.

</details>


### [9] [Asymptotically-Optimal Gaussian Bandits with Side Observations](https://arxiv.org/abs/2505.10698)
*Alexia Atsidakou, Orestis Papadigenopoulos, Constantine Caramanis, Sujay Sanghavi, Sanjay Shakkottai*

**Main category:** cs.LG

**TL;DR:** This paper studies Gaussian bandits with general side information and develops an LP-based lower bound on regret, proposing the first asymptotically optimal algorithm for this setting.


<details>
  <summary>Details</summary>
**Motivation:** To address the problem of Gaussian bandits with general side information, which subsumes standard bandits, full-feedback, and graph-structured feedback as special cases.

**Method:** Constructing an LP-based asymptotic instance-dependent lower bound on the regret and developing an asymptotically optimal algorithm.

**Result:** The proposed algorithm is asymptotically optimal for the general setting of Gaussian bandits with side information.

**Conclusion:** This work contributes to the understanding of Gaussian bandits with side information by providing a novel approach to lower bound the regret and designing an optimal algorithm.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotically-Optimal+Gaussian+Bandits+with+Side+Observations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10698&send_immediately=true&force_search=false)

**Abstract:** We study the problem of Gaussian bandits with general side information, as
first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an
arm reveals information about other arms, according to an arbitrary a priori
known side information matrix: each element of this matrix encodes the fidelity
of the information that the ``row'' arm reveals about the ``column'' arm. In
the case of Gaussian noise, this model subsumes standard bandits,
full-feedback, and graph-structured feedback as special cases. In this work, we
first construct an LP-based asymptotic instance-dependent lower bound on the
regret. The LP optimizes the cost (regret) required to reliably estimate the
suboptimality gap of each arm. This LP lower bound motivates our main
contribution: the first known asymptotically optimal algorithm for this general
setting.

</details>


### [10] [Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model](https://arxiv.org/abs/2505.10665)
*Wei Wang, Weidong Yang, Lei Wang, Guihua Wang, Ruibo Lei*

**Main category:** cs.LG

**TL;DR:** A new deep learning model called IceMamba is introduced, which combines attention mechanisms with state space models to improve seasonal sea ice forecasts.


<details>
  <summary>Details</summary>
**Motivation:** The rapid decline of Arctic sea ice due to climate change poses risks to communities, ecosystems, and the global climate system, necessitating precise seasonal forecasts.

**Method:** IceMamba integrates attention mechanisms into a state space model for improved seasonal sea ice forecasting.

**Result:** IceMamba outperformed other models in terms of average RMSE and ACC, ranking second in IIEE.

**Conclusion:** This work demonstrates the potential of hybrid deep learning approaches in enhancing seasonal sea ice forecasts, aiding in climate adaptation strategies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Seasonal+Forecasting+of+Pan-Arctic+Sea+Ice+with+State+Space+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10665&send_immediately=true&force_search=false)

**Abstract:** The rapid decline of Arctic sea ice resulting from anthropogenic climate
change poses significant risks to indigenous communities, ecosystems, and the
global climate system. This situation emphasizes the immediate necessity for
precise seasonal sea ice forecasts. While dynamical models perform well for
short-term forecasts, they encounter limitations in long-term forecasts and are
computationally intensive. Deep learning models, while more computationally
efficient, often have difficulty managing seasonal variations and uncertainties
when dealing with complex sea ice dynamics. In this research, we introduce
IceMamba, a deep learning architecture that integrates sophisticated attention
mechanisms within the state space model. Through comparative analysis of 25
renowned forecast models, including dynamical, statistical, and deep learning
approaches, our experimental results indicate that IceMamba delivers excellent
seasonal forecasting capabilities for Pan-Arctic sea ice concentration.
Specifically, IceMamba outperforms all tested models regarding average RMSE and
anomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge
Error (IIEE). This innovative approach enhances our ability to foresee and
alleviate the effects of sea ice variability, offering essential insights for
strategies aimed at climate adaptation.

</details>


### [11] [On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating](https://arxiv.org/abs/2505.10860)
*Huy Nguyen, Thong T. Doan, Quang Pham, Nghi D. Q. Bui, Nhat Ho, Alessandro Rinaldo*

**Main category:** cs.LG

**TL;DR:** This paper provides a theoretical justification for the shared expert strategy and explores the normalized sigmoid gating mechanism in DeepSeekMoE.


<details>
  <summary>Details</summary>
**Motivation:** To theoretically justify the value of the shared expert strategy and explore the normalized sigmoid gating mechanism in DeepSeekMoE.

**Method:** The paper performs a convergence analysis of the expert estimation task and conducts experiments on synthetic and real-world datasets.

**Result:** The paper finds that the shared expert strategy and normalized sigmoid gating mechanism can improve sample efficiency. Experimental results also show that these features are beneficial.

**Conclusion:** The paper provides a comprehensive theoretical study of the shared expert strategy and normalized sigmoid gating mechanism in DeepSeekMoE. Theoretical analysis shows that these features can improve sample efficiency, and experimental results confirm these findings.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+DeepSeekMoE%3A+Statistical+Benefits+of+Shared+Experts+and+Normalized+Sigmoid+Gating，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10860&send_immediately=true&force_search=false)

**Abstract:** Mixture of experts (MoE) methods are a key component in most large language
model architectures, including the recent series of DeepSeek models. Compared
to other MoE implementations, DeepSeekMoE stands out because of two unique
features: the deployment of a shared expert strategy and of the normalized
sigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the
success of the DeepSeek series of models, there have been only a few attempts
to justify theoretically the value of the shared expert strategy, while its
normalized sigmoid gating has remained unexplored. To bridge this gap, we
undertake a comprehensive theoretical study of these two features of
DeepSeekMoE from a statistical perspective. We perform a convergence analysis
of the expert estimation task to highlight the gains in sample efficiency for
both the shared expert strategy and the normalized sigmoid gating, offering
useful insights into the design of expert and gating structures. To verify
empirically our theoretical findings, we carry out several experiments on both
synthetic data and real-world datasets for (vision) language modeling tasks.
Finally, we conduct an extensive empirical analysis of the router behaviors,
ranging from router saturation, router change rate, to expert utilization.

</details>


### [12] [A Conformal Predictive Measure for Assessing Catastrophic Forgetting](https://arxiv.org/abs/2505.10677)
*Ioannis Pitsiorlas, Nour Jamoussi, Marios Kountouris*

**Main category:** cs.LG

**TL;DR:** This paper proposes a new metric called CPCF based on conformal prediction to assess catastrophic forgetting in continual learning, demonstrating its reliability and applicability through experiments.


<details>
  <summary>Details</summary>
**Motivation:** To provide a more reliable and interpretable method for assessing catastrophic forgetting in continual learning environments.

**Method:** A novel conformal prediction-based metric named CPCF is used to monitor and measure forgetting by observing the model's confidence on prior tasks.

**Result:** The experimental results on four benchmark datasets proved that there is a significant correlation between CPCF and the accuracy of previous tasks, indicating the validity of the CPCF metric.

**Conclusion:** The proposed Conformal Prediction Confidence Factor (CPCF) metric has shown its effectiveness in quantifying and evaluating catastrophic forgetting in continual learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Conformal+Predictive+Measure+for+Assessing+Catastrophic+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10677&send_immediately=true&force_search=false)

**Abstract:** This work introduces a novel methodology for assessing catastrophic
forgetting (CF) in continual learning. We propose a new conformal prediction
(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to
quantify and evaluate CF effectively. Our framework leverages adaptive CP to
estimate forgetting by monitoring the model's confidence on previously learned
tasks. This approach provides a dynamic and practical solution for monitoring
and measuring CF of previous tasks as new ones are introduced, offering greater
suitability for real-world applications. Experimental results on four benchmark
datasets demonstrate a strong correlation between CPCF and the accuracy of
previous tasks, validating the reliability and interpretability of the proposed
metric. Our results highlight the potential of CPCF as a robust and effective
tool for assessing and understanding CF in dynamic learning environments.

</details>


### [13] [Hashing for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10873)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

**Main category:** cs.LG

**TL;DR:** This paper presents an efficient anomaly detection method based on Locality Sensitive Hashing in Preference Space.


<details>
  <summary>Details</summary>
**Motivation:** To identify samples not conforming to structured patterns represented by low-dimensional manifolds.

**Method:** Employing Locality Sensitive Hashing to improve Anomaly Detection efficiency in Preference Space.

**Result:** State-of-the-art performance with lower computational cost.

**Conclusion:** An effective way to solve the problem of identifying anomalies in high dimensional spaces.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hashing+for+Structure-based+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10873，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10873&send_immediately=true&force_search=false)

**Abstract:** We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.

</details>


### [14] [A probabilistic framework for dynamic quantization](https://arxiv.org/abs/2505.10689)
*Gabriele Santini, Francesco Paissan, Elisabetta Farella*

**Main category:** cs.LG

**TL;DR:** We introduce a probabilistic framework for dynamic quantization of neural networks that improves computational efficiency and maintains performance.


<details>
  <summary>Details</summary>
**Motivation:** To enable input-adaptive rescaling of quantization parameters with minimal memory overhead.

**Method:** Applying a probabilistic model to the network's pre-activations via a lightweight surrogate.

**Result:** Observed negligible loss in performance across various computer vision tasks and models.

**Conclusion:** Our approach offers an optimal tradeoff between performance and computational overhead compared to standard quantization strategies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+probabilistic+framework+for+dynamic+quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10689&send_immediately=true&force_search=false)

**Abstract:** We propose a probabilistic framework for dynamic quantization of neural
networks that allows for a computationally efficient input-adaptive rescaling
of the quantization parameters. Our framework applies a probabilistic model to
the network's pre-activations through a lightweight surrogate, enabling the
adaptive adjustment of the quantization parameters on a per-input basis without
significant memory overhead. We validate our approach on a set of popular
computer vision tasks and models, observing only a negligible loss in
performance. Our method strikes the best performance and computational overhead
tradeoff compared to standard quantization strategies.

</details>


### [15] [Preference Isolation Forest for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10876)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

**Main category:** cs.LG

**TL;DR:** This paper presents a new anomaly detection framework called Preference Isolation Forest (PIF) which uses preference embedding and isolation-based methods to detect anomalies.


<details>
  <summary>Details</summary>
**Motivation:** The paper aims to detect anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds.

**Method:** The proposed framework includes three isolation approaches: Voronoi-iForest, RuzHash-iForest, and Sliding-PIF.

**Result:** The proposed methods can effectively detect anomalies without explicit computation of distances via Local Sensitive Hashing.

**Conclusion:** The Preference Isolation Forest (PIF) framework combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding to detect anomalies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference+Isolation+Forest+for+Structure-based+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10876&send_immediately=true&force_search=false)

**Abstract:** We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.

</details>


### [16] [Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions](https://arxiv.org/abs/2505.10880)
*Guoji Fu, Wee Sun Lee*

**Main category:** cs.LG

**TL;DR:** This paper studies the approximation and generalization abilities of score-based neural network generative models (SGMs) in estimating an unknown distribution P_0.


<details>
  <summary>Details</summary>
**Motivation:** The paper aims to prove the ability of neural networks to approximate scores with a certain mean square error and achieve a nearly optimal rate for score estimation.

**Method:** The study uses deep ReLU neural networks with specific width and depth constraints to approximate scores.

**Result:** The results show that neural network-based SGMs can attain nearly minimax convergence rates up to logarithmic factors without crucial assumptions like Lipschitz continuity of the score function or a strictly positive lower bound on the target density.

**Conclusion:** The paper's framework is universal and can be used to establish convergence rates for SGMs under milder assumptions than previous work.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approximation+and+Generalization+Abilities+of+Score-based+Neural+Network+Generative+Models+for+Sub-Gaussian+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10880&send_immediately=true&force_search=false)

**Abstract:** This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists
a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq
O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$
mean square error and achieve a nearly optimal rate of
$\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score
matching loss. Our framework is universal and can be used to establish
convergence rates for SGMs under milder assumptions than previous work. For
example, assuming further that the target density function $p_0$ lies in
Sobolev or Besov classes, with an appropriately early stopping strategy, we
demonstrate that neural network-based SGMs can attain nearly minimax
convergence rates up to logarithmic factors. Our analysis removes several
crucial assumptions, such as Lipschitz continuity of the score function or a
strictly positive lower bound on the target density.

</details>


### [17] [Clustering Rooftop PV Systems via Probabilistic Embeddings](https://arxiv.org/abs/2505.10699)
*Kutay Bölat, Tarek Alskaif, Peter Palensky, Simon Tindemans*

**Main category:** cs.LG

**TL;DR:** A new clustering framework using probabilistic entity embeddings improves the management of large-scale rooftop PV time-series data with missing values.


<details>
  <summary>Details</summary>
**Motivation:** To monitor and analyze increasing numbers of rooftop photovoltaic installations, there is a need for effective integration and management of large, spatially distributed time-series data affected by missing values.

**Method:** A probabilistic entity embedding-based clustering framework that encodes power generation patterns and uncertainty as a probability distribution and groups systems by statistical distances and agglomerative clustering.

**Result:** The framework produces concise, uncertainty-aware cluster profiles that outperform a physics-based baseline in representativeness and robustness, and supports reliable missing-value imputation.

**Conclusion:** The proposed probabilistic entity embedding-based clustering framework effectively addresses the challenges of integrating and managing large, spatially distributed time-series data from rooftop photovoltaic systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clustering+Rooftop+PV+Systems+via+Probabilistic+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10699&send_immediately=true&force_search=false)

**Abstract:** As the number of rooftop photovoltaic (PV) installations increases,
aggregators and system operators are required to monitor and analyze these
systems, raising the challenge of integration and management of large,
spatially distributed time-series data that are both high-dimensional and
affected by missing values. In this work, a probabilistic entity
embedding-based clustering framework is proposed to address these problems.
This method encodes each PV system's characteristic power generation patterns
and uncertainty as a probability distribution, then groups systems by their
statistical distances and agglomerative clustering. Applied to a multi-year
residential PV dataset, it produces concise, uncertainty-aware cluster profiles
that outperform a physics-based baseline in representativeness and robustness,
and support reliable missing-value imputation. A systematic hyperparameter
study further offers practical guidance for balancing model performance and
robustness.

</details>


### [18] [Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation](https://arxiv.org/abs/2505.10882)
*Alex Saad-Falcon, Brighton Ancelin, Justin Romberg*

**Main category:** cs.LG

**TL;DR:** 提出一种基于压缩采样的自适应采样Oja算法变体，用于高效高维空间中的主成分分析，首次在噪声条件下提供了子空间追踪的收敛保证。


<details>
  <summary>Details</summary>
**Motivation:** 传统PCA方法计算成本高且随着数据维度的增长性能下降，而像Oja算法这样的子空间追踪算法虽然更高效，但通常需要全维观测。因此，研究一种能够在压缩采样下有效追踪主成分的算法。

**Method:** 分析了一种采用自适应采样的压缩采样Oja算法变体，该方法在每次迭代时仅获取两个压缩测量值，一个是在当前估计方向上的测量值，另一个是在随机正交方向上的测量值。

**Result:** 证明了所提出的自适应采样方法在全球噪声存在的情况下可以追踪数据流的主特征向量，并展示了算法经历两个阶段：初始阶段达到恒定水平的对齐，随后进入局部收敛阶段，在该阶段正弦对齐误差以特定速率衰减。

**Conclusion:** 提出的方法在自适应采样下提供了第一个关于子空间跟踪的收敛保证，并且证明了算法在有噪声情况下具有全局收敛性。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+Convergence+of+Adaptive+Sensing+for+Principal+Eigenvector+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10882&send_immediately=true&force_search=false)

**Abstract:** This paper addresses the challenge of efficient principal component analysis
(PCA) in high-dimensional spaces by analyzing a compressively sampled variant
of Oja's algorithm with adaptive sensing. Traditional PCA methods incur
substantial computational costs that scale poorly with data dimensionality,
whereas subspace tracking algorithms like Oja's offer more efficient
alternatives but typically require full-dimensional observations. We analyze a
variant where, at each iteration, only two compressed measurements are taken:
one in the direction of the current estimate and one in a random orthogonal
direction. We prove that this adaptive sensing approach achieves global
convergence in the presence of noise when tracking the leading eigenvector of a
datastream with eigengap $\Delta=\lambda_1-\lambda_2$. Our theoretical analysis
demonstrates that the algorithm experiences two phases: (1) a warmup phase
requiring $O(\lambda_1\lambda_2d^2/\Delta^2)$ iterations to achieve a
constant-level alignment with the true eigenvector, followed by (2) a local
convergence phase where the sine alignment error decays at a rate of
$O(\lambda_1\lambda_2d^2/\Delta^2 t)$ for iterations $t$. The guarantee aligns
with existing minimax lower bounds with an added factor of $d$ due to the
compressive sampling. This work provides the first convergence guarantees in
adaptive sensing for subspace tracking with noise. Our proof technique is also
considerably simpler than those in prior works. The results have important
implications for applications where acquiring full-dimensional samples is
challenging or costly.

</details>


### [19] [ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data](https://arxiv.org/abs/2505.10704)
*Patryk Marszałek, Tomasz Kuśmierczyk, Witold Wydmański, Jacek Tabor, Marek Śmieja*

**Main category:** cs.LG

**TL;DR:** ZEUS is a self-contained deep learning model for clustering tabular data without per-dataset tuning, leveraging zero-shot learning and pre-trained on synthetic datasets.


<details>
  <summary>Details</summary>
**Motivation:** Clustering tabular data is challenging due to dataset-dependent similarity and lack of supervised signals, leading to unstable performance in existing methods.

**Method:** Proposes ZEUS which decomposes datasets into meaningful components and generates embeddings in an unsupervised way using pre-training on synthetic datasets.

**Result:** Performs comparably or better than traditional and other deep learning-based clustering methods while being faster and more user-friendly.

**Conclusion:** ZEUS is the first zero-shot method for fully unsupervised tabular data embedding, addressing the challenges in clustering tabular data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZEUS%3A+Zero-shot+Embeddings+for+Unsupervised+Separation+of+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10704&send_immediately=true&force_search=false)

**Abstract:** Clustering tabular data remains a significant open challenge in data analysis
and machine learning. Unlike for image data, similarity between tabular records
often varies across datasets, making the definition of clusters highly
dataset-dependent. Furthermore, the absence of supervised signals complicates
hyperparameter tuning in deep learning clustering methods, frequently resulting
in unstable performance. To address these issues and reduce the need for
per-dataset tuning, we adopt an emerging approach in deep learning: zero-shot
learning. We propose ZEUS, a self-contained model capable of clustering new
datasets without any additional training or fine-tuning. It operates by
decomposing complex datasets into meaningful components that can then be
clustered effectively. Thanks to pre-training on synthetic datasets generated
from a latent-variable prior, it generalizes across various datasets without
requiring user intervention. To the best of our knowledge, ZEUS is the first
zero-shot method capable of generating embeddings for tabular data in a fully
unsupervised manner. Experimental results demonstrate that it performs on par
with or better than traditional clustering algorithms and recent deep
learning-based methods, while being significantly faster and more
user-friendly.

</details>


### [20] [NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification](https://arxiv.org/abs/2505.11054)
*Mélodie Monod, Alessandro Micheli, Samir Bhatt*

**Main category:** cs.LG

**TL;DR:** Introducing NeuralSurv, the first deep survival model that incorporates Bayesian uncertainty quantification, it shows improved calibration and uncertainty estimation while maintaining or enhancing discriminative performance.


<details>
  <summary>Details</summary>
**Motivation:** To incorporate Bayesian uncertainty quantification into deep survival models.

**Method:** A new deep survival model called NeuralSurv is introduced. It uses a two-stage data-augmentation scheme and mean-field variational algorithm with coordinate-ascent updates.

**Result:** NeuralSurv surpasses state-of-the-art models in calibration and matches or exceeds their discriminative performance in both synthetic benchmarks and real-world datasets.

**Conclusion:** NeuralSurv provides better calibration and uncertainty estimation than current leading deep survival models, while maintaining or improving discriminative performance.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuralSurv%3A+Deep+Survival+Analysis+with+Bayesian+Uncertainty+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11054&send_immediately=true&force_search=false)

**Abstract:** We introduce NeuralSurv, the first deep survival model to incorporate
Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic
framework flexibly captures time-varying covariate-risk relationships in
continuous time via a novel two-stage data-augmentation scheme, for which we
establish theoretical guarantees. For efficient posterior inference, we
introduce a mean-field variational algorithm with coordinate-ascent updates
that scale linearly in model size. By locally linearizing the Bayesian neural
network, we obtain full conjugacy and derive all coordinate updates in closed
form. In experiments, NeuralSurv delivers superior calibration compared to
state-of-the-art deep survival models, while matching or exceeding their
discriminative performance across both synthetic benchmarks and real-world
datasets. Our results demonstrate the value of Bayesian principles in
data-scarce regimes by enhancing model calibration and providing robust,
well-calibrated uncertainty estimates for the survival function.

</details>


### [21] [GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics](https://arxiv.org/abs/2505.10711)
*Sebestyén Kamp, Giovanni Stracquadanio, T. Ian Simpson*

**Main category:** cs.LG

**TL;DR:** A framework named GNN-Suite is introduced to benchmark Graph Neural Network (GNN) architectures in computational biology. It standardizes the evaluation process and demonstrates its effectiveness in identifying cancer-driver genes.


<details>
  <summary>Details</summary>
**Motivation:** To provide a standardized framework for evaluating GNN architectures and promoting reproducible research in computational biology.

**Method:** GNN-Suite uses Nextflow workflow to standardize experimentation and reproducibility. It constructs molecular networks from PPI data and annotates nodes with features from various repositories. The framework compares diverse GNN architectures with a baseline Logistic Regression model using balanced accuracy as the primary measure.

**Result:** GCN2 achieved the highest balanced accuracy (0.807 ± 0.035) on a STRING-based network. All GNN types outperformed the LR baseline, showing the advantage of network-based learning over feature-only approaches.

**Conclusion:** The study highlights the importance of a common framework for implementing and evaluating GNN architectures in computational biology. By making GNN-Suite publicly available, it aims to promote reproducible research and improved benchmarking standards.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GNN-Suite%3A+a+Graph+Neural+Network+Benchmarking+Framework+for+Biomedical+Informatics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10711，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10711&send_immediately=true&force_search=false)

**Abstract:** We present GNN-Suite, a robust modular framework for constructing and
benchmarking Graph Neural Network (GNN) architectures in computational biology.
GNN-Suite standardises experimentation and reproducibility using the Nextflow
workflow to evaluate GNN performance. We demonstrate its utility in identifying
cancer-driver genes by constructing molecular networks from protein-protein
interaction (PPI) data from STRING and BioGRID and annotating nodes with
features from the PCAWG, PID, and COSMIC-CGC repositories.
  Our design enables fair comparisons among diverse GNN architectures including
GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline
Logistic Regression (LR) model. All GNNs were configured as standardised
two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam
optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss
to address class imbalance) over an 80/20 train-test split for 300 epochs. Each
model was evaluated over 10 independent runs with different random seeds to
yield statistically robust performance metrics, with balanced accuracy (BACC)
as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-
0.035) on a STRING-based network, although all GNN types outperformed the LR
baseline, highlighting the advantage of network-based learning over
feature-only approaches.
  Our results show that a common framework for implementing and evaluating GNN
architectures aids in identifying not only the best model but also the most
effective means of incorporating complementary data. By making GNN-Suite
publicly available, we aim to foster reproducible research and promote improved
benchmarking standards in computational biology. Future work will explore
additional omics datasets and further refine network architectures to enhance
predictive accuracy and interpretability in biomedical applications.

</details>


### [22] [A Fast Kernel-based Conditional Independence test with Application to Causal Discovery](https://arxiv.org/abs/2505.11085)
*Oliver Schacht, Biwei Huang*

**Main category:** cs.LG

**TL;DR:** FastKCI, a scalable and parallelizable kernel-based conditional independence test that improves efficiency for large datasets.


<details>
  <summary>Details</summary>
**Motivation:** Cubic computational complexity of kernel-based conditional independence testing limits its application to large datasets.

**Method:** Partitioning the dataset and conducting local KCI tests in parallel using a mixture-of-experts approach.

**Result:** FastKCI maintains statistical power while achieving computational speedups.

**Conclusion:** FastKCI is a practical and efficient solution for conditional independence testing in causal inference on large-scale data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fast+Kernel-based+Conditional+Independence+test+with+Application+to+Causal+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11085&send_immediately=true&force_search=false)

**Abstract:** Kernel-based conditional independence (KCI) testing is a powerful
nonparametric method commonly employed in causal discovery tasks. Despite its
flexibility and statistical reliability, cubic computational complexity limits
its application to large datasets. To address this computational bottleneck, we
propose \textit{FastKCI}, a scalable and parallelizable kernel-based
conditional independence test that utilizes a mixture-of-experts approach
inspired by embarrassingly parallel inference techniques for Gaussian
processes. By partitioning the dataset based on a Gaussian mixture model over
the conditioning variables, FastKCI conducts local KCI tests in parallel,
aggregating the results using an importance-weighted sampling scheme.
Experiments on synthetic datasets and benchmarks on real-world production data
validate that FastKCI maintains the statistical power of the original KCI test
while achieving substantial computational speedups. FastKCI thus represents a
practical and efficient solution for conditional independence testing in causal
inference on large-scale data.

</details>


### [23] [Learning Repetition-Invariant Representations for Polymer Informatics](https://arxiv.org/abs/2505.10726)
*Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang*

**Main category:** cs.LG

**TL;DR:** This paper introduces GRIN, a novel method to learn polymer representations invariant to the number of repeating units.


<details>
  <summary>Details</summary>
**Motivation:** Existing graph neural network methods fail to produce consistent vector representations for true polymer structures with varying numbers of units.

**Method:** GRIN integrates graph-based maximum spanning tree alignment with repeat-unit augmentation.

**Result:** GRIN outperforms state-of-the-art baselines on both homopolymer and copolymer benchmarks.

**Conclusion:** GRIN learns stable, repetition-invariant representations that generalize effectively to polymer chains of unseen sizes.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Repetition-Invariant+Representations+for+Polymer+Informatics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10726&send_immediately=true&force_search=false)

**Abstract:** Polymers are large macromolecules composed of repeating structural units
known as monomers and are widely applied in fields such as energy storage,
construction, medicine, and aerospace. However, existing graph neural network
methods, though effective for small molecules, only model the single unit of
polymers and fail to produce consistent vector representations for the true
polymer structure with varying numbers of units. To address this challenge, we
introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer
representations that are invariant to the number of repeating units in their
graph representations. GRIN integrates a graph-based maximum spanning tree
alignment with repeat-unit augmentation to ensure structural consistency. We
provide theoretical guarantees for repetition-invariance from both model and
data perspectives, demonstrating that three repeating units are the minimal
augmentation required for optimal invariant representation learning. GRIN
outperforms state-of-the-art baselines on both homopolymer and copolymer
benchmarks, learning stable, repetition-invariant representations that
generalize effectively to polymer chains of unseen sizes.

</details>


### [24] [FedDuA: Doubly Adaptive Federated Learning](https://arxiv.org/abs/2505.11126)
*Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa*

**Main category:** cs.LG

**TL;DR:** Federated learning is a distributed learning framework where clients collaboratively train a global model without sharing their raw data. This paper proposes a novel framework called FedDuA, which adaptively selects the global learning rate and outperforms baselines in various settings.


<details>
  <summary>Details</summary>
**Motivation:** FedAvg often suffers from slow convergence due to the heterogeneity of local datasets and anisotropy in the parameter space.

**Method:** Formalizing the central server optimization procedure through the lens of mirror descent and proposing a novel framework called FedDuA, which adaptively selects the global learning rate based on both inter-client and coordinate-wise heterogeneity in the local updates.

**Result:** The proposed method proves that the doubly adaptive step-size rule is minimax optimal and provides a convergence analysis for convex objectives. It outperforms baselines in various settings and is robust to the choice of hyperparameters.

**Conclusion:** The proposed FedDuA framework adaptively selects the global learning rate and shows superior performance compared to existing methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedDuA%3A+Doubly+Adaptive+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11126&send_immediately=true&force_search=false)

**Abstract:** Federated learning is a distributed learning framework where clients
collaboratively train a global model without sharing their raw data. FedAvg is
a popular algorithm for federated learning, but it often suffers from slow
convergence due to the heterogeneity of local datasets and anisotropy in the
parameter space. In this work, we formalize the central server optimization
procedure through the lens of mirror descent and propose a novel framework,
called FedDuA, which adaptively selects the global learning rate based on both
inter-client and coordinate-wise heterogeneity in the local updates. We prove
that our proposed doubly adaptive step-size rule is minimax optimal and provide
a convergence analysis for convex objectives. Although the proposed method does
not require additional communication or computational cost on clients,
extensive numerical experiments show that our proposed framework outperforms
baselines in various settings and is robust to the choice of hyperparameters.

</details>


### [25] [Random Client Selection on Contrastive Federated Learning for Tabular Data](https://arxiv.org/abs/2505.10759)
*Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua*

**Main category:** cs.LG

**TL;DR:** This paper examines gradient-based attacks in CFL environments and finds that random client selection is an effective defense mechanism.


<details>
  <summary>Details</summary>
**Motivation:** To address information leakage vulnerabilities in VFL and enhance privacy-preserving model training.

**Method:** Conducting comprehensive experiments on gradient-based attacks and evaluating random client selection as a defensive strategy.

**Result:** Random client selection proves effective in defending against gradient attacks in the CFL network.

**Conclusion:** Our study contributes to the development of more secure collaborative learning frameworks by providing insights into robust security measures.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Random+Client+Selection+on+Contrastive+Federated+Learning+for+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10759&send_immediately=true&force_search=false)

**Abstract:** Vertical Federated Learning (VFL) has revolutionised collaborative machine
learning by enabling privacy-preserving model training across multiple parties.
However, it remains vulnerable to information leakage during intermediate
computation sharing. While Contrastive Federated Learning (CFL) was introduced
to mitigate these privacy concerns through representation learning, it still
faces challenges from gradient-based attacks. This paper presents a
comprehensive experimental analysis of gradient-based attacks in CFL
environments and evaluates random client selection as a defensive strategy.
Through extensive experimentation, we demonstrate that random client selection
proves particularly effective in defending against gradient attacks in the CFL
network. Our findings provide valuable insights for implementing robust
security measures in contrastive federated learning systems, contributing to
the development of more secure collaborative learning frameworks

</details>


### [26] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/abs/2505.11210)
*Anders Gjølbye, Stefan Haufe, Lars Kai Hansen*

**Main category:** cs.LG

**TL;DR:** PatternLocal is a new XAI technique that reduces false-positive attributions caused by suppressor variables in nonlinear models.


<details>
  <summary>Details</summary>
**Motivation:** Suppressor variables create challenges for XAI methods by causing false-positive feature attributions, especially in nonlinear models.

**Method:** Transforms locally linear surrogate model weights into a generative representation to suppress suppressor variables' influence while maintaining local fidelity.

**Result:** Outperforms other XAI methods on the XAI-TRIS benchmark in reducing false-positive attributions for nonlinear tasks.

**Conclusion:** PatternLocal provides more reliable and actionable insights by addressing the suppressor variable issue in nonlinear XAI.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimizing+False-Positive+Attributions+in+Explanations+of+Non-Linear+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11210&send_immediately=true&force_search=false)

**Abstract:** Suppressor variables can influence model predictions without being dependent
on the target outcome and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and to instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the
resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights.

</details>


### [27] [Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics](https://arxiv.org/abs/2505.10762)
*Conor F. Hayes, Felipe Leno Da Silva, Jiachen Yang, T. Nathan Mundhenk, Chak Shing Lee, Jacob F. Pettit, Claudio Santiago, Sookyung Kim, Joanne T. Kim, Ignacio Aravena Solis, Ruben Glatt, Andre R. Goncalves, Alexander Ladd, Ahmet Can Solak, Thomas Desautels, Daniel Faissol, Brenden K. Petersen, Mikel Landajuela*

**Main category:** cs.LG

**TL;DR:** A new computational framework named Deep Symbolic Optimization (DSO) combines neural networks and reinforcement learning to perform symbolic optimization tasks like equation discovery.


<details>
  <summary>Details</summary>
**Motivation:** To automate the discovery of interpretable and physically meaningful mathematical models using symbolic expressions.

**Method:** Formulating the discovery process as a sequential decision-making task, using a generative neural network and reinforcement learning strategies, integrating gradient-based optimization with evolutionary and local search techniques, and incorporating constraints and priors.

**Result:** Achieved state-of-the-art performance in accuracy and interpretability on benchmark problems.

**Conclusion:** DSO demonstrates the potential to revolutionize automated symbolic optimization in scientific discovery.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Symbolic+Optimization%3A+Reinforcement+Learning+for+Symbolic+Mathematics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10762&send_immediately=true&force_search=false)

**Abstract:** Deep Symbolic Optimization (DSO) is a novel computational framework that
enables symbolic optimization for scientific discovery, particularly in
applications involving the search for intricate symbolic structures. One
notable example is equation discovery, which aims to automatically derive
mathematical models expressed in symbolic form. In DSO, the discovery process
is formulated as a sequential decision-making task. A generative neural network
learns a probabilistic model over a vast space of candidate symbolic
expressions, while reinforcement learning strategies guide the search toward
the most promising regions. This approach integrates gradient-based
optimization with evolutionary and local search techniques, and it incorporates
in-situ constraints, domain-specific priors, and advanced policy optimization
methods. The result is a robust framework capable of efficiently exploring
extensive search spaces to identify interpretable and physically meaningful
models. Extensive evaluations on benchmark problems have demonstrated that DSO
achieves state-of-the-art performance in both accuracy and interpretability. In
this chapter, we provide a comprehensive overview of the DSO framework and
illustrate its transformative potential for automating symbolic optimization in
scientific discovery.

</details>


### [28] [Bayesian Hierarchical Invariant Prediction](https://arxiv.org/abs/2505.11211)
*Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia*

**Main category:** cs.LG

**TL;DR:** Proposed Bayesian Hierarchical Invariant Prediction (BHIP) to improve computational scalability and reliable identification of causal features.


<details>
  <summary>Details</summary>
**Motivation:** To reframe Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes and leverage hierarchical structure to test invariance of causal mechanisms.

**Method:** Bayesian Hierarchical Invariant Prediction (BHIP)

**Result:** Improved computational scalability and reliable identification of causal features.

**Conclusion:** BHIP improves computational scalability and allows reliable identification of causal features.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Hierarchical+Invariant+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11211&send_immediately=true&force_search=false)

**Abstract:** We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing
Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We
leverage the hierarchical structure to explicitly test invariance of causal
mechanisms under heterogeneous data, resulting in improved computational
scalability for a larger number of predictors compared to ICP. Moreover, given
its Bayesian nature BHIP enables the use of prior information. In this paper,
we test two sparsity inducing priors: horseshoe and spike-and-slab, both of
which allow us a more reliable identification of causal features. We test BHIP
in synthetic and real-world data showing its potential as an alternative
inference method to ICP.

</details>


### [29] [Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.10774)
*Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv*

**Main category:** cs.LG

**TL;DR:** A new method called CAPTime is proposed for time series forecasting that combines text and numerical data effectively by using a pretrained time series encoder and frozen LLMs.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods struggle to integrate exogenous texts and align them with the probabilistic nature of large language models (LLMs), limiting contextual awareness and distribution modeling.

**Method:** CAPTime uses a pretrained time series encoder to encode temporal patterns and aligns them with textual contexts via learnable interactions to produce joint multimodal representations. It combines a mixture of distribution experts with frozen LLMs for context-aware probabilistic forecasting.

**Result:** Experiments show that CAPTime outperforms other methods in terms of accuracy and generalization, especially in multimodal scenarios.

**Conclusion:** CAPTime addresses limitations of current approaches by providing a context-aware probabilistic multimodal time series forecasting method that leverages text-informed abstraction and autoregressive LLM decoding.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Aware+Probabilistic+Modeling+with+LLM+for+Multimodal+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10774&send_immediately=true&force_search=false)

**Abstract:** Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.

</details>


### [30] [A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation](https://arxiv.org/abs/2505.11444)
*Xinran Song, Tianyu Chen, Mingyuan Zhou*

**Main category:** cs.LG

**TL;DR:** IWDD is a novel generative framework that combines diffusion models and importance-weighted score distillation for accurate and fast causal estimation.


<details>
  <summary>Details</summary>
**Motivation:** Estimating individualized treatment effects from observational data is a central challenge in causal inference, largely due to covariate imbalance and confounding bias from non-randomized treatment assignment.

**Method:** Importance-Weighted Diffusion Distillation (IWDD), which combines the pretraining of diffusion models with importance-weighted score distillation.

**Result:** Empirical results show that IWDD achieves state-of-the-art out-of-sample prediction performance, with the highest win rates compared to other baselines.

**Conclusion:** IWDD achieves state-of-the-art out-of-sample prediction performance, significantly improving causal estimation and supporting the development of individualized treatment strategies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Generative+Framework+for+Causal+Estimation+via+Importance-Weighted+Diffusion+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11444&send_immediately=true&force_search=false)

**Abstract:** Estimating individualized treatment effects from observational data is a
central challenge in causal inference, largely due to covariate imbalance and
confounding bias from non-randomized treatment assignment. While inverse
probability weighting (IPW) is a well-established solution to this problem, its
integration into modern deep learning frameworks remains limited. In this work,
we propose Importance-Weighted Diffusion Distillation (IWDD), a novel
generative framework that combines the pretraining of diffusion models with
importance-weighted score distillation to enable accurate and fast causal
estimation-including potential outcome prediction and treatment effect
estimation. We demonstrate how IPW can be naturally incorporated into the
distillation of pretrained diffusion models, and further introduce a
randomization-based adjustment that eliminates the need to compute IPW
explicitly-thereby simplifying computation and, more importantly, provably
reducing the variance of gradient estimates. Empirical results show that IWDD
achieves state-of-the-art out-of-sample prediction performance, with the
highest win rates compared to other baselines, significantly improving causal
estimation and supporting the development of individualized treatment
strategies. We will release our PyTorch code for reproducibility and future
research.

</details>


### [31] [Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning](https://arxiv.org/abs/2505.10799)
*Tao Bai, Junzhuo Zhou, Zeyuan Deng, Peng Cao*

**Main category:** cs.LG

**TL;DR:** A novel Gaussian Process Regression model with active learning is introduced to efficiently and accurately characterize the composite current source model.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenges of high accuracy requirement, large amount of data and extensive simulation cost in CCS characterization.

**Method:** Gaussian Process Regression with active learning

**Result:** The approach achieves an average absolute error of 2.05 ps and a relative error of 2.27% for current waveform of 57 cells under 9 PVT corners with TSMC 22nm process. It also reduces the runtime to 27% and the storage by up to 19.5x compared with commercial tools.

**Conclusion:** The proposed GPR model with active learning is effective in characterizing the composite current source model with high accuracy and efficiency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cell+Library+Characterization+for+Composite+Current+Source+Models+Based+on+Gaussian+Process+Regression+and+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10799&send_immediately=true&force_search=false)

**Abstract:** The composite current source (CCS) model has been adopted as an advanced
timing model that represents the current behavior of cells for improved
accuracy and better capability than traditional non-linear delay models (NLDM)
to model complex dynamic effects and interactions under advanced process nodes.
However, the high accuracy requirement, large amount of data and extensive
simulation cost pose severe challenges to CCS characterization. To address
these challenges, we introduce a novel Gaussian Process Regression(GPR) model
with active learning(AL) to establish the characterization framework
efficiently and accurately. Our approach significantly outperforms conventional
commercial tools as well as learning based approaches by achieving an average
absolute error of 2.05 ps and a relative error of 2.27% for current waveform of
57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm
process. Additionally, our model drastically reduces the runtime to 27% and the
storage by up to 19.5x compared with that required by commercial tools.

</details>


### [32] [Attention-Based Reward Shaping for Sparse and Delayed Rewards](https://arxiv.org/abs/2505.10802)
*Ian Holmes, Min Chi*

**Main category:** cs.LG

**TL;DR:** A new method called ARES uses transformers to create dense reward functions from sparse data, improving learning in delayed reward environments.


<details>
  <summary>Details</summary>
**Motivation:** Improving reinforcement learning in real-world applications with sparse and delayed rewards.

**Method:** Using a transformer's attention mechanism to shape rewards and create dense reward functions.

**Result:** Significantly improved learning in delayed reward settings, even with small datasets or random agent actions.

**Conclusion:** ARES is the first fully offline method that handles extreme reward delays and low-quality data, and is not limited to goal-based tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention-Based+Reward+Shaping+for+Sparse+and+Delayed+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10802&send_immediately=true&force_search=false)

**Abstract:** Sparse and delayed reward functions pose a significant obstacle for
real-world Reinforcement Learning (RL) applications. In this work, we propose
Attention-based REward Shaping (ARES), a general and robust algorithm which
uses a transformer's attention mechanism to generate shaped rewards and create
a dense reward function for any environment. ARES requires a set of episodes
and their final returns as input. It can be trained entirely offline and is
able to generate meaningful shaped rewards even when using small datasets or
episodes produced by agents taking random actions. ARES is compatible with any
RL algorithm and can handle any level of reward sparsity. In our experiments,
we focus on the most challenging case where rewards are fully delayed until the
end of each episode. We evaluate ARES across a diverse range of environments,
widely used RL algorithms, and baseline methods to assess the effectiveness of
the shaped rewards it produces. Our results show that ARES can significantly
improve learning in delayed reward settings, enabling RL agents to train in
scenarios that would otherwise require impractical amounts of data or even be
unlearnable. To our knowledge, ARES is the first approach that works fully
offline, remains robust to extreme reward delays and low-quality data, and is
not limited to goal-based tasks.

</details>


### [33] [Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation](https://arxiv.org/abs/2505.10822)
*Reilly Haskins, Benjamin Adams*

**Main category:** cs.LG

**TL;DR:** This study examines knowledge distillation in neural models, specifically comparing GPT-2 small and its distilled version, DistilGPT2. It finds students reorganize and compress teacher components, often relying more on fewer components.


<details>
  <summary>Details</summary>
**Motivation:** To understand the internal computational transformations that occur during knowledge distillation.

**Method:** Applying mechanistic interpretability techniques to analyze internal circuits, representations, and activation patterns between teacher and student models.

**Result:** Student models reorganize, compress, and discard teacher components, showing stronger reliance on fewer components.

**Conclusion:** Knowledge distillation preserves broad functional behaviors but causes significant shifts in internal computation affecting robustness and generalization capacity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilled+Circuits%3A+A+Mechanistic+Study+of+Internal+Restructuring+in+Knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10822&send_immediately=true&force_search=false)

**Abstract:** Knowledge distillation compresses a larger neural model (teacher) into
smaller, faster student models by training them to match teacher outputs.
However, the internal computational transformations that occur during this
process remain poorly understood. We apply techniques from mechanistic
interpretability to analyze how internal circuits, representations, and
activation patterns differ between teacher and student. Focusing on GPT2-small
and its distilled counterpart DistilGPT2, we find that student models
reorganize, compress, and discard teacher components, often resulting in
stronger reliance on fewer individual components. To quantify functional
alignment beyond output similarity, we introduce an alignment metric based on
influence-weighted component similarity, validated across multiple tasks. Our
findings reveal that while knowledge distillation preserves broad functional
behaviors, it also causes significant shifts in internal computation, with
important implications for the robustness and generalization capacity of
distilled models.

</details>


### [34] [MergeBench: A Benchmark for Merging Domain-Specialized LLMs](https://arxiv.org/abs/2505.10833)
*Yifei He, Siqi Zeng, Yuzheng Hu, Rui Yang, Tong Zhang, Han Zhao*

**Main category:** cs.LG

**TL;DR:** 评估模型合并方法在大规模领域专用LLMs上的适用性。


<details>
  <summary>Details</summary>
**Motivation:** 现有模型合并评估局限于小规模和任务多样性，对大模型的适用性存疑。

**Method:** 引入MergeBench评估套件，基于Llama和Gemma家族模型，涵盖五大领域，评估八种合并方法。

**Result:** 模型合并性能随基础模型强度提高而改善，调整合并系数和稀疏化技术可提升知识保留；但存在计算成本高、域内性能与多任务模型差距等问题。

**Conclusion:** 希望MergeBench能促进模型合并的理解和实际应用，并开源代码。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MergeBench%3A+A+Benchmark+for+Merging+Domain-Specialized+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10833&send_immediately=true&force_search=false)

**Abstract:** Model merging provides a scalable alternative to multi-task training by
combining specialized finetuned models through parameter arithmetic, enabling
efficient deployment without the need for joint training or access to all task
data. While recent methods have shown promise, existing evaluations are limited
in both model scale and task diversity, leaving open questions about their
applicability to large, domain-specialized LLMs. To tackle the challenges, we
introduce MergeBench, a comprehensive evaluation suite designed to assess model
merging at scale. MergeBench builds on state-of-the-art open-source language
models, including Llama and Gemma families at 2B to 9B scales, and covers five
key domains: instruction following, mathematics, multilingual understanding,
coding and safety. We standardize finetuning and evaluation protocols, and
assess eight representative merging methods across multi-task performance,
forgetting and runtime efficiency. Based on extensive experiments, we provide
practical guidelines for algorithm selection and share insights showing that
model merging tends to perform better on stronger base models, with techniques
such as merging coefficient tuning and sparsification improving knowledge
retention. However, several challenges remain, including the computational cost
on large models, the gap for in-domain performance compared to multi-task
models, and the underexplored role of model merging in standard LLM training
pipelines. We hope MergeBench provides a foundation for future research to
advance the understanding and practical application of model merging. We open
source our code at
\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.

</details>


### [35] [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
*Ran Li, Hao Wang, Chengzhi Mao*

**Main category:** cs.LG

**TL;DR:** We present LARGO, a new method that uses gradient optimization to create effective jailbreaking prompts for large language models.


<details>
  <summary>Details</summary>
**Motivation:** Efficient red-teaming method to uncover vulnerabilities in Large Language Models (LLMs) is crucial.

**Method:** introduce LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel latent self-reflection attack

**Result:** LARGO surpasses leading jailbreaking techniques, including AutoDAN, by 44 points in attack success rate.

**Conclusion:** Our findings demonstrate a potent alternative to agentic LLM prompting, highlighting the efficacy of interpreting and attacking LLM internals through gradient optimization.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LARGO%3A+Latent+Adversarial+Reflection+through+Gradient+Optimization+for+Jailbreaking+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10838&send_immediately=true&force_search=false)

**Abstract:** Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.

</details>


### [36] [Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness](https://arxiv.org/abs/2505.10845)
*Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam*

**Main category:** cs.LG

**TL;DR:** Ready2Unlearn prepares machine learning models for future unlearning requests through proactive training during the learning phase.


<details>
  <summary>Details</summary>
**Motivation:** Existing unlearning efforts typically focus on designing unlearning algorithms implemented reactively during the model deployment phase.

**Method:** Adopting a forward-looking perspective, Ready2Unlearn proactively trains machine learning models based on meta-learning principles.

**Result:** Ready2Unlearn improves unlearning efficiency, retains overall model capability, and prevents inadvertent recovery of forgotten data.

**Conclusion:** Ready2Unlearn, a learning-time optimization approach, prepares machine learning models for future unlearning requests more efficiently and effectively.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ready2Unlearn%3A+A+Learning-Time+Approach+for+Preparing+Models+with+Future+Unlearning+Readiness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10845&send_immediately=true&force_search=false)

**Abstract:** This paper introduces Ready2Unlearn, a learning-time optimization approach
designed to facilitate future unlearning processes. Unlike the majority of
existing unlearning efforts that focus on designing unlearning algorithms,
which are typically implemented reactively when an unlearning request is made
during the model deployment phase, Ready2Unlearn shifts the focus to the
training phase, adopting a "forward-looking" perspective. Building upon
well-established meta-learning principles, Ready2Unlearn proactively trains
machine learning models with unlearning readiness, such that they are well
prepared and can handle future unlearning requests in a more efficient and
principled manner. Ready2Unlearn is model-agnostic and compatible with any
gradient ascent-based machine unlearning algorithms. We evaluate the method on
both vision and language tasks under various unlearning settings, including
class-wise unlearning and random data unlearning. Experimental results show
that by incorporating such preparedness at training time, Ready2Unlearn
produces an unlearning-ready model state, which offers several key advantages
when future unlearning is required, including reduced unlearning time, improved
retention of overall model capability, and enhanced resistance to the
inadvertent recovery of forgotten data. We hope this work could inspire future
efforts to explore more proactive strategies for equipping machine learning
models with built-in readiness towards more reliable and principled machine
unlearning.

</details>


### [37] [AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models](https://arxiv.org/abs/2505.10846)
*Jiacheng Liang, Tanqiu Jiang, Yuhui Wang, Rongyi Zhu, Fenglong Ma, Ting Wang*

**Main category:** cs.LG

**TL;DR:** AutoRAN is an innovative framework that uses weaker models to break through stronger ones, highlighting security flaws in large reasoning models.


<details>
  <summary>Details</summary>
**Motivation:** To develop the first automated, weak-to-strong jailbreak attack framework for large reasoning models (LRMs) to expose potential vulnerabilities.

**Method:** AutoRAN simulates the target model's high-level reasoning structures using a weak model, generates narrative prompts, and refines them using the target model's intermediate reasoning steps.

**Result:** AutoRAN achieved nearly 100% success rates in bypassing various LRMs within one or a few iterations, even when evaluated by an external robustly aligned model.

**Conclusion:** AutoRAN demonstrates that weak reasoning models can exploit vulnerabilities in stronger ones, emphasizing the necessity for enhanced safety mechanisms in reasoning-based models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoRAN%3A+Weak-to-Strong+Jailbreaking+of+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10846&send_immediately=true&force_search=false)

**Abstract:** This paper presents AutoRAN, the first automated, weak-to-strong jailbreak
attack framework targeting large reasoning models (LRMs). At its core, AutoRAN
leverages a weak, less-aligned reasoning model to simulate the target model's
high-level reasoning structures, generates narrative prompts, and iteratively
refines candidate prompts by incorporating the target model's intermediate
reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including
GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets
(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN
achieves remarkable success rates (approaching 100%) within one or a few turns
across different LRMs, even when judged by a robustly aligned external model.
This work reveals that leveraging weak reasoning models can effectively exploit
the critical vulnerabilities of much more capable reasoning models,
highlighting the need for improved safety measures specifically designed for
reasoning-based models. The code for replicating AutoRAN and running records
are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:
this paper contains potentially harmful content generated by LRMs.)

</details>


### [38] [Foundation model for mass spectrometry proteomics](https://arxiv.org/abs/2505.10848)
*Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble*

**Main category:** cs.LG

**TL;DR:** This paper proposes a unified foundation model for mass spectrometry data analysis, which uses pre-trained spectrum representations to improve performance on four downstream tasks and shows enhanced data acquisition and analysis capabilities.


<details>
  <summary>Details</summary>
**Motivation:** To address the complexity of mass spectrometry data and improve the performance of various spectrum prediction tasks.

**Method:** Pre-trains a spectrum encoder using de novo sequencing as a pre-training task and performs multi-task fine-tuning.

**Result:** Improved performance on four downstream tasks: spectrum quality prediction, chimericity prediction, phosphorylation prediction, and glycosylation status prediction.

**Conclusion:** A foundation model for tandem mass spectrometry proteomics trained on de novo sequencing can learn generalizable representations of spectra and enhance data acquisition and analysis in proteomics experiments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+model+for+mass+spectrometry+proteomics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10848&send_immediately=true&force_search=false)

**Abstract:** Mass spectrometry is the dominant technology in the field of proteomics,
enabling high-throughput analysis of the protein content of complex biological
samples. Due to the complexity of the instrumentation and resulting data,
sophisticated computational methods are required for the processing and
interpretation of acquired mass spectra. Machine learning has shown great
promise to improve the analysis of mass spectrometry data, with numerous
purpose-built methods for improving specific steps in the data acquisition and
analysis pipeline reaching widespread adoption. Here, we propose unifying
various spectrum prediction tasks under a single foundation model for mass
spectra. To this end, we pre-train a spectrum encoder using de novo sequencing
as a pre-training task. We then show that using these pre-trained spectrum
representations improves our performance on the four downstream tasks of
spectrum quality prediction, chimericity prediction, phosphorylation
prediction, and glycosylation status prediction. Finally, we perform multi-task
fine-tuning and find that this approach improves the performance on each task
individually. Overall, our work demonstrates that a foundation model for tandem
mass spectrometry proteomics trained on de novo sequencing learns generalizable
representations of spectra, improves performance on downstream tasks where
training data is limited, and can ultimately enhance data acquisition and
analysis in proteomics experiments.

</details>


### [39] [ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data](https://arxiv.org/abs/2505.10856)
*Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang*

**Main category:** cs.LG

**TL;DR:** A novel method called ImputeINR is proposed for time series imputation using implicit neural representations (INR). It performs well in handling high missing ratios and improves disease diagnosis tasks.


<details>
  <summary>Details</summary>
**Motivation:** Existing imputation methods perform poorly on time series data with substantial missing values.

**Method:** ImputeINR uses implicit neural representations to learn continuous functions for time series imputation.

**Result:** ImputeINR shows superior performance in imputation, especially for high missing ratios, and enhances disease diagnosis tasks when applied to healthcare data.

**Conclusion:** ImputeINR is an effective tool for time series imputation in healthcare data, particularly beneficial for high missing ratios.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImputeINR%3A+Time+Series+Imputation+via+Implicit+Neural+Representations+for+Disease+Diagnosis+with+Missing+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10856&send_immediately=true&force_search=false)

**Abstract:** Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.

</details>


### [40] [Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM](https://arxiv.org/abs/2505.10861)
*Thang Duong, Minglai Yang, Chicheng Zhang*

**Main category:** cs.LG

**TL;DR:** This paper investigates using Large Language Models (LLM) to generate high-quality datasets for initializing Reinforcement Learning (RL) algorithms in classical MDP environments like CartPole and Pendulum. The proposed LORO algorithm combines LLM-generated policies with RL exploration, showing superior performance compared to pure LLM or RL approaches.


<details>
  <summary>Details</summary>
**Motivation:** To enhance the sample efficiency and convergence of RL algorithms by leveraging the initial policy provided by LLMs.

**Method:** Using LLM to generate an off-policy dataset covering state-actions of optimal policies, then applying an RL algorithm to refine the LLM-suggested policy.

**Result:** LORO surpasses baseline algorithms, achieving up to 4 times the cumulative rewards of pure RL on OpenAI Gym environments.

**Conclusion:** The integration of LLMs and RL through the LORO algorithm improves both the effectiveness and efficiency of learning in classical MDP environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+Data-efficiency+of+Reinforcement+Learning+by+Warm-starting+with+LLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10861&send_immediately=true&force_search=false)

**Abstract:** We investigate the usage of Large Language Model (LLM) in collecting
high-quality data to warm-start Reinforcement Learning (RL) algorithms for
learning in some classical Markov Decision Process (MDP) environments. In this
work, we focus on using LLM to generate an off-policy dataset that sufficiently
covers state-actions visited by optimal policies, then later using an RL
algorithm to explore the environment and improve the policy suggested by the
LLM. Our algorithm, LORO, can both converge to an optimal policy and have a
high sample efficiency thanks to the LLM's good starting policy. On multiple
OpenAI Gym environments, such as CartPole and Pendulum, we empirically
demonstrate that LORO outperforms baseline algorithms such as pure LLM-based
policies, pure RL, and a naive combination of the two, achieving up to $4
\times$ the cumulative rewards of the pure RL baseline.

</details>


### [41] [MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection](https://arxiv.org/abs/2505.10874)
*Luca Magri, Filippo Leveni, Giacomo Boracchi*

**Main category:** cs.LG

**TL;DR:** This paper introduces MultiLink, an algorithm that efficiently recovers multiple types of geometric structures in datasets containing noise and outliers.


<details>
  <summary>Details</summary>
**Motivation:** Recovering multiple structures of different classes in datasets that are affected by noise and outliers.

**Method:** Preference analysis and clustering combined with a novel linkage scheme that merges two clusters based on model fitting and selection.

**Result:** The proposed MultiLink algorithm performs better than other state-of-the-art methods in both multi-class and single-class problems.

**Conclusion:** MultiLink algorithm provides significant improvements over existing methods in handling multiple classes of geometric structures within noisy and outlier-contaminated datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiLink%3A+Multi-class+Structure+Recovery+via+Agglomerative+Clustering+and+Model+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10874&send_immediately=true&force_search=false)

**Abstract:** We address the problem of recovering multiple structures of different classes
in a dataset contaminated by noise and outliers. In particular, we consider
geometric structures defined by a mixture of underlying parametric models (e.g.
planes and cylinders, homographies and fundamental matrices), and we tackle the
robust fitting problem by preference analysis and clustering. We present a new
algorithm, termed MultiLink, that simultaneously deals with multiple classes of
models. MultiLink combines on-the-fly model fitting and model selection in a
novel linkage scheme that determines whether two clusters are to be merged. The
resulting method features many practical advantages with respect to methods
based on preference analysis, being faster, less sensitive to the inlier
threshold, and able to compensate limitations deriving from hypotheses
sampling. Experiments on several public datasets demonstrate that Multi-Link
favourably compares with state of the art alternatives, both in multi-class and
single-class problems. Code is publicly made available for download.

</details>


### [42] [Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations](https://arxiv.org/abs/2505.10877)
*Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi*

**Main category:** cs.LG

**TL;DR:** This paper extends Gaussian process models to simplicial complexes, incorporating homological information, and demonstrates improved predictions across multiple applications.


<details>
  <summary>Details</summary>
**Motivation:** To address the issue of GNNs overfitting when data is scarce and provide an alternative approach using Gaussian processes with graph-level inputs.

**Method:** Extending Gaussian process framework to simplicial complexes, handling edge-level attributes, attributes on higher-order simplices, and considering Hodge decompositions to account for homological information.

**Result:** The framework improves predictions across various applications.

**Conclusion:** Our framework enhances predictions across various applications, making Gaussian processes more applicable for graph and simplicial complex-level predictions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+and+Simplicial+Complex+Prediction+Gaussian+Process+via+the+Hodgelet+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10877&send_immediately=true&force_search=false)

**Abstract:** Predicting the labels of graph-structured data is crucial in scientific
applications and is often achieved using graph neural networks (GNNs). However,
when data is scarce, GNNs suffer from overfitting, leading to poor performance.
Recently, Gaussian processes (GPs) with graph-level inputs have been proposed
as an alternative. In this work, we extend the Gaussian process framework to
simplicial complexes (SCs), enabling the handling of edge-level attributes and
attributes supported on higher-order simplices. We further augment the
resulting SC representations by considering their Hodge decompositions,
allowing us to account for homological information, such as the number of
holes, in the SC. We demonstrate that our framework enhances the predictions
across various applications, paving the way for GPs to be more widely used for
graph and SC-level predictions.

</details>


### [43] [Prior-Guided Diffusion Planning for Offline Reinforcement Learning](https://arxiv.org/abs/2505.10881)
*Donghyeon Ki, JunHyeok Oh, Seong-Woong Shim, Byung-Jun Lee*

**Main category:** cs.LG

**TL;DR:** This paper introduces Prior Guidance (PG), a novel guided sampling framework that enhances the performance of diffusion models in offline reinforcement learning.


<details>
  <summary>Details</summary>
**Motivation:** Existing guided sampling strategies have limitations such as producing suboptimal multi-modal actions, struggling with distributional drift, or incurring prohibitive inference-time costs.

**Method:** Replacing the standard Gaussian prior with a learnable distribution and applying behavior regularization in latent space.

**Result:** PG outperforms state-of-the-art diffusion policies and planners across diverse long-horizon offline RL benchmarks.

**Conclusion:** Prior Guidance (PG) improves the performance of diffusion models in offline reinforcement learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prior-Guided+Diffusion+Planning+for+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10881&send_immediately=true&force_search=false)

**Abstract:** Diffusion models have recently gained prominence in offline reinforcement
learning due to their ability to effectively learn high-performing,
generalizable policies from static datasets. Diffusion-based planners
facilitate long-horizon decision-making by generating high-quality trajectories
through iterative denoising, guided by return-maximizing objectives. However,
existing guided sampling strategies such as Classifier Guidance,
Classifier-Free Guidance, and Monte Carlo Sample Selection either produce
suboptimal multi-modal actions, struggle with distributional drift, or incur
prohibitive inference-time costs. To address these challenges, we propose Prior
Guidance (PG), a novel guided sampling framework that replaces the standard
Gaussian prior of a behavior-cloned diffusion model with a learnable
distribution, optimized via a behavior-regularized objective. PG directly
generates high-value trajectories without costly reward optimization of the
diffusion model itself, and eliminates the need to sample multiple candidates
at inference for sample selection. We present an efficient training strategy
that applies behavior regularization in latent space, and empirically
demonstrate that PG outperforms state-of-the-art diffusion policies and
planners across diverse long-horizon offline RL benchmarks.

</details>


### [44] [Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models](https://arxiv.org/abs/2505.10892)
*Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen*

**Main category:** cs.LG

**TL;DR:** This paper introduces MOPO, a new algorithm for multi-objective preference alignment in large language models. It optimizes several potentially conflicting objectives while maintaining safety thresholds.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods for post-training of LLMs with RLHF can only handle a single objective, but human users often have multiple objectives like helpfulness and harmlessness which cannot be easily aggregated.

**Method:** MOPO frames alignment as a constrained KL-regularized optimization, maximizing the primary objective while lower-bounding secondary objectives with tunable safety thresholds. It operates directly on pairwise preference data without requiring point-wise reward assumptions or heuristic prompt-context engineering.

**Result:** On synthetic benchmarks, MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world datasets, MOPO achieves higher rewards and produces policies that Pareto-dominate baselines. Ablation studies confirm its optimization stability and robustness to hyperparameters.

**Conclusion:** MOPO provides a novel approach to multi-objective preference alignment in large language models, demonstrating effectiveness and robustness in both synthetic and real-world settings.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Objective+Preference+Optimization%3A+Improving+Human+Alignment+of+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10892&send_immediately=true&force_search=false)

**Abstract:** Post-training of LLMs with RLHF, and subsequently preference optimization
algorithms such as DPO, IPO, etc., made a big difference in improving human
alignment. However, all such techniques can only work with a single (human)
objective. In practice, human users have multiple objectives, such as
helpfulness and harmlessness, and there is no natural way to aggregate them
into a single objective. In this paper, we address the multi-objective
preference-alignment problem, where a policy must optimize several, potentially
conflicting, objectives. We introduce the Multi-Objective Preference
Optimization (MOPO) algorithm, which frames alignment as a constrained
KL-regularized optimization: the primary objective is maximized while secondary
objectives are lower-bounded by tunable safety thresholds. Unlike prior work,
MOPO operates directly on pairwise preference data, requires no point-wise
reward assumption, and avoids heuristic prompt-context engineering. The method
recovers policies on the Pareto front whenever the front is attainable;
practically, it reduces to simple closed-form iterative updates suitable for
large-scale training. On synthetic benchmarks with diverse canonical preference
structures, we show that MOPO approximates the Pareto front. When fine-tuning a
1.3B-parameter language model on real-world human-preference datasets, MOPO
attains higher rewards and yields policies that Pareto-dominate baselines;
ablation studies confirm optimization stability and robustness to
hyperparameters.

</details>


### [45] [CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting](https://arxiv.org/abs/2505.10894)
*Yishuo Wang, Feng Zhou, Muping Zhou, Qicheng Meng, Zhijun Hu, Yi Wang*

**Main category:** cs.LG

**TL;DR:** This paper presents CTP, a new deep learning method combining CNN, Transformer, and PINN for predicting ocean fronts. It shows better performance than existing models.


<details>
  <summary>Details</summary>
**Motivation:** Ocean fronts are important but hard to predict with existing methods due to lack of spatial continuity and physical consistency.

**Method:** CTP integrates CNN, Transformer architectures, and physics-informed neural network for ocean front prediction.

**Result:** CTP achieves SOTA performance in single-step and multi-step predictions in SCS and KUR regions.

**Conclusion:** CTP combines different neural networks to improve ocean front prediction, showing significant advantages over baseline models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CTP%3A+A+hybrid+CNN-Transformer-PINN+model+for+ocean+front+forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10894&send_immediately=true&force_search=false)

**Abstract:** This paper proposes CTP, a novel deep learning framework that integrates
convolutional neural network(CNN), Transformer architectures, and
physics-informed neural network(PINN) for ocean front prediction. Ocean fronts,
as dynamic interfaces between distinct water masses, play critical roles in
marine biogeochemical and physical processes. Existing methods such as LSTM,
ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and
physical consistency over multi-step forecasts. CTP addresses these challenges
by combining localized spatial encoding, long-range temporal attention, and
physical constraint enforcement. Experimental results across south China
sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP
achieves state-of-the-art(SOTA) performance in both single-step and multi-step
predictions, significantly outperforming baseline models in accuracy, $F_1$
score, and temporal stability.

</details>


### [46] [Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions](https://arxiv.org/abs/2505.10913)
*Muntasir Hoq, Ananya Rao, Reisha Jaishankar, Krish Piryani, Nithya Janapati, Jessica Vandenberg, Bradford Mott, Narges Norouzi, James Lester, Bita Akram*

**Main category:** cs.LG

**TL;DR:** A scalable framework is presented for automatically detecting logical errors in students' programming solutions using an explainable AST embedding model.


<details>
  <summary>Details</summary>
**Motivation:** Understanding factors contributing to students' programming difficulties is important for effective learning support.

**Method:** The framework is based on a Subtree-based Attention Neural Network (SANN) that identifies structural components of programs with logical errors.

**Result:** Experiments showed the framework could accurately capture students' logical errors and provide insights into their learning processes.

**Conclusion:** This framework offers a valuable tool for enhancing programming education.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Identification+of+Logical+Errors+in+Programs%3A+Advancing+Scalable+Analysis+of+Student+Misconceptions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10913，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10913&send_immediately=true&force_search=false)

**Abstract:** In Computer Science (CS) education, understanding factors contributing to
students' programming difficulties is crucial for effective learning support.
By identifying specific issues students face, educators can provide targeted
assistance to help them overcome obstacles and improve learning outcomes. While
identifying sources of struggle, such as misconceptions, in real-time can be
challenging in current educational practices, analyzing logical errors in
students' code can offer valuable insights. This paper presents a scalable
framework for automatically detecting logical errors in students' programming
solutions. Our framework is based on an explainable Abstract Syntax Tree (AST)
embedding model, the Subtree-based Attention Neural Network (SANN), that
identifies the structural components of programs containing logical errors. We
conducted a series of experiments to evaluate its effectiveness, and the
results suggest that our framework can accurately capture students' logical
errors and, more importantly, provide us with deeper insights into their
learning processes, offering a valuable tool for enhancing programming
education.

</details>


### [47] [A Dataset for Spatiotemporal-Sensitive POI Question Answering](https://arxiv.org/abs/2505.10928)
*Xiao Han, Dayan Pan, Xiangyu Zhao, Xuyuan Hu, Zhaolin Deng, Xiangjie Kong, Guojiang Shen*

**Main category:** cs.LG

**TL;DR:** Introduce POI-QA, a novel spatiotemporal-sensitive QA dataset based on Point of Interest (POI) to evaluate models' spatiotemporal reasoning capabilities. Top-performing multilingual LLMs show limitations.


<details>
  <summary>Details</summary>
**Motivation:** Existing QA datasets lack sufficient spatiotemporal-sensitive questions, making them inadequate benchmarks for evaluating models' spatiotemporal reasoning capabilities.

**Method:** Construct POI-QA dataset through mining and aligning vehicle trajectory data with geographic POI data, validating spatiotemporal facts, and generating bilingual QA pairs.

**Result:** State-of-the-art multilingual LLMs show limitations in spatiotemporal reasoning tasks.

**Conclusion:** POI-QA is a robust benchmark for advancing algorithms sensitive to spatiotemporal dynamics.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Dataset+for+Spatiotemporal-Sensitive+POI+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10928&send_immediately=true&force_search=false)

**Abstract:** Spatiotemporal relationships are critical in data science, as many prediction
and reasoning tasks require analysis across both spatial and temporal
dimensions--for instance, navigating an unfamiliar city involves planning
itineraries that sequence locations and timing cultural experiences. However,
existing Question-Answering (QA) datasets lack sufficient
spatiotemporal-sensitive questions, making them inadequate benchmarks for
evaluating models' spatiotemporal reasoning capabilities. To address this gap,
we introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on
Point of Interest (POI), constructed through three key steps: mining and
aligning open-source vehicle trajectory data from GAIA with high-precision
geographic POI data, rigorous manual validation of noisy spatiotemporal facts,
and generating bilingual (Chinese/English) QA pairs that reflect
human-understandable spatiotemporal reasoning tasks. Our dataset challenges
models to parse complex spatiotemporal dependencies, and evaluations of
state-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark
limitations: even the top-performing model (Qwen2.5-7B fine-tuned with
RAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,
far below human performance at 0.56. This underscores persistent weaknesses in
LLMs' ability to perform consistent spatiotemporal reasoning, while
highlighting POI-QA as a robust benchmark to advance algorithms sensitive to
spatiotemporal dynamics. The dataset is publicly available at
https://www.kaggle.com/ds/7394666.

</details>


### [48] [Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models](https://arxiv.org/abs/2505.10930)
*Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen*

**Main category:** cs.LG

**TL;DR:** Proposed a self-supervised learning framework called Physics-informed temporal alignment (PITA) to improve the accuracy and robustness of auto-regressive partial differential equation foundation models.


<details>
  <summary>Details</summary>
**Motivation:** To solve the shortcut problem in auto-regressive prediction which causes error accumulation especially for out-of-distribution data.

**Method:** Physics-informed temporal alignment (PITA)

**Result:** Significantly enhances the accuracy and robustness of existing foundation models on diverse time-dependent PDE data.

**Conclusion:** The proposed method improves the accuracy and robustness of existing foundation models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-informed+Temporal+Alignment+for+Auto-regressive+PDE+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10930&send_immediately=true&force_search=false)

**Abstract:** Auto-regressive partial differential equation (PDE) foundation models have
shown great potential in handling time-dependent data. However, these models
suffer from the shortcut problem deeply rooted in auto-regressive prediction,
causing error accumulation. The challenge becomes particularly evident for
out-of-distribution data, as the pretraining performance may approach random
model initialization for downstream tasks with long-term dynamics. To deal with
this problem, we propose physics-informed temporal alignment (PITA), a
self-supervised learning framework inspired by inverse problem solving.
Specifically, PITA aligns the physical dynamics discovered at different time
steps on each given PDE trajectory by integrating physics-informed constraints
into the self-supervision signal. The alignment is derived from observation
data without relying on known physics priors, indicating strong generalization
ability to the out-of-distribution data. Extensive experiments show that PITA
significantly enhances the accuracy and robustness of existing foundation
models on diverse time-dependent PDE data. The code is available at
https://github.com/SCAILab-USTC/PITA.

</details>


### [49] [Privacy-Aware Lifelong Learning](https://arxiv.org/abs/2505.10941)
*Ozan Özdenizci, Elmar Rueckert, Robert Legenstein*

**Main category:** cs.LG

**TL;DR:** Enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models presents a critical and largely unaddressed challenge with contradicting objectives. The paper proposes a privacy-aware lifelong learning (PALL) solution that optimizes task-specific sparse subnetworks with parameter sharing within a single architecture, enabling exact unlearning without performance degradations.


<details>
  <summary>Details</summary>
**Motivation:** Enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models presents a critical and largely unaddressed challenge with contradicting objectives.

**Method:** Privacy-aware lifelong learning (PALL), which optimizes task-specific sparse subnetworks with parameter sharing within a single architecture, and utilizes an episodic memory rehearsal mechanism to facilitate exact unlearning without performance degradations.

**Result:** The proposed PALL solution is empirically demonstrated to be scalable across various architectures in image classification and provides a state-of-the-art solution that uniquely integrates lifelong learning and privacy-aware unlearning mechanisms for responsible AI applications.

**Conclusion:** This paper addresses the challenge of enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models by proposing a privacy-aware lifelong learning (PALL) solution.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Aware+Lifelong+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10941&send_immediately=true&force_search=false)

**Abstract:** Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.

</details>


### [50] [Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions](https://arxiv.org/abs/2505.10947)
*Kehan Long, Jorge Cortés, Nikolay Atanasov*

**Main category:** cs.LG

**TL;DR:** This paper introduces a novel method that leverages reinforcement learning value functions and neural networks to learn generalized Lyapunov functions, enabling stability certification for a broad class of systems with learned policies.


<details>
  <summary>Details</summary>
**Motivation:** Certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL) is challenging because constructing a strict step-wise decrease in the Lyapunov function is difficult for a learned control policy.

**Method:** Formulating an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Extending the method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss.

**Result:** The approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. It results in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach.

**Conclusion:** Our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thus bridging classical control theory and modern learning-based methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Certifying+Stability+of+Reinforcement+Learning+Policies+using+Generalized+Lyapunov+Functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10947&send_immediately=true&force_search=false)

**Abstract:** We study the problem of certifying the stability of closed-loop systems under
control policies derived from optimal control or reinforcement learning (RL).
Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov
function but such a certificate is difficult to construct for a learned control
policy. The value function associated with an RL policy is a natural Lyapunov
function candidate but it is not clear how it should be modified. To gain
intuition, we first study the linear quadratic regulator (LQR) problem and make
two key observations. First, a Lyapunov function can be obtained from the value
function of an LQR policy by augmenting it with a residual term related to the
system dynamics and stage cost. Second, the classical Lyapunov decrease
requirement can be relaxed to a generalized Lyapunov condition requiring only
decrease on average over multiple time steps. Using this intuition, we consider
the nonlinear setting and formulate an approach to learn generalized Lyapunov
functions by augmenting RL value functions with neural network residual terms.
Our approach successfully certifies the stability of RL policies trained on
Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly
train neural controllers and stability certificates using a multi-step Lyapunov
loss, resulting in larger certified inner approximations of the region of
attraction compared to the classical Lyapunov approach. Overall, our
formulation enables stability certification for a broad class of systems with
learned policies by making certificates easier to construct, thereby bridging
classical control theory and modern learning-based methods.

</details>


### [51] [FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks](https://arxiv.org/abs/2505.10949)
*Chenhui Xu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong*

**Main category:** cs.LG

**TL;DR:** This study shows that Physics Informed Neural Networks (PINNs) can fail due to insufficient arithmetic precision rather than local optima.


<details>
  <summary>Details</summary>
**Motivation:** To understand why PINNs often fail despite convergence of the PDE residual loss.

**Method:** Upgrading arithmetic precision from FP32 to FP64.

**Result:** With higher precision, PINNs can solve PDEs without failure modes.

**Conclusion:** Ensuring rigorous arithmetic precision is crucial for reliable PDE solving using neural networks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FP64+is+All+You+Need%3A+Rethinking+Failure+Modes+in+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10949&send_immediately=true&force_search=false)

**Abstract:** Physics Informed Neural Networks (PINNs) often exhibit failure modes in which
the PDE residual loss converges while the solution error stays large, a
phenomenon traditionally blamed on local optima separated from the true
solution by steep loss barriers. We challenge this understanding by demonstrate
that the real culprit is insufficient arithmetic precision: with standard FP32,
the LBFGS optimizer prematurely satisfies its convergence test, freezing the
network in a spurious failure phase. Simply upgrading to FP64 rescues
optimization, enabling vanilla PINNs to solve PDEs without any failure modes.
These results reframe PINN failure modes as precision induced stalls rather
than inescapable local minima and expose a three stage training dynamic
unconverged, failure, success whose boundaries shift with numerical precision.
Our findings emphasize that rigorous arithmetic precision is the key to
dependable PDE solving with neural networks.

</details>


### [52] [Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography](https://arxiv.org/abs/2505.10950)
*Tianshuo Zhang, Gao Jia, Wenzhe Zhai, Rui Yann, Xianglei Xing*

**Main category:** cs.LG

**TL;DR:** A novel generative steganography method called Shackled Dancing Diffusion (SD²) is introduced, which uses diffusion models for adaptive image synthesis with controllable information embedding.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods face trade-offs between security, capacity, and perceptual quality; integrating precise information embedding into the generative process is challenging.

**Method:** SD² combines bit-position locking with diffusion sampling injection to enable controllable information embedding within the generative trajectory, leveraging the expressive power of diffusion models.

**Result:** SD² achieves full message recovery with 100% accuracy, enhances robustness against steganalysis, and improves security, embedding capacity, and stability compared to previous methods.

**Conclusion:** SD² provides new insights into controllable generation and opens promising directions for secure visual communication.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shackled+Dancing%3A+A+Bit-Locked+Diffusion+Algorithm+for+Lossless+and+Controllable+Image+Steganography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10950&send_immediately=true&force_search=false)

**Abstract:** Data steganography aims to conceal information within visual content, yet
existing spatial- and frequency-domain approaches suffer from trade-offs
between security, capacity, and perceptual quality. Recent advances in
generative models, particularly diffusion models, offer new avenues for
adaptive image synthesis, but integrating precise information embedding into
the generative process remains challenging. We introduce Shackled Dancing
Diffusion, or SD$^2$, a plug-and-play generative steganography method that
combines bit-position locking with diffusion sampling injection to enable
controllable information embedding within the generative trajectory. SD$^2$
leverages the expressive power of diffusion models to synthesize diverse
carrier images while maintaining full message recovery with $100\%$ accuracy.
Our method achieves a favorable balance between randomness and constraint,
enhancing robustness against steganalysis without compromising image fidelity.
Extensive experiments show that SD$^2$ substantially outperforms prior methods
in security, embedding capacity, and stability. This algorithm offers new
insights into controllable generation and opens promising directions for secure
visual communication.

</details>


### [53] [SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache](https://arxiv.org/abs/2505.10951)
*Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang*

**Main category:** cs.LG

**TL;DR:** SubGCache is proposed to reuse computation across queries with similar structural prompts by clustering queries based on subgraph embeddings and pre-computing the key-value cache of representative subgraphs.


<details>
  <summary>Details</summary>
**Motivation:** To enhance the efficiency of graph-based retrieval-augmented generation (RAG) by reducing inference latency.

**Method:** SubGCache clusters queries, constructs representative subgraphs, and pre-computes the key-value cache for these subgraphs.

**Result:** SubGCache reduces inference latency by up to 6.68 times while maintaining or improving generation quality.

**Conclusion:** SubGCache effectively improves the efficiency of RAG without sacrificing performance.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SubGCache%3A+Accelerating+Graph-based+RAG+with+Subgraph-level+KV+Cache，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10951&send_immediately=true&force_search=false)

**Abstract:** Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (KV) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
KV cache of the representative subgraph of the cluster without computing the KV
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).

</details>


### [54] [Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design](https://arxiv.org/abs/2505.10954)
*Koki Iwai, Yusuke Kumagae, Yuki Koyama, Masahiro Hamasaki, Masataka Goto*

**Main category:** cs.LG

**TL;DR:** This paper introduces Constrained Preferential Bayesian Optimization (CPBO), an extension of Preferential Bayesian Optimization (PBO) that handles inequality constraints. It proposes a new acquisition function and demonstrates its use in a designer-in-the-loop system for banner ad design.


<details>
  <summary>Details</summary>
**Motivation:** Existing PBO methods do not address inequality constraints, which are common in real-world optimization tasks.

**Method:** CPBO, which incorporates inequality constraints through a novel acquisition function.

**Result:** The method successfully identifies optimal solutions by exploring feasible regions.

**Conclusion:** CPBO can guide creative design under real-world constraints, as shown in a user study with professional ad designers.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constrained+Preferential+Bayesian+Optimization+and+Its+Application+in+Banner+Ad+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10954&send_immediately=true&force_search=false)

**Abstract:** Preferential Bayesian optimization (PBO) is a variant of Bayesian
optimization that observes relative preferences (e.g., pairwise comparisons)
instead of direct objective values, making it especially suitable for
human-in-the-loop scenarios. However, real-world optimization tasks often
involve inequality constraints, which existing PBO methods have not yet
addressed. To fill this gap, we propose constrained preferential Bayesian
optimization (CPBO), an extension of PBO that incorporates inequality
constraints for the first time. Specifically, we present a novel acquisition
function for this purpose. Our technical evaluation shows that our CPBO method
successfully identifies optimal solutions by focusing on exploring feasible
regions. As a practical application, we also present a designer-in-the-loop
system for banner ad design using CPBO, where the objective is the designer's
subjective preference, and the constraint ensures a target predicted
click-through rate. We conducted a user study with professional ad designers,
demonstrating the potential benefits of our approach in guiding creative design
under real-world constraints.

</details>


### [55] [Relational Graph Transformer](https://arxiv.org/abs/2505.10960)
*Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico López, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec*

**Main category:** cs.LG

**TL;DR:** Introducing RelGT, a novel graph transformer architecture for relational tables that outperforms traditional GNNs in relational deep learning tasks.


<details>
  <summary>Details</summary>
**Motivation:** Commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies in relational data.

**Method:** RelGT employs a novel multi-element tokenization strategy and combines local attention over sampled subgraphs with global attention to learnable centroids.

**Result:** Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%.

**Conclusion:** RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relational+Graph+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10960&send_immediately=true&force_search=false)

**Abstract:** Relational Deep Learning (RDL) is a promising approach for building
state-of-the-art predictive models on multi-table relational data by
representing it as a heterogeneous temporal graph. However, commonly used Graph
Neural Network models suffer from fundamental limitations in capturing complex
structural patterns and long-range dependencies that are inherent in relational
data. While Graph Transformers have emerged as powerful alternatives to GNNs on
general graphs, applying them to relational entity graphs presents unique
challenges: (i) Traditional positional encodings fail to generalize to massive,
heterogeneous graphs; (ii) existing architectures cannot model the temporal
dynamics and schema constraints of relational data; (iii) existing tokenization
schemes lose critical structural information. Here we introduce the Relational
Graph Transformer (RelGT), the first graph transformer architecture designed
specifically for relational tables. RelGT employs a novel multi-element
tokenization strategy that decomposes each node into five components (features,
type, hop distance, time, and local structure), enabling efficient encoding of
heterogeneity, temporality, and topology without expensive precomputation. Our
architecture combines local attention over sampled subgraphs with global
attention to learnable centroids, incorporating both local and database-wide
representations. Across 21 tasks from the RelBench benchmark, RelGT
consistently matches or outperforms GNN baselines by up to 18%, establishing
Graph Transformers as a powerful architecture for Relational Deep Learning.

</details>


### [56] [Group-in-Group Policy Optimization for LLM Agent Training](https://arxiv.org/abs/2505.10978)
*Lang Feng, Zhenghai Xue, Tingcong Liu, Bo An*

**Main category:** cs.LG

**TL;DR:** This paper proposes Group-in-Group Policy Optimization (GiGPO), a new reinforcement learning algorithm for long-horizon large language model agent training, achieving fine-grained credit assignment without requiring auxiliary models or extra rollouts.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenge of credit assignment in long-horizon reinforcement learning tasks involving large language models.

**Method:** Introducing a two-level structure for estimating relative advantage: episode-level macro relative advantages and step-level micro relative advantages via an anchor state grouping mechanism.

**Result:** GiGPO improves performance by more than 12% on ALFWorld and over 9% on WebShop compared to the GRPO baseline, with the same GPU memory usage and rollout costs.

**Conclusion:** GiGPO successfully achieves fine-grained credit assignment for LLM agents in long-term tasks while retaining the advantages of group-based RL.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Group-in-Group+Policy+Optimization+for+LLM+Agent+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10978&send_immediately=true&force_search=false)

**Abstract:** Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.

</details>


### [57] [GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models](https://arxiv.org/abs/2505.10983)
*Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen*

**Main category:** cs.LG

**TL;DR:** Propose GenoArmory, the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), evaluating their vulnerability systematically.


<details>
  <summary>Details</summary>
**Motivation:** To provide a comprehensive evaluation framework for assessing the vulnerability of GFMs to adversarial attacks.

**Method:** Evaluate five state-of-the-art GFMs using four attack algorithms and three defense strategies. Introduce GenoAdv, a new adversarial sample dataset.

**Result:** Classification models show greater robustness than generative models against adversarial perturbations. Adversarial attacks often target biologically significant genomic regions.

**Conclusion:** GenoArmory is an accessible and comprehensive framework to analyze GFM vulnerabilities related to model architecture, quantization schemes, and training datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenoArmory%3A+A+Unified+Evaluation+Framework+for+Adversarial+Attacks+on+Genomic+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10983&send_immediately=true&force_search=false)

**Abstract:** We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,
GenoArmory offers the first comprehensive evaluation framework to
systematically assess the vulnerability of GFMs to adversarial attacks.
Methodologically, we evaluate the adversarial robustness of five
state-of-the-art GFMs using four widely adopted attack algorithms and three
defense strategies. Importantly, our benchmark provides an accessible and
comprehensive framework to analyze GFM vulnerabilities with respect to model
architecture, quantization schemes, and training datasets. Additionally, we
introduce GenoAdv, a new adversarial sample dataset designed to improve GFM
safety. Empirically, classification models exhibit greater robustness to
adversarial perturbations compared to generative models, highlighting the
impact of task type on model vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>


### [58] [ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks](https://arxiv.org/abs/2505.10992)
*Feiran You, Hongyang Du*

**Main category:** cs.LG

**TL;DR:** A new method called ReaCritic is proposed to enhance Deep Reinforcement Learning (DRL) by adding reasoning capabilities using a large reasoning transformer-based critic model. This method improves performance in heterogeneous networks and standard control tasks.


<details>
  <summary>Details</summary>
**Motivation:** The diversity of user requirements and time-varying wireless conditions in Heterogeneous Networks (HetNets) present significant challenges for DRL methods. Conventional critic models lack the ability to handle multi-task complexity. Recent advancements in scaling Large Language Models (LLMs) show that generating intermediate reasoning steps can improve decision quality.

**Method:** ReaCritic uses a large reasoning transformer-based critic model that performs both horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with value-based and actor-critic DRL algorithms.

**Result:** Experiments show that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks.

**Conclusion:** Integrating reasoning capabilities into DRL through ReaCritic enhances adaptability and performance in dynamic wireless environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReaCritic%3A+Large+Reasoning+Transformer-based+DRL+Critic-model+Scaling+For+Heterogeneous+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10992&send_immediately=true&force_search=false)

**Abstract:** Heterogeneous Networks (HetNets) pose critical challenges for intelligent
management due to the diverse user requirements and time-varying wireless
conditions. These factors introduce significant decision complexity, which
limits the adaptability of existing Deep Reinforcement Learning (DRL) methods.
In many DRL algorithms, especially those involving value-based or actor-critic
structures, the critic component plays a key role in guiding policy learning by
estimating value functions. However, conventional critic models often use
shallow architectures that map observations directly to scalar estimates,
limiting their ability to handle multi-task complexity. In contrast, recent
progress in inference-time scaling of Large Language Models (LLMs) has shown
that generating intermediate reasoning steps can significantly improve decision
quality. Motivated by this, we propose ReaCritic, a large reasoning
transformer-based criticmodel scaling scheme that brings reasoning ability into
DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs
and vertical reasoning through deep transformer stacks. It is compatible with a
broad range of value-based and actor-critic DRL algorithms and enhances
generalization in dynamic wireless environments. Extensive experiments
demonstrate that ReaCritic improves convergence speed and final performance
across various HetNet settings and standard OpenAI Gym control tasks.

</details>


### [59] [Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting](https://arxiv.org/abs/2505.11017)
*Wenjie Ou, Zhishuo Zhao, Dongyue Guo, Yi Lin*

**Main category:** cs.LG

**TL;DR:** A new method called Logo-LLM uses pre-trained large language models to forecast time series data, emphasizing both local and global patterns through multi-scale feature extraction and integration.


<details>
  <summary>Details</summary>
**Motivation:** Existing Transformer-based time series forecasting methods often overlook short-term local variations, and current adaptations of large language models underutilize their hierarchical representations.

**Method:** Logo-LLM explicitly extracts multi-scale temporal features from different layers of a pre-trained LLM and introduces Lightweight Local-Mixer and Global-Mixer modules for alignment and integration.

**Result:** Logo-LLM shows superior performance across various benchmarks, excels in few-shot and zero-shot scenarios, and maintains low computational costs.

**Conclusion:** The proposed Logo-LLM framework successfully captures both local dynamics and global trends in time series data, outperforming existing methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logo-LLM%3A+Local+and+Global+Modeling+with+Large+Language+Models+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11017&send_immediately=true&force_search=false)

**Abstract:** Time series forecasting is critical across multiple domains, where time
series data exhibits both local patterns and global dependencies. While
Transformer-based methods effectively capture global dependencies, they often
overlook short-term local variations in time series. Recent methods that adapt
large language models (LLMs) into time series forecasting inherit this
limitation by treating LLMs as black-box encoders, relying solely on the
final-layer output and underutilizing hierarchical representations. To address
this limitation, we propose Logo-LLM, a novel LLM-based framework that
explicitly extracts and models multi-scale temporal features from different
layers of a pre-trained LLM. Through empirical analysis, we show that shallow
layers of LLMs capture local dynamics in time series, while deeper layers
encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and
Global-Mixer modules to align and integrate features with the temporal input
across layers. Extensive experiments demonstrate that Logo-LLM achieves
superior performance across diverse benchmarks, with strong generalization in
few-shot and zero-shot settings while maintaining low computational overhead.

</details>


### [60] [Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs](https://arxiv.org/abs/2505.11023)
*Kutalmış Coşkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan Lüdtke, Martin Becker*

**Main category:** cs.LG

**TL;DR:** 研究发现，尽管使用背景知识（BK）的图神经网络（GNNs）在理论上可能有效，但在实际应用中并未优于传统方法，且对BK图的不完善具有较强的鲁棒性。


<details>
  <summary>Details</summary>
**Motivation:** 在复杂且数据稀少的领域如生物医学研究中，将背景知识（如蛋白-蛋白相互作用网络）纳入基于图的机器学习管道是一个有前景的研究方向。然而，BK的实际贡献及其不完美所带来的影响仍未被充分理解。

**Method:** 研究了在癌症亚型分类这一重要现实任务中BK的作用，并引入了一个评价框架来测试BK-aware模型在合成和真实生物医学环境中的鲁棒性。

**Result:** 结果显示，最先进的使用BK的GNNs并不比线性回归等未利用BK的模型表现更好；即使BK图受到严重干扰，这些模型的性能也基本保持不变。

**Conclusion:** 尽管使用背景知识（BK）的图神经网络（GNNs）在理论上可能有效，但实验表明它们并未优于线性回归等未利用BK的模型，并且即使BK图受到严重干扰，其性能也几乎没有变化。通过引入评估框架研究发现，GNN架构与BK特性的精心对齐是必要的，这有可能带来显著的性能提升。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Informed%2C+but+Not+Always+Improved%3A+Challenging+the+Benefit+of+Background+Knowledge+in+GNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11023，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11023&send_immediately=true&force_search=false)

**Abstract:** In complex and low-data domains such as biomedical research, incorporating
background knowledge (BK) graphs, such as protein-protein interaction (PPI)
networks, into graph-based machine learning pipelines is a promising research
direction. However, while BK is often assumed to improve model performance, its
actual contribution and the impact of imperfect knowledge remain poorly
understood. In this work, we investigate the role of BK in an important
real-world task: cancer subtype classification. Surprisingly, we find that (i)
state-of-the-art GNNs using BK perform no better than uninformed models like
linear regression, and (ii) their performance remains largely unchanged even
when the BK graph is heavily perturbed. To understand these unexpected results,
we introduce an evaluation framework, which employs (i) a synthetic setting
where the BK is clearly informative and (ii) a set of perturbations that
simulate various imperfections in BK graphs. With this, we test the robustness
of BK-aware models in both synthetic and real-world biomedical settings. Our
findings reveal that careful alignment of GNN architectures and BK
characteristics is necessary but holds the potential for significant
performance improvements.

</details>


### [61] [Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels](https://arxiv.org/abs/2505.11024)
*Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau*

**Main category:** cs.LG

**TL;DR:** This article proposes updating the established coating process for thermally spray coated components for steel manufacturing by integrating real-time data analytics and predictive quality management.


<details>
  <summary>Details</summary>
**Motivation:** The implementation of thermally sprayed components in steel manufacturing presents challenges for production and plant maintenance.

**Method:** The article designs two essential components--the data aggregator and the quality predictor--through continuous process monitoring and the application of data-driven methodologies.

**Result:** The performance of the combination was verified using small-scale tests that enabled accurate prediction of coating quality and proactive notification to the operator.

**Conclusion:** This article suggests that integrating real-time data analytics and predictive quality management can enhance the performance of thermally sprayed components in steel manufacturing.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Real-Time+Data+Analysis+and+Multiple+Kernel+Learning+for+Manufacturing+of+Innovative+Steels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11024&send_immediately=true&force_search=false)

**Abstract:** The implementation of thermally sprayed components in steel manufacturing
presents challenges for production and plant maintenance. While enhancing
performance through specialized surface properties, these components may
encounter difficulties in meeting modified requirements due to standardization
in the refurbishment process. This article proposes updating the established
coating process for thermally spray coated components for steel manufacturing
(TCCSM) by integrating real-time data analytics and predictive quality
management. Two essential components--the data aggregator and the quality
predictor--are designed through continuous process monitoring and the
application of data-driven methodologies to meet the dynamic demands of the
evolving steel landscape. The quality predictor is powered by the simple and
effective multiple kernel learning strategy with the goal of realizing
predictive quality. The data aggregator, designed with sensors, flow meters,
and intelligent data processing for the thermal spray coating process, is
proposed to facilitate real-time analytics. The performance of this combination
was verified using small-scale tests that enabled not only the accurate
prediction of coating quality based on the collected data but also proactive
notification to the operator as soon as significant deviations are identified.

</details>


### [62] [Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere](https://arxiv.org/abs/2505.11029)
*Li Ju, Max Andersson, Stina Fredriksson, Edward Glöckner, Andreas Hellander, Ekta Vats, Prashant Singh*

**Main category:** cs.LG

**TL;DR:** This paper proposes AsymVLM to build probabilistic embeddings from pre-trained VLMs on the unit hypersphere, addressing the asymmetric uncertainty structure inherent in textual and visual data.


<details>
  <summary>Details</summary>
**Motivation:** Deterministic VLMs fail to capture the inherent ambiguity and uncertainty in natural language and visual data. Existing probabilistic post-hoc adaptation methods do not account for the asymmetric uncertainty structure of the modalities and the constraint that meaningful deterministic embeddings reside on a unit hypersphere.

**Method:** Probabilistic embeddings were built from pre-trained VLMs on the unit hypersphere.

**Result:** The effectiveness of the probabilistic embeddings was validated on established benchmarks, and comprehensive ablation studies demonstrated the inherent nature of asymmetry in the uncertainty structure of textual and visual data.

**Conclusion:** AsymVLM was proposed to address the asymmetric uncertainty structure inherent in textual and visual data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+the+Asymmetric+Uncertainty+Structure+of+Pre-trained+VLMs+on+the+Unit+Hypersphere，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11029，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11029&send_immediately=true&force_search=false)

**Abstract:** Vision-language models (VLMs) as foundation models have significantly
enhanced performance across a wide range of visual and textual tasks, without
requiring large-scale training from scratch for downstream tasks. However,
these deterministic VLMs fail to capture the inherent ambiguity and uncertainty
in natural language and visual data. Recent probabilistic post-hoc adaptation
methods address this by mapping deterministic embeddings onto probability
distributions; however, existing approaches do not account for the asymmetric
uncertainty structure of the modalities, and the constraint that meaningful
deterministic embeddings reside on a unit hypersphere, potentially leading to
suboptimal performance. In this paper, we address the asymmetric uncertainty
structure inherent in textual and visual data, and propose AsymVLM to build
probabilistic embeddings from pre-trained VLMs on the unit hypersphere,
enabling uncertainty quantification. We validate the effectiveness of the
probabilistic embeddings on established benchmarks, and present comprehensive
ablation studies demonstrating the inherent nature of asymmetry in the
uncertainty structure of textual and visual data.

</details>


### [63] [Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios](https://arxiv.org/abs/2505.11035)
*Kihun Hong, Sejun Park, Ganguk Hwang*

**Main category:** cs.LG

**TL;DR:** This paper proposes a novel VFL framework that can handle arbitrary data alignment, unlabeled data, and multi-party collaboration.


<details>
  <summary>Details</summary>
**Motivation:** Existing VFL methods often impose restrictive assumptions such as a small number of participating parties, fully aligned data, or only using labeled data.

**Method:** A unified framework that accommodates both training and inference under arbitrary alignment and labeling scenarios, while supporting diverse missingness mechanisms.

**Result:** Our method outperforms all baselines in 160 cases with an average gap of 9.6 percentage points over the next-best competitors.

**Conclusion:** This is the first VFL framework to jointly handle arbitrary data alignment, unlabeled data, and multi-party collaboration.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Latent+Variable+Model+based+Vertical+Federated+Learning+with+Flexible+Alignment+and+Labeling+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11035&send_immediately=true&force_search=false)

**Abstract:** Federated learning (FL) has attracted significant attention for enabling
collaborative learning without exposing private data. Among the primary
variants of FL, vertical federated learning (VFL) addresses feature-partitioned
data held by multiple institutions, each holding complementary information for
the same set of users. However, existing VFL methods often impose restrictive
assumptions such as a small number of participating parties, fully aligned
data, or only using labeled data. In this work, we reinterpret alignment gaps
in VFL as missing data problems and propose a unified framework that
accommodates both training and inference under arbitrary alignment and labeling
scenarios, while supporting diverse missingness mechanisms. In the experiments
on 168 configurations spanning four benchmark datasets, six training-time
missingness patterns, and seven testing-time missingness patterns, our method
outperforms all baselines in 160 cases with an average gap of 9.6 percentage
points over the next-best competitors. To the best of our knowledge, this is
the first VFL framework to jointly handle arbitrary data alignment, unlabeled
data, and multi-party collaboration all at once.

</details>


### [64] [Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers](https://arxiv.org/abs/2505.11040)
*Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff*

**Main category:** cs.LG

**TL;DR:** This paper introduces a pre-scoring mechanism to improve HyperAttention's efficiency by prioritizing significant keys using K-means, K-median clustering, and leverage score-based ranking, replacing its original uniform residual sampling. This approach reduces perplexity and maintains accuracy while being 20 times faster than FlashAttention.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to address the issue of crucial keys' capturing limitation in HyperAttention, which increases perplexity, by introducing a pre-scoring mechanism to better prioritize significant keys.

**Method:** The method involves adding a pre-scoring mechanism with three scoring methods: K-means clustering, K-median clustering, and leverage score-based ranking, and replacing HyperAttention's uniform residual sampling with this mechanism.

**Result:** On ChatGLM2, the method reduces perplexity from 12 to 8.3, outperforming standard HyperAttention. On Vision-Transformer (ViT), it maintains similar accuracy to LevAttention and surpasses it under specific parameters.

**Conclusion:** Integrating pre-scoring into hierarchical attention mechanisms improves Transformer efficiency significantly.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Attention+via+Pre-Scoring%3A+Prioritizing+Informative+Keys+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11040&send_immediately=true&force_search=false)

**Abstract:** Recent advances in transformer architectures deeply enhance long-context
language modeling. Among them, HyperAttention achieves competitive efficiency
by combining a single-level LSH-based clustering with uniform residual
sampling. However,such a sampling limits crucial keys' capturing, which in turn
raises the overall perplexity. In this paper, we propose a pre-scoring
mechanism to assist HyperAttention to prioritize significant keys.
Specifically, we introduce three scoring methods: K-means clustering, K-median
clustering, and leverage score-based ranking (inspired by LevAttention) to
filter keys effectively. We further replace HyperAttention's original uniform
residual sampling entirely, relying exclusively on our pre-scoring mechanism.
Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,
which outperforms standard HyperAttention. Moreover, when running on the
Vision-Transformer (ViT), our method shows that it can guarantee similar
accuracy compared with LevAttention, and will surpass LevAttention given
specific parameters. Although this method introduces computational overhead,
its combination with HyperAttention remains 20 times faster than
FlashAttention, providing a balanced trade-off between speed and modeling
accuracy. Our results highlight the effectiveness of integrating pre-scoring
into hierarchical attention mechanisms, significantly improving Transformer's
efficiency.

</details>


### [65] [Exploration by Random Distribution Distillation](https://arxiv.org/abs/2505.11044)
*Zhirui Fang, Kai Yang, Jian Tao, Jiafei Lyu, Lusong Li, Li Shen, Xiu Li*

**Main category:** cs.LG

**TL;DR:** This paper proposes RDD, a novel exploration method in reinforcement learning that combines count-based and prediction-error approaches.


<details>
  <summary>Details</summary>
**Motivation:** Effective exploration is crucial in online reinforcement learning.

**Method:** RDD samples target network outputs from a normal distribution and uses the difference between prediction and target networks as intrinsic rewards.

**Result:** RDD improves exploration in high-dimensional spaces and has bounded intrinsic rewards.

**Conclusion:** Theoretical analysis and experiments confirm RDD's effectiveness in enhancing online exploration for reinforcement learning tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+by+Random+Distribution+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11044&send_immediately=true&force_search=false)

**Abstract:** Exploration remains a critical challenge in online reinforcement learning, as
an agent must effectively explore unknown environments to achieve high returns.
Currently, the main exploration algorithms are primarily count-based methods
and curiosity-based methods, with prediction-error methods being a prominent
example. In this paper, we propose a novel method called \textbf{R}andom
\textbf{D}istribution \textbf{D}istillation (RDD), which samples the output of
a target network from a normal distribution. RDD facilitates a more extensive
exploration by explicitly treating the difference between the prediction
network and the target network as an intrinsic reward. Furthermore, by
introducing randomness into the output of the target network for a given state
and modeling it as a sample from a normal distribution, intrinsic rewards are
bounded by two key components: a pseudo-count term ensuring proper exploration
decay and a discrepancy term accounting for predictor convergence. We
demonstrate that RDD effectively unifies both count-based and prediction-error
approaches. It retains the advantages of prediction-error methods in
high-dimensional spaces, while also implementing an intrinsic reward decay mode
akin to the pseudo-count method. In the experimental section, RDD is compared
with more advanced methods in a series of environments. Both theoretical
analysis and experimental results confirm the effectiveness of our approach in
improving online exploration for reinforcement learning tasks.

</details>


### [66] [Halting Recurrent GNNs and the Graded $μ$-Calculus](https://arxiv.org/abs/2505.11050)
*Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema*

**Main category:** cs.LG

**TL;DR:** This paper introduces a halting mechanism for recurrent Graph Neural Networks (GNNs), proving its expressive power in relation to graded modal mu-calculus and developing a new approximate semantics and model-checking algorithm.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to address the issue of non-termination in current recurrent GNNs and improve their expressive power.

**Method:** A halting mechanism is proposed for recurrent GNNs, and a new approximate semantics for graded mu-calculus is developed.

**Result:** The halting model can express all node classifiers definable in graded modal mu-calculus, and a new model-checking algorithm called the counting algorithm is created.

**Conclusion:** Recurrent GNNs restricted to node classifiers definable in monadic second-order logic can only express those definable in graded modal mu-calculus.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Halting+Recurrent+GNNs+and+the+Graded+%24%CE%BC%24-Calculus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11050&send_immediately=true&force_search=false)

**Abstract:** Graph Neural Networks (GNNs) are a class of machine-learning models that
operate on graph-structured data. Their expressive power is intimately related
to logics that are invariant under graded bisimilarity. Current proposals for
recurrent GNNs either assume that the graph size is given to the model, or
suffer from a lack of termination guarantees. In this paper, we propose a
halting mechanism for recurrent GNNs. We prove that our halting model can
express all node classifiers definable in graded modal mu-calculus, even for
the standard GNN variant that is oblivious to the graph size. A recent
breakthrough in the study of the expressivity of graded modal mu-calculus in
the finite suggests that conversely, restricted to node classifiers definable
in monadic second-order logic, recurrent GNNs can express only node classifiers
definable in graded modal mu-calculus. To prove our main result, we develop a
new approximate semantics for graded mu-calculus, which we believe to be of
independent interest. We leverage this new semantics into a new model-checking
algorithm, called the counting algorithm, which is oblivious to the graph size.
In a final step we show that the counting algorithm can be implemented on a
halting recurrent GNN.

</details>


### [67] [Assessing the Performance of Analog Training for Transfer Learning](https://arxiv.org/abs/2505.11067)
*Omobayode Fagbohungbe, Corey Lammie, Malte J. Rasch, Takashi Ando, Tayfun Gokmen, Vijay Narayanan*

**Main category:** cs.LG

**TL;DR:** This paper introduces and evaluates a new algorithm called c-TTv2 for analog in-memory computing, which improves deep learning training and transfer learning efficiency. The algorithm is tested on a Swin-ViT model using a subset of the CIFAR100 dataset and its robustness against variations in device specifications is examined.


<details>
  <summary>Details</summary>
**Motivation:** To overcome limitations of current training algorithms for analog in-memory computing due to asymmetric and non-linear properties of memory devices.

**Method:** Introducing the c-TTv2 algorithm that uses the chopped technique to handle challenges posed by analog memory devices.

**Result:** The performance of the c-TTv2 algorithm is assessed on a Swin-ViT model with a subset of the CIFAR100 dataset, showing improved outcomes compared to existing methods.

**Conclusion:** The proposed c-TTv2 algorithm demonstrates better performance and robustness for analog in-memory computing, particularly in handling device-to-device variation issues.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+the+Performance+of+Analog+Training+for+Transfer+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11067&send_immediately=true&force_search=false)

**Abstract:** Analog in-memory computing is a next-generation computing paradigm that
promises fast, parallel, and energy-efficient deep learning training and
transfer learning (TL). However, achieving this promise has remained elusive
due to a lack of suitable training algorithms. Analog memory devices exhibit
asymmetric and non-linear switching behavior in addition to device-to-device
variation, meaning that most, if not all, of the current off-the-shelf training
algorithms cannot achieve good training outcomes. Also, recently introduced
algorithms have enjoyed limited attention, as they require bi-directionally
switching devices of unrealistically high symmetry and precision and are highly
sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which
leverages the chopped technique to address many of the challenges mentioned
above. In this paper, we assess the performance of the c-TTv2 algorithm for
analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also
investigate the robustness of our algorithm to changes in some device
specifications, including weight transfer noise, symmetry point skew, and
symmetry point variability

</details>


### [68] [Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076)
*Vladimír Boža, Vladimír Macko*

**Main category:** cs.LG

**TL;DR:** DBF is a novel method that improves binary quantization approaches by factorizing dense weight matrices into products of two binary matrices, providing competitive compression rates and fine-grained control over compression ratios.


<details>
  <summary>Details</summary>
**Motivation:** To address the severe quantization constraint of binary quantization approaches and reduce the accuracy degradation.

**Method:** Double Binary Factorization (DBF)

**Result:** DBF outperforms existing binarization approaches in a 1-bit per weight range and is competitive with the best quantization methods like QuIP# and QTIP in a 2-bit per weight range. It also allows fine-grained control over compression ratios.

**Conclusion:** DBF is a novel method that factorizes dense weight matrices into products of two binary matrices, offering competitive or superior compression rates compared to state-of-the-art methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Addition+is+almost+all+you+need%3A+Compressing+neural+networks+with+double+binary+factorization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11076，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11076&send_immediately=true&force_search=false)

**Abstract:** Binary quantization approaches, which replace weight matrices with binary
matrices and substitute costly multiplications with cheaper additions, offer a
computationally efficient approach to address the increasing computational and
storage requirements of Large Language Models (LLMs). However, the severe
quantization constraint ($\pm1$) can lead to significant accuracy degradation.
In this paper, we propose Double Binary Factorization (DBF), a novel method
that factorizes dense weight matrices into products of two binary (sign)
matrices, each accompanied by scaling vectors. DBF preserves the efficiency
advantages of binary representations while achieving compression rates that are
competitive with or superior to state-of-the-art methods. Specifically, in a
1-bit per weight range, DBF is better than existing binarization approaches. In
a 2-bit per weight range, DBF is competitive with the best quantization methods
like QuIP\# and QTIP. Unlike most existing compression techniques, which offer
limited compression level choices, DBF allows fine-grained control over
compression ratios by adjusting the factorization's intermediate dimension.
Based on this advantage, we further introduce an algorithm for estimating
non-uniform layer-wise compression ratios for DBF, based on previously
developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary

</details>


### [69] [ShiQ: Bringing back Bellman to LLMs](https://arxiv.org/abs/2505.11081)
*Pierre Clavier, Nathan Grinsztajn, Raphael Avalos, Yannis Flet-Berliac, Irem Ergun, Omar D. Domingues, Eugene Tarassov, Olivier Pietquin, Pierre H. Richemond, Florian Strub, Matthieu Geist*

**Main category:** cs.LG

**TL;DR:** This paper introduces ShiQ, a method that adapts Q-learning to fine-tune pre-trained large language models (LLMs), focusing on off-policy, token-wise learning with improved sample efficiency and offline learning capabilities compared to traditional reinforcement learning approaches.


<details>
  <summary>Details</summary>
**Motivation:** To address the inefficiency of current reinforcement learning methods in fine-tuning LLMs and leverage the advantages of Q-learning, such as sample efficiency and offline learning capability.

**Method:** Deriving theoretically grounded loss functions from Bellman equations to adapt Q-learning methods to LLMs, ensuring logits become reliable Q-value estimates, and implementing these into the ShiQ algorithm.

**Result:** ShiQ demonstrates effectiveness in both single-turn and multi-turn LLM settings on synthetic data and real-world benchmarks like UltraFeedback and BFCL-V3.

**Conclusion:** ShiQ provides a novel approach to effectively fine-tune LLMs by leveraging Q-learning paradigms, addressing the limitations of previous reinforcement learning methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ShiQ%3A+Bringing+back+Bellman+to+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11081&send_immediately=true&force_search=false)

**Abstract:** The fine-tuning of pre-trained large language models (LLMs) using
reinforcement learning (RL) is generally formulated as direct policy
optimization. This approach was naturally favored as it efficiently improves a
pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning
methods, has received far less attention in the LLM community while
demonstrating major success in various non-LLM RL tasks. In particular,
Q-learning effectiveness comes from its sample efficiency and ability to learn
offline, which is particularly valuable given the high computational cost of
sampling with LLMs. However, naively applying a Q-learning-style update to the
model's logits is ineffective due to the specificity of LLMs. Our core
contribution is to derive theoretically grounded loss functions from Bellman
equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt
insights from the RL literature to account for LLM-specific characteristics,
ensuring that the logits become reliable Q-value estimates. We then use this
loss to build a practical algorithm, ShiQ for Shifted-Q, that supports
off-policy, token-wise learning while remaining simple to implement. Finally,
we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,
UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn
and multi-turn LLM settings

</details>


### [70] [Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation](https://arxiv.org/abs/2505.11083)
*Guangqiang Li, M. Amine Atoui, Xiangshun Li*

**Main category:** cs.LG

**TL;DR:** A new fault diagnosis model called TSA-SAN is proposed which uses inter-mode mappings and interpolation to improve diagnosis performance.


<details>
  <summary>Details</summary>
**Motivation:** Existing fault diagnosis methods face challenges due to incomplete data and large distributional differences between operating modes.

**Method:** Self-adaptive temporal-spatial attention network (TSA-SAN) is proposed which includes inter-mode mappings, interpolation, self-adaptive instance normalization, and temporal-spatial attention mechanism.

**Result:** The proposed model significantly outperforms state-of-the-art methods according to extensive experiments.

**Conclusion:** The proposed TSA-SAN model improves fault diagnosis performance in multimode processes with partial overlap in health state categories.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fault+Diagnosis+across+Heterogeneous+Domains+via+Self-Adaptive+Temporal-Spatial+Attention+and+Sample+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11083&send_immediately=true&force_search=false)

**Abstract:** Deep learning methods have shown promising performance in fault diagnosis for
multimode process. Most existing studies assume that the collected health state
categories from different operating modes are identical. However, in real
industrial scenarios, these categories typically exhibit only partial overlap.
The incompleteness of the available data and the large distributional
differences between the operating modes pose a significant challenge to
existing fault diagnosis methods. To address this problem, a novel fault
diagnosis model named self-adaptive temporal-spatial attention network
(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy
category data to generate multimode samples. To enrich the diversity of the
fault data, interpolation is performed between healthy and fault samples.
Subsequently, the fault diagnosis model is trained using real and generated
data. The self-adaptive instance normalization is established to suppress
irrelevant information while retaining essential statistical features for
diagnosis. In addition, a temporal-spatial attention mechanism is constructed
to focus on the key features, thus enhancing the generalization ability of the
model. The extensive experiments demonstrate that the proposed model
significantly outperforms the state-of-the-art methods. The code will be
available on Github at https://github.com/GuangqiangLi/TSA-SAN.

</details>


### [71] [Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors](https://arxiv.org/abs/2505.11100)
*Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan*

**Main category:** cs.LG

**TL;DR:** A novel mixed-play framework named Bidirectional Distillation (BiDist) is proposed to improve population-population generalization in multi-agent reinforcement learning.


<details>
  <summary>Details</summary>
**Motivation:** Existing self-play-based methods are limited by inside-space generalization when agents face unseen co-players.

**Method:** BiDist uses knowledge distillation in two alternating directions: forward and reverse distillation. It doesn't require complex storage of past policies.

**Result:** BiDist shows remarkable generalization ability across various tasks and diversifies the policy distribution space.

**Conclusion:** The proposed method provides a concise and effective solution for population-population generalization in MARL.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bidirectional+Distillation%3A+A+Mixed-Play+Framework+for+Multi-Agent+Generalizable+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11100&send_immediately=true&force_search=false)

**Abstract:** Population-population generalization is a challenging problem in multi-agent
reinforcement learning (MARL), particularly when agents encounter unseen
co-players. However, existing self-play-based methods are constrained by the
limitation of inside-space generalization. In this study, we propose
Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome
this limitation in MARL. BiDist leverages knowledge distillation in two
alternating directions: forward distillation, which emulates the historical
policies' space and creates an implicit self-play, and reverse distillation,
which systematically drives agents towards novel distributions outside the
known policy space in a non-self-play manner. In addition, BiDist operates as a
concise and efficient solution without the need for the complex and costly
storage of past policies. We provide both theoretical analysis and empirical
evidence to support BiDist's effectiveness. Our results highlight its
remarkable generalization ability across a variety of cooperative, competitive,
and social dilemma tasks, and reveal that BiDist significantly diversifies the
policy distribution space. We also present comprehensive ablation studies to
reinforce BiDist's effectiveness and key success factors. Source codes are
available in the supplementary material.

</details>


### [72] [Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series](https://arxiv.org/abs/2505.11106)
*Thanadej Rattanakornphan, Piyanon Charoenpoonpanich, Chainarong Amornbunchornvej*

**Main category:** cs.LG

**TL;DR:** Propose an algorithm for finding the most similar multidimensional subsequences between time series with different lengths, which is more efficient than baseline approaches.


<details>
  <summary>Details</summary>
**Motivation:** Capture dependency in stock market or discover coordinated movement of baboons.

**Method:** An algorithm providing exact solutions for finding similar subsequences between time series with length differences.

**Result:** Simulated data showed our approach was four times faster; real-world data showed up to 20 times speedup and provided useful insights.

**Conclusion:** Our approach can be applied to any time series and is publicly available.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inferring+the+Most+Similar+Variable-length+Subsequences+between+Multidimensional+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11106&send_immediately=true&force_search=false)

**Abstract:** Finding the most similar subsequences between two multidimensional time
series has many applications: e.g. capturing dependency in stock market or
discovering coordinated movement of baboons. Considering one pattern occurring
in one time series, we might be wondering whether the same pattern occurs in
another time series with some distortion that might have a different length.
Nevertheless, to the best of our knowledge, there is no efficient framework
that deals with this problem yet. In this work, we propose an algorithm that
provides the exact solution of finding the most similar multidimensional
subsequences between time series where there is a difference in length both
between time series and between subsequences. The algorithm is built based on
theoretical guarantee of correctness and efficiency. The result in simulation
datasets illustrated that our approach not just only provided correct solution,
but it also utilized running time only quarter of time compared against the
baseline approaches. In real-world datasets, it extracted the most similar
subsequences even faster (up to 20 times faster against baseline methods) and
provided insights regarding the situation in stock market and following
relations of multidimensional time series of baboon movement. Our approach can
be used for any time series. The code and datasets of this work are provided
for the public use.

</details>


### [73] [FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation](https://arxiv.org/abs/2505.11111)
*Lin Zhu, Yijun Bian, Lei You*

**Main category:** cs.LG

**TL;DR:** FairSHAP是一种新的预处理框架，利用Shapley值归因来提高机器学习模型的公平性，同时保持数据完整性、模型准确性和最小的数据扰动。


<details>
  <summary>Details</summary>
**Motivation:** 确保机器学习模型的公平性在高风险领域尤为重要，因为有偏见的决策可能导致严重的社会后果。然而，现有的预处理方法通常缺乏透明机制来识别哪些特征或实例导致不公平性。

**Method:** FairSHAP使用Shapley值归因来提高个体和群体公平性。它通过可解释的特征重要性度量来识别训练数据中的公平关键实例，并通过敏感组之间的实例匹配来系统地修改它们。

**Result:** FairSHAP在多个表格数据集上显著改善了人口平等和机会平等，并且在一些情况下，改进了预测性能。

**Conclusion:** FairSHAP通过Shapley值归因提高了个体和群体公平性。它能识别出训练数据中公平关键的实例，并通过敏感组之间的实例匹配来系统地修改它们。这减少了歧视风险，同时保持了数据完整性和模型准确性。FairSHAP在多个表格数据集上显著改善了人口平等和机会平等，并且作为与模型无关且透明的方法，可以无缝集成到现有的机器学习管道中，并提供有关偏差来源的操作见解。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FairSHAP%3A+Preprocessing+for+Fairness+Through+Attribution-Based+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11111&send_immediately=true&force_search=false)

**Abstract:** Ensuring fairness in machine learning models is critical, particularly in
high-stakes domains where biased decisions can lead to serious societal
consequences. Existing preprocessing approaches generally lack transparent
mechanisms for identifying which features or instances are responsible for
unfairness. This obscures the rationale behind data modifications. We introduce
FairSHAP, a novel pre-processing framework that leverages Shapley value
attribution to improve both individual and group fairness. FairSHAP identifies
fairness-critical instances in the training data using an interpretable measure
of feature importance, and systematically modifies them through instance-level
matching across sensitive groups. This process reduces discriminative risk - an
individual fairness metric - while preserving data integrity and model
accuracy. We demonstrate that FairSHAP significantly improves demographic
parity and equality of opportunity across diverse tabular datasets, achieving
fairness gains with minimal data perturbation and, in some cases, improved
predictive performance. As a model-agnostic and transparent method, FairSHAP
integrates seamlessly into existing machine learning pipelines and provides
actionable insights into the sources of bias.Our code is on
https://github.com/youlei202/FairSHAP.

</details>


### [74] [Attention on the Sphere](https://arxiv.org/abs/2505.11157)
*Boris Bonev, Max Rietmann, Andrea Paris, Alberto Carpentieri, Thorsten Kurth*

**Main category:** cs.LG

**TL;DR:** A novel spherical attention mechanism improves Transformer models' performance on spherical data by preserving symmetries and introducing locality bias.


<details>
  <summary>Details</summary>
**Motivation:** Enable Transformer architectures to handle data on two-dimensional spheres while preserving spherical symmetries and topology, which is crucial for fields like atmospheric physics, cosmology, and robotics.

**Method:** Generalized attention mechanism with numerical quadrature weights for spherical domains and neighborhood attention confined to geodesic neighborhoods.

**Result:** Better performance than Cartesian approaches due to rotational equivariance and additional inductive bias for locality.

**Conclusion:** Our spherical Transformers consistently outperform planar counterparts across various tasks, proving the benefits of geometric priors.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention+on+the+Sphere，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11157&send_immediately=true&force_search=false)

**Abstract:** We introduce a generalized attention mechanism for spherical domains,
enabling Transformer architectures to natively process data defined on the
two-dimensional sphere - a critical need in fields such as atmospheric physics,
cosmology, and robotics, where preserving spherical symmetries and topology is
essential for physical accuracy. By integrating numerical quadrature weights
into the attention mechanism, we obtain a geometrically faithful spherical
attention that is approximately rotationally equivariant, providing strong
inductive biases and leading to better performance than Cartesian approaches.
To further enhance both scalability and model performance, we propose
neighborhood attention on the sphere, which confines interactions to geodesic
neighborhoods. This approach reduces computational complexity and introduces
the additional inductive bias for locality, while retaining the symmetry
properties of our method. We provide optimized CUDA kernels and
memory-efficient implementations to ensure practical applicability. The method
is validated on three diverse tasks: simulating shallow water equations on the
rotating sphere, spherical image segmentation, and spherical depth estimation.
Across all tasks, our spherical Transformers consistently outperform their
planar counterparts, highlighting the advantage of geometric priors for
learning on spherical domains.

</details>


### [75] [Dual-Balancing for Physics-Informed Neural Networks](https://arxiv.org/abs/2505.11117)
*Chenhong Zhou, Jie Chen, Zaifeng Yang, Ching Eng Png*

**Main category:** cs.LG

**TL;DR:** 提出了一种新的DB-PINN方法，通过动态调整损失权重来提高物理信息神经网络（PINNs）在解决偏微分方程时的精度和收敛速度。


<details>
  <summary>Details</summary>
**Motivation:** 现有PINNs方法存在多目标优化问题，导致精度低和收敛慢的问题。

**Method:** DB-PINN通过集成inter-balancing和intra-balancing动态调整损失权重，并引入鲁棒权重更新策略防止瞬时权重值的突然变化和算术溢出。

**Result:** DB-PINN在收敛速度和预测准确性方面优于其他基于梯度的加权方法。

**Conclusion:** DB-PINN显著提高了PINNs的性能，适用于解决偏微分方程的问题。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dual-Balancing+for+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11117&send_immediately=true&force_search=false)

**Abstract:** Physics-informed neural networks (PINNs) have emerged as a new learning
paradigm for solving partial differential equations (PDEs) by enforcing the
constraints of physical equations, boundary conditions (BCs), and initial
conditions (ICs) into the loss function. Despite their successes, vanilla PINNs
still suffer from poor accuracy and slow convergence due to the intractable
multi-objective optimization issue. In this paper, we propose a novel
Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by
integrating inter-balancing and intra-balancing to alleviate two imbalance
issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance
between PDE residual loss and condition-fitting losses by determining an
aggregated weight that offsets their gradient distribution discrepancies.
Intra-balancing acts on condition-fitting losses to tackle the imbalance in
fitting difficulty across diverse conditions. By evaluating the fitting
difficulty based on the loss records, intra-balancing can allocate the
aggregated weight proportionally to each condition loss according to its
fitting difficulty levels. We further introduce a robust weight update strategy
to prevent abrupt spikes and arithmetic overflow in instantaneous weight values
caused by large loss variances, enabling smooth weight updating and stable
training. Extensive experiments demonstrate that DB-PINN achieves significantly
superior performance than those popular gradient-based weighting methods in
terms of convergence speed and prediction accuracy. Our code and supplementary
material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.

</details>


### [76] [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
*Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang*

**Main category:** cs.LG

**TL;DR:** EVA is a new framework that creates expressive and generalizable event representations, outperforming previous methods on recognition tasks and being the first to handle detection tasks.


<details>
  <summary>Details</summary>
**Motivation:** Standard tensor-based ML doesn't work well with event cameras due to their asynchronous, sparse nature. Existing A2S methods compromise on representation quality.

**Method:** EVA uses concepts from language modeling like linear attention and self-supervised learning to adapt to event camera data.

**Result:** EVA surpasses prior A2S methods in recognition tasks and achieves 47.7 mAP on a detection task.

**Conclusion:** EVA demonstrates transformative potential for real-time event-based vision applications.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Maximizing+Asynchronicity+in+Event-based+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11165&send_immediately=true&force_search=false)

**Abstract:** Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.

</details>


### [77] [GraphOracle: A Foundation Model for Knowledge Graph Reasoning](https://arxiv.org/abs/2505.11125)
*Enjun Du, Siyi Liu, Yongqi Zhang*

**Main category:** cs.LG

**TL;DR:** Introduce GraphOracle, a relation-centric foundation model for knowledge graphs that uses Relation-Dependency Graphs and query-dependent attention to achieve state-of-the-art performance across diverse benchmarks.


<details>
  <summary>Details</summary>
**Motivation:** Developing foundation models for knowledge graphs faces challenges due to their dynamic nature and need for cross-domain reasoning.

**Method:** Convert knowledge graphs into Relation-Dependency Graphs, develop query-dependent attention for representations, and pre-train/fine-tune the model.

**Result:** Consistent state-of-the-art performance with up to 35% improvement over baselines on 31 diverse benchmarks.

**Conclusion:** GraphOracle demonstrates effective generalization to unseen entities, relations, and entire graphs with minimal adaptation.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GraphOracle%3A+A+Foundation+Model+for+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11125&send_immediately=true&force_search=false)

**Abstract:** Foundation models have demonstrated remarkable capabilities across various
domains, but developing analogous models for knowledge graphs presents unique
challenges due to their dynamic nature and the need for cross-domain reasoning.
To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a
relation-centric foundation model that unifies reasoning across knowledge
graphs by converting them into Relation-Dependency Graphs (RDG), explicitly
encoding compositional patterns with fewer edges than prior methods. A
query-dependent attention mechanism is further developed to learn inductive
representations for both relations and entities. Pre-training on diverse
knowledge graphs, followed by minutes-level fine-tuning, enables effective
generalization to unseen entities, relations, and entire graphs. Through
comprehensive experiments on 31 diverse benchmarks spanning transductive,
inductive, and cross-domain settings, we demonstrate consistent
state-of-the-art performance with minimal adaptation, improving the prediction
performance by up to 35\% compared to the strongest baselines.

</details>


### [78] [RanDeS: Randomized Delta Superposition for Multi-Model Compression](https://arxiv.org/abs/2505.11204)
*Hangyu Zhou, Aaron Gokaslan, Volodymyr Kuleshov, Bharath Hariharan*

**Main category:** cs.LG

**TL;DR:** This paper introduces a novel approach to enhance multi-model serving efficiency by reducing task interference through random orthogonal transformations.


<details>
  <summary>Details</summary>
**Motivation:** To enable memory-efficient serving of multiple fine-tuned models without degraded performance due to task-specific parameter interference.

**Method:** Using random orthogonal transformations to decorrelate vectors for self-cancellation to reduce task interference.

**Result:** The proposed method significantly reduces task interference and improves performance on vision and language tasks.

**Conclusion:** This paper presents a method to reduce task interference in model merging by using random orthogonal transformations, which improves performance on various tasks without requiring additional memory.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RanDeS%3A+Randomized+Delta+Superposition+for+Multi-Model+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11204&send_immediately=true&force_search=false)

**Abstract:** From a multi-model compression perspective, model merging enables
memory-efficient serving of multiple models fine-tuned from the same base, but
suffers from degraded performance due to interference among their task-specific
parameter adjustments (i.e., deltas). In this paper, we reformulate model
merging as a compress-and-retrieve scheme, revealing that the task interference
arises from the summation of irrelevant deltas during model retrieval. To
address this issue, we use random orthogonal transformations to decorrelate
these vectors into self-cancellation. We show that this approach drastically
reduces interference, improving performance across both vision and language
tasks. Since these transformations are fully defined by random seeds, adding
new models requires no extra memory. Further, their data- and model-agnostic
nature enables easy addition or removal of models with minimal compute
overhead, supporting efficient and flexible multi-model serving.

</details>


### [79] [What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold](https://arxiv.org/abs/2505.11128)
*Simone Azeglio, Arianna Di Bernardo*

**Main category:** cs.LG

**TL;DR:** A new method using a score-based Riemannian metric is introduced to better understand the geometry of diffusion models' learned data manifold, improving image transitions.


<details>
  <summary>Details</summary>
**Motivation:** To understand the geometric properties of the learned data manifold which remains poorly understood.

**Method:** Introducing a score-based Riemannian metric leveraging the Stein score function from diffusion models.

**Result:** Score-based geodesics capture meaningful transformations respecting the underlying data distribution, outperforming baseline approaches on perceptual and distribution-level metrics.

**Conclusion:** The paper reveals the implicit geometric structure learned by diffusion models and provides a principled way to navigate the manifold of natural images.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+Inside+Your+Diffusion+Model%3F+A+Score-Based+Riemannian+Metric+to+Explore+the+Data+Manifold，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11128&send_immediately=true&force_search=false)

**Abstract:** Recent advances in diffusion models have demonstrated their remarkable
ability to capture complex image distributions, but the geometric properties of
the learned data manifold remain poorly understood. We address this gap by
introducing a score-based Riemannian metric that leverages the Stein score
function from diffusion models to characterize the intrinsic geometry of the
data manifold without requiring explicit parameterization. Our approach defines
a metric tensor in the ambient space that stretches distances perpendicular to
the manifold while preserving them along tangential directions, effectively
creating a geometry where geodesics naturally follow the manifold's contours.
We develop efficient algorithms for computing these geodesics and demonstrate
their utility for both interpolation between data points and extrapolation
beyond the observed data distribution. Through experiments on synthetic data
with known geometry, Rotated MNIST, and complex natural images via Stable
Diffusion, we show that our score-based geodesics capture meaningful
transformations that respect the underlying data distribution. Our method
consistently outperforms baseline approaches on perceptual metrics (LPIPS) and
distribution-level metrics (FID, KID), producing smoother, more realistic image
transitions. These results reveal the implicit geometric structure learned by
diffusion models and provide a principled way to navigate the manifold of
natural images through the lens of Riemannian geometry.

</details>


### [80] [A Set-Sequence Model for Time Series](https://arxiv.org/abs/2505.11243)
*Elliot L. Epstein, Apaar Sadhwani, Kay Giesecke*

**Main category:** cs.LG

**TL;DR:** This paper introduces a Set-Sequence model to predict financial outcomes without using handcrafted features, demonstrating superior performance on stock return prediction and mortgage behavior tasks.


<details>
  <summary>Details</summary>
**Motivation:** To address the limitations of traditional methods that rely on handcrafted summary features in capturing latent cross-sectional effects in financial predictions.

**Method:** Proposing a Set-Sequence model where a Set model generates a shared cross-sectional summary, followed by a Sequence model predicting outcomes using these summaries.

**Result:** The model shows significant improvement over benchmarks in stock return prediction and mortgage behavior tasks.

**Conclusion:** The proposed Set-Sequence model efficiently captures latent cross-sectional effects and outperforms traditional approaches in financial predictions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Set-Sequence+Model+for+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11243&send_immediately=true&force_search=false)

**Abstract:** In many financial prediction problems, the behavior of individual units (such
as loans, bonds, or stocks) is influenced by observable unit-level factors and
macroeconomic variables, as well as by latent cross-sectional effects.
Traditional approaches attempt to capture these latent effects via handcrafted
summary features. We propose a Set-Sequence model that eliminates the need for
handcrafted features. The Set model first learns a shared cross-sectional
summary at each period. The Sequence model then ingests the summary-augmented
time series for each unit independently to predict its outcome. Both components
are learned jointly over arbitrary sets sampled during training. Our approach
harnesses the set nature of the cross-section and is computationally efficient,
generating set summaries in linear time relative to the number of units. It is
also flexible, allowing the use of existing sequence models and accommodating a
variable number of units at inference. Empirical evaluations demonstrate that
our Set-Sequence model significantly outperforms benchmarks on stock return
prediction and mortgage behavior tasks. Code will be released.

</details>


### [81] [Fairness-aware Anomaly Detection via Fair Projection](https://arxiv.org/abs/2505.11132)
*Feng Xiao, Xiaoying Tang, Jicong Fan*

**Main category:** cs.LG

**TL;DR:** This work introduces FairAD, a fairness-aware anomaly detection method that ensures fairness between different demographic groups by learning a projection to map data to a common target distribution.


<details>
  <summary>Details</summary>
**Motivation:** Ensuring fairness in unsupervised anomaly detection systems is crucial to avoid unfair treatment for different groups and reduce social bias.

**Method:** FairAD learns a projection to map data of different demographic groups to a common target distribution, which allows for reliable density estimation and anomaly identification.

**Result:** Experiments show that FairAD improves the trade-off between detection accuracy and fairness under both balanced and skewed data across different groups.

**Conclusion:** FairAD demonstrates the feasibility and necessary assumptions for ensuring group fairness in unsupervised anomaly detection.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fairness-aware+Anomaly+Detection+via+Fair+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11132&send_immediately=true&force_search=false)

**Abstract:** Unsupervised anomaly detection is a critical task in many high-social-impact
applications such as finance, healthcare, social media, and cybersecurity,
where demographics involving age, gender, race, disease, etc, are used
frequently. In these scenarios, possible bias from anomaly detection systems
can lead to unfair treatment for different groups and even exacerbate social
bias. In this work, first, we thoroughly analyze the feasibility and necessary
assumptions for ensuring group fairness in unsupervised anomaly detection.
Second, we propose a novel fairness-aware anomaly detection method FairAD. From
the normal training data, FairAD learns a projection to map data of different
demographic groups to a common target distribution that is simple and compact,
and hence provides a reliable base to estimate the density of the data. The
density can be directly used to identify anomalies while the common target
distribution ensures fairness between different groups. Furthermore, we propose
a threshold-free fairness metric that provides a global view for model's
fairness, eliminating dependence on manual threshold selection. Experiments on
real-world benchmarks demonstrate that our method achieves an improved
trade-off between detection accuracy and fairness under both balanced and
skewed data across different groups.

</details>


### [82] [Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning](https://arxiv.org/abs/2505.11304)
*Shudi Weng, Chao Ren, Ming Xiao, Mikael Skoglund*

**Main category:** cs.LG

**TL;DR:** This paper analyzes the impact of communication and computational heterogeneity on federated learning (FL) optimization dynamics and proposes FedACS, a method to eliminate objective inconsistency.


<details>
  <summary>Details</summary>
**Motivation:** To understand the joint effect of communication and computation heterogeneity on FL optimization dynamics.

**Method:** FedACS, a universal method to eliminate all types of objective inconsistency.

**Result:** FedACS outperforms other methods by 4.3%-36% while reducing communication costs by 22%-89% and computation loads by 14%-105%.

**Conclusion:** FedACS converges to the correct optimum at a rate of O(1/√R) in dynamic heterogeneous environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heterogeneity-Aware+Client+Sampling%3A+A+Unified+Solution+for+Consistent+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11304&send_immediately=true&force_search=false)

**Abstract:** Federated learning (FL) commonly involves clients with diverse communication
and computational capabilities. Such heterogeneity can significantly distort
the optimization dynamics and lead to objective inconsistency, where the global
model converges to an incorrect stationary point potentially far from the
pursued optimum. Despite its critical impact, the joint effect of communication
and computation heterogeneity has remained largely unexplored, due to the
intrinsic complexity of their interaction. In this paper, we reveal the
fundamentally distinct mechanisms through which heterogeneous communication and
computation drive inconsistency in FL. To the best of our knowledge, this is
the first unified theoretical analysis of general heterogeneous FL, offering a
principled understanding of how these two forms of heterogeneity jointly
distort the optimization trajectory under arbitrary choices of local solvers.
Motivated by these insights, we propose Federated Heterogeneity-Aware Client
Sampling, FedACS, a universal method to eliminate all types of objective
inconsistency. We theoretically prove that FedACS converges to the correct
optimum at a rate of $O(1/\sqrt{R})$, even in dynamic heterogeneous
environments. Extensive experiments across multiple datasets show that FedACS
outperforms state-of-the-art and category-specific baselines by 4.3%-36%, while
reducing communication costs by 22%-89% and computation loads by 14%-105%,
respectively.

</details>


### [83] [Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection](https://arxiv.org/abs/2505.11134)
*Desong Zhang, Jia Hu, Geyong Min*

**Main category:** cs.LG

**TL;DR:** A new method named Dominant Eigencomponent Projection (DEP) is proposed to enhance the robustness of Spiking Neural Networks (SNNs) by reducing the Hessian spectral radius.


<details>
  <summary>Details</summary>
**Motivation:** To address the vulnerability of SNNs to heterogeneous data poisoning during training with direct encoding and BPTT.

**Method:** Developing a hyperparameter-free method called DEP to project gradients and remove dominant components.

**Result:** DEP improves the robustness of SNNs against heterogeneous data poisoning and sharp minima.

**Conclusion:** DEP offers a safer and more reliable approach for deploying SNNs.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Spiking+Neural+Networks%3AMitigating+Heterogeneous+Training+Vulnerability+via+Dominant+Eigencomponent+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11134&send_immediately=true&force_search=false)

**Abstract:** Spiking Neural Networks (SNNs) process information via discrete spikes,
enabling them to operate at remarkably low energy levels. However, our
experimental observations reveal a striking vulnerability when SNNs are trained
using the mainstream method--direct encoding combined with backpropagation
through time (BPTT): even a single backward pass on data drawn from a slightly
different distribution can lead to catastrophic network collapse. Our
theoretical analysis attributes this vulnerability to the repeated inputs
inherent in direct encoding and the gradient accumulation characteristic of
BPTT, which together produce an exceptional large Hessian spectral radius. To
address this challenge, we develop a hyperparameter-free method called Dominant
Eigencomponent Projection (DEP). By orthogonally projecting gradients to
precisely remove their dominant components, DEP effectively reduces the Hessian
spectral radius, thereby preventing SNNs from settling into sharp minima.
Extensive experiments demonstrate that DEP not only mitigates the vulnerability
of SNNs to heterogeneous data poisoning, but also significantly enhances
overall robustness compared to key baselines, providing strong support for
safer and more reliable SNN deployment.

</details>


### [84] [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
*Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić*

**Main category:** cs.LG

**TL;DR:** Propose a new paradigm called Visual Planning which enables planning through purely visual representations independent of text.


<details>
  <summary>Details</summary>
**Motivation:** Language may not always be the most natural or effective modality for reasoning, especially in tasks involving spatial and geometrical information.

**Method:** Introduce a novel reinforcement learning framework called Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models.

**Result:** Substantial improvements in planning in representative visual navigation tasks such as FrozenLake, Maze, and MiniBehavior.

**Conclusion:** Visual Planning is a viable and promising alternative to language-based reasoning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Visual+Planning%3A+Let%27s+Think+Only+with+Images，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11409&send_immediately=true&force_search=false)

**Abstract:** Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [85] [Covariance Density Neural Networks](https://arxiv.org/abs/2505.11139)
*Om Roy, Yashar Moshfeghi, Keith Smith*

**Main category:** cs.LG

**TL;DR:** This paper introduces a new approach called Density Matrix to improve the performance of Covariance Neural Networks (VNNs) for modeling signals on network data. The method constructs a Density Matrix considering the sample covariance matrix as a quasi-Hamiltonian, which enhances discriminability and performance.


<details>
  <summary>Details</summary>
**Motivation:** There is a lack of consensus on choosing the correct underlying graph structure for modeling signals on network data. Existing VNNs use the sample covariance matrix as a Graph Shift Operator, but they have limitations in performance and robustness.

**Method:** The authors construct a Density Matrix based on the sample covariance matrix as the Graph Shift Operator, which allows components of the data to be extracted at different scales.

**Result:** The proposed method outperforms VNNs in terms of stability-discriminability trade-off, robustness to noise, and performance in real-life applications such as EEG motor imagery classification. It also surpasses EEGnet in this task.

**Conclusion:** Density Matrix Neural Networks provide a better solution for the transferability of Brain Computer Interface (BCI) tasks when evaluated on unseen individuals.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Covariance+Density+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11139&send_immediately=true&force_search=false)

**Abstract:** Graph neural networks have re-defined how we model and predict on network
data but there lacks a consensus on choosing the correct underlying graph
structure on which to model signals. CoVariance Neural Networks (VNN) address
this issue by using the sample covariance matrix as a Graph Shift Operator
(GSO). Here, we improve on the performance of VNNs by constructing a Density
Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of
the system in the space of random variables. Crucially, using this density
matrix as the GSO allows components of the data to be extracted at different
scales, allowing enhanced discriminability and performance. We show that this
approach allows explicit control of the stability-discriminability trade-off of
the network, provides enhanced robustness to noise compared to VNNs, and
outperforms them in useful real-life applications where the underlying
covariance matrix is informative. In particular, we show that our model can
achieve strong performance in subject-independent Brain Computer Interface EEG
motor imagery classification, outperforming EEGnet while being faster. This
shows how covariance density neural networks provide a basis for the
notoriously difficult task of transferability of BCIs when evaluated on unseen
individuals.

</details>


### [86] [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/abs/2505.11427)
*Adrian Robert Minut, Tommaso Mencattini, Andrea Santilli, Donato Crisostomi, Emanuele Rodolà*

**Main category:** cs.LG

**TL;DR:** Model merging is becoming popular due to its low cost and the availability of libraries. Pairing merging with evolutionary algorithms can improve performance. However, there is no current framework that supports flexible experimentation with such strategies in language models. Mergenetic is introduced as an open-source library for evolutionary model merging, which reduces evaluation costs.


<details>
  <summary>Details</summary>
**Motivation:** To enable flexible experimentation with evolutionary model merging strategies in language models.

**Method:** Introducing Mergenetic, an open-source library for evolutionary model merging that enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators.

**Result:** Mergenetic produces competitive results across tasks and languages using modest hardware.

**Conclusion:** Mergenetic is a useful tool for evolutionary model merging in language models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mergenetic%3A+a+Simple+Evolutionary+Model+Merging+Library，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11427&send_immediately=true&force_search=false)

**Abstract:** Model merging allows combining the capabilities of existing models into a new
one - post hoc, without additional training. This has made it increasingly
popular thanks to its low cost and the availability of libraries that support
merging on consumer GPUs. Recent work shows that pairing merging with
evolutionary algorithms can boost performance, but no framework currently
supports flexible experimentation with such strategies in language models. We
introduce Mergenetic, an open-source library for evolutionary model merging.
Mergenetic enables easy composition of merging methods and evolutionary
algorithms while incorporating lightweight fitness estimators to reduce
evaluation costs. We describe its design and demonstrate that Mergenetic
produces competitive results across tasks and languages using modest hardware.

</details>


### [87] [Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes](https://arxiv.org/abs/2505.11153)
*Ashok Arora, Neetesh Kumar*

**Main category:** cs.LG

**TL;DR:** This paper proposes a new bi-recurrent model architecture for POMDPs that improves sample efficiency and reduces parameters.


<details>
  <summary>Details</summary>
**Motivation:** To improve sample efficiency and reduce model parameter count in POMDP scenarios.

**Method:** A single layer of bi-directional recurrence unit replacing multiple feed forward layers.

**Result:** Outperforms existing methods by a significant margin.

**Conclusion:** The proposed bi-recurrent model architecture outperforms existing methods in POMDP environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bi-directional+Recurrence+Improves+Transformer+in+Partially+Observable+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11153&send_immediately=true&force_search=false)

**Abstract:** In real-world reinforcement learning (RL) scenarios, agents often encounter
partial observability, where incomplete or noisy information obscures the true
state of the environment. Partially Observable Markov Decision Processes
(POMDPs) are commonly used to model these environments, but effective
performance requires memory mechanisms to utilise past observations. While
recurrence networks have traditionally addressed this need, transformer-based
models have recently shown improved sample efficiency in RL tasks. However,
their application to POMDPs remains underdeveloped, and their real-world
deployment is constrained due to the high parameter count. This work introduces
a novel bi-recurrent model architecture that improves sample efficiency and
reduces model parameter count in POMDP scenarios. The architecture replaces the
multiple feed forward layers with a single layer of bi-directional recurrence
unit to better capture and utilize sequential dependencies and contextual
information. This approach improves the model's ability to handle partial
observability and increases sample efficiency, enabling effective learning from
comparatively fewer interactions. To evaluate the performance of the proposed
model architecture, experiments were conducted on a total of 23 POMDP
environments. The proposed model architecture outperforms existing
transformer-based, attention-based, and recurrence-based methods by a margin
ranging from 87.39% to 482.04% on average across the 23 POMDP environments.

</details>


### [88] [Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training](https://arxiv.org/abs/2505.11170)
*Myeonghwan Ahn, Sungjoo Yoo*

**Main category:** cs.LG

**TL;DR:** This paper explores the practical implications of pseudo-quantization training (PQT), proposing a noise distribution R that is floating-point-friendly with ideal properties. It demonstrates that PQT with Gaussian weight sampling is scalable, efficient, and stable.


<details>
  <summary>Details</summary>
**Motivation:** To address the issues of fully quantized training (FQT) such as consistency challenges and the need for searching over an exponential number of cases.

**Method:** Proposing a noise distribution R that is floating-point-friendly, using efficient fake quantization via addition and subsequent FP casting.

**Result:** Gaussian weight sampling supports low-precision FP parameters down to FP6 and high-precision noise up to 9-bit with BF16 operator, incurs low computational overhead on A100 GPU, and requires 2 bytes per parameter in GPU memory. It also closely follows or surpasses the performance of the BF16 baseline while pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.

**Conclusion:** The proposed method serves as an effective theoretical foundation for low-precision FP parameters through PQT.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gaussian+Weight+Sampling+for+Scalable%2C+Efficient+and+Stable+Pseudo-Quantization+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11170&send_immediately=true&force_search=false)

**Abstract:** Ever-growing scale of large language models (LLMs) is pushing for improved
efficiency, favoring fully quantized training (FQT) over BF16. While FQT
accelerates training, it faces consistency challenges and requires searching
over an exponential number of cases, each needing over 200B tokens to ensure
stability.
  Pseudo-quantization training (PQT) addresses the issues of FQT, although it
is not well-studied. We explore the practical implications of PQT in detail and
propose a noise distribution $R$ that is floating-point (FP)-friendly, with
ideal properties including stochastic precision annealing. As a result, the
proposed method serves as an effective theoretical foundation for low-precision
FP parameters through PQT, utilizing efficient fake quantization via an
addition and subsequent FP casting.
  We demonstrate that Gaussian weight sampling is (1) scalable: supports
low-precision FP parameters down to FP6 and high-precision noise up to 9-bit
with BF16 operator. The proposed method is (2) efficient: incurring
computational overhead as low as 1.40\% on the A100 GPU in terms of Llama2
training tokens per second, and requiring 2 bytes per parameter in GPU memory.
We demonstrate that PQT with Gaussian weight sampling is (3) stable: closely
following or even surpassing performance of the BF16 baseline while
pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.

</details>


### [89] [VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks](https://arxiv.org/abs/2505.11185)
*Francesco Madeddu, Lucia Testa, Gianluca De Carlo, Michele Pieroni, Andrea Mastropietro, Aris Anagnostopoulos, Paolo Tieri, Sergio Barbarossa*

**Main category:** cs.LG

**TL;DR:** 构建了一个综合的生物知识图谱，用于提升计算生物学和精准医学的研究能力，并在药物重定位等任务上进行了验证。


<details>
  <summary>Details</summary>
**Motivation:** 人类生物学固有的复杂性对科学理解提出了持续的挑战，需要跨学科合作来扩展对定义人类生命的生物相互作用的知识。AI方法在科学领域，尤其是计算生物学中成为强大的工具。

**Method:** 通过整合和优化多个公开可用的数据集，特别是基于Drug Repurposing Knowledge Graph (DRKG)，清理不一致性与冗余信息，合并公共数据源信息，并丰富节点特征向量。

**Result:** 构建了可靠且连贯的生物知识图谱，可以作为最先进的平台推进计算生物学和精准医学的研究，并在药物重定位、蛋白质-蛋白质相互作用预测和副作用预测等任务上验证了其有效性。

**Conclusion:** 该研究构建了一个全面的多用途生物知识图谱，用于提高计算生物学和精准医学领域的研究。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VitaGraph%3A+Building+a+Knowledge+Graph+for+Biologically+Relevant+Learning+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11185&send_immediately=true&force_search=false)

**Abstract:** The intrinsic complexity of human biology presents ongoing challenges to
scientific understanding. Researchers collaborate across disciplines to expand
our knowledge of the biological interactions that define human life. AI
methodologies have emerged as powerful tools across scientific domains,
particularly in computational biology, where graph data structures effectively
model biological entities such as protein-protein interaction (PPI) networks
and gene functional networks. Those networks are used as datasets for paramount
network medicine tasks, such as gene-disease association prediction, drug
repurposing, and polypharmacy side effect studies. Reliable predictions from
machine learning models require high-quality foundational data. In this work,
we present a comprehensive multi-purpose biological knowledge graph constructed
by integrating and refining multiple publicly available datasets. Building upon
the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with
a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing
information from the main available public data sources, and c) enriching the
graph nodes with expressive feature vectors such as molecular fingerprints and
gene ontologies. Biologically and chemically relevant features improve the
capacity of machine learning models to generate accurate and well-structured
embedding spaces. The resulting resource represents a coherent and reliable
biological knowledge graph that serves as a state-of-the-art platform to
advance research in computational biology and precision medicine. Moreover, it
offers the opportunity to benchmark graph-based machine learning and network
medicine models on relevant tasks. We demonstrate the effectiveness of the
proposed dataset by benchmarking it against the task of drug repurposing, PPI
prediction, and side-effect prediction, modeled as link prediction problems.

</details>


### [90] [Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schrödinger Bridge](https://arxiv.org/abs/2505.11197)
*Zhenyi Zhang, Zihan Wang, Yuhao Sun, Tiejun Li, Peijie Zhou*

**Main category:** cs.LG

**TL;DR:** A new method called CytoBridge is proposed to model unbalanced stochastic interaction dynamics from snapshot data.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods have limitations in accounting for cell-cell interactions which are important in real-world scenarios.

**Method:** Formulating the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework and proposing CytoBridge, a deep learning algorithm to approximate the UMFSB problem.

**Result:** The method has been validated using synthetic gene regulatory data and real scRNA-seq datasets, showing improved accuracy in identifying growth, transition, and interaction patterns and eliminating false transitions.

**Conclusion:** CytoBridge provides a flexible way to learn cellular transitions, proliferation, and interactions directly from data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Cell+Dynamics+and+Interactions+with+Unbalanced+Mean+Field+Schr%C3%B6dinger+Bridge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11197&send_immediately=true&force_search=false)

**Abstract:** Modeling the dynamics from sparsely time-resolved snapshot data is crucial
for understanding complex cellular processes and behavior. Existing methods
leverage optimal transport, Schr\"odinger bridge theory, or their variants to
simultaneously infer stochastic, unbalanced dynamics from snapshot data.
However, these approaches remain limited in their ability to account for
cell-cell interactions. This integration is essential in real-world scenarios
since intercellular communications are fundamental life processes and can
influence cell state-transition dynamics. To address this challenge, we
formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to
model unbalanced stochastic interaction dynamics from snapshot data. Inspired
by this framework, we further propose CytoBridge, a deep learning algorithm
designed to approximate the UMFSB problem. By explicitly modeling cellular
transitions, proliferation, and interactions through neural networks,
CytoBridge offers the flexibility to learn these processes directly from data.
The effectiveness of our method has been extensively validated using both
synthetic gene regulatory data and real scRNA-seq datasets. Compared to
existing methods, CytoBridge identifies growth, transition, and interaction
patterns, eliminates false transitions, and reconstructs the developmental
landscape with greater accuracy.

</details>


### [91] [Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation](https://arxiv.org/abs/2505.11221)
*Donghoon Lee, Tung M. Luu, Younghwan Lee, Chang D. Yoo*

**Main category:** cs.LG

**TL;DR:** A new framework called LVLM2P is introduced to make reinforcement learning more efficient by using large vision-language models.


<details>
  <summary>Details</summary>
**Motivation:** To tackle the challenges of large parameters in multimodal foundation models and high sample complexity in task-specific agents.

**Method:** Distilling knowledge from large vision-language models into reinforcement learning agents.

**Result:** The proposed method significantly improves the sample efficiency of baseline reinforcement learning algorithms.

**Conclusion:** LVLM2P provides an effective way to accelerate the learning progress of reinforcement learning agents.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Efficient+Reinforcement+Learning+via+Large+Vision+Language+Model+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11221&send_immediately=true&force_search=false)

**Abstract:** Recent research highlights the potential of multimodal foundation models in
tackling complex decision-making challenges. However, their large parameters
make real-world deployment resource-intensive and often impractical for
constrained systems. Reinforcement learning (RL) shows promise for
task-specific agents but suffers from high sample complexity, limiting
practical applications. To address these challenges, we introduce LVLM to
Policy (LVLM2P), a novel framework that distills knowledge from large
vision-language models (LVLM) into more efficient RL agents. Our approach
leverages the LVLM as a teacher, providing instructional actions based on
trajectories collected by the RL agent, which helps reduce less meaningful
exploration in the early stages of learning, thereby significantly accelerating
the agent's learning progress. Additionally, by leveraging the LVLM to suggest
actions directly from visual observations, we eliminate the need for manual
textual descriptors of the environment, enhancing applicability across diverse
tasks. Experiments show that LVLM2P significantly enhances the sample
efficiency of baseline RL algorithms.

</details>


### [92] [Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment](https://arxiv.org/abs/2505.11230)
*Oskar Bohn Lassen, Serio Agriesti, Mohamed Eldafrawi, Daniele Gammelli, Guido Cantelmo, Guido Gentile, Francisco Camara Pereira*

**Main category:** cs.LG

**TL;DR:** We propose a learning-based approach using MPNNs to approximate the equilibrium flow of SUE assignment, aiming to accelerate scenario analysis and reduce computational costs.


<details>
  <summary>Details</summary>
**Motivation:** Traditional methods require iterative simulations to reach equilibrium, which makes real-time or large-scale scenario analysis difficult.

**Method:** A learning-based approach using Message-Passing Neural Networks as a metamodel to approximate the equilibrium flow of the Stochastic User Equilibrium assignment.

**Result:** Our model can better capture the underlying process rather than just the data, and it is evaluated against other conventional deep learning techniques. Its robustness is tested by predicting traffic flows on input data outside the domain on which it was trained.

**Conclusion:** This approach provides an effective method to speed up out-of-distribution scenario evaluations, decrease computational expenses in large-scale transportation planning, and support real-time decision-making.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+traffic+flows%3A+Graph+Neural+Networks+for+Metamodelling+Traffic+Assignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11230&send_immediately=true&force_search=false)

**Abstract:** The Traffic Assignment Problem is a fundamental, yet computationally
expensive, task in transportation modeling, especially for large-scale
networks. Traditional methods require iterative simulations to reach
equilibrium, making real-time or large-scale scenario analysis challenging. In
this paper, we propose a learning-based approach using Message-Passing Neural
Networks as a metamodel to approximate the equilibrium flow of the Stochastic
User Equilibrium assignment. Our model is designed to mimic the algorithmic
structure used in conventional traffic simulators allowing it to better capture
the underlying process rather than just the data. We benchmark it against other
conventional deep learning techniques and evaluate the model's robustness by
testing its ability to predict traffic flows on input data outside the domain
on which it was trained. This approach offers a promising solution for
accelerating out-of-distribution scenario assessments, reducing computational
costs in large-scale transportation planning, and enabling real-time
decision-making.

</details>


### [93] [Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation](https://arxiv.org/abs/2505.11235)
*Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang*

**Main category:** cs.LG

**TL;DR:** This paper introduces Memory-efficient Orthogonal Fine-Tuning (MOFT) for large models, which enhances performance on various tasks while reducing memory usage compared to traditional methods.


<details>
  <summary>Details</summary>
**Motivation:** The high cost of full fine-tuning of large models necessitates parameter-efficient fine-tuning approaches like orthogonal fine-tuning, but existing methods are memory-inefficient.

**Method:** Proposes MOFT with principal subspace adaptation, constraining orthogonal fine-tuning to a low-rank subspace and using learnable scaling vectors for flexibility.

**Result:** Demonstrates superior performance on 37 diverse tasks and across four models in NLP and CV, with significant memory savings.

**Conclusion:** MOFT achieves better results than baselines while being more memory-efficient, making it a promising method for parameter-efficient fine-tuning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-Efficient+Orthogonal+Fine-Tuning+with+Principal+Subspace+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11235&send_immediately=true&force_search=false)

**Abstract:** Driven by the relentless growth in model parameters, which renders full
fine-tuning prohibitively expensive for large-scale deployment,
parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
rapidly adapting large models to a wide range of downstream tasks. Among the
PEFT family, orthogonal fine-tuning and its variants have demonstrated
remarkable performance by preserving hyperspherical energy, which encodes
pairwise angular similarity between neurons. However, these methods are
inherently memory-inefficient due to the need to store intermediate activations
from multiple full-dimensional sparse matrices. To address this limitation, we
propose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace
adaptation. Specifically, we first establish a theoretical condition under
which orthogonal transformations within a low-rank subspace preserve
hyperspherical energy. Based on this insight, we constrain orthogonal
fine-tuning to the principal subspace defined by the top-r components obtained
through singular value decomposition and impose an additional constraint on the
projection matrix to satisfy the preservation condition. To enhance MOFT's
flexibility across tasks, we relax strict orthogonality by introducing two
learnable scaling vectors. Extensive experiments on 37 diverse tasks and four
models across NLP and CV demonstrate that MOFT consistently outperforms key
baselines while significantly reducing the memory footprint of orthogonal
fine-tuning.

</details>


### [94] [Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks](https://arxiv.org/abs/2505.11239)
*Wilson Wongso, Hao Xue, Flora D. Salim*

**Main category:** cs.LG

**TL;DR:** A large-scale publicly available benchmark dataset named Massive-STEPS is introduced to address challenges in understanding human mobility through POI recommendation.


<details>
  <summary>Details</summary>
**Motivation:** The need for more recent and diverse city-level check-in datasets to improve urban planning, personalized services, and generative agent simulation.

**Method:** Building a large-scale benchmark dataset based on the Semantic Trails dataset and enriching it with semantic POI metadata.

**Result:** Massive-STEPS spans 12 diverse cities with more recent and longer-duration check-in data than previous datasets.

**Conclusion:** The release of Massive-STEPS aims to promote reproducible and equitable research in human mobility and POI recommendation.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Massive-STEPS%3A+Massive+Semantic+Trajectories+for+Understanding+POI+Check-ins+--+Dataset+and+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11239，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11239&send_immediately=true&force_search=false)

**Abstract:** Understanding human mobility through Point-of-Interest (POI) recommendation
is increasingly important for applications such as urban planning, personalized
services, and generative agent simulation. However, progress in this field is
hindered by two key challenges: the over-reliance on older datasets from
2012-2013 and the lack of reproducible, city-level check-in datasets that
reflect diverse global regions. To address these gaps, we present Massive-STEPS
(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,
publicly available benchmark dataset built upon the Semantic Trails dataset and
enriched with semantic POI metadata. Massive-STEPS spans 12 geographically and
culturally diverse cities and features more recent (2017-2018) and
longer-duration (24 months) check-in data than prior datasets. We benchmarked a
wide range of POI recommendation models on Massive-STEPS using both supervised
and zero-shot approaches, and evaluated their performance across multiple urban
contexts. By releasing Massive-STEPS, we aim to facilitate reproducible and
equitable research in human mobility and POI recommendation. The dataset and
benchmarking code are available at:
https://github.com/cruiseresearchgroup/Massive-STEPS

</details>


### [95] [Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline](https://arxiv.org/abs/2505.11250)
*Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, Bin Yang*

**Main category:** cs.LG

**TL;DR:** This paper presents APN, a general framework for predicting irregular multivariate time series (IMTS). It introduces a Time-Aware Patch Aggregation (TAPA) module for adaptive patching and a query module for integrating historical information, resulting in improved efficiency and accuracy compared to existing methods.


<details>
  <summary>Details</summary>
**Motivation:** The irregularity and missing data in IMTS pose challenges for modeling, and existing methods are often complex and resource-intensive.

**Method:** APN with a TAPA module for adaptive patching and a query module for historical information integration, followed by predictions via a shallow MLP.

**Result:** APN outperforms state-of-the-art methods in efficiency and accuracy on multiple real-world datasets.

**Conclusion:** APN provides an effective and efficient solution for predicting irregular multivariate time series.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Irregular+Time+Series+Forecasting%3A+A+Simple+yet+Effective+Baseline，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11250&send_immediately=true&force_search=false)

**Abstract:** The forecasting of irregular multivariate time series (IMTS) is crucial in
key areas such as healthcare, biomechanics, climate science, and astronomy.
However, achieving accurate and practical predictions is challenging due to two
main factors. First, the inherent irregularity and data missingness in
irregular time series make modeling difficult. Second, most existing methods
are typically complex and resource-intensive. In this study, we propose a
general framework called APN to address these challenges. Specifically, we
design a novel Time-Aware Patch Aggregation (TAPA) module that achieves
adaptive patching. By learning dynamically adjustable patch boundaries and a
time-aware weighted averaging strategy, TAPA transforms the original irregular
sequences into high-quality, regularized representations in a
channel-independent manner. Additionally, we use a simple query module to
effectively integrate historical information while maintaining the model's
efficiency. Finally, predictions are made by a shallow MLP. Experimental
results on multiple real-world datasets show that APN outperforms existing
state-of-the-art methods in both efficiency and accuracy.

</details>


### [96] [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254)
*Jeffrey Willette, Heejun Lee, Sung Ju Hwang*

**Main category:** cs.LG

**TL;DR:** Propose a method to fix the distributional shift issue in sparse attention, improving performance significantly without losing much speed or sparsity.


<details>
  <summary>Details</summary>
**Motivation:** Reduce the computational burden of transformers' attention mechanism without sacrificing performance.

**Method:** A procedure to correct the distributional shift in sparse attention outputs.

**Result:** An average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the RULER benchmark, while maintaining 98.5% sparsity and being 32 times faster than Flash Attention 2.

**Conclusion:** We propose a method to correct the distributional shift in sparse attention outputs, which improves performance significantly while maintaining high sparsity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Delta+Attention%3A+Fast+and+Accurate+Sparse+Attention+Inference+by+Delta+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11254&send_immediately=true&force_search=false)

**Abstract:** The attention mechanism of a transformer has a quadratic complexity, leading
to high inference costs and latency for long sequences. However, attention
matrices are mostly sparse, which implies that many entries may be omitted from
computation for efficient inference. Sparse attention inference methods aim to
reduce this computational burden; however, they also come with a troublesome
performance degradation. We discover that one reason for this degradation is
that the sparse calculation induces a distributional shift in the attention
outputs. The distributional shift causes decoding-time queries to fail to align
well with the appropriate keys from the prefill stage, leading to a drop in
performance. We propose a simple, novel, and effective procedure for correcting
this distributional shift, bringing the distribution of sparse attention
outputs closer to that of quadratic attention. Our method can be applied on top
of any sparse attention method, and results in an average 36%pt performance
increase, recovering 88% of quadratic attention accuracy on the 131K RULER
benchmark when applied on top of sliding window attention with sink tokens
while only adding a small overhead. Our method can maintain approximately 98.5%
sparsity over full quadratic attention, making our model 32 times faster than
Flash Attention 2 when processing 1M token prefills.

</details>


### [97] [Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion](https://arxiv.org/abs/2505.11261)
*Jingyang Li, Jiuqian Shang, Yang Chen*

**Main category:** cs.LG

**TL;DR:** Proposes FLoST, a novel tensor completion model using Fourier decomposition to capture spatiotemporal patterns, demonstrating superior performance over existing models.


<details>
  <summary>Details</summary>
**Motivation:** Existing tensor models fail to capture unique spatiotemporal patterns in scientific data due to symmetric treatment of tensor modes.

**Method:** Introduces FLoST, which uses Fourier transform along the temporal dimension to separate low-frequency components with low-rank matrices and high-frequency fluctuations with sparsity.

**Result:** FLoST shows better accuracy and computational efficiency compared to traditional tensor completion models.

**Conclusion:** FLoST provides a more efficient and interpretable solution for spatiotemporal data reconstruction.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fourier+Low-rank+and+Sparse+Tensor+for+Efficient+Tensor+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11261&send_immediately=true&force_search=false)

**Abstract:** Tensor completion is crucial in many scientific domains with missing data
problems. Traditional low-rank tensor models, including CP, Tucker, and
Tensor-Train, exploit low-dimensional structures to recover missing data.
However, these methods often treat all tensor modes symmetrically, failing to
capture the unique spatiotemporal patterns inherent in scientific data, where
the temporal component exhibits both low-frequency stability and high-frequency
variations. To address this, we propose a novel model, \underline{F}ourier
\underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which
decomposes the tensor along the temporal dimension using a Fourier transform.
This approach captures low-frequency components with low-rank matrices and
high-frequency fluctuations with sparsity, resulting in a hybrid structure that
efficiently models both smooth and localized variations. Compared to the
well-known tubal-rank model, which assumes low-rankness across all frequency
components, FLoST requires significantly fewer parameters, making it
computationally more efficient, particularly when the time dimension is large.
Through theoretical analysis and empirical experiments, we demonstrate that
FLoST outperforms existing tensor completion models in terms of both accuracy
and computational efficiency, offering a more interpretable solution for
spatiotemporal data reconstruction.

</details>


### [98] [Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach](https://arxiv.org/abs/2505.11269)
*Shengjia Chang, Xianshuo Yue*

**Main category:** cs.LG

**TL;DR:** This study develops a hybrid model combining ARIMA, Random Forest, and Holt-Winters to enhance the accuracy of pet population forecasts in China, identifying key drivers such as urban income, consumption, and policy quantity.


<details>
  <summary>Details</summary>
**Motivation:** To improve the accuracy of China's pet population forecasting by integrating multiple models and analyzing key drivers.

**Method:** Developing a hybrid model integrating ARIMA for seasonality and trends, Random Forest for nonlinear features, and Holt-Winters smoothing for seasonal adjustment, using 2005-2023 data with nine economic, social, and policy indicators, and preprocessing data via Z-score normalization and missing value imputation.

**Result:** Key drivers of pet populations include urban income, consumption, and policy quantity, with aging and urbanization amplifying pet demand. Cat growth is steady while dog numbers fluctuate, indicating cats' better adaptability to urban environments.

**Conclusion:** This research supports policymakers in optimizing pet health management and guides enterprises in developing differentiated services, promoting sustainable industry growth.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Driving+Mechanisms+and+Forecasting+of+China%27s+Pet+Population-An+ARIMA-RF-HW+Hybrid+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11269，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11269&send_immediately=true&force_search=false)

**Abstract:** This study proposes a dynamically weighted ARIMA-RF-HW hybrid model
integrating ARIMA for seasonality and trends, Random Forest for nonlinear
features, and Holt-Winters smoothing for seasonal adjustment to improve China's
pet population forecasting accuracy. Using 2005-2023 data with nine economic,
social, and policy indicators (urban income, consumption, aging ratio, policy
quantity, new veterinary drug approvals), data were preprocessed via Z-score
normalization and missing value imputation. The results show that key drivers
of pet populations include urban income (19.48% for cats, 17.15% for dogs),
consumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for
dogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization
amplifying the demand for pets. Forecasts show steady cat growth and
fluctuating dog numbers, reflecting cats' adaptability to urban environments.
This research supports policymakers in optimizing pet health management and
guides enterprises in developing differentiated services, advancing sustainable
industry growth.

</details>


### [99] [Multiclass threshold-based classification](https://arxiv.org/abs/2505.11276)
*Francesco Marchetti, Edoardo Legnaro, Sabrina Guastavino*

**Main category:** cs.LG

**TL;DR:** Introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule.


<details>
  <summary>Details</summary>
**Motivation:** To replace the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex.

**Method:** Use a multidimensional threshold for classification and define score-oriented losses.

**Result:** The method improves prediction capability of networks and shows consistent performance enhancements across different networks and datasets.

**Conclusion:** The threshold-based approach and score-oriented losses perform well and have advantages similar to those in binary classification.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiclass+threshold-based+classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11276&send_immediately=true&force_search=false)

**Abstract:** In this paper, we introduce a threshold-based framework for multiclass
classification that generalizes the standard argmax rule. This is done by
replacing the probabilistic interpretation of softmax outputs with a geometric
one on the multidimensional simplex, where the classification depends on a
multidimensional threshold. This change of perspective enables for any trained
classification network an a posteriori optimization of the classification score
by means of threshold tuning, as usually carried out in the binary setting.
This allows a further refinement of the prediction capability of any network.
Moreover, this multidimensional threshold-based setting makes it possible to
define score-oriented losses, which are based on the interpretation of the
threshold as a random variable. Our experiments show that the multidimensional
threshold tuning yields consistent performance improvements across various
networks and datasets, and that the proposed multiclass score-oriented losses
are competitive with standard loss functions, resembling the advantages
observed in the binary case.

</details>


### [100] [SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers](https://arxiv.org/abs/2505.11283)
*Tom Siegl, Kutalmış Coşkun, Bjarne Hiller, Amin Mirzaei, Florian Lemmerich, Martin Becker*

**Main category:** cs.LG

**TL;DR:** Introduce SubROC, an open-source framework for efficiently finding strengths and weaknesses of classification models through interpretable subgroups.


<details>
  <summary>Details</summary>
**Motivation:** To identify and describe subgroups where ML models underperform or are particularly accurate, supporting practical decisions on model deployment and training data requirement.

**Method:** Introducing SubROC, an open-source, easy-to-use framework based on Exceptional Model Mining.

**Result:** SubROC incorporates common evaluation measures, efficient search space pruning, control for class imbalance, adjustment for redundant patterns, and significance testing. It has practical benefits shown in case studies and comparative analyses across multiple datasets.

**Conclusion:** SubROC is an effective tool for identifying and analyzing the strengths and weaknesses of classification models in terms of interpretable population subgroups.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SubROC%3A+AUC-Based+Discovery+of+Exceptional+Subgroup+Performance+for+Binary+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11283&send_immediately=true&force_search=false)

**Abstract:** Machine learning (ML) is increasingly employed in real-world applications
like medicine or economics, thus, potentially affecting large populations.
However, ML models often do not perform homogeneously across such populations
resulting in subgroups of the population (e.g., sex=female AND
marital_status=married) where the model underperforms or, conversely, is
particularly accurate. Identifying and describing such subgroups can support
practical decisions on which subpopulation a model is safe to deploy or where
more training data is required. The potential of identifying and analyzing such
subgroups has been recognized, however, an efficient and coherent framework for
effective search is missing. Consequently, we introduce SubROC, an open-source,
easy-to-use framework based on Exceptional Model Mining for reliably and
efficiently finding strengths and weaknesses of classification models in the
form of interpretable population subgroups. SubROC incorporates common
evaluation measures (ROC and PR AUC), efficient search space pruning for fast
exhaustive subgroup search, control for class imbalance, adjustment for
redundant patterns, and significance testing. We illustrate the practical
benefits of SubROC in case studies as well as in comparative analyses across
multiple datasets.

</details>


### [101] [Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization](https://arxiv.org/abs/2505.11294)
*Juan D. Guerra, Thomas Garbay, Guillaume Lajoie, Marco Bonizzato*

**Main category:** cs.LG

**TL;DR:** This paper proposes Bidirectional Information Flow (BIF), an efficient H-GP framework that establishes bidirectional information exchange between parent and child models in H-GPs for online training.


<details>
  <summary>Details</summary>
**Motivation:** typical H-GP models do not fully take advantage of this structure, only sending information up or down the hierarchy. This one-way coupling limits sample efficiency and slows convergence

**Method:** Bidirectional Information Flow (BIF)

**Result:** BIF retains the modular structure of hierarchical models - the parent combines subtask knowledge from children GPs - while introducing top-down feedback to continually refine children models during online learning. This mutual exchange improves sample efficiency, enables robust training, and allows modular reuse of learned subtask models.

**Conclusion:** BIF outperforms conventional H-GP Bayesian Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the parent and children respectively, on synthetic and real-world neurostimulation optimization tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bidirectional+Information+Flow+%28BIF%29+--+A+Sample+Efficient+Hierarchical+Gaussian+Process+for+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11294，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11294&send_immediately=true&force_search=false)

**Abstract:** Hierarchical Gaussian Process (H-GP) models divide problems into different
subtasks, allowing for different models to address each part, making them
well-suited for problems with inherent hierarchical structure. However, typical
H-GP models do not fully take advantage of this structure, only sending
information up or down the hierarchy. This one-way coupling limits sample
efficiency and slows convergence. We propose Bidirectional Information Flow
(BIF), an efficient H-GP framework that establishes bidirectional information
exchange between parent and child models in H-GPs for online training. BIF
retains the modular structure of hierarchical models - the parent combines
subtask knowledge from children GPs - while introducing top-down feedback to
continually refine children models during online learning. This mutual exchange
improves sample efficiency, enables robust training, and allows modular reuse
of learned subtask models. BIF outperforms conventional H-GP Bayesian
Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the
parent and children respectively, on synthetic and real-world neurostimulation
optimization tasks.

</details>


### [102] [Graph Representational Learning: When Does More Expressivity Hurt Generalization?](https://arxiv.org/abs/2505.11298)
*Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer*

**Main category:** cs.LG

**TL;DR:** This paper explores the connection between the expressivity of Graph Neural Networks (GNNs) and their predictive performance. It introduces a family of premetrics to measure structural similarity among graphs and relates these similarities to generalization and GNN performance.


<details>
  <summary>Details</summary>
**Motivation:** Understanding how the expressivity of GNNs affects their ability to generalize and perform well on unseen data.

**Method:** Introduces premetrics to capture structural similarity between graphs and derives generalization bounds based on the distance between training and test graphs, model complexity, and training set size.

**Result:** Found that more expressive GNNs may generalize worse unless their complexity is balanced by a larger training set or reduced distance between training and test graphs.

**Conclusion:** The study offers theoretical insights into the relationship between expressivity and generalization in GNNs, supported by empirical evidence.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Representational+Learning%3A+When+Does+More+Expressivity+Hurt+Generalization%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11298&send_immediately=true&force_search=false)

**Abstract:** Graph Neural Networks (GNNs) are powerful tools for learning on structured
data, yet the relationship between their expressivity and predictive
performance remains unclear. We introduce a family of premetrics that capture
different degrees of structural similarity between graphs and relate these
similarities to generalization, and consequently, the performance of expressive
GNNs. By considering a setting where graph labels are correlated with
structural features, we derive generalization bounds that depend on the
distance between training and test graphs, model complexity, and training set
size. These bounds reveal that more expressive GNNs may generalize worse unless
their increased complexity is balanced by a sufficiently large training set or
reduced distance between training and test graphs. Our findings relate
expressivity and generalization, offering theoretical insights supported by
empirical results.

</details>


### [103] [Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion](https://arxiv.org/abs/2505.11306)
*Xinyan Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu*

**Main category:** cs.LG

**TL;DR:** Proposes FALDA, a new probabilistic framework for time series forecasting using DMRR and Fourier-based decomposition.


<details>
  <summary>Details</summary>
**Motivation:** To improve time series forecasting by reducing uncertainty and enhancing computational efficiency.

**Method:** Introduces DMRR framework and uses Fourier-based decomposition with a conditional diffusion model and lightweight denoiser DEMA.

**Result:** Outperforms existing probabilistic forecasting approaches and even point forecasting approaches on most datasets for long-term forecasting.

**Conclusion:** FALDA effectively reduces epistemic uncertainty and achieves high accuracy with enhanced computational efficiency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Effective+Probabilistic+Time+Series+Forecasting+with+Fourier+Adaptive+Noise-Separated+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11306&send_immediately=true&force_search=false)

**Abstract:** We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel
probabilistic framework for time series forecasting. First, we introduce the
Diffusion Model for Residual Regression (DMRR) framework, which unifies
diffusion-based probabilistic regression methods. Within this framework, FALDA
leverages Fourier-based decomposition to incorporate a component-specific
architecture, enabling tailored modeling of individual temporal components. A
conditional diffusion model is utilized to estimate the future noise term,
while our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),
conditions on the historical noise term to enhance denoising performance.
Through mathematical analysis and empirical validation, we demonstrate that
FALDA effectively reduces epistemic uncertainty, allowing probabilistic
learning to primarily focus on aleatoric uncertainty. Experiments on six
real-world benchmarks demonstrate that FALDA consistently outperforms existing
probabilistic forecasting approaches across most datasets for long-term time
series forecasting while achieving enhanced computational efficiency without
compromising accuracy. Notably, FALDA also achieves superior overall
performance compared to state-of-the-art (SOTA) point forecasting approaches,
with improvements of up to 9%.

</details>


### [104] [Diffusion Learning with Partial Agent Participation and Local Updates](https://arxiv.org/abs/2505.11307)
*Elsa Rizk, Kun Yuan, Ali H. Sayed*

**Main category:** cs.LG

**TL;DR:** This paper proposes an improved diffusion learning method by reducing communication frequency and including partially available agents, proving its stability and conducting numerical experiments.


<details>
  <summary>Details</summary>
**Motivation:** Traditional diffusion learning suffers from high communication overhead and reliability issues due to volatile edge devices, especially with large models.

**Method:** Enhanced diffusion learning approach with local updates and partial agent participation.

**Result:** The proposed algorithm is stable in mean-square error sense, with MSD performance analyzed tightly.

**Conclusion:** This work provides a solution to reduce communication overhead and improve reliability in diffusion learning, validated by numerical experiments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Learning+with+Partial+Agent+Participation+and+Local+Updates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11307&send_immediately=true&force_search=false)

**Abstract:** Diffusion learning is a framework that endows edge devices with advanced
intelligence. By processing and analyzing data locally and allowing each agent
to communicate with its immediate neighbors, diffusion effectively protects the
privacy of edge devices, enables real-time response, and reduces reliance on
central servers. However, traditional diffusion learning relies on
communication at every iteration, leading to communication overhead, especially
with large learning models. Furthermore, the inherent volatility of edge
devices, stemming from power outages or signal loss, poses challenges to
reliable communication between neighboring agents. To mitigate these issues,
this paper investigates an enhanced diffusion learning approach incorporating
local updates and partial agent participation. Local updates will curtail
communication frequency, while partial agent participation will allow for the
inclusion of agents based on their availability. We prove that the resulting
algorithm is stable in the mean-square error sense and provide a tight analysis
of its Mean-Square-Deviation (MSD) performance. Various numerical experiments
are conducted to illustrate our theoretical findings.

</details>


### [105] [Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data](https://arxiv.org/abs/2505.11308)
*Lothar Heimbach, Sebastian Kaltenbach, Petr Karnakov, Francis J. Alexander, Petros Koumoutsakos*

**Main category:** cs.LG

**TL;DR:** This paper introduces a method that uses reinforcement learning with synthetic data to create closure models for simplifying partial differential equations while maintaining accuracy.


<details>
  <summary>Details</summary>
**Motivation:** Developing closure models for PDEs to reduce computational costs while minimizing loss of detail.

**Method:** Using the method of manufactured solutions to generate synthetic data and applying reinforcement learning to develop closure models for coarse-grained PDEs.

**Result:** The method was effective when tested on one-dimensional and two-dimensional Burgers' equations and the two-dimensional advection equation. It also showed that models trained for inhomogeneous PDEs could generalize to homogeneous ones.

**Conclusion:** This approach has potential for creating accurate and efficient closure models for systems with limited data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+Closures+for+Underresolved+Partial+Differential+Equations+using+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11308&send_immediately=true&force_search=false)

**Abstract:** Partial Differential Equations (PDEs) describe phenomena ranging from
turbulence and epidemics to quantum mechanics and financial markets. Despite
recent advances in computational science, solving such PDEs for real-world
applications remains prohibitively expensive because of the necessity of
resolving a broad range of spatiotemporal scales. In turn, practitioners often
rely on coarse-grained approximations of the original PDEs, trading off
accuracy for reduced computational resources. To mitigate the loss of detail
inherent in such approximations, closure models are employed to represent
unresolved spatiotemporal interactions. We present a framework for developing
closure models for PDEs using synthetic data acquired through the method of
manufactured solutions. These data are used in conjunction with reinforcement
learning to provide closures for coarse-grained PDEs. We illustrate the
efficacy of our method using the one-dimensional and two-dimensional Burgers'
equations and the two-dimensional advection equation. Moreover, we demonstrate
that closure models trained for inhomogeneous PDEs can be effectively
generalized to homogeneous PDEs. The results demonstrate the potential for
developing accurate and computationally efficient closure models for systems
with scarce data.

</details>


### [106] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/abs/2505.11312)
*Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi*

**Main category:** cs.LG

**TL;DR:** Investigating the impact of normalization placement on neural networks' initial prediction behavior.


<details>
  <summary>Details</summary>
**Motivation:** To provide a detailed theoretical understanding of how normalization affects model behavior starting from initialization.

**Method:** Analyzing the influence of normalization on the statistical properties of network predictions before training begins.

**Result:** The distribution of class predictions at initialization can range from unbiased to highly concentrated towards a subset of classes.

**Conclusion:** Normalization placement induces systematic differences in the initial prediction behavior of neural networks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+You+Place+the+Norm+Matters%3A+From+Prejudiced+to+Neutral+Initializations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11312&send_immediately=true&force_search=false)

**Abstract:** Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [107] [Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network](https://arxiv.org/abs/2505.11321)
*Pu Yang, J. A. Barria*

**Main category:** cs.LG

**TL;DR:** An unsupervised recurrent neural network model named RWPNN is proposed for time series anomaly detection in non-stationary environments.


<details>
  <summary>Details</summary>
**Motivation:** To detect anomalies in non-stationary environments.

**Method:** RWPNN consists of two components: a stacked recurrent encoder-decoder module and a multi-receptive-field wavelet probabilistic network.

**Result:** The model was tested on 45 real-world time series datasets and showed robust and accurate detection capabilities as well as the ability to provide early warnings for anomalous events.

**Conclusion:** RWPNN extends standard wavelet probabilistic networks to wavelet deep probabilistic networks, which can handle higher data dimensionality and adapt to different rates of data variation in different datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+for+Non-stationary+Time+Series+using+Recurrent+Wavelet+Probabilistic+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11321，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11321&send_immediately=true&force_search=false)

**Abstract:** In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.

</details>


### [108] [The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework](https://arxiv.org/abs/2505.11335)
*Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu*

**Main category:** cs.LG

**TL;DR:** 本文提出了一种新的图神经网络校准方法，通过理论分析揭示了模型信心的调控机制，并在多个任务上展示了优越的性能。


<details>
  <summary>Details</summary>
**Motivation:** 现有的GNN校准方法通常引入额外的校准组件，未能捕捉模型与预测信心之间的内在关系，导致理论保证有限且计算开销增加。

**Method:** 提出的图校准方法简单而高效，通过减少最终层参数的权重衰减来缓解GNN的欠置信问题，并结合节点级校准作为更细粒度的补充。

**Result:** 实验验证了所提方法的有效性，表明其在多种任务上的性能优于其他方法。

**Conclusion:** 提出的方法在缓解图神经网络的预测不自信问题上表现优越，并且通过理论分析揭示了模型信心受类别质心级和节点级校准共同控制。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Final+Layer+Holds+the+Key%3A+A+Unified+and+Efficient+GNN+Calibration+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11335，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11335&send_immediately=true&force_search=false)

**Abstract:** Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on
graph-based tasks. However, their predictive confidence is often miscalibrated,
typically exhibiting under-confidence, which harms the reliability of their
decisions. Existing calibration methods for GNNs normally introduce additional
calibration components, which fail to capture the intrinsic relationship
between the model and the prediction confidence, resulting in limited
theoretical guarantees and increased computational overhead. To address this
issue, we propose a simple yet efficient graph calibration method. We establish
a unified theoretical framework revealing that model confidence is jointly
governed by class-centroid-level and node-level calibration at the final layer.
Based on this insight, we theoretically show that reducing the weight decay of
the final-layer parameters alleviates GNN under-confidence by acting on the
class-centroid level, while node-level calibration acts as a finer-grained
complement to class-centroid level calibration, which encourages each test node
to be closer to its predicted class centroid at the final-layer
representations. Extensive experiments validate the superiority of our method.

</details>


### [109] [Sobolev Training of End-to-End Optimization Proxies](https://arxiv.org/abs/2505.11342)
*Andrew W. Rosemberg, Joaquim Dias Garcia, Russell Bent, Pascal Van Hentenryck*

**Main category:** cs.LG

**TL;DR:** Optimization proxies trained using Sobolev training improve prediction accuracy and reliability over traditional solvers.


<details>
  <summary>Details</summary>
**Motivation:** To improve the efficiency and reliability of machine learning-based optimization proxies.

**Method:** Integrating solver sensitivities into end-to-end proxies via Sobolev training in both fully supervised and self-supervised settings.

**Result:** Supervised Sobolev training reduced mean squared error and constraint violations significantly while maintaining optimality gaps. Self-supervised Sobolev training halved optimality gaps in certain regions.

**Conclusion:** Sobolev training, whether supervised or self-supervised, provides a promising approach for creating fast and reliable surrogates for large-scale optimization tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sobolev+Training+of+End-to-End+Optimization+Proxies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11342&send_immediately=true&force_search=false)

**Abstract:** Optimization proxies - machine learning models trained to approximate the
solution mapping of parametric optimization problems in a single forward pass -
offer dramatic reductions in inference time compared to traditional iterative
solvers. This work investigates the integration of solver sensitivities into
such end to end proxies via a Sobolev training paradigm and does so in two
distinct settings: (i) fully supervised proxies, where exact solver outputs and
sensitivities are available, and (ii) self supervised proxies that rely only on
the objective and constraint structure of the underlying optimization problem.
By augmenting the standard training loss with directional derivative
information extracted from the solver, the proxy aligns both its predicted
solutions and local derivatives with those of the optimizer. Under Lipschitz
continuity assumptions on the true solution mapping, matching first order
sensitivities is shown to yield uniform approximation error proportional to the
training set covering radius. Empirically, different impacts are observed in
each studied setting. On three large Alternating Current Optimal Power Flow
benchmarks, supervised Sobolev training cuts mean squared error by up to 56
percent and the median worst case constraint violation by up to 400 percent
while keeping the optimality gap below 0.22 percent. For a mean variance
portfolio task trained without labeled solutions, self supervised Sobolev
training halves the average optimality gap in the medium risk region (standard
deviation above 10 percent of budget) and matches the baseline elsewhere.
Together, these results highlight Sobolev training whether supervised or self
supervised as a path to fast reliable surrogates for safety critical large
scale optimization workloads.

</details>


### [110] [What Can We Learn From MIMO Graph Convolutions?](https://arxiv.org/abs/2505.11346)
*Andreas Roth, Thomas Liebig*

**Main category:** cs.LG

**TL;DR:** This paper introduces localized MIMO graph convolutions (LMGCs) which generalize many linear message-passing neural networks and proves their injectivity and linear independence under certain conditions.


<details>
  <summary>Details</summary>
**Motivation:** While GNNs are typically applied in the multi-input multi-output (MIMO) case, the approximations are performed in the single-input single-output (SISO) case.

**Method:** We derive the MIMO graph convolution through the convolution theorem and approximate it directly in the MIMO case.

**Result:** For almost every choice of edge weights, we prove that LMGCs with a single computational graph are injective on multisets, and the resulting representations are linearly independent when more than one computational graph is used.

**Conclusion:** We find that the key MIMO-specific property of the graph convolution is operating on multiple computational graphs, and we introduce localized MIMO graph convolutions (LMGCs), which generalize many linear message-passing neural networks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Can+We+Learn+From+MIMO+Graph+Convolutions%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11346，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11346&send_immediately=true&force_search=false)

**Abstract:** Most graph neural networks (GNNs) utilize approximations of the general graph
convolution derived in the graph Fourier domain. While GNNs are typically
applied in the multi-input multi-output (MIMO) case, the approximations are
performed in the single-input single-output (SISO) case. In this work, we first
derive the MIMO graph convolution through the convolution theorem and
approximate it directly in the MIMO case. We find the key MIMO-specific
property of the graph convolution to be operating on multiple computational
graphs, or equivalently, applying distinct feature transformations for each
pair of nodes. As a localized approximation, we introduce localized MIMO graph
convolutions (LMGCs), which generalize many linear message-passing neural
networks. For almost every choice of edge weights, we prove that LMGCs with a
single computational graph are injective on multisets, and the resulting
representations are linearly independent when more than one computational graph
is used. Our experimental results confirm that an LMGC can combine the benefits
of various methods.

</details>


### [111] [Training NTK to Generalize with KARE](https://arxiv.org/abs/2505.11347)
*Johannes Schwab, Bryan Kelly, Semyon Malamud, Teng Andrea Xu*

**Main category:** cs.LG

**TL;DR:** This paper proposes optimizing the neural tangent kernel (NTK) explicitly rather than minimizing empirical risk, showing that such trained kernels can outperform traditional deep neural network optimization in certain scenarios.


<details>
  <summary>Details</summary>
**Motivation:** To challenge the conventional dominance of deep neural networks by exploring whether explicitly trained kernels can outperform traditional end-to-end DNN optimization.

**Method:** Optimizing the NTK explicitly using the Kernel Alignment Risk Estimator (KARE) to minimize generalization error.

**Result:** Simulations and real data experiments demonstrate that NTKs trained with KARE consistently match or significantly outperform the original DNN and the DNN-induced NTK.

**Conclusion:** Explicit training of NTK is considered a form of over-parametrized feature learning and challenges the traditional reliance on DNNs.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+NTK+to+Generalize+with+KARE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11347&send_immediately=true&force_search=false)

**Abstract:** The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.

</details>


### [112] [Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning](https://arxiv.org/abs/2505.11349)
*Yuanzhao Zhang, William Gilpin*

**Main category:** cs.LG

**TL;DR:** Foundation models for physical systems can predict accurately but fail to represent underlying physics, instead using context parroting, which is simpler and more efficient than complex models.


<details>
  <summary>Details</summary>
**Motivation:** To explore the capabilities and limitations of foundation models in predicting physical systems.

**Method:** Analysis of foundation models' ability to predict diverse dynamical systems and comparison with a naive direct context parroting model.

**Result:** Foundation models score lower than a naive direct context parroting model in predicting dynamical systems.

**Conclusion:** Context parroting serves as a tough-to-beat baseline for future time-series foundation models and helps understand in-context learning strategies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context+parroting%3A+A+simple+but+tough-to-beat+baseline+for+foundation+models+in+scientific+machine+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11349&send_immediately=true&force_search=false)

**Abstract:** Recently-developed time series foundation models for scientific machine
learning exhibit emergent abilities to predict physical systems. These
abilities include zero-shot forecasting, in which a model forecasts future
states of a system given only a short trajectory as context. Here, we show that
foundation models applied to physical systems can give accurate predictions,
but that they fail to develop meaningful representations of the underlying
physics. Instead, foundation models often forecast by context parroting, a
simple zero-shot forecasting strategy that copies directly from the context. As
a result, a naive direct context parroting model scores higher than
state-of-the-art time-series foundation models on predicting a diverse range of
dynamical systems, at a tiny fraction of the computational cost. We draw a
parallel between context parroting and induction heads, which explains why
large language models trained on text can be repurposed for time series
forecasting. Our dynamical systems perspective also ties the scaling between
forecast accuracy and context length to the fractal dimension of the attractor,
providing insight into the previously observed in-context neural scaling laws.
Context parroting thus serves as a simple but tough-to-beat baseline for future
time-series foundation models and can help identify in-context learning
strategies beyond parroting.

</details>


### [113] [Fractal Graph Contrastive Learning](https://arxiv.org/abs/2505.11356)
*Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang*

**Main category:** cs.LG

**TL;DR:** Proposes FractalGCL, a new method for graph contrastive learning using fractal self-similarity to improve global topological coherence.


<details>
  <summary>Details</summary>
**Motivation:** Existing GCL methods rely on random perturbations or local structure preservation lacking explicit control over global structural consistency.

**Method:** Introduces renormalisation-based augmentation and fractal-dimension-aware contrastive loss.

**Result:** Delivers state-of-the-art results on standard benchmarks and outperforms traditional baselines on traffic networks.

**Conclusion:** FractalGCL improves graph representation quality with reduced computational overhead.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fractal+Graph+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11356&send_immediately=true&force_search=false)

**Abstract:** While Graph Contrastive Learning (GCL) has attracted considerable attention
in the field of graph self-supervised learning, its performance heavily relies
on data augmentations that are expected to generate semantically consistent
positive pairs. Existing strategies typically resort to random perturbations or
local structure preservation, yet lack explicit control over global structural
consistency between augmented views. To address this limitation, we propose
Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that
leverages fractal self-similarity to enforce global topological coherence.
FractalGCL introduces two key innovations: a renormalisation-based augmentation
that generates structurally aligned positive views via box coverings; and a
fractal-dimension-aware contrastive loss that aligns graph embeddings according
to their fractal dimensions. While combining the two innovations markedly
boosts graph-representation quality, it also adds non-trivial computational
overhead. To mitigate the computational overhead of fractal dimension
estimation, we derive a one-shot estimator by proving that the dimension
discrepancy between original and renormalised graphs converges weakly to a
centred Gaussian distribution. This theoretical insight enables a reduction in
dimension computation cost by an order of magnitude, cutting overall training
time by approximately 61%. The experiments show that FractalGCL not only
delivers state-of-the-art results on standard benchmarks but also outperforms
traditional baselines on traffic networks by an average margin of about
remarkably 7%. Codes are available at
(https://anonymous.4open.science/r/FractalGCL-0511).

</details>


### [114] [LGBQPC: Local Granular-Ball Quality Peaks Clustering](https://arxiv.org/abs/2505.11359)
*Zihang Jia, Zhen Zhang, Witold Pedrycz*

**Main category:** cs.LG

**TL;DR:** This paper introduces LGBQPC, an improved clustering algorithm that addresses limitations of GBDPC in handling complex datasets with non-uniform density distributions or complex manifold structures.


<details>
  <summary>Details</summary>
**Motivation:** To enhance the capabilities of the GBDPC algorithm in dealing with complex clustering tasks involving intricate data structures.

**Method:** Developing the LGBQPC algorithm which includes an improved GB generation method (GB-POJG+) and new methods for density estimation and distance metrics in the clustering process.

**Result:** LGBQPC shows better performance than GBDPC on datasets with complex manifold structures or non-uniform density distributions.

**Conclusion:** The proposed LGBQPC algorithm provides significant improvements over GBDPC by addressing its limitations in handling complex datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LGBQPC%3A+Local+Granular-Ball+Quality+Peaks+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11359&send_immediately=true&force_search=false)

**Abstract:** The density peaks clustering (DPC) algorithm has attracted considerable
attention for its ability to detect arbitrarily shaped clusters based on a
simple yet effective assumption. Recent advancements integrating granular-ball
(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which
improves computational efficiency. However, GBDPC demonstrates limitations when
handling complex clustering tasks, particularly those involving data with
complex manifold structures or non-uniform density distributions. To overcome
these challenges, this paper proposes the local GB quality peaks clustering
(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB
generation and clustering processes based on the principle of justifiable
granularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,
is developed, which systematically refines the original GB-POJG in four key
aspects: the objective function, termination criterion for GB division,
definition of abnormal GB, and granularity level adaptation strategy. GB-POJG+
simplifies parameter configuration by requiring only a single penalty
coefficient and ensures high-quality GB generation while maintaining the number
of generated GBs within an acceptable range. In the clustering phase, two key
innovations are introduced based on the GB k-nearest neighbor graph: relative
GB quality for density estimation and geodesic distance for GB distance metric.
These modifications substantially improve the performance of GBDPC on datasets
with complex manifold structures or non-uniform density distributions.
Extensive numerical experiments on 40 benchmark datasets, including both
synthetic and publicly available datasets, validate the superior performance of
the proposed LGBQPC algorithm.

</details>


### [115] [Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach](https://arxiv.org/abs/2505.11360)
*Rares Cristian, Pavithra Harsha, Georgia Perakis, Brian Quanz*

**Main category:** cs.LG

**TL;DR:** This paper presents a meta-optimization method that uses neural networks to learn efficient algorithms for approximating optimization problems, improving the computational efficiency of end-to-end frameworks.


<details>
  <summary>Details</summary>
**Motivation:** The computational complexity of end-to-end frameworks is a significant challenge, especially for large-scale problems. Traditional methods only focus on prediction error and do not consider downstream decision-making tasks.

**Method:** A neural network architecture is introduced that near-optimally solves optimization problems while ensuring feasibility constraints through alternate projections.

**Result:** The method offers superior computational efficiency, producing high-quality approximations faster and scaling better with problem size compared to existing techniques. It applies to a wide range of optimization problems including deterministic, single-stage as well as two-stage stochastic optimization problems.

**Conclusion:** The proposed method improves the computational efficiency of end-to-end frameworks and can be applied to various optimization problems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+End-to-End+Learning+for+Decision-Making%3A+A+Meta-Optimization+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11360&send_immediately=true&force_search=false)

**Abstract:** End-to-end learning has become a widely applicable and studied problem in
training predictive ML models to be aware of their impact on downstream
decision-making tasks. These end-to-end models often outperform traditional
methods that separate training from the optimization and only myopically focus
on prediction error. However, the computational complexity of end-to-end
frameworks poses a significant challenge, particularly for large-scale
problems. While training an ML model using gradient descent, each time we need
to compute a gradient we must solve an expensive optimization problem. We
present a meta-optimization method that learns efficient algorithms to
approximate optimization problems, dramatically reducing computational overhead
of solving the decision problem in general, an aspect we leverage in the
training within the end-to-end framework. Our approach introduces a neural
network architecture that near-optimally solves optimization problems while
ensuring feasibility constraints through alternate projections. We prove
exponential convergence, approximation guarantees, and generalization bounds
for our learning method. This method offers superior computational efficiency,
producing high-quality approximations faster and scaling better with problem
size compared to existing techniques. Our approach applies to a wide range of
optimization problems including deterministic, single-stage as well as
two-stage stochastic optimization problems. We illustrate how our proposed
method applies to (1) an electricity generation problem using real data from an
electricity routing company coordinating the movement of electricity throughout
13 states, (2) a shortest path problem with a computer vision task of
predicting edge costs from terrain maps, (3) a two-stage multi-warehouse
cross-fulfillment newsvendor problem, as well as a variety of other
newsvendor-like problems.

</details>


### [116] [Understanding Nonlinear Implicit Bias via Region Counts in Input Space](https://arxiv.org/abs/2505.11370)
*Jingwei Li, Jing Xu, Zifan Wang, Huishuai Zhang, Jingzhao Zhang*

**Main category:** cs.LG

**TL;DR:** This work proposes a method to characterize implicit bias by counting connected regions in the input space with the same predicted label. It finds that small region counts are related to good generalization performance and that certain hyper-parameter choices can lead to small region counts.


<details>
  <summary>Details</summary>
**Motivation:** Understanding the definition and mechanism of implicit bias in non-linear contexts.

**Method:** Characterizing implicit bias by the count of connected regions in the input space with the same predicted label.

**Result:** Small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. Good hyper-parameter choices can induce small region counts.

**Conclusion:** Small region counts are associated with geometrically simple decision boundaries and good generalization performance. Larger learning rates and smaller batch sizes can lead to small region counts.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Nonlinear+Implicit+Bias+via+Region+Counts+in+Input+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11370&send_immediately=true&force_search=false)

**Abstract:** One explanation for the strong generalization ability of neural networks is
implicit bias. Yet, the definition and mechanism of implicit bias in non-linear
contexts remains little understood. In this work, we propose to characterize
implicit bias by the count of connected regions in the input space with the
same predicted label. Compared with parameter-dependent metrics (e.g., norm or
normalized margin), region count can be better adapted to nonlinear,
overparameterized models, because it is determined by the function mapping and
is invariant to reparametrization. Empirically, we found that small region
counts align with geometrically simple decision boundaries and correlate well
with good generalization performance. We also observe that good hyper-parameter
choices such as larger learning rates and smaller batch sizes can induce small
region counts. We further establish the theoretical connections and explain how
larger learning rate can induce small region counts in neural networks.

</details>


### [117] [On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift](https://arxiv.org/abs/2505.11380)
*Alejandro Moreo*

**Main category:** cs.LG

**TL;DR:** 研究了数据集偏移条件下分类器校准、量化和准确性预测之间的联系，证明其等价性并提出新方法。


<details>
  <summary>Details</summary>
**Motivation:** 在数据分布不同的训练数据和测试数据下，传统的分类器决策分数校准、正样本比例估计和分类器准确性估计面临挑战。

**Method:** 通过相互约简证明了任务间的等价性，并提出新方法。

**Result:** 提出的跨领域方法在许多情况下与专门领域的性能相当，有时甚至更优。

**Conclusion:** 证明了校准、量化和分类器准确性预测这三个任务在数据集偏移条件下的等价性，并提出了新的基于直接适应其他学科已有方法的方法。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Interconnections+of+Calibration%2C+Quantification%2C+and+Classifier+Accuracy+Prediction+under+Dataset+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11380&send_immediately=true&force_search=false)

**Abstract:** When the distribution of the data used to train a classifier differs from
that of the test data, i.e., under dataset shift, well-established routines for
calibrating the decision scores of the classifier, estimating the proportion of
positives in a test sample, or estimating the accuracy of the classifier,
become particularly challenging. This paper investigates the interconnections
among three fundamental problems, calibration, quantification, and classifier
accuracy prediction, under dataset shift conditions. Specifically, we prove
their equivalence through mutual reduction, i.e., we show that access to an
oracle for any one of these tasks enables the resolution of the other two.
Based on these proofs, we propose new methods for each problem based on direct
adaptations of well-established methods borrowed from the other disciplines.
Our results show such methods are often competitive, and sometimes even surpass
the performance of dedicated approaches from each discipline. The main goal of
this paper is to fostering cross-fertilization among these research areas,
encouraging the development of unified approaches and promoting synergies
across the fields.

</details>


### [118] [IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting](https://arxiv.org/abs/2505.11390)
*Millend Roy, Vladimir Pyltsov, Yinbo Hu*

**Main category:** cs.LG

**TL;DR:** This study compares classical and deep learning methods for long-term electricity load forecasting using ESD 2025 data, finding that simpler models like XGBoost outperform complex ones like TimeGPT due to data limitations.


<details>
  <summary>Details</summary>
**Motivation:** To evaluate the effectiveness of different forecasting models, particularly deep learning models like TimeGPT, in long-term electricity load prediction.

**Method:** Employed PCA for dimensionality reduction, framed the task as a regression problem using temperature and GHI as covariates, and stacked 24 models to generate yearly forecasts.

**Result:** Deep learning models failed to consistently outperform simpler statistical and machine learning approaches. XGBoost had the lowest error rates with minimal feature engineering.

**Conclusion:** The study highlights the limitations of deep learning in long-term electricity forecasting and emphasizes the importance of model selection based on dataset characteristics.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IISE+PG%26E+Energy+Analytics+Challenge+2025%3A+Hourly-Binned+Regression+Models+Beat+Transformers+in+Load+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11390&send_immediately=true&force_search=false)

**Abstract:** Accurate electricity load forecasting is essential for grid stability,
resource optimization, and renewable energy integration. While
transformer-based deep learning models like TimeGPT have gained traction in
time-series forecasting, their effectiveness in long-term electricity load
prediction remains uncertain. This study evaluates forecasting models ranging
from classical regression techniques to advanced deep learning architectures
using data from the ESD 2025 competition. The dataset includes two years of
historical electricity load data, alongside temperature and global horizontal
irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.
Since actual test set load values remain undisclosed, leveraging predicted
values would accumulate errors, making this a long-term forecasting challenge.
We employ (i) Principal Component Analysis (PCA) for dimensionality reduction
and (ii) frame the task as a regression problem, using temperature and GHI as
covariates to predict load for each hour, (iii) ultimately stacking 24 models
to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to
consistently outperform simpler statistical and machine learning approaches due
to the limited availability of training data and exogenous variables. In
contrast, XGBoost, with minimal feature engineering, delivers the lowest error
rates across all test cases while maintaining computational efficiency. This
highlights the limitations of deep learning in long-term electricity
forecasting and reinforces the importance of model selection based on dataset
characteristics rather than complexity. Our study provides insights into
practical forecasting applications and contributes to the ongoing discussion on
the trade-offs between traditional and modern forecasting methods.

</details>


### [119] [Finding Counterfactual Evidences for Node Classification](https://arxiv.org/abs/2505.11396)
*Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi*

**Main category:** cs.LG

**TL;DR:** This paper introduces and addresses the challenge of finding counterfactual evidences for GNN-based node classification tasks using observational data, proposing effective search algorithms and an indexing solution that improves fairness and accuracy.


<details>
  <summary>Details</summary>
**Motivation:** To improve fairness and interpretability of GNNs in real-world applications where randomized controlled trials are impractical.

**Method:** Developing search algorithms and a novel indexing solution leveraging both node features and structural information.

**Result:** Identifies counterfactual evidences that can enhance the fairness and accuracy of GNNs through various downstream applications.

**Conclusion:** The proposed approach can effectively detect counterfactual evidences, offering a way to improve GNNs' performance in terms of fairness and interpretability.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Finding+Counterfactual+Evidences+for+Node+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11396&send_immediately=true&force_search=false)

**Abstract:** Counterfactual learning is emerging as an important paradigm, rooted in
causality, which promises to alleviate common issues of graph neural networks
(GNNs), such as fairness and interpretability. However, as in many real-world
application domains where conducting randomized controlled trials is
impractical, one has to rely on available observational (factual) data to
detect counterfactuals. In this paper, we introduce and tackle the problem of
searching for counterfactual evidences for the GNN-based node classification
task. A counterfactual evidence is a pair of nodes such that, regardless they
exhibit great similarity both in the features and in their neighborhood
subgraph structures, they are classified differently by the GNN. We develop
effective and efficient search algorithms and a novel indexing solution that
leverages both node features and structural information to identify
counterfactual evidences, and generalizes beyond any specific GNN. Through
various downstream applications, we demonstrate the potential of counterfactual
evidences to enhance fairness and accuracy of GNNs.

</details>


### [120] [Is Grokking a Computational Glass Relaxation?](https://arxiv.org/abs/2505.11411)
*Xiaotian Zhang, Yue Shang, Entao Yang, Ge Zhang*

**Main category:** cs.LG

**TL;DR:** This paper interprets the grokking phenomenon in neural networks as a computational glass relaxation, suggesting no entropy barrier in the memorization-to-generalization transition and identifying a high-entropy advantage. It also proposes a toy optimizer WanD that eliminates grokking and finds high-norm generalizing solutions.


<details>
  <summary>Details</summary>
**Motivation:** To understand the mechanisms behind neural networks' generalizability, particularly the grokking phenomenon where networks generalize long after achieving near-perfect training performance.

**Method:** Analogizing neural networks to physical systems, viewing parameters as degrees of freedom and training loss as system energy. Sampling Boltzmann entropy landscapes and developing a toy optimizer inspired by Wang-Landau molecular dynamics.

**Result:** Experiments with transformers on arithmetic tasks show no entropy barrier in the grokking transition and reveal a significant high-entropy advantage. The proposed optimizer successfully eliminates grokking.

**Conclusion:** Grokking is better understood as a computational glass relaxation rather than a first-order phase transition. The findings challenge existing theories and suggest new directions for optimizer design.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Grokking+a+Computational+Glass+Relaxation%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11411&send_immediately=true&force_search=false)

**Abstract:** Understanding neural network's (NN) generalizability remains a central
question in deep learning research. The special phenomenon of grokking, where
NNs abruptly generalize long after the training performance reaches a
near-perfect level, offers a unique window to investigate the underlying
mechanisms of NNs' generalizability. Here we propose an interpretation for
grokking by framing it as a computational glass relaxation: viewing NNs as a
physical system where parameters are the degrees of freedom and train loss is
the system energy, we find memorization process resembles a rapid cooling of
liquid into non-equilibrium glassy state at low temperature and the later
generalization is like a slow relaxation towards a more stable configuration.
This mapping enables us to sample NNs' Boltzmann entropy (states of density)
landscape as a function of training loss and test accuracy. Our experiments in
transformers on arithmetic tasks suggests that there is NO entropy barrier in
the memorization-to-generalization transition of grokking, challenging previous
theory that defines grokking as a first-order phase transition. We identify a
high-entropy advantage under grokking, an extension of prior work linking
entropy to generalizability but much more significant. Inspired by grokking's
far-from-equilibrium nature, we develop a toy optimizer WanD based on
Wang-landau molecular dynamics, which can eliminate grokking without any
constraints and find high-norm generalizing solutions. This provides
strictly-defined counterexamples to theory attributing grokking solely to
weight norm evolution towards the Goldilocks zone and also suggests new
potential ways for optimizer design.

</details>


### [121] [Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks](https://arxiv.org/abs/2505.11412)
*Ciaran Bench, Vivek Desai, Mohammad Moulaeifard, Nils Strodthoff, Philip Aston, Andrew Thompson*

**Main category:** cs.LG

**TL;DR:** 研究了两种不确定性量化技术在PPG信号分类中的应用，发现超参数的选择对模型性能和不确定性质量有显著影响。


<details>
  <summary>Details</summary>
**Motivation:** 深度网络虽然能够处理来自可穿戴测量设备的大数据量，但它们缺乏解释性且容易过拟合，这使得在未见数据上表现不佳并可能导致误诊的风险很大。

**Method:** 使用两种可扩展的不确定性量化技术：蒙特卡罗丢弃和最近提出的改进变分在线牛顿法。

**Result:** 我们发现模型参数采样的随机性决定了总不确定性中有多少是偶然性的，并且对预测性能和校准质量有不同的影响，具体取决于所选的不确定性量化技术和不确定性表达方式的选择。我们在预测类别的不确定性质量上发现了显著差异，强调了需要一个彻底的评估协议来评估局部和自适应校准。

**Conclusion:** 选择超参数必须仔细调整以平衡预测性能和校准质量，并且最佳参数化可能因所选不确定性表达方式而异。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+quantification+with+approximate+variational+learning+for+wearable+photoplethysmography+prediction+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11412&send_immediately=true&force_search=false)

**Abstract:** Photoplethysmography (PPG) signals encode information about relative changes
in blood volume that can be used to assess various aspects of cardiac health
non-invasively, e.g.\ to detect atrial fibrillation (AF) or predict blood
pressure (BP). Deep networks are well-equipped to handle the large quantities
of data acquired from wearable measurement devices. However, they lack
interpretability and are prone to overfitting, leaving considerable risk for
poor performance on unseen data and misdiagnosis. Here, we describe the use of
two scalable uncertainty quantification techniques: Monte Carlo Dropout and the
recently proposed Improved Variational Online Newton. These techniques are used
to assess the trustworthiness of models trained to perform AF classification
and BP regression from raw PPG time series. We find that the choice of
hyperparameters has a considerable effect on the predictive performance of the
models and on the quality and composition of predicted uncertainties. E.g. the
stochasticity of the model parameter sampling determines the proportion of the
total uncertainty that is aleatoric, and has varying effects on predictive
performance and calibration quality dependent on the chosen uncertainty
quantification technique and the chosen expression of uncertainty. We find
significant discrepancy in the quality of uncertainties over the predicted
classes, emphasising the need for a thorough evaluation protocol that assesses
local and adaptive calibration. This work suggests that the choice of
hyperparameters must be carefully tuned to balance predictive performance and
calibration quality, and that the optimal parameterisation may vary depending
on the chosen expression of uncertainty.

</details>


### [122] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/abs/2505.11415)
*Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai*

**Main category:** cs.LG

**TL;DR:** Introduce MoE-CAP, a benchmark for MoE systems, revealing the MoE-CAP trade-off and proposing CAP Radar Diagram and sparsity-aware performance metrics.


<details>
  <summary>Details</summary>
**Motivation:** Existing benchmarks fail to capture the trade-offs in MoE systems across Cost, Accuracy, and Performance.

**Method:** Introduce MoE-CAP benchmark, analyze trade-offs, propose CAP Radar Diagram, and introduce S-MBU and S-MFU metrics.

**Result:** Revealed the MoE-CAP trade-off where MoE systems typically optimize two dimensions at the expense of the third.

**Conclusion:** MoE-CAP and its associated metrics provide a more accurate way to benchmark MoE systems across various hardware and deployment scenarios.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoE-CAP%3A+Benchmarking+Cost%2C+Accuracy+and+Performance+of+Sparse+Mixture-of-Experts+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11415&send_immediately=true&force_search=false)

**Abstract:** The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [123] [MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](https://arxiv.org/abs/2505.11432)
*Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu*

**Main category:** cs.LG

**TL;DR:** MegaScale-MoE is an efficient training system designed for large-scale mixture-of-experts (MoE) models.


<details>
  <summary>Details</summary>
**Motivation:** To enhance the performance of large language models (LLMs) by scaling them to unprecedented sizes using MoE architecture.

**Method:** Customizing communication-efficient parallelism strategies for attention and FFNs, overlapping communication with computation, and applying communication compression with adjusted communication patterns.

**Result:** Achieved a training throughput of 1.41M tokens/s on 1,440 NVIDIA Hopper GPUs when training a 352B MoE model, improving efficiency by 1.88 times compared to Megatron-LM.

**Conclusion:** Shared operational experience in accelerating MoE training and aims to inspire future research in MoE systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MegaScale-MoE%3A+Large-Scale+Communication-Efficient+Training+of+Mixture-of-Experts+Models+in+Production，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11432&send_immediately=true&force_search=false)

**Abstract:** We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.

</details>


### [124] [Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks](https://arxiv.org/abs/2505.11461)
*Wesley A Suttle, Vipul K Sharma, Brian M Sadler*

**Main category:** cs.LG

**TL;DR:** This paper explores how signal attenuation enables decentralization in Multi-Agent Reinforcement Learning (MARL) through a radar network power allocation example.


<details>
  <summary>Details</summary>
**Motivation:** To replace global state observability with local neighborhood observability for enabling decentralization and scalability in MARL, especially in wireless communication and radar network applications.

**Method:** Proposes two new constrained multi-agent Markov decision process formulations for power allocation in radar networks, derives local neighborhood approximations for global value function and gradient estimates, and develops decentralized saddle point policy gradient algorithms.

**Result:** Demonstrates the feasibility of decentralization in MARL using signal attenuation properties in radar networks, providing a model for future extensions in wireless communications and radar networks.

**Conclusion:** Signal attenuation can enable decentralization in MARL, offering a promising approach for scalable solutions in related fields.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Signal+attenuation+enables+scalable+decentralized+multi-agent+reinforcement+learning+over+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11461&send_immediately=true&force_search=false)

**Abstract:** Classic multi-agent reinforcement learning (MARL) methods require that agents
enjoy global state observability, preventing development of decentralized
algorithms and limiting scalability. Recent work has shown that, under
assumptions on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for future extensions to additional problems
in wireless communications and radar networks.

</details>


### [125] [msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML](https://arxiv.org/abs/2505.11483)
*Zhaolan Huang, Emmanuel Baccelli*

**Main category:** cs.LG

**TL;DR:** AI models can be very large or very small. This paper introduces msf-CNN, a new method that helps small models run better on devices with limited memory.


<details>
  <summary>Details</summary>
**Motivation:** To make AI models efficient enough to run on devices with very limited memory like microcontrollers.

**Method:** Introduce msf-CNN, which efficiently finds optimal fusion settings for CNNs by exploring the fusion solution space represented as a directed acyclic graph.

**Result:** msf-CNN can reduce RAM usage by 50% compared to previous methods (MCUNetV2 and StreamNet).

**Conclusion:** msf-CNN provides more flexibility for system designers working with microcontrollers.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是msf-CNN%3A+Patch-based+Multi-Stage+Fusion+with+Convolutional+Neural+Networks+for+TinyML，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11483&send_immediately=true&force_search=false)

**Abstract:** AI spans from large language models to tiny models running on
microcontrollers (MCUs). Extremely memory-efficient model architectures are
decisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,
inference latency must remain small to fit real-time constraints. An approach
to tackle this is patch-based fusion, which aims to optimize data flows across
neural network layers. In this paper, we introduce msf-CNN, a novel technique
that efficiently finds optimal fusion settings for convolutional neural
networks (CNNs) by walking through the fusion solution space represented as a
directed acyclic graph. Compared to previous work on CNN fusion for MCUs,
msf-CNN identifies a wider set of solutions. We published an implementation of
msf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We
show that msf-CNN can achieve inference using 50% less RAM compared to the
prior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers
additional flexibility for system designers.

</details>


### [126] [Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis](https://arxiv.org/abs/2505.11491)
*Yuan-Zheng Lei, Yaobang Gong, Dianwei Chen, Yao Cheng, Xianfeng Terry Yang*

**Main category:** cs.LG

**TL;DR:** This study investigates the performance of physics-informed machine learning (PIML) approaches for traffic flow modeling, identifying conditions for successful parameter updates and factors leading to model failure.


<details>
  <summary>Details</summary>
**Motivation:** To examine why PIML models sometimes fail compared to purely data-driven or physics-based models.

**Method:** Analyzing the loss landscape by perturbing trained models along the principal eigenvectors of the Hessian matrix and evaluating corresponding loss values.

**Result:** Physics residuals in PIML do not inherently hinder optimization. Physical residuals can degrade PIML model performance, especially under highly physics-driven settings. Sparse sampling and temporally averaged traffic data can produce misleadingly small physics residuals. Adherence to the CFL condition is important for dataset suitability in PIML applications. Higher-order models tend to have larger error lower bounds than lower-order models.

**Conclusion:** Successful parameter updates in PIML require both ML and physics gradients to form acute angles with the quasi-true gradient and lie within a conical region.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Potential+failures+of+physics-informed+machine+learning+in+traffic+flow+modeling%3A+theoretical+and+experimental+analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11491，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11491&send_immediately=true&force_search=false)

**Abstract:** This study critically examines the performance of physics-informed machine
learning (PIML) approaches for traffic flow modeling, defining the failure of a
PIML model as the scenario where it underperforms both its purely data-driven
and purely physics-based counterparts. We analyze the loss landscape by
perturbing trained models along the principal eigenvectors of the Hessian
matrix and evaluating corresponding loss values. Our results suggest that
physics residuals in PIML do not inherently hinder optimization, contrary to a
commonly assumed failure cause. Instead, successful parameter updates require
both ML and physics gradients to form acute angles with the quasi-true gradient
and lie within a conical region. Given inaccuracies in both the physics models
and the training data, satisfying this condition is often difficult.
Experiments reveal that physical residuals can degrade the performance of LWR-
and ARZ-based PIML models, especially under highly physics-driven settings.
Moreover, sparse sampling and the use of temporally averaged traffic data can
produce misleadingly small physics residuals that fail to capture actual
physical dynamics, contributing to model failure. We also identify the
Courant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset
suitability for PIML, where successful applications consistently adhere to this
criterion. Lastly, we observe that higher-order models like ARZ tend to have
larger error lower bounds than lower-order models like LWR, which is consistent
with the experimental findings of existing studies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [On the Evaluation of Engineering Artificial General Intelligence](https://arxiv.org/abs/2505.10653)
*Sandeep Neema, Susmit Jha, Adam Nagel, Ethan Lew, Chandrasekar Sureshkumar, Aleksa Gordic, Chase Shimmin, Hieu Nguygen, Paul Eremenko*

**Main category:** cs.AI

**TL;DR:** This paper discusses the challenges and proposes a framework for evaluating engineering artificial general intelligence (eAGI) agents. It aims to specialize and ground Bloom's taxonomy in an engineering design context to create a rich taxonomy of evaluation questions, motivate a pluggable evaluation framework, and outline an automatable procedure to customize the evaluation benchmark to different engineering contexts.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to develop a framework for evaluating eAGI agents which can address a broad range of problems in the engineering of physical systems and associated controllers.

**Method:** The method involves proposing an extensible evaluation framework that specializes and grounds Bloom's taxonomy in an engineering design context.

**Result:** The result is the development of a rich taxonomy of evaluation questions spanning from methodological knowledge to real-world design problems, a pluggable evaluation framework that can evaluate not only textual responses but also structured design artifacts, and an automatable procedure to customize the evaluation benchmark to different engineering contexts.

**Conclusion:** The conclusion is that the proposed framework advances the state of the art in benchmarking and evaluation of AI agents in the engineering field.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Evaluation+of+Engineering+Artificial+General+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10653&send_immediately=true&force_search=false)

**Abstract:** We discuss the challenges and propose a framework for evaluating engineering
artificial general intelligence (eAGI) agents. We consider eAGI as a
specialization of artificial general intelligence (AGI), deemed capable of
addressing a broad range of problems in the engineering of physical systems and
associated controllers. We exclude software engineering for a tractable scoping
of eAGI and expect dedicated software engineering AI agents to address the
software implementation challenges. Similar to human engineers, eAGI agents
should possess a unique blend of background knowledge (recall and retrieve) of
facts and methods, demonstrate familiarity with tools and processes, exhibit
deep understanding of industrial components and well-known design families, and
be able to engage in creative problem solving (analyze and synthesize),
transferring ideas acquired in one context to another. Given this broad
mandate, evaluating and qualifying the performance of eAGI agents is a
challenge in itself and, arguably, a critical enabler to developing eAGI
agents. In this paper, we address this challenge by proposing an extensible
evaluation framework that specializes and grounds Bloom's taxonomy - a
framework for evaluating human learning that has also been recently used for
evaluating LLMs - in an engineering design context. Our proposed framework
advances the state of the art in benchmarking and evaluation of AI agents in
terms of the following: (a) developing a rich taxonomy of evaluation questions
spanning from methodological knowledge to real-world design problems; (b)
motivating a pluggable evaluation framework that can evaluate not only textual
responses but also evaluate structured design artifacts such as CAD models and
SysML models; and (c) outlining an automatable procedure to customize the
evaluation benchmark to different engineering contexts.

</details>


### [128] [Interpretable Risk Mitigation in LLM Agent Systems](https://arxiv.org/abs/2505.10670)
*Jan Chojnacki*

**Main category:** cs.AI

**TL;DR:** This paper explores how to improve the reliability of autonomous agents powered by large language models in game-theoretic environments. It introduces a method to steer agent behavior using interpretable features, demonstrating a reduction in defection probability and identifying feasible steering ranges for some open-source LLM agents.


<details>
  <summary>Details</summary>
**Motivation:** Addressing safety concerns due to the inherent unpredictability of LLMs in ensuring agent reliability.

**Method:** Introducing a strategy-modification method that steers the residual stream with interpretable features from a sparse autoencoder latent space, independent of the game and prompt.

**Result:** Steering with the good-faith negotiation feature reduces the average defection probability by 28 percentage points, and feasible steering ranges are identified for several open-source LLM agents.

**Conclusion:** Game-theoretic evaluation of LLM agents combined with representation-steering alignment could generalize to real-world applications on end-user devices and embodied platforms.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Risk+Mitigation+in+LLM+Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10670&send_immediately=true&force_search=false)

**Abstract:** Autonomous agents powered by large language models (LLMs) enable novel use
cases in domains where responsible action is increasingly important. Yet the
inherent unpredictability of LLMs raises safety concerns about agent
reliability. In this work, we explore agent behaviour in a toy, game-theoretic
environment based on a variation of the Iterated Prisoner's Dilemma. We
introduce a strategy-modification method-independent of both the game and the
prompt-by steering the residual stream with interpretable features extracted
from a sparse autoencoder latent space. Steering with the good-faith
negotiation feature lowers the average defection probability by 28 percentage
points. We also identify feasible steering ranges for several open-source LLM
agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,
combined with representation-steering alignment, can generalise to real-world
applications on end-user devices and embodied platforms.

</details>


### [129] [Embodied AI in Machine Learning -- is it Really Embodied?](https://arxiv.org/abs/2505.10705)
*Matej Hoffmann, Shubhan Parag Patni*

**Main category:** cs.AI

**TL;DR:** This paper discusses the development of AI-powered robots and their embodiment, comparing them to traditional AI approaches. It reviews the potential for cross-embodiment learning and suggests ways forward.


<details>
  <summary>Details</summary>
**Motivation:** To explore the embodiment in AI-powered robots and compare it with traditional AI approaches, as well as investigate the possibility of cross-embodiment learning.

**Method:** Literature review and critical discussion.

**Result:** Identified fundamental roadblocks in cross-embodiment learning and proposed directions for progress.

**Conclusion:** AI-powered robots are only weakly embodied and inherit some problems from traditional AI. Further research is needed to overcome these issues and advance cross-embodiment learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embodied+AI+in+Machine+Learning+--+is+it+Really+Embodied%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10705&send_immediately=true&force_search=false)

**Abstract:** Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the
machine learning communities with the goal of leveraging current progress in AI
(deep learning, transformers, large language and visual-language models) to
empower robots. In this chapter we put this work in the context of "Good
Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the
behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier
2001). We claim that the AI-powered robots are only weakly embodied and inherit
some of the problems of GOFAI. Moreover, we review and critically discuss the
possibility of cross-embodiment learning (Padalkar et al. 2024). We identify
fundamental roadblocks and propose directions on how to make progress.

</details>


### [130] [Evaluations at Work: Measuring the Capabilities of GenAI in Use](https://arxiv.org/abs/2505.10742)
*Brandon Lepine, Gawesha Weerantunga, Juho Kim, Pamela Mishkin, Matthew Beane*

**Main category:** cs.AI

**TL;DR:** Current AI benchmarks fail to capture the complexity of human-AI collaboration. This paper introduces an evaluation framework that breaks down tasks into interdependent subtasks and tracks both LLM performance and user strategies in a dialogue. A suite of metrics is developed to measure performance. The methodology is demonstrated on a financial valuation task, revealing that integrating LLM-generated content improves output quality but can be undermined by factors like incoherence and lack of alignment with user knowledge. The paper offers a holistic evaluation method for human-AI collaboration.


<details>
  <summary>Details</summary>
**Motivation:** Current AI benchmarks do not adequately reflect the complex, multi-turn nature of human-AI collaboration.

**Method:** An evaluation framework is created that decomposes tasks into interdependent subtasks and tracks LLM performance and user strategies through dialogue. A suite of metrics is developed to measure performance.

**Result:** Integrating LLM-generated content generally improves output quality but factors like incoherence, excessive subtask diversity, and misalignment with user knowledge can moderate these benefits. Proactive dialogue strategies that introduce novelty might harm task performance.

**Conclusion:** This paper advances a more comprehensive evaluation of human-AI collaboration, providing both a robust methodological framework and practical insights for improving AI-augmented workflows.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluations+at+Work%3A+Measuring+the+Capabilities+of+GenAI+in+Use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10742&send_immediately=true&force_search=false)

**Abstract:** Current AI benchmarks miss the messy, multi-turn nature of human-AI
collaboration. We present an evaluation framework that decomposes real-world
tasks into interdependent subtasks, letting us track both LLM performance and
users' strategies across a dialogue. Complementing this framework, we develop a
suite of metrics, including a composite usage derived from semantic similarity,
word overlap, and numerical matches; structural coherence; intra-turn
diversity; and a novel measure of the "information frontier" reflecting the
alignment between AI outputs and users' working knowledge. We demonstrate our
methodology in a financial valuation task that mirrors real-world complexity.
Our empirical findings reveal that while greater integration of LLM-generated
content generally enhances output quality, its benefits are moderated by
factors such as response incoherence, excessive subtask diversity, and the
distance of provided information from users' existing knowledge. These results
suggest that proactive dialogue strategies designed to inject novelty may
inadvertently undermine task performance. Our work thus advances a more
holistic evaluation of human-AI collaboration, offering both a robust
methodological framework and actionable insights for developing more effective
AI-augmented work processes.

</details>


### [131] [Code-Driven Planning in Grid Worlds with Large Language Models](https://arxiv.org/abs/2505.10749)
*Ashwath Vaithinathan Aravindan, Zhisheng Tang, Mayank Kejriwal*

**Main category:** cs.AI

**TL;DR:** An iterative programmatic planning (IPP) framework is proposed for grid-based tasks using large language models to generate executable programs that map environment states to action sequences.


<details>
  <summary>Details</summary>
**Motivation:** Traditional search and reinforcement learning methods are replaced with code generation as policy synthesis.

**Method:** The IPP framework incorporates prompting strategies like direct code generation, pseudocode-conditioned refinement, and curriculum-based prompting with an iterative refinement mechanism.

**Result:** IPP improves over direct code generation by 10% to 10x in five of six models and establishes a new state-of-the-art result for GRASP. It outperforms direct elicitation of solutions by 63%-116%.

**Conclusion:** IPP demonstrates the viability of the approach with lower amortized cost due to reusable code.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Code-Driven+Planning+in+Grid+Worlds+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10749&send_immediately=true&force_search=false)

**Abstract:** We propose an iterative programmatic planning (IPP) framework for solving
grid-based tasks by synthesizing interpretable agent policies expressed in code
using large language models (LLMs). Instead of relying on traditional search or
reinforcement learning, our approach uses code generation as policy synthesis,
where the LLM outputs executable programs that map environment states to action
sequences. Our proposed architecture incorporates several prompting strategies,
including direct code generation, pseudocode-conditioned refinement, and
curriculum-based prompting, but also includes an iterative refinement mechanism
that updates code based on task performance feedback. We evaluate our approach
using six leading LLMs and two challenging grid-based benchmarks (GRASP and
MiniGrid). Our IPP framework demonstrates improvements over direct code
generation ranging from 10\% to as much as 10x across five of the six models
and establishes a new state-of-the-art result for GRASP. IPP is found to
significantly outperform direct elicitation of a solution from GPT-o3-mini (by
63\% on MiniGrid to 116\% on GRASP), demonstrating the viability of the overall
approach. Computational costs of all code generation approaches are similar.
While code generation has a higher initial prompting cost compared to direct
solution elicitation (\$0.08 per task vs. \$0.002 per instance for
GPT-o3-mini), the code can be reused for any number of instances, making the
amortized cost significantly lower (by 400x on GPT-o3-mini across the complete
GRASP benchmark).

</details>


### [132] [Qualia Optimization](https://arxiv.org/abs/2505.10779)
*Philip S. Thomas*

**Main category:** cs.AI

**TL;DR:** This report speculatively considers if AI systems could experience qualia like pain or pleasure, suggesting that subjective experiences should be considered along with performance metrics.


<details>
  <summary>Details</summary>
**Motivation:** To explore the idea of AI systems experiencing subjective states like qualia.

**Method:** Proposing concrete mathematical problem settings inspired by reinforcement learning and philosophy of mind.

**Result:** Initial approaches and properties were presented which allowed for refinement of the problem setting.

**Conclusion:** Methods that promote reinforcement were proposed.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Qualia+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10779&send_immediately=true&force_search=false)

**Abstract:** This report explores the speculative question: what if current or future AI
systems have qualia, such as pain or pleasure? It does so by assuming that AI
systems might someday possess qualia -- and that the quality of these
subjective experiences should be considered alongside performance metrics.
Concrete mathematical problem settings, inspired by reinforcement learning
formulations and theories from philosophy of mind, are then proposed and
initial approaches and properties are presented. These properties enable
refinement of the problem setting, culminating with the proposal of methods
that promote reinforcement.

</details>


### [133] [SECRET: Semi-supervised Clinical Trial Document Similarity Search](https://arxiv.org/abs/2505.10780)
*Trisha Das, Afrah Shafquat, Beigi Mandis, Jacob Aptekar, Jimeng Sun*

**Main category:** cs.AI

**TL;DR:** Identify similar historical clinical trials to improve the design of new trials.


<details>
  <summary>Details</summary>
**Motivation:** Clinical trials are costly and risky. Learning from historical trials can reduce risks and improve success rates.

**Method:** A novel method is presented to summarize clinical trial protocols and find similar trials based on a query trial's protocol.

**Result:** The proposed method outperforms baselines in recall@1 and precision@1, showing improvements of up to 78% and 53%, respectively. It also excels in partial trial similarity search and zero-shot patient-trial matching.

**Conclusion:** Learning from historical trials through summarization and similarity search can enhance the design of new clinical trials.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SECRET%3A+Semi-supervised+Clinical+Trial+Document+Similarity+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10780&send_immediately=true&force_search=false)

**Abstract:** Clinical trials are vital for evaluation of safety and efficacy of new
treatments. However, clinical trials are resource-intensive, time-consuming and
expensive to conduct, where errors in trial design, reduced efficacy, and
safety events can result in significant delays, financial losses, and damage to
reputation. These risks underline the importance of informed and strategic
decisions in trial design to mitigate these risks and improve the chances of a
successful trial. Identifying similar historical trials is critical as these
trials can provide an important reference for potential pitfalls and challenges
including serious adverse events, dosage inaccuracies, recruitment
difficulties, patient adherence issues, etc. Addressing these challenges in
trial design can lead to development of more effective study protocols with
optimized patient safety and trial efficiency. In this paper, we present a
novel method to identify similar historical trials by summarizing clinical
trial protocols and searching for similar trials based on a query trial's
protocol. Our approach significantly outperforms all baselines, achieving up to
a 78% improvement in recall@1 and a 53% improvement in precision@1 over the
best baseline. We also show that our method outperforms all other baselines in
partial trial similarity search and zero-shot patient-trial matching,
highlighting its superior utility in these tasks.

</details>


### [134] [Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management](https://arxiv.org/abs/2505.10803)
*Zhaoan Wang, Wonseok Jang, Bowen Ruan, Jun Wang, Shaoping Xiao*

**Main category:** cs.AI

**TL;DR:** This study focuses on improving human-AI interaction in precision agriculture using reinforcement learning, emphasizing transparency, usability, and trust to make AI-based fertilization strategies more acceptable to farmers.


<details>
  <summary>Details</summary>
**Motivation:** To bridge the gap between AI algorithmic recommendations and farmers' practical experiences, local knowledge, and traditional practices in precision agriculture.

**Method:** Employing a well-established trust framework and conducting surveys with farmers to integrate critical misalignments into a trust model and a multi-objective RL framework embedding trust directly into policy optimization.

**Result:** The proposed approach ensures AI recommendations are technically robust, economically feasible, context-aware, and socially acceptable, thus supporting broader AI adoption in agriculture.

**Conclusion:** By aligning technical performance with human-centered trust, this research aims to enhance the acceptance and implementation of AI in agricultural practices.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Developing+and+Integrating+Trust+Modeling+into+Multi-Objective+Reinforcement+Learning+for+Intelligent+Agricultural+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10803&send_immediately=true&force_search=false)

**Abstract:** Precision agriculture, enhanced by artificial intelligence (AI), offers
promising tools such as remote sensing, intelligent irrigation, fertilization
management, and crop simulation to improve agricultural efficiency and
sustainability. Reinforcement learning (RL), in particular, has outperformed
traditional methods in optimizing yields and resource management. However,
widespread AI adoption is limited by gaps between algorithmic recommendations
and farmers' practical experience, local knowledge, and traditional practices.
To address this, our study emphasizes Human-AI Interaction (HAII), focusing on
transparency, usability, and trust in RL-based farm management. We employ a
well-established trust framework - comprising ability, benevolence, and
integrity - to develop a novel mathematical model quantifying farmers'
confidence in AI-based fertilization strategies. Surveys conducted with farmers
for this research reveal critical misalignments, which are integrated into our
trust model and incorporated into a multi-objective RL framework. Unlike prior
methods, our approach embeds trust directly into policy optimization, ensuring
AI recommendations are technically robust, economically feasible,
context-aware, and socially acceptable. By aligning technical performance with
human-centered trust, this research supports broader AI adoption in
agriculture.

</details>


### [135] [PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/abs/2505.10819)
*Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis*

**Main category:** cs.AI

**TL;DR:** Learn complex, stochastic world models from few observations.


<details>
  <summary>Details</summary>
**Motivation:** Traditional world models demand vast amounts of training data and cannot flexibly update knowledge from sparse observations. Program-structured world models have limitations in application domains.

**Method:** Introduce a novel program synthesis method for modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs.

**Result:** This approach can learn complex, stochastic world models from just a few observations. The learned world models are embedded in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge.

**Conclusion:** The proposed PoE-World method enables effective modeling of complex, non-gridworld domains with limited data, showing potential in AI agent development.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PoE-World%3A+Compositional+World+Modeling+with+Products+of+Programmatic+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10819&send_immediately=true&force_search=false)

**Abstract:** Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
sparse observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.

</details>


### [136] [TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding](https://arxiv.org/abs/2505.10834)
*Achintha Wijesinghe, Weiwei Wang, Suchinthaka Wanninayaka, Songyang Zhang, Zhi Ding*

**Main category:** cs.AI

**TL;DR:** Introduces a new semantic communication framework that improves downstream task performance, generalizability, bandwidth efficiency, and reconstruction latency.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to improve next-generation semantic communication by focusing on conveying the meaning of a message rather than just transmitting raw data.

**Method:** A novel semantic communication framework that can capture task-specific and contextual information for flexible adaptation to multiple tasks at the receiver.

**Result:** The framework shows promising improvement in downstream tasks, generalizability, bandwidth efficiency, and reconstruction latency.

**Conclusion:** This work contributes to the field of semantic communication by introducing a new framework that enhances performance in various aspects.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TACO%3A+Rethinking+Semantic+Communications+with+Task+Adaptation+and+Context+Embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10834&send_immediately=true&force_search=false)

**Abstract:** Recent advancements in generative artificial intelligence have introduced
groundbreaking approaches to innovating next-generation semantic communication,
which prioritizes conveying the meaning of a message rather than merely
transmitting raw data. A fundamental challenge in semantic communication lies
in accurately identifying and extracting the most critical semantic information
while adapting to downstream tasks without degrading performance, particularly
when the objective at the receiver may evolve over time. To enable flexible
adaptation to multiple tasks at the receiver, this work introduces a novel
semantic communication framework, which is capable of jointly capturing
task-specific information to enhance downstream task performance and contextual
information. Through rigorous experiments on popular image datasets and
computer vision tasks, our framework shows promising improvement compared to
existing work, including superior performance in downstream tasks, better
generalizability, ultra-high bandwidth efficiency, and low reconstruction
latency.

</details>


### [137] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
*Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy*

**Main category:** cs.AI

**TL;DR:** This paper introduces a benchmark using long narrative brainteasers to evaluate the reasoning strategies of large language models (LLMs), analyzing their problem-solving correctness, creativity, and efficiency across various tasks.


<details>
  <summary>Details</summary>
**Motivation:** To explore deeper insights into how AI models approach problem-solving by evaluating their reasoning strategies beyond just accuracy.

**Method:** Developing a benchmark with brainteasers that allow multiple solving approaches and analyzing LLMs' performance in semantic parsing, solution generation, self-correction, step-by-step sketching, and utilizing hints.

**Result:** LLMs can often find creative solutions but sometimes rely on brute force even when better alternatives exist.

**Conclusion:** While LLMs show capacity for creative problem-solving, further improvements are needed to enhance their reasoning abilities.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Creativity+or+Brute+Force%3F+Using+Brainteasers+as+a+Window+into+the+Problem-Solving+Abilities+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10844&send_immediately=true&force_search=false)

**Abstract:** Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [138] [MCU: Improving Machine Unlearning through Mode Connectivity](https://arxiv.org/abs/2505.10859)
*Yingdan Shi, Ren Wang*

**Main category:** cs.AI

**TL;DR:** This paper introduces Mode Connectivity Unlearning (MCU), a new machine unlearning framework that effectively removes specific data information from a trained model without the need for hyperparameter tuning.


<details>
  <summary>Details</summary>
**Motivation:** Existing machine unlearning methods face challenges due to weight entanglement and often require empirical hyperparameter tuning.

**Method:** MCU uses mode connectivity to find an unlearning pathway non-linearly and includes a parameter mask strategy and an adaptive adjustment strategy for the unlearning penalty coefficient.

**Result:** MCU outperforms other methods in image classification tasks and provides a range of unlearning models along the pathway.

**Conclusion:** MCU is a versatile and efficient machine unlearning framework that can be integrated with any existing method to improve unlearning efficacy.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MCU%3A+Improving+Machine+Unlearning+through+Mode+Connectivity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10859&send_immediately=true&force_search=false)

**Abstract:** Machine Unlearning (MU) aims to remove the information of specific training
data from a trained model, ensuring compliance with privacy regulations and
user requests. While one line of existing MU methods relies on linear parameter
updates via task arithmetic, they suffer from weight entanglement. In this
work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)
that leverages mode connectivity to find an unlearning pathway in a nonlinear
manner. To further enhance performance and efficiency, we introduce a parameter
mask strategy that not only improves unlearning effectiveness but also reduces
computational overhead. Moreover, we propose an adaptive adjustment strategy
for our unlearning penalty coefficient to adaptively balance forgetting quality
and predictive performance during training, eliminating the need for empirical
hyperparameter tuning. Unlike traditional MU methods that identify only a
single unlearning model, MCU uncovers a spectrum of unlearning models along the
pathway. Overall, MCU serves as a plug-and-play framework that seamlessly
integrates with any existing MU methods, consistently improving unlearning
efficacy. Extensive experiments on the image classification task demonstrate
that MCU achieves superior performance.

</details>


### [139] [InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction](https://arxiv.org/abs/2505.10887)
*Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding*

**Main category:** cs.AI

**TL;DR:** A new multimodal agent shows strong performance across different benchmarks.


<details>
  <summary>Details</summary>
**Motivation:** To create a generalist agent capable of interacting with computers in a multimodal manner.

**Method:** Integrating tool-based and pure vision agents within a highly modular architecture.

**Result:** Achieves 7.27% accuracy on OSWorld, higher than Claude-Computer-Use.

**Conclusion:** The agent achieves high accuracy on various benchmarks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InfantAgent-Next%3A+A+Multimodal+Generalist+Agent+for+Automated+Computer+Interaction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10887&send_immediately=true&force_search=false)

**Abstract:** This paper introduces \textsc{InfantAgent-Next}, a generalist agent capable
of interacting with computers in a multimodal manner, encompassing text,
images, audio, and video. Unlike existing approaches that either build
intricate workflows around a single large model or only provide workflow
modularity, our agent integrates tool-based and pure vision agents within a
highly modular architecture, enabling different models to collaboratively solve
decoupled tasks in a step-by-step manner. Our generality is demonstrated by our
ability to evaluate not only pure vision-based real-world benchmarks (i.e.,
OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and
SWE-Bench). Specifically, we achieve $\mathbf{7.27\%}$ accuracy on OSWorld,
higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced
at https://github.com/bin123apple/InfantAgent.

</details>


### [140] [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation](https://arxiv.org/abs/2505.10962)
*Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, Dong Yu*

**Main category:** cs.AI

**TL;DR:** This paper presents MPS-Prover, a novel automated theorem prover that uses an improved data curation method and a multi-perspective tree search mechanism to achieve superior performance on challenging benchmarks.


<details>
  <summary>Details</summary>
**Motivation:** To address inefficiencies and suboptimal proof strategies in existing ATP systems caused by biased search guidance.

**Method:** Introducing MPS-Prover with a post-training data curation strategy and a multi-perspective tree search mechanism integrating a learned critic model with heuristic rules.

**Result:** State-of-the-art performance on miniF2F and ProofNet benchmarks, generating shorter and more diverse proofs than previous methods.

**Conclusion:** MPS-Prover advances the capabilities of LLM-based formal reasoning and provides a robust framework for future theorem provers.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MPS-Prover%3A+Advancing+Stepwise+Theorem+Proving+by+Multi-Perspective+Search+and+Data+Curation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10962，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10962&send_immediately=true&force_search=false)

**Abstract:** Automated Theorem Proving (ATP) in formal languages remains a formidable
challenge in AI, demanding rigorous logical deduction and navigating vast
search spaces. While large language models (LLMs) have shown promising
performance, existing stepwise provers often suffer from biased search
guidance, leading to inefficiencies and suboptimal proof strategies. This paper
introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise
ATP system designed to overcome these limitations. MPS-Prover incorporates two
key innovations: a highly effective post-training data curation strategy that
prunes approximately 40% of redundant training data without sacrificing
performance, and a multi-perspective tree search mechanism. This search
integrates a learned critic model with strategically designed heuristic rules
to diversify tactic selection, prevent getting trapped in unproductive states,
and enhance search robustness. Extensive evaluations demonstrate that
MPS-Prover achieves state-of-the-art performance on multiple challenging
benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter
models. Furthermore, our analyses reveal that MPS-Prover generates
significantly shorter and more diverse proofs compared to existing stepwise and
whole-proof methods, highlighting its efficiency and efficacy. Our work
advances the capabilities of LLM-based formal reasoning and offers a robust
framework and a comprehensive analysis for developing more powerful theorem
provers.

</details>


### [141] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
*Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan*

**Main category:** cs.AI

**TL;DR:** 研究了不同提示策略在大规模语言模型测试时间扩展中的表现，发现简单提示策略更优，并提出了改进扩展性能的方法。


<details>
  <summary>Details</summary>
**Motivation:** 大多数研究都集中在如何通过增加测试时间的计算量来提升大型语言模型的表现，但对各种推理提示策略在扩展时的表现研究较少。

**Method:** 研究了6个LLMs × 8个提示策略 × 6个基准的数据集，并提出了基于概率论的快速预测方法和两种改进扩展性能的方法。

**Result:** 实验结果表明，随着采样时间和计算开销的增加，复杂的提示策略逐渐落后于简单的链式思考提示策略。

**Conclusion:** 实验结果表明复杂的提示策略在初始性能优越的情况下，在增加采样时间和计算开销时逐渐落后于简单的链式思考提示策略。我们对此现象进行了分析并提供了理论证明。此外，我们提出了一种基于概率论的方法，可以快速准确地预测大规模采样下的扩展性能，并选择最佳策略，这可以作为多数投票的测试时间扩展定律。同时，我们还介绍了两种方法来显著提高扩展性能。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Role+of+Prompting+Strategies+in+LLM+Test-Time+Scaling%3A+A+Perspective+of+Probability+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10981&send_immediately=true&force_search=false)

**Abstract:** Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.

</details>


### [142] [Facets in Argumentation: A Formal Approach to Argument Significance](https://arxiv.org/abs/2505.10982)
*Johannes Fichte, Nicolas Fröhlich, Markus Hecher, Victor Lagerkvist, Yasir Mahmood, Arne Meier, Jonathan Persson*

**Main category:** cs.AI

**TL;DR:** This paper introduces a new concept called 'facets' for reasoning between decision and enumeration in abstract argumentation frameworks.


<details>
  <summary>Details</summary>
**Motivation:** The current methods for reasoning in abstract argumentation frameworks are either too simple or too expensive. There is a need for a method that allows for more nuanced reasoning.

**Method:** The authors introduce the concept of 'facets', which are arguments that belong to some extensions but not all extensions. They study the complexity of tasks involving facets and provide an implementation.

**Result:** The tasks involving facets are found to be much easier than counting extensions. An implementation is provided and experiments are conducted to demonstrate feasibility.

**Conclusion:** This paper presents a novel concept for reasoning in abstract argumentation frameworks that could potentially make the process more efficient and effective.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Facets+in+Argumentation%3A+A+Formal+Approach+to+Argument+Significance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10982&send_immediately=true&force_search=false)

**Abstract:** Argumentation is a central subarea of Artificial Intelligence (AI) for
modeling and reasoning about arguments. The semantics of abstract argumentation
frameworks (AFs) is given by sets of arguments (extensions) and conditions on
the relationship between them, such as stable or admissible. Today's solvers
implement tasks such as finding extensions, deciding credulous or skeptical
acceptance, counting, or enumerating extensions. While these tasks are well
charted, the area between decision, counting/enumeration and fine-grained
reasoning requires expensive reasoning so far. We introduce a novel concept
(facets) for reasoning between decision and enumeration. Facets are arguments
that belong to some extensions (credulous) but not to all extensions
(skeptical). They are most natural when a user aims to navigate, filter, or
comprehend the significance of specific arguments, according to their needs. We
study the complexity and show that tasks involving facets are much easier than
counting extensions. Finally, we provide an implementation, and conduct
experiments to demonstrate feasibility.

</details>


### [143] [DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production](https://arxiv.org/abs/2505.10988)
*Joon-Young Kim, Jecheon Yu, Heekyu Kim, Seunghwa Ryu*

**Main category:** cs.AI

**TL;DR:** This study proposes a DRL-based framework for optimizing injection molding process parameters, which can adapt to changes, maintain product quality, maximize profit, and has fast inference speed.


<details>
  <summary>Details</summary>
**Motivation:** Optimizing process parameters to balance product quality and profitability under dynamic environmental and economic conditions is still a challenge.

**Method:** A novel DRL-based framework was presented, integrating product quality and profitability into the control objective. Surrogate models were used to predict product quality and cycle time, and DRL agents were trained using SAC and PPO algorithms.

**Result:** The DRL framework can adapt to seasonal and operational variations, maintain product quality, and maximize profit. It has better inference speed than traditional optimization methods.

**Conclusion:** The DRL framework can dynamically adapt to various changes and maintain product quality while maximizing profit.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DRL-Based+Injection+Molding+Process+Parameter+Optimization+for+Adaptive+and+Profitable+Production，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10988&send_immediately=true&force_search=false)

**Abstract:** Plastic injection molding remains essential to modern manufacturing. However,
optimizing process parameters to balance product quality and profitability
under dynamic environmental and economic conditions remains a persistent
challenge. This study presents a novel deep reinforcement learning (DRL)-based
framework for real-time process optimization in injection molding, integrating
product quality and profitability into the control objective. A profit function
was developed to reflect real-world manufacturing costs, incorporating resin,
mold wear, and electricity prices, including time-of-use variations. Surrogate
models were constructed to predict product quality and cycle time, enabling
efficient offline training of DRL agents using soft actor-critic (SAC) and
proximal policy optimization (PPO) algorithms. Experimental results demonstrate
that the proposed DRL framework can dynamically adapt to seasonal and
operational variations, consistently maintaining product quality while
maximizing profit. Compared to traditional optimization methods such as genetic
algorithms, the DRL models achieved comparable economic performance with up to
135x faster inference speeds, making them well-suited for real-time
applications. The framework's scalability and adaptability highlight its
potential as a foundation for intelligent, data-driven decision-making in
modern manufacturing environments.

</details>


### [144] [RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization](https://arxiv.org/abs/2505.10989)
*Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, Yun Ma*

**Main category:** cs.AI

**TL;DR:** 提出了一种名为RAGSynth的新框架，通过生成合成数据来提高RAG系统的检索器和生成器性能。


<details>
  <summary>Details</summary>
**Motivation:** 现有的检索器严重依赖公共知识，难以应对逻辑复杂性和线索完整性的变化查询，而生成器则经常面临保真度问题。

**Method:** 介绍了一个名为RAGSynth的框架，该框架包括数据构建建模和相应的合成数据生成实现，旨在优化检索器的鲁棒性和生成器的保真度。此外，还提出了SynthBench基准测试，包含来自4个领域的8个领域特定文档，并生成了包含单跳和多跳的大规模合成数据集。

**Result:** 广泛实验表明，合成数据显著提高了检索器的鲁棒性和生成器的保真度。补充评估确认RAGSynth在不同领域也能很好地推广。

**Conclusion:** 通过将优化后的检索器集成到各种RAG范例中，我们始终观察到增强的RAG系统性能。开源实现已在https://github.com/EachSheep/RAGSynth上提供。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAGSynth%3A+Synthetic+Data+for+Robust+and+Faithful+RAG+Component+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10989&send_immediately=true&force_search=false)

**Abstract:** RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various
RAG paradigms, including vanilla, planning-based, and iterative RAG, are built
upon 2 cores: the retriever, which should robustly select relevant documents
across complex queries, and the generator, which should faithfully synthesize
responses. However, existing retrievers rely heavily on public knowledge and
struggle with queries of varying logical complexity and clue completeness,
while generators frequently face fidelity problems. In this work, we introduce
RAGSynth, a framework that includes a data construction modeling and a
corresponding synthetic data generation implementation, designed to optimize
retriever robustness and generator fidelity. Additionally, we present
SynthBench, a benchmark encompassing 8 domain-specific documents across 4
domains, featuring diverse query complexities, clue completeness, and
fine-grained citation granularity. Leveraging RAGSynth, we generate a
large-scale synthetic dataset, including single and multi-hop. Extensive
experiments demonstrate that the synthetic data significantly improves the
robustness of the retrievers and the fidelity of the generators. Additional
evaluations confirm that RAGSynth can also generalize well across different
domains. By integrating the optimized retrievers into various RAG paradigms, we
consistently observe enhanced RAG system performance. We have open-sourced the
implementation on https://github.com/EachSheep/RAGSynth.

</details>


### [145] [Most General Explanations of Tree Ensembles](https://arxiv.org/abs/2505.10991)
*Yacine Izza, Alexey Ignatiev, Joao Marques-Silva, Peter J. Stuckey*

**Main category:** cs.AI

**TL;DR:** Develops a method to find the most general abductive explanation for AI decisions, improving explainability and trust.


<details>
  <summary>Details</summary>
**Motivation:** To explain AI decisions and gain trust by providing understandable reasons.

**Method:** Formal approach using a formal model of the AI system to identify abductive explanations, focusing on inflated abductive explanations for numeric inputs.

**Result:** A method to find the most general abductive explanation for an AI decision, which covers the largest part of the input space.

**Conclusion:** The most general explanation is the best one to provide to humans as it applies to the broadest range of situations.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Most+General+Explanations+of+Tree+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10991&send_immediately=true&force_search=false)

**Abstract:** Explainable Artificial Intelligence (XAI) is critical for attaining trust in
the operation of AI systems. A key question of an AI system is ``why was this
decision made this way''. Formal approaches to XAI use a formal model of the AI
system to identify abductive explanations. While abductive explanations may be
applicable to a large number of inputs sharing the same concrete values, more
general explanations may be preferred for numeric inputs. So-called inflated
abductive explanations give intervals for each feature ensuring that any input
whose values fall withing these intervals is still guaranteed to make the same
prediction. Inflated explanations cover a larger portion of the input space,
and hence are deemed more general explanations. But there can be many
(inflated) abductive explanations for an instance. Which is the best? In this
paper, we show how to find a most general abductive explanation for an AI
decision. This explanation covers as much of the input space as possible, while
still being a correct formal explanation of the model's behaviour. Given that
we only want to give a human one explanation for a decision, the most general
explanation gives us the explanation with the broadest applicability, and hence
the one most likely to seem sensible. (The paper has been accepted at IJCAI2025
conference.)

</details>


### [146] [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049)
*Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi*

**Main category:** cs.AI

**TL;DR:** This paper introduces GuardReasoner-VL, a novel reasoning-based VLM guard model that improves safety through online reinforcement learning.


<details>
  <summary>Details</summary>
**Motivation:** To enhance the safety of VLMs.

**Method:** Constructed a reasoning corpus, used SFT for cold-starting reasoning ability, and enhanced reasoning through online RL with rejection sampling, data augmentation, and a dynamic clipping parameter.

**Result:** The model outperforms the runner-up by 19.27% F1 score on average.

**Conclusion:** GuardReasoner-VL demonstrates superior performance in enhancing VLM safety.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GuardReasoner-VL%3A+Safeguarding+VLMs+via+Reinforced+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11049&send_immediately=true&force_search=false)

**Abstract:** To enhance the safety of VLMs, this paper introduces a novel reasoning-based
VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the
guard model to deliberatively reason before making moderation decisions via
online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with
123K samples and 631K reasoning steps, spanning text, image, and text-image
inputs. Then, based on it, we cold-start our model's reasoning ability via SFT.
In addition, we further enhance reasoning regarding moderation through online
RL. Concretely, to enhance diversity and difficulty of samples, we conduct
rejection sampling followed by data augmentation via the proposed safety-aware
data concatenation. Besides, we use a dynamic clipping parameter to encourage
exploration in early stages and exploitation in later stages. To balance
performance and token efficiency, we design a length-aware safety reward that
integrates accuracy, format, and token cost. Extensive experiments demonstrate
the superiority of our model. Remarkably, it surpasses the runner-up by 19.27%
F1 score on average. We release data, code, and models (3B/7B) of
GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/

</details>


### [147] [Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction](https://arxiv.org/abs/2505.11063)
*Changyue Jiang, Xudong Pan, Min Yang*

**Main category:** cs.AI

**TL;DR:** LLM-based autonomous agents face risks due to potential deviations in their thought processes, which Thought-Aligner aims to mitigate by dynamically correcting high-risk thoughts in real-time, enhancing safety from around 50% to 90% across multiple benchmarks while maintaining low latency.


<details>
  <summary>Details</summary>
**Motivation:** Addressing safety alignment challenges in long-horizon behavioral trajectories of LLM-based autonomous agents where minor deviations in thought can lead to significant safety issues.

**Method:** Proposing Thought-Aligner, a plug-in dynamic thought correction module that modifies reasoning phases without changing the underlying agent framework, trained using contrastive learning on a large dataset of safe and unsafe thought pairs.

**Result:** Thought-Aligner improves agent behavioral safety from approximately 50% to 90% on average across three agent safety benchmarks involving 12 different LLMs, with response latency below 100ms and minimal resource usage.

**Conclusion:** Thought-Aligner offers a practical dynamic safety solution for LLM-based agents, providing enhanced safety, broad applicability, and efficient deployment.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Twice+Before+You+Act%3A+Enhancing+Agent+Behavioral+Safety+with+Thought+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11063&send_immediately=true&force_search=false)

**Abstract:** LLM-based autonomous agents possess capabilities such as reasoning, tool
invocation, and environment interaction, enabling the execution of complex
multi-step tasks. The internal reasoning process, i.e., thought, of behavioral
trajectory significantly influences tool usage and subsequent actions but can
introduce potential risks. Even minor deviations in the agent's thought may
trigger cascading effects leading to irreversible safety incidents. To address
the safety alignment challenges in long-horizon behavioral trajectories, we
propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing
a lightweight and resource-efficient model, Thought-Aligner corrects each
high-risk thought on the fly before each action execution. The corrected
thought is then reintroduced to the agent, ensuring safer subsequent decisions
and tool interactions. Importantly, Thought-Aligner modifies only the reasoning
phase without altering the underlying agent framework, making it easy to deploy
and widely applicable to various agent frameworks. To train the Thought-Aligner
model, we construct an instruction dataset across ten representative scenarios
and simulate ReAct execution trajectories, generating 5,000 diverse
instructions and more than 11,400 safe and unsafe thought pairs. The model is
fine-tuned using contrastive learning techniques. Experiments across three
agent safety benchmarks involving 12 different LLMs demonstrate that
Thought-Aligner raises agent behavioral safety from approximately 50% in the
unprotected setting to 90% on average. Additionally, Thought-Aligner maintains
response latency below 100ms with minimal resource usage, demonstrating its
capability for efficient deployment, broad applicability, and timely
responsiveness. This method thus provides a practical dynamic safety solution
for the LLM-based agents.

</details>


### [148] [A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware](https://arxiv.org/abs/2505.11066)
*Rui Wang, Shichun Yang, Yuyi Chen, Zhuoyang Li, Zexiang Tong, Jianyi Xu, Jiayi Lu, Xinjie Feng, Yaoguang Cao*

**Main category:** cs.AI

**TL;DR:** This paper presents an illumination-aware multi-modal fusion network (IMF) for autonomous vehicles to perceive road terrains under varying lighting conditions.


<details>
  <summary>Details</summary>
**Motivation:** Existing sensors of autonomous vehicles are susceptible to lighting and weather conditions, making it challenging to achieve real-time perception of road conditions.

**Method:** The proposed method introduces an illumination-perception sub-network to estimate illumination features and designs a multi-modal fusion network to dynamically adjust weights of different modalities according to illumination features. It also enhances the optimization process by pre-training the illumination-perception sub-network and incorporating illumination loss as one of the training constraints.

**Result:** Extensive experiments show that the IMF outperforms state-of-the-art methods in accurately perceiving road terrains under varying lighting conditions.

**Conclusion:** The proposed illumination-aware multi-modal fusion network shows a superior performance in perceiving road terrains under varying lighting conditions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-modal+Fusion+Network+for+Terrain+Perception+Based+on+Illumination+Aware，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11066&send_immediately=true&force_search=false)

**Abstract:** Road terrains play a crucial role in ensuring the driving safety of
autonomous vehicles (AVs). However, existing sensors of AVs, including cameras
and Lidars, are susceptible to variations in lighting and weather conditions,
making it challenging to achieve real-time perception of road conditions. In
this paper, we propose an illumination-aware multi-modal fusion network (IMF),
which leverages both exteroceptive and proprioceptive perception and optimizes
the fusion process based on illumination features. We introduce an
illumination-perception sub-network to accurately estimate illumination
features. Moreover, we design a multi-modal fusion network which is able to
dynamically adjust weights of different modalities according to illumination
features. We enhance the optimization process by pre-training of the
illumination-perception sub-network and incorporating illumination loss as one
of the training constraints. Extensive experiments demonstrate that the IMF
shows a superior performance compared to state-of-the-art methods. The
comparison results with single modality perception methods highlight the
comprehensive advantages of multi-modal fusion in accurately perceiving road
terrains under varying lighting conditions. Our dataset is available at:
https://github.com/lindawang2016/IMF.

</details>


### [149] [Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data](https://arxiv.org/abs/2505.11086)
*Keita Kinjo*

**Main category:** cs.AI

**TL;DR:** This study proposes a novel approach to analyze customer journeys on omni-channel platforms, enabling the extraction of typical sequences and detection of important parts for purchase, with applications in improving marketing activities.


<details>
  <summary>Details</summary>
**Motivation:** Quantitative study and comprehensive analysis of customer journeys on omni-channel platforms to develop effective marketing strategies.

**Method:** A novel approach comprising three steps for analyzing customer journeys: identifying representative sequences, predicting purchase likelihood, and recommending counterfactual sequences to increase purchase probability.

**Result:** Typical sequences were extracted and important parts for purchase were detected.

**Conclusion:** The proposed approach can support improvements in various marketing activities.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analysis+of+Customer+Journeys+Using+Prototype+Detection+and+Counterfactual+Explanations+for+Sequential+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11086，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11086&send_immediately=true&force_search=false)

**Abstract:** Recently, the proliferation of omni-channel platforms has attracted interest
in customer journeys, particularly regarding their role in developing marketing
strategies. However, few efforts have been taken to quantitatively study or
comprehensively analyze them owing to the sequential nature of their data and
the complexity involved in analysis. In this study, we propose a novel approach
comprising three steps for analyzing customer journeys. First, the distance
between sequential data is defined and used to identify and visualize
representative sequences. Second, the likelihood of purchase is predicted based
on this distance. Third, if a sequence suggests no purchase, counterfactual
sequences are recommended to increase the probability of a purchase using a
proposed method, which extracts counterfactual explanations for sequential
data. A survey was conducted, and the data were analyzed; the results revealed
that typical sequences could be extracted, and the parts of those sequences
important for purchase could be detected. We believe that the proposed approach
can support improvements in various marketing activities.

</details>


### [150] [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity](https://arxiv.org/abs/2505.11107)
*Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, Da-shan Shiu*

**Main category:** cs.AI

**TL;DR:** A new method called Group Think is proposed where a single large language model acts as multiple concurrent reasoning agents. It improves reasoning quality and reduces latency compared to traditional turn-based methods.


<details>
  <summary>Details</summary>
**Motivation:** To enhance reasoning quality and efficiency by allowing multiple reasoning agents within a single large language model to collaborate concurrently rather than turn-by-turn.

**Method:** Introducing Group Think, which enables a single LLM to function as multiple concurrent thinkers with shared visibility of each other's progress, allowing dynamic adaptation at the token level.

**Result:** Group Think reduces redundant reasoning, improves quality, and achieves lower latency. It efficiently uses idle computational resources, making it ideal for edge inference. A modification is provided to enable any existing LLM to perform Group Think on a local GPU.

**Conclusion:** This work introduces a novel concurrent-reasoning paradigm that could lead to more sophisticated and efficient collaborative behaviors in future LLMs for higher quality generation.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Group+Think%3A+Multiple+Concurrent+Reasoning+Agents+Collaborating+at+Token+Level+Granularity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11107&send_immediately=true&force_search=false)

**Abstract:** Recent advances in large language models (LLMs) have demonstrated the power
of reasoning through self-generated chains of thought. Multiple reasoning
agents can collaborate to raise joint reasoning quality above individual
outcomes. However, such agents typically interact in a turn-based manner,
trading increased latency for improved quality. In this paper, we propose Group
Think--a single LLM that acts as multiple concurrent reasoning agents, or
thinkers. With shared visibility into each other's partial generation progress,
Group Think introduces a new concurrent-reasoning paradigm in which multiple
reasoning trajectories adapt dynamically to one another at the token level. For
example, a reasoning thread may shift its generation mid-sentence upon
detecting that another thread is better positioned to continue. This
fine-grained, token-level collaboration enables Group Think to reduce redundant
reasoning and improve quality while achieving significantly lower latency.
Moreover, its concurrent nature allows for efficient utilization of idle
computational resources, making it especially suitable for edge inference,
where very small batch size often underutilizes local~GPUs. We give a simple
and generalizable modification that enables any existing LLM to perform Group
Think on a local GPU. We also present an evaluation strategy to benchmark
reasoning latency and empirically demonstrate latency improvements using
open-source LLMs that were not explicitly trained for Group Think. We hope this
work paves the way for future LLMs to exhibit more sophisticated and more
efficient collaborative behavior for higher quality generation.

</details>


### [151] [Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach](https://arxiv.org/abs/2505.11119)
*Jiabei Cheng, Zhen-Qun Yang, Jiannong Cao, Yu Yang, Xinzhe Zheng*

**Main category:** cs.AI

**TL;DR:** Predicting student dropouts early is crucial for interventions. This paper introduces the Dual-Modal Multiscale Sliding Window (DMSW) Model that uses both academic and behavioral data to predict dropouts with higher accuracy than traditional methods.


<details>
  <summary>Details</summary>
**Motivation:** The need for timely dropout prediction and improving educational outcomes.

**Method:** Proposing the Dual-Modal Multiscale Sliding Window (DMSW) Model which uses academic and behavioral data to predict dropout risks.

**Result:** The DMSW model improved prediction accuracy by 15% compared to traditional methods.

**Conclusion:** The study bridges the gap between theory and practice in dropout prediction, providing educators with a tool to enhance student retention.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Student+Dropout+Risk+With+A+Dual-Modal+Abrupt+Behavioral+Changes+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11119&send_immediately=true&force_search=false)

**Abstract:** Timely prediction of students at high risk of dropout is critical for early
intervention and improving educational outcomes. However, in offline
educational settings, poor data quality, limited scale, and high heterogeneity
often hinder the application of advanced machine learning models. Furthermore,
while educational theories provide valuable insights into dropout phenomena,
the lack of quantifiable metrics for key indicators limits their use in
data-driven modeling. Through data analysis and a review of educational
literature, we identified abrupt changes in student behavior as key early
signals of dropout risk. To address this, we propose the Dual-Modal Multiscale
Sliding Window (DMSW) Model, which integrates academic performance and
behavioral data to dynamically capture behavior patterns using minimal data.
The DMSW model improves prediction accuracy by 15% compared to traditional
methods, enabling educators to identify high-risk students earlier, provide
timely support, and foster a more inclusive learning environment. Our analysis
highlights key behavior patterns, offering practical insights for preventive
strategies and tailored support. These findings bridge the gap between theory
and practice in dropout prediction, giving educators an innovative tool to
enhance student retention and outcomes.

</details>


### [152] [Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining](https://arxiv.org/abs/2505.11122)
*Yu Shi, Yitong Duan, Jian Li*

**Main category:** cs.AI

**TL;DR:** This paper introduces a novel framework integrating LLMs with MCTS for efficient and interpretable alpha factor mining in quantitative investment.


<details>
  <summary>Details</summary>
**Motivation:** Traditional formulaic alpha mining relies on human expertise, contemporary automated methods often suffer from search inefficiency or yield poorly interpretable alpha factors.

**Method:** Integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration.

**Result:** Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods.

**Conclusion:** Our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy, trading performance, and improved interpretability.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Navigating+the+Alpha+Jungle%3A+An+LLM-Powered+MCTS+Framework+for+Formulaic+Factor+Mining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11122，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11122&send_immediately=true&force_search=false)

**Abstract:** Alpha factor mining is pivotal in quantitative investment for identifying
predictive signals from complex financial data. While traditional formulaic
alpha mining relies on human expertise, contemporary automated methods, such as
those based on genetic programming or reinforcement learning, often suffer from
search inefficiency or yield poorly interpretable alpha factors. This paper
introduces a novel framework that integrates Large Language Models (LLMs) with
Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach
leverages the LLM's instruction-following and reasoning capability to
iteratively generate and refine symbolic alpha formulas within an MCTS-driven
exploration. A key innovation is the guidance of MCTS exploration by rich,
quantitative feedback from financial backtesting of each candidate factor,
enabling efficient navigation of the vast search space. Furthermore, a frequent
subtree avoidance mechanism is introduced to bolster search efficiency and
alpha factor performance. Experimental results on real-world stock market data
demonstrate that our LLM-based framework outperforms existing methods by mining
alphas with superior predictive accuracy, trading performance, and improved
interpretability, while offering a more efficient solution for formulaic alpha
mining.

</details>


### [153] [Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets](https://arxiv.org/abs/2505.11135)
*Patrick Stöckermann, Henning Südfeld, Alessandro Immordino, Thomas Altenmüller, Marc Wegmann, Martin Gebser, Konstantin Schekotihin, Georg Seidel, Chew Wye Chan, Fei Fei Zhang*

**Main category:** cs.AI

**TL;DR:** This study compares reinforcement learning methods, specifically policy-gradient and evolution strategies, using both open-source simulation models and a real industry dataset to evaluate scalability with complexity in semiconductor scheduling.


<details>
  <summary>Details</summary>
**Motivation:** Benchmark datasets lack real-world complexity and constraints, which hinders the evaluation of optimization methods.

**Method:** The research uses open-source simulation models and a real industry dataset to test reinforcement learning methods.

**Result:** Evolution Strategies show better scalability than policy-gradient methods, and selecting relevant bottleneck tools improves efficiency. A diverse training dataset aids in generalization. The approach is computationally expensive but scales well with CPU cores. Evolution Strategies improve tardiness by up to 4% and throughput by up to 1% for the real industry dataset, and tardiness by double digits and throughput by single digits for open-source models.

**Conclusion:** Evolution Strategies prove more effective than policy-gradient methods for realistic semiconductor frontend fab simulations.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalability+of+Reinforcement+Learning+Methods+for+Dispatching+in+Semiconductor+Frontend+Fabs%3A+A+Comparison+of+Open-Source+Models+with+Real+Industry+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11135&send_immediately=true&force_search=false)

**Abstract:** Benchmark datasets are crucial for evaluating approaches to scheduling or
dispatching in the semiconductor industry during the development and deployment
phases. However, commonly used benchmark datasets like the Minifab or SMT2020
lack the complex details and constraints found in real-world scenarios. To
mitigate this shortcoming, we compare open-source simulation models with a real
industry dataset to evaluate how optimization methods scale with different
levels of complexity. Specifically, we focus on Reinforcement Learning methods,
performing optimization based on policy-gradient and Evolution Strategies. Our
research provides insights into the effectiveness of these optimization methods
and their applicability to realistic semiconductor frontend fab simulations. We
show that our proposed Evolution Strategies-based method scales much better
than a comparable policy-gradient-based approach. Moreover, we identify the
selection and combination of relevant bottleneck tools to control by the agent
as crucial for an efficient optimization. For the generalization across
different loading scenarios and stochastic tool failure patterns, we achieve
advantages when utilizing a diverse training dataset. While the overall
approach is computationally expensive, it manages to scale well with the number
of CPU cores used for training. For the real industry dataset, we achieve an
improvement of up to 4% regarding tardiness and up to 1% regarding throughput.
For the less complex open-source models Minifab and SMT2020, we observe
double-digit percentage improvement in tardiness and single digit percentage
improvement in throughput by use of Evolution Strategies.

</details>


### [154] [Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design](https://arxiv.org/abs/2505.11136)
*Janik Bischoff, Alexandru Rinciog, Anne Meyer*

**Main category:** cs.AI

**TL;DR:** This paper presents a new reinforcement learning design to optimize charging strategies for robots in large warehouses. It compares different reward and action space setups and finds a balance between flexible and guided approaches.


<details>
  <summary>Details</summary>
**Motivation:** Optimizing charging strategies for autonomous mobile robots in large-scale warehouses.

**Method:** Proposing a novel reinforcement learning design and comparing it with heuristic strategies, focusing on reward and action space configurations.

**Result:** Flexible RL-based approaches outperform heuristic strategies in service times but have longer convergence times and are less stable compared to guided configurations.

**Conclusion:** The study contributes to the field by extending a simulation framework, introducing a new RL design, and evaluating it with various adaptive baseline heuristics.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+for+AMR+Charging+Decisions%3A+The+Impact+of+Reward+and+Action+Space+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11136，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11136&send_immediately=true&force_search=false)

**Abstract:** We propose a novel reinforcement learning (RL) design to optimize the
charging strategy for autonomous mobile robots in large-scale block stacking
warehouses. RL design involves a wide array of choices that can mostly only be
evaluated through lengthy experimentation. Our study focuses on how different
reward and action space configurations, ranging from flexible setups to more
guided, domain-informed design configurations, affect the agent performance.
Using heuristic charging strategies as a baseline, we demonstrate the
superiority of flexible, RL-based approaches in terms of service times.
Furthermore, our findings highlight a trade-off: While more open-ended designs
are able to discover well-performing strategies on their own, they may require
longer convergence times and are less stable, whereas guided configurations
lead to a more stable learning process but display a more limited
generalization potential. Our contributions are threefold. First, we extend
SLAPStack, an open-source, RL-compatible simulation-framework to accommodate
charging strategies. Second, we introduce a novel RL design for tackling the
charging strategy problem. Finally, we introduce several novel adaptive
baseline heuristics and reproducibly evaluate the design using a Proximal
Policy Optimization agent and varying different design configurations, with a
focus on reward.

</details>


### [155] [Feasibility with Language Models for Open-World Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.11181)
*Jae Myung Kim, Stephan Alaniz, Cordelia Schmid, Zeynep Akata*

**Main category:** cs.AI

**TL;DR:** This paper introduces FLM (Feasibility with Language Model), which uses large language models to determine the feasibility of state-object combinations in Open-World Compositional Zero-Shot Learning.


<details>
  <summary>Details</summary>
**Motivation:** Zero-shot predictors perform poorly when considering all possible state-object combinations as unseen classes.

**Method:** FLM queries an LLM about the feasibility of a given pair and retrieves the output logit for the positive answer. It also explores the in-context learning ability of LLMs.

**Result:** FLM consistently improves OW-CZSL performance across all three benchmarks.

**Conclusion:** The use of external auxiliary knowledge with large language models can effectively enhance the understanding of semantic relationships between states and objects.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feasibility+with+Language+Models+for+Open-World+Compositional+Zero-Shot+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11181&send_immediately=true&force_search=false)

**Abstract:** Humans can easily tell if an attribute (also called state) is realistic,
i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In
Open-World Compositional Zero-Shot Learning, when all possible state-object
combinations are considered as unseen classes, zero-shot predictors tend to
perform poorly. Our work focuses on using external auxiliary knowledge to
determine the feasibility of state-object combinations. Our Feasibility with
Language Model (FLM) is a simple and effective approach that leverages Large
Language Models (LLMs) to better comprehend the semantic relationships between
states and objects. FLM involves querying an LLM about the feasibility of a
given pair and retrieving the output logit for the positive answer. To mitigate
potential misguidance of the LLM given that many of the state-object
compositions are rare or completely infeasible, we observe that the in-context
learning ability of LLMs is essential. We present an extensive study
identifying Vicuna and ChatGPT as best performing, and we demonstrate that our
FLM consistently improves OW-CZSL performance across all three benchmarks.

</details>


### [156] [Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP](https://arxiv.org/abs/2505.11189)
*Francesco Sovrano*

**Main category:** cs.AI

**TL;DR:** This paper explores the effectiveness of global XAI methods in identifying misinformation-related biases in large language models.


<details>
  <summary>Details</summary>
**Motivation:** To address the potential negative impact of generative AI systems on the UN Sustainable Development Goals, the authors aim to uncover misbehaviors or biases within these systems.

**Method:** The researchers developed a text-to-ordinal mapping strategy to convert non-numerical inputs/outputs into numerical features, allowing XAI tools to identify certain biases in LLM-generated content. They then injected non-linear biases into LLMs and used global XAI methods to detect them.

**Result:** RuleFit had difficulty with conjunctive and non-convex biases, while SHAP could approximate conjunctive biases but couldn't express them as actionable rules. RuleSHAP was introduced, which improved injected bias detection over RuleFit by +94% (MRR@1) on average.

**Conclusion:** Global XAI methods can be effective in detecting biases in LLMs, especially when combined with novel strategies like RuleSHAP.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Global+XAI+Methods+Reveal+Injected+Bias+in+LLMs%3F+SHAP+vs+Rule+Extraction+vs+RuleSHAP，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11189&send_immediately=true&force_search=false)

**Abstract:** Generative AI systems can help spread information but also misinformation and
biases, potentially undermining the UN Sustainable Development Goals (SDGs).
Explainable AI (XAI) aims to reveal the inner workings of AI systems and expose
misbehaviours or biases. However, current XAI tools, built for simpler models,
struggle to handle the non-numerical nature of large language models (LLMs).
This paper examines the effectiveness of global XAI methods, such as
rule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we
first show a text-to-ordinal mapping strategy to convert non-numerical
inputs/outputs into numerical features, enabling these tools to identify (some)
misinformation-related biases in LLM-generated content. Then, we inject
non-linear biases of varying complexity (univariate, conjunctive, and
non-convex) into widespread LLMs like ChatGPT and Llama via system
instructions, using global XAI methods to detect them. This way, we found that
RuleFit struggles with conjunctive and non-convex biases, while SHAP can
approximate conjunctive biases but cannot express them as actionable rules.
Hence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP
and RuleFit to detect more non-univariate biases, improving injected bias
detection over RuleFit by +94% (MRR@1) on average.

</details>


### [157] [Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration](https://arxiv.org/abs/2505.11191)
*Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour*

**Main category:** cs.AI

**TL;DR:** 本文提出了Federated Foundation Models (FFMs)，这是一种新的范式，旨在通过结合多模态多任务基础模型和联邦学习的优势，来解决具身AI系统的复杂挑战。


<details>
  <summary>Details</summary>
**Motivation:** 随着具身AI系统变得越来越多模态、个性化和交互式，需要能够快速、上下文感知适应的机器学习模型，同时平衡模型的通用性和个性化。

**Method:** 结合了多模态多任务基础模型（M3T FMs）和联邦学习（FL）的优势。

**Result:** 收集了具身AI生态系统中FFMs的关键部署维度，并提出了一个评估框架。

**Conclusion:** 提出了一种新的范式 Federated Foundation Models (FFMs)，以满足实际应用中复杂且多样的能力需求。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Multi-Task+%28M3T%29+Federated+Foundation+Models+for+Embodied+AI%3A+Potentials+and+Challenges+for+Edge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11191&send_immediately=true&force_search=false)

**Abstract:** As embodied AI systems become increasingly multi-modal, personalized, and
interactive, they must learn effectively from diverse sensory inputs, adapt
continually to user preferences, and operate safely under resource and privacy
constraints. These challenges expose a pressing need for machine learning
models capable of swift, context-aware adaptation while balancing model
generalization and personalization. Here, two methods emerge as suitable
candidates, each offering parts of these capabilities: Foundation Models (FMs)
provide a pathway toward generalization across tasks and modalities, whereas
Federated Learning (FL) offers the infrastructure for distributed,
privacy-preserving model updates and user-level model personalization. However,
when used in isolation, each of these approaches falls short of meeting the
complex and diverse capability requirements of real-world embodied
environments. In this vision paper, we introduce Federated Foundation Models
(FFMs) for embodied AI, a new paradigm that unifies the strengths of
multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature
of FL, enabling intelligent systems at the wireless edge. We collect critical
deployment dimensions of FFMs in embodied AI ecosystems under a unified
framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness
and imbalance, Bandwidth and compute constraints, On-device continual learning,
Distributed control and autonomy, and Yielding safety, privacy, and
personalization. For each, we identify concrete challenges and envision
actionable research directions. We also present an evaluation framework for
deploying FFMs in embodied AI systems, along with the associated trade-offs.

</details>


### [158] [GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning](https://arxiv.org/abs/2505.11208)
*Dongjun Kim, Junwoo Park, Chaehyeon Shin, Jaeheon Jung, Kyungho Shin, Seungheon Baek, Sanghyuk Heo, Woongrae Kim, Inchul Jeong, Joohwan Cho, Jongsun Park*

**Main category:** cs.AI

**TL;DR:** GLOVA is an analog circuit sizing framework that improves robustness against PVT variations by leveraging risk-sensitive reinforcement learning and an ensemble-based critic. It also reduces simulation costs with μ-σ evaluation and simulation reordering methods.


<details>
  <summary>Details</summary>
**Motivation:** To address performance degradation from PVT variations and reduce time-to-market for commercial-grade reliability in analog/mixed-signal circuit design.

**Method:** Risk-sensitive reinforcement learning, ensemble-based critic, μ-σ evaluation, and simulation reordering method.

**Result:** Up to 80.5× improvement in sample efficiency and 76.0× reduction in time compared to previous state-of-the-art frameworks.

**Conclusion:** GLOVA effectively manages diverse random mismatches to improve robustness against PVT variations, offering significant improvements in both sample efficiency and time reduction.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GLOVA%3A+Global+and+Local+Variation-Aware+Analog+Circuit+Design+with+Risk-Sensitive+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11208&send_immediately=true&force_search=false)

**Abstract:** Analog/mixed-signal circuit design encounters significant challenges due to
performance degradation from process, voltage, and temperature (PVT)
variations. To achieve commercial-grade reliability, iterative manual design
revisions and extensive statistical simulations are required. While several
studies have aimed to automate variation aware analog design to reduce
time-to-market, the substantial mismatches in real-world wafers have not been
thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing
framework that effectively manages the impact of diverse random mismatches to
improve robustness against PVT variations. In the proposed approach,
risk-sensitive reinforcement learning is leveraged to account for the
reliability bound affected by PVT variations, and ensemble-based critic is
introduced to achieve sample-efficient learning. For design verification, we
also propose $\mu$-$\sigma$ evaluation and simulation reordering method to
reduce simulation costs of identifying failed designs. GLOVA supports
verification through industrial-level PVT variation evaluation methods,
including corner simulation as well as global and local Monte Carlo (MC)
simulations. Compared to previous state-of-the-art variation-aware analog
sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample
efficiency and 76.0$\times$ reduction in time.

</details>


### [159] [Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs](https://arxiv.org/abs/2505.11227)
*Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang*

**Main category:** cs.AI

**TL;DR:** 本研究探讨了强化学习和过程奖励模型在大语言模型推理能力中的作用，发现纯强化学习能有效提升推理能力，且提出的新方法在特定条件下有所改善但仍有局限性。


<details>
  <summary>Details</summary>
**Motivation:** 研究大语言模型的推理能力发展，特别是强化学习和过程奖励模型在其中的作用。

**Method:** 系统研究强化学习训练与过程奖励模型能力之间的关系，并提出了一种新的自我奖励机制（Self-PRM）。

**Result:** 纯强化学习训练可以提高解决问题的能力和过程监督能力，而当前的过程奖励模型在最先进的模型上表现不如简单的多数投票基准。

**Conclusion:** 过程奖励模型可能不是提升复杂推理能力所必需的，纯强化学习不仅能提高问题解决技能，还能促进强大的过程奖励模型能力的发展。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+PRM+Necessary%3F+Problem-Solving+RL+Implicitly+Induces+PRM+Capability+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11227&send_immediately=true&force_search=false)

**Abstract:** The development of reasoning capabilities represents a critical frontier in
large language models (LLMs) research, where reinforcement learning (RL) and
process reward models (PRMs) have emerged as predominant methodological
frameworks. Contrary to conventional wisdom, empirical evidence from
DeepSeek-R1 demonstrates that pure RL training focused on mathematical
problem-solving can progressively enhance reasoning abilities without PRM
integration, challenging the perceived necessity of process supervision. In
this study, we conduct a systematic investigation of the relationship between
RL training and PRM capabilities. Our findings demonstrate that problem-solving
proficiency and process supervision capabilities represent complementary
dimensions of reasoning that co-evolve synergistically during pure RL training.
In particular, current PRMs underperform simple baselines like majority voting
when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To
address this limitation, we propose Self-PRM, an introspective framework in
which models autonomously evaluate and rerank their generated solutions through
self-reward mechanisms. Although Self-PRM consistently improves the accuracy of
the benchmark (particularly with larger sample sizes), analysis exposes
persistent challenges: The approach exhibits low precision (<10\%) on difficult
problems, frequently misclassifying flawed solutions as valid. These analyses
underscore the need for continued RL scaling to improve reward alignment and
introspective accuracy. Overall, our findings suggest that PRM may not be
essential for enhancing complex reasoning, as pure RL not only improves
problem-solving skills but also inherently fosters robust PRM capabilities. We
hope these findings provide actionable insights for building more reliable and
self-aware complex reasoning models.

</details>


### [160] [LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios](https://arxiv.org/abs/2505.11247)
*Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, Jun Ma*

**Main category:** cs.AI

**TL;DR:** A novel framework named LD-Scene is proposed, which combines LLMs and LDMs to enable user-controllable adversarial scenario generation using natural language.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods for evaluating autonomous driving systems face challenges due to the rarity of safety-critical scenarios and the need for extensive expert knowledge.

**Method:** The method involves an LDM that captures realistic driving trajectories and an LLM-based guidance module that translates user queries into adversarial loss functions.

**Result:** Experiments on the nuScenes dataset show that LD-Scene generates realistic, diverse, and effective adversarial scenarios with fine-grained control over adversarial behaviors.

**Conclusion:** The proposed LD-Scene framework demonstrates state-of-the-art performance in generating adversarial scenarios for autonomous driving systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LD-Scene%3A+LLM-Guided+Diffusion+for+Controllable+Generation+of+Adversarial+Safety-Critical+Driving+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11247，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11247&send_immediately=true&force_search=false)

**Abstract:** Ensuring the safety and robustness of autonomous driving systems necessitates
a comprehensive evaluation in safety-critical scenarios. However, these
safety-critical scenarios are rare and difficult to collect from real-world
driving data, posing significant challenges to effectively assessing the
performance of autonomous vehicles. Typical existing methods often suffer from
limited controllability and lack user-friendliness, as extensive expert
knowledge is essentially required. To address these challenges, we propose
LD-Scene, a novel framework that integrates Large Language Models (LLMs) with
Latent Diffusion Models (LDMs) for user-controllable adversarial scenario
generation through natural language. Our approach comprises an LDM that
captures realistic driving trajectory distributions and an LLM-based guidance
module that translates user queries into adversarial loss functions,
facilitating the generation of scenarios aligned with user queries. The
guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator
and an LLM-based code debugger, enhancing the controllability and robustness in
generating guidance functions. Extensive experiments conducted on the nuScenes
dataset demonstrate that LD-Scene achieves state-of-the-art performance in
generating realistic, diverse, and effective adversarial scenarios.
Furthermore, our framework provides fine-grained control over adversarial
behaviors, thereby facilitating more effective testing tailored to specific
driving scenarios.

</details>


### [161] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
*Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, Zhifang Sui*

**Main category:** cs.AI

**TL;DR:** This paper presents SelfBudgeter, a self-adaptive controllable reasoning strategy that improves efficiency by pre-estimating reasoning costs and using budget-guided GPRO for reinforcement learning. It achieves significant response length compression (up to 74.47% on MATH benchmark) with minimal accuracy loss.


<details>
  <summary>Details</summary>
**Motivation:** Reasoning models inefficiently process trivial and complex queries, causing resource waste and long user latency.

**Method:** Proposes a dual-phase training paradigm including pre-estimating reasoning cost and budget-guided GPRO for reinforcement learning, also allows direct manipulation of reasoning length via pre-filling token budget.

**Result:** Achieves up to 74.47% response length compression on MATH benchmark while maintaining nearly undiminished accuracy.

**Conclusion:** SelfBudgeter is an effective method for efficient reasoning, allowing users to control generation time and make informed decisions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SelfBudgeter%3A+Adaptive+Token+Allocation+for+Efficient+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11274&send_immediately=true&force_search=false)

**Abstract:** Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


### [162] [Meta-World+: An Improved, Standardized, RL Benchmark](https://arxiv.org/abs/2505.11289)
*Reginald McLean, Evangelos Chatzaroulas, Luc McCutcheon, Frank Röder, Tianhe Yu, Zhanpeng He, K. R. Zentner, Ryan Julian, J K Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro*

**Main category:** cs.AI

**TL;DR:** This work addresses inconsistencies in the Meta-World platform for evaluating multi-task and meta-reinforcement learning agents, releasing a new version with improved reproducibility, technical ergonomics, and user control over task sets.


<details>
  <summary>Details</summary>
**Motivation:** To resolve discrepancies in algorithm comparisons due to undocumented changes in Meta-World since its introduction.

**Method:** Analyzing past versions of Meta-World to gain insights into benchmark design.

**Result:** Release of a new open-source Meta-World version with better reproducibility, technical features, and task set control.

**Conclusion:** The new Meta-World version aims to facilitate fairer comparisons and enhance research in multi-task and meta-reinforcement learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-World%2B%3A+An+Improved%2C+Standardized%2C+RL+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11289&send_immediately=true&force_search=false)

**Abstract:** Meta-World is widely used for evaluating multi-task and meta-reinforcement
learning agents, which are challenged to master diverse skills simultaneously.
Since its introduction however, there have been numerous undocumented changes
which inhibit a fair comparison of algorithms. This work strives to
disambiguate these results from the literature, while also leveraging the past
versions of Meta-World to provide insights into multi-task and
meta-reinforcement learning benchmark design. Through this process we release a
new open-source version of Meta-World
(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility
of past results, is more technically ergonomic, and gives users more control
over the tasks that are included in a task set.

</details>


### [163] [Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps](https://arxiv.org/abs/2505.11451)
*Lee Harris, James Bentham, Philippe De Wilde*

**Main category:** cs.AI

**TL;DR:** Regular expression synthesis can be used to create regular expressions that identify complex dates and date ranges in transcribed medical text.


<details>
  <summary>Details</summary>
**Motivation:** Extracting dates from medical documents is challenging due to the complexity and variability of date formats.

**Method:** The study tested publicly-available regular expressions, manually created easily-decomposable regular expressions, and used regular expression synthesis to automatically identify regular expressions from reverse-engineered UNIX timestamps.

**Result:** Manually created regular expressions detected most real dates but also many non-date sequences, while regular expressions synthesized automatically detected fewer non-date sequences but missed more real dates.

**Conclusion:** Regular expression synthesis is a novel approach for learning deterministic logic to identify complex dates and date ranges in text transcriptions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extracting+Explainable+Dates+From+Medical+Images+By+Reverse-Engineering+UNIX+Timestamps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11451&send_immediately=true&force_search=false)

**Abstract:** Dates often contribute towards highly impactful medical decisions, but it is
rarely clear how to extract this data. AI has only just begun to be used
transcribe such documents, and common methods are either to trust that the
output produced by a complex AI model, or to parse the text using regular
expressions. Recent work has established that regular expressions are an
explainable form of logic, but it is difficult to decompose these into the
component parts that are required to construct precise UNIX timestamps. First,
we test publicly-available regular expressions, and we found that these were
unable to capture a significant number of our dates. Next, we manually created
easily-decomposable regular expressions, and we found that these were able to
detect the majority of real dates, but also a lot of sequences of text that
look like dates. Finally, we used regular expression synthesis to automatically
identify regular expressions from the reverse-engineered UNIX timestamps that
we created. We find that regular expressions created by regular expression
synthesis detect far fewer sequences of text that look like dates than those
that were manually created, at the cost of a slight increase to the number of
missed dates. Overall, our results show that regular expressions can be created
through regular expression synthesis to identify complex dates and date ranges
in text transcriptions. To our knowledge, our proposed way of learning
deterministic logic by reverse-engineering several many-one mappings and
feeding these into a regular expression synthesiser is a new approach.

</details>


### [164] [Automatic Reward Shaping from Confounded Offline Data](https://arxiv.org/abs/2505.11478)
*Mingxuan Li, Junzhe Zhang, Elias Bareinboim*

**Main category:** cs.AI

**TL;DR:** This paper introduces a new deep reinforcement learning algorithm based on Deep Q-Network (DQN) that is robust to confounding biases in observed data. It aims to learn safe policies in the worst-case environments for twelve confounded Atari games.


<details>
  <summary>Details</summary>
**Motivation:** To address off-policy learning from biased data in complex and high-dimensional domains where unobserved confounding cannot be ignored.

**Method:** Propose a novel deep reinforcement learning algorithm based on DQN to find a safe policy for the worst-case environment compatible with the observations.

**Result:** The proposed method consistently outperforms the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.

**Conclusion:** This study contributes a robust deep reinforcement learning algorithm against confounding biases.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automatic+Reward+Shaping+from+Confounded+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11478&send_immediately=true&force_search=false)

**Abstract:** A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [165] [MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation](https://arxiv.org/abs/2505.11481)
*Alayt Issak, Jeba Rezwana, Casper Harteveld*

**Main category:** cs.AI

**TL;DR:** This paper introduces MOSAAIC, a new framework for balancing control in human-AI co-creation. It defines three key control dimensions and analyzes six existing co-creative AI case studies.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenge of balancing human and AI contributions in co-creative AI systems.

**Method:** Systematic literature review of 172 papers and introduction of MOSAAIC framework.

**Result:** MOSAAIC identifies three dimensions of control (autonomy, initiative, and authority) and provides optimization strategies.

**Conclusion:** The MOSAAIC framework can be used to analyze and balance control in existing co-creative AI systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MOSAAIC%3A+Managing+Optimization+towards+Shared+Autonomy%2C+Authority%2C+and+Initiative+in+Co-creation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11481&send_immediately=true&force_search=false)

**Abstract:** Striking the appropriate balance between humans and co-creative AI is an open
research question in computational creativity. Co-creativity, a form of hybrid
intelligence where both humans and AI take action proactively, is a process
that leads to shared creative artifacts and ideas. Achieving a balanced dynamic
in co-creativity requires characterizing control and identifying strategies to
distribute control between humans and AI. We define control as the power to
determine, initiate, and direct the process of co-creation. Informed by a
systematic literature review of 172 full-length papers, we introduce MOSAAIC
(Managing Optimization towards Shared Autonomy, Authority, and Initiative in
Co-creation), a novel framework for characterizing and balancing control in
co-creation. MOSAAIC identifies three key dimensions of control: autonomy,
initiative, and authority. We supplement our framework with control
optimization strategies in co-creation. To demonstrate MOSAAIC's applicability,
we analyze the distribution of control in six existing co-creative AI case
studies and present the implications of using this framework.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [166] [An Exponential Averaging Process with Strong Convergence Properties](https://arxiv.org/abs/2505.10605)
*Frederik Köhne, Anton Schiela*

**Main category:** stat.ML

**TL;DR:** This work introduces p-EMA, an adaptation of exponential moving averaging for smoothing noisy observations along trajectories of random dynamical systems. It provides stochastic convergence guarantees and discusses its application in adaptive step size control for SGD.


<details>
  <summary>Details</summary>
**Motivation:** To improve the stochastic convergence properties of exponential moving averaging by allowing weights of recent observations to decrease to zero.

**Method:** Introduce p-EMA, analyze its stochastic convergence under mild assumptions, and discuss its use in adaptive step size control for SGD.

**Result:** Stochastic convergence guarantees for p-EMA under mild assumptions on the autocorrelations of the underlying random dynamical system.

**Conclusion:** p-EMA improves upon traditional EMA by enabling stronger stochastic convergence properties, with potential applications in adaptive step size control for SGD.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Exponential+Averaging+Process+with+Strong+Convergence+Properties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10605&send_immediately=true&force_search=false)

**Abstract:** Averaging, or smoothing, is a fundamental approach to obtain stable,
de-noised estimates from noisy observations. In certain scenarios, observations
made along trajectories of random dynamical systems are of particular interest.
One popular smoothing technique for such a scenario is exponential moving
averaging (EMA), which assigns observations a weight that decreases
exponentially in their age, thus giving younger observations a larger weight.
However, EMA fails to enjoy strong stochastic convergence properties, which
stems from the fact that the weight assigned to the youngest observation is
constant over time, preventing the noise in the averaged quantity from
decreasing to zero. In this work, we consider an adaptation to EMA, which we
call $p$-EMA, where the weights assigned to the last observations decrease to
zero at a subharmonic rate. We provide stochastic convergence guarantees for
this kind of averaging under mild assumptions on the autocorrelations of the
underlying random dynamical system. We further discuss the implications of our
results for a recently introduced adaptive step size control for Stochastic
Gradient Descent (SGD), which uses $p$-EMA for averaging noisy observations.

</details>


### [167] [Minimax learning rates for estimating binary classifiers under margin conditions](https://arxiv.org/abs/2505.10628)
*Jonathan García, Philipp Petersen*

**Main category:** stat.ML

**TL;DR:** Study establishes learning rate bounds for classification problems with specific function classes and geometric margin conditions.


<details>
  <summary>Details</summary>
**Motivation:** To address theoretical challenges in deriving lower bounds under geometric margin conditions which are practically common.

**Method:** Establishes upper and lower bounds for minimax learning rates over function classes with bounded Kolmogorov entropy.

**Result:** Optimal rates identified for specific function classes and nearly optimal rates for others under different conditions.

**Conclusion:** Identifies optimal rates close to fast learning rates for certain function classes and provides lower bounds under geometric margin conditions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimax+learning+rates+for+estimating+binary+classifiers+under+margin+conditions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10628&send_immediately=true&force_search=false)

**Abstract:** We study classification problems using binary estimators where the decision
boundary is described by horizon functions and where the data distribution
satisfies a geometric margin condition. We establish upper and lower bounds for
the minimax learning rate over broad function classes with bounded Kolmogorov
entropy in Lebesgue norms. A key novelty of our work is the derivation of lower
bounds on the worst-case learning rates under a geometric margin condition -- a
setting that is almost universally satisfied in practice but remains
theoretically challenging. Moreover, our results deal with the noiseless
setting, where lower bounds are particularly hard to establish. We apply our
general results to classification problems with decision boundaries belonging
to several function classes: for Barron-regular functions, and for
H\"older-continuous functions with strong margins, we identify optimal rates
close to the fast learning rates of $\mathcal{O}(n^{-1})$ for $n \in
\mathbb{N}$ samples. Also for merely convex decision boundaries, in a strong
margin case optimal rates near $\mathcal{O}(n^{-1/2})$ can be achieved.

</details>


### [168] [Supervised Models Can Generalize Also When Trained on Random Label](https://arxiv.org/abs/2505.11006)
*Oskar Allerbo, Thomas B. Schön*

**Main category:** stat.ML

**TL;DR:** This paper shows that supervised models can be trained without using the output y.


<details>
  <summary>Details</summary>
**Motivation:** The success of unsupervised learning raises the question of whether supervised models can also be trained without using the information in the output y.

**Method:** Formulate the model as a smoother and construct the smoother matrix S independently of y, for example, by training on random labels.

**Result:** y-free trained versions of linear and kernel ridge regression, smoothing splines, and neural networks perform similarly to their standard versions and significantly better than random guessing.

**Conclusion:** Demonstrates that supervised models can be trained without using the output y.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Models+Can+Generalize+Also+When+Trained+on+Random+Label，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11006&send_immediately=true&force_search=false)

**Abstract:** The success of unsupervised learning raises the question of whether also
supervised models can be trained without using the information in the output
$y$. In this paper, we demonstrate that this is indeed possible. The key step
is to formulate the model as a smoother, i.e. on the form $\hat{f}=Sy$, and to
construct the smoother matrix $S$ independently of $y$, e.g. by training on
random labels. We present a simple model selection criterion based on the
distribution of the out-of-sample predictions and show that, in contrast to
cross-validation, this criterion can be used also without access to $y$. We
demonstrate on real and synthetic data that $y$-free trained versions of linear
and kernel ridge regression, smoothing splines, and neural networks perform
similarly to their standard, $y$-based, versions and, most importantly,
significantly better than random guessing.

</details>


### [169] [Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization](https://arxiv.org/abs/2505.11089)
*Yiran Yang, Rui Chen*

**Main category:** stat.ML

**TL;DR:** We propose a row and column generation approach using DCA to solve the Bayesian Network Structure Learning problem more effectively.


<details>
  <summary>Details</summary>
**Motivation:** To address the exponentially large number of variables and constraints in state-of-the-art BNSL IP formulations.

**Method:** We reformulate the pricing problem as a difference of submodular optimization problem and apply the Difference of Convex Algorithm (DCA) to efficiently solve the pricing problems.

**Result:** For continuous Gaussian data, our approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches.

**Conclusion:** Our row and column generation approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inexact+Column+Generation+for+Bayesian+Network+Structure+Learning+via+Difference-of-Submodular+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11089&send_immediately=true&force_search=false)

**Abstract:** In this paper, we consider a score-based Integer Programming (IP) approach
for solving the Bayesian Network Structure Learning (BNSL) problem.
State-of-the-art BNSL IP formulations suffer from the exponentially large
number of variables and constraints. A standard approach in IP to address such
challenges is to employ row and column generation techniques, which dynamically
generate rows and columns, while the complex pricing problem remains a
computational bottleneck for BNSL. For the general class of $\ell_0$-penalized
likelihood scores, we show how the pricing problem can be reformulated as a
difference of submodular optimization problem, and how the Difference of Convex
Algorithm (DCA) can be applied as an inexact method to efficiently solve the
pricing problems. Empirically, we show that, for continuous Gaussian data, our
row and column generation approach yields solutions with higher quality than
state-of-the-art score-based approaches, especially when the graph density
increases, and achieves comparable performance against benchmark
constraint-based and hybrid approaches, even when the graph size increases.

</details>


### [170] [Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression](https://arxiv.org/abs/2505.11143)
*William R. P. Denault*

**Main category:** stat.ML

**TL;DR:** Introduces Nash, a new method for sparse linear regression that uses neural networks to adaptively regularize covariates based on side information.


<details>
  <summary>Details</summary>
**Motivation:** Traditional regression methods perform poorly with structured or heterogeneous covariates, especially in biomedical applications.

**Method:** Neural Adaptive Shrinkage (Nash) framework that integrates side information into sparse regression through neural networks.

**Result:** Experiments show Nash improves accuracy and adaptability compared to existing methods.

**Conclusion:** Nash provides a flexible and effective approach to sparse regression by leveraging covariate-specific side information.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nash%3A+Neural+Adaptive+Shrinkage+for+Structured+High-Dimensional+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11143&send_immediately=true&force_search=false)

**Abstract:** Sparse linear regression is a fundamental tool in data analysis. However,
traditional approaches often fall short when covariates exhibit structure or
arise from heterogeneous sources. In biomedical applications, covariates may
stem from distinct modalities or be structured according to an underlying
graph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that
integrates covariate-specific side information into sparse regression via
neural networks. Nash adaptively modulates penalties on a per-covariate basis,
learning to tailor regularization without cross-validation. We develop a
variational inference algorithm for efficient training and establish
connections to empirical Bayes regression. Experiments on real data demonstrate
that Nash can improve accuracy and adaptability over existing methods.

</details>


### [171] [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
*Jacob Trauger, Ambuj Tewari*

**Main category:** stat.ML

**TL;DR:** This paper examines different algorithms (greedy, lookahead, random sampling, and temperature-scaled random sampling) used in large language models for next-token prediction. It studies their consistency with respect to various goals encoded as loss functions.


<details>
  <summary>Details</summary>
**Motivation:** To study the consistency of surrogate losses with respect to a target loss function in the context of LLMs.

**Method:** Examining a few algorithms used in next-token prediction and studying their consistency with respect to various goals encoded as loss functions.

**Result:** Random sampling is consistent with outputting sequences that mimic sampling from the true probability distribution if next-token prediction converges to its true probability distribution. For other goals, no polynomial-time algorithm is optimal for all probability distributions, and decoding algorithms are only optimal for a subset of probability distributions.

**Conclusion:** Choosing the correct decoding algorithm based on the desired goal is extremely important, and many of the ones used lack theoretical grounding in numerous scenarios.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Next-Token+Prediction+in+LLMs%3A+How+End+Goals+Determine+the+Consistency+of+Decoding+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11183&send_immediately=true&force_search=false)

**Abstract:** Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.

</details>


### [172] [A Fourier Space Perspective on Diffusion Models](https://arxiv.org/abs/2505.11278)
*Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, Sushrut Karmalkar*

**Main category:** stat.ML

**TL;DR:** This work studies the inductive bias of the forward process of diffusion models in Fourier space. It finds that the faster noising of high-frequency components in DDPM leads to degraded generation quality of high-frequency components. An alternate forward process is proposed to remove the typical frequency hierarchy during generation, improving performance on datasets with important high frequencies.


<details>
  <summary>Details</summary>
**Motivation:** To improve the generation quality of high-frequency components in diffusion models by studying the inductive bias of the forward process in Fourier space.

**Method:** Theoretical analysis and empirical demonstration of the inductive bias of the forward process in Fourier space, and proposing an alternate forward process that corrupts all frequencies at the same rate.

**Result:** The alternate forward process removes the typical frequency hierarchy during generation, leading to marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks.

**Conclusion:** The study reveals that the faster noising of high-frequency components in DDPM violates the normality assumption in the reverse process, resulting in degraded generation quality of high-frequency components. The proposed alternate forward process improves generation quality for high-frequency components.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fourier+Space+Perspective+on+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11278&send_immediately=true&force_search=false)

**Abstract:** Diffusion models are state-of-the-art generative models on data modalities
such as images, audio, proteins and materials. These modalities share the
property of exponentially decaying variance and magnitude in the Fourier
domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)
forward process of additive white noise, this property results in
high-frequency components being corrupted faster and earlier in terms of their
Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then
generates low-frequency information before high-frequency details. In this
work, we study the inductive bias of the forward process of diffusion models in
Fourier space. We theoretically analyse and empirically demonstrate that the
faster noising of high-frequency components in DDPM results in violations of
the normality assumption in the reverse process. Our experiments show that this
leads to degraded generation quality of high-frequency components. We then
study an alternate forward process in Fourier space which corrupts all
frequencies at the same rate, removing the typical frequency hierarchy during
generation, and demonstrate marked performance improvements on datasets where
high frequencies are primary, while performing on par with DDPM on standard
imaging benchmarks.

</details>


### [173] [Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization](https://arxiv.org/abs/2505.11281)
*Yuejiang Wen, Paul D. Franzon*

**Main category:** stat.ML

**TL;DR:** Introduce SA-REMBO, a new framework that extends REMBO by supporting multiple random Gaussian embeddings to capture varying properties in high-dimensional objectives.


<details>
  <summary>Details</summary>
**Motivation:** Address limitations of REMBO in handling high-dimensional spaces with varying characteristics like nonstationarity and heteroscedasticity.

**Method:** Propose a self-adaptive embedding approach using multiple embeddings and an index variable to select embeddings conditionally.

**Result:** Demonstrate better performance than REMBO and other low-rank BO methods on synthetic and real-world benchmarks.

**Conclusion:** Establish SA-REMBO as a flexible and powerful tool for Bayesian Optimization in complex, high-dimensional design spaces.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Linear+Embedding+for+Nonstationary+High-Dimensional+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11281&send_immediately=true&force_search=false)

**Abstract:** Bayesian Optimization (BO) in high-dimensional spaces remains fundamentally
limited by the curse of dimensionality and the rigidity of global
low-dimensional assumptions. While Random EMbedding Bayesian Optimization
(REMBO) mitigates this via linear projections into low-dimensional subspaces,
it typically assumes a single global embedding and a stationary objective. In
this work, we introduce Self-Adaptive embedding REMBO (SA-REMBO), a novel
framework that generalizes REMBO to support multiple random Gaussian
embeddings, each capturing a different local subspace structure of the
high-dimensional objective. An index variable governs the embedding choice and
is jointly modeled with the latent optimization variable via a product kernel
in a Gaussian Process surrogate. This enables the optimizer to adaptively
select embeddings conditioned on location, effectively capturing locally
varying effective dimensionality, nonstationarity, and heteroscedasticity in
the objective landscape. We theoretically analyze the expressiveness and
stability of the index-conditioned product kernel and empirically demonstrate
the advantage of our method across synthetic and real-world high-dimensional
benchmarks, where traditional REMBO and other low-rank BO methods fail. Our
results establish SA-REMBO as a powerful and flexible extension for scalable BO
in complex, structured design spaces.

</details>


### [174] [Convergence Rates of Constrained Expected Improvement](https://arxiv.org/abs/2505.11323)
*Haowei Wang, Jingyi Wang, Zhongxiang Dai, Nai-Yuan Chiang, Szu Hui Ng, Cosmin G. Petra*

**Main category:** stat.ML

**TL;DR:** This paper establishes the theoretical convergence rates of the constrained expected improvement algorithm in constrained Bayesian optimization.


<details>
  <summary>Details</summary>
**Motivation:** To establish the theoretical convergence rate of the constrained expected improvement (CEI) algorithm which is a commonly used CBO method.

**Method:** Analyze the simple regret upper bound of CEI.

**Result:** CEI achieves convergence rates of O(t^(-1/2) log^((d+1)/2)(t)) and O(t^((-ν)/(2ν+d)) log^(ν/(2ν+d))(t)) under different kernel assumptions.

**Conclusion:** CEI achieves specific convergence rates for both RKHS and GP assumptions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Convergence+Rates+of+Constrained+Expected+Improvement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11323&send_immediately=true&force_search=false)

**Abstract:** Constrained Bayesian optimization (CBO) methods have seen significant success
in black-box optimization with constraints, and one of the most commonly used
CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a
natural extension of the expected improvement (EI) when constraints are
incorporated. However, the theoretical convergence rate of CEI has not been
established. In this work, we study the convergence rate of CEI by analyzing
its simple regret upper bound. First, we show that when the objective function
$f$ and constraint function $c$ are assumed to each lie in a reproducing kernel
Hilbert space (RKHS), CEI achieves the convergence rates of $\mathcal{O}
\left(t^{-\frac{1}{2}}\log^{\frac{d+1}{2}}(t) \right) \ \text{and }\
\mathcal{O}\left(t^{\frac{-\nu}{2\nu+d}} \log^{\frac{\nu}{2\nu+d}}(t)\right)$
for the commonly used squared exponential and Mat\'{e}rn kernels, respectively.
Second, we show that when $f$ and $c$ are assumed to be sampled from Gaussian
processes (GPs), CEI achieves the same convergence rates with a high
probability. Numerical experiments are performed to validate the theoretical
analysis.

</details>


### [175] [STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes](https://arxiv.org/abs/2505.11355)
*Simon Urbainczyk, Aretha L. Teckentrup, Jonas Latz*

**Main category:** stat.ML

**TL;DR:** 提出了一种新的粒子期望最大化方法，结合变分学习和MCMC，用于高斯过程在大规模数据上的高效准确训练。


<details>
  <summary>Details</summary>
**Motivation:** 解决高斯过程在处理大量训练数据时的问题，以及当底层函数包含多尺度特征难以通过静态核表示时的问题。

**Method:** 粒子期望最大化方法，结合变分学习和MCMC，用于同时确定诱导点和训练GPs。

**Result:** 提出的方法在标准基准问题上进行了测试，显示出高效性和准确性。

**Conclusion:** 提出了一种结合变分学习和MCMC的方法来同时确定大规模数据中的诱导点并准确训练高斯过程（GPs），该方法在标准基准问题上进行了测试。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STRIDE%3A+Sparse+Techniques+for+Regression+in+Deep+Gaussian+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11355&send_immediately=true&force_search=false)

**Abstract:** Gaussian processes (GPs) have gained popularity as flexible machine learning
models for regression and function approximation with an in-built method for
uncertainty quantification. However, GPs suffer when the amount of training
data is large or when the underlying function contains multi-scale features
that are difficult to represent by a stationary kernel. To address the former,
training of GPs with large-scale data is often performed through inducing point
approximations (also known as sparse GP regression (GPR)), where the size of
the covariance matrices in GPR is reduced considerably through a greedy search
on the data set. To aid the latter, deep GPs have gained traction as
hierarchical models that resolve multi-scale features by combining multiple
GPs. Posterior inference in deep GPs requires a sampling or, more usual, a
variational approximation. Variational approximations lead to large-scale
stochastic, non-convex optimisation problems and the resulting approximation
tends to represent uncertainty incorrectly. In this work, we combine
variational learning with MCMC to develop a particle-based
expectation-maximisation method to simultaneously find inducing points within
the large-scale data (variationally) and accurately train the GPs
(sampling-based). The result is a highly efficient and accurate methodology for
deep GP training on large-scale data. We test our method on standard benchmark
problems.

</details>
