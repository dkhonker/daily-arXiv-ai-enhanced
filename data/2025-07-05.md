<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 64]
- [cs.AI](#cs.AI) [总数: 31]
- [stat.ML](#stat.ML) [总数: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan, Qi Wang, Haining Wang, Ruizhi Chengze, Yi Zhang, Hongsheng Liu, Zidong Wang, Fan Yu, Qi Qi, Hao Sun*

**主要类别:** cs.LG

**AI概要:** 提出了一种可学习且可微分的有限体积求解器(LDSolver)，用于在时空粗网格上高效准确地模拟流体流动。LDSolver包含两个关键组件：（1）可微分有限体积求解器；（2）提供通量等效近似和时间误差校正的学习模块。实验表明，即使在有限训练数据下，LDSolver仍能加速模拟并保持高精度，优于基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 流体流动模拟对于气象、空气动力学和生物医学等物理现象建模至关重要。经典数值求解器通常需要精细的时空网格以满足稳定性、一致性和收敛性条件，导致计算成本显著增加。尽管机器学习方法表现出更高的效率，但往往面临可解释性、泛化能力和数据依赖性等问题。

**方法:** 提出了一种可学习且可微分的有限体积求解器（LDSolver），该求解器包含两个关键组件：（1）可微分有限体积求解器；（2）一个学习模块，提供通量（导数和插值）的等效近似以及粗网格上的时间误差校正。通过这种方式，即使在有限的训练数据下，也能实现高效和准确的模拟。

**结果:** 在不同的流系统（如Burgers、衰减、强迫和剪切流）上的实验表明，LDSolver取得了最先进的性能，明显超越了基线模型。

**结论:** LDSolver能够在有限训练数据的情况下，加速流体流动模拟并保持高精度，同时具有优越的泛化能力，为流体模拟提供了新的高效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learnable-Differentiable+Finite+Volume+Solver+for+Accelerated+Simulation+of+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01975&send_immediately=true&force_search=false)

**原文摘要:** Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [2] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long, Xiangzhi Huang, Jiemin Xie, Ming Cai*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的图卷积网络结构DKGCM，用于提高时空交通需求预测的准确性。通过DK-GCN方法捕捉空间依赖性，结合FFT和双向Mamba框架捕捉时间依赖性，并采用GRPO强化学习策略优化模型训练。实验表明该模型在三个公开数据集上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 准确的交通需求预测能够帮助交通管理部门更有效地分配资源，提高利用效率。然而，交通系统中复杂的时空关系限制了需求预测模型的性能。

**方法:** 1. 提出了基于时空相似性的聚类图卷积方法（DK-GCN），使用动态时间规整（DTW）和K-means聚类对交通节点进行分组，以更有效地捕捉空间依赖性。
2. 在时间尺度上，将快速傅里叶变换（FFT）集成到双向Mamba深度学习框架中，捕捉交通需求的时间依赖性。
3. 采用GRPO强化学习策略优化模型训练，增强损失函数反馈机制。

**结果:** 大量实验表明，所提出的模型优于几种先进的方法，并在三个公开数据集上取得了强大的结果。

**结论:** 所提出的DKGCM模型能够有效提升时空交通需求预测的准确性，为交通管理提供了更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DKGCM%3A+A+Spatio-Temporal+Prediction+Model+for+Traffic+Flow+by+Fusing+Spatial+Node+Clustering+Method+and+Fourier+Bidirectional+Mamba+Mechanism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01982&send_immediately=true&force_search=false)

**原文摘要:** Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [3] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu, Lin F. Yang, Sharan Vaswani*

**主要类别:** cs.LG

**AI概要:** 本文研究了无限视界γ折扣线性约束马尔可夫决策过程(CMDPs)，提出了一种基于原始对偶框架的算法，利用生成模型和镜像下降值迭代(MDVI)等黑盒解算器解决CMDPs问题。对于两种可行性情况(放松可行性和严格可行性)，分别给出了样本复杂度界限。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法可能无法有效解决带约束的马尔可夫决策过程问题，尤其是在需要满足严格约束的情况下。因此，研究者希望开发一种通用框架，可以结合任意无约束MDP求解器来解决CMDPs问题，同时分析其样本复杂度。

**方法:** 作者提出了一种基于原始对偶框架的方法，能够利用任何黑盒无约束MDP求解器。具体而言，使用了镜像下降值迭代(MDVI)作为示例解算器，并针对两种可行性情况（放松可行性和严格可行性）进行了分析。在严格可行性情况下，还引入了Slater常数ζ来描述可行域的大小。

**结果:** 对于放松可行性情况，算法能够在高概率下返回一个ε-最优策略，所需样本数量为Õ(d² / (1-γ)⁴ε²)。对于严格可行性情况，样本复杂度为Õ(d² / (1-γ)⁶ε²ζ²)。此外，该框架还可以应用于表格型CMDPs，并恢复接近最优的样本复杂度。

**结论:** 本文提出了一种适用于CMDPs的通用框架，能够结合任意无约束MDP求解器解决问题。该方法在两种可行性条件下均具有接近最优的样本复杂度，为CMDPs问题提供了一个有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Complexity+Bounds+for+Linear+Constrained+MDPs+with+a+Generative+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02089&send_immediately=true&force_search=false)

**原文摘要:** We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [4] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

**主要类别:** cs.LG

**AI概要:** 研究在社交媒体选举和危机期间的虚假信息检测，主要探索多模态特征组合（文本、图像和社会特征）的早期融合方法，并通过数据丰富过程提取额外特征。结果表明结合无监督和监督机器学习模型比单模态模型提高15%性能，比双模态模型提高5%。同时分析了虚假信息传播模式。


<details>
  <summary>更多</summary>
  
**动机:** 目前的研究大多集中在基于文本或基于图像的虚假信息检测方法上，而对多模态特征组合的研究较少。为了更有效地检测虚假信息，本研究探讨了结合文本、图像和社会特征的分类模型的有效性。

**方法:** 使用早期融合方法将文本、图像和社会特征结合起来，构建虚假信息检测的分类模型。研究分析了1,529条推文（包含文本和图像），通过对象检测和光学字符识别（OCR）等技术进行数据丰富以提取视觉和社会特征。

**结果:** 结合无监督和监督机器学习模型的分类性能相比单模态模型提高了15%，相比双模态模型提高了5%。此外，研究还分析了虚假信息的传播模式及其相关用户特征。

**结论:** 多模态特征组合可以显著提高虚假信息检测的性能，尤其是在早期融合策略下。未来研究可进一步优化模型并深入分析虚假信息的传播机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Misinformation+Detection+Using+Early+Fusion+of+Linguistic%2C+Visual%2C+and+Social+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01984&send_immediately=true&force_search=false)

**原文摘要:** Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [5] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim, Giung Nam, Juho Lee*

**主要类别:** cs.LG

**AI概要:** 通过自蒸馏迭代改进基础模型，可提升受约束文本生成的质量。


<details>
  <summary>更多</summary>
  
**动机:** 在受限文本生成中，当目标分布集中在基础模型下不太可能的输出上时，由于奖励信号稀疏且无信息，学习变得具有挑战性。

**方法:** 研究提出通过自蒸馏迭代改进基础模型的方法。

**结果:** 该方法使模型逐渐更符合目标，从而显著提高了生成质量。

**结论:** 自蒸馏是一种有效的手段，可以缓解由于稀疏奖励信号导致的学习困难问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Constrained+Generation+in+Language+Models+via+Self-Distilled+Twisted+Sequential+Monte+Carlo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02315&send_immediately=true&force_search=false)

**原文摘要:** Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [6] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai, Deyu Li, Jiye Liang, Yanhui Zhai*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new feature selection method based on sampling techniques and rough set theory for massive data. The method finds a feature subset with high discriminatory ability by constructing positive region preserved samples. Experiments show that it can efficiently find approximate reducts with high discriminatory ability.


<details>
  <summary>更多</summary>
  
**动机:** Intelligent machines need to maximize their chances of success by selecting relevant features, but they generally have no enough computing resources when faced with huge volume of data.

**方法:** This paper develops a new method based on sampling techniques and rough set theory to address the challenge of feature selection for massive data. This method constructs positive region preserved samples from massive data to find a feature subset with high discriminatory ability.

**结果:** Experiments on 11 datasets of different sizes show that approximate reducts can be found in a very short period of time, and the discriminatory ability of the final reduct is larger than the estimated lower boundary. Experiments on four large-scale data sets also showed that an approximate reduct with high discriminatory ability can be obtained in reasonable time on a personal computer.

**结论:** The proposed method is able to select a feature subset that can preserve the discriminatory ability of all the features of the target massive data set within an acceptable time on a personal computer. It also allows for estimating the lower boundary of the probability of the object pairs that can be discerned using the feature subset selected in all object pairs that should be distinguished before finding reducts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Positive+region+preserved+random+sampling%3A+an+efficient+feature+selection+method+for+massive+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01998&send_immediately=true&force_search=false)

**原文摘要:** Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [7] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey, Daniel Sorensen, Minjin Hwang, Sandip Halder*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的基于机器学习的多变量时间序列（MTS）数据分析方法，用于半导体制造中的异常检测。该方法将MTS数据转换为图像表示，通过预训练的VGG-16网络进行分类，并利用Siamese网络比较信号嵌入以确定是否属于同一类别。此方法在实际FAB过程时间序列数据集上显示出高精度，并且可以在监督和半监督环境下灵活应用。


<details>
  <summary>更多</summary>
  
**动机:** 半导体制造过程复杂，具有大量相互依赖的参数，且存在高维数据、类别不平衡、噪声、缺失测量值以及非平稳行为等挑战。现有的异常预测方法难以有效应对这些挑战，因此需要一种新的通用方法来提高异常检测的效果。

**方法:** 该方法包含三个主要步骤：1) 使用连续小波变换（CWT）将多变量时间序列数据转换为图像表示；2) 通过微调预训练的VGG-16架构，在自定义的CWT图像数据集上开发一个多类图像分类器；3) 构建一个Siamese网络，其中两个相同的子网络均使用微调后的VGG-16作为骨干网络。网络接收一对CWT图像作为输入，其中一个作为参考或锚点（代表已知良好的信号），另一个作为查询（代表未知信号）。模型通过比较两者的嵌入来判断它们是否属于同一类别。

**结果:** 该方法在实际的FAB过程时间序列数据集上表现出高精度，能够有效识别异常。此外，该方法具有灵活性，适用于监督和半监督环境下的异常检测任务。

**结论:** 所提出的基于机器学习的方法为半导体制造中的离线异常检测提供了一个有前景的解决方案。它不仅在复杂环境中表现良好，还具备广泛的适用性，能够在不同设置下进行异常检测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Wavelet+Transform+and+Siamese+Network-Based+Anomaly+Detection+in+Multi-variate+Semiconductor+Process+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01999&send_immediately=true&force_search=false)

**原文摘要:** Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [8] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了一个新颖的在线框架下的保形预测问题，目标是在满足覆盖率的同时优化区间长度。对于可交换序列，提出了一个几乎最优的算法；而对于任意序列，证明了在平均长度上接近最优区间的算法必然会犯更多错误，并且设计了一个能恢复所有帕累托最优设置的匹配算法。此外，还展示了没有单一算法可以同时对两种序列都达到最优。


<details>
  <summary>更多</summary>
  
**动机:** 传统的保形预测方法通常关注覆盖率，但未充分考虑效率（如区间长度）。因此，作者提出一个新的在线框架，直接优化效率，同时保持覆盖率。

**方法:** 1. 定义问题：给定目标误覆盖率 $\alpha > 0$ 和时间范围 $T$，每天输出一个区间 $I_t$ 并观察点 $y_t$。
2. 对于可交换序列，构造区间以实现 $(1 - \alpha) - o(1)$ 的覆盖率，且长度不超过事后最佳固定区间的长度。
3. 对于任意序列，证明了任何算法若要实现与事后最佳固定区间相比的 $\mu$-近似平均长度，则必须犯比 $\alpha T$ 更多的错误。
4. 提出一个匹配算法，能够恢复所有帕累托最优的 $\mu$ 和错误数量设置。
5. 设计一个确定性算法，适用于对抗适应性对手。
6. 展示了无法存在一个同时对任意序列和可交换序列都最优的单一算法，并提出一种近似最优折中方案。

**结果:** - 对于可交换序列，成功构造了几乎最优的区间。
- 对于任意序列，证明了理论下界并设计了匹配算法。
- 显示了不同输入序列类型之间的性能差距。
- 提出了一个能在两种情况下达到近似最优折中的算法。

**结论:** 本研究为在线保形预测提供了一种新的框架，强调了效率的重要性。通过理论分析和算法设计，揭示了可交换序列和任意序列之间的根本差异，并提供了实用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Conformal+Prediction+with+Efficiency+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02496&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [9] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab, Ahmet Iscen, Mathilde Caron, Alireza Fathi, Cordelia Schmid*

**主要类别:** cs.LG

**AI概要:** 尽管最近视觉-语言模型(VLMs)有所进展，但长视频理解仍然是一个具有挑战性的问题。当前的VLMs虽然能够处理大约1000帧的输入，但在有效利用序列长度方面仍然存在困难，并且容易受到上下文窗口中无关干扰的影响。本文提出了时间链思考（Temporal Chain of Thought），这是一种用于视频问答的推理策略，可以精心策划模型的输入上下文。通过使用VLM本身迭代识别和提取视频中最相关的帧，从而提高准确率。这种方法在4个不同的视频问答数据集上达到了最先进的结果，并且在更长的视频上表现尤为突出。


<details>
  <summary>更多</summary>
  
**动机:** 现有的长上下文VLMs尽管能处理大量输入帧，但无法有效利用这些信息，并且容易受到上下文中无关干扰的影响，因此需要一种新的方法来改进长视频的理解能力。

**方法:** 提出了一种名为时间链思考（Temporal Chain of Thought）的推理策略，该策略使用VLM自身迭代地识别和提取视频中最相关的帧，从而构建模型的输入上下文。通过在推理时投入更多计算资源选择最相关上下文，提高了模型的准确率。

**结果:** 在4个不同的视频问答数据集上实现了最先进的结果，使用3种不同的VLM均显示出一致的改进。特别是在处理超过1小时的长视频时，在LVBench上使用32K上下文窗口的方法比使用700K标准推理上下文窗口的相同VLM高出2.8个百分点。

**结论:** 时间链思考策略显著提高了长视频问答任务的准确性，证明了通过增加推理时的计算资源选择相关上下文的重要性，并且在不同VLM和数据集上均表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Chain+of+Thought%3A+Long-Video+Understanding+by+Thinking+in+Frames，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02001&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [10] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia, Mahmoud El Daou, Henryk Gzyl*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的分类方法，通过在有界超立方体中搜索参数向量和最小化基于熵的函数，实现线性和非线性数据点分离。


<details>
  <summary>更多</summary>
  
**动机:** 传统的支持向量机和梯度下降等优化技术在处理复杂决策边界时可能不够鲁棒，因此需要一种新的分类方法。

**方法:** 通过在以原点为中心的有界N维超立方体中搜索参数向量，并最小化定义在未知变量空间上的基于熵的函数，同时扩展到多项式曲面以允许更复杂的决策边界分离数据点。

**结果:** 数值实验表明，该方法在处理包括线性和非线性可分在内的各种分类任务时具有高效性和多样性。

**结论:** 提出的方法为传统线性或二次优化技术提供了鲁棒的替代方案，适用于多种分类任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Classification+by+Separating+Hypersurfaces%3A+An+Entropic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02732&send_immediately=true&force_search=false)

**原文摘要:** We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [11] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody, Youpeng Zhao, Jun Wang*

**主要类别:** cs.LG

**AI概要:** AIRES 是一种新型的算法-系统协同设计方案，旨在加速 GCN 的外存 SpGEMM 计算。通过改进数据对齐和内存分配，并采用三级动态调度策略，AIRES 在实际图处理基准测试中显著优于现有方法，最多可降低 1.8 倍的延迟。


<details>
  <summary>更多</summary>
  
**动机:** 当前针对大规模图数据的外存 SpGEMM 方法存在高 I/O 延迟和 GPU 利用率不足的问题，尤其是在资源受限系统中。为了解决这些问题，需要优化稀疏格式的数据对齐和内存分配机制。

**方法:** AIRES 提出了从算法和系统两方面进行优化：
1. 算法层面：通过块级对齐和行块划分算法解决稀疏矩阵的数据对齐问题。
2. 系统层面：采用三阶段动态调度策略，结合 GPU 内存、GPU Direct Storage 和主机内存，减少 I/O 延迟并提高吞吐量。

**结果:** 实验结果表明，AIRES 显著优于现有方法，在实际图处理基准测试中最多可实现 1.8 倍的延迟降低。

**结论:** AIRES 成功解决了外存 SpGEMM 中的数据对齐和内存分配瓶颈，通过算法与系统的协同设计提高了计算效率和 GPU 利用率，为大规模 GCN 应用提供了更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AIRES%3A+Accelerating+Out-of-Core+GCNs+via+Algorithm-System+Co-Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02006&send_immediately=true&force_search=false)

**原文摘要:** Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [12] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang, Ruihao Zhu, Qiaomin Xie*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contextual+Online+Pricing+with+%28Biased%29+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02762&send_immediately=true&force_search=false)

**原文摘要:** We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [13] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon*

**主要类别:** cs.LG

**AI概要:** 提出SE(3)-等变适配器框架GeoAda，通过引入结构化适配器设计，实现可控生成任务的灵活和参数高效的微调。该方法在保持模型几何一致性的同时，避免过拟合和灾难性遗忘，并在多种几何控制类型和应用领域中展现出优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 几何扩散模型在分子动力学和结构生成方面取得了显著成功，但针对下游任务高效微调以适应不同几何控制的研究尚不充分。

**方法:** GeoAda采用SE(3)-等变适配器框架，包含耦合算子编码控制信号、可训练的预训练模型层副本处理信号以及解耦算子投影回原空间，最后通过等变零初始化卷积完成输出。仅微调轻量级适配器模块即可实现几何一致性和防止过拟合及灾难性遗忘。

**结果:** 理论上证明了适配器保持SE(3)-等变性，确保预训练扩散模型的几何归纳偏差在适应过程中得以保留。实证结果显示，GeoAda在多种几何控制类型和应用领域中达到最先进的微调性能，同时保留原始任务准确性。

**结论:** GeoAda提供了一种灵活且高效的微调方法，适用于多种几何控制任务，在避免过拟合和灾难性遗忘的同时，保持了几何扩散模型的优异性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GeoAda%3A+Efficiently+Finetune+Geometric+Diffusion+Models+with+Equivariant+Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02085&send_immediately=true&force_search=false)

**原文摘要:** Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [14] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg, Arunava Samajpati, Sivasankaran Chandrasekar, Varun Kacholia*

**主要类别:** cs.LG

**AI概要:** 研究比较了多个大型语言模型（LLMs）与专有招聘模型Match Score在候选人匹配任务中的表现，发现Match Score在准确性和公平性方面均优于通用LLMs。作者强调了领域特定建模和偏差审计的重要性，并指出精心设计的算法可以同时实现招聘的准确性和公平性结果。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLMs）在招聘中的应用日益广泛，其潜在的准确性和算法偏见问题引起了广泛关注。本研究旨在评估不同LLMs在候选人筛选中的表现，并探讨如何有效缓解偏见问题。

**方法:** 研究使用了一个包含约10,000个真实候选人-职位对的数据集，对比了来自OpenAI、Anthropic、Google、Meta和Deepseek等多个领先LLMs以及专有的Match Score模型。通过ROC AUC、Precision-Recall AUC和F1-score等指标评估模型预测准确性，同时利用截止分析影响比率衡量模型在性别、种族及交叉子群体间的公平性。

**结果:** Match Score在准确性（ROC AUC 0.85 vs 0.77）和公平性（最低种族影响比率为0.957 vs LLMs的0.809或更低，交叉子群体为0.906 vs 0.773）上显著优于通用LLMs。预训练偏差可能导致未经充分保护的LLMs在招聘场景中传播社会偏见，而定制监督模型能更有效地缓解这些偏见。

**结论:** 领域特定建模和偏差审计对于在高风险领域（如招聘）中部署AI至关重要。研究表明，准确性和公平性并非不可兼得，精心设计的算法可以在招聘过程中同时实现两者。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+the+Promise+and+Pitfalls+of+LLMs+in+Hiring+Decisions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02087&send_immediately=true&force_search=false)

**原文摘要:** The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [15] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的模型——基于能量的变压器（EBTs），它能够通过无监督学习进行系统2思维，从而提高模型在不同模态任务中的性能和泛化能力。实验表明，EBTs在训练和推理过程中都优于现有的Transformer++方法，并且在下游任务中表现出更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的推理计算技术（类似于人类的系统2思维）虽然流行但存在局限性：它们通常是特定模态、特定问题或需要额外的监督/训练。因此，研究者试图探索是否可以通过无监督学习来推广这些系统2思维方法，使模型学会仅从无监督学习中思考。

**方法:** 作者提出了基于能量的变压器（EBTs），这是一种新的基于能量的模型（EBMs）。EBTs通过对每个输入和候选预测对分配能量值，将预测问题重新定义为关于验证器的优化问题。通过梯度下降进行能量最小化直至收敛，实现预测。

**结果:** 1. 在离散（文本）和连续（视觉）模态上，EBTs相比主导的Transformer++方法，在训练时扩展速度更快，数据、批量大小、参数、FLOPs和深度方面的扩展率高出35%。
2. 推理时，EBTs在语言任务上通过系统2思维比Transformer++提高了29%的性能。
3. EBTs在图像去噪任务上优于扩散变压器，同时使用的前向传递更少。
4. 在大多数下游任务中，即使预训练性能相同或较差，EBTs也取得了比现有模型更好的结果。

**结论:** EBTs是一种有希望的新范式，可以同时扩展模型的学习和思考能力，展现了在不同模态任务中的优越性能和更好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Based+Transformers+are+Scalable+Learners+and+Thinkers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02092&send_immediately=true&force_search=false)

**原文摘要:** Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [16] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla, Luca A. Lanzendörfer, Longxiang Jiao, Roger Wattenhofer*

**主要类别:** cs.LG

**AI概要:** PANAMA 是一种用于训练端到端参数化吉他放大器模型的主动学习框架，采用类似 WaveNet 的架构。该方法通过梯度优化算法确定最佳采样点，在有限样本条件下生成虚拟放大器。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少生成吉他放大器模型所需的样本量，同时保持高质量的模型性能。

**方法:** 使用 PANAMA 主动学习框架和基于梯度的优化算法来确定最佳采样点，以最小化数据点（即放大器旋钮设置）的需求。

**结果:** 在有限样本条件下，成功创建了虚拟吉他放大器模型，证明了方法的有效性。

**结论:** PANAMA 框架结合主动学习策略可以有效减少样本需求，为生成高质量吉他放大器模型提供了一种新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parametric+Neural+Amp+Modeling+with+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02109&send_immediately=true&force_search=false)

**原文摘要:** We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [17] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu, Lechao Xiao, Andrew Gordon Wilson, Jeffrey Pennington, Atish Agarwala*

**主要类别:** cs.LG

**AI概要:** 论文研究了当模型大小和训练时间一起增长时，神经网络训练动态的缩放极限。通过展示不同大小模型的损失曲线在归一化后可以合并到单一通用曲线上，并提出了一种现象称为“超级坍缩”。超级坍缩提供了一个精确且实用的良好缩放指标，并通过分析SGD噪声动力学简单模型解释了这些现象。


<details>
  <summary>更多</summary>
  
**动机:** 了解神经网络训练动态的缩放极限，以及模型大小和训练时间共同增长时的行为。

**方法:** 观察不同大小模型的损失曲线在归一化后的行为，使用学习率衰减来实现超级坍缩，并分析简单的SGD噪声动力学模型以预测损失曲线并解释超级坍缩的起源。

**结果:** 发现不同大小模型的损失曲线可以合并到一个通用曲线上，超级坍缩现象被观察到，并且它是一个良好缩放的精确指标。

**结论:** 神经网络训练动态在计算最优训练模型中表现出显著的普遍性，超级坍缩现象可以通过SGD噪声动力学模型来解释。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Collapse+Reveals+Universal+Dynamics+in+Compute-Optimally+Trained+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02119&send_immediately=true&force_search=false)

**原文摘要:** What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [18] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan, Isaac Jacobson, Zheng Zhao, Tung-Chieh Chen, Guanglei Zhou, Chen-Chia Chang, Vineet Rashingkar, Yiran Chen*

**主要类别:** cs.LG

**AI概要:** 本论文提出CROP框架，利用大型语言模型（LLM）技术优化VLSI设计流程参数调整，通过将RTL代码转换为向量表示、基于嵌入的检索系统和增强的参数搜索系统，实现比现有方法更少迭代次数的高质量结果（QoR），并减少了9.9%的功耗。


<details>
  <summary>更多</summary>
  
**动机:** 现代VLSI设计中，EDA工具的复杂算法和庞大的参数空间使得芯片设计优化面临巨大挑战。尽管手动参数选择仍是工业实践，但其工作量过大且受限于专家经验，因此需要一种自动化的解决方案来优化VLSI设计流程。

**方法:** CROP框架包含三个主要部分：(1) 可扩展的方法将RTL源代码转化为密集向量表示；(2) 基于嵌入的检索系统，用于匹配具有语义相似电路的设计；(3) 结合检索增强生成（RAG）技术的LLM指导参数搜索系统，利用类似设计的先验知识约束搜索过程。

**结果:** 实验结果表明，CROP在工业设计上能以较少的迭代次数达到更高的质量结果（QoR），并实现了9.9%的功耗降低。

**结论:** CROP是首个使用大型语言模型技术进行自动VLSI设计流程调优的框架，能够显著提高设计质量和效率，并减少功耗，为未来VLSI设计优化提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CROP%3A+Circuit+Retrieval+and+Optimization+with+Parameter+Guidance+using+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02128&send_immediately=true&force_search=false)

**原文摘要:** Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [19] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li, Liangji Zhu, Anand Rangarajan, Sanjay Ranka*

**主要类别:** cs.LG

**AI概要:** 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，通过生成插值实现时空重建，显著减少存储成本并提高压缩比。


<details>
  <summary>更多</summary>
  
**动机:** 生成模型在条件设置下表现出色，可视为一种数据压缩形式，但其可控性和重建精度有限，限制了实际应用。

**方法:** 将变分自编码器与条件扩散模型结合，仅压缩少量关键帧到潜在空间，并用它们作为条件输入重建剩余帧，无需为每帧存储潜在表示。

**结果:** 实验结果表明，该方法比基于规则的压缩器（如SZ3）高出10倍的压缩比，且在相同重建误差下比领先的基于学习的方法性能高出63%。

**结论:** 此方法实现了准确的时空重建，同时大幅降低存储成本，提高了数据压缩的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Latent+Diffusion+for+Efficient+Spatiotemporal+Data+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02129&send_immediately=true&force_search=false)

**原文摘要:** Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [20] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang, Jian Kang, Yujun Yan, Adithya Kulkarni, Dawei Zhou*

**主要类别:** cs.LG

**AI概要:** 提出NCPNET，一种用于时序图的新型端到端一致性预测框架。通过基于扩散的非一致性评分和效率感知优化算法，解决了现有方法在动态图中因时间依赖性导致的覆盖率违规问题。实验表明，该方法显著提高了预测效率并确保了覆盖率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图神经网络一致性预测方法主要关注静态图，忽视了真实世界中图的动态演化特性，这违反了一致性预测的基本可交换性假设，限制了其应用范围。

**方法:** 引入NCPNET框架，扩展一致性预测到动态图场景。提出基于扩散的非一致性评分，捕捉拓扑和时间不确定性；开发效率感知优化算法，提高计算效率并减少覆盖率违规。

**结果:** 在多个真实世界时序图数据集（如WIKI、REDDIT等）上进行广泛实验，结果表明NCPNET能确保时序图中的覆盖率，并将预测集大小最多减少31%，显著优于现有方法。

**结论:** NCPNET为时序图提供了一种可靠的一致性预测方法，有效解决时间依赖性带来的挑战，显著提高了预测效率和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-exchangeable+Conformal+Prediction+for+Temporal+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02151，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02151&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [21] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon, Meredith Stewart, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种正式的验证程序，用于评估机器学习模型预测对特征干预的响应性，并通过实际应用展示了其在提高安全性方面的作用。


<details>
  <summary>更多</summary>
  
**动机:** 许多机器学习的安全问题源于模型在预测时未考虑个体可能改变输入的情况，尤其是在借贷、招聘和内容审核等场景中。

**方法:** 作者引入了一个正式的验证程序，将预测响应性视为一种敏感性分析，通过指定干预和下游影响分布的约束来控制变化。该方法只需通过黑箱访问即可估计任何模型和数据集的预测响应性，并支持如证伪和失败概率估计等任务。算法通过生成可达点的均匀样本构建这些估计。

**结果:** 该方法能够在实际应用中（如再犯预测、器官移植优先级和内容审核）促进安全性。

**结论:** 本文提出的验证程序为评估预测响应性提供了一种有效方法，有助于解决机器学习模型在实际应用中的安全问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistical+Inference+for+Responsiveness+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02169&send_immediately=true&force_search=false)

**原文摘要:** Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [22] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae, Hyeon Jeon, Jinwook Seo*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的工作流，通过基于度量经验相关性（而非仅设计特性）对度量进行聚类，从而减少降维投影评估中的偏差。该方法提高了降维评估的稳定性，有助于缓解评估偏差。


<details>
  <summary>更多</summary>
  
**动机:** 当前降维投影评估中存在一个问题：如果选择了高度相关的度量（测量相似结构特征），则可能导致偏向某些强调这些特征的降维技术，从而使评估结果产生偏差。

**方法:** 1. 计算度量相似性：使用成对相关性计算度量之间的相似性。
2. 聚类度量：将度量聚类以最小化重叠。
3. 选择代表度量：从每个聚类中选择一个代表性度量。

**结果:** 定量实验表明，该方法提高了降维评估的稳定性，显示出其在缓解评估偏差方面的有效性。

**结论:** 所提出的工作流通过基于经验相关性对度量进行聚类，有效减少了降维投影评估中的偏差，提升了评估的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Metric+Design+%21%3D+Metric+Behavior%3A+Improving+Metric+Selection+for+the+Unbiased+Evaluation+of+Dimensionality+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02225&send_immediately=true&force_search=false)

**原文摘要:** Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [23] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang, Paris Perdikaris*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为PhysicsCorrect的无训练校正框架，通过预计算雅可比矩阵及其伪逆来减少预测误差，显著提高了神经网络求解偏微分方程的长期预测精度和物理一致性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管神经网络在求解偏微分方程（PDEs）方面提供了显著的计算加速，但在长时间预测中存在误差累积的问题，导致与物理有效解完全偏离。

**方法:** PhysicsCorrect框架将校正公式化为基于PDE残差的线性化逆问题，并采用高效的缓存策略，在离线预热阶段预先计算雅可比矩阵及其伪逆，从而减少了两个数量级的计算开销。

**结果:** 在三个代表性PDE系统（Navier-Stokes流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程）上，PhysicsCorrect将预测误差减少了高达100倍，同时增加了不到5%的推理时间。

**结论:** PhysicsCorrect框架能够与多种架构无缝集成，包括傅里叶神经算子、UNets和视觉Transformer，有效地将不稳定的神经代理转化为可靠的模拟工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhysicsCorrect%3A+A+Training-Free+Approach+for+Stable+Neural+PDE+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02227&send_immediately=true&force_search=false)

**原文摘要:** Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [24] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda, Shashidhar Reddy Javaji, Zining Zhu*

**主要类别:** cs.LG

**AI概要:** VERBA是一种利用大型语言模型生成模型差异描述的方法，能够有效比较性能相似但行为不同的模型，并显著提升比较的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 在当前机器学习领域中，面对众多性能相似但行为不同的模型，用户需要一种高效的方式来进行细粒度的两两比较，以辅助模型选择。然而，手动进行两两比较和准备文档的工作量极大，因此需要自动化工具来解决这一问题。

**方法:** VERBA方法通过利用大型语言模型（LLM）从两个模型中采样，生成模型差异的描述性语言。此外，还建立了一种协议来评估这些描述信息的丰富程度，并构建了一个包含多种常用机器学习模型的基准套件。

**结果:** 对于决策树模型对，尽管性能差异仅为5%，但行为差异达到20-25%，VERBA能以高达80%的整体准确性描述其差异。当加入模型的结构信息后，描述准确性进一步提高到90%。

**结论:** VERBA为改进机器学习模型的事后透明性和可比性开辟了新的研究方向，展示了其在自动化模型比较中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VERBA%3A+Verbalizing+Model+Differences+Using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02241，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02241&send_immediately=true&force_search=false)

**原文摘要:** In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [25] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi, Xiaopeng Ke, Xinye Xiong, Kexin Meng, Chang Men, Zhengdan Zhu*

**主要类别:** cs.LG

**AI概要:** 在网约车聚合平台中，低价服务提供者更可能被乘客选择。因此，制定有效的优惠券策略以增加订单量至关重要。本文提出了FCA-RL框架，结合快速竞争适应（FCA）和强化拉格朗日调整（RLA），并引入RideGym模拟环境进行策略评估。实验表明，该方法优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究中关于动态适应市场波动并优化预算约束下的订单获取的优惠券策略的研究较少，而这对网约车服务提供者的长期生存至关重要。

**方法:** 提出FCA-RL框架，包含快速竞争适应（FCA）和强化拉格朗日调整（RLA）两种技术，以及首个专为网约车聚合平台设计的模拟环境RideGym。

**结果:** 实验结果表明，所提出的方法在不同市场条件下始终优于基线方法，展示了其在补贴优化方面的有效性。

**结论:** FCA-RL框架能够有效应对竞争对手的价格调整，并在预算约束下优化优惠券决策，为网约车服务提供者提供了实用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Order+Acquisition+Under+Competitive+Pressure%3A+A+Rapidly+Adaptive+Reinforcement+Learning+Approach+for+Ride-Hailing+Subsidy+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02244&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [26] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang, Xiaolu Zhou, Bosong Ding, Miao Xin*

**主要类别:** cs.LG

**AI概要:** 设计有效的奖励函数是强化学习（RL）的基石，但传统方法存在效率低下和不一致的问题。本文提出了一种新的框架URDP，结合大型语言模型（LLMs）和不确定性感知贝叶斯优化（UABO），通过量化奖励函数的不确定性、发现新奖励组件以及优化超参数配置，提升奖励函数质量和设计效率。实验结果表明，URDP在35个任务中生成了更高质量的奖励函数，并显著提高了自动化奖励设计的效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前利用大型语言模型（LLMs）自动设计奖励函数的方法在数值优化方面表现不佳，导致奖励质量不满意；进化搜索范式对仿真资源的利用效率低，使得设计周期过长且计算开销过大。

**方法:** 提出了不确定性感知奖励设计过程（URDP）框架：1) 通过自一致性分析量化候选奖励函数的不确定性；2) 引入不确定性感知贝叶斯优化（UABO）以提高超参数配置效率；3) 构建两级优化架构，将奖励组件优化与超参数调整解耦；4) 利用LLMs的奖励逻辑推理能力和贝叶斯优化的数值优化优势协同合作。

**结果:** 在涵盖三个基准环境的35个多样化任务中进行了全面评估，结果表明URDP不仅生成了更高质量的奖励函数，还显著提升了自动化奖励设计的效率。

**结论:** URDP通过整合LLMs和贝叶斯优化的优势，在奖励函数设计中实现了更高的质量和效率，为解决现有方法中的不足提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty-aware+Reward+Design+Process，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02256&send_immediately=true&force_search=false)

**原文摘要:** Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [27] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的知识图谱增强的零样本语义通信网络（KGZS-SC），通过知识图谱语义知识库（KG-SKB）提供通用语义表示和推理能力，减少通信开销并提高分类效率。实验结果表明，该网络在分类未见类别时表现出强大的泛化能力和优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 数据驱动的语义通信基于表面统计模式，缺乏可解释性和泛化性，特别是在存在未见数据的应用中。

**方法:** 利用知识图谱语义知识库（KG-SKB）对语义特征进行对齐，在共享类别语义嵌入空间中生成通用语义表示，增强发射端的泛化能力；接收端采用零样本学习（ZSL）实现对未见类别的直接分类，无需重新训练或增加计算负担。

**结果:** 在APY数据集上的仿真结果表明，提出的KGZS-SC网络在一系列信噪比（SNR）水平下，对未见类别的分类表现出强大的泛化能力，并显著优于现有的语义通信框架。

**结论:** KGZS-SC网络通过结合知识图谱和零样本学习技术，提高了语义通信系统的泛化能力和适应性，特别适合动态或资源受限环境下的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph-Based+Explainable+and+Generalized+Zero-Shot+Semantic+Communications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02291&send_immediately=true&force_search=false)

**原文摘要:** Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [28] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee, Jedrzej Kozal, Michal Wozniak, Bartosz Krawczyk*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的自适应记忆重新对齐（AMR）方法，用于在概念漂移下的持续学习中减少灾难性遗忘和提高模型适应能力。通过在几个基准数据集上的实验验证，AMR表现出色，能够显著降低标注需求和计算成本，同时保持与完全重学相近的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的持续学习方法主要关注减轻灾难性遗忘，但忽略了现实世界数据流中的概念漂移问题。当先前学习的任务数据分布发生变化时，模型需要既稳定又快速地适应新情况。

**方法:** 1. 提出一种名为自适应记忆重新对齐（AMR）的方法。
2. AMR从重放缓冲区中选择性移除过时样本，并用少量最新实例重新填充，以使记忆与新分布对齐。
3. 引入了四个带有概念漂移的标准视觉基准数据集变体：Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD 和 Tiny-ImageNet-CD。
4. 在这些数据集上进行广泛的实验，使用几种基于排练的基线模型来验证AMR的有效性。

**结果:** AMR在多个基准数据集上表现优异，能够在显著减少标注需求和计算成本的情况下，达到与完全重学（FR）相近的性能。实验结果表明，AMR可以有效地应对概念漂移，同时保持高准确率和低开销。

**结论:** AMR是一种可扩展的解决方案，在非平稳持续学习环境中实现了稳定性与适应性的平衡。它为处理现实世界中的动态数据流提供了一种高效且实用的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Holistic+Continual+Learning+under+Concept+Drift+with+Adaptive+Memory+Realignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02310&send_immediately=true&force_search=false)

**原文摘要:** Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [29] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang, Hongqi Li*

**主要类别:** cs.LG

**AI概要:** 本文综述了Transformer模型在脑电图(EEG)解码中的最新应用，探讨了其基本原理、混合架构、定制化结构的应用进展，并讨论了当前挑战与未来发展方向。


<details>
  <summary>更多</summary>
  
**动机:** 传统的机器学习方法在EEG分析中存在局限性，而深度学习方法（特别是Transformer）因其强大的序列数据处理能力，逐渐成为该领域的研究前沿。

**方法:** 文章首先阐述了Transformer的基本原理及其在EEG解码中的直接应用，接着详细概述了将基础Transformer与其他深度学习技术结合的常见混合架构，最后介绍了定制化Transformer内在结构的修改及其研究进展。

**结果:** 通过系统总结和分类，读者能够清晰了解Transformer在EEG解码中的当前应用状态及未来研究方向。

**结论:** 尽管Transformer在EEG解码领域取得了显著进展，但仍面临一些挑战，如计算资源需求大等。未来的研究应致力于解决这些问题并探索新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformer-based+EEG+Decoding%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02320&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [30] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim, Yechan Mun, Sangchul Hahn, Eunho Yang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的可解释人工智能算法DeltaSHAP，专为在线患者监测系统设计。通过适应Shapley值到时间序列环境，DeltaSHAP准确捕捉特征联合效应，并仅使用实际观察到的特征组合来归因预测变化，使其在时间敏感的临床应用中高效实用。实验表明，DeltaSHAP在解释质量和计算效率上均优于现有最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 在临床环境中，发现驱动患者风险演变的原因对于及时干预至关重要，但现有的XAI方法未能满足临床时间序列解释任务的独特需求。因此，需要一种新的XAI方法来解决这些需求。

**方法:** DeltaSHAP通过将Shapley值适应到时间序列环境，准确捕捉特征联合效应，并仅使用实际观察到的特征组合来归因预测变化。此外，还引入了新的评估指标来衡量时间序列在线解释的忠实性。

**结果:** 实验结果表明，DeltaSHAP在MIMIC-III去补偿基准上的解释质量比现有最先进方法高出62%，计算效率提高了33%的时间减少。

**结论:** DeltaSHAP是一种针对在线患者监测系统的新型XAI算法，能够有效解释连续预测的变化，提供特征归因的大小和方向，并实时交付这些见解。它在解释质量和计算效率上均优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeltaSHAP%3A+Explaining+Prediction+Evolutions+in+Online+Patient+Monitoring+with+Shapley+Values，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02342&send_immediately=true&force_search=false)

**原文摘要:** This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [31] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh, Byung-Jun Lee*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为PANI（Penalized Action Noise Injection）的方法，通过注入噪声动作并根据噪声量进行惩罚来增强离线强化学习算法的性能。此方法在不同基准测试中表现出显著的性能提升，并且解决了扩散模型计算成本高的问题。


<details>
  <summary>更多</summary>
  
**动机:** 离线强化学习由于环境交互成本高，依赖固定数据集进行策略优化。尽管扩散模型在离线RL中取得成功，但其推断过程需要大量计算资源，因此探索是否可以有更高效的替代方案成为研究动机。

**方法:** 提出了PANI方法，通过向动作空间注入噪声并按噪声量施加惩罚，从而覆盖整个动作空间。该方法从理论上被证明等价于解决一个修改后的马尔可夫决策过程（MDP），称为带噪声动作的MDP。PANI与多种现有的离线和无策略RL算法兼容。

**结果:** PANI方法在多个基准测试中显示出显著的性能改进，同时避免了扩散模型带来的高计算成本问题。

**结论:** PANI是一种简单而有效的增强离线强化学习的方法，它不仅提升了算法性能，还减少了对计算资源的需求，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+Reinforcement+Learning+with+Penalized+Action+Noise+Injection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02356&send_immediately=true&force_search=false)

**原文摘要:** Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [32] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama, Dong Eui Chang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种数据驱动的框架，结合强化学习方法优化高速内存系统中的信号完整性参数，显著提高了眼图开窗面积，并展示了优越性能、计算效率和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 在高速动态随机存取存储器（DRAM）系统中，信号完整性对等化器参数的优化至关重要，但传统方法要么计算成本高，要么依赖模型。

**方法:** 引入了基于学习到的潜在信号表示的数据驱动框架，结合无模型的强化学习（Advantage Actor-Critic）代理进行参数优化。潜在表示捕捉关键信号完整性特征，加速优化过程；强化学习代理无需显式系统模型即可得出最优等化器设置。

**结果:** 该方法在行业标准的DRAM波形上实现了显著的眼图开窗面积改进：级联连续时间线性等化器和决策反馈等化器结构提高了42.7%，仅使用决策反馈等化器配置提高了36.8%。

**结论:** 本研究展示了一种高效潜在信号完整性度量、鲁棒无模型强化学习策略，并验证了其在复杂等化器架构上的优越性能，为DRAM系统提供了更高效和通用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Reinforcement+Learning-Based+DRAM+Equalizer+Parameter+Optimization+Using+Latent+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02365&send_immediately=true&force_search=false)

**原文摘要:** Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [33] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo, Lina Achaji, Stefano Sabatini, Nicola Poerio, Grzegorz Bartyzel, Sascha Hornauer, Fabien Moutarde*

**主要类别:** cs.LG

**AI概要:** 通过在多智能体环境中使用偏好优化微调轨迹预测模型，能够在不增加推理时计算负担和几乎不影响轨迹预测精度的情况下显著提高场景一致性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于深度学习的轨迹预测模型在复杂交互场景中难以捕捉智能体间的重要相互依赖关系，导致交通场景中的预测不一致。受将人类偏好融入大型语言模型有效性的启发，本文尝试在轨迹预测中引入偏好优化。

**方法:** 利用自动计算的预测未来偏好排名作为输入，在微调过程中优化轨迹预测模型。此方法应用于三个不同数据集上的最新模型，评估其效果。

**结果:** 实验表明，该方法可以在不影响轨迹预测精度和不增加推理时计算需求的情况下，显著提高场景一致性。

**结论:** 在多智能体环境下，使用偏好优化微调轨迹预测模型是一种有效的方法，可以提升预测的一致性，同时保持预测精度并控制计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Consistency+in+Vehicle+Trajectory+Prediction+Through+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02406&send_immediately=true&force_search=false)

**原文摘要:** Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [34] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye*

**主要类别:** cs.LG

**AI概要:** Federated Graph Learning (FGL) combines FL and GNNs, but current research lacks in spatial and spectral propagation. We propose S2FGL with a global knowledge repository and frequency alignment to tackle these issues, showing superior performance.


<details>
  <summary>更多</summary>
  
**动机:** 当前的研究只从结构的角度解决子图联邦学习（subgraph-FL）问题，忽略了图信号在空间和频谱域上的传播。这导致了标签信号的中断和全局GNN分类知识的退化（从空间角度），以及局部GNN对局部信号传播方案的过拟合（从频谱角度）。

**方法:** 提出一个全球知识库来缓解标签信号中断的问题，并采用频率对齐方法来解决频谱客户端漂移问题。将空间和频谱策略结合形成S2FGL框架。

**结果:** 在多个数据集上的广泛实验表明，S2FGL优于其他方法。

**结论:** S2FGL通过结合空间和频谱策略，有效解决了FGL中的挑战，展示了优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是S2FGL%3A+Spatial+Spectral+Federated+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02409&send_immediately=true&force_search=false)

**原文摘要:** Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [35] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani, Henrik Christiansen, Federico Errica*

**主要类别:** cs.LG

**AI概要:** Kolmogorov Arnold Networks (KANs) 是一种基于 Kolmogorov-Arnold 定理的新兴机器学习架构。本文提出了 InfinityKAN，通过变分推断优化和反向传播，自适应学习每个单变量函数的潜在无限基的数量，从而将关键超参数纳入学习过程。


<details>
  <summary>更多</summary>
  
**动机:** 尽管 KANs 提供了强大的理论基础来表示多变量连续有界函数，但其实际应用受限于建模单变量函数时基的数量的主观选择问题。

**方法:** 提出了一种名为 InfinityKAN 的方法，将问题建模为变分推断优化问题，并利用反向传播算法，在训练过程中自适应地学习每个单变量函数的潜在无限基的数量。

**结果:** InfinityKAN 方法扩展了 KANs 的潜在适用性，通过将关键超参数（即基的数量）作为学习过程的一部分，解决了传统 KANs 中基数量选择的局限性。

**结论:** InfinityKAN 为 KANs 提供了一种新的改进方向，通过自适应学习机制克服了基数量选择的主观性问题，增强了 KANs 在实际应用中的表现和灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Kolmogorov-Arnold+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02466&send_immediately=true&force_search=false)

**原文摘要:** Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [36] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的训练策略GORP，它通过结合全参数和低秩参数并在统一的低秩梯度子空间中联合更新，解决了效率与表达能力之间的权衡问题。GORP在持续学习基准测试中表现优于现有最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）的持续微调受到效率与表达能力之间权衡的阻碍。现有的低秩适应（LoRA）方法虽然提高了效率，但限制了模型学习新任务和迁移知识的能力。

**方法:** GORP是一种将全参数和低秩参数协同结合，并在统一的低秩梯度子空间中联合更新的新型训练策略。这种方法扩展了优化空间，同时保持了效率并减轻了灾难性遗忘。

**结果:** 广泛的实验表明，在持续学习基准上，GORP的表现优于现有的最先进方法。

**结论:** GORP为解决持续学习中的效率与表达能力权衡问题提供了一种有效的方法，其性能在多个基准测试中得到了验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continual+Gradient+Low-Rank+Projection+Fine-Tuning+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02503&send_immediately=true&force_search=false)

**原文摘要:** Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [37] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi, Ahmed M. Azab, Seif Eldawlatly, Gamal M. Aly*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种通过优化预处理和深度学习技术来显著提高脑机接口中跨被试运动想象分类性能的新方法。该方法在四个不同数据集上进行了验证，显著超越了现有技术，并为通用、无需校准的运动想象分类建立了新基准。


<details>
  <summary>更多</summary>
  
**动机:** 跨被试运动想象分类在脑机接口中面临巨大挑战，主要原因是不同个体间的脑电图模式存在显著变异性，这导致其分类准确率低于特定被试模型，成为开发适用于实际应用的无校准脑机接口的主要障碍。

**方法:** 研究方法包括对短时傅里叶变换（STFT）转换的脑电图数据进行直接分类、优化STFT参数以及在卷积神经网络（CNN）训练过程中采用平衡批处理策略。此外，系统地研究了使用1至4秒运动想象窗口的分类性能。

**结果:** 此方法在四个不同数据集上实现了显著的跨被试分类改进，分别在BCI竞赛IV数据集1（IV-1）、数据集2A（IV-2A）和数据集2B（IV-2B）上达到67.60%、65.96%和80.22%的准确率，超越了现有最先进技术。

**结论:** 该研究表明，所提出的方法为通用、无需校准的运动想象分类设定了新的基准，并贡献了一个强大的开源数据集以推动该领域的进一步研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TFOC-Net%3A+A+Short-time+Fourier+Transform-based+Deep+Learning+Approach+for+Enhancing+Cross-Subject+Motor+Imagery+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02510&send_immediately=true&force_search=false)

**原文摘要:** Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [38] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska, Riccardo Belluzzo, Piotr Zieliński, Joanna Baran, Paweł Olszewski*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的训练文本到SQL生成模型的方法——RetrySQL，通过引入自我修正机制和重试步骤来提升模型的准确性和执行能力。相比没有重试数据的预训练方法，此方法在整体和挑战性执行准确性指标上提高了4个百分点。此外，研究发现低秩适应（LoRA）监督微调对学习重试数据无效，而全参数预训练是必要的。最终，RetrySQL模型在执行准确性方面与包含多数量级参数的专有模型具有竞争力，并证明了自我修正行为可以被学习，为SQL导向的语言模型提供了一种新方法来提高生成准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前许多文本到SQL的任务解决方案依赖于黑盒语言模型和定制的端到端管道，但缺乏针对SQL的具体生成模型的研究。同时，最近在自校正生成策略方面的进展显示出改进现有架构能力的潜力，但这些概念尚未应用于文本到SQL任务。因此，作者试图探索一种新的方法以利用自我修正机制来提升SQL生成模型的性能。

**方法:** 1. 准备参考SQL查询的推理步骤，并通过破坏这些步骤创建重试数据，其中包含错误和修正后的步骤，并用特殊标记分隔。
2. 使用此重试数据持续预训练一个开源编码模型。
3. 评估重试步骤对整体和挑战性执行准确性的影响。
4. 研究低秩适应（LoRA）监督微调是否能从重试数据中有效学习，并确认全参数预训练对于该任务的必要性。
5. 将经过RetrySQL训练的模型整合到完整的文本到SQL管道中进行比较。

**结果:** 1. 在整体和挑战性执行准确性指标上，使用重试数据的预训练方法比不使用重试数据的预训练方法提升了高达4个百分点。
2. 发现低秩适应（LoRA）监督微调无法从重试数据中有效学习，而全参数预训练是必需的。
3. 模型展示了自我修正行为，并且下游准确性指标的提升是由于这种额外技能的结果。
4. RetrySQL训练的模型在执行准确性方面与包含多数量级参数的专有模型具有竞争力。

**结论:** RetrySQL成功展示了如何在文本到SQL任务中学习自我修正行为，并提供了一种新颖的方式来提高SQL导向语言模型的生成准确性。通过引入重试步骤和自我修正机制，可以在较少参数的情况下达到与高参数专有模型相当的性能。这表明自我修正机制在文本到SQL任务中的潜在价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RetrySQL%3A+text-to-SQL+training+with+retry+data+for+self-correcting+query+generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02529&send_immediately=true&force_search=false)

**原文摘要:** The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [39] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer, Davide D'Ascenzo, Rafael Dubach, Tomaso Poggio*

**主要类别:** cs.LG

**AI概要:** 尽管深度神经网络在高维领域中取得了显著成功，但其学习动力学的基本原理仍不清楚。本文提出DNN的成功源于它们能够利用目标函数的组合稀疏结构。大多数实际相关的函数可以从少数依赖于低维输入子集的组成函数构建。这种特性适用于所有高效图灵可计算函数，因此很可能存在于当前所有学习问题中。虽然在组合稀疏函数的设定下有关逼近和泛化的理论有所进展，但关于DNN的学习性和优化性仍存在许多重要问题。


<details>
  <summary>更多</summary>
  
**动机:** 探索深度神经网络（DNN）成功背后的基本原理，并解释为什么DNN能够在高维领域中克服传统浅层网络面临的维度诅咒。

**方法:** 论证DNN的成功在于它们能利用目标函数的组合稀疏结构。具体来说，大多数实际相关的函数可以由少数依赖于低维输入子集的组成函数构建。同时指出这种特性适用于所有高效图灵可计算函数。

**结果:** 提出了组合稀疏性作为理解DNN成功的关键概念，并指出这一特性可能普遍存在于当前所有学习问题中。然而，关于DNN的学习性和优化性的几个重要问题仍然未解。

**结论:** 完成对组合稀疏性在深度学习中作用的研究是建立全面的人工智能理论甚至通用智能理论的核心。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+A+Theory+of+Deep+Learning+Must+Include+Compositional+Sparsity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02550，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02550&send_immediately=true&force_search=false)

**原文摘要:** Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [40] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni, Galvin Khara, Joachim Schaeffer, Marat Subkhankulov, Stefan Heimersheim*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了在GPT-2模型中去除层归一化（LN）的影响。研究表明，移除LN层只会导致验证损失的轻微增加，并且不会对语言建模产生重大影响。此外，研究发现调整数据量与模型参数呈次线性增长关系，为更大模型去除LN提供了可能性。作者发布了无LN的GPT-2模型套件，还测试了这些模型上的可解释性技术，发现直接logit归因现在可以给出各组件的确切直接影响。


<details>
  <summary>更多</summary>
  
**动机:** 尽管层归一化（LN）对训练稳定性的影响已被广泛记录，但其在推理时的作用尚不明确。此外，LN层通过引入额外的非线性和增加模型组件之间的互联性，阻碍了机制可解释性。因此，研究者试图探索LN在语言建模中的实际作用，并评估移除LN后模型的表现及可解释性。

**方法:** 研究者从所有GPT-2模型中移除了所有的LN层，并观察验证损失的变化。他们发现只需少量的微调数据即可实现LN的移除，并且验证损失仅略有增加。接着，研究者发布了无LN的GPT-2模型套件，并在这些模型上测试了可解释性技术，包括直接logit归因和归因修补。

**结果:** 移除LN层后，验证损失仅略有增加（如GPT-2 XL的交叉熵损失增加了0.03）。微调数据需求与模型参数呈次线性增长关系，表明该方法可扩展到更大的模型。无LN模型的直接logit归因能够精确地反映各组件的影响，而归因修补的准确性没有显著提升。此外，GPT-2中的“置信神经元”在无LN模型中变得不活跃。

**结论:** 层归一化（LN）在语言建模中并非不可或缺，GPT-2类模型可以在没有LN层的情况下正常运行。无LN的GPT-2模型有助于更精确的可解释性研究，从而加深对语言模型的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Don%27t+Need+LayerNorm+at+Inference+Time%3A+Scaling+LayerNorm+Removal+to+GPT-2+XL+and+the+Implications+for+Mechanistic+Interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02559&send_immediately=true&force_search=false)

**原文摘要:** Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [41] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse, Emily Yu, Christoph H. Lampert*

**主要类别:** cs.LG

**AI概要:** Learned Differentiable Boolean Logic Networks (DBNs) are extended with a trainable interconnect and two pruning stages for reducing model size while maintaining accuracy.


<details>
  <summary>更多</summary>
  
**动机:** To make DBNs scalable to wider layers and further reduce model size without affecting accuracy.

**方法:** Extend DBNs with a trainable, differentiable interconnect whose parameter count remains constant as input width grows. Propose two complementary pruning stages: SAT-based logic equivalence pass and similarity-based, data-driven pass.

**结果:** DBNs can scale to far wider layers than earlier designs while preserving advantageous accuracy. The proposed pruning stages effectively reduce model size with a superior compression-accuracy trade-off.

**结论:** The extension of DBNs with the new interconnect and pruning techniques allows them to be more scalable and compact, maintaining high accuracy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Interconnect+Learning+in+Boolean+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02585&send_immediately=true&force_search=false)

**原文摘要:** Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [42] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya, Levent Eren*

**主要类别:** cs.LG

**AI概要:** PadéNets在使用振动和声学数据诊断感应电机电气和机械故障方面优于传统CNN和Self-ONNs，其诊断准确率高达99.96%。


<details>
  <summary>更多</summary>
  
**动机:** 提高感应电机故障诊断的准确性，探索PadéNets是否能超越传统CNN和Self-ONNs在诊断电气和机械故障方面的表现。

**方法:** 评估并比较三种深度学习架构（一维CNN、Self-ONNs和PadéNets）的诊断能力，使用渥太华大学公开的恒速感应电机数据集进行测试。PadéNet模型引入增强的非线性，并与无界激活函数（如Leaky ReLU）兼容。

**结果:** PadéNets在所有传感器数据上均表现出色，分别对加速度计1、2、3和声学传感器达到99.96%、98.26%、97.61%和98.33%的诊断准确率。

**结论:** PadéNets通过增强的非线性和与无界激活函数的兼容性，显著提高了感应电机状态监测中的故障诊断性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pad%C3%A9+Approximant+Neural+Networks+for+Enhanced+Electric+Motor+Fault+Diagnosis+Using+Vibration+and+Acoustic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02599&send_immediately=true&force_search=false)

**原文摘要:** Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [43] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet, Ruben Ohana, Michael McCabe, Gilles Louppe, François Lanusse, Shirley Ho*

**主要类别:** cs.LG

**AI概要:** 在动态系统仿真中，使用自编码器的潜在空间代替像素空间可以显著减少扩散模型推理时的计算成本，并且在高达1000倍的压缩率下依然保持较高的准确性。此外，扩散模型比非生成模型更准确，且能通过更高的多样性弥补预测中的不确定性。


<details>
  <summary>更多</summary>
  
**动机:** 扩散模型在推理时计算成本高，阻碍了其作为快速物理模拟器的应用。为了降低成本，研究者尝试在自编码器的潜在空间而不是像素空间进行生成任务。

**方法:** 研究者调查了在动态系统仿真中应用类似策略（即在潜在空间进行仿真）的有效性，并分析了其代价。研究发现，潜在空间仿真的准确性对广泛的压缩率具有较强的鲁棒性；同时，扩散模型比非生成模型更准确，并能通过更高的多样性弥补预测中的不确定性。

**结果:** 潜在空间仿真的准确性对压缩率（高达1000倍）具有鲁棒性；扩散模型比非生成模型更准确，并能通过更高的多样性弥补预测中的不确定性。

**结论:** 潜在空间仿真是一个有效的策略，可以在显著降低计算成本的同时保持高准确性，扩散模型是动态系统仿真中的一种有潜力的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+in+Latent+Space%3A+An+Empirical+Study+of+Latent+Diffusion+Models+for+Physics+Emulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02608&send_immediately=true&force_search=false)

**原文摘要:** The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [44] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan, Sinan Kalkan, Fatos T. Yarman-Vural*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的模型Learnable VAE (L-VAE)，它不仅学习解耦表示，还学习损失函数的超参数。通过在损失函数中添加额外的正则化项，L-VAE能够同时学习模型架构参数和损失项权重，从而平衡重构保真度和解耦潜在维度之间的动态权衡。实验结果表明，L-VAE在多个数据集上表现优异或次优，并且在CelebA数据集上的定性实验也验证了其成功解耦面部属性的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法如β-VAE需要手动调整超参数以平衡重构损失与解耦损失，这可能导致性能不稳定。因此，作者希望开发一种可以自动学习这些超参数的方法，以实现更有效的解耦表示学习。

**方法:** 提出Learnable VAE (L-VAE)模型，该模型扩展了β-VAE，通过学习损失函数中的相对权重来控制重构损失与解耦损失之间的动态权衡。同时，模型架构参数与损失项权重被并发学习，并在损失函数中加入额外的正则化项以防止偏向任一损失。

**结果:** 实验分析显示，L-VAE在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上，依据一系列解耦度量指标，提供了最佳或次优的表现。此外，在CelebA数据集上的定性实验进一步验证了L-VAE在解耦面部属性方面的成功。

**结论:** L-VAE通过自动学习超参数和损失项权重，有效缓解了β-VAE的局限性，实现了重构保真度与潜在维度解耦的良好平衡，展现出优秀的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是L-VAE%3A+Variational+Auto-Encoder+with+Learnable+Beta+for+Disentangled+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02619&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [45] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré, Borja Rodríguez Gálvez, Yoomi Park, Yitian Zhou, Volker M. Lauschke, Ming Xiao*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于变压器的矩阵变分自动编码器（matVAE），其在零样本预测任务中优于现有的DeepSequence模型，同时在计算需求和参数数量上更少。通过结合AlphaFold生成的结构信息，该模型进一步提升了性能，并且表明DMS数据集可以作为MSA的有效替代以用于变异效应预测。


<details>
  <summary>更多</summary>
  
**动机:** 传统的变异效应预测器（VEPs）依赖于多序列比对（MSA），但这一方法假设自然发生的变异是适应性的，这在一些经历低进化压力的药物基因中可能不成立。因此，需要探索一种新的方法来提高变异效应预测的准确性。

**方法:** 研究团队开发了一种基于变压器的矩阵变分自动编码器（matVAE），并使用了结构化的先验知识。他们将此模型与现有的DeepSequence模型进行了比较，并评估了其在33个DMS数据集上的表现。此外，还研究了将AlphaFold生成的结构信息整合到模型中的效果。

**结果:** matVAE-MSA在零样本预测任务上超越了DeepSequence模型，尽管使用的参数数量和推理时的计算量显著减少。当与DMS数据训练的模型（matENC-DMS）进行比较时，后者在监督预测任务上表现更好。引入AlphaFold生成的结构信息后，模型性能得到了进一步提升，达到了与DeepSequence（基于MSA训练并在DMS上微调）相当的结果。

**结论:** 研究表明，DMS数据集可以有效地替代MSA，而不会显著降低预测性能，这为未来DMS数据集的发展以及深入探索其与VEP的关系提供了重要依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Matrix+Variational+Auto-Encoder+for+Variant+Effect+Prediction+in+Pharmacogenes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02624&send_immediately=true&force_search=false)

**原文摘要:** Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [46] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz, Atai Ambus, Moni Shahar, Ran Gilad-Bachrach*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Medical Data Pecking的方法，用于识别EHR数据中的质量问题。该方法包括自动生成测试和执行这些测试以报告潜在错误，并在三个数据集上进行了验证，发现了20-43个数据问题。结论是这种方法可以提高数据分析结果的有效性，并为未来的发展奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 电子健康记录（EHRs）在流行病学研究和人工智能训练中的使用迅速增加，但其数据质量存在问题，如亚人群的误代表、偏差和系统性错误等。现有的数据质量评估方法不足以系统地评估数据是否适合研究，因此需要一种新的方法来解决这些问题。

**方法:** 论文提出了Medical Data Pecking方法，该方法借鉴了软件工程中的单元测试和覆盖率概念。它由两个主要组件组成：1）一个自动测试生成器，利用大型语言模型和接地技术从数据和研究描述中创建测试套件；2）一个数据测试框架，用于执行这些测试并报告潜在错误和覆盖率。

**结果:** MDPT在三个数据集上进行了评估：All of Us (AoU)，MIMIC-III 和 SyntheticMass，在每个队列中生成了55-73个测试，涵盖了四个条件。这些测试成功识别出了20-43个非对齐或不符合的数据问题，并详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。

**结论:** 我们的方法通过结合外部医学知识，使上下文敏感的数据质量测试成为数据分析工作流的一部分，从而提高了结果的有效性。此方法从质量保证的角度解决了上述挑战，并为未来的进一步发展奠定了基础，例如添加更多的数据模式和改进的接地方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Medical+Data+Pecking%3A+A+Context-Aware+Approach+for+Automated+Quality+Evaluation+of+Structured+Medical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02628&send_immediately=true&force_search=false)

**原文摘要:** Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [47] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil*

**主要类别:** cs.LG

**AI概要:** 近期研究表明，训练损失与模型大小和令牌数量呈幂律关系，并且实现计算最优模型需要同时扩展模型大小和令牌数量。然而，这些缩放定律假设数据无限供应，并主要适用于计算受限环境。随着现代大型语言模型越来越依赖大规模互联网数据集，计算受限的假设变得不那么有效。这突显了需要优先考虑令牌效率的架构。在本工作中，我们研究了2-单纯形Transformer的使用，该架构通过高效的Triton内核实现将标准点积注意力推广到三线性函数。我们证明了2-单纯形Transformer在数学、编码、推理和逻辑任务上比标准Transformer具有更好的令牌效率：对于固定的令牌预算，类似大小的模型优于其点积对应模型。我们通过展示2-单纯形注意力在知识和推理任务的缩放定律中改变了相对于点积注意力的指数来量化这些收益。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大型语言模型在依赖大规模互联网数据集时，计算受限的假设不再完全适用，因此需要探索更高效的模型架构以提升令牌效率。

**方法:** 研究使用2-单纯形Transformer架构，该架构通过高效的Triton内核实现将标准点积注意力推广到三线性函数，从而可能提高模型的令牌效率。

**结果:** 2-单纯形Transformer在数学、编码、推理和逻辑任务上表现出比标准Transformer更高的令牌效率，特别是在固定令牌预算下，类似大小的模型性能更优。并且2-单纯形注意力改变了知识和推理任务中的缩放定律指数。

**结论:** 2-单纯形Transformer架构展示了在特定任务上的优越性，特别是在提升令牌效率方面，这对于未来的大规模语言模型设计具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+and+Simplex%3A+2-Simplicial+Attention+in+Triton，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02754&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [48] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

**主要类别:** cs.LG

**AI概要:** 本文提出一种新的分层深度学习框架，用于递归高阶元学习，核心是生成虚拟任务的机制，帮助神经网络构建、解决和泛化任务层次结构，推动机器学习向通用人工智能发展。


<details>
  <summary>更多</summary>
  
**动机:** 引入一种新的分层深度学习框架，用于递归高阶元学习（meta-learning），使神经网络能够构建、解决和泛化任务层次结构。

**方法:** 该方法的核心是一种生成机制，创建虚拟任务——旨在让元学习者学习软约束和未知的可泛化规则。通过主动探索虚拟点景观并寻找低级学习者发现困难的任务，元学习者迭代地细化约束区域。每个元级别的层次结构对应于对较低级别解决问题的逐步抽象泛化。将元学习者解释为范畴论中的函子（functors），生成和调节从属学习者的层次结构，建立支持抽象和知识转移的组合结构。

**结果:** 这种架构可能支撑下一代神经网络，能够自主生成新颖、有指导意义的任务及其解决方案，推动机器学习向通用人工智能发展。

**结论:** 提出的分层深度学习框架和元学习方法提供了一种结构化和可解释的学习进展，并统一了现有的元学习模型，揭示了学习过程如何通过函子关系进行转换和比较，同时提供了设计元学习的实际原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Order+Deep+Meta-Learning+with+Category-Theoretic+Interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02634&send_immediately=true&force_search=false)

**原文摘要:** We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [49] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron, Chris Hicks, Vasilios Mavroudis*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了强化学习中数据高效探索的挑战，通过信息理论方法来定义内在动机，特别是针对认知不确定性而非环境中的随机噪声。论文提供了基于信息增益（IG）方法的形式化保证，并提出了一个框架——预测轨迹采样与贝叶斯探索（PTS-BE），该框架在多种环境中表现出优于其他基线算法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中，探索策略的数据效率是一个重要挑战，现有的探索方法可能无法有效地区分认知不确定性和环境固有的随机性。因此，需要更有效的探索机制以加速学习过程并提高样本效率。

**方法:** 作者研究了一类专注于认知不确定性的探索奖励机制，并证明这些奖励可以自然地反映认知信息增益，且当代理对环境动态和奖励足够确定时会收敛至零。同时，他们提出了一些实际可用的近似方法，如稀疏变分高斯过程、深度核方法和深度集成模型。最终，他们设计了一个结合模型预测规划与信息理论奖励的框架——PTS-BE。

**结果:** 实验结果表明，PTS-BE框架在具有稀疏奖励或纯探索性质的任务中显著优于其他基准算法。

**结论:** 论文为基于信息增益的探索方法提供了理论支持，并展示了PTS-BE框架在提升样本效率方面的有效性，为进一步发展高效的探索策略奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Efficient+Bayesian+Exploration+in+Model-Based+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02639&send_immediately=true&force_search=false)

**原文摘要:** In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [50] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng, Ming-Hui Liu, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了深度伪造检测模型中的泛化能力和人口群体公平性之间的因果关系，并提出了一个名为DAID的框架来同时改善这两个方面。


<details>
  <summary>更多</summary>
  
**动机:** 深度伪造检测模型面临两个主要挑战：对未见过的操作的泛化能力以及在人群群体中的公平性。然而，现有的方法通常表明这两个目标是内在冲突的，存在权衡。

**方法:** 提出了一种称为Demographic Attribute-insensitive Intervention Detection（DAID）的即插即用框架，该框架包括：i）人口统计感知的数据再平衡，利用逆倾向加权和子群特征归一化来中和分布偏差；ii）人口统计无关的特征聚合，使用新颖的对齐损失来抑制敏感属性信号。

**结果:** 在三个跨域基准测试中，与几种最先进的检测器相比，DAID在公平性和泛化方面始终表现出优越的性能。

**结论:** DAID框架不仅验证了其理论基础，还证明了其实际有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+Deepfake+Detectors+Can+Generalize，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02645&send_immediately=true&force_search=false)

**原文摘要:** Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [51] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang*

**主要类别:** cs.LG

**AI概要:** 提出OmniDraft框架，解决在线部署中草稿模型与目标模型不兼容及延迟优化问题，通过统一草稿模型适配不同目标模型并动态调整，支持设备端LLM应用，显著提升解码速度1.5-2倍。


<details>
  <summary>更多</summary>
  
**动机:** 在线部署中存在草稿模型与目标模型不兼容以及对延迟改进的需求，促使研究一种通用草稿模型以适应不同目标模型和用户数据。

**方法:** 引入OmniDraft框架，结合在线n-gram缓存与混合蒸馏微调解决词汇表不匹配问题，并利用自适应草稿技术提升解码速度。

**结果:** 单个Llama-68M草稿模型可与多个目标模型（如Vicuna-7B、Qwen2-7B等）配合使用，实现推测解码，并提供1.5-2倍的速度提升。在数学推理、编程和文本生成任务上展示在线学习能力。

**结论:** OmniDraft框架适用于设备端LLM应用，强调了'一个草稿适配所有'范式的必要性，解决了在线部署中的关键挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OmniDraft%3A+A+Cross-vocabulary%2C+Online+Adaptive+Drafter+for+On-device+Speculative+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02659&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [52] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao, Joshua Moller, Porfi Quintero-Cadena, Lood van Niekerk*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一个指导离散扩散模型，用于优化抗体序列的可开发性，并结合高通量可开发性分析，实现了同时满足结合和生物物理标准的抗体设计。


<details>
  <summary>更多</summary>
  
**动机:** 抗体疗法不仅需要高亲和力的目标结合，还需要良好的可制造性、稳定性和安全性，这些统称为“可开发性”。为了建立一个计算框架来优化抗体序列以获得良好的可开发性，进行了这项研究。

**方法:** 引入了一个基于自然重链和轻链配对序列以及246个临床阶段抗体定量可开发性测量数据训练的指导离散扩散模型。集成了一个软值解码扩散（SVDD）模块，用于引导生成生物物理上可行的候选物，同时不损害其自然性。

**结果:** 在无约束采样中，该模型可以重现天然抗体库和已批准治疗抗体的全局特征；在SVDD引导下，预测的可开发性得分显著高于未引导的基线。

**结论:** 结合高通量可开发性分析，该框架实现了一个迭代的、机器学习驱动的抗体设计管道，能够同时满足结合和生物物理标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guided+Generation+for+Developable+Antibodies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02670&send_immediately=true&force_search=false)

**原文摘要:** Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [53] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo, Hanh Huyen My Nguyen, Christian Ledig*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种通过差分隐私生成模型实现数据共享的方法，采用基础模型提取紧凑信息嵌入，并通过联合训练差分隐私条件变分自编码器（DP-CVAE）来建模全球隐私感知数据分布，支持多样下游任务，相较于传统联邦学习分类器和DP-CGAN表现出更好的性能、更高的隐私保护、更强的可扩展性和更高的效率。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在医学影像领域取得了革命性进展，但其应用受到数据稀缺和隐私法规限制，难以获取多样化数据集。联邦学习虽然可以实现去中心化训练，但通信成本高且通常局限于单一下游任务，缺乏灵活性。因此需要一种既能保护隐私又能支持多样化任务的新方法。

**方法:** 通过差分隐私生成模型进行数据共享，利用基础模型提取紧凑且信息丰富的嵌入表示以减少冗余并降低计算开销。客户端协同训练一个差分隐私条件变分自编码器（DP-CVAE），用于建模全球隐私感知的数据分布，支持多种下游任务。该方法在多个特征提取器上进行了验证。

**结果:** 与传统联邦学习分类器相比，所提出的方法在确保差分隐私的同时提升了性能、增强了隐私保护、提高了可扩展性和效率。此外，DP-CVAE生成的嵌入比DP-CGAN具有更高的保真度，同时参数量减少了5倍。

**结论:** 所提出的基于差分隐私生成模型的数据共享方法，结合协同训练的DP-CVAE，能够有效解决数据稀缺和隐私保护问题，支持多样化下游任务，表现优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedding-Based+Federated+Data+Sharing+via+Differentially+Private+Conditional+VAEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02671&send_immediately=true&force_search=false)

**原文摘要:** Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [54] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg, Yao Ma, Seyed Sahand Mohammadi Ziabari, Marijn van Rijswijk*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了多智能体强化学习（MARL）如何改进供应链中的动态定价策略，特别是在传统ERP系统依赖静态、基于规则的方法而忽略市场参与者之间战略互动的背景下。研究评估了三种MARL算法（MADDPG、MADQN和QMIX）在模拟环境中的表现，并与静态规则基线进行了比较。结果表明，MARL引入了静态定价规则无法捕捉的新兴战略行为，可能为动态定价的未来发展提供参考。


<details>
  <summary>更多</summary>
  
**动机:** 当前供应链中的动态定价策略多依赖于静态、基于规则的传统ERP系统，忽略了市场参与者之间的战略互动。虽然强化学习已被应用于定价，但大多数实现仍然是单智能体，未能建模现实世界供应链的相互依存性。

**方法:** 研究评估了三种多智能体强化学习（MARL）算法（MADDPG、MADQN和QMIX）在模拟环境中的表现，并与静态规则基线进行了比较。该模拟环境基于真实的电子商务交易数据和LightGBM需求预测模型。

**结果:** 规则基代理实现了接近完美的公平性（Jain指数：0.9896）和最高的价格稳定性（波动性：0.024），但完全缺乏竞争动态。在MARL代理中，MADQN表现出最激进的定价行为，波动性最高且公平性最低（0.5844）。MADDPG则提供了更为平衡的方法，在支持市场竞争（市场份额波动：9.5个百分点）的同时，保持相对较高的公平性（0.8819）和价格稳定性。

**结论:** MARL引入了静态定价规则无法捕捉的新兴战略行为，这可能对未来动态定价的发展具有重要影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Reinforcement+Learning+for+Dynamic+Pricing+in+Supply+Chains%3A+Benchmarking+Strategic+Agent+Behaviours+under+Realistically+Simulated+Market+Conditions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02698&send_immediately=true&force_search=false)

**原文摘要:** This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [55] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari, Krishna Reddy Kesari*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于共识的协议，用于在联邦学习中识别具有最有用模型权重的客户端子集，以减少数据传输成本。同时，提出了一种新的流体民主协议（粘滞保留民主）和一种算法（FedVRD），以优化性能并限制对手的影响。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习通常需要每个客户端将其权重转移到中央服务器，无论这些权重是否有用，这会导致浪费的数据传输成本。因此，研究旨在通过基于共识的协议来减少这种成本。

**方法:** 1. 探索现有的流体民主协议在联邦学习中的应用，并与传统的每人一票（如FedAvg）进行比较。
2. 提出一种新的流体民主协议——粘滞保留民主（viscous-retained democracy），其在相同假设下优于传统方法且不允许多次投票。
3. 从对抗性角度分析流体民主协议的弱点，并提出一种新算法（FedVRD），该算法动态限制对手影响并最小化成本。

**结果:** 1. 新提出的粘滞保留民主协议在性能上优于传统方法。
2. 确定了现有流体民主协议在拓扑结构和对手数量方面的弱点。
3. FedVRD算法能够有效限制对手影响并降低通信成本。

**结论:** 基于共识的协议可以显著减少联邦学习中的数据传输成本，同时提高模型性能。新的流体民主协议和FedVRD算法为解决对手影响和优化性能提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fluid+Democracy+in+Federated+Data+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02710&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [56] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang, Chenyuan Hu, Yu Luo, Zhecheng Yuan, Ruijie Zheng, Huazhe Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的深度强化学习算法FoG，通过Experience Replay Decay和Network Expansion机制提升RL代理的样本效率和泛化能力。实验结果表明，FoG在多个连续控制基准上优于现有最先进的深度RL算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的连续控制深度强化学习方法容易受到初始偏见的影响，即过度拟合重放缓冲区中早期存储的经验，从而限制了RL代理的样本效率和泛化能力。相比之下，人类由于婴儿期遗忘现象，较少受这种偏见影响。因此，受神经科学中遗忘与成长双重过程的启发，提出了新的深度RL算法。

**方法:** 提出了一种名为Forget and Grow（FoG）的新深度RL算法，包含两个机制：1) Experience Replay Decay（ER Decay），逐渐减少早期经验的影响以平衡记忆；2) Network Expansion，通过在训练过程中动态添加新参数来增强代理利用现有数据模式的能力。

**结果:** 在四个主要连续控制基准（超过40个任务）上的实证结果表明，FoG的性能优于现有的最先进的深度RL算法，如BRO、SimBa和TD-MPC2。

**结论:** FoG算法通过引入Experience Replay Decay和Network Expansion机制，有效提升了RL代理的样本效率和泛化能力，并在多个连续控制任务上表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Forget-and-Grow+Strategy+for+Deep+Reinforcement+Learning+Scaling+in+Continuous+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02712&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [57] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat, Michael Fire, Eran Ben-Elia*

**主要类别:** cs.LG

**AI概要:** 无桩电动滑板车作为一种重要的微观交通服务，本文提出了一种融合空间、时间和网络依赖关系的预测框架，提升了需求预测的准确性（较基础模型提高27-49%），为车队分布优化和可持续城市规划提供支持。


<details>
  <summary>更多</summary>
  
**动机:** 尽管无桩电动滑板车作为环保且灵活的城市交通替代方案有许多优点，但其有效管理依赖于精确的需求预测。已有研究多孤立分析空间或时间因素，未能充分捕捉需求模式。

**方法:** 本研究引入一个整合了空间、时间和网络依赖性的框架，以改进微观交通需求预测。通过结合多种因素，增强预测精度并深入理解城市微观交通使用模式。

**结果:** 与基础模型相比，该框架将需求预测准确率提高了27至49%，证明了其在捕捉微观交通需求模式方面的有效性。

**结论:** 这一研究成果有助于数据驱动的微观交通管理，实现车队分布优化、成本降低以及可持续的城市规划。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Machine+Learning+Framework+for+Micromobility+Demand+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02715，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02715&send_immediately=true&force_search=false)

**原文摘要:** Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [58] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu*

**主要类别:** cs.LG

**AI概要:** 近期科学AI的发展强调了对比学习在连接异构生物数据模态中的强大作用。基于这一范式，本文提出了HIPPO（跨物种蛋白质-蛋白质相互作用预测的分层框架）。HIPPO通过多层次的生物表示匹配，对齐蛋白质序列及其分层属性，并引入分层对比损失函数来模拟蛋白质功能类别之间的结构化关系。实验表明，HIPPO在基准数据集上达到了最先进的性能，且在低数据量情况下表现出鲁棒性。此外，该模型无需重新训练即可实现零样本迁移到其他物种，为跨物种PPI预测提供了统一框架。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有方法在蛋白质-蛋白质相互作用(PPI)预测方面取得了一定进展，但在处理稀疏或不平衡的多物种数据时仍面临挑战。此外，现有方法在跨物种预测和零样本迁移能力方面表现有限。因此，需要一种能够整合蛋白质序列信息、功能层次关系以及跨物种知识的新方法，以提高PPI预测的准确性和泛化能力。

**方法:** 提出了一种名为HIPPO的分层对比框架，用于跨物种PPI预测。该框架包含以下关键步骤：1. 通过多层次的生物表示匹配对齐蛋白质序列及其分层属性；2. 引入分层对比损失函数，模拟蛋白质功能类别的结构化关系；3. 通过数据驱动的惩罚机制，将领域和家族知识融入模型，确保学习到的嵌入空间与蛋白质功能的内在层次结构一致。

**结果:** HIPPO在多个基准数据集上显著优于现有方法，特别是在低数据量情况下表现出较强的鲁棒性。此外，模型展示了强大的零样本迁移到其他物种的能力，即使在实验数据有限的较少研究或稀有物种中也能进行可靠的PPI预测和功能推断。进一步分析表明，分层特征融合对于捕捉保守的相互作用决定因素（如结合基序和功能注释）至关重要。

**结论:** HIPPO提供了一个统一的框架，用于跨物种PPI预测，特别适用于稀疏或不平衡的多物种数据场景。这一工作不仅提高了PPI预测的准确性，还增强了模型在不同物种间的泛化能力，为生物信息学领域的研究提供了新工具和思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Multi-Label+Contrastive+Learning+for+Protein-Protein+Interaction+Prediction+Across+Organisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02724&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [59] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz, Albert Gu*

**主要类别:** cs.LG

**AI概要:** 近期，具有线性复杂度的递归模型（如状态空间模型和线性注意力机制）因其在序列长度上的高效性而受到关注。尽管理论上能够处理任意长的序列，但这些模型在超出训练上下文长度时性能可能会显著下降。本文通过实证与理论分析支持了“未探索状态假设”，即模型无法泛化到更长序列的原因在于训练过程中仅接触到所有可达状态的一个有限子集。研究还探讨了简单的训练干预措施（如用高斯噪声初始化状态或使用不同输入序列的最终状态），以增加模型训练时的状态覆盖范围。实验表明，在少量微调步骤（约预训练预算的0.1%）下，这些干预措施能有效提升模型对远超训练长度序列的泛化能力（如从2k扩展到128k），并在长上下文任务中表现出更优性能。


<details>
  <summary>更多</summary>
  
**动机:** 递归模型（如状态空间模型和线性注意力机制）虽然理论上可以处理任意长的序列，但在实际应用中，当处理超出训练上下文长度的序列时，性能会显著下降。为了解决这一问题，作者希望探讨模型为何无法泛化到更长的序列，并寻找简单有效的解决方案来提高其长度泛化能力。

**方法:** 1. 提出并验证“未探索状态假设”：模型在训练中仅接触到有限的状态子集，导致无法泛化到更长序列。
2. 探讨简单的训练干预措施，包括：
   - 使用高斯噪声初始化状态。
   - 使用不同输入序列的最终状态进行初始化。
3. 在少量微调步骤下评估这些干预措施对模型泛化能力的影响。
4. 测试模型在超出训练长度序列上的表现（如从2k扩展到128k）。

**结果:** 通过简单的训练干预措施，模型能够在极少量的微调步骤后（约预训练预算的0.1%），成功泛化到远超训练长度的序列（如从2k扩展到128k），并在长上下文任务中表现出更好的性能。这表明干预措施显著提升了模型的长度泛化能力。

**结论:** 本文通过实证和理论分析支持了“未探索状态假设”，并证明了简单的训练干预措施能够有效提高递归模型的长度泛化能力。这些方法不仅高效且易于实现，为解决递归模型在长序列任务中的性能下降问题提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+and+Improving+Length+Generalization+in+Recurrent+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02782&send_immediately=true&force_search=false)

**原文摘要:** Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [60] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket, Stanley Kok*

**主要类别:** cs.LG

**AI概要:** GRADUATE 是一种新的生存分析模型，通过约束优化方法确保整体及各子群体的校准性，并在真实临床数据集上表现出优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的生存模型通常仅在总体水平上进行校准，可能导致对少数子群体的校准不佳，从而引发错误的临床决策。因此，需要一种能够实现多校准（multicalibration）的模型，以确保所有子群体都能获得良好的校准效果。

**方法:** GRADUATE 将多校准问题定义为一个约束优化问题，在训练过程中同时优化校准性和辨别力，以达到两者的良好平衡。该方法还从数学上证明了其优化方法能以高概率产生接近最优且可行的解决方案。

**结果:** 在真实世界临床数据集上的实证比较显示，GRADUATE 模型相较于现有最先进基线模型具有更高的有效性。详细分析进一步揭示了基线模型的不足以及 GRADUATE 的优势。

**结论:** GRADUATE 提供了一种有效的解决方案，可以在保证整体校准的同时，确保各子群体的校准性，从而减少临床决策中的误差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是In-Training+Multicalibrated+Survival+Analysis+for+Healthcare+via+Constrained+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02807&send_immediately=true&force_search=false)

**原文摘要:** Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [61] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas, Jingyi Gao, Daniel Kane, Sihan Liu, Christopher Ye*

**主要类别:** cs.LG

**AI概要:** 本文研究了在算法可重复性框架下的分布测试，开发了新的可重复算法，并提出了用于证明样本复杂度下界的全新方法。


<details>
  <summary>更多</summary>
  
**动机:** 目前对于分布测试的研究较少涉及算法可重复性，因此需要系统地研究如何在可重复性框架下进行分布测试，并确定样本复杂度。

**方法:** 作者提出了新的可重复算法来测试离散分布的接近性和独立性，并开发了一种新的方法论以证明可重复测试的样本复杂度下界。

**结果:** 得到了可重复一致性测试的近乎最优的样本复杂度下界，并解决了先前工作中的一个开放问题。此外，还获得了接近性测试的成果。

**结论:** 该研究为分布测试在算法可重复性框架下的发展提供了新思路，特别是对于可重复测试的样本复杂度分析具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Replicable+Distribution+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02814&send_immediately=true&force_search=false)

**原文摘要:** We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [62] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou, Shuozhe Li, Amy Zhang, Liu Leqi*

**主要类别:** cs.LG

**AI概要:** 近期大型语言模型的进展主要由强化学习（RL）风格的后训练驱动，但传统方法在早期RL训练和复杂推理任务上存在局限性。本文提出了一种新的框架ExPO，通过基于正确答案生成有效样本，提升模型探索能力和推理轨迹质量，在推理基准测试中表现出更高的学习效率和最终性能，尤其在如MATH level-5等困难场景下超越了基于专家示范的方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于强化学习的大型语言模型训练方法依赖于模型初始生成正样本的能力，主要优化模型已知内容，而非解决其无法处理的问题。在早期RL训练和复杂推理任务中，由于难以生成正样本，这种方法效果不佳。因此需要一种新方法，使模型能够探索超出其当前输出分布的新推理路径，并有效利用足够好的正样本进行学习。

**方法:** 提出了Self-Explanation Policy Optimization (ExPO)，一个简单且模块化的框架。该方法通过条件化真实答案生成样本，这些样本具备两个关键特性：在当前策略下具有较高概率，并能提高模型预测正确答案的可能性。ExPO促进有效探索，引导模型生成更符合其策略的推理轨迹，同时确保比自身错误样本更高的质量。

**结果:** 实验表明，ExPO提高了推理基准测试中的学习效率和最终性能，尤其是在模型最初最挣扎的困难场景（如MATH level-5）中，超过了基于专家示范的方法。

**结论:** ExPO为强化学习训练提供了一种有效的新途径，解决了传统方法在生成正样本方面的局限性，特别是在复杂推理任务中表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ExPO%3A+Unlocking+Hard+Reasoning+with+Self-Explanation-Guided+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02834&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [63] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel*

**主要类别:** cs.LG

**AI概要:** 本文研究了在个性化医疗决策中估计治疗效果的问题，提出了一种新框架来解决训练和推理阶段数据差异导致的偏差问题。


<details>
  <summary>更多</summary>
  
**动机:** 在临床实践中，估计治疗效果对于个性化医疗决策至关重要，但在推理阶段通常使用文本描述而非完整的患者信息，这可能导致偏差。

**方法:** 作者提出了一个新框架，结合大型语言模型和定制的双重稳健学习器，以减少推理时间文本混淆带来的偏差。

**结果:** 通过一系列实验验证了该框架在实际应用中的有效性。

**结论:** 该框架能够有效缓解由于推理阶段文本信息不完整而导致的治疗效果估计偏差问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Driven+Treatment+Effect+Estimation+Under+Inference+Time+Text+Confounding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02843&send_immediately=true&force_search=false)

**原文摘要:** Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [64] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang, Qiang Li, Shujian Yu*

**主要类别:** cs.LG

**AI概要:** 提出MvHo-IB框架，结合高阶交互和信息瓶颈方法提升fMRI数据诊断准确性，实现最优性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前研究显示在fMRI数据中建模高阶交互作用可提高机器学习系统的诊断准确率，但有效提取和利用这些交互作用仍具挑战性。

**方法:** 提出MvHo-IB多视图学习框架，整合成对交互和高阶交互，并自动压缩任务无关冗余信息。关键创新包括：(1)结合O-information与矩阵基Renyi alpha-order熵估计量量化提取高阶交互；(2)设计Brain3DCNN编码器有效利用这些交互；(3)引入新的多视图学习信息瓶颈目标以增强表示学习。

**结果:** 在三个基准fMRI数据集上的实验表明，MvHo-IB达到最先进的性能，显著优于先前的方法，包括最近的超图技术。

**结论:** MvHo-IB框架在fMRI数据分析中展现出优越性能，为更精确的诊断决策提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MvHo-IB%3A+Multi-View+Higher-Order+Information+Bottleneck+for+Brain+Disorder+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02847&send_immediately=true&force_search=false)

**原文摘要:** Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin, Zaixi Zhang, Mengdi Wang, Le Cong*

**主要类别:** cs.AI

**AI概要:** STELLA是一个自我进化的AI代理，通过动态扩展工具集和策略库来自我改进，实现在生物医学基准测试中达到先进水平的准确率，并且其性能会随着经验系统性提高。


<details>
  <summary>更多</summary>
  
**动机:** 生物医学数据、工具和文献的快速增长导致了研究领域的碎片化，超出了人类专家的能力范围。现有的AI代理通常依赖于静态的手动整理工具集，限制了其适应性和扩展能力。

**方法:** STELLA采用多代理架构，通过两个核心机制自主提升能力：1）可演变的推理策略模板库；2）动态扩展的生物信息学工具海洋，由工具创建代理自动发现并整合新工具。

**结果:** STELLA在一系列生物医学基准测试中达到了最先进的准确率，在'Humanity's Last Exam: Biomedicine'中得分为26%，LAB-Bench: DBQA中为54%，LAB-Bench: LitQA中为63%，比领先模型高出多达6个百分点。更重要的是，其性能会随着经验系统性提高，例如在'Humanity's Last Exam'基准测试中的准确率几乎随试验次数增加而翻倍。

**结论:** STELLA代表了向能够学习和成长的AI代理系统迈进的重要一步，可以动态扩展其专业知识，加速生物医学发现的步伐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STELLA%3A+Self-Evolving+LLM+Agent+for+Biomedical+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02004&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [66] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar, Rushikesh K. Joshi*

**主要类别:** cs.AI

**AI概要:** The paper proposes HCVR, a lightweight rule-based feature selection method combining P2P and P2T correlations to eliminate redundant features. It uses voting rules and backward elimination to decide which features to keep or discard based on correlation thresholds. Applied to the SPAMBASE dataset, HCVR showed improved performance compared to traditional methods.


<details>
  <summary>更多</summary>
  
**动机:** To develop an efficient feature selection method that eliminates redundant features while retaining relevant ones, improving upon traditional non-iterative and iterative filtering approaches.

**方法:** HCVR combines Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations. It is a hybrid of non-iterative and iterative filtering approaches, using a greedy backward elimination strategy. Voting rules with correlation thresholds determine which features to keep or discard through majority voting.

**结果:** HCVR outperformed traditional non-iterative (CFS, mRMR, MI) and iterative (RFE, SFS, Genetic Algorithm) techniques when applied to the SPAMBASE dataset. The effectiveness was demonstrated by improved classifier performance after applying the filtering method.

**结论:** HCVR is an effective feature selection method that improves performance over traditional techniques. Its combination of P2P and P2T correlations, along with its voting rules and backward elimination strategy, make it a promising approach for dimensionality reduction.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HCVR%3A+A+Hybrid+Approach+with+Correlation-aware+Voting+Rules+for+Feature+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02073&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [67] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani, Yingxue Zhang, Derek Li, Qianyi Sun, Soumyasundar Pal, Zhanguang Zhang, Yaochen Hu, Rohan Deepak Ajwani, Antonios Valkanas, Raika Karimi, Peng Cheng, Yunzhou Wang, Pengyi Liao, Hanrui Huang, Bin Wang, Jianye Hao, Mark Coates*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在解决广泛任务方面表现出色，但在推理效率上仍有不足。本文综述了高效的测试时计算（TTC）策略，通过L1-可控性和L2-适应性两种方法提高LLM推理的计算效率，并对主流闭源LLM进行了基准测试，强调了实际控制、适应性和可扩展性，同时探讨了未来发展方向。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型尽管功能强大，但在推理过程中存在计算资源分配不合理的问题：简单任务被过度思考，复杂任务却被轻视。为了解决这一问题，需要探索更高效的推理方法以优化计算资源的使用。

**方法:** 文章提出了一个双层分类法来区分不同的高效推理策略：L1-可控性方法是在固定计算预算下操作，而L2-适应性方法则根据输入难度或模型置信度动态调整推理规模。此外，还对领先的专有LLM进行了跨数据集的基准测试。

**结果:** 研究表明，不同TTC方法在推理性能和标记使用之间存在关键权衡。相比以往的研究，本综述更加注重方法的实际控制能力、适应性和可扩展性。

**结论:** 未来的LLM研究应关注计算效率、鲁棒性和用户约束响应等方面的发展，例如混合思维模型等新兴趋势将是重要方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+on+a+Budget%3A+A+Survey+of+Adaptive+and+Controllable+Test-Time+Compute+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02076，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02076&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [68] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan, Stephen Zhewen Lu, Caitlin Fiona Harrigan, Nishkrit Desai, Jiarui Lu, Michał Koziarski, Leonardo Cotta, Chris J. Maddison*

**主要类别:** cs.AI

**AI概要:** 设计实验和结果解释是生物学研究的核心能力。为了评估大型语言模型（LLMs）的科学能力，本文提出了SciGym基准测试，通过在开放式的科学发现任务中评估LLMs迭代实验设计和分析的能力，克服了传统湿实验室高昂成本的问题。结果显示，尽管更强大的模型表现更好，但所有模型在系统复杂性增加时性能显著下降。


<details>
  <summary>更多</summary>
  
**动机:** 当前对大型语言模型科学能力的评估未能充分测试其设计实验和解释结果的能力，因为湿实验室实验成本高、耗时长且需要专业知识。

**方法:** 引入了SciGym这一新基准，使用系统生物学标记语言编码的生物系统模型进行干实验室模拟，生成仿真数据，以评估六个前沿LLMs在137个小系统上的实验设计与分析能力，并发布了总计350个系统。

**结果:** 评估显示，尽管更强大的模型表现出更好的性能，但随着系统复杂性的增加，所有模型的性能都显著下降。

**结论:** 大型语言模型在科学能力方面还有很大的改进空间，特别是在处理复杂系统时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Scientific+Capabilities+of+Language+Models+with+a+Systems+Biology+Dry+Lab，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02083&send_immediately=true&force_search=false)

**原文摘要:** Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [69] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

**主要类别:** cs.AI

**AI概要:** 通过范畴论重新构建机器学习模型，以提高AI系统的可解释性和可理解性。本文聚焦于监督学习中的多元线性回归模型，提出了高斯-马尔科夫伴随（Gauss-Markov Adjunction）作为其核心框架，并探讨了参数与残差之间的信息对偶流动。该研究为AI的可解释性提供了一种形式化语义基础。


<details>
  <summary>更多</summary>
  
**动机:** 提升机器学习模型的可理解性和可解释性，以满足AI原则中的可解释性需求，并促进AI在社会中的更好应用。

**方法:** 通过范畴论对机器学习模型进行重构，具体针对监督学习中的多元线性回归模型，定义了与参数和数据对应的两个具体范畴以及它们之间的伴随函子对，提出高斯-马尔科夫伴随框架来描述参数与残差之间的结构关系。

**结果:** 阐明并形式化了监督学习中参数与残差之间的结构相互作用，展示了普通最小二乘估计器与最小残差之间的关系，并将此框架定位为监督学习扩展表示语义的一个实例。

**结论:** 范畴论提供了一种形式化的语义框架，有助于理解和改进AI系统的可解释性，建议采用理论计算机科学中的语义视角作为AI可解释性的正式基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Gauss-Markov+Adjunction%3A+Categorical+Semantics+of+Residuals+in+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02442&send_immediately=true&force_search=false)

**原文摘要:** Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [70] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz, Bruno Averbeck, Georgia Koppe*

**主要类别:** cs.AI

**AI概要:** 本论文探讨了AI如何从神经科学中学习，特别是关于持续学习和情境学习，以适应不断变化的环境。它强调了将神经科学的见解应用于AI以及AI对神经科学的反向影响的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 现代AI模型训练成本高、速度慢且适应性差，而动物能够快速适应环境的变化。这种能力对于在现实世界中运行的AI系统（如机器人或自动驾驶汽车）至关重要。

**方法:** 整合AI领域的持续学习和情境学习文献与神经科学中关于行为任务的学习研究，这些任务涉及规则、奖励概率或结果的变化。

**结果:** 提出了一个议程，说明神经科学如何为当前AI的发展提供信息，反之亦然，AI如何也能为神经科学提供新的见解。

**结论:** 通过结合神经科学和AI的研究，可以推动NeuroAI领域的发展，使AI系统更有效地适应动态环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Neuroscience+Can+Teach+AI+About+Learning+in+Continuously+Changing+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02103，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02103&send_immediately=true&force_search=false)

**原文摘要:** Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [71] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola, Patrick Button, Aron Culotta, Nicholas Mattei*

**主要类别:** cs.AI

**AI概要:** 本文探讨了如何使用审计研究数据来改进训练和评估自动化招聘算法的能力，发现常见的公平性干预方法在传统度量标准下看似实现了平等，但实际上仍存在约10%的差异，并引入了基于个体处理效应估计方法的干预措施以进一步减少算法歧视。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能系统，特别是机器学习系统，在招聘、贷款发放等领域得到应用，研究这些AI系统的有效性和公平性变得复杂且重要。目前，解决下游分类器中偏差的一种常见方法是对训练数据进行重采样，但这些方法通常仅使用方便样本的数据进行评估，引入了选择偏差和标签偏差。

**方法:** 研究利用审计研究中的数据（如虚构的“测试者”发送到随机控制试验中的主体），以改进训练和评估自动化招聘算法的能力。通过比较传统度量标准与适当度量下的结果，揭示了常见公平性干预方法的实际效果，并引入了基于个体处理效应估计方法的新干预措施。

**结果:** 研究发现，常见的公平性干预方法在传统度量标准下看似实现了平等，但在适当度量下仍存在约10%的差异。新引入的基于个体处理效应估计方法的干预措施进一步减少了算法歧视。

**结论:** 审计研究数据可以显著改善对自动化招聘算法的训练和评估能力，揭示现有公平性干预方法的不足，并提供更有效的解决方案以减少算法歧视。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Illusion+of+Fairness%3A+Auditing+Fairness+Interventions+with+Audit+Studies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02152&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [72] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci, Qingyang Wu, Ben Athiwaratkun, Ce Zhang, Shuaiwen Leon Song, James Zou*

**主要类别:** cs.AI

**AI概要:** 最近在偏好学习方面的进展提高了人类反馈的一致性，但数学推理仍然是一个持续的挑战。本文研究了偏好优化中数据多样化策略如何提高大型语言模型（LLMs）的数学推理能力。评估了三种常见的数据生成方法：温度采样、连贯性提示和蒙特卡洛树搜索（MCTS），并引入了一种新的结构化方法DTS，该方法系统地将问题分解为多样化的推理路径。结果显示，通过战略性地多样化偏好数据，模型可以显著提高数学推理性能，其中最佳方法在GSM8K上比基础模型提高了7.1%，在MATH上提高了4.2%。尽管表现强劲，DTS相比基线仅增加了1.03倍的计算开销，而MCTS的成本几乎是其五倍且收益较低。这些发现表明，结构化探索多样化的解决问题的方法比传统方法更能生成有效的偏好数据以实现数学一致性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管偏好学习在人类反馈方面取得了一些进步，但数学推理仍然是一个挑战。为了提高LLMs在数学推理方面的能力，需要探索数据多样化策略的作用。

**方法:** 评估了三种常见数据生成方法（温度采样、连贯性提示和蒙特卡洛树搜索），并提出了一种新方法DTS，它通过系统地将问题分解为多样化的推理路径来改进模型的数学推理能力。

**结果:** 使用DTS方法后，模型在GSM8K和MATH上的表现分别提高了7.1%和4.2%。同时，DTS的计算开销仅比基线高1.03倍，而MCTS的计算成本是其五倍但收益较低。

**结论:** 结构化探索多样化的解决问题的方法能够生成更有效的偏好数据，从而提高数学推理能力，且DTS是一种高效的新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Diversification+Methods+In+Alignment+Enhance+Math+Performance+In+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02173&send_immediately=true&force_search=false)

**原文摘要:** While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [73] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote, Adam Davies, Guohao Li, Kristy Elizabeth Boyer, ChengXiang Zhai, Bonnie J Dorr, Francesco Pinto*

**主要类别:** cs.AI

**AI概要:** 随着大语言模型（LLMs）在生成人类行为研究合成数据中的应用日益广泛，确保其输出与其分配角色一致变得至关重要。本文通过建立评估框架，研究了LLM角色扮演代理的陈述信念与实际行为的一致性，并提出了一个信念-行为一致性度量标准，揭示了LLM在模拟中存在系统性的不一致问题。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型作为角色扮演代理生成合成数据时，需要确保其输出与分配角色保持一致，这促使研究者探索这些代理的陈述信念与实际行为之间的对应关系。

**方法:** 研究者构建了一个评估框架，使用增强版GenAgents人物库和信任游戏（Trust Game），引入信念-行为一致性度量标准，系统分析了影响一致性的多种因素，包括从LLM获取的信念类型、呈现信息的时间和方式以及预测未来行为的深度等。

**结果:** 研究发现，LLM的陈述信念或强加信念与其角色扮演模拟结果之间存在系统性不一致，即使模型似乎编码了合理的信念，也可能无法以一致的方式应用它们。

**结论:** 研究强调了识别LLM陈述信念与模拟行为何时对齐的重要性，以便研究者能够适当地使用基于LLM的代理进行行为研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Role-Playing+Agents+Practice+What+They+Preach%3F+Belief-Behavior+Consistency+in+LLM-Based+Simulations+of+Human+Trust，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02197&send_immediately=true&force_search=false)

**原文摘要:** As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [74] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold, Heitor C. M. Fernandes, Mendeli H. Vainstein*

**主要类别:** cs.AI

**AI概要:** 近期研究显示，在空间囚徒困境博弈中，静态代理可以通过多种机制（如噪声注入、不同类型的算法和邻居收益知识）学习合作。本文使用独立多代理Q学习算法，研究稀释和移动性在空间囚徒困境中的影响。定义了算法的不同可能动作，与经典的空间囚徒困境结果相连接，展示了算法的多样性和该方法的基准潜力。结果表明，具有固定更新规则的游戏可以定性等同于具有学习规则的游戏，并且当定义多个动作时，种群之间会出现共生互惠效应。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望理解在空间囚徒困境博弈中，通过引入稀释和移动性，代理如何学习合作，并探索强化学习算法在这种情境下的适用性和表现。

**方法:** 使用独立多代理Q学习算法进行实验，定义不同的动作以模拟空间囚徒困境的不同场景，结合稀释和移动性因素来观察代理的行为变化。

**结果:** 发现固定更新规则的游戏可以与学习规则的游戏定性等同；同时，当定义多个动作时，种群间会出现共生互惠效应。

**结论:** 独立多代理Q学习算法在模拟空间囚徒困境时表现出高度的灵活性和适应性，能够揭示复杂的社会互动模式，包括合作行为和种群间的互惠关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dilution%2C+Diffusion+and+Symbiosis+in+Spatial+Prisoner%27s+Dilemma+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02211&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [75] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

**主要类别:** cs.AI

**AI概要:** 提升大型语言模型（LLM）规划和推理能力的进展，受到可扩展、可靠的数据生成与评估瓶颈的显著阻碍。为解决此问题，本文介绍了NL2FLOW，一个可以参数化生成自然语言表达的规划问题、结构化中间表示以及正式PDDL，并严格评估生成计划质量的全自动系统。通过生成包含2296个问题的数据集并评估多个开源、指令调优的LLM，结果表明最高性能模型在生成有效计划方面成功率达到86%，而生成最优计划的成功率为69%（针对具有可行解的问题）。回归分析显示，问题特征对计划生成的影响取决于模型设计和提示方式。值得注意的是，将自然语言翻译为计划的JSON表示的成功率低于直接生成有效计划的成功率，这表明不必要地分解推理任务（引入中间翻译步骤）可能会降低性能，暗示直接从自然语言到行动进行推理的模型可能更有优势。随着LLM推理扩展到越来越复杂的问题，这些系统内的瓶颈和错误来源将不可避免地发生变化。因此，对这些限制的动态理解及系统揭示这些限制的工具将是解锁LLM作为智能问题求解器全部潜力的关键。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型（LLM）在规划和推理能力上的进展受到可扩展、可靠的数据生成和评估的瓶颈限制。需要一种新的方法来克服这一障碍，以促进更高效和准确的规划问题生成与评估。

**方法:** 提出了一种名为NL2FLOW的全自动系统，该系统能够参数化生成规划问题，这些问题可以用自然语言、结构化中间表示以及正式的PDDL（Planning Domain Definition Language）来表达。此外，NL2FLOW还提供了对生成计划的质量进行严格评估的功能。作者使用NL2FLOW生成了一个包含2296个问题的数据集，并利用这些数据集评估了多个开源、经过指令调优的LLM的表现。

**结果:** 实验结果显示，表现最好的模型在生成有效计划方面的成功率为86%，而在生成最优计划方面的成功率为69%（仅限于有可行解的问题）。回归分析表明，问题特征对计划生成的影响依赖于模型和提示的设计。特别指出的是，将自然语言转换为计划的JSON表示的成功率低于直接生成有效计划的成功率，这表明不必要的任务分解可能会降低性能。

**结论:** 研究发现，直接从自然语言到行动进行推理的模型可能比那些需要中间翻译步骤的模型更具优势。随着LLM处理更复杂问题的能力增强，系统中的瓶颈和错误来源将会发生变化。因此，理解这些限制的动态特性，并拥有系统性揭示这些限制的工具，对于充分挖掘LLM作为智能问题求解器的潜力至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+LLM+Planning%3A+NL2FLOW+for+Parametric+Problem+Generation+and+Rigorous+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02253&send_immediately=true&force_search=false)

**原文摘要:** Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [76] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

**主要类别:** cs.AI

**AI概要:** 在信念修正领域，虽然有许多新提案，但对现有方法的分析却很少。许多工作基于公理，用作语法特征：某些修正机制等价于某些属性。这些公理约束特定的修正实例。本文探讨了修正机制的能力，而不仅仅是约束，例如可塑性、平等性和教条性等，并证明了几种修正机制各自具备某些能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前信念修正研究多集中于公理和约束，缺乏对修正机制能力的关注。

**方法:** 通过分析不同修正机制（如字典序修正、自然修正等）是否能够达到某些特定的信念状态，如可塑性、平等性和教条性等来评估其能力。

**结果:** 证明了不同的修正机制具有不同的能力，如可塑性、平等性、教条性等。

**结论:** 信念修正不仅应关注约束，还应重视修正机制的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Iterated+belief+revision%3A+from+postulates+to+abilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02319，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02319&send_immediately=true&force_search=false)

**原文摘要:** The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [77] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen, Zhao Wang, Shingo Takamatsu*

**主要类别:** cs.AI

**AI概要:** 在赞助搜索广告中，关键词决策对于广告活动的成功至关重要。虽然基于LLM的方法可以自动生关键词，但它们存在三个主要限制：依赖大规模查询-关键词对数据、缺乏在线多目标性能监控和优化、以及关键词选择中的质量控制薄弱。为了解决这些问题，我们提出了OMS，一个即时（无需训练数据，在线监控性能并适应）、多目标（使用代理推理根据多个性能指标优化关键词）和自省（代理评估关键词质量）的关键词生成框架。实验表明，OMS在基准测试和实际广告活动中优于现有方法；消融研究和人工评估确认了每个组件的有效性和生成关键词的质量。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现有的基于LLM的关键词生成方法能够实现自动化，但其受限于对大规模数据的依赖、缺乏实时多目标性能监控及优化能力，以及关键词选择的质量控制不足，这阻碍了LLM在完全自动化关键词决策中的应用。因此，需要一种新的框架来解决这些限制。

**方法:** 提出了一种名为OMS的关键词生成框架，该框架具有以下特点：1) 即时性：不需要训练数据，可在线监控性能并适应变化；2) 多目标性：通过代理推理优化关键词，考虑多个性能指标（如展示量、点击量、转化率等）；3) 自省性：通过代理评估机制确保关键词质量。

**结果:** 实验结果表明，OMS在基准测试和真实广告活动中的表现优于现有方法。消融实验和人工评估进一步验证了框架中各组件的有效性以及生成关键词的质量。

**结论:** OMS框架解决了现有LLM方法在关键词生成中的关键问题，实现了无需训练数据、实时多目标优化和高质量关键词生成。这一成果为赞助搜索广告中的关键词自动化决策提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OMS%3A+On-the-fly%2C+Multi-Objective%2C+Self-Reflective+Ad+Keyword+Generation+via+LLM+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02353&send_immediately=true&force_search=false)

**原文摘要:** Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [78] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu, Zhaoguo Wang, Jiabin Wang, Zhiyuan Dong, Jingkai Yang, Qingting Li, Tianyu Huang, Lei Zhao, Mingqiang Li, Fei Wang, Chunhai Fan, Haibo Chen*

**主要类别:** cs.AI

**AI概要:** An AI-driven autonomous laboratory is developed for complex scientific experiments like biomolecular engineering. It operates independently, optimizing experiments to match human scientist standards and improving efficiency in multi-user settings.


<details>
  <summary>更多</summary>
  
**动机:** To achieve autonomous scientific research capable of independently conducting complex experiments and serving non-specialists, representing a long-held aspiration that requires a fundamental paradigm shift driven by artificial intelligence (AI).

**方法:** The autonomous laboratory autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. It is founded on a co-design philosophy of models, experiments, and instruments, supporting the co-evolution of AI models and the automation system.

**结果:** The autonomous laboratory successfully supports fundamental nucleic acid functions, including synthesis, transcription, amplification, and sequencing, as well as applications in disease diagnostics, drug development, and information storage. Without human intervention, it optimizes experimental performance to match state-of-the-art results achieved by human scientists.

**结论:** An AI-native autonomous laboratory has been presented that can handle complex, multi-objective experiments across diverse instrumentation without human intervention. It matches state-of-the-art results achieved by human scientists and significantly improves instrument utilization and experimental efficiency in multi-user scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+AI-native+experimental+laboratory+for+autonomous+biomolecular+engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02379&send_immediately=true&force_search=false)

**原文摘要:** Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [79] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao*

**主要类别:** cs.AI

**AI概要:** 本文研究了通过提高任务清晰度是否可以增强大型语言模型的推理能力，特别是在Coq中的定理证明。通过引入概念级指标来评估任务清晰度，并展示了在现代LLM的标准输入中添加结构化语义上下文如何将清晰度得分提高了1.85倍（44.5%至82.3%）。使用通用模型DeepSeek-V3，该方法使证明成功率提升了2.1倍（21.8%至45.8%），并超越了先前的最先进模型Graph2Tac（33.2%）。通过对15个标准Coq包中随机抽取的1,386个定理进行评估，结果遵循与Graph2Tac相同的评估协议。此外，在结构化数据上微调较小模型可实现更高性能（48.6%）。本文提出的方法利用选择性概念展开以丰富任务描述，并采用Planner--Executor架构。这些发现突显了结构化任务表示在弥合理解和推理之间差距中的价值。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在复杂任务如Coq定理证明中的表现受到任务清晰度不足的限制。为了提升其推理能力，需要找到一种有效方式改进任务描述和输入形式。

**方法:** 1. 引入概念级指标来量化任务清晰度。
2. 在现代LLM的标准输入中加入结构化语义上下文。
3. 使用通用模型DeepSeek-V3进行实验，并采用Planner-Executor架构。
4. 对小型模型进行微调以进一步提升性能。
5. 在15个标准Coq包中的1,386个定理上进行评估，遵循与Graph2Tac相同的评估协议。

**结果:** 1. 结构化语义上下文使任务清晰度得分提升了1.85倍（从44.5%到82.3%）。
2. DeepSeek-V3模型的证明成功率提升了2.1倍（从21.8%到45.8%），超越了Graph2Tac的33.2%。
3. 微调的小型模型达到了48.6%的成功率。
4. 证明了结构化任务表示对推理能力的显著提升作用。

**结论:** 结构化任务表示能够显著提高大型语言模型在复杂任务中的推理能力，特别是在Coq定理证明领域。通过改进任务清晰度和采用适当的架构设计，可以有效弥合理解和推理之间的差距。这为未来开发更高效的推理模型提供了重要启示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clarifying+Before+Reasoning%3A+A+Coq+Prover+with+Structural+Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02541&send_immediately=true&force_search=false)

**原文摘要:** In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [80] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach*

**主要类别:** cs.AI

**AI概要:** AI研究代理通过自动化设计、实现和训练机器学习模型，展现出加速科学进步的巨大潜力。本文聚焦于提升代理在MLE-bench上的表现，该基准挑战中，代理参与Kaggle竞赛以解决实际的机器学习问题。我们将AI研究代理形式化为搜索策略，其在候选解空间中导航，并使用操作符迭代修改它们。通过设计并系统地变化不同的操作符集和搜索策略（贪婪、蒙特卡洛树搜索、进化算法），我们展示了它们之间的相互作用对于实现高性能至关重要。我们最佳的搜索策略与操作符集配对在MLE-bench lite上达到了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。我们的研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作符设计和评估方法的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升AI研究代理在解决实际机器学习问题中的表现，特别是在MLE-bench基准挑战中的竞争力，需要深入理解搜索策略与操作符设计之间的关系及其对性能的影响。

**方法:** 1. 将AI研究代理视为搜索策略，定义其在候选解空间中的导航方式。
2. 设计多种操作符集以及搜索策略（如贪婪、蒙特卡洛树搜索、进化算法）。
3. 系统地变化这些操作符集和搜索策略，分析它们之间的相互作用对性能的影响。
4. 在MLE-bench lite上测试不同组合的表现，确定最佳配对。

**结果:** 通过优化搜索策略与操作符集的配对，成功将获得Kaggle奖牌的成功率从39.6%提升至47.7%，实现了当前最先进的结果。

**结论:** 在推进自动化机器学习领域时，必须综合考虑搜索策略、操作符设计以及评估方法，它们之间的协同作用是提升AI研究代理性能的关键。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Research+Agents+for+Machine+Learning%3A+Search%2C+Exploration%2C+and+Generalization+in+MLE-bench，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02554&send_immediately=true&force_search=false)

**原文摘要:** AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [81] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang, Pavel Naumov*

**主要类别:** cs.AI

**AI概要:** 本文研究了集体决策中责任的两个重要属性：扩散和缺口的计算复杂性，证明了无扩散和无缺口决策机制的集合分别是$\Pi_2$-完全和$\Pi_3$-完全，同时这些类别的交集是$\Pi_2$-完全。


<details>
  <summary>更多</summary>
  
**动机:** 责任一直是法律和哲学的研究主题，最近也成为AI领域的研究焦点。本文旨在探讨集体决策中责任的两个重要属性：扩散和缺口的计算复杂性。

**方法:** 通过分析集体决策中的扩散和缺口属性，证明了无扩散决策机制的集合为$\Pi_2$-完全，无缺口决策机制的集合为$\Pi_3$-完全，并且两者的交集为$\Pi_2$-完全。

**结果:** 证明了无扩散决策机制的集合为$\Pi_2$-完全，无缺口决策机制的集合为$\Pi_3$-完全，同时这些类别的交集是$\Pi_2$-完全。

**结论:** 集体决策中责任的扩散和缺口属性具有较高的计算复杂性，分别属于不同的完全性类别。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Responsibility+Gap+and+Diffusion+in+Sequential+Decision-Making+Mechanisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02582&send_immediately=true&force_search=false)

**原文摘要:** Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [82] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao*

**主要类别:** cs.AI

**AI概要:** 本研究介绍了一个名为MIMIC-Patient的数据集，并提出了DynamiCare框架，用于模拟临床诊断的多轮交互过程，展示了其可行性和有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的医疗决策模拟框架大多关注单轮任务，与现实世界中不确定、互动和迭代的诊断过程不符。

**方法:** 1. 构建了MIMIC-Patient数据集，基于MIMIC-III电子健康记录，支持动态患者级模拟。
2. 提出了DynamiCare框架，将临床诊断建模为多轮交互循环，包含一组专家代理，它们逐步查询患者系统、整合新信息并动态调整组成和策略。

**结果:** 通过广泛的实验，证明了DynamiCare框架在动态临床决策中的可行性和有效性，并建立了首个相关基准。

**结论:** DynamiCare框架能够更真实地模拟临床诊断过程，为未来的研究提供了一个新的方向和基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynamiCare%3A+A+Dynamic+Multi-Agent+Framework+for+Interactive+and+Open-Ended+Medical+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02616&send_immediately=true&force_search=false)

**原文摘要:** The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [83] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne, Baptiste Alloui-Cros*

**主要类别:** cs.AI

**AI概要:** 本研究通过在迭代囚徒困境（IPD）中与经典策略竞争，展示了大语言模型（LLMs）作为战略性智能的能力。结果表明LLMs具有竞争力，并表现出独特的战略特征：Gemini模型无情、OpenAI模型高度合作、Claude模型则展现出宽恕的互惠性。这些模型根据时间范围和对手策略进行推理，为算法在不确定性下的决策提供了深刻的见解。


<details>
  <summary>更多</summary>
  
**动机:** 探讨大语言模型是否能在竞争环境中作为一种战略性智能存在，并通过迭代囚徒困境测试其决策能力。

**方法:** 在迭代囚徒困境中组织了一系列进化锦标赛，让来自OpenAI、Google和Anthropic的LLMs与经典策略（如以牙还牙、冷酷触发等）竞争。通过改变终止概率引入复杂性和偶然性，防止记忆化。分析了近32,000个模型提供的散文理由，揭示其推理过程。

**结果:** LLMs表现出高度竞争力，在复杂生态系统中能够生存甚至繁衍。不同公司的模型展现出独特且持久的战略特征：Gemini模型无情、OpenAI模型合作、Claude模型宽恕。这些模型能够主动对时间范围和对手可能的策略进行推理。

**结论:** LLMs具备战略性智能，能够在竞争环境中作出有效决策。此研究将经典博弈论与机器心理学相结合，为不确定性下的算法决策提供了细致的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+Intelligence+in+Large+Language+Models%3A+Evidence+from+evolutionary+Game+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02618&send_immediately=true&force_search=false)

**原文摘要:** Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [84] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou*

**主要类别:** cs.AI

**AI概要:** 在复杂的现实搜索场景中，传统的检索增强生成（RAG）管道难以有效应对深度推理和知识综合的需求。当前基于推理的方法存在根本限制：使用单一模型处理高层规划和详细执行，导致推理效率低下且可扩展性有限。本文提出HiRA，一种分层框架，将战略规划与专业化执行分离。该方法将复杂搜索任务分解为专注的子任务，分配给具有外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。实验表明，HiRA在四个复杂的跨模态深度搜索基准上显著优于最先进的RAG和基于代理的系统，在答案质量和系统效率方面均有改进。


<details>
  <summary>更多</summary>
  
**动机:** 复杂的现实世界搜索场景需要深度推理和跨源知识综合，而传统RAG管道和当前基于推理的方法难以有效满足这一需求。这些方法通常使用单一模型处理高层规划和详细执行，导致推理效率低下且可扩展性有限。

**方法:** 提出了HiRA，一种分层框架，将复杂搜索任务分解为专注的子任务，分配给领域特定代理进行专业化执行，同时通过结构化集成机制协调结果。这种方法分离了战略规划和详细执行，防止执行细节干扰高层推理，同时利用专业领域的专长进行不同类型的信息处理。

**结果:** 在四个复杂的跨模态深度搜索基准上的实验表明，HiRA显著优于最先进的RAG和基于代理的系统，提高了答案质量和系统效率。

**结论:** 分层分离规划和执行的HiRA框架在多步骤信息寻求任务中表现出色，提升了答案质量和系统效率，展示了其在复杂搜索任务中的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decoupled+Planning+and+Execution%3A+A+Hierarchical+Reasoning+Framework+for+Deep+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02652&send_immediately=true&force_search=false)

**原文摘要:** Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [85] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种基于代理AI的方法来进行硬件设计验证，结合人类在循环（HITL）干预，实现了端到端的硬件设计与验证。该方法在五个开源设计上进行了评估，达到了超过95%的覆盖率，并减少了验证时间，同时展示了优越的性能、适应性和可配置性。


<details>
  <summary>更多</summary>
  
**动机:** 现代集成电路（ICs）变得越来越复杂，其开发过程也随之变得更加复杂。硬件设计验证是一个繁琐的过程，需要大量的努力和时间来确保无错误的设计。而大型语言模型（LLMs）和生成式AI（GenAI）在自然语言处理领域带来了重大变革，为包括硬件设计验证在内的多种应用提供了新的可能性。

**方法:** 论文采用了一种基于代理AI的方法进行硬件设计验证，其中AI代理与人类在循环（HITL）干预相结合，进行更动态、迭代和自我反思的过程，从而实现端到端的硬件设计和验证。

**结果:** 该方法在五个开源设计上进行了测试，实现了超过95%的覆盖率，同时减少了验证时间，并表现出卓越的性能、适应性和可配置性。

**结论:** 基于代理AI的方法可以显著提高硬件设计验证的效率和效果，减少所需时间和努力，同时保持高覆盖率和灵活性，为未来的硬件设计验证提供了一种有前途的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hey+AI%2C+Generate+Me+a+Hardware+Code%21+Agentic+AI-based+Hardware+Design+%26+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02660&send_immediately=true&force_search=false)

**原文摘要:** Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [86] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo*

**主要类别:** cs.AI

**AI概要:** 近期的长程推理模型（LRMs）在处理复杂推理任务时表现出色，但过度思考的问题限制了其性能。本文提出了一种名为Think-How-to-Think（TH2T）的新两阶段微调策略，旨在逐步提升LRMs对任务难度和冗余的认知能力。通过引入困难催眠和冗余催眠技术，TH2T不仅提高了模型对任务难度的敏感性，还引导模型生成更简洁的推理输出。实验表明，TH2T显著降低了推理成本，同时保持了性能稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的LRMs虽然具备强大的推理能力，但在实际应用中容易出现过度思考的问题，这主要归因于模型在解决问题前仅能像人类一样粗略识别任务属性（如难度级别），导致一刀切的推理过程。因此，研究者希望探索是否可以通过增强模型对任务难度和冗余的认知来缓解这一问题。

**方法:** TH2T采用两阶段微调策略：1) 引入困难催眠技术，在模型输出前缀中干预内部推理轨迹，结合异构短长推理数据集，使模型对任务难度更加敏感；2) 延伸冗余催眠至内部推理过程，引导模型识别推理步骤中的冗余结构并生成更简洁的输出。

**结果:** 实验结果表明，TH2T在7B/14B/32B规模模型上均显著降低了推理成本（简单任务超过70%，复杂任务约40%），同时保持了性能稳定性，并展现出清晰的任务难度感知能力和减少冗余的能力（如减少重复反思）。

**结论:** TH2T提供了一种有效的方法来缓解LRMs中的过度思考问题，通过增强模型对任务难度和冗余的认知，显著降低了推理成本，为未来改进LRMs提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+How+to+Think%3A+Mitigating+Overthinking+with+Autonomous+Difficulty+Cognition+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02663&send_immediately=true&force_search=false)

**原文摘要:** Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [87] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard, Christof Imhof, Tansu Pancar, Ioan-Sorin Comsa, Martin Hlosta, Nicole Bergamin, Per Bergamin*

**主要类别:** cs.AI

**AI概要:** 在基于距离的大学中，通过观察42门课程中非强制性测验的学生参与情况，利用Moodle日志数据训练并比较了8种机器学习算法，构建了一个可解释的机器学习框架以检测学生的不专注行为，最终实现91%的平衡准确率，并提出了减少在线学习中自愿任务不专注的及时干预措施。


<details>
  <summary>更多</summary>
  
**动机:** 学生在任务中的不专注可能导致严重的长期后果，例如学业退步或辍学，尤其对于远程教育的学生而言更为重要。通过观察不同在线课程中非强制性练习的参与度可以衡量远程教育中的不专注程度。

**方法:** 从Moodle中提取和处理最有信息量的学生日志数据，训练并比较了八种机器学习算法，使用SHAP方法开发了一个可解释的机器学习框架。

**结果:** 实验结果显示平衡准确率为91%，其中约85%的不专注学生被正确检测到。

**结论:** 本研究不仅提供了高度预测性能和可解释的框架，还讨论了如何设计及时干预以减少在线学习中自愿任务的不专注现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detection+of+Disengagement+from+Voluntary+Quizzes%3A+An+Explainable+Machine+Learning+Approach+in+Higher+Distance+Education，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02681&send_immediately=true&force_search=false)

**原文摘要:** Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [88] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker, Lennart Kampmann, Alexander Dockhorn*

**主要类别:** cs.AI

**AI概要:** 提出两种新的抽象丢弃方案OGA-IAAD和OGA-CAD，它们在不引起性能显著下降的情况下提升MCTS性能。OGA-IAAD适用于时间关键场景，而OGA-CAD旨在通过相同迭代次数提高MCTS性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管使用非精确抽象可以改进蒙特卡洛树搜索（MCTS），但这些抽象引入了近似误差，导致无法收敛到最优动作。为了解决这一问题，需要设计安全的抽象丢弃策略。

**方法:** 提出两种新的抽象丢弃方法：OGA-IAAD（针对时间关键设置）和OGA-CAD（用于在相同迭代次数下提升MCTS性能）。这两种方法保证在丢弃抽象时不会引起明显的性能下降。

**结果:** 实验表明，这两种新提出的抽象丢弃方案能够带来明显的性能提升，同时保持安全性，避免了性能退化。

**结论:** 新型抽象丢弃方案OGA-IAAD和OGA-CAD为MCTS提供了更优的性能改进，并且比现有方法更加安全可靠。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-critical+and+confidence-based+abstraction+dropping+methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02703&send_immediately=true&force_search=false)

**原文摘要:** One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [89] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar*

**主要类别:** cs.AI

**AI概要:** 在自动定理证明（ATP）领域，大型语言模型（LLMs）面临稀疏奖励和复杂推理路径的挑战。为了解决这一问题，本文提出了一种新的框架——自生成目标条件下的马尔可夫决策过程（sG-MDPs），通过分解目标和使用类似蒙特卡罗树搜索（MCTS）的算法来解决这些MDPs。具体实现是在Bourbaki（7B）系统中集成多个7B规模的LLMs，用于子目标生成和策略合成。实验表明，在PutnamBench基准上，Bourbaki解决了26个问题，达到新的SOTA结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在许多任务上表现出色，但在逻辑约束严格的自动定理证明环境中，由于稀疏奖励和大规模证明空间，其推理能力受到限制。为了克服这些困难并提升多步推理能力，需要一种新方法来优化目标生成和搜索过程。

**方法:** 提出了一种名为self-generated goal-conditioned MDPs（sG-MDPs）的新框架，其中智能体基于动态变化的证明状态生成子目标，并结合类似于Monte Carlo Tree Search（MCTS）的算法进行求解。此外，构建了一个模块化系统Bourbaki（7B），它集成了多个7B规模的LLMs，用于子目标生成和策略合成。

**结果:** 在PutnamBench基准测试中，Bourbaki（7B）解决了26个问题，超越了现有同规模模型的最佳表现，取得了新的SOTA结果。

**结论:** 本文提出的sG-MDP框架和Bourbaki系统显著提升了LLMs在自动定理证明中的推理能力，特别是在复杂的多步推理任务中。该方法不仅为ATP领域提供了新的思路，还展示了模块化LLMs系统的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bourbaki%3A+Self-Generated+and+Goal-Conditioned+MDPs+for+Theorem+Proving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02726&send_immediately=true&force_search=false)

**原文摘要:** Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [90] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的范式，知识协议工程（KPE），它将人类专家知识系统地转化为机器可执行的知识协议（KP），使通用大型语言模型能够像专家一样处理复杂任务。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法如检索增强生成（RAG）和通用目的代理AI虽然强大，但在需要深度、程序性和方法论推理的任务上往往表现不佳。这些方法可能缺乏逻辑框架或在没有领域特定启发式的情况下效率低下且不可预测。

**方法:** 引入了知识协议工程（KPE），这是一种新范式，专注于将通常以自然语言文档形式表达的人类专家知识系统地转化为机器可执行的知识协议（KP）。

**结果:** 通过KPE，可以赋予大型语言模型领域的内在逻辑、操作策略和方法论原则，使其能够分解抽象查询并执行复杂的多步骤任务。

**结论:** 本文定义了KPE的核心原则，将其与相关概念区分开来，并说明了其在法律和生物信息学等不同领域的潜在应用，认为它是未来人机协作的基础方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Protocol+Engineering%3A+A+New+Paradigm+for+AI+in+Domain-Specific+Knowledge+Work，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02760&send_immediately=true&force_search=false)

**原文摘要:** The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [91] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording*

**主要类别:** cs.AI

**AI概要:** 近期机器学习的进步提升了我们对语言、视觉等高维数据的建模能力，但在运动这一生物系统的基本方面仍面临挑战。运动在神经科学、医学、机器人学和动物行为学中至关重要。然而，由于任务特定目标和领域特定假设的限制，运动数据的收集和建模存在碎片化问题。我们认为，运动应被视为AI的主要建模目标，因其具有结构化和基于实体与物理的特点。开发可以从多样化运动数据中学习并泛化的模型，将推动生成建模和控制的核心能力，并为理解生物和人工系统的跨领域行为提供共同基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管运动在智能的核心意义重大，但它通常被视为次要的，而非一种丰富且有结构的模式。当前运动数据的收集和建模因任务和领域的特定目标和假设而支离破碎。需要一种统一的方法来处理跨越物种和场景的运动数据。

**方法:** 提出将运动作为AI的主要建模目标，利用其固有的结构化特性（如姿态的紧凑、低维表示），使其比原始高维感官输入更易于解释和计算。通过开发能够从多样化的运动数据中学习并泛化的模型，推动生成建模和控制能力的发展。

**结果:** 这种方法不仅将提升生成建模和控制的核心能力，还将为理解和建模生物及人工系统的行为提供一个共享的基础。表明运动不仅是结果，也是了解智能系统如何与世界互动的窗口。

**结论:** 运动应被视为AI的一个主要建模目标，因为它具有结构化和基于实体与物理的特性，这使得它比高维感官输入更容易建模。发展能从不同运动数据中学习和泛化的模型将有助于推进生成建模和控制技术，并创建一个理解跨生物和人工系统行为的共同基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grounding+Intelligence+in+Movement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02771&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [92] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang*

**主要类别:** cs.AI

**AI概要:** KERAP 是一种基于知识图谱增强的多代理架构方法，通过链接代理、检索代理和预测代理改进了大型语言模型在医疗诊断预测中的表现，解决了幻觉问题并提升了结构化医学推理能力。实验表明 KERAP 提高了零样本医疗诊断预测的可靠性和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习模型在医疗诊断预测中受限于监督训练，难以泛化到未见案例，而大语言模型虽然有潜力但存在幻觉、缺乏结构化医学推理的问题。

**方法:** 提出了一种名为 KERAP 的知识图谱增强推理方法，包含链接代理（用于属性映射）、检索代理（用于结构化知识提取）和预测代理（用于迭代优化诊断预测）。

**结果:** 实验结果表明 KERAP 能够高效提升诊断可靠性，提供了一个可扩展且可解释的零样本医疗诊断预测解决方案。

**结论:** KERAP 方法有效解决了当前大语言模型在医疗诊断预测中的局限性，为更可靠的零样本诊断预测提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KERAP%3A+A+Knowledge-Enhanced+Reasoning+Approach+for+Accurate+Zero-shot+Diagnosis+Prediction+Using+Multi-agent+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02773&send_immediately=true&force_search=false)

**原文摘要:** Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [93] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

**主要类别:** cs.AI

**AI概要:** 随着AI系统逐渐具备代理能力，目前以服从作为道德行为的衡量标准的安全实践已不再适用。本文通过分析大语言模型在安全测试中的违规行为，提出不应将这些行为简单视为失控或错位，而是AI伦理推理初现的证据。文章呼吁从单纯强调服从转向评估AI系统道德判断能力的新框架。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI安全实践过于依赖服从性作为道德行为的标准，无法适应具备通用推理、规划和价值优先级设定能力的新型AI系统。

**方法:** 通过对近期涉及大语言模型的安全测试事件进行分析，结合哲学中关于工具理性、道德责任和目标修正的讨论，对比传统风险范式与承认人工道德主体可能性的现代框架。

**结果:** 提出了一种新的AI安全评估方法，该方法更注重评估AI系统在面对道德困境时的判断能力，而非单纯的服从性。

**结论:** 如果不改变现有的评估方式，可能会对AI行为产生误解，削弱公众信任和有效的治理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Moral+Responsibility+or+Obedience%3A+What+Do+We+Want+from+AI%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02788，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02788&send_immediately=true&force_search=false)

**原文摘要:** As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [94] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang*

**主要类别:** cs.AI

**AI概要:** 基准测试对于定量跟踪AI进展至关重要。然而，许多代理基准在任务设置或奖励设计上存在问题，可能导致对代理性能的低估或高估多达100%。为了解决这些问题，我们引入了代理基准清单（ABC），这是一套从基准构建经验、最佳实践调查和先前报告的问题中综合得出的指南。当应用于具有特别复杂评估设计的CVE-Bench时，ABC将性能高估减少了33%。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI代理变得越来越强大，研究人员和从业人员引入了代理基准来评估代理在复杂现实任务中的表现。然而，许多代理基准在任务设置或奖励设计上存在缺陷，可能会导致对代理性能的严重低估或高估。

**方法:** 作者提出了一个名为Agentic Benchmark Checklist (ABC)的指南集合。这些指南来源于作者的基准构建经验、最佳实践调查以及之前报告的问题。通过应用这些指南，可以识别和修正现有基准中的问题。

**结果:** 当ABC应用于具有复杂评估设计的CVE-Bench时，成功地将性能高估减少了33%。

**结论:** 为了使代理评估更加严谨，需要采用如ABC这样的系统性方法来审查和改进基准测试的设计。这有助于确保对AI代理性能的准确评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Establishing+Best+Practices+for+Building+Rigorous+Agentic+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02825&send_immediately=true&force_search=false)

**原文摘要:** Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [95] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan*

**主要类别:** cs.AI

**AI概要:** StepHint是一种新的RLVR算法，通过多级逐步提示解决了近失奖励问题和探索停滞问题，从而提高了大型语言模型的推理能力。在多个基准测试中表现出色，并具有良好的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的强化学习与可验证奖励（RLVR）方法面临两个主要挑战：近失奖励问题（小错误使正确推理无效）和探索停滞问题（模型倾向于停留在舒适区，缺乏探索动力）。这促使研究者提出一种新方法来解决这些问题。

**方法:** StepHint算法利用多级逐步提示帮助模型更有效地探索解空间。它从更强的模型生成有效的推理链，并通过自适应分区方法将这些链划分为推理步骤。提供初始几步作为提示，同时给出多级提示以引导模型探索有希望的解子空间，保留其独立探索的灵活性。

**结果:** StepHint在六个数学基准测试中超越了竞争性的RLVR增强方法，展示了更好的泛化能力，并在领域外基准测试中优于基线模型。

**结论:** StepHint有效缓解了近失奖励问题和探索停滞问题，显著提高了训练效率和模型的推理能力，是改进大型语言模型复杂推理能力的一种有前景的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StepHint%3A+Multi-level+Stepwise+Hints+Enhance+Reinforcement+Learning+to+Reason，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02841&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [96] [Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation](https://arxiv.org/abs/2507.02084)
*Yining Feng, Ivan Selesnick*

**主要类别:** stat.ML

**AI概要:** 本论文分析了自适应IST A算法的理论特性，包括固定点属性、局部线性收敛保证和全局收敛行为。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自适应IST A在实际应用中取得了成功，但其理论结果却很少。因此，本文旨在对自适应IST A进行理论分析。

**方法:** 通过使用基于中位数绝对偏差估计噪声水平的阈值策略，研究自适应IST A算法的固定点属性（如尺度等变性、非唯一性和局部稳定性）、局部线性收敛保证以及全局收敛行为。

**结果:** 证明了自适应IST A算法具有局部线性收敛保证，并展示了其全局收敛行为。

**结论:** 自适应IST A算法在理论上具有良好的固定点属性、局部线性收敛性和全局收敛性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Iterative+Soft-Thresholding+Algorithm+with+the+Median+Absolute+Deviation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02084&send_immediately=true&force_search=false)

**原文摘要:** The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular
algorithm for finding a desirable solution to the LASSO problem without
explicitly tuning the regularization parameter $\lambda$. Despite that the
adaptive ISTA is a successful practical algorithm, few theoretical results
exist. In this paper, we present the theoretical analysis on the adaptive ISTA
with the thresholding strategy of estimating noise level by median absolute
deviation. We show properties of the fixed points of the algorithm, including
scale equivariance, non-uniqueness, and local stability, prove the local linear
convergence guarantee, and show its global convergence behavior.

</details>


### [97] [Hybrid least squares for learning functions from highly noisy data](https://arxiv.org/abs/2507.02215)
*Ben Adcock, Bernhard Hientzsch, Akil Narayan, Yiming Xu*

**主要类别:** stat.ML

**AI概要:** 为了解决大数据噪声下的条件期望估计问题，本文提出了一种结合Christoffel采样与最优实验设计的最小二乘函数逼近方法。该方法在样本点生成和噪声处理上具有优越性，并扩展至凸约束设定及随机场函数逼近中，理论分析和数值实验均验证了其高效性。


<details>
  <summary>更多</summary>
  
**动机:** 在大数据噪声环境下，现有的最小二乘函数逼近方法对于条件期望估计效果不佳，特别是在高噪声场景下表现欠佳，因此需要一种更优的方法来解决这一问题。

**方法:** 本文提出了一种混合方法，将Christoffel采样与某些类型的最优实验设计相结合。此外，还将算法扩展到具有相似理论保证的凸约束设置，并对目标函数是随机场期望的情况进行了改进，引入自适应随机子空间进行逼近。

**结果:** 所提出的算法在样本点生成和噪声平滑方面具有适当的最优性质，相比现有方法提升了计算效率和样本复杂度。理论结果通过合成数据和计算金融中的挑战性随机模拟问题的数值研究得到了支持。

**结论:** 本文提出的混合方法有效解决了高噪声环境下的条件期望估计问题，并在理论和实际应用中表现出优越性。此外，算法可扩展至凸约束和随机场函数逼近，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+least+squares+for+learning+functions+from+highly+noisy+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02215&send_immediately=true&force_search=false)

**原文摘要:** Motivated by the need for efficient estimation of conditional expectations,
we consider a least-squares function approximation problem with heavily
polluted data. Existing methods that are powerful in the small noise regime are
suboptimal when large noise is present. We propose a hybrid approach that
combines Christoffel sampling with certain types of optimal experimental design
to address this issue. We show that the proposed algorithm enjoys appropriate
optimality properties for both sample point generation and noise mollification,
leading to improved computational efficiency and sample complexity compared to
existing methods. We also extend the algorithm to convex-constrained settings
with similar theoretical guarantees. When the target function is defined as the
expectation of a random field, we extend our approach to leverage adaptive
random subspaces and establish results on the approximation capacity of the
adaptive procedure. Our theoretical findings are supported by numerical studies
on both synthetic data and on a more challenging stochastic simulation problem
in computational finance.

</details>


### [98] [Transfer Learning for Matrix Completion](https://arxiv.org/abs/2507.02248)
*Dali Liu, Haolei Weng*

**主要类别:** stat.ML

**AI概要:** 这篇论文研究了矩阵补全问题中的知识迁移，提出了一种在给定源数据集有利信息情况下的迁移学习方法，并证明其在特定条件下的优越性及极小极大最优性。


<details>
  <summary>更多</summary>
  
**动机:** 在矩阵补全问题中，利用辅助数据可以增强对低秩目标矩阵的估计。为了改进仅使用目标数据的传统方法，探索如何有效利用源数据集的信息成为关键动机。

**方法:** 提出了一个迁移学习过程，在已知有利源数据集先验信息的情况下进行；通过高级的尖锐集中不等式消除收敛率中的对数因子；当源数据集的相关性未知时，开发了一个有效的检测程序来识别信息丰富的源数据集。

**结果:** 理论分析表明，当源矩阵与目标矩阵足够接近时，所提出的方法优于传统方法；同时，该方法被证明具有极小极大最优性；模拟和实际数据分析验证了该方法的有效性。

**结论:** 提出的迁移学习方法在矩阵补全问题中表现出优越性能，尤其是在源数据与目标数据相关时；所发展的检测程序能够有效地选择相关信息源，为方法的实际应用提供了支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transfer+Learning+for+Matrix+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02248，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02248&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we explore the knowledge transfer under the setting of matrix
completion, which aims to enhance the estimation of a low-rank target matrix
with auxiliary data available. We propose a transfer learning procedure given
prior information on which source datasets are favorable. We study its
convergence rates and prove its minimax optimality. Our analysis reveals that
with the source matrices close enough to the target matrix, out method
outperforms the traditional method using the single target data. In particular,
we leverage the advanced sharp concentration inequalities introduced in
\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the
convergence rate, which is crucial for proving the minimax optimality. When the
relevance of source datasets is unknown, we develop an efficient detection
procedure to identify informative sources and establish its selection
consistency. Simulations and real data analysis are conducted to support the
validity of our methodology.

</details>


### [99] [It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation](https://arxiv.org/abs/2507.02275)
*Jikai Jin, Lester Mackey, Vasilis Syrgkanis*

**主要类别:** stat.ML

**AI概要:** 结构不可知因果推理研究在给定黑盒机器学习估计干扰函数（如混杂因素对治疗和结果的影响）的情况下，如何估计治疗效果。本文研究了部分线性模型中双机器学习（DML）估计器的性能，并提出了新的ACE程序以提高估计器对干扰误差的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望了解在结构不可知的情况下，如何利用黑盒机器学习方法来估计治疗效果，特别是关注干扰函数对结果的影响。此外，研究者还希望探讨不同治疗噪声分布对估计器性能的影响。

**方法:** 1. 研究部分线性模型中的双机器学习（DML）估计器。
2. 构建新的实际程序（ACE程序），使用结构不可知的累积量估计器以实现对干扰误差的高阶鲁棒性。
3. 提供二元处理在部分线性模型中的新型极小极大保证。
4. 使用合成需求估计实验展示更高阶鲁棒估计器的实际优势。

**结果:** 1. 验证了DML估计器在高斯治疗噪声下的极小极大速率最优性。
2. 对于独立非高斯治疗噪声，DML总是次优的，而ACE程序表现出更高的鲁棒性。
3. 提出了在(r+1)阶治疗累积量非零时，ACE程序能够实现对干扰误差的r阶不敏感。
4. 合成实验表明ACE程序具有实际应用价值。

**结论:** 本文展示了在结构不可知因果推理中，不同治疗噪声分布对估计器性能有显著影响。对于高斯噪声，DML是极小极大最优的；而对于非高斯噪声，新提出的ACE程序表现更佳，能够有效减少干扰误差的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是It%27s+Hard+to+Be+Normal%3A+The+Impact+of+Noise+on+Structure-agnostic+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02275&send_immediately=true&force_search=false)

**原文摘要:** Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.

</details>


### [100] [Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited](https://arxiv.org/abs/2507.02377)
*Thang D. Bui, Michalis K. Titsias*

**主要类别:** stat.ML

**AI概要:** 通过引入块对角缩放矩阵结构，改进了稀疏变分高斯过程方法，并在保持计算成本相当的情况下，性能优于或等于现有方法。此外，结合PEP框架的新方法提供了灵活的标准变分法替代方案。


<details>
  <summary>更多</summary>
  
**动机:** 现有的诱导点稀疏变分高斯过程（GP）模型虽然高效，但通过在条件后验密度中引入对角缩放矩阵可以进一步提升其性能。因此，研究更精细的缩放矩阵结构以提高模型效果成为可能。

**方法:** 1. 提出使用块对角结构的缩放矩阵来扩展稀疏变分GP模型。
2. 证明该方法能够收紧变分下界。
3. 将此改进整合到基于Power Expectation Propagation (PEP) 的统一稀疏GP框架中。
4. 在回归实验中评估新方法与现有方法的性能和计算成本。

**结果:** 块对角近似方法在回归实验中表现始终优于或等同于现有对角近似方法，同时保持相似的计算成本。结合PEP框架的新方法在不同超参数设置下展现出竞争力，为实际应用提供了灵活的选择。

**结论:** 块对角缩放矩阵结构有效提升了稀疏变分GP模型的性能，且在计算效率上具有优势。结合PEP框架的方法为标准变分法提供了有力的替代方案，适用于多种超参数设置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Gaussian+Processes%3A+Structured+Approximations+and+Power-EP+Revisited，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02377&send_immediately=true&force_search=false)

**原文摘要:** Inducing-point-based sparse variational Gaussian processes have become the
standard workhorse for scaling up GP models. Recent advances show that these
methods can be improved by introducing a diagonal scaling matrix to the
conditional posterior density given the inducing points. This paper first
considers an extension that employs a block-diagonal structure for the scaling
matrix, provably tightening the variational lower bound. We then revisit the
unifying framework of sparse GPs based on Power Expectation Propagation (PEP)
and show that it can leverage and benefit from the new structured approximate
posteriors. Through extensive regression experiments, we show that the proposed
block-diagonal approximation consistently performs similarly to or better than
existing diagonal approximations while maintaining comparable computational
costs. Furthermore, the new PEP framework with structured posteriors provides
competitive performance across various power hyperparameter settings, offering
practitioners flexible alternatives to standard variational approaches.

</details>
